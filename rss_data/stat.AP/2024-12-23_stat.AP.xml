<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 23 Dec 2024 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Protocol for an Observational Study on the Effects of Paternal Alcohol Use Disorder on Children's Later Life Outcomes</title>
      <link>https://arxiv.org/abs/2412.15535</link>
      <description>arXiv:2412.15535v1 Announce Type: new 
Abstract: The harmful effects of growing up with a parent with an alcohol use disorder have been closely examined in children and adolescents, and are reported to include mental and physical health problems, interpersonal difficulties, and a worsened risk of future substance use disorders. However, few studies have investigated how these impacts evolve into later life adulthood, leaving the ensuing long-term effects of interest. In this article, we provide the protocol for our observational study of the long-term consequences of growing up with a father who had an alcohol use disorder. We will use data from the Wisconsin Longitudinal Study to examine impacts on long-term economic success, interpersonal relationships, physical, and mental health. To reinforce our findings, we will conduct this investigation on two discrete subpopulations of individuals in our study, allowing us to analyze the replicability of our conclusions. We introduce a novel statistical design, called data turnover, to carry out this analysis. Data turnover allows a single group of statisticians and domain experts to work together to assess the strength of evidence gathered across multiple data splits, while incorporating both qualitative and quantitative findings from data exploration. We delineate our analysis plan using this new method and conclude with a brief discussion of some additional considerations for our study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15535v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Marina Bogomolov, Ruth Heller, Matthew Spivey, Kevin G. Lynch, David W. Oslin, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>A District-level Ensemble Model to Enhance Dengue Prediction and Control for the Mekong Delta Region of Vietnam</title>
      <link>https://arxiv.org/abs/2412.15645</link>
      <description>arXiv:2412.15645v1 Announce Type: new 
Abstract: The Mekong Delta Region of Vietnam faces increasing dengue risks driven by urbanization, globalization, and climate change. This study introduces a probabilistic forecasting model for predicting dengue incidence and outbreaks with one to three month lead times, integrating meteorological, sociodemographic, preventive, and epidemiological data. Seventy-two models were evaluated, and an ensemble combining top-performing spatiotemporal, supervised PCA, and semi-mechanistic hhh4 frameworks was developed. Using data from 2004-2022 for training, validation, and evaluation, the ensemble model demonstrated 69% accuracy at a 3-month horizon, outperforming a baseline model. While effective, its performance declined in years with atypical seasonality, such as 2019 and 2022. The model provides critical lead time for targeted dengue prevention and control measures, addressing a growing public health need in the region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15645v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wala Draidi Areed, Thi Thanh Thao Nguyen, Kien Quoc Do, Thinh Nguyen, Vinh Bui, Elisabeth Nelson, Joshua L. Warren, Quang-Van Doan, Nam Vu Sinh, Nicholas Osborne, Russell Richards, Nu Quy Linh Tran, Hong Le, Tuan Pham, Trinh Manh Hung, Son Nghiem, Hai Phung, Cordia Chu, Robert Dubrow, Daniel M. Weinberger, Dung Phung</dc:creator>
    </item>
    <item>
      <title>Climate Impact Assessment Requires Weighting: Introducing the Weighted Climate Dataset</title>
      <link>https://arxiv.org/abs/2412.15699</link>
      <description>arXiv:2412.15699v1 Announce Type: new 
Abstract: High-resolution gridded climate data are readily available from multiple sources, yet climate research and decision-making increasingly require country and region-specific climate information weighted by socio-economic factors. Moreover, the current landscape of disparate data sources and inconsistent weighting methodologies exacerbates the reproducibility crisis and undermines scientific integrity. To address these issues, we have developed a globally comprehensive dataset at both country (GADM0) and region (GADM1) levels, encompassing various climate indicators (precipitation, temperature, SPEI, wind gust). Our methodology involves weighting gridded climate data by population density, night-time light intensity, cropland area, and concurrent population count -- all proxies for socio-economic activity -- before aggregation. We process data from multiple sources, offering daily, monthly, and annual climate variables spanning from 1900 to 2023. A unified framework streamlines our preprocessing steps, and rigorous validation against leading climate impact studies ensures data reliability. The resulting Weighted Climate Dataset is publicly accessible through an online dashboard at https://weightedclimatedata.streamlit.app/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15699v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>NeurIPS 2024 workshop on Tackling Climate Change with Machine Learning</arxiv:journal_reference>
      <dc:creator>Marco Gortan, Lorenzo Testa, Giorgio Fagiolo, Francesco Lamperti</dc:creator>
    </item>
    <item>
      <title>Static and Dynamic Load Tests on the Bridge Vahrendorfer Stadtweg</title>
      <link>https://arxiv.org/abs/2412.15713</link>
      <description>arXiv:2412.15713v1 Announce Type: new 
Abstract: Load tests are an essential tool to ensure the compliance of bridges with their design specifications. In this paper, a series of static and dynamic load tests on the bridge Vahrendorfer Stadtweg are documented. The bridge is equipped with a long-term Structural Health Monitoring (SHM) system, providing data on an entire seasonal cycle. The objectives of the static and dynamic tests are (i) to capture the bridge's current condition under various loading scenarios while identifying potential structural weaknesses, (ii) to evaluate the system's sensitivity to small mass variations, and (iii) to generate data for model calibration and validation of anomaly detection algorithms by simulating a design load case. Additionally, the paper conducts plausibility checks on the measurement data and outlines the measurement campaign. In particular, the long-term reference measurements and the time-synchronous additional measurements during the load test have provided an appropriate data set, which can be used in the future to validate finite element models and damage detection algorithms. Follow-up publications will analyze the Structural Health Monitoring (SHM) data in collaboration with multiple research groups. The collected data will also be made available upon request for academic research purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15713v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin K\"ohncke, Yogi Jaelani, Alexander Mendler, Lizzie Neumann, Philipp Wittenberg, Alina Rode-Klemm, Sylvia Ke{\ss}ler</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Earthquake Impact Modelling</title>
      <link>https://arxiv.org/abs/2412.15791</link>
      <description>arXiv:2412.15791v1 Announce Type: new 
Abstract: Immediately following a disaster event, such as an earthquake, estimates of the damage extent play a key role in informing the coordination of response and recovery efforts. We develop a novel impact estimation tool that leverages a generalised Bayesian approach to generate earthquake impact estimates across three impact types: mortality, population displacement, and building damage. Inference is performed within a likelihood-free framework, and a scoring-rule-based posterior avoids information loss from non-sufficient summary statistics. We propose an adaptation of existing scoring-rule-based loss functions that accommodates the use of an approximate Bayesian computation sequential Monte Carlo (ABC-SMC) framework. The fitted model achieves results comparable to those of two leading impact estimation tools in the prediction of total mortality when tested on a set of held-out past events. The proposed method provides four advantages over existing empirical approaches: modelling produces a gridded spatial map of the estimated impact, predictions benefit from the Bayesian quantification and interpretation of uncertainty, there is direct handling of multi-shock earthquake events, and the use of a joint model between impact types allows predictions to be updated as impact observations become available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15791v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Anderson Loake, Hamish Patten, David Steinsaltz</dc:creator>
    </item>
    <item>
      <title>TraianProt: a user-friendly R shiny application for wide format proteomics data downstream analysis</title>
      <link>https://arxiv.org/abs/2412.15806</link>
      <description>arXiv:2412.15806v1 Announce Type: new 
Abstract: Summary: Mass spectrometry coupled to liquid chromatography (LC-MS/MS) is a powerful technique for the charac-terisation of proteomes. However, the diverse software platforms available for processing the raw proteomics data, each produce their own output format, making the extraction of meaningful and interpretable results a difficult task. We present TraianProt, a web-based, user-friendly proteomics data analysis platform, that enables the analysis of both label-free and labeled data from Data-Dependent or Data-Independent Acquisition mass spectrometry mode support-ing different computational platforms such as MaxQuant, MSFragger, DIA-NN, ProteoScape and Proteome Discoverer output formats. TraianProt provides a dynamic framework that includes several processing modules allowing the user to perform a complete downstream analysis covering the stages of data pre-processing, differential expression analy-sis, functional analysis and protein-protein interaction analysis. Data output includes a wide range of high-quality, cus-tomisable graphs such as heatmap, volcano plot, boxplot and barplot. This allows users to extract biological insights from proteomic data without any programming skills. Availability and implementation: TraianProt is implemented in R. Its code and documentation are available on GitHub at https://github.com/SamueldelaCamaraFuentes/TraianProt along with a step-by-step tutorial incorporated in the repository. Contact: sdelacam@ucm.es Supplementary information: Supplementary data are available at Bioinformatics online</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15806v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel de la Camara-Fuentes, Dolores Gutierrez-Blazquez, Maria Luisa Hernaez, Concha Gil</dc:creator>
    </item>
    <item>
      <title>Geographic distribution of the global agricultural workforce every decade for the years 2000-2100</title>
      <link>https://arxiv.org/abs/2412.15841</link>
      <description>arXiv:2412.15841v1 Announce Type: new 
Abstract: Agricultural workers play a vital role in the global economy and food security by cultivating, transporting, and processing food for populations worldwide. Despite their importance, detailed spatial data on the global agricultural workforce have remained scarce. Here, we present a new gridded dataset that maps the global distribution of agricultural workers for every decade over the years 2000-2100, distributed at 0.083$\times$0.083 degrees resolution, roughly $\sim$10km$\times$10km at the Equator. The dataset is developed using an empirical modeling framework relying on generalized additive mixed models (GAMMs) that integrate socioeconomic variables, including gross domestic product per capita, total population, rural population size, and agricultural land use. The predictions are consistent with Shared Socio-economic Pathways and we distribute full time series data for all SSPs 1 to 5. This dataset opens new avenues for future research on labour force health, productivity and risk, and could be very useful for developing informed, forward-looking strategies that address the challenges of climate resilience in agriculture. The dataset and code for reproducing it are available for the user community [publicly available on publication at DOI: 10.5281/zenodo.14443333].</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15841v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Naia Ormaza Zulueta, Steve Miller, Zia Mehrabi</dc:creator>
    </item>
    <item>
      <title>Learning-by-teaching with ChatGPT: The effect of teachable ChatGPT agent on programming education</title>
      <link>https://arxiv.org/abs/2412.15226</link>
      <description>arXiv:2412.15226v1 Announce Type: cross 
Abstract: This study investigates the potential of using ChatGPT as a teachable agent to support students' learning by teaching process, specifically in programming education. While learning by teaching is an effective pedagogical strategy for promoting active learning, traditional teachable agents have limitations, particularly in facilitating natural language dialogue. Our research explored whether ChatGPT, with its ability to engage learners in natural conversations, can support this process. The findings reveal that interacting with ChatGPT improves students' knowledge gains and programming abilities, particularly in writing readable and logically sound code. However, it had limited impact on developing learners' error-correction skills, likely because ChatGPT tends to generate correct code, reducing opportunities for students to practice debugging. Additionally, students' self-regulated learning (SRL) abilities improved, suggesting that teaching ChatGPT fosters learners' higher self-efficacy and better implementation of SRL strategies. This study discussed the role of natural dialogue in fostering socialized learning by teaching, and explored ChatGPT's specific contributions in supporting students' SRL through the learning by teaching process. Overall, the study highlights ChatGPT's potential as a teachable agent, offering insights for future research on ChatGPT-supported education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15226v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angxuan Chen, Yuang Wei, Huixiao Le, Yan Zhang</dc:creator>
    </item>
    <item>
      <title>Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations</title>
      <link>https://arxiv.org/abs/2412.15433</link>
      <description>arXiv:2412.15433v1 Announce Type: cross 
Abstract: We present a quantitative model for tracking dangerous AI capabilities over time. Our goal is to help the policy and research community visualise how dangerous capability testing can give us an early warning about approaching AI risks. We first use the model to provide a novel introduction to dangerous capability testing and how this testing can directly inform policy. Decision makers in AI labs and government often set policy that is sensitive to the estimated danger of AI systems, and may wish to set policies that condition on the crossing of a set threshold for danger. The model helps us to reason about these policy choices. We then run simulations to illustrate how we might fail to test for dangerous capabilities. To summarise, failures in dangerous capability testing may manifest in two ways: higher bias in our estimates of AI danger, or larger lags in threshold monitoring. We highlight two drivers of these failure modes: uncertainty around dynamics in AI capabilities and competition between frontier AI labs. Effective AI policy demands that we address these failure modes and their drivers. Even if the optimal targeting of resources is challenging, we show how delays in testing can harm AI policy. We offer preliminary recommendations for building an effective testing ecosystem for dangerous capabilities and advise on a research agenda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15433v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Bova, Alessandro Di Stefano, The Anh Han</dc:creator>
    </item>
    <item>
      <title>Simulation-based Bayesian predictive probability of success for interim monitoring of clinical trials with competing event data: two case studies</title>
      <link>https://arxiv.org/abs/2412.15899</link>
      <description>arXiv:2412.15899v1 Announce Type: cross 
Abstract: Bayesian predictive probabilities of success (PPoS) use interim trial data to calculate the probability of trial success. These quantities can be used to optimize trial size or to stop for futility. In this paper, we describe a simulation-based approach to compute the PPoS for clinical trials with competing event data, for which no specific methodology is currently available. The proposed procedure hinges on modelling the joint distribution of time to event and event type by specifying Bayesian models for the cause-specific hazards of all event types. This allows the prediction of outcome data at the conclusion of the trial. The PPoS is obtained by numerically averaging the probability of success evaluated at fixed parameter values over the posterior distribution of the parameters. Our work is motivated by two randomised clinical trials: the I-SPY COVID phase II trial for the treatment of severe COVID-19 (NCT04488081) and the STHLM3 prostate cancer diagnostic trial (ISRCTN84445406), both of which are characterised by competing event data. We present different modelling alternatives for the joint distribution of time to event and event type and show how the choice of the prior distributions can be used to assess the PPoS under different scenarios. The role of the PPoS analyses in the decision making process for these two trials is also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15899v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Micoli, Alessio Crippa, Jason T. Connor, I-SPY COVID Consortium, Martin Eklund, Andrea Discacciati</dc:creator>
    </item>
    <item>
      <title>Camera-Based Localization and Enhanced Normalized Mutual Information</title>
      <link>https://arxiv.org/abs/2412.16137</link>
      <description>arXiv:2412.16137v1 Announce Type: cross 
Abstract: Robust and fine localization algorithms are crucial for autonomous driving. For the production of such vehicles as a commodity, affordable sensing solutions and reliable localization algorithms must be designed. This work considers scenarios where the sensor data comes from images captured by an inexpensive camera mounted on the vehicle and where the vehicle contains a fine global map. Such localization algorithms typically involve finding the section in the global map that best matches the captured image. In harsh environments, both the global map and the captured image can be noisy. Because of physical constraints on camera placement, the image captured by the camera can be viewed as a noisy perspective transformed version of the road in the global map. Thus, an optimal algorithm should take into account the unequal noise power in various regions of the captured image, and the intrinsic uncertainty in the global map due to environmental variations. This article briefly reviews two matching methods: (i) standard inner product (SIP) and (ii) normalized mutual information (NMI). It then proposes novel and principled modifications to improve the performance of these algorithms significantly in noisy environments. These enhancements are inspired by the physical constraints associated with autonomous vehicles. They are grounded in statistical signal processing and, in some context, are provably better. Numerical simulations demonstrate the effectiveness of such modifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16137v1</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishnu Teja Kunde, Jean-Francois Chamberland, Siddharth Agarwal</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Kernel Two-Sample Tests</title>
      <link>https://arxiv.org/abs/2302.07415</link>
      <description>arXiv:2302.07415v4 Announce Type: replace-cross 
Abstract: We consider the variable selection problem for two-sample tests, aiming to select the most informative variables to determine whether two collections of samples follow the same distribution. To address this, we propose a novel framework based on the kernel maximum mean discrepancy (MMD). Our approach seeks a subset of variables with a pre-specified size that maximizes the variance-regularized kernel MMD statistic. We focus on three commonly used types of kernels: linear, quadratic, and Gaussian. From a computational perspective, we derive mixed-integer programming formulations and propose exact and approximation algorithms with performance guarantees to solve these formulations. From a statistical viewpoint, we derive the rate of testing power of our framework under appropriate conditions. These results show that the sample size requirements for the three kernels depend crucially on the number of selected variables, rather than the data dimension. Experimental results on synthetic and real datasets demonstrate the superior performance of our method, compared to other variable selection frameworks, particularly in high-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07415v4</guid>
      <category>stat.ML</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jie Wang, Santanu S. Dey, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Learning ECG Signal Features Without Backpropagation Using Linear Laws</title>
      <link>https://arxiv.org/abs/2307.01930</link>
      <description>arXiv:2307.01930v2 Announce Type: replace-cross 
Abstract: This paper introduces LLT-ECG, a novel method for electrocardiogram (ECG) signal classification that leverages concepts from theoretical physics to automatically generate features from time series data. Unlike traditional deep learning approaches, LLT-ECG operates in a forward manner, eliminating the need for backpropagation and hyperparameter tuning. By identifying linear laws that capture shared patterns within specific classes, the proposed method constructs a compact and verifiable representation, enhancing the effectiveness of downstream classifiers. We demonstrate LLT-ECG's state-of-the-art performance on real-world ECG datasets from PhysioNet, underscoring its potential for medical applications where speed and verifiability are crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.01930v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P\'eter P\'osfay, Marcell T. Kurbucz, P\'eter Kov\'acs, Antal Jakov\'ac</dc:creator>
    </item>
    <item>
      <title>Optimizing Heat Alert Issuance with Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2312.14196</link>
      <description>arXiv:2312.14196v4 Announce Type: replace-cross 
Abstract: A key strategy in societal adaptation to climate change is using alert systems to prompt preventative action and reduce the adverse health impacts of extreme heat events. This paper implements and evaluates reinforcement learning (RL) as a tool to optimize the effectiveness of such systems. Our contributions are threefold. First, we introduce a new publicly available RL environment enabling the evaluation of the effectiveness of heat alert policies to reduce heat-related hospitalizations. The rewards model is trained from a comprehensive dataset of historical weather, Medicare health records, and socioeconomic/geographic features. We use scalable Bayesian techniques tailored to the low-signal effects and spatial heterogeneity present in the data. The transition model uses real historical weather patterns enriched by a data augmentation mechanism based on climate region similarity. Second, we use this environment to evaluate standard RL algorithms in the context of heat alert issuance. Our analysis shows that policy constraints are needed to improve RL's initially poor performance. Third, a post-hoc contrastive analysis provides insight into scenarios where our modified heat alert-RL policies yield significant gains/losses over the current National Weather Service alert policy in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14196v4</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25), AI for Social Impact Track. 2025</arxiv:journal_reference>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery, Gregory A. Wellenius, Francesca Dominici, Mauricio Tec</dc:creator>
    </item>
  </channel>
</rss>
