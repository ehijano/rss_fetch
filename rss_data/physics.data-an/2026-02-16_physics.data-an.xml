<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:01:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>When Stein-Type Test Detects Equilibrium Distributions of Finite N-Body Systems</title>
      <link>https://arxiv.org/abs/2602.12297</link>
      <description>arXiv:2602.12297v1 Announce Type: cross 
Abstract: Starting from the probability distribution of finite N-body systems, which maximises the Havrda--Charv\'at entropy, we build a Stein-type goodness-of-fit test. The Maxwell--Boltzmann distribution is exact only in the thermodynamic limit, where the system is composed of infinitely many particles as N approaches infinity. For an isolated system with a finite number of particles, the equilibrium velocity distribution is compact and markedly non-Gaussian, being restricted by the fixed total energy. Using Stein's method, we first obtain a differential operator that characterises the target density. Its eigenfunctions are symmetric Jacobi polynomials, whose orthogonality yields a simple, parameter-free statistic. Under the null hypothesis that the data follows the finite-N distribution, the statistic converges to a chi-squared law, so critical values are available in closed form. Large-scale Monte Carlo experiments confirm exact size control and give a clear picture of the power. These findings quantify how quickly a finite system approaches the classical limit and provide a practical tool for testing kinetic models in regimes where normality cannot be assumed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12297v1</guid>
      <category>math-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jae Wan Shim</dc:creator>
    </item>
    <item>
      <title>A Quantum Reservoir Computing Approach to Quantum Stock Price Forecasting in Quantum-Invested Markets</title>
      <link>https://arxiv.org/abs/2602.13094</link>
      <description>arXiv:2602.13094v1 Announce Type: cross 
Abstract: We present a quantum reservoir computing (QRC) framework based on a small-scale quantum system comprising at most six interacting qubits, designed for nonlinear financial time-series forecasting. We apply the model to predict future daily closing trading volumes of 20 quantum-sector publicly traded companies over the period from April 11, 2020, to April 11, 2025, as well as minute-by-minute trading volumes during out-of-market hours on July 7, 2025. Our analysis identifies optimal reservoir parameters that yield stock trend (up/down) classification accuracies exceeding $86 \%$. Importantly, the QRC model is platform-agnostic and can be realized across diverse physical implementations of qubits, including superconducting circuits and trapped ions. These results demonstrate the expressive power and robustness of small-scale quantum reservoirs for modeling complex temporal correlations in financial data, highlighting their potential applicability to real-world forecasting tasks on near-term quantum hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13094v1</guid>
      <category>quant-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wendy Otieno, Alexandre Zagoskin, Alexander G. Balanov, Juan Totero Gongora, Sergey E. Savel'ev</dc:creator>
    </item>
    <item>
      <title>Profiling systematic uncertainties in Simulation-Based Inference with Factorizable Normalizing Flows</title>
      <link>https://arxiv.org/abs/2602.13184</link>
      <description>arXiv:2602.13184v1 Announce Type: cross 
Abstract: Unbinned likelihood fits aim at maximizing the information one can extract from experimental data, yet their application in realistic statistical analyses is often hindered by the computational cost of profiling systematic uncertainties. Additionally, current machine learning-based inference methods are typically limited to estimating scalar parameters in a multidimensional space rather than full differential distributions. We propose a general framework for Simulation-Based Inference (SBI) that efficiently profiles nuisance parameters while measuring multivariate Distributions of Interest (DoI), defined as learnable invertible transformations of the feature space. We introduce Factorizable Normalizing Flows to model systematic variations as parametric deformations of a nominal density, preserving tractability without combinatorial explosion. Crucially, we develop an amortized training strategy that learns the conditional dependence of the DoI on nuisance parameters in a single optimization process, bypassing the need for repetitive training during the likelihood scan. This allows for the simultaneous extraction of the underlying distribution and the robust profiling of nuisances. The method is validated on a synthetic dataset emulating a high-energy physics measurement with multiple systematic sources, demonstrating its potential for unbinned, functional measurements in complex analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13184v1</guid>
      <category>hep-ph</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Valsecchi, Mauro Doneg\`a, Rainer Wallny</dc:creator>
    </item>
    <item>
      <title>Non-parametric finite-sample credible intervals with one-dimensional priors: a middle ground between Bayesian and frequentist intervals</title>
      <link>https://arxiv.org/abs/2601.17621</link>
      <description>arXiv:2601.17621v2 Announce Type: replace-cross 
Abstract: We propose a new type of statistical interval obtained by weakening the definition of a p% credible interval: After observing the interval, but not inspecting the full dataset ourselves, we should put at least a p% belief in it. From a decision-theoretical point of view the resulting intervals occupy a middle ground between frequentist and fully Bayesian statistical intervals, both practically and philosophically: To a p% Bayesian credible interval we should assign (at least a) p% belief after observing both the dataset and the interval, while p% frequentist intervals we can in general only assign a p% belief before seeing either the data or the interval.
  We derive concrete implementations for two cases: estimation of the fraction of a distribution that falls below a certain value (i.e., the CDF), and of the mean of a distribution with bounded support. Even though the problems are fully non-parametric, these methods require only one-dimensional priors. They share many of the practical advantages of Bayesian methods while avoiding the complexity of assigning high-dimensional priors altogether. Asymptotically they give intervals equivalent to the fully Bayesian approach and somewhat wider intervals, respectively. We discuss promising directions where the proposed type of interval may provide significant advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17621v2</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tim Ritmeester</dc:creator>
    </item>
  </channel>
</rss>
