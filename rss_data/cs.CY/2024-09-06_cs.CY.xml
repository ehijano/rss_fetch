<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Sep 2024 04:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Apocalypse, survivalism, occultism and esotericism communities on Brazilian Telegram: when faith is used to sell quantum courses and open doors to harmful conspiracy theories</title>
      <link>https://arxiv.org/abs/2409.03130</link>
      <description>arXiv:2409.03130v1 Announce Type: new 
Abstract: Brazilian communities on Telegram have increasingly turned to apocalyptic and survivalist theories, especially in times of crisis such as the COVID-19 pandemic, where narratives of occultism and esotericism find fertile ground. Therefore, this study aims to address the research question: how are Brazilian conspiracy theory communities on apocalypse, survivalism, occultism and esotericism topics characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies whose main objective is to understand and characterize Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on arXiv at Cornell University, applying a mirrored method across the seven studies, changing only the thematic object of analysis and providing investigation replicability, including with proprietary and authored codes, adding to the culture of free and open-source software. Regarding the main findings of this study, the following were observed: Occult and esoteric communities function as gateways to apocalypse theories; Conspiracies about the New World Order are amplified by apocalyptic discussions; Survivalist narratives grew significantly during the Pandemic; Occultism and esotericism are sources of invitations to off-label drug communities, reinforcing scientific disinformation; Discussions about the apocalypse serve as a start for other conspiracy theories, expanding their reach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03130v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>Content Moderation by LLM: From Accuracy to Legitimacy</title>
      <link>https://arxiv.org/abs/2409.03219</link>
      <description>arXiv:2409.03219v1 Announce Type: new 
Abstract: One trending application of LLM (large language model) is to use it for content moderation in online platforms. Most current studies on this application have focused on the metric of accuracy - the extent to which LLM makes correct decisions about content. This article argues that accuracy is insufficient and misleading, because it fails to grasp the distinction between easy cases and hard cases as well as the inevitable trade-offs in achieving higher accuracy. Closer examination reveals that content moderation is a constitutive part of platform governance, the key of which is to gain and enhance legitimacy. Instead of making moderation decisions correct, the chief goal of LLM is to make them legitimate. In this regard, this article proposes a paradigm shift from the single benchmark of accuracy towards a legitimacy-based framework of evaluating the performance of LLM moderators. The framework suggests that for easy cases, the key is to ensure accuracy, speed and transparency, while for hard cases, what matters is reasoned justification and user participation. Examined under this framework, LLM's real potential in moderation is not accuracy improvement. Rather, LLM can better contribute in four other aspects: to conduct screening of hard cases from easy cases, to provide quality explanations for moderation decisions, to assist human reviewers in getting more contextual information, and to facilitate user participation in a more interactive way. Using normative theories from law and social sciences to critically assess the new technological application, this article seeks to redefine LLM's role in content moderation and redirect relevant research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03219v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tao Huang</dc:creator>
    </item>
    <item>
      <title>AI data transparency: an exploration through the lens of AI incidents</title>
      <link>https://arxiv.org/abs/2409.03307</link>
      <description>arXiv:2409.03307v1 Announce Type: new 
Abstract: Knowing more about the data used to build AI systems is critical for allowing different stakeholders to play their part in ensuring responsible and appropriate deployment and use. Meanwhile, a 2023 report shows that data transparency lags significantly behind other areas of AI transparency in popular foundation models. In this research, we sought to build on these findings, exploring the status of public documentation about data practices within AI systems generating public concern.
  Our findings demonstrate that low data transparency persists across a wide range of systems, and further that issues of transparency and explainability at model- and system- level create barriers for investigating data transparency information to address public concerns about AI systems. We highlight a need to develop systematic ways of monitoring AI data transparency that account for the diversity of AI system types, and for such efforts to build on further understanding of the needs of those both supplying and using data transparency information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03307v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophia Worth, Ben Snaith, Arunav Das, Gefion Thuermer, Elena Simperl</dc:creator>
    </item>
    <item>
      <title>Innovation in Education: Developing and Assessing Gamification in the University of the Philippines Open University Massive Open Online Courses</title>
      <link>https://arxiv.org/abs/2409.03309</link>
      <description>arXiv:2409.03309v1 Announce Type: new 
Abstract: The University of the Philippines Open University has been at the forefront of providing Massive Open Online Courses to address knowledge and skill gaps, aiming to make education accessible and contributing to societal goals. Recognising challenges in student engagement and completion rates within Massive Open Online Courses, the authors conducted a study by incorporating gamification into one of the University of the Philippines Open University's Massive Open Online Courses to assess its impact on these aspects. Gamification involves integrating game elements to motivate and engage users. This study explored the incorporation of Moodle elements such as badges, leaderboards, and progress bars. Using Moodle analytics, the study also tracked student engagement, views, and posts throughout the course, offering valuable insights into the influence of gamification on user behaviour. Furthermore, the study delved into participant feedback gathered through post-evaluation surveys, providing a comprehensive understanding of their experiences with the gamified course design. With a 28.86% completion rate and positive participant reception, the study concluded that gamification can enhance learner motivation, participation, and overall satisfaction. This research contributes to the ongoing discourse on innovative educational methods, positioning gamification as a promising avenue for creating interactive and impactful online learning experiences in the Philippines and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03309v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.13691445</arxiv:DOI>
      <arxiv:journal_reference>AJODL 16(1)(2024) 27-42</arxiv:journal_reference>
      <dc:creator>Cecille Moldez, Mari Anjeli Crisanto, Ma Gian Rose Cerde\~na, Diego S. Maranan, Roberto Figueroa</dc:creator>
    </item>
    <item>
      <title>Journalists are most likely to receive abuse: Analysing online abuse of UK public figures across sport, politics, and journalism on Twitter</title>
      <link>https://arxiv.org/abs/2409.03376</link>
      <description>arXiv:2409.03376v1 Announce Type: new 
Abstract: Engaging with online social media platforms is an important part of life as a public figure in modern society, enabling connection with broad audiences and providing a platform for spreading ideas. However, public figures are often disproportionate recipients of hate and abuse on these platforms, degrading public discourse. While significant research on abuse received by groups such as politicians and journalists exists, little has been done to understand the differences in the dynamics of abuse across different groups of public figures, systematically and at scale. To address this, we present analysis of a novel dataset of 45.5M tweets targeted at 4,602 UK public figures across 3 domains (members of parliament, footballers, journalists), labelled using fine-tuned transformer-based language models. We find that MPs receive more abuse in absolute terms, but that journalists are most likely to receive abuse after controlling for other factors. We show that abuse is unevenly distributed in all groups, with a small number of individuals receiving the majority of abuse, and that for some groups, abuse is more temporally uneven, being driven by specific events, particularly for footballers. We also find that a more prominent online presence and being male are indicative of higher levels of abuse across all 3 domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03376v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liam Burke-Moore, Angus R. Williams, Jonathan Bright</dc:creator>
    </item>
    <item>
      <title>Automated Journalism</title>
      <link>https://arxiv.org/abs/2409.03462</link>
      <description>arXiv:2409.03462v1 Announce Type: new 
Abstract: Developed as a response to the increasing popularity of data-driven journalism, automated journalism refers to the process of automating the collection, production, and distribution of news content and other data with the assistance of computer programs. Although the algorithmic technologies associated with automated journalism remain in the initial stage of development, early adopters have already praised the usefulness of automated journalism for generating routine news based on clean, structured data. Most noticeably, the Associated Press and The New York Times have been automating news content to cover financial and sports issues for over a decade. Nevertheless, research on automated journalism is also alerting to the dangers of using algorithms for news creation and distribution, including the possible bias behind AI systems or the human bias of those who develop computer programs. The popularization of automated news content also has important implications for the infrastructure of the newsroom, the role performance of journalists and other non-journalistic professionals, and the distribution of news content to a datafied audience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03462v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wang Ngai Yeung, Tom\'as Dodds</dc:creator>
    </item>
    <item>
      <title>Disclosure of AI-Generated News Increases Engagement but Does Not Reduce Aversion, Despite Positive Quality Ratings</title>
      <link>https://arxiv.org/abs/2409.03500</link>
      <description>arXiv:2409.03500v1 Announce Type: new 
Abstract: The advancement of artificial intelligence (AI) has led to its application in many areas, including journalism. One key issue is the public's perception of AI-generated content. This preregistered study investigates (i) the perceived quality of AI-assisted and AI-generated versus human-generated news articles, (ii) whether disclosure of AI's involvement in generating these news articles influences engagement with them, and (iii) whether such awareness affects the willingness to read AI-generated articles in the future. We employed a between-subjects survey experiment with 599 participants from the German-speaking part of Switzerland, who evaluated the credibility, readability, and expertise of news articles. These articles were either written by journalists (control group), rewritten by AI (AI-assisted group), or entirely generated by AI (AI-generated group). Our results indicate that all news articles, regardless of whether they were written by journalists or AI, were perceived to be of equal quality. When participants in the treatment groups were subsequently made aware of AI's involvement in generating the articles, they expressed a higher willingness to engage with (i.e., continue reading) the articles than participants in the control group. However, they were not more willing to read AI-generated news in the future. These results suggest that aversion to AI usage in news media is not primarily rooted in a perceived lack of quality, and that by disclosing using AI, journalists could attract more immediate engagement with their content, at least in the short term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03500v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabrizio Gilardi, Sabrina Di Lorenzo, Juri Ezzaini, Beryl Santa, Benjamin Streiff, Eric Zurfluh, Emma Hoes</dc:creator>
    </item>
    <item>
      <title>From MOOC to MAIC: Reshaping Online Teaching and Learning through LLM-driven Agents</title>
      <link>https://arxiv.org/abs/2409.03512</link>
      <description>arXiv:2409.03512v1 Announce Type: new 
Abstract: Since the first instances of online education, where courses were uploaded to accessible and shared online platforms, this form of scaling the dissemination of human knowledge to reach a broader audience has sparked extensive discussion and widespread adoption. Recognizing that personalized learning still holds significant potential for improvement, new AI technologies have been continuously integrated into this learning format, resulting in a variety of educational AI applications such as educational recommendation and intelligent tutoring. The emergence of intelligence in large language models (LLMs) has allowed for these educational enhancements to be built upon a unified foundational model, enabling deeper integration. In this context, we propose MAIC (Massive AI-empowered Course), a new form of online education that leverages LLM-driven multi-agent systems to construct an AI-augmented classroom, balancing scalability with adaptivity. Beyond exploring the conceptual framework and technical innovations, we conduct preliminary experiments at Tsinghua University, one of China's leading universities. Drawing from over 100,000 learning records of more than 500 students, we obtain a series of valuable observations and initial analyses. This project will continue to evolve, ultimately aiming to establish a comprehensive open platform that supports and unifies research, technology, and applications in exploring the possibilities of online education in the era of large model AI. We envision this platform as a collaborative hub, bringing together educators, researchers, and innovators to collectively explore the future of AI-driven online education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03512v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jifan Yu, Zheyuan Zhang, Daniel Zhang-li, Shangqing Tu, Zhanxin Hao, Rui Miao Li, Haoxuan Li, Yuanchun Wang, Hanming Li, Linlu Gong, Jie Cao, Jiayin Lin, Jinchang Zhou, Fei Qin, Haohua Wang, Jianxiao Jiang, Lijun Deng, Yisi Zhan, Chaojun Xiao, Xusheng Dai, Xuan Yan, Nianyi Lin, Nan Zhang, Ruixin Ni, Yang Dang, Lei Hou, Yu Zhang, Xu Han, Manli Li, Juanzi Li, Zhiyuan Liu, Huiqin Liu, Maosong Sun</dc:creator>
    </item>
    <item>
      <title>A Large-Scale Survey of Password Entry Practices on Non-Desktop Devices</title>
      <link>https://arxiv.org/abs/2409.03044</link>
      <description>arXiv:2409.03044v1 Announce Type: cross 
Abstract: Password managers encourage users to generate passwords to improve their security. However, research has shown that users avoid generating passwords, often giving the rationale that it is difficult to enter generated passwords on devices without a password manager. In this paper, we conduct a survey ($n=999$) of individuals from the US, UK, and Europe, exploring the range of devices on which they enter passwords and the challenges associated with password entry on those devices. We find that password entry on devices without password managers is a common occurrence and comes with significant usability challenges. These usability challenges lead users to weaken their passwords to increase the ease of entry. We conclude this paper with a discussion of how future research could address these challenges and encourage users to adopt generated passwords.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03044v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>John Sadik, Scott Ruoti</dc:creator>
    </item>
    <item>
      <title>What is Normal? A Big Data Observational Science Model of Anonymized Internet Traffic</title>
      <link>https://arxiv.org/abs/2409.03111</link>
      <description>arXiv:2409.03111v1 Announce Type: cross 
Abstract: Understanding what is normal is a key aspect of protecting a domain. Other domains invest heavily in observational science to develop models of normal behavior to better detect anomalies. Recent advances in high performance graph libraries, such as the GraphBLAS, coupled with supercomputers enables processing of the trillions of observations required. We leverage this approach to synthesize low-parameter observational models of anonymized Internet traffic with a high regard for privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03111v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Kepner, Hayden Jananthan, Michael Jones, William Arcand, David Bestor, William Bergeron, Daniel Burrill, Aydin Buluc, Chansup Byun, Timothy Davis, Vijay Gadepally, Daniel Grant, Michael Houle, Matthew Hubbell, Piotr Luszczek, Lauren Milechin, Chasen Milner, Guillermo Morales, Andrew Morris, Julie Mullen, Ritesh Patel, Alex Pentland, Sandeep Pisharody, Andrew Prout, Albert Reuther, Antonio Rosa, Gabriel Wachman, Charles Yee, Peter Michaleas</dc:creator>
    </item>
    <item>
      <title>Fast algorithms to improve fair information access in networks</title>
      <link>https://arxiv.org/abs/2409.03127</link>
      <description>arXiv:2409.03127v1 Announce Type: cross 
Abstract: When information spreads across a network via pairwise sharing, large disparities in information access can arise from the network's structural heterogeneity. Algorithms to improve the fairness of information access seek to maximize the minimum access of a node to information by sequentially selecting new nodes to seed with the spreading information. However, existing algorithms are computationally expensive. Here, we develop and evaluate a set of 10 new scalable algorithms to improve information access in social networks; in order to compare them to the existing state-of-the-art, we introduce both a new performance metric and a new benchmark corpus of networks. Additionally, we investigate the degree to which algorithm performance on minimizing information access gaps can be predicted ahead of time from features of a network's structure. We find that while no algorithm is strictly superior to all others across networks, our new scalable algorithms are competitive with the state-of-the-art and orders of magnitude faster. We introduce a meta-learner approach that learns which of the fast algorithms is best for a specific network and is on average only 20% less effective than the state-of-the-art performance on held-out data, while about 75-130 times faster. Furthermore, on about 20% of networks the meta-learner's performance exceeds the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03127v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Robert Windham, Caroline J. Wendt, Alex Crane, Sorelle A. Friedler, Blair D. Sullivan, Aaron Clauset</dc:creator>
    </item>
    <item>
      <title>Interactive Surgical Liver Phantom for Cholecystectomy Training</title>
      <link>https://arxiv.org/abs/2409.03535</link>
      <description>arXiv:2409.03535v1 Announce Type: cross 
Abstract: Training and prototype development in robot-assisted surgery requires appropriate and safe environments for the execution of surgical procedures. Current dry lab laparoscopy phantoms often lack the ability to mimic complex, interactive surgical tasks. This work presents an interactive surgical phantom for the cholecystectomy. The phantom enables the removal of the gallbladder during cholecystectomy by allowing manipulations and cutting interactions with the synthetic tissue. The force-displacement behavior of the gallbladder is modelled based on retraction demonstrations. The force model is compared to the force model of ex-vivo porcine gallbladders and evaluated on its ability to estimate retraction forces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03535v1</guid>
      <category>cs.RO</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Schuessler, Rayan Younis, Jamie Paik, Martin Wagner, Franziska Mathis-Ullrich, Christian Kunz</dc:creator>
    </item>
    <item>
      <title>Safety vs. Performance: How Multi-Objective Learning Reduces Barriers to Market Entry</title>
      <link>https://arxiv.org/abs/2409.03734</link>
      <description>arXiv:2409.03734v1 Announce Type: cross 
Abstract: Emerging marketplaces for large language models and other large-scale machine learning (ML) models appear to exhibit market concentration, which has raised concerns about whether there are insurmountable barriers to entry in such markets. In this work, we study this issue from both an economic and an algorithmic point of view, focusing on a phenomenon that reduces barriers to entry. Specifically, an incumbent company risks reputational damage unless its model is sufficiently aligned with safety objectives, whereas a new company can more easily avoid reputational damage. To study this issue formally, we define a multi-objective high-dimensional regression framework that captures reputational damage, and we characterize the number of data points that a new company needs to enter the market. Our results demonstrate how multi-objective considerations can fundamentally reduce barriers to entry -- the required number of data points can be significantly smaller than the incumbent company's dataset size. En route to proving these results, we develop scaling laws for high-dimensional linear regression in multi-objective environments, showing that the scaling rate becomes slower when the dataset size is large, which could be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03734v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meena Jagadeesan, Michael I. Jordan, Jacob Steinhardt</dc:creator>
    </item>
    <item>
      <title>LLM-CI: Assessing Contextual Integrity Norms in Language Models</title>
      <link>https://arxiv.org/abs/2409.03735</link>
      <description>arXiv:2409.03735v1 Announce Type: cross 
Abstract: Large language models (LLMs), while memorizing parts of their training data scraped from the Internet, may also inadvertently encode societal preferences and norms. As these models are integrated into sociotechnical systems, it is crucial that the norms they encode align with societal expectations. These norms could vary across models, hyperparameters, optimization techniques, and datasets. This is especially challenging due to prompt sensitivity$-$small variations in prompts yield different responses, rendering existing assessment methodologies unreliable. There is a need for a comprehensive framework covering various models, optimization, and datasets, along with a reliable methodology to assess encoded norms.
  We present LLM-CI, the first open-sourced framework to assess privacy norms encoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette methodology to assess the encoded norms across different contexts and LLMs. We propose the multi-prompt assessment methodology to address prompt sensitivity by assessing the norms from only the prompts that yield consistent responses across multiple variants. Using LLM-CI and our proposed methodology, we comprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior work, examining the impact of model properties (e.g., hyperparameters, capacity) and optimization strategies (e.g., alignment, quantization).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03735v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Shvartzshnaider, Vasisht Duddu, John Lacalamita</dc:creator>
    </item>
    <item>
      <title>Trust, Because You Can't Verify:Privacy and Security Hurdles in Education Technology Acquisition Practices</title>
      <link>https://arxiv.org/abs/2405.11712</link>
      <description>arXiv:2405.11712v2 Announce Type: replace 
Abstract: The education technology (EdTech) landscape is expanding rapidly in higher education institutes (HEIs). This growth brings enormous complexity. Protecting the extensive data collected by these tools is crucial for HEIs as data breaches and misuses can have dire security and privacy consequences on the data subjects, particularly students, who are often compelled to use these tools. This urges an in-depth understanding of HEI and EdTech vendor dynamics, which is largely understudied.
  To address this gap, we conducted a semi-structured interview study with 13 participants who are in EdTech leadership roles at seven HEIs. Our study uncovers the EdTech acquisition process in the HEI context, the consideration of security and privacy issues throughout that process, the pain points of HEI personnel in establishing adequate protection mechanisms in service contracts, and their struggle in holding vendors accountable due to a lack of visibility into their system and power-asymmetry, among other reasons. We discuss certain observations about the status quo and conclude with recommendations for HEIs, researchers, and regulatory bodies to improve the situation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11712v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Easton Kelso, Ananta Soneji, Sazzadur Rahaman, Yan Soshitaishvili, Rakibul Hasan</dc:creator>
    </item>
    <item>
      <title>Ontology-driven Reinforcement Learning for Personalized Student Support</title>
      <link>https://arxiv.org/abs/2407.10332</link>
      <description>arXiv:2407.10332v2 Announce Type: replace 
Abstract: In the search for more effective education, there is a widespread effort to develop better approaches to personalize student education. Unassisted, educators often do not have time or resources to personally support every student in a given classroom. Motivated by this issue, and by recent advancements in artificial intelligence, this paper presents a general-purpose framework for personalized student support, applicable to any virtual educational system such as a serious game or an intelligent tutoring system. To fit any educational situation, we apply ontologies for their semantic organization, combining them with data collection considerations and multi-agent reinforcement learning. The result is a modular system that can be adapted to any virtual educational software to provide useful personalized assistance to students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10332v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryan Hare, Ying Tang</dc:creator>
    </item>
    <item>
      <title>Online Advertising is a Regrettable Necessity: On the Dangers of Pay-Walling the Web</title>
      <link>https://arxiv.org/abs/2409.00026</link>
      <description>arXiv:2409.00026v2 Announce Type: replace 
Abstract: The exponential growth of the web and its benefits can be attributed largely to its open model where anyone with internet connection can access information on the web for free. This has created unprecedented opportunities for various members of society including the most vulnerable, as recognized by organizations such as the UN. This again can be attributed to online advertising, which has been the main financier to the open web. However, recent trends of paywalling information and services on the web are creating imminent dangers to such open model of the web, inhibiting access for the economically vulnerable, and eventually creating digital segregation. In this paper, we argue that this emerging model lacks sustainability, exacerbates digital divide, and might lead to collapse of online advertising. We revisit the ad-supported open web business model and demonstrate how global users actually pay for the ads they see. Using data on GNI (gross national income) per capita and average paywall access costs, we established a simple income-paywall expenditure gap baseline. With this baseline we show that 135 countries with a total population estimate of 6.56 billion people cannot afford a scenario of a fully paywalled web. We further discuss how a mixed model of the so-called "premium services" creates digital segregation and poses danger to online advertising ecosystem. Finally, we call for further research and policy initiatives to keep the web open and more inclusive with a sustainable business model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00026v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonas Kassa</dc:creator>
    </item>
    <item>
      <title>Counterpart Fairness -- Addressing Systematic between-group Differences in Fairness Evaluation</title>
      <link>https://arxiv.org/abs/2305.18160</link>
      <description>arXiv:2305.18160v3 Announce Type: replace-cross 
Abstract: When using machine learning (ML) to aid decision-making, it is critical to ensure that an algorithmic decision is fair and does not discriminate against specific individuals/groups, particularly those from underprivileged populations. Existing group fairness methods aim to ensure equal outcomes (such as loan approval rates) across groups delineated by protected variables like race or gender. However, these methods overlook the intricate, inherent differences among these groups that could influence outcomes. The confounding factors, which are non-protected variables but manifest systematic differences, can significantly affect fairness evaluation. Therefore, we recommend a more refined and comprehensive approach that accounts for both the systematic differences within groups and the multifaceted, intertwined confounding effects. We proposed a fairness metric based on counterparts (i.e., individuals who are similar with respect to the task of interest) from different groups, whose group identities cannot be distinguished algorithmically by exploring confounding factors. We developed a propensity-score-based method for identifying counterparts, avoiding the issue of comparing "oranges" with "apples". In addition, we introduced a counterpart-based statistical fairness index, called Counterpart-Fairness (CFair), to assess the fairness of ML models. Various empirical studies were conducted to validate the effectiveness of CFair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18160v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifei Wang, Zhengyang Zhou, Liqin Wang, John Laurentiev, Peter Hou, Li Zhou, Pengyu Hong</dc:creator>
    </item>
    <item>
      <title>Towards Neural Network based Cognitive Models of Dynamic Decision-Making by Humans</title>
      <link>https://arxiv.org/abs/2407.17622</link>
      <description>arXiv:2407.17622v2 Announce Type: replace-cross 
Abstract: Modeling human cognitive processes in dynamic decision-making tasks has been an endeavor in AI for a long time because such models can help make AI systems more intuitive, personalized, mitigate any human biases, and enhance training in simulation. Some initial work has attempted to utilize neural networks (and large language models) but often assumes one common model for all humans and aims to emulate human behavior in aggregate. However, the behavior of each human is distinct, heterogeneous, and relies on specific past experiences in certain tasks. For instance, consider two individuals responding to a phishing email: one who has previously encountered and identified similar threats may recognize it quickly, while another without such experience might fall for the scam. In this work, we build on Instance Based Learning (IBL) that posits that human decisions are based on similar situations encountered in the past. However, IBL relies on simple fixed form functions to capture the mapping from past situations to current decisions. To that end, we propose two new attention-based neural network models to have open form non-linear functions to model distinct and heterogeneous human decision-making in dynamic settings. We experiment with two distinct datasets gathered from human subject experiment data, one focusing on detection of phishing email by humans and another where humans act as attackers in a cybersecurity setting and decide on an attack option. We conducted extensive experiments with our two neural network models, IBL, and GPT3.5, and demonstrate that the neural network models outperform IBL significantly in representing human decision-making, while providing similar interpretability of human decisions as IBL. Overall, our work yields promising results for further use of neural networks in cognitive modeling of human decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17622v2</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changyu Chen, Shashank Reddy Chirra, Maria Jos\'e Ferreira, Cleotilde Gonzalez, Arunesh Sinha, Pradeep Varakantham</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of Wealth Index Predictions in Africa between three Multi-Source Inference Models</title>
      <link>https://arxiv.org/abs/2408.01631</link>
      <description>arXiv:2408.01631v2 Announce Type: replace-cross 
Abstract: Poverty map inference is a critical area of research, with growing interest in both traditional and modern techniques, ranging from regression models to convolutional neural networks applied to tabular data, images, and networks. Despite extensive focus on the validation of training phases, the scrutiny of final predictions remains limited. Here, we compare the Relative Wealth Index (RWI) inferred by Chi et al. (2022) with the International Wealth Index (IWI) inferred by Lee and Braithwaite (2022) and Esp\'in-Noboa et al. (2023) across six Sub-Saharan African countries. Our analysis focuses on identifying trends and discrepancies in wealth predictions over time. Our results show that the predictions by Chi et al. and Esp\'in-Noboa et al. align with general GDP trends, with differences expected due to the distinct time-frames of the training sets. However, predictions by Lee and Braithwaite diverge significantly, indicating potential issues with the validity of the model. These discrepancies highlight the need for policymakers and stakeholders in Africa to rigorously audit models that predict wealth, especially those used for decision-making on the ground. These and other techniques require continuous verification and refinement to enhance their reliability and ensure that poverty alleviation strategies are well-founded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01631v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Fri, 06 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>M\'arton Karsai, J\'anos Kert\'esz, Lisette Esp\'in-Noboa</dc:creator>
    </item>
  </channel>
</rss>
