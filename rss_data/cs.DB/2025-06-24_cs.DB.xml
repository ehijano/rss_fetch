<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>DCMF: A Dynamic Context Monitoring and Caching Framework for Context Management Platforms</title>
      <link>https://arxiv.org/abs/2506.17226</link>
      <description>arXiv:2506.17226v1 Announce Type: new 
Abstract: The rise of context-aware IoT applications has increased the demand for timely and accurate context information. Context is derived by aggregating and inferring from dynamic IoT data, making it highly volatile and posing challenges in maintaining freshness and real-time accessibility. Caching is a potential solution, but traditional policies struggle with the transient nature of context in IoT (e.g., ensuring real-time access for frequent queries or handling fast-changing data). To address this, we propose the Dynamic Context Monitoring Framework (DCMF) to enhance context caching in Context Management Platforms (CMPs) by dynamically evaluating and managing context. DCMF comprises two core components: the Context Evaluation Engine (CEE) and the Context Management Module (CMM). The CEE calculates the Probability of Access (PoA) using parameters such as Quality of Service (QoS), Quality of Context (QoC), Cost of Context (CoC), timeliness, and Service Level Agreements (SLAs), assigning weights to assess access likelihood. Based on this, the CMM applies a hybrid Dempster-Shafer approach to manage Context Freshness (CF), updating belief levels and confidence scores to determine whether to cache, evict, or refresh context items. We implemented DCMF in a Context-as-a-Service (CoaaS) platform and evaluated it using real-world smart city data, particularly traffic and roadwork scenarios. Results show DCMF achieves a 12.5% higher cache hit rate and reduces cache expiry by up to 60% compared to the m-CAC technique, ensuring timely delivery of relevant context and reduced latency. These results demonstrate DCMF's scalability and suitability for dynamic context-aware IoT environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17226v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Manchanda, Prem Prakash Jayaraman, Abhik Banerjee, Kaneez Fizza, Arkady Zaslavsky</dc:creator>
    </item>
    <item>
      <title>Transient Concepts in Streaming Graphs</title>
      <link>https://arxiv.org/abs/2506.17451</link>
      <description>arXiv:2506.17451v1 Announce Type: new 
Abstract: Concept Drift (CD) occurs when a change in a hidden context can induce changes in a target concept. CD is a natural phenomenon in non-stationary settings such as data streams. Understanding, detection, and adaptation to CD in streaming data is (i) vital for effective and efficient analytics as reliable output depends on adaptation to fresh input, (ii) challenging as it requires efficient operations as well as effective performance evaluations, and (iii) impactful as it applies to a variety of use cases and is a crucial initial step for data management systems. Current works are mostly focused on passive CD detection as part of supervised adaptation, on independently generated data instances or graph snapshots, on target concepts as a function of data labels, on static data management, and on specific temporal order of data record. These methods do not always work. We revisit CD for the streaming graphs setting and introduce two first-of-its-kind frameworks SGDD and SGDP for streaming graph CD detection and prediction. Both frameworks discern the change of generative source. SGDD detects the CDs due to the changes of generative parameters with significant delays such that it is difficult to evaluate the performance, while SGDP predicts these CDs between 7374 to 0.19 milliseconds ahead of their occurrence, without accessing the payloads of data records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17451v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aida Sheshbolouki, M. Tamer Ozsu</dc:creator>
    </item>
    <item>
      <title>Lower Bounds for Conjunctive Query Evaluation</title>
      <link>https://arxiv.org/abs/2506.17702</link>
      <description>arXiv:2506.17702v1 Announce Type: new 
Abstract: In this tutorial, we will survey known results on the complexity of conjunctive query evaluation in different settings, ranging from Boolean queries over counting to more complex models like enumeration and direct access. A particular focus will be on showing how different relatively recent hypotheses from complexity theory connect to query answering and allow showing that known algorithms in several cases can likely not be improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17702v1</guid>
      <category>cs.DB</category>
      <category>cs.CC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stefan Mengel</dc:creator>
    </item>
    <item>
      <title>Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks</title>
      <link>https://arxiv.org/abs/2506.18013</link>
      <description>arXiv:2506.18013v1 Announce Type: new 
Abstract: Computing the shortest-path distance between any two given vertices in road networks is an important problem. A tremendous amount of research has been conducted to address this problem, most of which are limited to static road networks. Since road networks undergo various real-time traffic conditions, there is a pressing need to address this problem for dynamic road networks. Existing state-of-the-art methods incrementally maintain an indexing structure to reflect dynamic changes on road networks. However, these methods suffer from either slow query response time or poor maintenance performance, particularly when road networks are large. In this work, we propose an efficient solution \emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road networks from a novel perspective, which incorporates two hierarchies with different but complementary data structures to support efficient query and update processing. Specifically, our proposed solution is comprised of three main components: \emph{query hierarchy}, \emph{update hierarchy}, and \emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient query answering by exploring only a small subset of vertices in the labels of two query vertices and \emph{update hierarchy} supports efficient maintenance of distance labelling under edge weight increase or decrease. We further develop dynamic algorithms to reflect dynamic changes by efficiently maintaining the update hierarchy and hierarchical labelling. We also propose a parallel variant of our dynamic algorithms by exploiting labelling structure. We evaluate our methods on 10 large road networks and it shows that our methods significantly outperform the state-of-the-art methods, i.e., achieving considerably faster construction and update time, while being consistently 2-4 times faster in terms of query processing and consuming only 10\%-20\% labelling space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18013v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Farhan, Henning Koehler, Qing Wang</dc:creator>
    </item>
    <item>
      <title>Floating-Point Data Transformation for Lossless Compression</title>
      <link>https://arxiv.org/abs/2506.18062</link>
      <description>arXiv:2506.18062v1 Announce Type: new 
Abstract: Floating-point data is widely used across various domains. Depending on the required precision, each floating-point value can occupy several bytes. Lossless storage of this information is crucial due to its critical accuracy, as seen in applications such as medical imaging and language model weights. In these cases, data size is often significant, making lossless compression essential. Previous approaches either treat this data as raw byte streams for compression or fail to leverage all patterns within the dataset. However, because multiple bytes represent a single value and due to inherent patterns in floating-point representations, some of these bytes are correlated. To leverage this property, we propose a novel data transformation method called Typed Data Transformation (\DTT{}) that groups related bytes together to improve compression. We implemented and tested our approach on various datasets across both CPU and GPU. \DTT{} achieves a geometric mean compression ratio improvement of 1.16$\times$ over state-of-the-art compression tools such as zstd, while also improving both compression and decompression throughput by 1.18--3.79$\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18062v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samirasadat Jamalidinan, Kazem Cheshmi</dc:creator>
    </item>
    <item>
      <title>Learning Lineage Constraints for Data Science Operations</title>
      <link>https://arxiv.org/abs/2506.18252</link>
      <description>arXiv:2506.18252v1 Announce Type: new 
Abstract: Data science workflows often integrate functionalities from a diverse set of libraries and frameworks. Tasks such as debugging require data lineage that crosses library boundaries. The problem is that the way that "lineage" is represented is often intimately tied to particular data models and data manipulation paradigms. Inspired by the use of intermediate representations (IRs) in cross-library performance optimizations, this vision paper proposes a similar architecture for lineage - how do we specify logical lineage across libraries in a common parameterized way? In practice, cross-library workflows will contain both known operations and unknown operations, so a key design of XProv to link both materialized lineage graphs of data transformations and the aforementioned abstracted logical patterns. We further discuss early ideas on how to infer logical patterns when only the materialized graphs are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18252v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinjin Zhao</dc:creator>
    </item>
    <item>
      <title>Fast Capture of Cell-Level Provenance in Numpy</title>
      <link>https://arxiv.org/abs/2506.18255</link>
      <description>arXiv:2506.18255v1 Announce Type: new 
Abstract: Effective provenance tracking enhances reproducibility, governance, and data quality in array workflows. However, significant challenges arise in capturing this provenance, including: (1) rapidly evolving APIs, (2) diverse operation types, and (3) large-scale datasets. To address these challenges, this paper presents a prototype annotation system designed for arrays, which captures cell-level provenance specifically within the numpy library. With this prototype, we explore straightforward memory optimizations that substantially reduce annotation latency. We envision this provenance capture approach for arrays as part of a broader governance system for tracking for structured data workflows and diverse data science applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18255v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinjin Zhao, Sanjay Krishnan</dc:creator>
    </item>
    <item>
      <title>TableVault: Managing Dynamic Data Collections for LLM-Augmented Workflows</title>
      <link>https://arxiv.org/abs/2506.18257</link>
      <description>arXiv:2506.18257v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have emerged as powerful tools for automating and executing complex data tasks. However, their integration into more complex data workflows introduces significant management challenges. In response, we present TableVault - a data management system designed to handle dynamic data collections in LLM-augmented environments. TableVault meets the demands of these workflows by supporting concurrent execution, ensuring reproducibility, maintaining robust data versioning, and enabling composable workflow design. By merging established database methodologies with emerging LLM-driven requirements, TableVault offers a transparent platform that efficiently manages both structured data and associated data artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18257v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinjin Zhao, Sanjay Krishnan</dc:creator>
    </item>
    <item>
      <title>Patient Journey Ontology: Representing Medical Encounters for Enhanced Patient-Centric Applications</title>
      <link>https://arxiv.org/abs/2506.18772</link>
      <description>arXiv:2506.18772v1 Announce Type: new 
Abstract: The healthcare industry is moving towards a patient-centric paradigm that requires advanced methods for managing and representing patient data. This paper presents a Patient Journey Ontology (PJO), a framework that aims to capture the entirety of a patient's healthcare encounters. Utilizing ontologies, the PJO integrates different patient data sources like medical histories, diagnoses, treatment pathways, and outcomes; it enables semantic interoperability and enhances clinical reasoning. By capturing temporal, sequential, and causal relationships between medical encounters, the PJO supports predictive analytics, enabling earlier interventions and optimized treatment plans. The ontology's structure, including its main classes, subclasses, properties, and relationships, as detailed in the paper, demonstrates its ability to provide a holistic view of patient care. Quantitative and qualitative evaluations by Subject Matter Experts (SMEs) demonstrate strong capabilities in patient history retrieval, symptom tracking, and provider interaction representation, while identifying opportunities for enhanced diagnosis-symptom linking. These evaluations reveal the PJO's reliability and practical applicability, demonstrating its potential to enhance patient outcomes and healthcare efficiency. This work contributes to the ongoing efforts of knowledge representation in healthcare, offering a reliable tool for personalized medicine, patient journey analysis and advancing the capabilities of Generative AI in healthcare applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18772v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan S. Al Khatib, Subash Neupane, Sudip Mittal, Shahram Rahimi, Nina Marhamati, Sean Bozorgzad</dc:creator>
    </item>
    <item>
      <title>LIGHTHOUSE: Fast and precise distance to shoreline calculations from anywhere on earth</title>
      <link>https://arxiv.org/abs/2506.18842</link>
      <description>arXiv:2506.18842v1 Announce Type: new 
Abstract: We introduce a new dataset and algorithm for fast and efficient coastal distance calculations from Anywhere on Earth (AoE). Existing global coastal datasets are only available at coarse resolution (e.g. 1-4 km) which limits their utility. Publicly available satellite imagery combined with computer vision enable much higher precision. We provide a global coastline dataset at 10 meter resolution, a 100+ fold improvement in precision over existing data. To handle the computational challenge of querying at such an increased scale, we introduce a new library: Layered Iterative Geospatial Hierarchical Terrain-Oriented Unified Search Engine (Lighthouse). Lighthouse is both exceptionally fast and resource-efficient, requiring only 1 CPU and 2 GB of RAM to achieve millisecond online inference, making it well suited for real-time applications in resource-constrained environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18842v1</guid>
      <category>cs.DB</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Beukema, Henry Herzog, Yawen Zhang, Hunter Pitelka, Favyen Bastani</dc:creator>
    </item>
    <item>
      <title>Mapping the Evolution of Research Contributions using KnoVo</title>
      <link>https://arxiv.org/abs/2506.17508</link>
      <description>arXiv:2506.17508v1 Announce Type: cross 
Abstract: This paper presents KnoVo (Knowledge Evolution), an intelligent framework designed for quantifying and analyzing the evolution of research novelty in the scientific literature. Moving beyond traditional citation analysis, which primarily measures impact, KnoVo determines a paper's novelty relative to both prior and subsequent work within its multilayered citation network. Given a target paper's abstract, KnoVo utilizes Large Language Models (LLMs) to dynamically extract dimensions of comparison (e.g., methodology, application, dataset). The target paper is then compared to related publications along these same extracted dimensions. This comparative analysis, inspired by tournament selection, yields quantitative novelty scores reflecting the relative improvement, equivalence, or inferiority of the target paper in specific aspects. By aggregating these scores and visualizing their progression, for instance, through dynamic evolution graphs and comparative radar charts, KnoVo facilitates researchers not only to assess originality and identify similar work, but also to track knowledge evolution along specific research dimensions, uncover research gaps, and explore cross-disciplinary connections. We demonstrate these capabilities through a detailed analysis of 20 diverse papers from multiple scientific fields and report on the performance of various open-source LLMs within the KnoVo framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17508v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sajratul Y. Rubaiat, Syed N. Sakib, Hasan M. Jamil</dc:creator>
    </item>
    <item>
      <title>Contextual Pattern Mining and Counting</title>
      <link>https://arxiv.org/abs/2506.17613</link>
      <description>arXiv:2506.17613v1 Announce Type: cross 
Abstract: Given a string $P$ of length $m$, a longer string $T$ of length $n&gt;m$, and two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$ occurs in $T$. We introduce two problems related to the notion of context: (1) the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an integer $\tau&gt;0$, asks for outputting the context of each substring $P$ of length $m$ of $T$, provided that the size of the context of $P$ is at least $\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for preprocessing $T$ so that the size of the context of a given query string $P$ of length $m$ can be found efficiently.
  For CPM, we propose a linear-work algorithm that either uses only internal memory, or a bounded amount of internal memory and external memory, which allows much larger datasets to be handled. For CPC, we propose an $\widetilde{\mathcal{O}}(n)$-space index that can be constructed in $\widetilde{\mathcal{O}}n)$ time and answers queries in $\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the practical performance of the CPC index by optimizations that exploit the LZ77 factorization of $T$ and an upper bound on the query length. Using billion-letter datasets from different domains, we show that the external memory version of our CPM algorithm can deal with very large datasets using a small amount of internal memory while its runtime is comparable to that of the internal memory version. Interestingly, we also show that our optimized index for CPC outperforms an approach based on the state of the art for the reporting version of CPC [Navarro, SPIRE 2020] in terms of query time, index size, construction time, and construction space, often by more than an order of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17613v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ling Li, Daniel Gibney, Sharma V. Thankachan, Solon P. Pissis, Grigorios Loukides</dc:creator>
    </item>
    <item>
      <title>SliceGX: Layer-wise GNN Explanation with Model-slicing</title>
      <link>https://arxiv.org/abs/2506.17977</link>
      <description>arXiv:2506.17977v1 Announce Type: cross 
Abstract: Ensuring the trustworthiness of graph neural networks (GNNs) as black-box models requires effective explanation methods. Existing GNN explanations typically apply input perturbations to identify subgraphs that are responsible for the occurrence of the final output of GNNs. However, such approaches lack finer-grained, layer-wise analysis of how intermediate representations contribute to the final result, capabilities that are crucial for model diagnosis and architecture optimization. This paper introduces SliceGX, a novel GNN explanation approach that generates explanations at specific GNN layers in a progressive manner. Given a GNN M, a set of selected intermediate layers, and a target layer, SliceGX automatically segments M into layer blocks ("model slice") and discovers high-quality explanatory subgraphs in each layer block that clarifies the occurrence of output of M at the targeted layer. Although finding such layer-wise explanations is computationally challenging, we develop efficient algorithms and optimization techniques that incrementally generate and maintain these subgraphs with provable approximation guarantees. Additionally, SliceGX offers a SPARQL-like query interface, providing declarative access and search capacities for the generated explanations. Through experiments on large real-world graphs and representative GNN architectures, we verify the effectiveness and efficiency of SliceGX, and illustrate its practical utility in supporting model debugging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17977v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tingting Zhu, Tingyang Chen, Yinghui Wu, Arijit Khan, Xiangyu Ke</dc:creator>
    </item>
    <item>
      <title>PuckTrick: A Library for Making Synthetic Data More Realistic</title>
      <link>https://arxiv.org/abs/2506.18499</link>
      <description>arXiv:2506.18499v1 Announce Type: cross 
Abstract: The increasing reliance on machine learning (ML) models for decision-making requires high-quality training data. However, access to real-world datasets is often restricted due to privacy concerns, proprietary restrictions, and incomplete data availability. As a result, synthetic data generation (SDG) has emerged as a viable alternative, enabling the creation of artificial datasets that preserve the statistical properties of real data while ensuring privacy compliance. Despite its advantages, synthetic data is often overly clean and lacks real-world imperfections, such as missing values, noise, outliers, and misclassified labels, which can significantly impact model generalization and robustness. To address this limitation, we introduce Pucktrick, a Python library designed to systematically contaminate synthetic datasets by introducing controlled errors. The library supports multiple error types, including missing data, noisy values, outliers, label misclassification, duplication, and class imbalance, offering a structured approach to evaluating ML model resilience under real-world data imperfections. Pucktrick provides two contamination modes: one for injecting errors into clean datasets and another for further corrupting already contaminated datasets. Through extensive experiments on real-world financial datasets, we evaluate the impact of systematic data contamination on model performance. Our findings demonstrate that ML models trained on contaminated synthetic data outperform those trained on purely synthetic, error-free data, particularly for tree-based and linear models such as SVMs and Extra Trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18499v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandra Agostini, Andrea Maurino, Blerina Spahiu</dc:creator>
    </item>
    <item>
      <title>When Large Language Models Meet Vector Databases: A Survey</title>
      <link>https://arxiv.org/abs/2402.01763</link>
      <description>arXiv:2402.01763v4 Announce Type: replace 
Abstract: This survey explores the synergistic potential of Large Language Models (LLMs) and Vector Databases (VecDBs), a burgeoning but rapidly evolving research area. With the proliferation of LLMs comes a host of challenges, including hallucinations, outdated knowledge, prohibitive commercial application costs, and memory issues. VecDBs emerge as a compelling solution to these issues by offering an efficient means to store, retrieve, and manage the high-dimensional vector representations intrinsic to LLM operations. Through this nuanced review, we delineate the foundational principles of LLMs and VecDBs and critically analyze their integration's impact on enhancing LLM functionalities. This discourse extends into a discussion on the speculative future developments in this domain, aiming to catalyze further research into optimizing the confluence of LLMs and VecDBs for advanced data handling and knowledge extraction capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01763v4</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Jing, Yongye Su, Yikun Han, Bo Yuan, Haiyun Xu, Chunjiang Liu, Kehai Chen, Min Zhang</dc:creator>
    </item>
    <item>
      <title>LaPuda: LLM-Enabled Policy-Based Query Optimizer for Multi-modal Data</title>
      <link>https://arxiv.org/abs/2403.13597</link>
      <description>arXiv:2403.13597v3 Announce Type: replace 
Abstract: Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not been well addressed today. In this paper, we investigate the query optimization ability of LLM and use LLM to design LaPuda, a novel LLM and Policy based multi-modal query optimizer. Instead of enumerating specific and detailed rules, LaPuda only needs a few abstract policies to guide LLM in the optimization, by which much time and human effort are saved. Furthermore, to prevent LLM from making mistakes or negative optimization, we borrow the idea of gradient descent and propose a guided cost descent (GCD) algorithm to perform the optimization, such that the optimization can be kept in the correct direction. In our evaluation, our methods consistently outperform the baselines in most cases. For example, the optimized plans generated by our methods result in 1~3x higher execution speed than those by the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13597v3</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.IR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-96-8180-8_14</arxiv:DOI>
      <arxiv:journal_reference>Advances in Knowledge Discovery and Data Mining. PAKDD 2025</arxiv:journal_reference>
      <dc:creator>Yifan Wang, Haodi Ma, Daisy Zhe Wang</dc:creator>
    </item>
    <item>
      <title>Shapley Revisited: Tractable Responsibility Measures for Query Answers</title>
      <link>https://arxiv.org/abs/2503.22358</link>
      <description>arXiv:2503.22358v2 Announce Type: replace 
Abstract: The Shapley value, originating from cooperative game theory, has been employed to define responsibility measures that quantify the contributions of database facts to obtaining a given query answer. For non-numeric queries, this is done by considering a cooperative game whose players are the facts and whose wealth function assigns 1 or 0 to each subset of the database, depending on whether the query answer holds in the given subset. While conceptually simple, this approach suffers from a notable drawback: the problem of computing such Shapley values is #P-hard in data complexity, even for simple conjunctive queries. This motivates us to revisit the question of what constitutes a reasonable responsibility measure and to introduce a new family of responsibility measures -- weighted sums of minimal supports (WSMS) -- which satisfy intuitive properties. Interestingly, while the definition of WSMSs is simple and bears no obvious resemblance to the Shapley value formula, we prove that every WSMS measure can be equivalently seen as the Shapley value of a suitably defined cooperative game. Moreover, WSMS measures enjoy tractable data complexity for a large class of queries, including all unions of conjunctive queries. We further explore the combined complexity of WSMS computation and establish (in)tractability results for various subclasses of conjunctive queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22358v2</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meghyn Bienvenu, Diego Figueira, Pierre Lafourcade</dc:creator>
    </item>
    <item>
      <title>EnhanceGraph: A Continuously Enhanced Graph-based Index for High-dimensional Approximate Nearest Neighbor Search</title>
      <link>https://arxiv.org/abs/2506.13144</link>
      <description>arXiv:2506.13144v2 Announce Type: replace 
Abstract: Recently, Approximate Nearest Neighbor Search in high-dimensional vector spaces has garnered considerable attention due to the rapid advancement of deep learning techniques. We observed that a substantial amount of search and construction logs are generated throughout the lifespan of a graph-based index. However, these two types of valuable logs are not fully exploited due to the static nature of existing indexes. We present the EnhanceGraph framework, which integrates two types of logs into a novel structure called a conjugate graph. The conjugate graph is then used to improve search quality. Through theoretical analyses and observations of the limitations of graph-based indexes, we propose several optimization methods. For the search logs, the conjugate graph stores the edges from local optima to global optima to enhance routing to the nearest neighbor. For the construction logs, the conjugate graph stores the pruned edges from the proximity graph to enhance retrieving of k nearest neighbors. Our experimental results on several public and real-world industrial datasets show that EnhanceGraph significantly improves search accuracy with the greatest improvement on recall from 41.74% to 93.42%, but does not sacrifices search efficiency. In addition, our EnhanceGraph algorithm has been integrated into Ant Group's open-source vector library, VSAG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13144v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyao Zhong, Jiabao Jin, Peng Cheng, Mingyu Yang, Haoyang Li, Zhitao Shen, Heng Tao Shen, Jingkuan Song</dc:creator>
    </item>
  </channel>
</rss>
