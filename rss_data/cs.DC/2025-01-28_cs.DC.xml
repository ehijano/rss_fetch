<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Jan 2025 02:33:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Orthrus: Accelerating Multi-BFT Consensus through Concurrent Partial Ordering of Transactions</title>
      <link>https://arxiv.org/abs/2501.14732</link>
      <description>arXiv:2501.14732v1 Announce Type: new 
Abstract: Multi-Byzantine Fault Tolerant (Multi-BFT) consensus allows multiple consensus instances to run in parallel, resolving the leader bottleneck problem inherent in classic BFT consensus. However, the global ordering of Multi-BFT consensus enforces a strict serialized sequence of transactions, imposing additional confirmation latency and also limiting concurrency. In this paper, we introduce Orthrus, a Multi-BFT protocol that accelerates transaction confirmation through partial ordering while reserving global ordering for transactions requiring stricter sequencing. To this end, Orthrus strategically partitions transactions to maximize concurrency and ensure consistency. Additionally, it incorporates an escrow mechanism to manage interactions between partially and globally ordered transactions. We evaluated Orthrus through extensive experiments in realistic settings, deploying 128 replicas in WAN and LAN environments. Our findings demonstrate latency reductions of up to 87% in WAN compared to existing Multi-BFT protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14732v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanzheng Lyu, Shaokang Xie, Jianyu Niu, Ivan Beschastnikh, Yinqian Zhang, Mohammad Sadoghi, Chen Feng</dc:creator>
    </item>
    <item>
      <title>LLM as HPC Expert: Extending RAG Architecture for HPC Data</title>
      <link>https://arxiv.org/abs/2501.14733</link>
      <description>arXiv:2501.14733v1 Announce Type: new 
Abstract: High-Performance Computing (HPC) is crucial for performing advanced computational tasks, yet their complexity often challenges users, particularly those unfamiliar with HPC-specific commands and workflows. This paper introduces Hypothetical Command Embeddings (HyCE), a novel method that extends Retrieval-Augmented Generation (RAG) by integrating real-time, user-specific HPC data, enhancing accessibility to these systems. HyCE enriches large language models (LLM) with real-time, user-specific HPC information, addressing the limitations of fine-tuned models on such data. We evaluate HyCE using an automated RAG evaluation framework, where the LLM itself creates synthetic questions from the HPC data and serves as a judge, assessing the efficacy of the extended RAG with the evaluation metrics relevant for HPC tasks. Additionally, we tackle essential security concerns, including data privacy and command execution risks, associated with deploying LLMs in HPC environments. This solution provides a scalable and adaptable approach for HPC clusters to leverage LLMs as HPC expert, bridging the gap between users and the complex systems of HPC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14733v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusuke Miyashita, Patrick Kin Man Tung, Johan Barth\'elemy</dc:creator>
    </item>
    <item>
      <title>Research on the Application of Spark Streaming Real-Time Data Analysis System and large language model Intelligent Agents</title>
      <link>https://arxiv.org/abs/2501.14734</link>
      <description>arXiv:2501.14734v1 Announce Type: new 
Abstract: This study explores the integration of Agent AI with LangGraph to enhance real-time data analysis systems in big data environments. The proposed framework overcomes limitations of static workflows, inefficient stateful computations, and lack of human intervention by leveraging LangGraph's graph-based workflow construction and dynamic decision-making capabilities. LangGraph allows large language models (LLMs) to dynamically determine control flows, invoke tools, and assess the necessity of further actions, improving flexibility and efficiency.
  The system architecture incorporates Apache Spark Streaming, Kafka, and LangGraph to create a high-performance sentiment analysis system. LangGraph's capabilities include precise state management, dynamic workflow construction, and robust memory checkpointing, enabling seamless multi-turn interactions and context retention. Human-in-the-loop mechanisms are integrated to refine sentiment analysis, particularly in ambiguous or high-stakes scenarios, ensuring greater reliability and contextual relevance.
  Key features such as real-time state streaming, debugging via LangGraph Studio, and efficient handling of large-scale data streams make this framework ideal for adaptive decision-making. Experimental results confirm the system's ability to classify inquiries, detect sentiment trends, and escalate complex issues for manual review, demonstrating a synergistic blend of LLM capabilities and human oversight.
  This work presents a scalable, adaptable, and reliable solution for real-time sentiment analysis and decision-making, advancing the use of Agent AI and LangGraph in big data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14734v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialin Wang, Zhihua Duan</dc:creator>
    </item>
    <item>
      <title>Reproduction Research of FSA-Benchmark</title>
      <link>https://arxiv.org/abs/2501.14739</link>
      <description>arXiv:2501.14739v1 Announce Type: new 
Abstract: In the current landscape of big data, the reliability and performance of storage systems are essential to the success of various applications and services. as data volumes continue to grow exponentially, the complexity and scale of the storage infrastructures needed to manage this data also increase. a significant challenge faced by data centers and storage systems is the detection and management of fail-slow disks that experience a gradual decline in performance before ultimately failing. Unlike outright disk failures, fail-slow conditions can go undetected for prolonged periods, leading to considerable impacts on system performance and user experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14739v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Ludolf, Yesmin Reyna-Hernandez, Matthew Trevino</dc:creator>
    </item>
    <item>
      <title>Datapath Combinational Equivalence Checking With Hybrid Sweeping Engines and Parallelization</title>
      <link>https://arxiv.org/abs/2501.14740</link>
      <description>arXiv:2501.14740v1 Announce Type: new 
Abstract: In the application of IC design for microprocessors, there are often demands for optimizing the implementation of datapath circuits, on which various arithmetic operations are performed. Combinational equivalence checking (CEC) plays an essential role in ensuring the correctness of design optimization. The most prevalent CEC algorithms are based on SAT sweeping, which utilizes SAT to prove the equivalence of the internal node pairs in topological order, and the equivalent nodes are merged. Datapath circuits usually contain equivalent pairs for which the transitive fan-in cones are small but have a high XOR chain density, and proving such node pairs is very difficult for SAT solvers. An exact probability-based simulation (EPS) is suitable for verifying such pairs, while this method is not suitable for pairs with many primary inputs due to the memory cost. We first reduce the memory cost of EPS and integrate it to improve the SAT sweeping method. Considering the complementary abilities of SAT and EPS, we design an engine selection heuristic to dynamically choose SAT or EPS in the sweeping process, according to XOR chain density. Our method is further improved by reducing unnecessary engine calls by detecting regularity. Furthermore, we parallelized the SAT and EPS engines of HybridCEC, leading to the parallel CEC prover. Experiments on a benchmark suite from industrial datapath circuits show that our method is much faster than the state-of-the-art CEC tool namely ABC &amp;cec on nearly all instances, and is more than 100x faster on 30% of the instances, 1000x faster on 12% of the instances. In addition, the 64 threads version of our method achieved 77x speedup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14740v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCAD57390.2023.10323876</arxiv:DOI>
      <dc:creator>Zhihan Chen, Xindi Zhang, Yuhang Qian, Shaowei Cai</dc:creator>
    </item>
    <item>
      <title>KVDirect: Distributed Disaggregated LLM Inference</title>
      <link>https://arxiv.org/abs/2501.14743</link>
      <description>arXiv:2501.14743v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have become the new foundation for many applications, reshaping human society like a storm. Disaggregated inference, which separates prefill and decode stages, is a promising approach to improving hardware utilization and service quality. However, due to inefficient inter-node communication, existing systems restrict disaggregated inference to a single node, limiting resource allocation flexibility and reducing service capacity. This paper introduces KVDirect, which optimizes KV cache transfer to enable a distributed disaggregated LLM inference. KVDirect achieves this through the following contributions. First, we propose a novel tensor-centric communication mechanism that reduces the synchronization overhead in traditional distributed GPU systems. Second, we design a custom communication library to support dynamic GPU resource scheduling and efficient KV cache transfer. Third, we introduce a pull-based KV cache transfer strategy that reduces GPU resource idling and improves latency. Finally, we implement KVDirect as an open-source LLM inference framework. Our evaluation demonstrates that KVDirect reduces per-request latency by 55% compared to the baseline across diverse workloads under the same resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14743v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shiyang Chen, Rain Jiang, Dezhi Yu, Jinlai Xu, Mengyuan Chao, Fanlong Meng, Chenyu Jiang, Wei Xu, Hang Liu</dc:creator>
    </item>
    <item>
      <title>AI-Driven Health Monitoring of Distributed Computing Architecture: Insights from XGBoost and SHAP</title>
      <link>https://arxiv.org/abs/2501.14745</link>
      <description>arXiv:2501.14745v1 Announce Type: new 
Abstract: With the rapid development of artificial intelligence technology, its application in the optimization of complex computer systems is becoming more and more extensive. Edge computing is an efficient distributed computing architecture, and the health status of its nodes directly affects the performance and reliability of the entire system. In view of the lack of accuracy and interpretability of traditional methods in node health status judgment, this paper proposes a health status judgment method based on XGBoost and combines the SHAP method to analyze the interpretability of the model. Through experiments, it is verified that XGBoost has superior performance in processing complex features and nonlinear data of edge computing nodes, especially in capturing the impact of key features (such as response time and power consumption) on node status. SHAP value analysis further reveals the global and local importance of features, so that the model not only has high precision discrimination ability but also can provide intuitive explanations, providing data support for system optimization. Research shows that the combination of AI technology and computer system optimization can not only realize the intelligent monitoring of the health status of edge computing nodes but also provide a scientific basis for dynamic optimization scheduling, resource management and anomaly detection. In the future, with the in-depth development of AI technology, model dynamics, cross-node collaborative optimization and multimodal data fusion will become the focus of research, providing important support for the intelligent evolution of edge computing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14745v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoxuan Sun, Yue Yao, Xiaoye Wang, Pochun Li, Xuan Li</dc:creator>
    </item>
    <item>
      <title>ABACUS: A FinOps Service for Cloud Cost Optimization</title>
      <link>https://arxiv.org/abs/2501.14753</link>
      <description>arXiv:2501.14753v1 Announce Type: new 
Abstract: In recent years, as more enterprises have moved their infrastructure to the cloud, significant challenges have emerged in achieving holistic cloud spend visibility and cost optimization. FinOps practices provide a way for enterprises to achieve these business goals by optimizing cloud costs and bringing accountability to cloud spend. This paper presents ABACUS - Automated Budget Analysis and Cloud Usage Surveillance, a FinOps solution for optimizing cloud costs by setting budgets, enforcing those budgets through blocking new deployments, and alerting appropriate teams if spending breaches a budget threshold. ABACUS also leverages best practices like Infrastructure-as-Code to alert engineering teams of the expected cost of deployment before resources are deployed in the cloud. Finally, future research directions are proposed to advance the state of the art in this important field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14753v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Saurabh Deochake</dc:creator>
    </item>
    <item>
      <title>Data-Juicer 2.0: Cloud-Scale Adaptive Data Processing for Foundation Models</title>
      <link>https://arxiv.org/abs/2501.14755</link>
      <description>arXiv:2501.14755v1 Announce Type: new 
Abstract: The burgeoning field of foundation models necessitates advanced data processing mechanisms capable of harnessing vast valuable data with varied types utilized by these models. Nevertheless, the current landscape presents unique challenges that traditional data processing frameworks cannot handle effectively, especially with multimodal intricacies. In response, we present Data-Juicer 2.0, a new system offering fruitful data processing capabilities backed by over a hundred operators spanning various modalities like text, image, audio, and video. With seamless compatibility and dedicated optimization to popular dataset hubs like Hugging Face and computing engines like Ray, Data-Juicer 2.0 enhances its predecessor in both usability, efficiency, and programmability. It features an easily accessible user interface layer that supports decoupled Python interactions, RESTful APIs, and conversational commands. Alongside this, it contains a core runtime layer optimized for adaptive execution and management across different dataset scales, processing demands, and computational environments, while shielding unnecessary system details. Extensive empirical evaluations demonstrate Data-Juicer 2.0's remarkable performance and scalability, highlighting its capability to efficiently process tens of billions of data samples with tens of thousands of CPU cores. The system is publicly available, actively maintained, and broadly adopted in diverse research endeavors, practical applications, and real-world products such as Alibaba Cloud PAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14755v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daoyuan Chen, Yilun Huang, Xuchen Pan, Nana Jiang, Haibin Wang, Ce Ge, Yushuo Chen, Wenhao Zhang, Zhijian Ma, Yilei Zhang, Jun Huang, Wei Lin, Yaliang Li, Bolin Ding, Jingren Zhou</dc:creator>
    </item>
    <item>
      <title>Heat: Satellite's meat is GPU's poison</title>
      <link>https://arxiv.org/abs/2501.14757</link>
      <description>arXiv:2501.14757v1 Announce Type: new 
Abstract: In satellite applications, managing thermal conditions is a significant challenge due to the extreme fluctuations in temperature during orbital cycles. One of the solutions is to heat the satellite when it is not exposed to sunlight, which could protect the satellites from extremely low temperatures. However, heat dissipation is necessary for Graphics Processing Units (GPUs) to operate properly and efficiently. In this way, this paper investigates the use of GPU as a means of passive heating in low-earth orbit (LEO) satellites. Our approach uses GPUs to generate heat during the eclipse phase of satellite orbits, substituting traditional heating systems, while the GPUs are also cooled down during this process. The results highlight the potential advantages and limitations of this method, including the cost implications, operational restrictions, and the technical complexity involved. Also, this paper explores the thermal behavior of GPUs under different computational loads, specifically focusing on execution-dominated and FLOP-dominated workloads. Moreover, this paper discusses future directions for improving GPU-based heating solutions, including further cost analysis, system optimization, and practical testing in real satellite missions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14757v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhehu Yuan, Jinyang Liu, Guanqun Song, Ting Zhu</dc:creator>
    </item>
    <item>
      <title>Intent-driven scheduling of backup jobs</title>
      <link>https://arxiv.org/abs/2501.14763</link>
      <description>arXiv:2501.14763v1 Announce Type: new 
Abstract: Job scheduling under various constraints to achieve global optimization is a well-studied problem. However, in scenarios that involve time-dependent constraints, such as scheduling backup jobs, achieving global optimization may not always be desirable. This paper presents a framework for scheduling new backup jobs in the presence of existing job schedules, focusing on satisfying intent-based constraints without disrupting current schedules. The proposed method accommodates various scheduling intents and constraints, and its effectiveness is validated through extensive testing against a variety of backup scenarios on real-world data from Veritas Netbackup customer policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14763v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souvik Dutta, Suri Brahmaroutu</dc:creator>
    </item>
    <item>
      <title>Hybrid Cooperative Co-Evolution Algorithm for Deadlock-prone Distributed Assembly Flowshop Scheduling with Limited buffers Using Petri nets</title>
      <link>https://arxiv.org/abs/2501.14765</link>
      <description>arXiv:2501.14765v1 Announce Type: new 
Abstract: The distributed assembly flowshop scheduling problem (DAFSP) can be applied to immense manufacturing environments. In DAFSP, jobs are first processed in distributed flowshops, and then assembled into final products by an assembly machine, which usually has limited buffers in practical application. This limited capacity can lead to deadlocks, halting job completion and blocking the entire manufacturing process. However, existing scheduling methods fail to address these deadlocks in DAFSP effectively. As such, we develop a hybrid cooperative co-evolution (HCCE) algorithm for solving the deadlock-prone DAFSP by minimizing the makespan. For the first time, we use Petri nets to analyze the deadlocks in DAFSP and propose a Petri net-based deadlock amending method (IDAM), which is further integrated into HCCE to ensure the feasibility (i.e., deadlock-freeness) of solutions. Importantly, HCCE contains an elite archive (EAR) and two subpopulations. It uses the problem-specific operators for heuristic initialization and global-search. To enhance the quality and diversity of solutions, an information transfer mechanism (ITM) is developed among subpopulation and EAR, and four local-search operators are performed sequentially on each individual in EAR. Finally, comprehensive experiments demonstrate the effectiveness and superiority of the proposed HCCE algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14765v1</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyi Wang, Yanxiang Feng, Xiaoling Li, Guanghui Zhang, Yikang Yang</dc:creator>
    </item>
    <item>
      <title>Optimizing SSD Caches for Cloud Block Storage Systems Using Machine Learning Approaches</title>
      <link>https://arxiv.org/abs/2501.14770</link>
      <description>arXiv:2501.14770v1 Announce Type: new 
Abstract: The growing demand for efficient cloud storage solutions has led to the widespread adoption of Solid-State Drives (SSDs) for caching in cloud block storage systems. The management of data writes to SSD caches plays a crucial role in improving overall system performance, reducing latency, and extending the lifespan of storage devices. A critical challenge arises from the large volume of write-only data, which significantly impacts the performance of SSD caches when handled inefficiently. Specifically, writes that have not been read for a certain period may introduce unnecessary write traffic to the SSD cache without offering substantial benefits for cache performance. This paper proposes a novel approach to mitigate this issue by leveraging machine learning techniques to dynamically optimize the write policy in cloud-based storage systems. The proposed method identifies write-only data and selectively filters it out in real-time, thereby minimizing the number of unnecessary write operations and improving the overall performance of the cache system. Experimental results demonstrate that the proposed machine learning-based policy significantly outperforms traditional approaches by reducing the number of harmful writes and optimizing cache utilization. This solution is particularly suitable for cloud environments with varying and unpredictable workloads, where traditional cache management strategies often fall short.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14770v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.OS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao</dc:creator>
    </item>
    <item>
      <title>Dynamic Adaptation in Data Storage: Real-Time Machine Learning for Enhanced Prefetching</title>
      <link>https://arxiv.org/abs/2501.14771</link>
      <description>arXiv:2501.14771v1 Announce Type: new 
Abstract: The exponential growth of data storage demands has necessitated the evolution of hierarchical storage management strategies [1]. This study explores the application of streaming machine learning [3] to revolutionize data prefetching within multi-tiered storage systems. Unlike traditional batch-trained models, streaming machine learning [5] offers adaptability, real-time insights, and computational efficiency, responding dynamically to workload variations. This work designs and validates an innovative framework that integrates streaming classification models for predicting file access patterns, specifically the next file offset. Leveraging comprehensive feature engineering and real-time evaluation over extensive production traces, the proposed methodology achieves substantial improvements in prediction accuracy, memory efficiency, and system adaptability. The results underscore the potential of streaming models in real-time storage management, setting a precedent for advanced caching and tiering strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14771v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.OS</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiyu Cheng, Chang Zhou, Yang Zhao, Jin Cao</dc:creator>
    </item>
    <item>
      <title>LEISA: A Scalable Microservice-based System for Efficient Livestock Data Sharing</title>
      <link>https://arxiv.org/abs/2501.14781</link>
      <description>arXiv:2501.14781v1 Announce Type: new 
Abstract: In the livestock sector, the fragmented data landscape across isolated systems presents a significant challenge, necessitating interoperability and integration. In this article, we introduce the Livestock Event Information Sharing Architecture (LEISA), a novel architecture designed to facilitate the sharing of livestock event information among various stakeholders, such as farmers and service providers. We comprehensively analysed both functional and non-functional requirements of such an architecture to ensure data integrity, standardisation, validation, ownership, and real-time data dissemination across various stakeholders while maintaining reliability, scalability, and interoperability. We designed and implemented LEISA using a cloud-based microservice architecture incorporating advanced technologies such as RESTful APIs and message brokers. We evaluated the efficiency and effectiveness of LEISA in terms of performance, scalability, and reliability, and examined its applicability through several real-world use-case scenarios. LEISA facilitates seamless data sharing, system interoperability, and enhanced decision-making capabilities, promoting synergy among stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14781v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahir Habib, Muhammad Ashad Kabir, Lihong Zheng</dc:creator>
    </item>
    <item>
      <title>Persistent HyTM via Fast Path Fine-Grained Locking</title>
      <link>https://arxiv.org/abs/2501.14783</link>
      <description>arXiv:2501.14783v1 Announce Type: new 
Abstract: Utilizing hardware transactional memory (HTM) in conjunction with non-volatile memory (NVM) to achieve persistence is quite difficult and somewhat awkward due to the fact that the primitives utilized to write data to NVM will abort HTM transactions. We present several persistent hybrid transactional memory (HyTM) that, perhaps counterintuitively, utilize an HTM fast path primarily to read or acquire fine-grained locks which protect data items. Our implementations guarantee durable linearizable transactions and the STM path satisfies either weak progressiveness or strong progressiveness. We discuss the design choices related to the differing progress guarantees and we examine how these design choices impact performance. We evaluate our persistent HyTM implementations using various microbenchmarks. Our implementations achieve improved performance especially for read dominant workloads compared to state of the art persistent STMs and persistent HyTMs despite the challenges and apparent awkwardness of using current implementation HTM to achieve persistence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14783v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaetano Coccimiglio, Trevor Brown, Srivatsan Ravi</dc:creator>
    </item>
    <item>
      <title>DeServe: Towards Affordable Offline LLM Inference via Decentralization</title>
      <link>https://arxiv.org/abs/2501.14784</link>
      <description>arXiv:2501.14784v1 Announce Type: new 
Abstract: The rapid growth of generative AI and its integration into everyday workflows have significantly increased the demand for large language model (LLM) inference services. While proprietary models remain popular, recent advancements in open-source LLMs have positioned them as strong contenders. However, deploying these models is often constrained by the high costs and limited availability of GPU resources. In response, this paper presents the design of a decentralized offline serving system for LLM inference. Utilizing idle GPU resources, our proposed system, DeServe, decentralizes access to LLMs at a lower cost. DeServe specifically addresses key challenges in optimizing serving throughput in high-latency network environments. Experiments demonstrate that DeServe achieves a 6.7x-12.6x improvement in throughput over existing serving system baselines in such conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14784v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linyu Wu, Xiaoyuan Liu, Tianneng Shi, Zhe Ye, Dawn Song</dc:creator>
    </item>
    <item>
      <title>Punch Out Model Synthesis: A Stochastic Algorithm for Constraint Based Tiling Generation</title>
      <link>https://arxiv.org/abs/2501.14786</link>
      <description>arXiv:2501.14786v1 Announce Type: new 
Abstract: As an artistic aid in tiled level design, Constraint Based Tiling Generation (CBTG) algorithms can help to automatically create level realizations from a set of tiles and placement constraints. Merrell's Modify in Blocks Model Synthesis (MMS) and Gumin's Wave Function Collapse (WFC) have been proposed as Constraint Based Tiling Generation (CBTG) algorithms that work well for many scenarios but have limitations in problem size, problem setup and solution biasing. We present Punch Out Model Synthesis (POMS), a Constraint Based Tiling Generation algorithm, that can handle large problem sizes, requires minimal assumptions for setup and can help mitigate solution biasing. POMS attempts to resolve indeterminate grid regions by trying to progressively realize sub-blocks, performing a stochastic boundary erosion on previously resolved regions should sub-block resolution fail. We highlight the results of running a reference implementation on different tile sets and discuss a tile correlation length, implied by the tile constraints, and its role in choosing an appropriate block size to aid POMS in successfully finding grid realizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14786v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zzyv Zzyzek</dc:creator>
    </item>
    <item>
      <title>HeteroLLM: Accelerating Large Language Model Inference on Mobile SoCs platform with Heterogeneous AI Accelerators</title>
      <link>https://arxiv.org/abs/2501.14794</link>
      <description>arXiv:2501.14794v1 Announce Type: new 
Abstract: With the rapid advancement of artificial intelligence technologies such as ChatGPT, AI agents and video generation,contemporary mobile systems have begun integrating these AI capabilities on local devices to enhance privacy and reduce response latency. To meet the computational demands of AI tasks, current mobile SoCs are equipped with diverse AI accelerators, including GPUs and Neural Processing Units (NPUs). However, there has not been a comprehensive characterization of these heterogeneous processors, and existing designs typically only leverage a single AI accelerator for LLM inference, leading to suboptimal use of computational resources and memory bandwidth. In this paper, we first summarize key performance characteristics of mobile SoC, including heterogeneous processors, unified memory, synchronization, etc. Drawing on these observations, we propose different tensor partition strategies to fulfill the distinct requirements of the prefill and decoding phases. We further design a fast synchronization mechanism that leverages the unified memory address provided by mobile SoCs. By employing these techniques, we present HeteroLLM, the fastest LLM inference engine in mobile devices which supports both layer-level and tensor-level heterogeneous execution. Evaluation results show that HeteroLLM achieves 9.99 and 4.36 performance improvement over other mobile-side LLM inference engines: MLC and MNN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14794v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Le Chen, Dahu Feng, Erhu Feng, Rong Zhao, Yingrui Wang, Yubin Xia, Haibo Chen, Pinjie Xu</dc:creator>
    </item>
    <item>
      <title>Accelerating Particle-Mesh Algorithms with FPGAs and OmpSs@OpenCL</title>
      <link>https://arxiv.org/abs/2501.14795</link>
      <description>arXiv:2501.14795v1 Announce Type: new 
Abstract: Due to its flexible architecture, FPGAs support unique, deep hardware pipeline implementations for accelerating HPC applications. However, these devices are quite new in the HPC space, and thus, have been scarcely explored outside some specific scientific domain, such as machine learning or biological sequence alignment. The objective of this thesis is to characterize the FPGA-based solution for accelerating particle-mesh algorithms, in which the force applied to each particle is computed based on the fields deposited in a finite mesh (or grid). Our starting point is a 2D kinetic PIC plasma simulator called ZPIC that has the same core algorithm and functionalities as OSIRIS. To create an efficient hardware design, the program keeps the particles strictly sorted by tiles (a group of cells) and uses the local memory as an explicitly managed cache. We also create multiple copies of the local current buffer to solve dependencies during the deposition phase. The resulting pipeline was replicated multiple times to explore data parallelism and increase its throughput. We then compare our hardware solution against similar implementations on GPU and multicore CPUs, showing promising results in term of power efficiency and performance.
  Keywords: FPGA, OpenCL, Kinetic Plasma Simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14795v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Lee Guidotti</dc:creator>
    </item>
    <item>
      <title>DNN-Powered MLOps Pipeline Optimization for Large Language Models: A Framework for Automated Deployment and Resource Management</title>
      <link>https://arxiv.org/abs/2501.14802</link>
      <description>arXiv:2501.14802v1 Announce Type: new 
Abstract: The exponential growth in the size and complexity of Large Language Models (LLMs) has introduced unprecedented challenges in their deployment and operational management. Traditional MLOps approaches often fail to efficiently handle the scale, resource requirements, and dynamic nature of these models. This research presents a novel framework that leverages Deep Neural Networks (DNNs) to optimize MLOps pipelines specifically for LLMs. Our approach introduces an intelligent system that automates deployment decisions, resource allocation, and pipeline optimization while maintaining optimal performance and cost efficiency. Through extensive experimentation across multiple cloud environments and deployment scenarios, we demonstrate significant improvements: 40% enhancement in resource utilization, 35% reduction in deployment latency, and 30% decrease in operational costs compared to traditional MLOps approaches. The framework's ability to adapt to varying workloads and automatically optimize deployment strategies represents a significant advancement in automated MLOps management for large-scale language models. Our framework introduces several novel components including a multi-stream neural architecture for processing heterogeneous operational metrics, an adaptive resource allocation system that continuously learns from deployment patterns, and a sophisticated deployment orchestration mechanism that automatically selects optimal strategies based on model characteristics and environmental conditions. The system demonstrates robust performance across various deployment scenarios, including multi-cloud environments, high-throughput production systems, and cost-sensitive deployments. Through rigorous evaluation using production workloads from multiple organizations, we validate our approach's effectiveness in reducing operational complexity while improving system reliability and cost efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14802v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mahesh Vaijainthymala Krishnamoorthy, Kuppusamy Vellamadam Palavesam, Siva Venkatesh Arcot, Rajarajeswari Chinniah Kuppuswami</dc:creator>
    </item>
    <item>
      <title>An efficient GPU approach for designing 3D cultural heritage information systems</title>
      <link>https://arxiv.org/abs/2501.14807</link>
      <description>arXiv:2501.14807v1 Announce Type: new 
Abstract: We propose a new architecture for 3D information systems that takes advantage of the inherent parallelism of the GPUs. This new solution structures information as thematic layers, allowing a level of detail independent of the resolution of the meshes. Previous proposals of layer based systems present issues, both in terms of performance and storage, due to the use of octrees to index information. In contrast, our approach employs two-dimensional textures, highly efficient in GPU, to store and index information. In this article we describe this architecture and detail the GPU algorithms required to edit these layers. Finally, we present a performance comparison of our approach against an octree based system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14807v1</guid>
      <category>cs.DC</category>
      <category>cs.GR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.culher.2019.05.003</arxiv:DOI>
      <arxiv:journal_reference>Journal of Cultural Heritage, Volume 41, January-February 2020, Pages 142-151</arxiv:journal_reference>
      <dc:creator>Luis L\'opez, Juan Carlos Torres, Germ\'an Arroyo, Pedro Cano, Domingo Mart\'in</dc:creator>
    </item>
    <item>
      <title>HyGen: Efficient LLM Serving via Elastic Online-Offline Request Co-location</title>
      <link>https://arxiv.org/abs/2501.14808</link>
      <description>arXiv:2501.14808v1 Announce Type: new 
Abstract: Recent advancements in large language models (LLMs) have facilitated a wide range of applications with distinct quality-of-experience requirements, from latency-sensitive online tasks, such as interactive chatbots, to throughput-focused offline tasks like document summarization. While deploying dedicated machines for these services ensures high-quality performance, it often results in resource underutilization. This paper introduces HyGen, an interference-aware LLM serving system that enables efficient co-location of online and offline workloads while preserving latency requirements. HyGen incorporates two key innovations: (1) performance control mechanisms, including a latency predictor for batch execution time estimation and an SLO-aware profiler to quantify interference, and (2) SLO-aware offline scheduling policies that maximize throughput and prevent starvation, without compromising online serving latency. Our evaluation on production workloads shows that HyGen achieves up to 5.84x higher throughput compared to existing advances while maintaining comparable latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14808v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ting Sun, Penghan Wang, Fan Lai</dc:creator>
    </item>
    <item>
      <title>Implementation and Evaluation of GBDI Memory Compression Algorithm Using C/C++ on a Broader Range of Workloads</title>
      <link>https://arxiv.org/abs/2501.14812</link>
      <description>arXiv:2501.14812v1 Announce Type: new 
Abstract: Memory compression is an important approach in computer architecture for decreasing memory footprint and improving system performance. In this paper, we use C/C++ to develop a current memory compression algorithm; the Global Bases Delta Immediate (GBDI) algorithm, which was proposed at HPCA'2022. By using global bases and enabling deltas within the same block to vary in size, the GBDI compression algorithm decreases the size of encoded data. The goal of this research is to assess the effectivenessof the GBDI algorithm by examining its compression ratios under a broader range of workloads. Our research leads to a better knowledge of the GBDI algorithm's effectiveness and the potential benefits of memory compression techniques for various sorts of applications. Furthermore, our C/C++ version of the algorithm gives academics and practitioners a high degree of freedom over customizing the algorithm for individual workloads and optimizing its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14812v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.13140/RG.2.2.11020.28804</arxiv:DOI>
      <dc:creator>Adeyemi Aina</dc:creator>
    </item>
    <item>
      <title>A VM-HDL Co-Simulation Framework for Systems with PCIe-Connected FPGAs</title>
      <link>https://arxiv.org/abs/2501.14815</link>
      <description>arXiv:2501.14815v1 Announce Type: new 
Abstract: PCIe-connected FPGAs are gaining popularity as an accelerator technology in data centers. However, it is challenging to jointly develop and debug host software and FPGA hardware. Changes to the hardware design require a time-consuming FPGA synthesis process, and modification to the software, especially the operating system and device drivers, can frequently cause the system to hang, without providing enough information for debugging. The combination of these problems results in long debug iterations and a slow development process. To overcome these problems, we designed a VM-HDL co-simulation framework, which is capable of running the same software, operating system, and hardware designs as the target physical system, while providing full visibility and significantly shorter debug iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14815v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shenghsun Cho, Mrunal Patel, Basavaraj Kaladagi, Han Chen, Tapti Palit, Michael Ferdman, Peter Milder</dc:creator>
    </item>
    <item>
      <title>Quantifying Energy and Cost Benefits of Hybrid Edge Cloud: Analysis of Traditional and Agentic Workloads</title>
      <link>https://arxiv.org/abs/2501.14823</link>
      <description>arXiv:2501.14823v1 Announce Type: new 
Abstract: This paper examines the workload distribution challenges in centralized cloud systems and demonstrates how Hybrid Edge Cloud (HEC) [1] mitigates these inefficiencies. Workloads in cloud environments often follow a Pareto distribution, where a small percentage of tasks consume most resources, leading to bottlenecks and energy inefficiencies. By analyzing both traditional workloads reflective of typical IoT and smart device usage and agentic workloads, such as those generated by AI agents, robotics, and autonomous systems, this study quantifies the energy and cost savings enabled by HEC. Our findings reveal that HEC achieves energy savings of up to 75% and cost reductions exceeding 80%, even in resource-intensive agentic scenarios. These results highlight the critical role of HEC in enabling scalable, cost-effective, and sustainable computing for the next generation of intelligent systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14823v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siavash Alamouti</dc:creator>
    </item>
    <item>
      <title>Resource Allocation Driven by Large Models in Future Semantic-Aware Networks</title>
      <link>https://arxiv.org/abs/2501.14832</link>
      <description>arXiv:2501.14832v1 Announce Type: new 
Abstract: Large model has emerged as a key enabler for the popularity of future networked intelligent applications. However, the surge of data traffic brought by intelligent applications puts pressure on the resource utilization and energy consumption of the future networks. With efficient content understanding capabilities, semantic communication holds significant potential for reducing data transmission in intelligent applications. In this article, resource allocation driven by large models in semantic-aware networks is investigated. Specifically, a semantic-aware communication network architecture based on scene graph models and multimodal pre-trained models is designed to achieve efficient data transmission. On the basis of the proposed network architecture, an intelligent resource allocation scheme in semantic-aware network is proposed to further enhance resource utilization efficiency. In the resource allocation scheme, the semantic transmission quality is adopted as an evaluation metric and the impact of wireless channel fading on semantic transmission is analyzed. To maximize the semantic transmission quality for multiple users, a diffusion model-based decision-making scheme is designed to address the power allocation problem in semantic-aware networks. Simulation results demonstrate that the proposed large-model-driven network architecture and resource allocation scheme achieve high-quality semantic transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14832v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haijun Zhang, Jiaxin Ni, Zijun Wu, Xiangnan Liu, V. C. M. Leung</dc:creator>
    </item>
    <item>
      <title>Pod: An Optimal-Latency, Censorship-Free, and Accountable Generalized Consensus Layer</title>
      <link>https://arxiv.org/abs/2501.14931</link>
      <description>arXiv:2501.14931v1 Announce Type: new 
Abstract: This work addresses the inherent issues of high latency in blockchains and low scalability in traditional consensus protocols. We present pod, a novel notion of consensus whose first priority is to achieve the physically optimal latency of one round trip, i.e., requiring only one round for writing a new transaction and one round for reading it. To accomplish this, we first eliminate inter-replica communication. Instead, clients send transactions directly to all replicas, each replica independently processes transactions and appends them to its log, and then clients receive and extract information from these logs. The replicas employ techniques such as transaction timestamping and replica-log sequencing, which allow clients to extract valuable information about the transactions and the state of the system.
  Necessarily, this construction achieves weaker properties than a total-order broadcast protocol, due to existing lower bounds. Our work models the primitive of pod and defines its security properties. We then prove that our pod-core construction satisfies properties such as transaction confirmation within $2\delta$, censorship resistance against Byzantine replicas, and accountability for safety violations. We show that a wire range of applications, such as payment systems, auctions, and decentralized data stores, can be based on a pod primitive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14931v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Orestis Alpos, Bernardo David, Dionysis Zindros</dc:creator>
    </item>
    <item>
      <title>Fully-Automated Code Generation for Efficient Computation of Sparse Matrix Permanents on GPUs</title>
      <link>https://arxiv.org/abs/2501.15126</link>
      <description>arXiv:2501.15126v1 Announce Type: new 
Abstract: Registers are the fastest memory components within the GPU's complex memory hierarchy, accessed by names rather than addresses. They are managed entirely by the compiler through a process called register allocation, during which the compiler attempts to cache predictable data from thread-local memory into thread-private registers. Computing the permanent of a sparse matrix poses a challenge for compilers, as optimizing this process is hindered by the unpredictable distribution of nonzero elements, which only become known at runtime. In this work, we employ fully-automated code generation to address this, producing highly optimized kernels tailored to the matrix's sparsity pattern. State-of-the-art permanent computation algorithms require each thread to store a private array, denoted x, of size n. We first propose a technique that fully stores these arrays in registers, with inclusion and exclusion kernels generated for each column. To minimize control divergence and reduce the number of unique kernels within a warp, we exploit the internal structure of Gray codes, which are also used in the state-of-the-art algorithm. Our second technique reduces register pressure by utilizing both registers and global memory and introduces a matrix ordering and partitioning strategy for greater efficiency. On synthetic matrices, this approach achieves a 31x speedup over state-of-the-art CPU implementations on 112 cores, and an 8x speedup compared to our traditional GPU implementation. For real-world matrices, these speedups are 24.9x and 4.9x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15126v1</guid>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Elbek, Kamer Kaya</dc:creator>
    </item>
    <item>
      <title>ExClique: An Express Consensus Algorithm for High-Speed Transaction Process in Blockchains</title>
      <link>https://arxiv.org/abs/2501.15289</link>
      <description>arXiv:2501.15289v1 Announce Type: new 
Abstract: Proof of Authority (PoA) plays a pivotal role in blockchains for reaching consensus. Clique, which selects consensus nodes to generate blocks with a pre-determined order, is the most popular implementation of PoA due to its low communication overhead and energy consumption. However, our study unveils that the speed to process transactions by Clique is severely restricted by 1) the long communication delay of full blocks (each containing a certain number of transactions) between consensus nodes; and 2) occurrences of no-turn blocks, generated by no-turn nodes if an in-turn block generation fails. Consequently, Clique struggles to support distributed applications requiring a high transaction processing speed, e.g., online gaming. To overcome this deficiency, we propose an Express Clique (ExClique) algorithm by improving Clique from two perspectives: compacting blocks for broadcasting to shorten communication delay and prohibiting the occurrences of no-turn blocks. For performance evaluation, we implement ExClique by modifying Geth of Ethereum, the software implementing Clique, and deploy a permissioned blockchain network by using container technology. The experimental results show that ExClique achieves a substantial enhancement in transactions per second (TPS). Specifically, it boosts TPS by 2.25X in a typical network with 21 consensus nodes and an impressive 7.01X in a large-scale network with 101 consensus nodes when compared to Clique.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15289v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chonghe Zhao, Yipeng Zhou, Shengli Zhang, Quan Z. Sheng, Yang Zhang, Shiting Wen</dc:creator>
    </item>
    <item>
      <title>Task Scheduling in Geo-Distributed Computing: A Survey</title>
      <link>https://arxiv.org/abs/2501.15504</link>
      <description>arXiv:2501.15504v1 Announce Type: new 
Abstract: Geo-distributed computing, a paradigm that assigns computational tasks to globally distributed nodes, has emerged as a promising approach in cloud computing, edge computing, cloud-edge computing and supercomputer computing (HPC). It enables low-latency services, ensures data locality, and handles large-scale applications. As global computing capacity and task demands increase rapidly, scheduling tasks for efficient execution in geo-distributed computing systems has become an increasingly critical research challenge. It arises from the inherent characteristics of geographic distribution, including heterogeneous network conditions, region-specific resource pricing, and varying computational capabilities across locations. Researchers have developed diverse task scheduling methods tailored to geo-distributed scenarios, aiming to achieve objectives such as performance enhancement, fairness assurance, and fault-tolerance improvement. This survey provides a comprehensive and systematic review of task scheduling techniques across four major distributed computing environments, with an in-depth analysis of these approaches based on their core scheduling objectives. Through our analysis, we identify key research challenges and outline promising directions for advancing task scheduling in geo-distributed computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15504v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujian Wu, Shanjiang Tang, Ce Yu, Bin Yang, Chao Sun, Jian Xiao, Hutong Wu</dc:creator>
    </item>
    <item>
      <title>Adaptive AI-based Decentralized Resource Management in the Cloud-Edge Continuum</title>
      <link>https://arxiv.org/abs/2501.15802</link>
      <description>arXiv:2501.15802v1 Announce Type: new 
Abstract: The increasing complexity of application requirements and the dynamic nature of the Cloud-Edge Continuum present significant challenges for efficient resource management. These challenges stem from the ever-changing infrastructure, which is characterized by additions, removals, and reconfigurations of nodes and links, as well as the variability of application workloads. Traditional centralized approaches struggle to adapt to these changes due to their static nature, while decentralized solutions face challenges such as limited global visibility and coordination overhead. This paper proposes a hybrid decentralized framework for dynamic application placement and resource management. The framework utilizes Graph Neural Networks (GNNs) to embed resource and application states, enabling comprehensive representation and efficient decision-making. It employs a collaborative multi-agent reinforcement learning (MARL) approach, where local agents optimize resource management in their neighborhoods and a global orchestrator ensures system-wide coordination. By combining decentralized application placement with centralized oversight, our framework addresses the scalability, adaptability, and accuracy challenges inherent in the Cloud-Edge Continuum. This work contributes to the development of decentralized application placement strategies, the integration of GNN embeddings, and collaborative MARL systems, providing a foundation for efficient, adaptive and scalable resource management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15802v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lanpei Li, Jack Bell, Massimo Coppola, Vincenzo Lomonaco</dc:creator>
    </item>
    <item>
      <title>Aging-aware CPU Core Management for Embodied Carbon Amortization in Cloud LLM Inference</title>
      <link>https://arxiv.org/abs/2501.15829</link>
      <description>arXiv:2501.15829v1 Announce Type: new 
Abstract: Broad adoption of Large Language Models (LLM) demands rapid expansions of cloud LLM inference clusters, leading to accumulation of embodied carbon$-$the emissions from manufacturing and supplying IT assets$-$that mostly concentrate on inference server CPU. This paper delves into the challenges of sustainable growth of cloud LLM inference, emphasizing extended amortization of CPU embodied over an increased lifespan. Given the reliability risks of silicon aging, we propose an aging-aware CPU core management technique to delay CPU aging effects, allowing the cluster operator to safely increase CPU life. Our technique exploits CPU underutilization patterns that we uncover in cloud LLM inference by halting aging in unused cores and even-outing aging in active cores via selective deep idling and aging-aware inference task allocation. Through extensive simulations using real-world Azure inference traces and an extended LLM cluster simulator from Microsoft, we show superior performance of our technique over existing methods with an estimated 37.67\% reduction in yearly embodied carbon emissions through p99 performance of managing CPU aging effects, a 77\% reduction in CPU underutilization, and less than 10\% impact to the inference service quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15829v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tharindu B. Hewage, Shashikant Ilager, Maria Rodriguez Read, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Snowman for partial synchrony</title>
      <link>https://arxiv.org/abs/2501.15904</link>
      <description>arXiv:2501.15904v1 Announce Type: new 
Abstract: Snowman is the consensus protocol run by blockchains on Avalanche. Recent work established a rigorous proof of probabilistic consistency for Snowman in the \emph{synchronous} setting, under the simplifying assumption that correct processes execute sampling rounds in `lockstep'. In this paper, we describe a modification of the protocol that ensures consistency in the \emph{partially synchronous} setting, and when correct processes carry out successive sampling rounds at their own speed, with the time between sampling rounds determined by local message delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15904v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron Buchwald, Stephen Buttolph, Andrew Lewis-Pye, Kevin Sekniqi</dc:creator>
    </item>
    <item>
      <title>Share a Tiny Space of Your Freezer to Preserve Seed Diversity</title>
      <link>https://arxiv.org/abs/2501.15962</link>
      <description>arXiv:2501.15962v1 Announce Type: new 
Abstract: The Food and Agriculture Organization (FAO), estimates that 75% of crop diversity was lost since the 1900s. That lack of diversity presents a severe risk to the security of global food systems. Without seed diversity, it is difficult for plants to adapt to pests, diseases, and changing climate conditions. Genebanks, such as the Svalbard Global Seed Vault, are valuable initiatives to preserve seed diversity in a single secure and safe place. However, according to our analysis of the data available in the Seed Portal, the redundancy for some species might be limited, posing a potential threat to their future availability. Interestingly, the conditions to properly store seeds in genebanks, are the ones available in the freezers of our homes. This paper lays out a vision for Distributed Seed Storage relying on a peer-to-peer infrastructure of domestic freezers to increase the overall availability of seeds. We present a Proof-of-Concept focused on monitoring the proper seed storing conditions and incentive user participation through a Blockchain lottery. The PoC proves the feasibility of the proposed approach and outlines the main technical issues that still need to be efficiently solved to realize a fully-fledged solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15962v1</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Vitaletti</dc:creator>
    </item>
    <item>
      <title>Static Batching of Irregular Workloads on GPUs: Framework and Application to Efficient MoE Model Inference</title>
      <link>https://arxiv.org/abs/2501.16103</link>
      <description>arXiv:2501.16103v1 Announce Type: new 
Abstract: It has long been a problem to arrange and execute irregular workloads on massively parallel devices. We propose a general framework for statically batching irregular workloads into a single kernel with a runtime task mapping mechanism on GPUs. We further apply this framework to Mixture-of-Experts (MoE) model inference and implement an optimized and efficient CUDA kernel. Our MoE kernel achieves up to 91% of the peak Tensor Core throughput on NVIDIA H800 GPU and 95% on NVIDIA H20 GPU.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16103v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinghan Li, Yifei Li, Jiejing Zhang, Bujiao Chen, Xiaotong Chen, Lian Duan, Yejun Jin, Zheng Li, Xuanyu Liu, Haoyu Wang, Wente Wang, Yajie Wang, Jiacheng Yang, Peiyang Zhang, Laiwen Zheng, Wenyuan Yu</dc:creator>
    </item>
    <item>
      <title>EPOCH: Enabling Preemption Operation for Context Saving in Heterogeneous FPGA Systems</title>
      <link>https://arxiv.org/abs/2501.16205</link>
      <description>arXiv:2501.16205v1 Announce Type: new 
Abstract: FPGAs are increasingly used in multi-tenant cloud environments to offload compute-intensive tasks from the main CPU. The operating system (OS) plays a vital role in identifying tasks suitable for offloading and coordinating between the CPU and FPGA for seamless task execution. The OS leverages preemption to manage CPU efficiently and balance CPU time; however, preempting tasks running on FPGAs without context loss remains challenging. Despite growing reliance on FPGAs, vendors have yet to deliver a solution that fully preserves and restores task context.
  This paper presents EPOCH, the first out-of-the-box framework to seamlessly preserve the state of tasks running on multi-tenant cloud FPGAs. EPOCH enables interrupting a tenant's execution at any arbitrary clock cycle, capturing its state, and saving this 'state snapshot' in off-chip memory with fine-grain granularity. Subsequently, when task resumption is required, EPOCH can resume execution from the saved 'state snapshot', eliminating the need to restart the task from scratch. EPOCH automates intricate processes, shields users from complexities, and synchronizes all underlying logic in a common clock domain, mitigating timing violations and ensuring seamless handling of interruptions.
  EPOCH proficiently captures the state of fundamental FPGA elements, such as look-up tables, flip-flops, block--RAMs, and digital signal processing units. On real hardware, ZynQ-XC7Z020 SoC, the proposed solution achieves context save and restore operations per frame in 62.2us and 67.4us, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16205v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arsalan Ali Malik, Emre Karabulut, Aydin Aysu</dc:creator>
    </item>
    <item>
      <title>SP-IMPact: A Framework for Static Partitioning Interference Mitigation and Performance Analysis</title>
      <link>https://arxiv.org/abs/2501.16245</link>
      <description>arXiv:2501.16245v1 Announce Type: new 
Abstract: Modern embedded systems are evolving toward complex, heterogeneous architectures to accommodate increasingly demanding applications. Driven by SWAP-C constraints, this shift has led to consolidating multiple systems onto single hardware platforms. Static Partitioning Hypervisors offer a promising solution to partition hardware resources and provide spatial isolation between critical workloads. However, shared resources like the Last-Level Cache and system bus can introduce temporal interference between virtual machines (VMs), negatively impacting performance and predictability. Over the past decade, academia and industry have developed interference mitigation techniques, such as cache partitioning and memory bandwidth reservation. However, configuring these techniques is complex and time-consuming. Cache partitioning requires balancing cache sections across VMs, while memory bandwidth reservation needs tuning bandwidth budgets and periods. Testing all configurations is impractical and often leads to suboptimal results. Moreover, understanding how these techniques interact is limited, as their combined use can produce compounded or conflicting effects on performance. Static analysis tools estimating worst-case execution times offer guidance for configuring mitigation techniques but often fail to capture the complexity of modern multi-core systems. They typically focus on limited shared resources while neglecting others, such as IOMMUs and interrupt controllers. To address these challenges, we present SP-IMPact, an open-source framework for analyzing and guiding interference mitigation configurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth reservation, while evaluating their interactions and cumulative impact. By providing insights on real hardware, SP-IMPact helps optimize configurations for mixed-criticality systems, ensuring performance and predictability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16245v1</guid>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diogo Costa, Gon\c{c}alo Moreira, Afonso Oliveira, Jos\'e Martins, Sandro Pinto</dc:creator>
    </item>
    <item>
      <title>RadiK: Scalable and Optimized GPU-Parallel Radix Top-K Selection</title>
      <link>https://arxiv.org/abs/2501.14336</link>
      <description>arXiv:2501.14336v1 Announce Type: cross 
Abstract: Top-k selection, which identifies the largest or smallest k elements from a data set, is a fundamental operation in data-intensive domains such as databases and deep learning, so its scalability and efficiency are critical for these high-performance systems. However, previous studies on its efficient GPU implementation are mostly merge-based and rely heavily on the fast but size-limited on-chip memory, thereby limiting the scalability with a restricted upper bound on k. This work introduces a scalable and optimized GPU-parallel radix top-k selection that supports significantly larger k values than existing methods without compromising efficiency, regardless of input length and batch size. Our method incorporates a novel optimization framework tailored for high memory bandwidth and resource utilization, achieving up to 2.5x speedup over the prior art for non-batch queries and up to 4.8x speedup for batch queries. In addition, we propose an adaptive scaling technique that strengthens the robustness, which further provides up to 2.7x speedup on highly adversarial input distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14336v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3650200.3656596</arxiv:DOI>
      <arxiv:journal_reference>ICS '24: Proceedings of the 38th ACM International Conference on Supercomputing, 2024, Pages 537-548</arxiv:journal_reference>
      <dc:creator>Yifei Li, Bole Zhou, Jiejing Zhang, Xuechao Wei, Yinghan Li, Yingda Chen</dc:creator>
    </item>
    <item>
      <title>Two-sorted algebraic decompositions of Brookes's shared-state denotational semantics</title>
      <link>https://arxiv.org/abs/2501.15104</link>
      <description>arXiv:2501.15104v1 Announce Type: cross 
Abstract: We use a two sorted equational theory of algebraic effects to model concurrent shared state with preemptive interleaving, recovering Brookes's seminal 1996 trace-based model precisely. The decomposition allows us to analyse Brookes's model algebraically in terms of separate but interacting components. The multiple sorts partition terms into layers. We use two sorts: a "hold" sort for layers that disallow interleaving of environment memory accesses, analogous to holding a global lock on the memory; and a "cede" sort for the opposite. The algebraic signature comprises of independent interlocking components: two new operators that switch between these sorts, delimiting the atomic layers, thought of as acquiring and releasing the global lock; non-deterministic choice; and state-accessing operators. The axioms similarly divide cleanly: the delimiters behave as a closure pair; all operators are strict, and distribute over non-empty non-deterministic choice; and non-deterministic global state obeys Plotkin and Power's presentation of global state. Our representation theorem expresses the free algebras over a two-sorted family of variables as sets of traces with suitable closure conditions. When the held sort has no variables, we recover Brookes's trace semantics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15104v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yotam Dvir, Ohad Kammar, Ori Lahav, Gordon Plotkin</dc:creator>
    </item>
    <item>
      <title>Enhancing Disaster Resilience with UAV-Assisted Edge Computing: A Reinforcement Learning Approach to Managing Heterogeneous Edge Devices</title>
      <link>https://arxiv.org/abs/2501.15305</link>
      <description>arXiv:2501.15305v1 Announce Type: cross 
Abstract: Edge sensing and computing is rapidly becoming part of intelligent infrastructure architecture leading to operational reliance on such systems in disaster or emergency situations. In such scenarios there is a high chance of power supply failure due to power grid issues, and communication system issues due to base stations losing power or being damaged by the elements, e.g., flooding, wildfires etc. Mobile edge computing in the form of unmanned aerial vehicles (UAVs) has been proposed to provide computation offloading from these devices to conserve their battery, while the use of UAVs as relay network nodes has also been investigated previously. This paper considers the use of UAVs with further constraints on power and connectivity to prolong the life of the network while also ensuring that the data is received from the edge nodes in a timely manner. Reinforcement learning is used to investigate numerous scenarios of various levels of power and communication failure. This approach is able to identify the device most likely to fail in a given scenario, thus providing priority guidance for maintenance personnel. The evacuations of a rural town and urban downtown area are also simulated to demonstrate the effectiveness of the approach at extending the life of the most critical edge devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15305v1</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Talha Azfar, Kaicong Huang, Ruimin Ke</dc:creator>
    </item>
    <item>
      <title>ReInc: Scaling Training of Dynamic Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2501.15348</link>
      <description>arXiv:2501.15348v1 Announce Type: cross 
Abstract: Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to their applicability in diverse domains such as traffic network prediction, epidemiological forecasting, and social network analysis. In this paper, we present ReInc, a system designed to enable efficient and scalable training of DGNNs on large-scale graphs. ReInc introduces key innovations that capitalize on the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural Networks (RNNs) inherent in DGNNs. By reusing intermediate results and incrementally computing aggregations across consecutive graph snapshots, ReInc significantly enhances computational efficiency. To support these optimizations, ReInc incorporates a novel two-level caching mechanism with a specialized caching policy aligned to the DGNN execution workflow. Additionally, ReInc addresses the challenges of managing structural and temporal dependencies in dynamic graphs through a new distributed training strategy. This approach eliminates communication overheads associated with accessing remote features and redistributing intermediate results. Experimental results demonstrate that ReInc achieves up to an order of magnitude speedup compared to state-of-the-art frameworks, tested across various dynamic GNN architectures and real-world graph datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15348v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingyu Guan, Saumia Singhal, Taesoo Kim, Anand Padmanabha Iyer</dc:creator>
    </item>
    <item>
      <title>FedAlign: Federated Domain Generalization with Cross-Client Feature Alignment</title>
      <link>https://arxiv.org/abs/2501.15486</link>
      <description>arXiv:2501.15486v1 Announce Type: cross 
Abstract: Federated Learning (FL) offers a decentralized paradigm for collaborative model training without direct data sharing, yet it poses unique challenges for Domain Generalization (DG), including strict privacy constraints, non-i.i.d. local data, and limited domain diversity. We introduce FedAlign, a lightweight, privacy-preserving framework designed to enhance DG in federated settings by simultaneously increasing feature diversity and promoting domain invariance. First, a cross-client feature extension module broadens local domain representations through domain-invariant feature perturbation and selective cross-client feature transfer, allowing each client to safely access a richer domain space. Second, a dual-stage alignment module refines global feature learning by aligning both feature embeddings and predictions across clients, thereby distilling robust, domain-invariant features. By integrating these modules, our method achieves superior generalization to unseen domains while maintaining data privacy and operating with minimal computational and communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15486v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunny Gupta, Vinay Sutar, Varunav Singh, Amit Sethi</dc:creator>
    </item>
    <item>
      <title>Harnessing CUDA-Q's MPS for Tensor Network Simulations of Large-Scale Quantum Circuits</title>
      <link>https://arxiv.org/abs/2501.15939</link>
      <description>arXiv:2501.15939v1 Announce Type: cross 
Abstract: Quantum computer simulators are an indispensable tool for prototyping quantum algorithms and verifying the functioning of existing quantum computer hardware. The current largest quantum computers feature more than one thousand qubits, challenging their classical simulators. State-vector quantum simulators are challenged by the exponential increase of representable quantum states with respect to the number of qubits, making more than fifty qubits practically unfeasible. A more appealing approach for simulating quantum computers is adopting the tensor network approach, whose memory requirements fundamentally depend on the level of entanglement in the quantum circuit, and allows simulating the current largest quantum computers. This work investigates and evaluates the CUDA-Q tensor network simulators on an Nvidia Grace Hopper system, particularly the Matrix Product State (MPS) formulation. We compare the performance of the CUDA-Q state vector implementation and validate the correctness of MPS simulations. Our results highlight that tensor network-based methods provide a significant opportunity to simulate large-qubit circuits, albeit approximately. We also show that current GPU-accelerated computation cannot fully utilize GPU efficiently in the case of MPS simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15939v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabin Schieffer, Stefano Markidis, Ivy Peng</dc:creator>
    </item>
    <item>
      <title>Brain-Inspired Decentralized Satellite Learning in Space Computing Power Networks</title>
      <link>https://arxiv.org/abs/2501.15995</link>
      <description>arXiv:2501.15995v1 Announce Type: cross 
Abstract: Satellite networks are able to collect massive space information with advanced remote sensing technologies, which is essential for real-time applications such as natural disaster monitoring. However, traditional centralized processing by the ground server incurs a severe timeliness issue caused by the transmission bottleneck of raw data. To this end, Space Computing Power Networks (Space-CPN) emerges as a promising architecture to coordinate the computing capability of satellites and enable on board data processing. Nevertheless, due to the natural limitations of solar panels, satellite power system is difficult to meet the energy requirements for ever-increasing intelligent computation tasks of artificial neural networks. To tackle this issue, we propose to employ spiking neural networks (SNNs), which is supported by the neuromorphic computing architecture, for on-board data processing. The extreme sparsity in its computation enables a high energy efficiency. Furthermore, to achieve effective training of these on-board models, we put forward a decentralized neuromorphic learning framework, where a communication-efficient inter-plane model aggregation method is developed with the inspiration from RelaySum. We provide a theoretical analysis to characterize the convergence behavior of the proposed algorithm, which reveals a network diameter related convergence speed. We then formulate a minimum diameter spanning tree problem on the inter-plane connectivity topology and solve it to further improve the learning performance. Extensive experiments are conducted to evaluate the superiority of the proposed method over benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15995v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Yang, Ting Wang, Haibin Cai, Yuanming Shi, Chunxiao Jiang, Linling Kuang</dc:creator>
    </item>
    <item>
      <title>TOPLOC: A Locality Sensitive Hashing Scheme for Trustless Verifiable Inference</title>
      <link>https://arxiv.org/abs/2501.16007</link>
      <description>arXiv:2501.16007v1 Announce Type: cross 
Abstract: Large language models (LLMs) have proven to be very capable, but access to the best models currently rely on inference providers which introduces trust challenges -- how can we be sure that the provider is using the model configuration they claim? We propose TOPLOC, a novel method for verifiable inference that addresses this problem. TOPLOC leverages a compact locality sensitive hashing mechanism for intermediate activations which can detect unauthorized modifications to models, prompts, or precision with 100% accuracy, achieving no false positives or negatives in our empirical evaluations. Our approach is robust across diverse hardware configurations, GPU types, and algebraic reorderings, which allows for validation speeds significantly faster than the original inference. By introducing a polynomial encoding scheme, TOPLOC minimizes memory overhead of the generated commits by $1000\times$, requiring only 258 bytes of storage per 32 new tokens compared to the 262KB requirement of storing the token embeddings directly for Llama-3.1-8B-Instruct. Our method empowers users to verify LLM inference computations efficiently, fostering greater trust and transparency in open ecosystems and lays a foundation for decentralized and verifiable AI services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16007v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Min Ong, Matthew Di Ferrante, Aaron Pazdera, Ryan Garner, Sami Jaghouar, Manveer Basra, Johannes Hagemann</dc:creator>
    </item>
    <item>
      <title>Ringmaster ASGD: The First Asynchronous SGD with Optimal Time Complexity</title>
      <link>https://arxiv.org/abs/2501.16168</link>
      <description>arXiv:2501.16168v1 Announce Type: cross 
Abstract: Asynchronous Stochastic Gradient Descent (Asynchronous SGD) is a cornerstone method for parallelizing learning in distributed machine learning. However, its performance suffers under arbitrarily heterogeneous computation times across workers, leading to suboptimal time complexity and inefficiency as the number of workers scales. While several Asynchronous SGD variants have been proposed, recent findings by Tyurin &amp; Richt\'arik (NeurIPS 2023) reveal that none achieve optimal time complexity, leaving a significant gap in the literature. In this paper, we propose Ringmaster ASGD, a novel Asynchronous SGD method designed to address these limitations and tame the inherent challenges of Asynchronous SGD. We establish, through rigorous theoretical analysis, that Ringmaster ASGD achieves optimal time complexity under arbitrarily heterogeneous and dynamically fluctuating worker computation times. This makes it the first Asynchronous SGD method to meet the theoretical lower bounds for time complexity in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16168v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artavazd Maranjyan, Alexander Tyurin, Peter Richt\'arik</dc:creator>
    </item>
    <item>
      <title>Measuring Heterogeneity in Machine Learning with Distributed Energy Distance</title>
      <link>https://arxiv.org/abs/2501.16174</link>
      <description>arXiv:2501.16174v1 Announce Type: cross 
Abstract: In distributed and federated learning, heterogeneity across data sources remains a major obstacle to effective model aggregation and convergence. We focus on feature heterogeneity and introduce energy distance as a sensitive measure for quantifying distributional discrepancies. While we show that energy distance is robust for detecting data distribution shifts, its direct use in large-scale systems can be prohibitively expensive. To address this, we develop Taylor approximations that preserve key theoretical quantitative properties while reducing computational overhead. Through simulation studies, we show how accurately capturing feature discrepancies boosts convergence in distributed learning. Finally, we propose a novel application of energy distance to assign penalty weights for aligning predictions across heterogeneous nodes, ultimately enhancing coordination in federated and distributed settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16174v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengchen Fan, Baocheng Geng, Roman Shterenberg, Joseph A. Casey, Zhong Chen, Keren Li</dc:creator>
    </item>
    <item>
      <title>Uncoded Download in Lagrange-Coded Elastic Computing with Straggler Tolerance</title>
      <link>https://arxiv.org/abs/2501.16298</link>
      <description>arXiv:2501.16298v1 Announce Type: cross 
Abstract: Coded elastic computing, introduced by Yang et al. in 2018, is a technique designed to mitigate the impact of elasticity in cloud computing systems, where machines can be preempted or be added during computing rounds. This approach utilizes maximum distance separable (MDS) coding for both storage and download in matrix-matrix multiplications. The proposed scheme is unable to tolerate stragglers and has high encoding complexity and upload cost. In 2023, we addressed these limitations by employing uncoded storage and Lagrange-coded download. However, it results in a large storage size. To address the challenges of storage size and upload cost, in this paper, we focus on Lagrange-coded elastic computing based on uncoded download. We propose a new class of elastic computing schemes, using Lagrange-coded storage with uncoded download (LCSUD). Our proposed schemes address both elasticity and straggler challenges while achieving lower storage size, reduced encoding complexity, and upload cost compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16298v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xi Zhong, Samuel Lu, Joerg Kliewer, Mingyue Ji</dc:creator>
    </item>
    <item>
      <title>Automated Market Makers for Cross-chain DeFi and Sharded Blockchains</title>
      <link>https://arxiv.org/abs/2309.14290</link>
      <description>arXiv:2309.14290v3 Announce Type: replace 
Abstract: We consider Uniswap-like automated market makers, and, specifically, constant product liquidity pools, operating on blockchains. An important feature of Uniswap is the ability for a trader to carry out a sequence of asset swaps atomically, without other traders changing the prices along the way. This atomic-execution feature is not immediately available in cross-chain or sharded blockchain settings, where different liquidity pools are distributed across different chains or shards. Our contribution is a description and suggested implementation of a new functionality that might be added to individual liquidity pools, the {\em lock-swap}. The lock-swap enables a trader to get a guarantee for the price associated with a swap but only decide later whether or not to carry out the swap. Applied across several liquidity pools, it guarantees the trader assured prices for all swaps in a swap sequence and lets these prices inform the trader's decision about whether or not to carry out the sequence, thus essentially giving the trader the same benefits an atomic execution of the sequence would have provided him. However, in contrast to an atomic execution, our functionality does not prevent other traders from doing swaps during the time where the sequence is planned and possibly carried out. Nor does it prevent liquidity providers from adding or removing liquidity to and from the liquidity pool in that time period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14290v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon Michael Aanes, Jesper Balman Gravgaard, Peter Bro Miltersen, Kurt Nielsen, Mohsen Pourpouneh</dc:creator>
    </item>
    <item>
      <title>Black Hole Search by Scattered Agents in Dynamic Rings</title>
      <link>https://arxiv.org/abs/2404.15132</link>
      <description>arXiv:2404.15132v3 Announce Type: replace 
Abstract: In this paper, we address the challenge of locating a black hole within a dynamic graph using a set of scattered agents, which start from arbitrary positions in the graph. A black hole is defined as a node that silently eliminates any agent that visits it, effectively modeling network failures such as a crashed host or a destructive virus. The black hole search problem is considered solved when at least one agent survives and possesses a complete map of the graph, including the precise location of the black hole.
  Our study focuses on the scenario where the underlying graph is a dynamic 1-interval connected ring: a ring graph where, in each round, one edge may be absent. Agents communicate with other agents using movable pebbles that can be placed on nodes. In this setting, we demonstrate that three agents are sufficient to identify the black hole in $O(n^2)$ moves. Furthermore, we prove that this number of agents is optimal. Additionally, we establish that the complexity bound is tight, requiring $\Omega(n^2)$ moves for any algorithm solving the problem with three agents, even when stronger communication mechanisms, such as unlimited-size whiteboards on nodes, are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15132v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Giuseppe Antonio Di Luna, Paola Flocchini, Giuseppe Prencipe, Nicola Santoro</dc:creator>
    </item>
    <item>
      <title>Agreement Tasks in Fault-Prone Synchronous Networks of Arbitrary Structure</title>
      <link>https://arxiv.org/abs/2410.21538</link>
      <description>arXiv:2410.21538v2 Announce Type: replace 
Abstract: Consensus is arguably the most studied problem in distributed computing as a whole, and particularly in the distributed message-passing setting. In this latter framework, research on consensus has considered various hypotheses regarding the failure types, the memory constraints, the algorithmic performances (e.g., early stopping and obliviousness), etc. Surprisingly, almost all of this work assumes that messages are passed in a \emph{complete} network, i.e., each process has a direct link to every other process. A noticeable exception is the recent work of Casta\~neda et al. (Inf. Comput. 2023) who designed a generic oblivious algorithm for consensus running in $\radius(G,t)$ rounds in every graph~$G$, when up to $t$ nodes can crash by irrevocably stopping, where $t$ is smaller than the node-connectivity $\kappa$ of~$G$. Here, $\radius(G,t)$ denotes a graph parameter called the \emph{radius of~$G$ whenever up to $t$ nodes can crash}. For $t=0$, this parameter coincides with $\radius(G)$, the standard radius of a graph, and, for $G=K_n$, the running time $\radius(K_n,t)=t +1$ of the algorithm exactly matches the known round-complexity of consensus in the clique~$K_n$.
  Our main result is a proof that $\radius(G,t)$ rounds are necessary for oblivious algorithms solving consensus in $G$ when up to $t$ nodes can crash, thus validating a conjecture of Casta\~neda et al., and demonstrating that their consensus algorithm is optimal for any graph~$G$. We also extend the result of Casta\~neda et al. to two different settings: First, to the case where the number $t$ of failures is not necessarily smaller than the connectivity $\kappa$ of the considered graph; Second, to the $k$-set agreement problem for which agreement is not restricted to be on a single value as in consensus, but on up to $k$ different values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21538v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Fraigniaud, Minh Hang Nguyen, Ami Paz</dc:creator>
    </item>
    <item>
      <title>SoK: Consensus for Fair Message Ordering</title>
      <link>https://arxiv.org/abs/2411.09981</link>
      <description>arXiv:2411.09981v2 Announce Type: replace 
Abstract: Distributed ledger systems, such as blockchains, rely on consensus protocols that constantly commit messages in an agreed order for processing. In practice, message ordering within these systems is often reward-driven. This raises concerns about fairness, particularly in decentralized finance applications, where nodes can exploit transaction orders to maximize rewards (Maximal Extractable Value, MEV). This paper provides a structured review of consensus protocols that order messages with different approaches, especially focusing on the ones that promote order fairness, using methods including First-In-First-Out (FIFO), random, and blind ordering. We review the challenges and trade-offs of deriving fair message ordering in a Byzantine fault-tolerant setting, and summarize the key steps for making a fair message ordering consensus protocol. We introduce a design guideline, with which we propose a performance optimization to the state-of-the-art FIFO ordering protocol Themis. This work establishes a unified framework for accessing and enhancing fairness in distributed ledger systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09981v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuolun Li, Evangelos Pournaras</dc:creator>
    </item>
    <item>
      <title>Quality Time: Carbon-Aware Quality Adaptation for Energy-Intensive Services</title>
      <link>https://arxiv.org/abs/2411.19058</link>
      <description>arXiv:2411.19058v2 Announce Type: replace 
Abstract: The energy demand of modern cloud services, particularly those related to generative AI, is increasing at an unprecedented pace. While hyperscalers collectively fail to meet their self-imposed emission reduction targets, they face increasing pressure from environmental sustainability reporting across many jurisdictions. To date, carbon-aware computing strategies have primarily focused on batch process scheduling or geo-distributed load balancing. However, such approaches are not applicable to services that require constant availability at specific locations due to latency, privacy, data, or infrastructure constraints.
  In this paper, we explore how the carbon footprint of energy-intensive services can be reduced by adjusting the fraction of requests served by different service quality tiers. We show that adapting this quality of responses with respect to grid carbon intensity can lead to additional carbon savings beyond resource and energy efficiency. Building on this, we introduce a forecast-based multi-horizon optimization that reaches close-to-optimal carbon savings and is able to automatically adapt service quality for best-effort users to stay within an annual carbon budget. Our approach can reduce the emissions of large-scale LLM services, which we estimate at multiple 10,000 tons of CO2 annually, by up to 10%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19058v2</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Wiesner, Dennis Grinwald, Philipp Wei{\ss}, Patrick Wilhelm, Ramin Khalili, Odej Kao</dc:creator>
    </item>
    <item>
      <title>Reciprocating Locks</title>
      <link>https://arxiv.org/abs/2501.02380</link>
      <description>arXiv:2501.02380v4 Announce Type: replace 
Abstract: We present "Reciprocating Locks", a novel mutual exclusion locking algorithm, targeting cache-coherent shared memory (CC), that enjoys a number of desirable properties. The doorway arrival phase and the release operation both run in constant-time. Waiting threads use local spinning and only a single waiting element is required per thread, regardless of the number of locks a thread might hold at a given time. While our lock does not provide strict FIFO admission, it bounds bypass and has strong anti-starvation properties. The lock is compact, space efficient, and has been intentionally designed to be readily usable in real-world general purpose computing environments such as the linux kernel, pthreads, or C++. We show the lock exhibits high throughput under contention and low latency in the uncontended case. The performance of Reciprocating Locks is competitive with and often better than the best state-of-the-art scalable spin locks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02380v4</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dave Dice, Alex Kogan</dc:creator>
    </item>
    <item>
      <title>OciorABA: Improved Error-Free Asynchronous Byzantine Agreement via Partial Vector Agreement</title>
      <link>https://arxiv.org/abs/2501.11788</link>
      <description>arXiv:2501.11788v2 Announce Type: replace 
Abstract: In this work, we propose an error-free, information-theoretically secure multi-valued asynchronous Byzantine agreement (ABA) protocol, called OciorABA. This protocol achieves ABA consensus on an $\ell$-bit message with an expected communication complexity of $O(n\ell + n^3 \log q )$ bits and an expected round complexity of $O(1)$ rounds, under the optimal resilience condition $n \geq 3t + 1$ in an $n$-node network, where up to $t$ nodes may be dishonest. Here, $q$ denotes the alphabet size of the error correction code used in the protocol. In our protocol design, we introduce a new primitive: asynchronous partial vector agreement (APVA). In APVA, the distributed nodes input their vectors and aim to output a common vector, where some of the elements of those vectors may be missing or unknown. We propose an APVA protocol with an expected communication complexity of $O( n^3 \log q )$ bits and an expected round complexity of $O(1)$ rounds. This APVA protocol serves as a key building block for our OciorABA protocol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11788v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinyuan Chen</dc:creator>
    </item>
    <item>
      <title>DeepFlow: Serverless Large Language Model Serving at Scale</title>
      <link>https://arxiv.org/abs/2501.14417</link>
      <description>arXiv:2501.14417v2 Announce Type: replace 
Abstract: This paper introduces DeepFlow, a scalable and serverless AI platform designed to efficiently serve large language models (LLMs) at scale in cloud environments. DeepFlow addresses key challenges such as resource allocation, serving efficiency, and cold start latencies through four main design components. First, it uses a simple serverless abstraction called the request-job-task model, which helps manage AI workloads across post-training and model serving tasks. Second, it builds an in-house serving engine FlowServe using a microkernel-inspired design, NPU-centric execution, and SPMD-based parallelism to optimize LLM serving. The system also includes novel scheduling policies tailored for both PD-disaggregated and PD-colocated configurations. With optimizations like pre-warmed pods, DRAM pre-loading, and NPU-fork, DeepFlow can scale up to 64 instances in seconds. DeepFlow has been in production for over a year, operating on a large Ascend NPU cluster and providing industrystandard APIs for fine-tuning, agent serving, and model serving to our customers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14417v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junhao Hu, Jiang Xu, Zhixia Liu, Yulong He, Yuetao Chen, Hao Xu, Jiang Liu, Baoquan Zhang, Shining Wan, Gengyuan Dan, Zhiyu Dong, Zhihao Ren, Jie Meng, Chao He, Changhong Liu, Tao Xie, Dayun Lin, Qin Zhang, Yue Yu, Hao Feng, Xusheng Chen, Yizhou Shan</dc:creator>
    </item>
    <item>
      <title>Decentralized Sporadic Federated Learning: A Unified Algorithmic Framework with Convergence Guarantees</title>
      <link>https://arxiv.org/abs/2402.03448</link>
      <description>arXiv:2402.03448v3 Announce Type: replace-cross 
Abstract: Decentralized federated learning (DFL) captures FL settings where both (i) model updates and (ii) model aggregations are exclusively carried out by the clients without a central server. Existing DFL works have mostly focused on settings where clients conduct a fixed number of local updates between local model exchanges, overlooking heterogeneity and dynamics in communication and computation capabilities. In this work, we propose Decentralized Sporadic Federated Learning ($\texttt{DSpodFL}$), a DFL methodology built on a generalized notion of $\textit{sporadicity}$ in both local gradient and aggregation processes. $\texttt{DSpodFL}$ subsumes many existing decentralized optimization methods under a unified algorithmic framework by modeling the per-iteration (i) occurrence of gradient descent at each client and (ii) exchange of models between client pairs as arbitrary indicator random variables, thus capturing $\textit{heterogeneous and time-varying}$ computation/communication scenarios. We analytically characterize the convergence behavior of $\texttt{DSpodFL}$ for both convex and non-convex models and for both constant and diminishing learning rates, under mild assumptions on the communication graph connectivity, data heterogeneity across clients, and gradient noises. We show how our bounds recover existing results from decentralized gradient descent as special cases. Experiments demonstrate that $\texttt{DSpodFL}$ consistently achieves improved training speeds compared with baselines under various system settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03448v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahryar Zehtabi, Dong-Jun Han, Rohit Parasnis, Seyyedali Hosseinalipour, Christopher G. Brinton</dc:creator>
    </item>
    <item>
      <title>General-Purpose Multicore Architectures</title>
      <link>https://arxiv.org/abs/2408.12999</link>
      <description>arXiv:2408.12999v2 Announce Type: replace-cross 
Abstract: The first years of the 2000s led to an inflection point in computer architectures: while the number of available transistors on a chip continued to grow, crucial transistor scaling properties started to break down and result in increasing power consumption, while aggressive single-core performance optimizations were resulting in diminishing returns due to inherent limits in instruction-level parallelism. This led to the rise of multicore CPU architectures, which are now commonplace in modern computers at all scales. In this chapter, we discuss the evolution of multicore CPUs since their introduction. Starting with a historic overview of multiprocessing, we explore the basic microarchitecture of a multicore CPU, key challenges resulting from shared memory resources, operating system modifications to optimize multicore CPU support, popular metrics for multicore evaluation, and recent trends in multicore CPU design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12999v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saugata Ghose</dc:creator>
    </item>
    <item>
      <title>Joint Task Offloading and Routing in Wireless Multi-hop Networks Using Biased Backpressure Algorithm</title>
      <link>https://arxiv.org/abs/2412.15385</link>
      <description>arXiv:2412.15385v3 Announce Type: replace-cross 
Abstract: A significant challenge for computation offloading in wireless multi-hop networks is the complex interaction among traffic flows in the presence of interference. Existing approaches often ignore these key effects and/or rely on outdated queueing and channel state information. To fill these gaps, we reformulate joint offloading and routing as a routing problem on an extended graph with physical and virtual links. We adopt the state-of-the-art shortest path-biased Backpressure routing algorithm, which allows the destination and the route of a job to be dynamically adjusted at every time step based on network-wide long-term information and real-time states of local neighborhoods. In large networks, our approach achieves smaller makespan than existing approaches, such as separated Backpressure offloading and joint offloading and routing based on linear programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15385v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyuan Zhao, Jake Perazzone, Gunjan Verma, Kevin Chan, Ananthram Swami, Santiago Segarra</dc:creator>
    </item>
    <item>
      <title>FedTLU: Federated Learning with Targeted Layer Updates</title>
      <link>https://arxiv.org/abs/2412.17692</link>
      <description>arXiv:2412.17692v2 Announce Type: replace-cross 
Abstract: Federated learning (FL) addresses privacy concerns in training language models by enabling multiple clients to contribute to the training, without sending their data to others. However, non-IID (identically and independently distributed) data across clients often limits FL's performance. This issue is especially challenging during model fine-tuning, as noise due to variations in clients' data distributions can harm model convergence near stationary points. This paper proposes a targeted layer update strategy for fine-tuning in FL. Instead of randomly updating layers of the language model, as often done in practice, we use a scoring mechanism to identify and update the most critical layers, avoiding excessively noisy or even poisoned updates by freezing the parameters in other layers. We show in extensive experiments that our method improves convergence and performance in non-IID settings, offering a more efficient approach to fine-tuning federated language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17692v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jong-Ik Park, Carlee Joe-Wong</dc:creator>
    </item>
    <item>
      <title>Thunderdome: Timelock-Free Rationally-Secure Virtual Channels</title>
      <link>https://arxiv.org/abs/2501.14418</link>
      <description>arXiv:2501.14418v2 Announce Type: replace-cross 
Abstract: Payment channel networks (PCNs) offer a promising solution to address the limited transaction throughput of deployed blockchains. However, several attacks have recently been proposed that stress the vulnerability of PCNs to timelock and censoring attacks. To address such attacks, we introduce Thunderdome, the first timelock-free PCN. Instead, Thunderdome leverages the design rationale of virtual channels to extend a timelock-free payment channel primitive, thereby enabling multi-hop transactions without timelocks. Previous works either utilize timelocks or do not accommodate transactions between parties that do not share a channel.
  At its core, Thunderdome relies on a committee of non-trusted watchtowers, known as wardens, who ensure that no honest party loses funds, even when offline, during the channel closure process. We introduce tailored incentive mechanisms to ensure that all participants follow the protocol's correct execution. Besides a traditional security proof that assumes an honest majority of the committee, we conduct a formal game-theoretic analysis to demonstrate the security of Thunderdome when all participants, including wardens, act rationally. We implement a proof of concept of Thunderdome on Ethereum to validate its feasibility and evaluate its costs. Our evaluation shows that deploying Thunderdome, including opening the underlying payment channel, costs approximately \$15 (0.0089 ETH), while the worst-case cost for closing a channel is about \$7 (0.004 ETH).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14418v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeta Avarikioti, Yuheng Wang, Yuyi Wang</dc:creator>
    </item>
  </channel>
</rss>
