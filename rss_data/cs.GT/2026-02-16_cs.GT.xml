<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 16 Feb 2026 05:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Truthful Fair Division under Stochastic Valuations</title>
      <link>https://arxiv.org/abs/2602.12359</link>
      <description>arXiv:2602.12359v1 Announce Type: new 
Abstract: We study no-money mechanisms for allocating indivisible items to strategic agents with additive preferences under a stochastic model. In this model, items' values are drawn from an underlying distribution and mechanisms are evaluated with respect to this draw (e.g., in expectation, or with high probability). Motivated by worst-case impossibilities which show that truthfulness severely restricts fairness and efficiency, we ask whether truthful mechanisms continue to perform poorly on random instances.
  We first focus on dominant-strategy incentive compatible (DSIC) mechanisms. For two agents, we obtain a tight picture. Specifically, we show that there exists a distribution under which no DSIC mechanism achieves an expected welfare approximation better than $\frac{2+\sqrt{2}}{4}\approx 0.854$, and we give a DSIC mechanism that matches this bound for all distributions simultaneously. We further show that, for every distribution, there exists a DSIC mechanism that is envy-free with high probability and obtains the same welfare. A key ingredient is a new, tight connection between welfare guarantees of a family of DSIC, no-money mechanisms and i.i.d.\ prophet inequalities. This connection allows us to generalize to $n$ agents; in particular, we obtain a DSIC mechanism that achieves a $\approx 0.745$ approximation to welfare, and another DSIC mechanism achieving a $1/2$-approximation welfare that is envy-free with high probability.
  We then turn to Bayesian incentive compatibility (BIC). Under i.i.d.\ valuations, we show that BIC comes at essentially no cost: we design a prior-independent BIC mechanism that achieves a $(1-\varepsilon)$-approximation to the optimal welfare, while being envy-free with high probability. Under independent but non-identical priors, we obtain BIC mechanisms that are $(1-\varepsilon)$-approximately Pareto efficient and envy-free with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12359v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Halpern, Alexandros Psomas, Shirley Zhang</dc:creator>
    </item>
    <item>
      <title>Online Advertising with Spatial Interactions</title>
      <link>https://arxiv.org/abs/2602.12481</link>
      <description>arXiv:2602.12481v1 Announce Type: new 
Abstract: Online advertising platforms must decide how to allocate multiple ads across limited screen real estate, where each ad's effectiveness depends not only on its own placement but also on nearby ads competing for user attention. Such spatial externalities - arising from proximity, clutter, or crowding - can significantly alter welfare and revenue outcomes, yet existing auction and allocation models typically treat ad slots as independent or ordered along a single dimension.
  We introduce a new framework for spatial externalities in online advertising, in which the value of an ad depends on both its slot and the configuration of surrounding ads. We model ad slots as points in a metric space, and model an advertiser's value as a function of both their bid and a discount factor determined by the configuration of other displayed ads. Within this framework, we analyze two natural models. For the Nearest-Neighbor model, where the value suppression depends only on the closest neighboring ad, we present a polynomial-time algorithm that achieves a constant approximation for the general case. We show that the allocation rule is monotone and can be implemented as a truthful mechanism. For a structured setting of 2D Euclidean space, we provide a PTAS. In contrast, for the Product-Distance model, where interference is aggregated multiplicatively across all neighbors, we establish a strong (and nearly-tight) hardness of approximation - no polynomial-time algorithm can achieve any polynomial-factor approximation unless P=NP, via a reduction from Max-Independent-Set.
  Our results provide a foundation for reasoning about spatial externalities in ad allocation and for designing efficient, truthful mechanisms under such interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12481v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gagan Aggarwal, Yifan Wang, Mingfei Zhao</dc:creator>
    </item>
    <item>
      <title>Opinion dynamics and mutual influence with LLM agents through dialog simulation</title>
      <link>https://arxiv.org/abs/2602.12583</link>
      <description>arXiv:2602.12583v1 Announce Type: new 
Abstract: A fundamental challenge in opinion dynamics research is the scarcity of real-world longitudinal opinion data, which complicates the validation of theoretical models. To address this, we propose a novel simulation framework using large language model (LLM) agents in structured multi-round dialogs. Each agent's dialog history is iteratively updated with its own previously stated opinions and those of others analogous to the classical DeGroot model. Furthermore, by retaining each agent's initial opinion throughout the dialog, we simulate anchoring effects consistent with the Friedkin-Johnsen model of opinion dynamics. Our framework thus bridges classical opinion dynamics models and modern multi-agent LLM systems, providing a scalable tool for simulating and analyzing opinion formation when real-world data is limited or inaccessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12583v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yulong He, Dutao Zhang, Sergey Kovalchuk, Pengyi Li, Artem Sedakov</dc:creator>
    </item>
    <item>
      <title>Feature-based Uncertainty Model for School Choice</title>
      <link>https://arxiv.org/abs/2602.12615</link>
      <description>arXiv:2602.12615v1 Announce Type: new 
Abstract: In this work, we consider a school choice scenario where a student does not exactly know which college is better for her. Although it is hard for a student to obtain an exact preference, she can usually compare specific features of colleges, such as reputation, location, and campus facilities. Motivated by this, we propose a feature-based uncertainty model for school choice where a student's preference is based on a linear combination of her utilities over different features, and the coefficients of the combination are treated as random variables. Our main goal is to achieve a higher probability of stability (ProS) and incentive compatibility (IC) for students. Unfortunately, these two goals are incompatible in general. We show that a student-proposing deferred acceptance (DA) that prioritizes colleges with higher expected ranking can achieve a worst-case approximation ratio of $(1/n)^n$ on ProS, while a DA with a carefully defined iterated comparison vector can guarantee the strongest achievable form of IC. Finally, we provide additional results for some specific restrictions on the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12615v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yao Zhang, Makoto Yokoo</dc:creator>
    </item>
    <item>
      <title>Decentralized Optimal Equilibrium Learning in Stochastic Games via Single-bit Feedback</title>
      <link>https://arxiv.org/abs/2602.12830</link>
      <description>arXiv:2602.12830v1 Announce Type: new 
Abstract: We study decentralized equilibrium selection in stochastic games under severe information and communication constraints. In such settings, convergence to equilibrium alone is insufficient, as stochastic games typically admit many equilibria with markedly different welfare properties. We address decentralized optimal equilibrium selection, where agents coordinate on equilibria that optimize a designer-specified social welfare objective while allowing heterogeneous tolerance to deviations from strict best responses. Agents observe only the global state trajectory and their realized rewards, and exchange a single randomized bit of feedback per agent per round. This semantic content/discontent signaling mechanism implicitly aligns decentralized learning dynamics with the global welfare objective. We develop explore-and-commit and online variants applicable to general stochastic games, accommodating heterogeneous model-based or model-free methods for solving the induced Markov decision processes, and establish explicit finite-time regret guarantees, showing logarithmic expected regret under mild conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12830v1</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seref Taha Kiremitci, Ahmed Said Donmez, Muhammed O. Sayin</dc:creator>
    </item>
    <item>
      <title>Experimentation, Biased Learning, and Conjectural Variations in Competitive Dynamic Pricing</title>
      <link>https://arxiv.org/abs/2602.12888</link>
      <description>arXiv:2602.12888v1 Announce Type: new 
Abstract: We study competitive dynamic pricing among multiple sellers, motivated by the rise of large-scale experimentation and algorithmic pricing in retail and online marketplaces. Sellers repeatedly set prices using simple learning rules and observe only their own prices and realized demand, even though demand depends on all sellers' prices and is subject to random shocks. Each seller runs two-point A/B price experiments, in the spirit of switchback-style designs, and updates a baseline price using a linear demand estimate fitted to its own data. Under certain conditions on demand, the resulting dynamics converge to a Conjectural Variations (CV) equilibrium, a classic static equilibrium notion in which each seller best responds under a conjecture that rivals' prices respond systematically to changes in its own price. Unlike standard CV models that treat conjectures as behavioral primitives, we show that these conjectures arise endogenously from the bias in demand learning induced by correlated experimentation (e.g., due to synchronized repricing schedules). This learning bias selects the long-run equilibrium, often leading to supra-competitive prices. Notably, we show that under independent experimentation, this bias vanishes and the learning dynamics converge to the standard Nash equilibrium. We provide simple sufficient conditions on demand for convergence in standard models and establish a finite-sample guarantee: up to logarithmic factors, the squared price error decays on the order of $T^{-1/2}$. Our results imply that in competitive markets, experimentation design can serve as a market design lever, selecting the equilibrium reached by practical learning algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12888v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bar Light, Wenyu Wang</dc:creator>
    </item>
    <item>
      <title>Contextual Online Bilateral Trade</title>
      <link>https://arxiv.org/abs/2602.12903</link>
      <description>arXiv:2602.12903v1 Announce Type: new 
Abstract: We study repeated bilateral trade when the valuations of the sellers and the buyers are contextual. More precisely, the agents' valuations are given by the inner product of a context vector with two unknown $d$-dimensional vectors -- one for the buyers and one for the sellers.
  At each time step $t$, the learner receives a context and posts two prices, one for the seller and one for the buyer, and the trade happens if both agents accept their price. We study two objectives for this problem, gain from trade and profit, proving no-regret with respect to a surprisingly strong benchmark: the best omniscient dynamic strategy.
  In the natural scenario where the learner observes \emph{separately} whether the agents accept their price -- the so-called \emph{two-bit} feedback -- we design algorithms that achieve $O(d\log d)$ regret for gain from trade, and $O(d \log\log T + d\log d)$ regret for profit maximization. Both results are tight, up to the $\log(d)$ factor, and implement per-step budget balance, meaning that the learner never incurs negative profit.
  In the less informative \emph{one-bit} feedback model, the learner only observes whether a trade happens or not. For this scenario, we show that the tight two-bit regret regimes are still attainable, at the cost of allowing the learner to possibly incur a small negative profit of order $O(d\log d)$, which is notably independent of the time horizon. As a final set of results, we investigate the combination of one-bit feedback and per-step budget balance. There, we design an algorithm for gain from trade that suffers regret independent of the time horizon, but \emph{exponential} in the dimension $d$. For profit maximization, we maintain this exponential dependence on the dimension, which gets multiplied by a $\log T$ factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12903v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Cosson, Federico Fusco, Anupam Gupta, Stefano Leonardi, Renato Paes Leme, Matteo Russo</dc:creator>
    </item>
    <item>
      <title>Nonparametric Contextual Online Bilateral Trade</title>
      <link>https://arxiv.org/abs/2602.12904</link>
      <description>arXiv:2602.12904v1 Announce Type: new 
Abstract: We study the problem of contextual online bilateral trade. At each round, the learner faces a seller-buyer pair and must propose a trade price without observing their private valuations for the item being sold. The goal of the learner is to post prices to facilitate trades between the two parties. Before posting a price, the learner observes a $d$-dimensional context vector that influences the agent's valuations. Prior work in the contextual setting has focused on linear models. In this work, we tackle a general nonparametric setting in which the buyer's and seller's valuations behave according to arbitrary Lipschitz functions of the context. We design an algorithm that leverages contextual information through a hierarchical tree construction and guarantees regret $\widetilde{O}(T^{{(d-1)}/d})$. Remarkably, our algorithm operates under two stringent features of the setting: (1) one-bit feedback, where the learner only observes whether a trade occurred or not, and (2) strong budget balance, where the learner cannot subsidize or profit from the market participants. We further provide a matching lower bound in the full-feedback setting, demonstrating the tightness of our regret bound.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12904v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emanuele Coccia, Martino Bernasconi, Andrea Celli</dc:creator>
    </item>
    <item>
      <title>Solving Qualitative Multi-Objective Stochastic Games</title>
      <link>https://arxiv.org/abs/2602.12927</link>
      <description>arXiv:2602.12927v1 Announce Type: new 
Abstract: Many problems in compositional synthesis and verification of multi-agent systems -- such as rational verification and assume-guarantee verification in probabilistic systems -- reduce to reasoning about two-player multi-objective stochastic games. This motivates us to study the problem of characterizing the complexity and memory requirements for two-player stochastic games with Boolean combinations of qualitative reachability and safety objectives. Reachability objectives require that a given set of states is reached; safety requires that a given set is invariant. A qualitative winning condition asks that an objective is satisfied almost surely (AS) or (in negated form) with non-zero (NZ) probability.
  We study the determinacy and complexity landscape of the problem. We show that games with conjunctions of AS and NZ reachability and safety objectives are determined, and determining the winner is PSPACE-complete. The same holds for positive boolean combinations of AS reachability and safety, as well as for negations thereof. On the other hand, games with full Boolean combinations of qualitative objectives are not determined, and are NEXPTIME-hard. Our hardness results show a connection between stochastic games and logics with partially-ordered quantification. Our results shed light on the relationship between determinacy and complexity, and extend the complexity landscape for stochastic games in the multi-objective setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12927v1</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moritz Graf, Anthony Lin, Rupak Majumdar</dc:creator>
    </item>
    <item>
      <title>GT-HarmBench: Benchmarking AI Safety Risks Through the Lens of Game Theory</title>
      <link>https://arxiv.org/abs/2602.12316</link>
      <description>arXiv:2602.12316v1 Announce Type: cross 
Abstract: Frontier AI systems are increasingly capable and deployed in high-stakes multi-agent environments. However, existing AI safety benchmarks largely evaluate single agents, leaving multi-agent risks such as coordination failure and conflict poorly understood. We introduce GT-HarmBench, a benchmark of 2,009 high-stakes scenarios spanning game-theoretic structures such as the Prisoner's Dilemma, Stag Hunt and Chicken. Scenarios are drawn from realistic AI risk contexts in the MIT AI Risk Repository. Across 15 frontier models, agents choose socially beneficial actions in only 62% of cases, frequently leading to harmful outcomes. We measure sensitivity to game-theoretic prompt framing and ordering, and analyze reasoning patterns driving failures. We further show that game-theoretic interventions improve socially beneficial outcomes by up to 18%. Our results highlight substantial reliability gaps and provide a broad standardized testbed for studying alignment in multi-agent environments. The benchmark and code are available at https://github.com/causalNLP/gt-harmbench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12316v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pepijn Cobben, Xuanqiang Angelo Huang, Thao Amelia Pham, Isabel Dahlgren, Terry Jingchen Zhang, Zhijing Jin</dc:creator>
    </item>
    <item>
      <title>Provably Convergent Actor-Critic in Risk-averse MARL</title>
      <link>https://arxiv.org/abs/2602.12386</link>
      <description>arXiv:2602.12386v1 Announce Type: cross 
Abstract: Learning stationary policies in infinite-horizon general-sum Markov games (MGs) remains a fundamental open problem in Multi-Agent Reinforcement Learning (MARL). While stationary strategies are preferred for their practicality, computing stationary forms of classic game-theoretic equilibria is computationally intractable -- a stark contrast to the comparative ease of solving single-agent RL or zero-sum games. To bridge this gap, we study Risk-averse Quantal response Equilibria (RQE), a solution concept rooted in behavioral game theory that incorporates risk aversion and bounded rationality. We demonstrate that RQE possesses strong regularity conditions that make it uniquely amenable to learning in MGs. We propose a novel two-timescale Actor-Critic algorithm characterized by a fast-timescale actor and a slow-timescale critic. Leveraging the regularity of RQE, we prove that this approach achieves global convergence with finite-sample guarantees. We empirically validate our algorithm in several environments to demonstrate superior convergence properties compared to risk-neutral baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12386v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yizhou Zhang, Eric Mazumdar</dc:creator>
    </item>
    <item>
      <title>Local Coordination and the Geometry of Social Networks</title>
      <link>https://arxiv.org/abs/2602.12571</link>
      <description>arXiv:2602.12571v1 Announce Type: cross 
Abstract: We study agents playing a pure coordination game on a large social network. Agents are restricted to coordinate locally, without access to a global communication device, and so different regions of the network will converge to different actions, precluding perfect coordination. We show that the extent of this inefficiency depends on the network geometry: on some networks, near-perfect efficiency is achievable, while on others welfare is strictly bounded away from the optimum. We provide a geometric condition on the network structure that characterizes when near-efficiency is attainable. On networks in which it is unattainable, our results more generally preclude high correlations between outcomes in a large spectrum of dynamic games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12571v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>math.PR</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Hutchcroft, Olga Rospuskova, Omer Tamuz</dc:creator>
    </item>
    <item>
      <title>Additively Competitive Secretaries</title>
      <link>https://arxiv.org/abs/2602.12632</link>
      <description>arXiv:2602.12632v1 Announce Type: cross 
Abstract: In the secretary problem, a set of secretary candidates arrive in a uniformly random order and reveal their values one by one. A company, who can only hire one candidate and hopes to maximize the expected value of its hire, needs to make irrevocable online decisions about whether to hire the current candidate. The classical framework of evaluating a policy is to compute its worst-case competitive ratio against the optimal solution in hindsight, and there the best policy -- the ``$1/e$ law'' -- has a competitive ratio of $1/e$.
  We propose an alternative evaluation framework through the lens of regret -- the worst-case additive difference between the optimal hindsight solution and the expected performance of the policy, assuming that each value is normalized between $0$ and $1$. The $1/e$ law for the classical framework has a regret of $1 - 1/e \approx 0.632$; by contrast, we show that the class of ``pricing curves'' algorithms can guarantee a regret of at most $1/4 = 0.25$ (which is tight within the class), and the class of ``best-only pricing curves'' algorithms can guarantee a regret of at most $0.190$ (with a lower bound of $0.171$). In addition, we show that in general, no policy can give a regret guarantee better than $0.152$. Finally, we discuss other objectives in our regret-minimization framework, such as selecting the top-$k$ candidates for $k &gt; 1$, or maximizing revenue during the selection process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12632v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Mahdian, Jieming Mao, Enze Sun, Kangning Wang, Yifan Wang</dc:creator>
    </item>
    <item>
      <title>Revenue Variance Minimization: Beyond First Price Auctions</title>
      <link>https://arxiv.org/abs/2403.04856</link>
      <description>arXiv:2403.04856v4 Announce Type: replace 
Abstract: We study revenue variance in the sale of $k$ homogeneous items to risk-neutral, unit-demand bidders with independent private values. Although the Revenue Equivalence Theorem implies that standard auctions generate the same expected revenue, the distribution of revenue differs across mechanisms. Prior work shows that, in single-item environments with ex-post individual rationality (IR), the first-price auction minimizes revenue variance. We show that this result is fragile. Under interim IR, the optimality of the first-price auction breaks down in asymmetric single-item settings, and we characterize the variance-minimizing mechanisms for any implementable allocation rule in this environment. In multi-item symmetric regular environments with interim IR, we construct a mechanism that implements the efficient allocation and guarantees constant revenue while maintaining non-negative payments. Under ex-post IR, we show that revenue variance can be reduced relative to winner-pays-bid formats by introducing negative correlations in payments. Nevertheless, we show that the variance ranking between the winner-pays-bid auction and the uniform $(k+1)$-st price auction is maintained in multi-unit settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04856v4</guid>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marek Bojko, Preston McAfee, Renato Paes Leme, Balasubramanian Sivan, Sergei Vassilvitskii</dc:creator>
    </item>
    <item>
      <title>Can Users Fix Algorithms? A Game-Theoretic Analysis of Collective Content Amplification in Recommender Systems</title>
      <link>https://arxiv.org/abs/2506.04525</link>
      <description>arXiv:2506.04525v3 Announce Type: replace 
Abstract: Users of social media platforms based on recommendation systems (e.g. TikTok, X, YouTube) strategically interact with platform content to influence future recommendations. On some such platforms, users have been documented to form large-scale grassroots movements encouraging others to purposefully interact with algorithmically suppressed content in order to counteractively ``boost'' its recommendation. However, despite widespread documentation of this phenomenon, there is little theoretical work analyzing its impact on the platform or users themselves. We study a game between users and a RecSys, where users (potentially strategically) interact with the content available to them, and the RecSys -- limited by preference learning ability -- provides each user her approximately most-preferred item. We compare recommendations and social welfare when users interact with content according to their personal interests and when a collective of users intentionally interacts with an otherwise suppressed item. We provide sufficient conditions to ensure a pareto improvement in recommendations and strict increases in user social welfare under collective interaction, and provide a robust algorithm to find an effective collective strategy. Interestingly, despite the intended algorithmic protest of these movements, we show that for commonly assumed recommender utility functions, effective collective strategies also improve the utility of the RecSys. Our theoretical analysis is complemented by empirical results of effective collective interaction strategies on the GoodReads dataset and an online survey on how real-world users attempt to influence others' recommendations on RecSys platforms. Our findings examine how and when platforms' recommendation algorithms may incentivize users to collectivize and interact with content in algorithmic protest as well as what this collectivization means for the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04525v3</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.SI</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ekaterina Fedorova, Madeline Kitch, Chara Podimata</dc:creator>
    </item>
    <item>
      <title>Tight Efficiency Bounds for the Probabilistic Serial and Related Mechanisms</title>
      <link>https://arxiv.org/abs/2507.03359</link>
      <description>arXiv:2507.03359v2 Announce Type: replace 
Abstract: The Probabilistic Serial (PS) mechanism -- also known as the simultaneous eating algorithm -- is a canonical solution for the random assignment problem under ordinal preferences. It guarantees envy-freeness and ordinal efficiency in the resulting random assignment. However, under cardinal preferences, its efficiency may degrade significantly: it is known that PS may yield allocations that are $\Omega(\ln{n})$-worse than Pareto optimal, but whether this bound is tight remained an open question.
  Our first result resolves this question by proving that the PS mechanism guarantees $(\ln n+1)$-approximate Pareto efficiency under cardinal preferences. The key part of our analysis shows that PS achieves a logarithmic $(\ln n + 1)$-approximation to the maximum Nash welfare, in stark contrast to the $O(\sqrt{n})$ loss that can arise in utilitarian social welfare. Our results also extend to the more general submodular setting introduced by Fujishige, Sano, and Zhan (ACM TEAC 2018). In addition, we present a polynomial-time algorithm that computes an allocation which is envy-free and $e^{1/e}$-approximately Pareto-efficient, answering an open question posed by Tr\"obst and Vazirani (EC 2024).
  The PS mechanism also applies to the allocation of chores instead of goods. We prove that it guarantees an $n$-approximately Pareto-efficient allocation in this setting, and that this bound is asymptotically tight. This result provides the first known approximation guarantee for computing a fair and efficient allocation in the random assignment problem with chores under cardinal preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03359v2</guid>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jugal Garg, Yixin Tao, L\'aszl\'o A. V\'egh</dc:creator>
    </item>
    <item>
      <title>Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation</title>
      <link>https://arxiv.org/abs/2602.12089</link>
      <description>arXiv:2602.12089v2 Announce Type: replace 
Abstract: As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants access to a single LLM assistance modality: proactive recommendations from an Advisor, reactive feedback from a Coach, or autonomous execution by a Delegate; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the Advisor modality, participants achieve the highest mean individual gains with the Delegate, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in access-to-delegate treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the Delegate agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12089v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 16 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehang Zhu, Nithum Thain, Vivian Tsai, James Wexler, Crystal Qian</dc:creator>
    </item>
  </channel>
</rss>
