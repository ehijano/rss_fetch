<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:13:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>How Consistent Are Humans When Grading Programming Assignments?</title>
      <link>https://arxiv.org/abs/2409.12967</link>
      <description>arXiv:2409.12967v1 Announce Type: new 
Abstract: Providing consistent summative assessment to students is important, as the grades they are awarded affect their progression through university and future career prospects. While small cohorts are typically assessed by a single assessor, such as the class leader, larger cohorts are often assessed by multiple assessors, which increases the risk of inconsistent grading. To investigate the consistency of human grading of programming assignments, we asked 28 participants to each grade 40 CS1 introductory Java assignments, providing grades and feedback for correctness, code elegance, readability and documentation; the 40 assignments were split into two batches of 20. In the second batch of 20, we duplicated one assignment from the first to analyse the internal consistency of individual assessors. We measured the inter-rater reliability of the groups using Krippendorf's $\alpha$ -- an $\alpha &gt; 0.667$ is recommended to make tentative conclusions based on the rating. Our groups were inconsistent, with an average $\alpha = 0.2$ when grading correctness and an average $\alpha &lt; 0.1$ for code elegance, readability and documentation. To measure the individual consistency of graders, we measured the distance between the grades they awarded for the duplicated assignment in batch one and batch two. Only one participant of the 22 who didn't notice that the assignment was a duplicate was awarded the same grade for correctness, code elegance, readability and documentation. The average grade difference was 1.79 for correctness and less than 1.6 for code elegance, readability and documentation. Our results show that human graders in our study can not agree on the grade to give a piece of student work and are often individually inconsistent, suggesting that the idea of a ``gold standard'' of human grading might be flawed, and highlights that a shared rubric alone is not enough to ensure consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12967v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcus Messer, Neil C. C. Brown, Michael K\"olling, Miaojing Shi</dc:creator>
    </item>
    <item>
      <title>Excel: Automated Ledger or Analytics IDE?</title>
      <link>https://arxiv.org/abs/2409.12976</link>
      <description>arXiv:2409.12976v1 Announce Type: new 
Abstract: Since the inception of VisiCalc over four decades ago, spreadsheets have undergone a gradual transformation, evolving from simple ledger automation tools to the current state of Excel, which can be described as an Integrated Development Environment (IDE) for analytics. The slow evolution of Excel from an automation tool for ledgers to an IDE for analytics explains why many people have not noticed that Excel includes a fully functional database, an OLAP Engine, multiple statistical programming languages, multiple third-party software libraries, dynamic charts, and real time data connectors. The simplicity of accessing these multiple tools is a low-code framework controlled from the Excel tool that is effectively an IDE. Once we acknowledge Excel's shift from a desk top application to an IDE for analytics, the importance of establishing a comprehensive risk framework for managing this distinctive development environment becomes clear. In this paper we will explain how the current risk framework for spreadsheets needs to be expanded to manage the growing risks of using Excel as an IDE for analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12976v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the EuSpRIG 2024 Conference 'Spreadsheet Productivity &amp; Risks' ISBN : 978-1-905404-59-9</arxiv:journal_reference>
      <dc:creator>Andrew Kumiega</dc:creator>
    </item>
    <item>
      <title>New world order, globalism and QAnon communities on Brazilian Telegram: how conspiracism opens doors to more harmful groups</title>
      <link>https://arxiv.org/abs/2409.12983</link>
      <description>arXiv:2409.12983v1 Announce Type: new 
Abstract: Conspiracy theories involving the New World Order (NWO), Globalism, and QAnon have become central to discussions on Brazilian Telegram, especially during global crises such as the COVID-19 pandemic. Therefore, this study aims to address the research question: how are Brazilian conspiracy theory communities on new world order, globalism and QAnon topics characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies whose main objective is to understand and characterize Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on arXiv at Cornell University, applying a mirrored method across the seven studies, changing only the thematic object of analysis and providing investigation replicability, including with proprietary and authored codes, adding to the culture of free and open-source software. Regarding the main findings of this study, the following were observed: NWO and Globalism have become central catalysts for the dissemination of conspiracy theories; QAnon acts as a hub narrative that connects NWO and Globalism; During crises, mentions of NWO have grown exponentially, reflecting distrust in institutions; NWO and Globalism attract followers of other conspiracy theories, such as anti-vaccines, serving as the main gatekeeper of the entire conspiracy theory network; Religious narratives are often used to legitimize NWO, reinforcing ideological cohesion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12983v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>Large Language Model-Enhanced Interactive Agent for Public Education on Newborn Auricular Deformities</title>
      <link>https://arxiv.org/abs/2409.12984</link>
      <description>arXiv:2409.12984v2 Announce Type: new 
Abstract: Auricular deformities are quite common in newborns with potential long-term negative effects of mental and even hearing problems.Early diagnosis and subsequent treatment are critical for the illness; yet they are missing most of the time due to lack of knowledge among parents. With the help of large language model of Ernie of Baidu Inc., we derive a realization of interactive agent. Firstly, it is intelligent enough to detect which type of auricular deformity corresponding to uploaded images, which is accomplished by PaddleDetection, with precision rate 75\%. Secondly, in terms of popularizing the knowledge of auricular deformities, the agent can give professional suggestions of the illness to parents. The above two effects are evaluated via tests on volunteers with control groups in the paper. The agent can reach parents with newborns as well as their pediatrician remotely via Internet in vast, rural areas with quality medical diagnosis capabilities and professional query-answering functions, which is good news for newborn auricular deformity and other illness that requires early intervention for better treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12984v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuyue Wang, Liujie Ren, Tianyao Zhou, Lili Chen, Tianyu Zhang, Yaoyao Fu, Shuo Wang</dc:creator>
    </item>
    <item>
      <title>Protecting Africa's Future: Cybersecurity Strategies for Child Safety, Learning, and Skill Acquisition in Tanzania</title>
      <link>https://arxiv.org/abs/2409.13159</link>
      <description>arXiv:2409.13159v1 Announce Type: new 
Abstract: Today, children across Africa are at a growing risk from the Internet. Dangers include harmful content, violence, exploitation, abuse, and neglect. All these have increased due to increased mobile and Internet technology use, which not only places their safety at risk but also affects their ability to learn essential skills for their future. This paper provides an overview of the unique challenges faced by third-world African countries in ensuring the online safety of children while also supporting their developmental needs. It highlights effective practices and policies adopted by other nations to safeguard children from online threats and enhance their digital literacy. We are focusing on sharing the best practices and policies other countries have used to protect children from abuse and help them succeed in the digital world. The study emphasizes the online safety strategies, legal frameworks, and recommendations specific to the United Republic of Tanzania, along with the significance of international collaborations with organizations like UNICEF and the UN. The goal is to provide African policymakers, educators, and cybersecurity professionals with practical guidance and recommendations to strengthen child online safety initiatives both within and beyond the continent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13159v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezekia Gilliard, Abdul Maziko, Gideon Rwechungura, Ahmed Abubakar Aliyu, Erasto Kayumbe</dc:creator>
    </item>
    <item>
      <title>BoilerTAI: A Platform for Enhancing Instruction Using Generative AI in Educational Forums</title>
      <link>https://arxiv.org/abs/2409.13196</link>
      <description>arXiv:2409.13196v1 Announce Type: new 
Abstract: Contribution: This Full paper in the Research Category track describes a practical, scalable platform that seamlessly integrates Generative AI (GenAI) with online educational forums, offering a novel approach to augment the instructional capabilities of staff. The platform empowers instructional staff to efficiently manage, refine, and approve responses by facilitating interaction between student posts and a Large Language Model (LLM). This contribution enhances the efficiency and effectiveness of instructional support and significantly improves the quality and speed of responses provided to students, thereby enriching the overall learning experience.
  Background: Grounded in Vygotsky's socio-cultural theory and the concept of the More Knowledgeable Other (MKO), the study examines how GenAI can act as an auxiliary MKO to enrich educational dialogue between students and instructors.
  Research Question: How effective is GenAI in reducing the workload of instructional staff when used to pre-answer student questions posted on educational discussion forums?
  Methodology: Using a mixed-methods approach in large introductory programming courses, human Teaching Assistants (AI-TAs) employed an AI-assisted platform to pre-answer student queries. We analyzed efficiency indicators like the frequency of modifications to AI-generated responses and gathered qualitative feedback from AI-TAs.
  Findings: The findings indicate no significant difference in student reception to responses generated by AI-TAs compared to those provided by human instructors. This suggests that GenAI can effectively meet educational needs when adequately managed. Moreover, AI-TAs experienced a reduction in the cognitive load required for responding to queries, pointing to GenAI's potential to enhance instructional efficiency without compromising the quality of education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13196v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anvit Sinha, Shruti Goyal, Zachary Sy, Rhianna Kuperus, Ethan Dickey, Andres Bejarano</dc:creator>
    </item>
    <item>
      <title>Engagement, Content Quality and Ideology over Time on the Facebook URL Dataset</title>
      <link>https://arxiv.org/abs/2409.13461</link>
      <description>arXiv:2409.13461v1 Announce Type: new 
Abstract: Unpacking the relationship between the ideology of social media users and their online news consumption offers critical insight into the feedback loop between users' engagement behavior and the recommender systems' content provision. However, disentangling inherent user behavior from platform-induced influences poses significant challenges, particularly when working with datasets covering limited time periods. In this study, we conduct both aggregate and longitudinal analyses using the Facebook Privacy-Protected Full URLs Dataset, examining user engagement metrics related to news URLs in the U.S. from January 2017 to December 2020. By incorporating the ideological alignment and quality of news sources, along with users' political preferences, we construct weighted averages of ideology and quality of news consumption for liberal, conservative, and moderate audiences. This allows us to track the evolution of (i) the ideological gap between liberals and conservatives and (ii) the average quality of each group's news consumption. These metrics are linked to broader phenomena such as polarization and misinformation. We identify two significant shifts in trends for both metrics, each coinciding with changes in user engagement. Interestingly, during both inflection points, the ideological gap widens and news quality declines; however, engagement increases after the first one and decreases after the second. Finally, we contextualize these changes by discussing their potential relation to two major updates to Facebook's News Feed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13461v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Fraxanet, Fabrizio Germano, Andreas Kaltenbrunner, Vicen\c{c} G\'omez</dc:creator>
    </item>
    <item>
      <title>A Law of One's Own: The Inefficacy of the DMCA for Non-Consensual Intimate Media</title>
      <link>https://arxiv.org/abs/2409.13575</link>
      <description>arXiv:2409.13575v1 Announce Type: new 
Abstract: Non-consensual intimate media (NCIM) presents internet-scale harm to individuals who are depicted. One of the most powerful tools for requesting its removal is the Digital Millennium Copyright Act (DMCA). However, the DMCA was designed to protect copyright holders rather than to address the problem of NCIM. Using a dataset of more than 54,000 DMCA reports and over 85 million infringing URLs spanning over a decade, this paper evaluates the efficacy of the DMCA for NCIM takedown. Results show less than 50% of infringing URLs are removed from website hosts in 60 days, and Google Search takes a median of 11.7 days to deindex infringing content. Across web hosts, only 4% of URLs are removed within the first 48 hours. Additionally, the most frequently reported domains for non-commercial NCIM are smaller websites, not large platforms. We stress the need for new laws that ensure a shorter time to takedown that are enforceable across big and small platforms alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13575v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Qiwei, Shihui Zhang, Samantha Paige Pratt, Andrew Timothy Kasper, Eric Gilbert, Sarita Schoenebeck</dc:creator>
    </item>
    <item>
      <title>h4rm3l: A Dynamic Benchmark of Composable Jailbreak Attacks for LLM Safety Assessment</title>
      <link>https://arxiv.org/abs/2408.04811</link>
      <description>arXiv:2408.04811v2 Announce Type: cross 
Abstract: The safety of Large Language Models (LLMs) remains a critical concern due to a lack of adequate benchmarks for systematically evaluating their ability to resist generating harmful content. Previous efforts towards automated red teaming involve static or templated sets of illicit requests and adversarial prompts which have limited utility given jailbreak attacks' evolving and composable nature. We propose a novel dynamic benchmark of composable jailbreak attacks to move beyond static datasets and taxonomies of attacks and harms. Our approach consists of three components collectively called h4rm3l: (1) a domain-specific language that formally expresses jailbreak attacks as compositions of parameterized prompt transformation primitives, (2) bandit-based few-shot program synthesis algorithms that generate novel attacks optimized to penetrate the safety filters of a target black box LLM, and (3) open-source automated red-teaming software employing the previous two components. We use h4rm3l to generate a dataset of 2656 successful novel jailbreak attacks targeting 6 state-of-the-art (SOTA) open-source and proprietary LLMs. Several of our synthesized attacks are more effective than previously reported ones, with Attack Success Rates exceeding 90% on SOTA closed language models such as claude-3-haiku and GPT4-o. By generating datasets of jailbreak attacks in a unified formal representation, h4rm3l enables reproducible benchmarking and automated red-teaming, contributes to understanding LLM safety limitations, and supports the development of robust defenses in an increasingly LLM-integrated world.
  Warning: This paper and related research artifacts contain offensive and potentially disturbing prompts and model-generated content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04811v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Moussa Koulako Bala Doumbouya, Ananjan Nandi, Gabriel Poesia, Davide Ghilardi, Anna Goldie, Federico Bianchi, Dan Jurafsky, Christopher D. Manning</dc:creator>
    </item>
    <item>
      <title>Are Large Language Models Good Essay Graders?</title>
      <link>https://arxiv.org/abs/2409.13120</link>
      <description>arXiv:2409.13120v1 Announce Type: cross 
Abstract: We evaluate the effectiveness of Large Language Models (LLMs) in assessing essay quality, focusing on their alignment with human grading. More precisely, we evaluate ChatGPT and Llama in the Automated Essay Scoring (AES) task, a crucial natural language processing (NLP) application in Education. We consider both zero-shot and few-shot learning and different prompting approaches. We compare the numeric grade provided by the LLMs to human rater-provided scores utilizing the ASAP dataset, a well-known benchmark for the AES task. Our research reveals that both LLMs generally assign lower scores compared to those provided by the human raters; moreover, those scores do not correlate well with those provided by the humans. In particular, ChatGPT tends to be harsher and further misaligned with human evaluations than Llama. We also experiment with a number of essay features commonly used by previous AES methods, related to length, usage of connectives and transition words, and readability metrics, including the number of spelling and grammar mistakes. We find that, generally, none of these features correlates strongly with human or LLM scores. Finally, we report results on Llama 3, which are generally better across the board, as expected. Overall, while LLMs do not seem an adequate replacement for human grading, our results are somewhat encouraging for their use as a tool to assist humans in the grading of written essays in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13120v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anindita Kundu, Denilson Barbosa</dc:creator>
    </item>
    <item>
      <title>Economic Policy Challenges for the Age of AI</title>
      <link>https://arxiv.org/abs/2409.13168</link>
      <description>arXiv:2409.13168v1 Announce Type: cross 
Abstract: This paper examines the profound challenges that transformative advances in AI towards Artificial General Intelligence (AGI) will pose for economists and economic policymakers. I examine how the Age of AI will revolutionize the basic structure of our economies by diminishing the role of labor, leading to unprecedented productivity gains but raising concerns about job disruption, income distribution, and the value of education and human capital. I explore what roles may remain for labor post-AGI, and which production factors will grow in importance. The paper then identifies eight key challenges for economic policy in the Age of AI: (1) inequality and income distribution, (2) education and skill development, (3) social and political stability, (4) macroeconomic policy, (5) antitrust and market regulation, (6) intellectual property, (7) environmental implications, and (8) global AI governance. It concludes by emphasizing how economists can contribute to a better understanding of these challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13168v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anton Korinek</dc:creator>
    </item>
    <item>
      <title>A Survey on Moral Foundation Theory and Pre-Trained Language Models: Current Advances and Challenges</title>
      <link>https://arxiv.org/abs/2409.13521</link>
      <description>arXiv:2409.13521v1 Announce Type: cross 
Abstract: Moral values have deep roots in early civilizations, codified within norms and laws that regulated societal order and the common good. They play a crucial role in understanding the psychological basis of human behavior and cultural orientation. The Moral Foundation Theory (MFT) is a well-established framework that identifies the core moral foundations underlying the manner in which different cultures shape individual and social lives. Recent advancements in natural language processing, particularly Pre-trained Language Models (PLMs), have enabled the extraction and analysis of moral dimensions from textual data. This survey presents a comprehensive review of MFT-informed PLMs, providing an analysis of moral tendencies in PLMs and their application in the context of the MFT. We also review relevant datasets and lexicons and discuss trends, limitations, and future directions. By providing a structured overview of the intersection between PLMs and MFT, this work bridges moral psychology insights within the realm of PLMs, paving the way for further research and development in creating morally aware AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13521v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Zangari, Candida M. Greco, Davide Picca, Andrea Tagarelli</dc:creator>
    </item>
    <item>
      <title>The Impact of Large Language Models in Academia: from Writing to Speaking</title>
      <link>https://arxiv.org/abs/2409.13686</link>
      <description>arXiv:2409.13686v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly impacting human society, particularly in textual information. Based on more than 30,000 papers and 1,000 presentations from machine learning conferences, we examined and compared the words used in writing and speaking, representing the first large-scale investigating study of how LLMs influence the two main modes of verbal communication and expression within the same group of people. Our empirical results show that LLM-style words such as "significant" have been used more frequently in abstracts and oral presentations. The impact on speaking is beginning to emerge and is likely to grow in the future, calling attention to the implicit influence and ripple effect of LLMs on human society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13686v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingmeng Geng, Caixi Chen, Yanru Wu, Dongping Chen, Yao Wan, Pan Zhou</dc:creator>
    </item>
    <item>
      <title>GAIDE: A Framework for Using Generative AI to Assist in Course Content Development</title>
      <link>https://arxiv.org/abs/2308.12276</link>
      <description>arXiv:2308.12276v4 Announce Type: replace 
Abstract: This paper introduces "GAIDE: Generative AI for Instructional Development and Education," a novel framework for using Generative AI (GenAI) to enhance educational content creation. GAIDE stands out by offering a practical approach for educators to produce diverse, engaging, and academically rigorous materials. It integrates GenAI into curriculum design, easing the workload of instructors and elevating material quality. With GAIDE, we present a distinct, adaptable model that harnesses technological progress in education, marking a step towards more efficient instructional development. Motivated by the demand for innovative educational content and the rise of GenAI use among students, this research tackles the challenge of adapting and integrating technology into teaching. GAIDE aims to streamline content development, encourage the creation of dynamic materials, and demonstrate GenAI's utility in instructional design. The framework is grounded in constructivist learning theory and TPCK, emphasizing the importance of integrating technology in a manner that complements pedagogical goals and content knowledge. Our approach aids educators in crafting effective GenAI prompts and guides them through interactions with GenAI tools, both of which are critical for generating high-quality, contextually appropriate content. Initial evaluations indicate GAIDE reduces time and effort in content creation, without compromising on the breadth or depth of the content. Moreover, the use of GenAI has shown promise in deterring conventional cheating methods, suggesting a positive impact on academic integrity and student engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12276v4</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ethan Dickey, Andres Bejarano</dc:creator>
    </item>
    <item>
      <title>From Cash to Cashless: UPI's Impact on Spending Behavior Among Indian Users and Prototyping Financially Responsible Interfaces</title>
      <link>https://arxiv.org/abs/2401.09937</link>
      <description>arXiv:2401.09937v3 Announce Type: replace 
Abstract: Unified Payments Interface (UPI) is a groundbreaking innovation making waves in digital payment systems in India. It has revolutionised financial transactions by offering enhanced convenience and security. While previous research has primarily focused on the macroeconomic effects of digital payments, our study examines UPI's impact on individual spending behavior. Through a survey of 276 respondents and 20 follow-up interviews, we found that approximately 75% of participants reported increased spending due to UPI. Many attributed this to UPI's intangible nature, which reduced feelings of guilt typically associated with spending. Additionally, participants provided suggestions to improve the user experience of existing UPI applications. Utilizing this feedback, we developed a high-fidelity prototype based on a popular UPI app in India and conducted usability testing with 34 participants. The insights gathered from this testing shaped the final prototype and its features. This study offers valuable design recommendations for UPI app developers and other stakeholders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09937v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Harshal Dev, Raj Gupta, Sahiti Dharmavaram, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>Navigating LLM Ethics: Advancements, Challenges, and Future Directions</title>
      <link>https://arxiv.org/abs/2406.18841</link>
      <description>arXiv:2406.18841v3 Announce Type: replace 
Abstract: This study addresses ethical issues surrounding Large Language Models (LLMs) within the field of artificial intelligence. It explores the common ethical challenges posed by both LLMs and other AI systems, such as privacy and fairness, as well as ethical challenges uniquely arising from LLMs. It highlights challenges such as hallucination, verifiable accountability, and decoding censorship complexity, which are unique to LLMs and distinct from those encountered in traditional AI systems. The study underscores the need to tackle these complexities to ensure accountability, reduce biases, and enhance transparency in the influential role that LLMs play in shaping information dissemination. It proposes mitigation strategies and future directions for LLM ethics, advocating for interdisciplinary collaboration. It recommends ethical frameworks tailored to specific domains and dynamic auditing systems adapted to diverse contexts. This roadmap aims to guide responsible development and integration of LLMs, envisioning a future where ethical considerations govern AI advancements in society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18841v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junfeng Jiao, Saleh Afroogh, Yiming Xu, Connor Phillips</dc:creator>
    </item>
    <item>
      <title>An integrated view of Quantum Technology? Mapping Media, Business, and Policy Narratives</title>
      <link>https://arxiv.org/abs/2408.02236</link>
      <description>arXiv:2408.02236v2 Announce Type: replace 
Abstract: Narratives play a vital role in shaping public perceptions and policy on emerging technologies like quantum technology (QT). However, little is known about the construction and variation of QT narratives across societal domains. This study examines how QT is presented in business, media, and government texts using thematic narrative analysis. Our research design utilizes an extensive dataset of 36 government documents, 165 business reports, and 2,331 media articles published over 20 years. We employ a computational social science approach, combining BERTopic modeling with qualitative assessment to extract themes and narratives. The findings show that public discourse on QT reflects prevailing social and political agendas, focusing on technical and commercial potential, global conflicts, national strategies, and social issues. Media articles provide the most balanced coverage, while business and government discourses often overlook societal implications. We discuss the ramifications for integrating QT into society and the need for wellinformed public discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02236v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Viktor Suter, Charles Ma, Gina Poehlmann, Miriam Meckel, Lea Steinacker</dc:creator>
    </item>
    <item>
      <title>Gender Representation and Bias in Indian Civil Service Mock Interviews</title>
      <link>https://arxiv.org/abs/2409.12194</link>
      <description>arXiv:2409.12194v3 Announce Type: replace-cross 
Abstract: This paper makes three key contributions. First, via a substantial corpus of 51,278 interview questions sourced from 888 YouTube videos of mock interviews of Indian civil service candidates, we demonstrate stark gender bias in the broad nature of questions asked to male and female candidates. Second, our experiments with large language models show a strong presence of gender bias in explanations provided by the LLMs on the gender inference task. Finally, we present a novel dataset of 51,278 interview questions that can inform future social science studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12194v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somonnoy Banerjee, Sujan Dutta, Soumyajit Datta, Ashiqur R. KhudaBukhsh</dc:creator>
    </item>
  </channel>
</rss>
