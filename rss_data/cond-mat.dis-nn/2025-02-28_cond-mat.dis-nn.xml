<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:00:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Spectral Analysis of Representational Similarity with Limited Neurons</title>
      <link>https://arxiv.org/abs/2502.19648</link>
      <description>arXiv:2502.19648v1 Announce Type: new 
Abstract: Measuring representational similarity between neural recordings and computational models is challenging due to constraints on the number of neurons that can be recorded simultaneously. In this work, we investigate how such limitations affect similarity measures, focusing on Canonical Correlation Analysis (CCA) and Centered Kernel Alignment (CKA). Leveraging tools from Random Matrix Theory, we develop a predictive spectral framework for these measures and demonstrate that finite neuron sampling systematically underestimates similarity due to eigenvector delocalization. To overcome this, we introduce a denoising method to infer population-level similarity, enabling accurate analysis even with small neuron samples. Our theory is validated on synthetic and real datasets, offering practical strategies for interpreting neural data under finite sampling constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19648v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunmo Kang, Abdulkadir Canatar, SueYeon Chung</dc:creator>
    </item>
    <item>
      <title>Effects of Galactic Irradiation on Thermal and Electronic Transport in Tungsten</title>
      <link>https://arxiv.org/abs/2502.19449</link>
      <description>arXiv:2502.19449v1 Announce Type: cross 
Abstract: The impact of irradiation on the thermal and electronic properties of materials is a persistent puzzle, particularly defect formation at the atomic and nanoscales. This work examines the nanoscale effects of low-energy irradiation on tungsten (W), focusing on defect-induced modifications to thermal and electronic transport. Using the Site-Projected Thermal Conductivity (SPTC) method [A. Gautam et al. PSS-RRL, 2400306, 2024], we analyze bulk and twin-grain boundary W with vacancy defects based on the Norgett-Robinson-Torrens displacements per atom (NRT-dpa) model. SPTC provides a detailed prediction of post-cascade spatial thermal conductivity distribution. We estimate electronic conductivity activity using the "N2 method" [K. Nepal et al. Carbon, 119711, 2025] to explore the consequences of vacancies and grain boundaries, highlighting the defect-dependent nature of charge transport behavior. These findings offer high-resolution insights into irradiation-driven transport phenomena, with implications for space-exposed materials and nanoscale thermal/electronic management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19449v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. Ugwumadu, D. A. Drabold, R. Tutchton</dc:creator>
    </item>
    <item>
      <title>An Analysis of First- and Second-Order Optimization Algorithms in Variational Monte Carlo</title>
      <link>https://arxiv.org/abs/2502.19576</link>
      <description>arXiv:2502.19576v1 Announce Type: cross 
Abstract: Many quantum many-body wavefunctions, such as Jastrow-Slater, tensor network, and neural quantum states, are studied with the variational Monte Carlo technique, where stochastic optimization is usually performed to obtain a faithful approximation to the ground-state of a given Hamiltonian. While first order gradient descent methods are commonly used for such optimizations, recent second order optimization formulations offer the potential of faster convergence under certain theoretical conditions, but with a similar cost per sample to first order methods. However, the relative performance of first order and second order optimizers is influenced in practice by many factors, including the sampling requirements for a faithful optimization step, the influence of wavefunction quality, as well as the wavefunction parametrization and expressivity. Here we analyze these performance characteristics of first order and second order optimization methods for a variety of Hamiltonians, with the additional context of understanding the scaling of these methods (for good performance) as a function of system size. Our findings help clarify the role of first order and second order methods in variational Monte Carlo calculations and the conditions under which they should respectively be used. In particular, we find that unlike in deterministic optimization, where closeness to the variational minimum determines the suitability of second order methods, in stochastic optimization the main factor is the overall expressivity of the wavefunction: second order methods lead to an overall reduction in cost relative to first order methods when the wavefunction is sufficiently expressive to represent the ground-state, even when starting away from the ground state. This makes second order methods an important technique when used with wavefunctions with arbitrarily improvable accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19576v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruojing Peng, Garnet Kin-Lic Chan</dc:creator>
    </item>
    <item>
      <title>Local ergotropy dynamically witnesses many-body localized phases</title>
      <link>https://arxiv.org/abs/2502.20002</link>
      <description>arXiv:2502.20002v1 Announce Type: cross 
Abstract: Many-body localization is a dynamical phenomenon characteristic of strongly interacting and disordered many-body quantum systems which fail to achieve thermal equilibrium. From a quantum information perspective, the fingerprint of this phenomenon is the logarithmic growth of the entanglement entropy over time. We perform intensive numerical simulations, applied to a paradigmatic model system, showing that the local ergotropy, the maximum extractable work via local unitary operations on a small subsystem in the presence of Hamiltonian coupling, dynamically witnesses the change from ergodic to localized phases. Within the many-body localized phase, both the local ergotropy and its quantum fluctuations slowly vary over time with a characteristic logarithmic law analogous to the behaviour of entanglement entropy. This showcases how directly leveraging local control, instead of local observables or entropies analyzed in previous works, provides a thermodynamic marker of localization phenomena based on the locally extractable work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20002v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.str-el</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Formicola, Grazia Di Bello, Giulio De Filippis, Vittorio Cataudella, Donato Farina, Carmine Antonio Perroni</dc:creator>
    </item>
    <item>
      <title>On the interpretability of neural network decoders</title>
      <link>https://arxiv.org/abs/2502.20269</link>
      <description>arXiv:2502.20269v1 Announce Type: cross 
Abstract: Neural-network (NN) based decoders are becoming increasingly popular in the field of quantum error correction (QEC), including for decoding of state-of-the-art quantum computation experiments. In this work, we make use of established interpretability methods from the field of machine learning, to introduce a toolbox to achieve an understanding of the underlying decoding logic of NN decoders, which have been trained but otherwise typically operate as black-box models. To illustrate the capabilities of the employed interpretability method, based on the Shapley value approximation, we provide an examplary case study of a NN decoder that is trained for flag-qubit based fault-tolerant (FT) QEC with the Steane code. We show how particular decoding decisions of the NN can be interpreted, and reveal how the NN learns to capture fundamental structures in the information gained from syndrome and flag qubit measurements, in order to come to a FT correction decision. Further, we show that the understanding of how the NN obtains a decoding decision can be used on the one hand to identify flawed processing of error syndrome information by the NN, resulting in decreased decoding performance, as well as for well-informed improvements of the NN architecture. The diagnostic capabilities of the interpretability method we present can help ensure successful application of machine learning for decoding of QEC protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20269v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas B\"odeker, Luc J. B. Kusters, Markus M\"uller</dc:creator>
    </item>
    <item>
      <title>Formation of Representations in Neural Networks</title>
      <link>https://arxiv.org/abs/2410.03006</link>
      <description>arXiv:2410.03006v2 Announce Type: replace-cross 
Abstract: Understanding neural representations will help open the black box of neural networks and advance our scientific understanding of modern AI systems. However, how complex, structured, and transferable representations emerge in modern neural networks has remained a mystery. Building on previous results, we propose the Canonical Representation Hypothesis (CRH), which posits a set of six alignment relations to universally govern the formation of representations in most hidden layers of a neural network. Under the CRH, the latent representations (R), weights (W), and neuron gradients (G) become mutually aligned during training. This alignment implies that neural networks naturally learn compact representations, where neurons and weights are invariant to task-irrelevant transformations. We then show that the breaking of CRH leads to the emergence of reciprocal power-law relations between R, W, and G, which we refer to as the Polynomial Alignment Hypothesis (PAH). We present a minimal-assumption theory proving that the balance between gradient noise and regularization is crucial for the emergence of the canonical representation. The CRH and PAH lead to an exciting possibility of unifying major key deep learning phenomena, including neural collapse and the neural feature ansatz, in a single framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03006v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liu Ziyin, Isaac Chuang, Tomer Galanti, Tomaso Poggio</dc:creator>
    </item>
    <item>
      <title>Hierarchy of chaotic dynamics in random modular networks</title>
      <link>https://arxiv.org/abs/2410.06361</link>
      <description>arXiv:2410.06361v2 Announce Type: replace-cross 
Abstract: We introduce a model of randomly connected neural populations and study its dynamics by means of the dynamical mean-field theory and simulations. Our analysis uncovers a rich phase diagram, featuring high- and low-dimensional chaotic phases, separated by a crossover region characterized by low values of the maximal Lyapunov exponent and participation ratio dimension, but with high values of the Lyapunov dimension that change significantly across the region. Counterintuitively, chaos can be attenuated by either adding noise to strongly modular connectivity or by introducing modularity into random connectivity. Extending the model to include a multilevel, hierarchical connectivity reveals that a loose balance between activities across levels drives the system towards the edge of chaos.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06361v2</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.NE</category>
      <category>nlin.CD</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Ku\'smierz, Ulises Pereira-Obilinovic, Zhixin Lu, Dana Mastrovito, Stefan Mihalas</dc:creator>
    </item>
    <item>
      <title>Robust Prediction of Frictional Contact Network in Near-Jamming Suspensions Employing Deep Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2502.18743</link>
      <description>arXiv:2502.18743v2 Announce Type: replace-cross 
Abstract: The viscosity of the suspension consisting of fine particles dispersed in a Newtonian liquid diverges close to the jamming packing fraction. The contact microstructure in suspensions governs this macroscopic behavior in the vicinity of jamming through a frictional contact network (FCN). FCN is composed of mechanical load-bearing contacts that lead to the emergence of rigidity near the jamming transition. The stress transmission and network topology, in turn, depend sensitively on constraints on the relative motion of the particles. Despite their significance, predicting the FCN, especially close to jamming conditions, remains challenging due to experimental and computational impediments. This study introduces a cost-effective machine learning approach to predict the FCN using a graph neural network (GNN), which inherently captures hidden features and underlying patterns in dense suspension by mapping interparticle interactions. Employing a variation of GNN called the Deep Graph Convolutional Network (DeepGCN) trained on data-driven simulations, this study demonstrates robust generalization and extrapolation capabilities, accurately predicting FCNs in systems with divergent flow parameters and phase spaces, despite each being trained exclusively on a single condition. The study covers a wide range of phase space, from semi-dilute to jammed states, spanning transient to steady states, while systematically varying parameters such as shear stress (${\sigma}_{xy}$), packing fraction(${\phi}$) and sliding and rolling friction (${{\mu}_s, {\mu}_r}$). The results of this research pave the way for innovative transferable techniques in predicting the properties of particulate systems, offering new avenues for advancement in material science and related fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18743v2</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.flu-dyn</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armin Aminimajd, Joao Maia, Abhinendra Singh</dc:creator>
    </item>
    <item>
      <title>2064 global population crisis scenario predicted by the most general dynamic model</title>
      <link>https://arxiv.org/abs/2502.19063</link>
      <description>arXiv:2502.19063v2 Announce Type: replace-cross 
Abstract: There is currently no consensus on how the global population will evolve in the next decades and in the next century. The reason for this uncertainty is the absence of reliable population dynamic models. In this paper, we remedy to this situation by reporting on a population dynamic model, a single nonlinear differential equation adapted from the physics of disordered systems, which is able to mathematically describe all the various regimes encountered in the global population recorded as a function of time, over the past 12000 years until now. Regimes of simple exponential growth (Malthus), logistic (Verhulst) plateaus as well as stretched-exponential and compressed-exponential growth regimes are all reliably described by this mathematical equation in its various limits. Besides showing that this is, indeed, the most general population dynamic model, we use it to explore its solutions projected into the future. In particular, two different scenarios are predicted. In one of them, which assumes that the future evolution would continue along a similar pattern as the past decades (hence without any major global ecological crisis affecting the resource exploitation), a von Foerster-type doomsday scenario with a sudden rise of the global population to unsustainable levels could appear as early as 2078. In the opposite scenario, if a global ecological crisis were to set in today, affecting the ability to exploit resources, given the current estimates of the Earth's carrying capacity, the global population is forecasted to reduce by half by 2064. Furthermore, the proposed dynamic model provides with a new aggregated parameter (K, in the model) that can be monitored and controlled so as to avoid the doomsday scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19063v2</guid>
      <category>q-bio.PE</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.CD</category>
      <category>physics.soc-ph</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessio Zaccone, Kostya Trachenko</dc:creator>
    </item>
  </channel>
</rss>
