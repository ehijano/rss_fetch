<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.OS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.OS</link>
    <description>cs.OS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.OS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Dec 2024 02:52:57 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimizing System Memory Bandwidth with Micron CXL Memory Expansion Modules on Intel Xeon 6 Processors</title>
      <link>https://arxiv.org/abs/2412.12491</link>
      <description>arXiv:2412.12491v1 Announce Type: new 
Abstract: High-Performance Computing (HPC) and Artificial Intelligence (AI) workloads typically demand substantial memory bandwidth and, to a degree, memory capacity. CXL memory expansion modules, also known as CXL "type-3" devices, enable enhancements in both memory capacity and bandwidth for server systems by utilizing the CXL protocol which runs over the PCIe interfaces of the processor. This paper discusses experimental findings on achieving increased memory bandwidth for HPC and AI workloads using Micron's CXL modules. This is the first study that presents real data experiments utilizing eight CXL E3.S (x8) Micron CZ122 devices on the Intel Xeon 6 processor 6900P (previously codenamed Granite Rapids AP) featuring 128 cores, alongside Micron DDR-5 memory operating at 6400 MT/s on each of the CPU's 12 DRAM channels. The eight CXL memories were set up as a unified NUMA configuration, employing software-based page level interleaving mechanism, available in Linux kernel v6.9+, between DDR5 and CXL memory nodes to improve overall system bandwidth. Memory expansion via CXL boosts read-only bandwidth by 24% and mixed read/write bandwidth by up to 39%. Across HPC and AI workloads, the geometric mean of performance speedups is 24%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12491v1</guid>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Sehgal, Vishal Tanna, Vinicius Petrucci, Anil Godbole</dc:creator>
    </item>
    <item>
      <title>Performance Characterization of AutoNUMA Memory Tiering on Graph Analytics</title>
      <link>https://arxiv.org/abs/2212.04344</link>
      <description>arXiv:2212.04344v1 Announce Type: cross 
Abstract: Non-Volatile Memory (NVM) can deliver higher density and lower cost per bit when compared with DRAM. Its main drawback is that it is slower than DRAM. On the other hand, DRAM has scalability problems due to its cost and energy consumption. NVM will likely coexist with DRAM in computer systems and the biggest challenge is to know which data to allocate on each type of memory. A state-of-the-art approach is AutoNUMA, in the Linux kernel. Prior work is limited to measuring AutoNUMA solely in terms of the application execution time, without understanding AutoNUMA's behavior. In this work we provide a more in-depth characterization of AutoNUMA, for instance, identifying where exactly a set of pages are allocated, while keeping track of promotion and demotion decisions performed by AutoNUMA. Our analysis shows that AutoNUMA's benefits can be modest when running graph processing applications, or graph analytics, because most pages have only one access over the entire execution time and other pages accesses have no temporal locality. We make a case for exploring application characteristics using object-level mappings between DRAM and NVM. Our preliminary experiments show that an object-level memory tiering can better capture the application behavior and reduce the execution time of graph analytics by 21% (avg) and 51% (max), when compared to AutoNUMA, while significantly reducing the number of memory accesses in NVM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.04344v1</guid>
      <category>cs.PF</category>
      <category>cs.OS</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IISWC55918.2022.00024</arxiv:DOI>
      <dc:creator>Diego Moura, Vinicius Petrucci, Daniel Mosse</dc:creator>
    </item>
    <item>
      <title>Scaling Inter-procedural Dataflow Analysis on the Cloud</title>
      <link>https://arxiv.org/abs/2412.12579</link>
      <description>arXiv:2412.12579v1 Announce Type: cross 
Abstract: Apart from forming the backbone of compiler optimization, static dataflow analysis has been widely applied in a vast variety of applications, such as bug detection, privacy analysis, program comprehension, etc. Despite its importance, performing interprocedural dataflow analysis on large-scale programs is well known to be challenging. In this paper, we propose a novel distributed analysis framework supporting the general interprocedural dataflow analysis. Inspired by large-scale graph processing, we devise dedicated distributed worklist algorithms for both whole-program analysis and incremental analysis. We implement these algorithms and develop a distributed framework called BigDataflow running on a large-scale cluster. The experimental results validate the promising performance of BigDataflow -- BigDataflow can finish analyzing the program of millions lines of code in minutes. Compared with the state-of-the-art, BigDataflow achieves much more analysis efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12579v1</guid>
      <category>cs.PL</category>
      <category>cs.OS</category>
      <category>cs.SE</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zewen Sun, Yujin Zhang, Duanchen Xu, Yiyu Zhang, Yun Qi, Yueyang Wang, Yi Li, Zhaokang Wang, Yue Li, Xuandong Li, Zhiqiang Zuo, Qingda Lu, Wenwen Peng, Shengjian Guo</dc:creator>
    </item>
    <item>
      <title>EDM: An Ultra-Low Latency Ethernet Fabric for Memory Disaggregation</title>
      <link>https://arxiv.org/abs/2411.08300</link>
      <description>arXiv:2411.08300v2 Announce Type: replace 
Abstract: Achieving low remote memory access latency remains the primary challenge in realizing memory disaggregation over Ethernet within the datacenters. We present EDM that attempts to overcome this challenge using two key ideas. First, while existing network protocols for remote memory access over the Ethernet, such as TCP/IP and RDMA, are implemented on top of the MAC layer, EDM takes a radical approach by implementing the entire network protocol stack for remote memory access within the Physical layer (PHY) of the Ethernet. This overcomes fundamental latency and bandwidth overheads imposed by the MAC layer, especially for small memory messages. Second, EDM implements a centralized, fast, in-network scheduler for memory traffic within the PHY of the Ethernet switch. Inspired by the classic Parallel Iterative Matching (PIM) algorithm, the scheduler dynamically reserves bandwidth between compute and memory nodes by creating virtual circuits in the PHY, thus eliminating queuing delay and layer 2 packet processing delay at the switch for memory traffic, while maintaining high bandwidth utilization. Our FPGA testbed demonstrates that EDM's network fabric incurs a latency of only $\sim$300 ns for remote memory access in an unloaded network, which is an order of magnitude lower than state-of-the-art Ethernet-based solutions such as RoCEv2 and comparable to emerging PCIe-based solutions such as CXL. Larger-scale network simulations indicate that even at high network loads, EDM's average latency remains within 1.3$\times$ its unloaded latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08300v2</guid>
      <category>cs.OS</category>
      <category>cs.NI</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3669940.3707221</arxiv:DOI>
      <dc:creator>Weigao Su, Vishal Shrivastav</dc:creator>
    </item>
    <item>
      <title>HyperGraphOS: A Modern Meta-Operating System for the Scientific and Engineering Domains</title>
      <link>https://arxiv.org/abs/2412.10487</link>
      <description>arXiv:2412.10487v2 Announce Type: replace-cross 
Abstract: This paper presents HyperGraphOS, a significant innovation in the domain of operating systems, specifically designed to address the needs of scientific and engineering domains. This platform aims to combine model-based engineering, graph modeling, data containers, and documents, along with tools for handling computational elements. HyperGraphOS functions as an Operating System offering to users an infinite workspace for creating and managing complex models represented as graphs with customizable semantics. By leveraging a web-based architecture, it requires only a modern web browser for access, allowing organization of knowledge, documents, and content into models represented in a network of workspaces. Elements of the workspace are defined in terms of domain-specific languages (DSLs). These DSLs are pivotal for navigating workspaces, generating code, triggering AI components, and organizing information and processes. The models' dual nature as both visual drawings and data structures allows dynamic modifications and inspections both interactively as well as programaticaly. We evaluated HyperGraphOS's efficiency and applicability across a large set of diverse domains, including the design and development of a virtual Avatar dialog system, a robotic task planner based on large language models (LLMs), a new meta-model for feature-based code development and many others. Our findings show that HyperGraphOS offers substantial benefits in the interaction with a computer as information system, as platoform for experiments and data analysis, as streamlined engineering processes, demonstrating enhanced flexibility in managing data, computation and documents, showing an innovative approaches to persistent desktop environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10487v2</guid>
      <category>cs.SE</category>
      <category>cs.OS</category>
      <category>cs.PL</category>
      <pubDate>Wed, 18 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonello Ceravola, Frank Joublin</dc:creator>
    </item>
  </channel>
</rss>
