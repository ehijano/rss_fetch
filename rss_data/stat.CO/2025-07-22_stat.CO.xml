<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 23 Jul 2025 01:41:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Inference for Diffusion Processes via Controlled Sequential Monte Carlo and Splitting Schemes</title>
      <link>https://arxiv.org/abs/2507.14535</link>
      <description>arXiv:2507.14535v1 Announce Type: new 
Abstract: We introduce an inferential framework for a wide class of semi-linear stochastic differential equations (SDEs). Recent work has shown that numerical splitting schemes can preserve critical properties of such types of SDEs, give rise to explicit pseudolikelihoods, and hence allow for parameter inference for fully observed processes. Here, under several discrete time observation regimes (particularly, partially and fully observed with and without noise), we represent the implied pseudolikelihood as the normalising constant of a Feynman--Kac flow, allowing its efficient estimation via controlled sequential Monte Carlo and adapt likelihood-based methods to exploit this pseudolikelihood for inference. The strategy developed herein allows us to obtain good inferential results across a range of problems. Using diffusion bridges, we are able to computationally reduce bias coming from time-discretisation without recourse to more complex numerical schemes which typically require considerable application-specific efforts. Simulations illustrate that our method provides an excellent trade-off between computational efficiency and accuracy, under hypoellipticity, for both point and posterior estimation. Application to a neuroscience example shows the good performance of the method in challenging settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14535v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu Huang, Richard G. Everitt, Massimiliano Tamborrino, Adam M. Johansen</dc:creator>
    </item>
    <item>
      <title>Bayesian Inversion via Probabilistic Cellular Automata: an application to image denoising</title>
      <link>https://arxiv.org/abs/2507.14869</link>
      <description>arXiv:2507.14869v1 Announce Type: new 
Abstract: We propose using Probabilistic Cellular Automata (PCA) to address inverse problems with the Bayesian approach. In particular, we use PCA to sample from an approximation of the posterior distribution. The peculiar feature of PCA is their intrinsic parallel nature, which allows for a straightforward parallel implementation that allows the exploitation of parallel computing architecture in a natural and efficient manner. We compare the performance of the PCA method with the standard Gibbs sampler on an image denoising task in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity (SSIM). The numerical results obtained with this approach suggest that PCA-based algorithms are a promising alternative for Bayesian inference in high-dimensional inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14869v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danilo Costarelli, Michele Piconi, Alessio Troiani</dc:creator>
    </item>
    <item>
      <title>Algorithms for Approximating Conditionally Optimal Bounds</title>
      <link>https://arxiv.org/abs/2507.15529</link>
      <description>arXiv:2507.15529v1 Announce Type: new 
Abstract: This work develops algorithms for non-parametric confidence regions for samples from a univariate distribution whose support is a discrete mesh bounded on the left. We generalize the theory of Learned-Miller to preorders over the sample space. In this context, we show that the lexicographic low and lexicographic high orders are in some way extremal in the class of monotone preorders. From this theory we derive several approximation algorithms: 1) Closed form approximations for the lexicographic low and high orders with error tending to zero in the mesh size; 2) A polynomial-time approximation scheme for quantile orders with error tending to zero in the mesh size; 3) Monte Carlo methods for calculating quantile and lexicographic low orders applicable to any mesh size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15529v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Bissias</dc:creator>
    </item>
    <item>
      <title>Predictive Representativity: Uncovering Racial Bias in AI-based Skin Cancer Detection</title>
      <link>https://arxiv.org/abs/2507.14176</link>
      <description>arXiv:2507.14176v1 Announce Type: cross 
Abstract: Artificial intelligence (AI) systems increasingly inform medical decision-making, yet concerns about algorithmic bias and inequitable outcomes persist, particularly for historically marginalized populations. This paper introduces the concept of Predictive Representativity (PR), a framework of fairness auditing that shifts the focus from the composition of the data set to outcomes-level equity. Through a case study in dermatology, we evaluated AI-based skin cancer classifiers trained on the widely used HAM10000 dataset and on an independent clinical dataset (BOSQUE Test set) from Colombia. Our analysis reveals substantial performance disparities by skin phototype, with classifiers consistently underperforming for individuals with darker skin, despite proportional sampling in the source data. We argue that representativity must be understood not as a static feature of datasets but as a dynamic, context-sensitive property of model predictions. PR operationalizes this shift by quantifying how reliably models generalize fairness across subpopulations and deployment contexts. We further propose an External Transportability Criterion that formalizes the thresholds for fairness generalization. Our findings highlight the ethical imperative for post-hoc fairness auditing, transparency in dataset documentation, and inclusive model validation pipelines. This work offers a scalable tool for diagnosing structural inequities in AI systems, contributing to discussions on equity, interpretability, and data justice and fostering a critical re-evaluation of fairness in data-driven healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14176v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'es Morales-Forero (Polytechnique Montr\'eal), Lili J. Rueda (Universidad El Bosque), Ronald Herrera (Boehringer Ingelheim International GmbH), Samuel Bassetto (Polytechnique Montr\'eal), Eric Coatanea (Tampere University)</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture Approach for Clustering and Characterizing Cancer Data</title>
      <link>https://arxiv.org/abs/2507.14380</link>
      <description>arXiv:2507.14380v1 Announce Type: cross 
Abstract: Model-based clustering is widely used for identifying and distinguishing types of diseases. However, modern biomedical data coming with high dimensions make it challenging to perform the model estimation in traditional cluster analysis. The incorporation of factor analyzer into the mixture model provides a way to characterize the large set of data features, but the current estimation method is computationally impractical for massive data due to the intrinsic slow convergence of the embedded algorithms, and the incapability to vary the size of the factor analyzers, preventing the implementation of a generalized mixture of factor analyzers and further characterization of the data clusters. We propose a hybrid matrix-free computational scheme to efficiently estimate the clusters and model parameters based on a Gaussian mixture along with generalized factor analyzers to summarize the large number of variables using a small set of underlying factors. Our approach outperforms the existing method with faster convergence while maintaining high clustering accuracy. Our algorithms are applied to accurately identify and distinguish types of breast cancer based on large tumor samples, and to provide a generalized characterization for subtypes of lymphoma using massive gene records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14380v1</guid>
      <category>stat.ME</category>
      <category>q-bio.TO</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Machine Learning-Based Prediction: A Polynomial Chaos Expansion Approach for Joint Model and Input Uncertainty Propagation</title>
      <link>https://arxiv.org/abs/2507.14782</link>
      <description>arXiv:2507.14782v1 Announce Type: cross 
Abstract: Machine learning (ML) surrogate models are increasingly used in engineering analysis and design to replace computationally expensive simulation models, significantly reducing computational cost and accelerating decision-making processes. However, ML predictions contain inherent errors, often estimated as model uncertainty, which is coupled with variability in model inputs. Accurately quantifying and propagating these combined uncertainties is essential for generating reliable engineering predictions. This paper presents a robust framework based on Polynomial Chaos Expansion (PCE) to handle joint input and model uncertainty propagation. While the approach applies broadly to general ML surrogates, we focus on Gaussian Process regression models, which provide explicit predictive distributions for model uncertainty. By transforming all random inputs into a unified standard space, a PCE surrogate model is constructed, allowing efficient and accurate calculation of the mean and standard deviation of the output. The proposed methodology also offers a mechanism for global sensitivity analysis, enabling the accurate quantification of the individual contributions of input variables and ML model uncertainty to the overall output variability. This approach provides a computationally efficient and interpretable framework for comprehensive uncertainty quantification, supporting trustworthy ML predictions in downstream engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14782v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoping Du</dc:creator>
    </item>
    <item>
      <title>Hypergraphs on high dimensional time series sets using signature transform</title>
      <link>https://arxiv.org/abs/2507.15802</link>
      <description>arXiv:2507.15802v1 Announce Type: cross 
Abstract: In recent decades, hypergraphs and their analysis through Topological Data Analysis (TDA) have emerged as powerful tools for understanding complex data structures. Various methods have been developed to construct hypergraphs -- referred to as simplicial complexes in the TDA framework -- over datasets, enabling the formation of edges between more than two vertices. This paper addresses the challenge of constructing hypergraphs from collections of multivariate time series. While prior work has focused on the case of a single multivariate time series, we extend this framework to handle collections of such time series. Our approach generalizes the method proposed in Chretien and al. by leveraging the properties of signature transforms to introduce controlled randomness, thereby enhancing the robustness of the construction process. We validate our method on synthetic datasets and present promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15802v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Vaucher, Paul Minchella</dc:creator>
    </item>
    <item>
      <title>Recursive variational Gaussian approximation with the Whittle likelihood for linear non-Gaussian state space models</title>
      <link>https://arxiv.org/abs/2406.15998</link>
      <description>arXiv:2406.15998v3 Announce Type: replace 
Abstract: Parameter inference for linear and non-Gaussian state space models is challenging because the likelihood function contains an intractable integral over the latent state variables. While Markov chain Monte Carlo (MCMC) methods provide exact samples from the posterior distribution as the number of samples goes to infinity, they tend to have high computational cost, particularly for observations of a long time series. When inference with MCMC methods is computationally expensive, variational Bayes (VB) methods are a useful alternative. VB methods approximate the posterior density of the parameters with a simple and tractable distribution found through optimisation. This work proposes a novel sequential VB approach that makes use of the Whittle likelihood for computationally efficient parameter inference in linear, non-Gaussian state space models. Our algorithm, called Recursive Variational Gaussian Approximation with the Whittle Likelihood (R-VGA-Whittle), updates the variational parameters by processing data in the frequency domain. At each iteration, R-VGA-Whittle requires the gradient and Hessian of the Whittle log-likelihood, which are available in closed form. Through several examples involving a linear Gaussian state space model; a univariate/bivariate stochastic volatility model; and a state space model with Student's t measurement error, where the latent states follow an autoregressive fractionally integrated moving average (ARFIMA) model, we show that R-VGA-Whittle provides good approximations to posterior distributions of the parameters, and that it is very computationally efficient when compared to asymptotically exact methods such as Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15998v3</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bao Anh Vu, David Gunawan, Andrew Zammit-Mangion</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo with one categorical variable</title>
      <link>https://arxiv.org/abs/2506.16582</link>
      <description>arXiv:2506.16582v2 Announce Type: replace 
Abstract: We study randomized quasi-Monte Carlo (RQMC) estimation of a multivariate integral where one of the variables takes only a finite number of values. This problem arises when the variable of integration is drawn from a mixture distribution as is common in importance sampling and also arises in some recent work on transport maps. We find that when integration error decreases at an RQMC rate that it is then beneficial to oversample the smallest mixture components instead of using a proportional allocation. The optimal allocations depend on the possibly unknown convergence rates. Designing the sample with an incorrect assumption on the rate still attains that convergence rate, with an inferior implied constant. The penalty for using a pessimistic rate is typically higher than for using an optimistic one. We also find that for the most accurate RQMC sampling methods, it is advantageous to arrange that our $n=2^m$ randomized Sobol' points split into subsample sizes that are also powers of $2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16582v2</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valerie N. P. Ho, Art B. Owen, Zexin Pan</dc:creator>
    </item>
    <item>
      <title>Sampling-Based Estimation of Jaccard Containment and Similarity</title>
      <link>https://arxiv.org/abs/2507.10019</link>
      <description>arXiv:2507.10019v3 Announce Type: replace 
Abstract: This paper addresses the problem of estimating the containment and similarity between two sets using only random samples from each set, without relying on sketches of full sets. The study introduces a binomial model for predicting the overlap between samples, demonstrating that it is both accurate and practical when sample sizes are small compared to the original sets. The paper compares this model to previous approaches and shows that it provides better estimates under the considered conditions. It also analyzes the statistical properties of the estimator, including error bounds and sample size requirements needed to achieve a desired level of accuracy and confidence. The framework is extended to estimate set similarity, and the paper provides guidance for applying these methods in large scale data systems where only partial or sampled data is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10019v3</guid>
      <category>stat.CO</category>
      <category>cs.DB</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pranav Joshi</dc:creator>
    </item>
    <item>
      <title>Comparison Theorems for the Mixing Times of Systematic and Random Scan Dynamics</title>
      <link>https://arxiv.org/abs/2410.11136</link>
      <description>arXiv:2410.11136v2 Announce Type: replace-cross 
Abstract: A popular method for sampling from high-dimensional distributions is the \emph{Gibbs sampler}, which iteratively resamples sites from the conditional distribution of the desired measure given the values of the other coordinates. It is natural to ask to what extent does the order of site updates matter in the mixing time? Two natural choices are (i) standard, or \emph{random scan}, Glauber dynamics where the updated variable is chosen uniformly at random, and (ii) the \emph{systematic scan} dynamics where variables are updated in a fixed, cyclic order. We first show that for systems of dimension $n$, one round of the systematic scan dynamics has spectral gap at most a factor of order $n$ worse than the corresponding spectral gap of a single step of Glauber dynamics, tightening existing bounds in the literature by He, et al. [NeurIPS '16] and Chlebicka, {\L}atuszy\'nski, and Miasodejow [Ann. Appl. Probab. '25]. The corresponding bound on mixing times is sharp even for simple spin systems by an explicit example of Roberts and Rosenthal [Int. J. Statist. Prob. '15]. We complement this with a converse statement: if all, or even just one scan order rapidly mixes, the Glauber dynamics has a polynomially related mixing time, resolving a question of Chlebicka, {\L}atuszy\'nski, and Miasodejow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11136v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Gaitonde, Elchanan Mossel</dc:creator>
    </item>
  </channel>
</rss>
