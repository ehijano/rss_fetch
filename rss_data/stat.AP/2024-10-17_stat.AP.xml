<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Oct 2024 04:00:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Development of COVID-19 Booster Vaccine Policy by Microsimulation and Q-learning</title>
      <link>https://arxiv.org/abs/2410.12936</link>
      <description>arXiv:2410.12936v1 Announce Type: new 
Abstract: The COVID-19 pandemic highlighted the urgent need for effective vaccine policies, but traditional clinical trials often lack sufficient data to capture the diverse population characteristics necessary for comprehensive public health strategies. Ethical concerns around randomized trials during a pandemic further complicate policy development for public health. Reinforcement Learning (RL) offers a promising alternative for vaccine policy development. However, direct online RL exploration in real-world scenarios can result in suboptimal and potentially harmful decisions. This study proposes a novel framework combining tabular Q-learning with microsimulation (i.e., a Recurrent Neural Network (RNN) environment simulator) to address these challenges in public health vaccine policymaking, which enables effective vaccine policy learning without real-world interaction, addressing both ethical and exploration challenges. The RNN environment simulator captures temporal associations between infection and patient characteristics, generating realistic simulation data. Our tabular Q-learning model produces an interpretable policy table that balances the risks of severe infection against vaccination side effects. Applied to COVID-19 booster policies, the learned Q-learning-based policy outperforms current practices, offering a path toward more effective vaccination strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12936v1</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guoxuan Ma, Lili Zhao, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Analyzing longitudinal electronic health records data with clinically informative visiting process: possible choices and comparisons</title>
      <link>https://arxiv.org/abs/2410.13113</link>
      <description>arXiv:2410.13113v1 Announce Type: cross 
Abstract: Analyzing longitudinal electronic health records (EHR) data is challenging due to clinically informative observation processes, where the timing and frequency of patient visits often depend on their underlying health condition. Traditional longitudinal data analysis rarely considers observation times as stochastic or models them as functions of covariates. In this work, we evaluate the impact of informative visiting processes on two common statistical tasks: (1) estimating the effect of an exposure on a longitudinal biomarker, and (2) assessing the effect of longitudinal biomarkers on a time-to-event outcome, such as disease diagnosis. The methods we consider range from using simple summaries of the observed longitudinal data, imputation, inversely weighting by the estimated intensity of the visit process, and joint modeling. To our knowledge, this is the most comprehensive evaluation of inferential methods specifically tailored for EHR-like visiting processes. For the first task, we improve certain methods around 18 times faster, making them more suitable for large-scale EHR data analysis. For the second task, where no method currently accounts for the visiting process in joint models of longitudinal and time-to-event data, we propose incorporating the historical number of visits to adjust for informative visiting. Using data from the longitudinal biobank at the University of Michigan Health System, we investigate two case studies: 1) the association between genetic variants and lab markers with repeated measures (known as the LabWAS); and 2) the association between cardiometabolic health markers and time to hypertension diagnosis. We show how accounting for informative visiting processes affects the analysis results. We develop an R package CIMPLE (Clinically Informative Missingness handled through Probabilities, Likelihood, and Estimating equations) that integrates all these methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13113v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiacong Du, Xu Shi, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Exploring Scientific Contributions through Citation Context and Division of Labor</title>
      <link>https://arxiv.org/abs/2410.13133</link>
      <description>arXiv:2410.13133v1 Announce Type: cross 
Abstract: Scientific contributions are a direct reflection of a research paper's value, illustrating its impact on existing theories or practices. Existing measurement methods assess contributions based on the authors' perceived or self-identified contributions, while the actual contributions made by the papers are rarely investigated. This study measures the actual contributions of papers published in Nature and Science using 1.53 million citation contexts from citing literature and explores the impact pattern of division of labor (DOL) inputs on the actual contributions of papers from an input-output perspective. Results show that experimental contributions are predominant, contrasting with the theoretical and methodological contributions self-identified by authors. This highlights a notable discrepancy between actual contributions and authors' self-perceptions, indicating an 'ideal bias'. There is no significant correlation between the overall labor input pattern and the actual contribution pattern of papers, but a positive correlation appears between input and output for specific types of scientific contributions, reflecting a 'more effort, more gain' effect. Different types of DOL input in papers exhibit a notable co-occurrence trend. However, once the paper reaches the dissemination stage, the co-occurrence of different types of actual contributions becomes weaker, indicating that a paper's contributions are often focused on a single type.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13133v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liyue Chen, Jielan Ding, Donghuan Song, Zihao Qu</dc:creator>
    </item>
    <item>
      <title>Latent Image and Video Resolution Prediction using Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2410.13227</link>
      <description>arXiv:2410.13227v1 Announce Type: cross 
Abstract: This paper introduces a Video Quality Assessment (VQA) problem that has received little attention in the literature, called the latent resolution prediction problem. The problem arises when images or videos are upscaled from their native resolution and are reported as having a higher resolution than their native resolution. This paper formulates the problem, constructs a dataset for training and evaluation, and introduces several machine learning algorithms, including two Convolutional Neural Networks (CNNs), to address this problem. Experiments indicate that some proposed methods can predict the latent video resolution with about 95% accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13227v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rittwika Kansabanik, Adrian Barbu</dc:creator>
    </item>
    <item>
      <title>Fair comparisons of causal parameters with many treatments and positivity violations</title>
      <link>https://arxiv.org/abs/2410.13522</link>
      <description>arXiv:2410.13522v1 Announce Type: cross 
Abstract: Comparing outcomes across treatments is essential in medicine and public policy. To do so, researchers typically estimate a set of parameters, possibly counterfactual, with each targeting a different treatment. Treatment-specific means (TSMs) are commonly used, but their identification requires a positivity assumption -- that every subject has a non-zero probability of receiving each treatment. This assumption is often implausible, especially when treatment can take many values. Causal parameters based on dynamic stochastic interventions can be robust to positivity violations. However, comparing these parameters may be unfair because they may depend on outcomes under non-target treatments. To address this, and clarify when fair comparisons are possible, we propose a fairness criterion: if the conditional TSM for one treatment is greater than that for another, then the corresponding causal parameter should also be greater. We derive two intuitive properties equivalent to this criterion and show that only a mild positivity assumption is needed to identify fair parameters. We then provide examples that satisfy this criterion and are identifiable under the milder positivity assumption. These parameters are non-smooth, making standard nonparametric efficiency theory inapplicable, so we propose smooth approximations of them. We then develop doubly robust-style estimators that attain parametric convergence rates under nonparametric conditions. We illustrate our methods with an analysis of dialysis providers in New York State.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13522v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Yiting Li, Sunjae Bae, Mara A. McAdams-DeMarco, Iv\'an D\'iaz, Wenbo Wu</dc:creator>
    </item>
    <item>
      <title>A multiscale method for data collected from network edges via the line graph</title>
      <link>https://arxiv.org/abs/2410.13693</link>
      <description>arXiv:2410.13693v1 Announce Type: cross 
Abstract: Data collected over networks can be modelled as noisy observations of an unknown function over the nodes of a graph or network structure, fully described by its nodes and their connections, the edges. In this context, function estimation has been proposed in the literature and typically makes use of the network topology such as relative node arrangement, often using given or artificially constructed node Euclidean coordinates. However, networks that arise in fields such as hydrology (for example, river networks) present features that challenge these established modelling setups since the target function may naturally live on edges (e.g., river flow) and/or the node-oriented modelling uses noisy edge data as weights. This work tackles these challenges and develops a novel lifting scheme along with its associated (second) generation wavelets that permit data decomposition across the network edges. The transform, which we refer to under the acronym LG-LOCAAT, makes use of a line graph construction that first maps the data in the line graph domain. We thoroughly investigate the proposed algorithm's properties and illustrate its performance versus existing methodologies. We conclude with an application pertaining to hydrology that involves the denoising of a water quality index over the England river network, backed up by a simulation study for a river flow dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13693v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingjia Cao, Marina I. Knight, Guy P. Nason</dc:creator>
    </item>
    <item>
      <title>Assessing the Optimistic Bias in the Natural Inflow Forecasts: A Call for Model Monitoring in Brazil</title>
      <link>https://arxiv.org/abs/2410.13763</link>
      <description>arXiv:2410.13763v1 Announce Type: cross 
Abstract: Hydroelectricity accounted for roughly 66% of the total generation in Brazil in 2023 and addressed most of the intermittency of wind and solar generation. Thus, one of the most important steps in the operation planning of this country is the forecast of the natural inflow energy (NIE) time series, an approximation of the energetic value of the water inflows. To manage water resources over time, the Brazilian system operator performs long-term forecasts for the NIE to assess the water values through long-term hydrothermal planning models, which are then used to define the short-term merit order in day-ahead scheduling. Therefore, monitoring optimistic bias in NIE forecasts is crucial to prevent an optimistic view of future system conditions and subsequent riskier storage policies. In this article, we investigate and showcase strong evidence of an optimistic bias in the official NIE forecasts, with predicted values consistently exceeding the observations over the past 12 years in the two main subsystems (Southeast and Northeast). Rolling window out-of-sample tests conducted with real data demonstrate that the official forecast model exhibits a statistically significant bias of 6%, 13%, 18%, and 23% for 1, 6, 12, and 24 steps ahead in the Southeast subsystem, and 19%, 57%, 80%, and 108% in the Northeast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13763v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arthur Brigatto, Alexandre Street, Cristiano Fernandes, Davi Valladao, Guilherme Bodin, Joaquim Dias Garcia</dc:creator>
    </item>
    <item>
      <title>Evaluating causal effects on time-to-event outcomes in an RCT in Oncology with treatment discontinuation</title>
      <link>https://arxiv.org/abs/2310.06653</link>
      <description>arXiv:2310.06653v2 Announce Type: replace 
Abstract: In clinical trials, patients may discontinue treatments prematurely, breaking the initial randomization and, thus, challenging inference. Stakeholders in drug development are generally interested in going beyond the Intention-To-Treat (ITT) analysis, which provides valid causal estimates of the effect of treatment assignment but does not inform on the effect of the actual treatment receipt. Our study is motivated by an RCT in oncology, where patients assigned the investigational treatment may discontinue it due to adverse events. We propose adopting a principal stratum strategy and decomposing the overall ITT effect into principal causal effects for groups of patients defined by their potential discontinuation behavior. We first show how to implement a principal stratum strategy to assess causal effects on a survival outcome in the presence of continuous time treatment discontinuation, its advantages, and the conclusions one can draw. Our strategy deals with the time-to-event intermediate variable that may not be defined for patients who would not discontinue; moreover, discontinuation time and the primary endpoint are subject to censoring. We employ a flexible model-based Bayesian approach to tackle these complexities, providing easily interpretable results. We apply this Bayesian principal stratification framework to analyze synthetic data of the motivating oncology trial. We simulate data under different assumptions that reflect real scenarios where patients' behavior depends on critical baseline covariates. Supported by a simulation study, we shed light on the role of covariates in this framework: beyond making structural and parametric assumptions more credible, they lead to more precise inference and can be used to characterize patients' discontinuation behavior, which could help inform clinical practice and future protocols.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06653v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Veronica Ballerini, Bj\"orn Bornkamp, Alessandra Mattei, Fabrizia Mealli, Craig Wang, Yufen Zhang</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman Inversion for Geothermal Reservoir Modelling</title>
      <link>https://arxiv.org/abs/2410.09017</link>
      <description>arXiv:2410.09017v3 Announce Type: replace 
Abstract: Numerical models of geothermal reservoirs typically depend on hundreds or thousands of unknown parameters, which must be estimated using sparse, noisy data. However, these models capture complex physical processes, which frequently results in long run-times and simulation failures, making the process of estimating the unknown parameters a challenging task. Conventional techniques for parameter estimation and uncertainty quantification, such as Markov chain Monte Carlo (MCMC), can require tens of thousands of simulations to provide accurate results and are therefore challenging to apply in this context. In this paper, we study the ensemble Kalman inversion (EKI) algorithm as an alternative technique for approximate parameter estimation and uncertainty quantification for geothermal reservoir models. EKI possesses several characteristics that make it well-suited to a geothermal setting; it is derivative-free, parallelisable, robust to simulation failures, and requires far fewer simulations than conventional uncertainty quantification techniques such as MCMC. We illustrate the use of EKI in a reservoir modelling context using a combination of synthetic and real-world case studies. Through these case studies, we also demonstrate how EKI can be paired with flexible parametrisation techniques capable of accurately representing prior knowledge of the characteristics of a reservoir and adhering to geological constraints, and how the algorithm can be made robust to simulation failures. Our results demonstrate that EKI provides a reliable and efficient means of obtaining accurate parameter estimates for large-scale, two-phase geothermal reservoir models, with appropriate characterisation of uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09017v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex de Beer, Elvar K Bjarkason, Michael Gravatt, Ruanui Nicholson, John P O'Sullivan, Michael J O'Sullivan, Oliver J Maclaren</dc:creator>
    </item>
    <item>
      <title>Dynamic Topic Language Model on Heterogeneous Children's Mental Health Clinical Notes</title>
      <link>https://arxiv.org/abs/2312.14180</link>
      <description>arXiv:2312.14180v2 Announce Type: replace-cross 
Abstract: Mental health diseases affect children's lives and well-beings which have received increased attention since the COVID-19 pandemic. Analyzing psychiatric clinical notes with topic models is critical to evaluating children's mental status over time. However, few topic models are built for longitudinal settings, and most existing approaches fail to capture temporal trajectories for each document. To address these challenges, we develop a dynamic topic model with consistent topics and individualized temporal dependencies on the evolving document metadata. Our model preserves the semantic meaning of discovered topics over time and incorporates heterogeneity among documents. In particular, when documents can be categorized, we propose a classifier-free approach to maximize topic heterogeneity across different document groups. We also present an efficient variational optimization procedure adapted for the multistage longitudinal setting. In this case study, we apply our method to the psychiatric clinical notes from a large tertiary pediatric hospital in Southern California and achieve a 38% increase in the overall coherence of extracted topics. Our real data analysis reveals that children tend to express more negative emotions during state shutdowns and more positive when schools reopen. Furthermore, it suggests that sexual and gender minority (SGM) children display more pronounced reactions to major COVID-19 events and a greater sensitivity to vaccine-related news than non-SGM children. This study examines children's mental health progression during the pandemic and offers clinicians valuable insights to recognize disparities in children's mental health related to their sexual and gender identities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14180v2</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanwen Ye, Tatiana Moreno, Adrianne Alpern, Louis Ehwerhemuepha, Annie Qu</dc:creator>
    </item>
    <item>
      <title>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</title>
      <link>https://arxiv.org/abs/2410.12690</link>
      <description>arXiv:2410.12690v2 Announce Type: replace-cross 
Abstract: A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12690v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 18 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinming Wang, Simon Mak, John Miller, Jianguo Wu</dc:creator>
    </item>
  </channel>
</rss>
