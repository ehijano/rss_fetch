<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:21:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Self-Normalization for CUSUM-based Change Detection in Locally Stationary Time Series</title>
      <link>https://arxiv.org/abs/2509.07112</link>
      <description>arXiv:2509.07112v1 Announce Type: new 
Abstract: A novel self-normalization procedure for CUSUM-based change detection in the mean of a locally stationary time series is introduced. Classical self-normalization relies on the factorization of a constant long-run variance and a stochastic factor. In this case, the CUSUM statistic can be divided by another statistic proportional to the long-run variance, so that the latter cancels. Thereby, a tedious estimation of the long-run variance can be avoided. Under local stationarity, the partial sum process converges to $\int_0^t \sigma(x) d B_x$ and no such factorization is possible. To overcome this obstacle, a self-normalized test statistic is constructed from a carefully designed bivariate partial-sum process. Weak convergence of the process is proven, and it is shown that the resulting self-normalized test attains asymptotic level $\alpha$ under the null hypothesis of no change, while being consistent against a broad class of alternatives. Extensive simulations demonstrate better finite-sample properties compared to existing methods. Applications to real data illustrate the method's practical effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07112v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Heinrichs</dc:creator>
    </item>
    <item>
      <title>On the exact region between Chatterjee's rank correlation and Spearman's footrule</title>
      <link>https://arxiv.org/abs/2509.07232</link>
      <description>arXiv:2509.07232v1 Announce Type: new 
Abstract: Chatterjee's rank correlation \(\xi\) has emerged as a popular measure quantifying the strength of directed functional dependence between random variables $X$ and $Y$. If $X$ and $Y$ are continuous, $\xi$ equals Spearman's footrule~\(\psi\) for the Markov product of the copula induced by $(X,Y)$ and its transpose. We analyze the relationship between these two measures more in depth by studying the attainable region of possible pairs \((\xi, \psi)\) over all bivariate copulas. In particular, we show that for given $\xi$, the maximal possible value of $\psi$ is uniquely attained by a Fr\'echet copula. As a by-product of this and a known result for Markov products of copulas, we obtain that \(\xi\le\psi\le \sqrt{\xi}\) characterizes the exact region of stochastically increasing copulas. Regarding the minimal possible value of \(\psi\) for given \(\xi\), we give a lower bound based on Jensen's inequality and construct a two-parameter copula family that comes comparably close.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07232v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcus Rockel</dc:creator>
    </item>
    <item>
      <title>A novel statistical workflow for nonstationary modelling of successive Fr\'{e}chet extremes</title>
      <link>https://arxiv.org/abs/2509.07296</link>
      <description>arXiv:2509.07296v1 Announce Type: new 
Abstract: Accurate estimation of the frequency and magnitude of successive extreme events in energy demand is critical for strategic resource planning. Traditional approaches based on extreme value theory (EVT) are typically limited to modelling isolated extreme events and struggle to capture the dynamics of temporally clustered extremes, such as those driven by prolonged extreme weather events. These limitations are exacerbated by the scarcity of historical data and computational costs of longrun simulations leading to high uncertainty in return level estimates for successive extremes. Here, we introduce a novel statistical framework leveraging recent theoretical advances in successive extreme value modelling in dynamical systems. Under reasonable assumptions of the time series data (e.g. the data follow a fat-tailed Fr\'{e}chet distribution), our tool allows for significantly more robust estimates of returns and magnitudes of successive extreme events compared to standard likelihood methods. We illustrate our statistical workflow on scenarios of forecasted gas supply levels from 2025 to 2050. Common measures of statistical accuracy are provided as benchmarks for comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07296v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grace Burtenshaw, Joe Lane, Meagan Carney</dc:creator>
    </item>
    <item>
      <title>Matrix-variate integer-valued autoregressive processes</title>
      <link>https://arxiv.org/abs/2509.07347</link>
      <description>arXiv:2509.07347v1 Announce Type: new 
Abstract: In the fields of sociology and economics, the modeling of matrix-variate integervalued time series is urgent. However, no prior studies have addressed the modeling of such data. To address this topic, this paper proposes a novel matrix-variate integer-valued autoregressive model. The key techniques lie in defining two leftand right-matricial thinning operators. The probabilistic and statistical properties of the proposed model are investigated. Furthermore, two estimation methods are developed: projection estimation and iterative least squares estimation. The corresponding asymptotic properties of these estimators are established. Additionally, the order-determination problem is addressed. In the simulation studies, the estimation results are given and the theoretical properties are verified. Finally, it is shown that the matrix-variate integer-valued autoregressive model is superior to the continuous matrix-variate autoregressive and multivariate integer-valued autoregressive models for matrix-variate integer-valued time series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07347v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuo Xu, Kai Yang, Fukang Zhu</dc:creator>
    </item>
    <item>
      <title>Besting Good--Turing: Optimality of Non-Parametric Maximum Likelihood for Distribution Estimation</title>
      <link>https://arxiv.org/abs/2509.07355</link>
      <description>arXiv:2509.07355v1 Announce Type: new 
Abstract: When faced with a small sample from a large universe of possible outcomes, scientists often turn to the venerable Good--Turing estimator. Despite its pedigree, however, this estimator comes with considerable drawbacks, such as the need to hand-tune smoothing parameters and the lack of a precise optimality guarantee. We introduce a parameter-free estimator that bests Good--Turing in both theory and practice. Our method marries two classic ideas, namely Robbins's empirical Bayes and Kiefer--Wolfowitz non-parametric maximum likelihood estimation (NPMLE), to learn an implicit prior from data and then convert it into probability estimates. We prove that the resulting estimator attains the optimal instance-wise risk up to logarithmic factors in the competitive framework of Orlitsky and Suresh, and that the Good--Turing estimator is strictly suboptimal in the same framework. Our simulations on synthetic data and experiments with English corpora and U.S. Census data show that our estimator consistently outperforms both the Good--Turing estimator and explicit Bayes procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07355v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjun Han, Jonathan Niles-Weed, Yandi Shen, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>Bayesian inference with Besov-Laplace priors for spatially inhomogeneous binary classification surfaces</title>
      <link>https://arxiv.org/abs/2509.07439</link>
      <description>arXiv:2509.07439v1 Announce Type: new 
Abstract: In this article, we study the binary classification problem with supervised data, in the case where the covariate-to-probability-of-success map is possibly spatially inhomogeneous. We devise nonparametric Bayesian procedures with Besov-Laplace priors, which are prior distributions on function spaces routinely used in imaging and inverse problems in view of their useful edge-preserving and sparsity-promoting properties. Building on a recent line of work in the literature, we investigate the theoretical asymptotic recovery properties of the associated posterior distributions, and show that suitably tuned Besov-Laplace priors lead to minimax-optimal posterior contraction rates as the sample size increases, under the frequentist assumption that the data have been generated by a spatially inhomogeneous ground truth belonging to a Besov space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07439v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano</dc:creator>
    </item>
    <item>
      <title>Adaptive Density Estimation Using Projection Kernels and Penalized Comparison to Overfitting</title>
      <link>https://arxiv.org/abs/2509.07800</link>
      <description>arXiv:2509.07800v1 Announce Type: new 
Abstract: In this work, we study wavelet projection estimators for density estimation, focusing on their construction from $\mathcal{S}$-regular, compactly supported wavelet bases. A key aspect of such estimators is the choice of the resolution level, which controls the balance between bias and variance. To address this, we employ the Penalized Comparison to Overfitting (PCO) method, providing a fully data-driven selection of the multiresolution level. This approach ensures both statistical accuracy and computational feasibility, while retaining the adaptability of wavelet methods. Our results establish an oracle inequality for the proposed estimator and show that it attains the optimal rate of convergence in density estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07800v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Ha Hoang, Tien Dat Nguyen, Thi Mong Ngoc Nguyen</dc:creator>
    </item>
    <item>
      <title>A unified theory of the high-dimensional Laplace approximation with application to Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2509.07952</link>
      <description>arXiv:2509.07952v1 Announce Type: new 
Abstract: The Laplace approximation (LA) to posteriors is a ubiquitous tool to simplify Bayesian computation, particularly in the high-dimensional settings arising in Bayesian inverse problems. Precisely quantifying the LA accuracy is a challenging problem in the high-dimensional regime. We develop a theory of the LA accuracy to high-dimensional posteriors which both subsumes and unifies a number of results in the literature. The primary advantage of our theory is that we introduce a new degree of flexibility, which can be used to obtain problem-specific upper bounds which are much tighter than previous "rigid" bounds. We demonstrate the theory in a prototypical example of a Bayesian inverse problem, in which this flexibility enables us to improve on prior bounds by an order of magnitude. Our optimized bounds in this setting are dimension-free, and therefore valid in arbitrarily high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07952v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anya Katsevich, Vladimir Spokoiny</dc:creator>
    </item>
    <item>
      <title>Open problems in information geometry: a discussion at FDIG 2025</title>
      <link>https://arxiv.org/abs/2509.06989</link>
      <description>arXiv:2509.06989v1 Announce Type: cross 
Abstract: Open problems in information geometry are collected and discussed in the conference ``Further Developments of Information Geometry (FDIG) 2025'' held at the University of Tokyo, Japan, from March 18 to 21, 2025.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06989v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomonari Sei, Hiroshi Matsuzoe</dc:creator>
    </item>
    <item>
      <title>Loss Functions for Detecting Outliers in Panel Data</title>
      <link>https://arxiv.org/abs/2509.07014</link>
      <description>arXiv:2509.07014v1 Announce Type: cross 
Abstract: The detection of outliers is of critical importance in the assurance of data quality. Outliers may exist in observed data or in data derived from these observed data, such as estimates and forecasts. An outlier may indicate a problem with its data generation process or may simply be a true statement about the world. Without making any distributional assumptions, this paper proposes the use of loss functions to detect these outliers in panel data. An unsigned loss function is derived axiomatically. A signed loss function is developed to account for positive and negative outliers separately. In the case of nominal time an exact parametrization of the loss function is obtained. A time-invariant loss function permits the comparison of data at multiple times on the same basis. Several examples are provided, including an example in which the outliers are classified by another variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07014v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman, Thomas Bryan</dc:creator>
    </item>
    <item>
      <title>On the edge eigenvalues of sparse random geometric graphs</title>
      <link>https://arxiv.org/abs/2509.07372</link>
      <description>arXiv:2509.07372v1 Announce Type: cross 
Abstract: In this paper, we study the edge eigenvalues of random geometric graphs (RGGs) generated by multivariate Gaussian samples in the sparse regime under a broad class of distance metrics. Previous work on edge eigenvalues under related setups has relied on methods based on integral operators or the Courant-Fischer min-max principle with interpolation. However, these approaches typically require either a dense regime or sampling distributions that are compactly supported and non-vanishing, and therefore cannot be generalized to our setting. We introduce a two-step smoothing-matching argument. First, we construct a smoothed empirical operator from the given RGG. We then match its edge eigenvalues to those of a continuum limit operator via a counting argument. We show that, after proper normalization, the first few nontrivial edge eigenvalues of the RGG converge with high probability to those of a differential operator, which can be computed explicitly through a simple second-order linear partial differential equation. To the best of our knowledge, these are the first results on the edge eigenvalues of RGGs generated from samples with unbounded support and vanishing density functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07372v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiucai Ding, Yichen Hu</dc:creator>
    </item>
    <item>
      <title>Learning Barycenters from Signature Matrices</title>
      <link>https://arxiv.org/abs/2509.07815</link>
      <description>arXiv:2509.07815v1 Announce Type: cross 
Abstract: The expected signature of a family of paths need not be a signature of a path itself. Motivated by this, we consider the notion of a Lie group barycenter introduced by Buser and Karcher to propose a barycenter on path signatures. We show that every element of the free nilpotent Lie group is a barycenter of a group sample, where all but one sample element can be fixed arbitrarily. In the case of piecewise linear paths, we study the problem of recovering an underlying path corresponding to the barycenter of signatures. We determine the minimal number of segments required to learn from signature matrices, providing explicit transformations to the associated congruence normal forms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07815v1</guid>
      <category>math.RA</category>
      <category>math.AG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Leonard Schmitz</dc:creator>
    </item>
    <item>
      <title>Optimal linear prediction with functional observations: Why you can use a simple post-dimension reduction estimator</title>
      <link>https://arxiv.org/abs/2401.06326</link>
      <description>arXiv:2401.06326v5 Announce Type: replace 
Abstract: We study the optimal linear prediction of a random function that takes values in an infinite dimensional Hilbert space. We begin by characterizing the mean square prediction error (MSPE) associated with a linear predictor and discussing the minimal achievable MSPE. This analysis reveals that, in general, there are multiple non-unique linear predictors that minimize the MSPE, and even if a unique solution exists, consistently estimating it from finite samples is generally impossible. Nevertheless, we can define asymptotically optimal linear operators whose empirical MSPEs approach the minimal achievable level as the sample size increases. We show that, interestingly, standard post-dimension reduction estimators, which have been widely used in the literature, attain such asymptotic optimality under minimal conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06326v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Won-Ki Seo</dc:creator>
    </item>
    <item>
      <title>Universality of High-Dimensional Logistic Regression and a Novel CGMT under Dependence with Applications to Data Augmentation</title>
      <link>https://arxiv.org/abs/2502.15752</link>
      <description>arXiv:2502.15752v3 Announce Type: replace 
Abstract: Over the last decade, a wave of research has characterized the exact asymptotic risk of many high-dimensional models in the proportional regime. Two foundational results have driven this progress: Gaussian universality, which shows that the asymptotic risk of estimators trained on non-Gaussian and Gaussian data is equivalent, and the convex Gaussian min-max theorem (CGMT), which characterizes the risk under Gaussian settings. However, these results rely on the assumption that the data consists of independent random vectors--an assumption that significantly limits its applicability to many practical setups. In this paper, we address this limitation by generalizing both results to the dependent setting. More precisely, we prove that Gaussian universality still holds for high-dimensional logistic regression under block dependence, $m$-dependence and special cases of mixing, and establish a novel CGMT framework that accommodates for correlation across both the covariates and observations. Using these results, we establish the impact of data augmentation, a widespread practice in deep learning, on the asymptotic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15752v3</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of Thirty Eighth Conference on Learning Theory, PMLR 291:1799-1918, 2025</arxiv:journal_reference>
      <dc:creator>Matthew Esmaili Mallory, Kevin Han Huang, Morgane Austern</dc:creator>
    </item>
    <item>
      <title>Location Characteristics of Conditional Selective Confidence Intervals via Polyhedral Methods</title>
      <link>https://arxiv.org/abs/2502.20917</link>
      <description>arXiv:2502.20917v2 Announce Type: replace 
Abstract: We examine the location properties of a conditional selective confidence interval constructed via the polyhedral method. The interval is derived from the distribution of a test statistic conditional on the event of statistical significance. For a one-sided test, its behavior depends on whether the parameter is highly or only marginally significant. In the highly significant case, the interval closely resembles the conventional confidence interval that ignores selection. By contrast, when the parameter is only marginally significant, the interval may shift far to the left of zero, potentially excluding all a priori plausible parameter values. This "location problem" does not arise if significance is determined by a two-sided test or by a one-sided test with randomized response (e.g., data carving).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20917v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andreas Dzemski, Ryo Okui, Wenjie Wang</dc:creator>
    </item>
    <item>
      <title>Two statistical problems for multivariate mixture distributions</title>
      <link>https://arxiv.org/abs/2503.12147</link>
      <description>arXiv:2503.12147v3 Announce Type: replace 
Abstract: We address two important statistical problems: that of estimating for mixtures of multivariate normal distributions and mixtures of $t$-distributions based of univariate projections, and that of measuring the agreement between two different random partitions. The results are based on an earlier work of the authors, where it was shown that mixtures of multivariate Gaussian or $t$-distributions can be distinguished by projecting them onto a certain predetermined finite set of lines, the number of lines depending only on the total number of distributions involved and on the ambient dimension. We also compare our proposal with robust versions of the expectation-maximization method EM. In each case, we present algorithms for effecting the task, and compare them with existing methods by carrying out some simulati</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12147v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Fraiman, Leonardo Moreno, Thomas Ransford</dc:creator>
    </item>
    <item>
      <title>Estimating the size of a set using cascading exclusion</title>
      <link>https://arxiv.org/abs/2508.05901</link>
      <description>arXiv:2508.05901v2 Announce Type: replace 
Abstract: Let $S$ be a finite set, and $X_1,\ldots,X_n$ an i.i.d. uniform sample from $S$. To estimate the size $|S|$, without further structure, one can wait for repeats and use the birthday problem. This requires a sample size of the order $|S|^\frac{1}{2}$. On the other hand, if $S=\{1,2,\ldots,|S|\}$, the maximum of the sample blown up by $n/(n-1)$ gives an efficient estimator based on any growing sample size. This paper gives refinements that interpolate between these extremes. A general non-asymptotic theory is developed. This includes estimating the volume of a compact convex set, the unseen species problem, and a host of testing problems that follow from the question `Is this new observation a typical pick from a large prespecified population?' We also treat regression style predictors. A general theorem gives non-parametric finite $n$ error bounds in all cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05901v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sourav Chatterjee, Persi Diaconis, Susan Holmes</dc:creator>
    </item>
    <item>
      <title>Sequential Change-point Detection for Compositional Time Series with Exogenous Variables</title>
      <link>https://arxiv.org/abs/2402.18130</link>
      <description>arXiv:2402.18130v2 Announce Type: replace-cross 
Abstract: Sequential change-point detection for time series enables us to sequentially check the hypothesis that the model still holds as more and more data are observed. It is widely used in data monitoring in practice. In this work, we consider sequential change-point detection for compositional time series, time series in which the observations are proportions. For fitting compositional time series, we propose a generalized Beta AR(1) model, which can incorporate exogenous variables upon which the time series observations are dependent. We show the compositional time series are strictly stationary and geometrically ergodic and consider maximum likelihood estimation for model parameters. We show the partial MLEs are consistent and asymptotically normal and propose a parametric sequential change-point detection method for the compositional time series model. The change-point detection method is illustrated using a time series of Covid-19 positivity rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18130v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yajun Liu, Beth Andrews</dc:creator>
    </item>
    <item>
      <title>Counterfactual Cocycles: A Framework for Robust and Coherent Counterfactual Transports</title>
      <link>https://arxiv.org/abs/2405.13844</link>
      <description>arXiv:2405.13844v3 Announce Type: replace-cross 
Abstract: Estimating joint distributions (a.k.a. couplings) over counterfactual outcomes is central to personalized decision-making and treatment risk assessment. Two emergent frameworks with identifiability guarantees are: (i) bijective structural causal models (SCMs), which are flexible but brittle to mis-specified latent noise; and (ii) optimal-transport (OT) methods, which avoid latent noise assumptions but can produce incoherent counterfactual transports which fail to identify higher-order couplings. In this work, we bridge the gap with \emph{counterfactual cocycles}: a framework for counterfactual transports that use algebraic structure to provide coherence and identifiability guarantees. Every counterfactual cocycle corresponds to an equivalence class of SCMs, however the cocycle is invariant to the latent noise distribution, enabling us to sidestep various mis-specification problems. We characterize the structure of all identifiable counterfactual cocycles; propose flexible model parameterizations; introduce a novel cocycle estimator that avoids any distributional assumptions; and derive mis-specification robustness properties of the resulting counterfactual inference method. We demonstrate state-of-the-art performance and noise-robustness of counterfactual cocycles across synthetic benchmarks and a 401(k) eligibility study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13844v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>Neural Operators for Forward and Inverse Potential-Density Mappings in Classical Density Functional Theory</title>
      <link>https://arxiv.org/abs/2506.06623</link>
      <description>arXiv:2506.06623v2 Announce Type: replace-cross 
Abstract: Neural operators are capable of capturing nonlinear mappings between infinite-dimensional functional spaces, offering a data-driven approach to modeling complex functional relationships in classical density functional theory (cDFT). In this work, we evaluate the performance of several neural operator architectures in learning the functional relationships between the one-body density profile $\rho(x)$, the one-body direct correlation function $c_1(x)$, and the external potential $V_{ext}(x)$ of inhomogeneous one-dimensional (1D) hard-rod fluids, using training data generated from analytical solutions of the underlying statistical-mechanical model. We compared their performance in terms of the Mean Squared Error (MSE) loss in establishing the functional relationships as well as in predicting the excess free energy across two test sets: (1) a group test set generated via random cross-validation (CV) to assess interpolation capability, and (2) a newly constructed dataset for leave-one-group CV to evaluate extrapolation performance. Our results show that FNO achieves the most accurate predictions of the excess free energy, with the squared ReLU activation function outperforming other activation choices. Among the DeepONet variants, the Residual Multiscale Convolutional Neural Network (RMSCNN) combined with a trainable Gaussian derivative kernel (GK-RMSCNN-DeepONet) demonstrates the best performance. Additionally, we applied the trained models to solve for the density profiles at various external potentials and compared the results with those obtained from the direct mapping $V_{ext} \mapsto \rho$ with neural operators, as well as with Gaussian Process Regression (GPR) combined with Active Learning by Error Control (ALEC), which has shown strong performance in previous studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06623v2</guid>
      <category>physics.chem-ph</category>
      <category>math.ST</category>
      <category>physics.comp-ph</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runtong Pan, Xinyi Fang, Kamyar Azizzadenesheli, Miguel Liu-Schiaffini, Mengyang Gu, Jianzhong Wu</dc:creator>
    </item>
  </channel>
</rss>
