<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Nov 2025 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Words Matter: Forecasting Economic Downside Risks with Corporate Textual Data</title>
      <link>https://arxiv.org/abs/2511.04935</link>
      <description>arXiv:2511.04935v1 Announce Type: new 
Abstract: Accurate forecasting of downside risks to economic growth is critically important for policymakers and financial institutions, particularly in the wake of recent economic crises. This paper extends the Growth-at-Risk (GaR) approach by introducing a novel daily sentiment indicator derived from textual analysis of mandatory corporate disclosures (SEC 10-K and 10-Q reports) to forecast downside risks to economic growth. Using the Loughran--McDonald dictionary and a word-count methodology, I compute firm-level tone growth as the year-over-year difference between positive and negative sentiment expressed in corporate filings. These firm-specific sentiment metrics are aggregated into a weekly tone index, weighted by firms' market capitalizations to capture broader, economy-wide sentiment dynamics. Integrated into a mixed-data sampling (MIDAS) quantile regression framework, this sentiment-based indicator enhances the prediction of GDP growth downturns, outperforming traditional financial market indicators such as the National Financial Conditions Index (NFCI). The findings underscore corporate textual data as a powerful and timely resource for macroeconomic risk assessment and informed policymaking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04935v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cansu Isler</dc:creator>
    </item>
    <item>
      <title>Training and Testing with Multiple Splits: A Central Limit Theorem for Split-Sample Estimators</title>
      <link>https://arxiv.org/abs/2511.04957</link>
      <description>arXiv:2511.04957v1 Announce Type: new 
Abstract: As predictive algorithms grow in popularity, using the same dataset to both train and test a new model has become routine across research, policy, and industry. Sample-splitting attains valid inference on model properties by using separate subsamples to estimate the model and to evaluate it. However, this approach has two drawbacks, since each task uses only part of the data, and different splits can lead to widely different estimates. Averaging across multiple splits, I develop an inference approach that uses more data for training, uses the entire sample for testing, and improves reproducibility. I address the statistical dependence from reusing observations across splits by proving a new central limit theorem for a large class of split-sample estimators under arguably mild and general conditions. Importantly, I make no restrictions on model complexity or convergence rates. I show that confidence intervals based on the normal approximation are valid for many applications, but may undercover in important cases of interest, such as comparing the performance between two models. I develop a new inference approach for such cases, explicitly accounting for the dependence across splits. Moreover, I provide a measure of reproducibility for p-values obtained from split-sample estimators. Finally, I apply my results to two important problems in development and public economics: predicting poverty and learning heterogeneous treatment effects in randomized experiments. I show that my inference approach with repeated cross-fitting achieves better power than previous alternatives, often enough to find statistical significance that would otherwise be missed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04957v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Fava</dc:creator>
    </item>
    <item>
      <title>Do Test Scores Help Teachers Give Better Track Advice to Students? A Principal Stratification Analysis</title>
      <link>https://arxiv.org/abs/2511.05128</link>
      <description>arXiv:2511.05128v1 Announce Type: new 
Abstract: We study whether access to standardized test scores improves the quality of teachers' secondary school track recommendations, using Dutch data and a metric based on Principal Stratification in a quasi-randomized setting. Allowing teachers to revise their recommendations when test results exceed expectations increases the share of students successfully placed in more demanding tracks by at least 6%, but misplaces 7% of weaker students. However, only implausibly high weights on the short-term losses of students who must change track because of misplacement would justify prohibiting test-score-based upgrades. Access to test scores also induces fairer recommendations for immigrant and low-SES students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.05128v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Ichino, Fabrizia Mealli, Javier Viviens</dc:creator>
    </item>
    <item>
      <title>Externally Valid Policy Choice</title>
      <link>https://arxiv.org/abs/2205.05561</link>
      <description>arXiv:2205.05561v4 Announce Type: replace 
Abstract: We consider the problem of estimating personalized treatment policies that are "externally valid" or "generalizable": they perform well in target populations that differ from the experimental (or training) population from which the data are sampled. We first show that welfare-maximizing policies for the experimental population are robust to a certain class of shifts in the distribution of potential outcomes between the experimental and target populations (holding characteristics fixed). We then develop methods for estimating policies that are robust to shifts in the joint distribution of outcomes and characteristics. In doing so, we highlight how treatment effect heterogeneity within the experimental population shapes external validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.05561v4</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Adjaho, Timothy Christensen</dc:creator>
    </item>
    <item>
      <title>Asymptotically Unbiased Synthetic Control Methods by Moment Matching</title>
      <link>https://arxiv.org/abs/2307.11127</link>
      <description>arXiv:2307.11127v5 Announce Type: replace 
Abstract: Synthetic Control Methods (SCMs) have become a fundamental tool for comparative case studies. The core idea behind SCMs is to estimate treatment effects by predicting counterfactual outcomes for a treated unit using a weighted combination of observed outcomes from untreated units. The accuracy of these predictions is crucial for evaluating the treatment effect of a policy intervention. Subsequent research has therefore focused on estimating SC weights. In this study, we highlight a key endogeneity issue in existing SCMs-namely, the correlation between the outcomes of untreated units and the error term of the synthetic control, which leads to bias in both counterfactual outcome prediction and treatment effect estimation. To address this issue, we propose a novel SCM based on moment matching, assuming that the outcome distribution of the treated unit can be approximated by a weighted mixture of the distributions of untreated units. Under this assumption, we estimate SC weights by matching the moments of the treated outcomes with the weighted sum of the moments of the untreated outcomes. Our method offers three advantages: first, under the mixture model assumption, our estimator is asymptotically unbiased; second, this asymptotic unbiasedness reduces the mean squared error in counterfactual predictions; and third, our method provides full distributions of the treatment effect rather than just expected values, thereby broadening the applicability of SCMs. Finally, we present experimental results that demonstrate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.11127v5</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato, Akari Ohda</dc:creator>
    </item>
    <item>
      <title>Probabilistic Targeted Factor Analysis</title>
      <link>https://arxiv.org/abs/2412.06688</link>
      <description>arXiv:2412.06688v4 Announce Type: replace 
Abstract: We develop a probabilistic variant of Partial Least Squares (PLS) we call Probabilistic Targeted Factor Analysis (PTFA), which can be used to extract common factors in predictors that are useful to predict a set of predetermined target variables. Along with the technique, we provide an efficient expectation-maximization (EM) algorithm to learn the parameters and forecast the targets of interest. We develop a number of extensions to missing-at-random data, stochastic volatility, factor dynamics, and mixed-frequency data for real-time forecasting. In a simulation exercise, we show that PTFA outperforms PLS at recovering the common underlying factors affecting both features and target variables delivering better in-sample fit, and providing valid forecasts under contamination such as measurement error or outliers. Finally, we provide three applications in Economics and Finance where PTFA outperforms compared with PLS and Principal Component Analysis (PCA) at out-of-sample forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06688v4</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Miguel C. Herculano, Santiago Montoya-Bland\'on</dc:creator>
    </item>
    <item>
      <title>Latent Variable Modelling by Supervised Diffusion</title>
      <link>https://arxiv.org/abs/2506.04488</link>
      <description>arXiv:2506.04488v3 Announce Type: replace 
Abstract: The main contribution of this paper is threefold. First, a method of supervised diffusion is proposed for estimating latent variable models. Second, a fixed point solution is derived for a linear supervised diffusion model called LARX -- a superset of the ubiquitous ARX model in which all variables can be latent. Third, a minor contribution is made to the field of matrix calculus: A new matrix operator is defined and applied to solve a class of Lagrangian optimisation problems with interactions between multiple coefficient vectors subject to case-by-case constraints.
  In the empirical section, the LARX methodology is used to re-examine the relationship between stock market performance and real economic activity in the United States. The LARX model attains an out-of-sample R-squared of 79.7% compared to 50.3% for the baseline OLS model. New evidence is found that sector rotations are as useful for forecasting real economic activity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04488v3</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniil Bargman</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment in Randomized Experiments Motivated by Higher-Order Influence Functions</title>
      <link>https://arxiv.org/abs/2411.08491</link>
      <description>arXiv:2411.08491v3 Announce Type: replace-cross 
Abstract: Higher-Order Influence Functions (HOIF), developed in a series of papers over the past twenty years, are a fundamental theoretical device for constructing rate-optimal causal-effect estimators from observational studies. However, the value of HOIF for analyzing well-conducted randomized controlled trials (RCT) has not been explicitly explored. In the recent U.S. Food and Drug Administration and European Medicines Agency guidelines on the practice of covariate adjustment in analyzing RCT, in addition to the simple, unadjusted difference-in-mean estimator, it was also recommended to report the estimator adjusting for baseline covariates via a simple parametric working model, such as a linear model. However, when the number of baseline covariates $p$ is large, the recommendation is somewhat murky. In this paper, we show that HOIF-motivated estimators for the treatment-specific mean have significantly improved statistical properties compared to popular adjusted estimators in practice when $p$ is relatively large relative to the sample size $n$. We also characterize the conditions under which the HOIF-motivated estimator improves upon the unadjusted one. More importantly, we demonstrate that several state-of-the-art adjusted estimators proposed recently can be interpreted as particular HOIF-motivated estimators, thereby placing these estimators in a more unified framework. Numerical and empirical studies are conducted to corroborate our theoretical findings. An accompanying R package can be found on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08491v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sihui Zhao, Xinbo Wang, Lin Liu, Xin Zhang</dc:creator>
    </item>
    <item>
      <title>It's Hard to Be Normal: The Impact of Noise on Structure-agnostic Estimation</title>
      <link>https://arxiv.org/abs/2507.02275</link>
      <description>arXiv:2507.02275v3 Announce Type: replace-cross 
Abstract: Structure-agnostic causal inference studies how well one can estimate a treatment effect given black-box machine learning estimates of nuisance functions (like the impact of confounders on treatment and outcomes). Here, we find that the answer depends in a surprising way on the distribution of the treatment noise. Focusing on the partially linear model of \citet{robinson1988root}, we first show that the widely adopted double machine learning (DML) estimator is minimax rate-optimal for Gaussian treatment noise, resolving an open problem of \citet{mackey2018orthogonal}. Meanwhile, for independent non-Gaussian treatment noise, we show that DML is always suboptimal by constructing new practical procedures with higher-order robustness to nuisance errors. These \emph{ACE} procedures use structure-agnostic cumulant estimators to achieve $r$-th order insensitivity to nuisance errors whenever the $(r+1)$-st treatment cumulant is non-zero. We complement these core results with novel minimax guarantees for binary treatments in the partially linear model. Finally, using synthetic demand estimation experiments, we demonstrate the practical benefits of our higher-order robust estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.02275v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 10 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jikai Jin, Lester Mackey, Vasilis Syrgkanis</dc:creator>
    </item>
  </channel>
</rss>
