<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 May 2025 01:29:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comparative Analysis of Weather-Based Indexes and the Actuaries Climate Index$^{TM}$ for Crop Yield Prediction</title>
      <link>https://arxiv.org/abs/2504.21143</link>
      <description>arXiv:2504.21143v1 Announce Type: new 
Abstract: Climate change poses significant challenges to agriculture sector, affecting both crop productivity and economic stability. This study investigates the utility of climate indexes in predicting crop yields under diverse climatic conditions. Using generalized statistical models and machine learning algorithms, this research compares the predictive performance of long-established weather-based indexes with the Actuaries Climate Index$^{TM}$ (ACI), a newer entrant in the field, showcasing its potential for real-time applications and broader applicability across various sectors. To enhance model reliability and address multicollinearity among weather-related variables, the study incorporates both principal component analysis and functional principal component analysis. A total of 22 models, each constructed with different explanatory variables, highlight the significant impact of wind speed and sea-level changes alongside temperature and precipitation on crop yield variability. Furthermore, the ACI exhibits strong predictive capabilities, offering valuable insights for climate risk assessment and decision-making in agriculture and insurance-related sectors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21143v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cem Yavrum, A. Sevtap Selcuk-Kestel, Jos\'e Garrido</dc:creator>
    </item>
    <item>
      <title>The role of joint utility and pragmatic reasoning in cooperative communication</title>
      <link>https://arxiv.org/abs/2504.21224</link>
      <description>arXiv:2504.21224v1 Announce Type: new 
Abstract: Humans are able to communicate in sophisticated ways with only sparse signals, especially when cooperating. Two parallel theoretical perspectives on cooperative communication emphasize pragmatic reasoning and joint utility mechanisms to help solve ambiguity. For the current study, we collected behavioral data which tested how humans select ambiguous signals in a cooperative grid world task. The results provide support for a joint utility reasoning mechanism. We then compared human strategies to predictions from Rational Speech Acts (RSA), an established model of language pragmatics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21224v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiling Yun, Stephanie Stacy, Tao Gao</dc:creator>
    </item>
    <item>
      <title>Sales predictive analysis for improving supply chain drug sample</title>
      <link>https://arxiv.org/abs/2504.21256</link>
      <description>arXiv:2504.21256v1 Announce Type: new 
Abstract: The delivery of drug samples allows increasing sales of pharmaceutical products [6]. However, we discovered some problems that can be improved in the supply chain that delivers drug samples (used for the treatment of excess glucose). Databases were integrated; then we apply data extraction and transformation; and finally we apply multiple regression analysis to explain drug sales. The first analysis evaluates the integration of regional data and the second analysis refers to data dis-aggregated by region. We identify the region with the greatest impact on sales and the impact of the delivery of drug samples in the Mexican market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21256v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.procs.2025.01.233</arxiv:DOI>
      <dc:creator>S. C. T\'ellez-Ballesteros, R. Torres-Mendoza, J. A. Marmolejo-Saucedo, R. Rodriguez-Aguilar</dc:creator>
    </item>
    <item>
      <title>Modelling the error structure in Urban Building Energy Models with a Gaussian Processbased approach</title>
      <link>https://arxiv.org/abs/2504.21407</link>
      <description>arXiv:2504.21407v1 Announce Type: new 
Abstract: Urban Building Energy Models (UBEM) support urbanscale energy decisions and have recently been applied to use cases requiring dynamic outputs like grid management. However, their predictive capability remains insufficiently addressed, limiting confidence in UBEM application when validation experiments (VE) are unavailable. This study proposes a Gaussian Process (GP)-based method to model the error structure of UBEM, involving: (1) creating a training dataset mapping VE conditions to validation errors, (2) fitting a GP model, and (3) using cross-validation to assess prediction accuracy and uncertainty while extrapolating to unknown scenarios. Applied to the Blagnac (France) district heating network with the UBEM DIMOSIM, GP models effectively capture the inherent structure of UBEM error and uncertainties. Results reveal relationships between model performance and application conditions (e.g., load variation and weather), and show great potential in estimating within-domain model error and extrapolating beyond the validation domain. Key Innovations: $\bullet$ Use GP based approach to quantify the error structure of a UBEM, $\bullet$ Extrapolate the UBEM predictive capability to unvalidated buildings, $\bullet$ Discover different interaction patterns between validation experiment (VE) conditions and UBEM performance. Practical Implications:This paper allows UBEM developpers to have a more comprehensive view of UBEM performance, and helps practionners to prioritise measurement campaigns and to better design further validation experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21407v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunxiao Wang (CSTB, CEEP), Bruno Duplessis (CEEP), Eric Peirano (CSTB), Pascal Schetelat (CSTB), Peter Riederer (CSTB)</dc:creator>
    </item>
    <item>
      <title>Assessing Racial Disparities in Healthcare Expenditures Using Causal Path-Specific Effects</title>
      <link>https://arxiv.org/abs/2504.21688</link>
      <description>arXiv:2504.21688v1 Announce Type: new 
Abstract: Racial disparities in healthcare expenditures are well-documented, yet the underlying drivers remain complex and require further investigation. This study employs causal and counterfactual path-specific effects to quantify how various factors, including socioeconomic status, insurance access, health behaviors, and health status, mediate these disparities. Using data from the Medical Expenditures Panel Survey, we estimate how expenditures would differ under counterfactual scenarios in which the values of specific mediators were aligned across racial groups along selected causal pathways. A key challenge in this analysis is ensuring robustness against model misspecification while addressing the zero-inflation and right-skewness of healthcare expenditures. For reliable inference, we derive asymptotically linear estimators by integrating influence function-based techniques with flexible machine learning methods, including super learners and a two-part model tailored to the zero-inflated, right-skewed nature of healthcare expenditures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21688v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaxian Ou, Xinwei He, David Benkeser, Razieh Nabi</dc:creator>
    </item>
    <item>
      <title>Is Stephen Curry really a guard? New perspective on players typology using functional data analysis</title>
      <link>https://arxiv.org/abs/2504.21761</link>
      <description>arXiv:2504.21761v1 Announce Type: new 
Abstract: We present a novel representation of NBA players' shooting patterns based on Functional Data Analysis (FDA). Each player's charts of made and missed shots are treated as smooth functional data defined over a two-dimensional domain corresponding to the offensive half-court. This continuous representation enables a parsimonious multivariate functional principal components analysis (MFPCA) decomposition, producing a set of common principal component functions that capture the primary modes of variability in shooting patterns, along with player-specific scores that quantify individual deviations from the average behavior. We first interpret the principal component functions to characterize the main sources of variation in shooting tendencies. We then apply $k$-medoids clustering to the principal component scores to construct a data-driven taxonomy of players. Comparing our empirical clusters to conventional NBA position labels reveals low agreement, suggesting that our shooting-pattern representation might capture aspects of playing style not fully reflected in official designations. The proposed methodology provides a flexible, interpretable, and continuous framework for analyzing player tendencies, with potential applications in coaching, scouting, and historical player or match comparisons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21761v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Golovkine, Edward Gunning</dc:creator>
    </item>
    <item>
      <title>A Hybrid Mixture of $t$-Factor Analyzers for Clustering High-dimensional Data</title>
      <link>https://arxiv.org/abs/2504.21120</link>
      <description>arXiv:2504.21120v1 Announce Type: cross 
Abstract: This paper develops a novel hybrid approach for estimating the mixture model of $t$-factor analyzers (MtFA) that employs multivariate $t$-distribution and factor model to cluster and characterize grouped data. The traditional estimation method for MtFA faces computational challenges, particularly in high-dimensional settings, where the eigendecomposition of large covariance matrices and the iterative nature of Expectation-Maximization (EM) algorithms lead to scalability issues. We propose a computational scheme that integrates a profile likelihood method into the EM framework to efficiently obtain the model parameter estimates. The effectiveness of our approach is demonstrated through simulations showcasing its superior computational efficiency compared to the existing method, while preserving clustering accuracy and resilience against outliers. Our method is applied to cluster the Gamma-ray bursts, reinforcing several claims in the literature that Gamma-ray bursts have heterogeneous subpopulations and providing characterizations of the estimated groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21120v1</guid>
      <category>stat.ME</category>
      <category>astro-ph.HE</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazeem Kareem, Fan Dai</dc:creator>
    </item>
    <item>
      <title>Composite Safety Potential Field for Highway Driving Risk Assessment</title>
      <link>https://arxiv.org/abs/2504.21158</link>
      <description>arXiv:2504.21158v1 Announce Type: cross 
Abstract: In the era of rapid advancements in vehicle safety technologies, driving risk assessment has become a focal point of attention. Technologies such as collision warning systems, advanced driver assistance systems (ADAS), and autonomous driving require driving risks to be evaluated proactively and in real time. To be effective, driving risk assessment metrics must not only accurately identify potential collisions but also exhibit human-like reasoning to enable safe and seamless interactions between vehicles. Existing safety potential field models assess driving risks by considering both objective and subjective safety factors. However, their practical applicability in real-world risk assessment tasks is limited. These models are often challenging to calibrate due to the arbitrary nature of their structures, and calibration can be inefficient because of the scarcity of accident statistics. Additionally, they struggle to generalize across both longitudinal and lateral risks. To address these challenges, we propose a composite safety potential field framework, namely C-SPF, involving a subjective field to capture drivers' risk perception about spatial proximity and an objective field to quantify the imminent collision probability, to comprehensively evaluate driving risks. The C-SPF is calibrated using abundant two-dimensional spacing data from trajectory datasets, enabling it to effectively capture drivers' proximity risk perception and provide a more realistic explanation of driving behaviors. Analysis of a naturalistic driving dataset demonstrates that the C-SPF can capture both longitudinal and lateral risks that trigger drivers' safety maneuvers. Further case studies highlight the C-SPF's ability to explain lateral driver behaviors, such as abandoning lane changes or adjusting lateral position relative to adjacent vehicles, which are capabilities that existing models fail to achieve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21158v1</guid>
      <category>cs.RO</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dachuan Zuo, Zilin Bian, Fan Zuo, Kaan Ozbay</dc:creator>
    </item>
    <item>
      <title>Hurdle Network Model With Latent Dynamic Shrinkage For Enhanced Edge Prediction in Zero-Inflated Directed Network Time Series</title>
      <link>https://arxiv.org/abs/2504.21275</link>
      <description>arXiv:2504.21275v1 Announce Type: cross 
Abstract: This article aims to model international trade relationships among 29 countries in the apparel industry between 1994 and 2013. Bilateral trade flows can be represented as a directed network, where nodes correspond to countries and directed edges indicate trade flows (i.e., whether one country exported to another in a given year). Additionally, node (e.g., GDP) and edge-specific (e.g., labor provision) covariates are also available. The study focuses on two key challenges: (1) capturing multiple forms of temporal and network dependence, and dependence on covariates; and (2) accounting for potential trade volume as an important but partially observed edge-specific covariate, which is only available for country pairs that engaged in trade.
  To address these challenges, we introduce the dynamic hurdle network model (Hurdle-Net) for zero-inflated directed network time series that incorporates several novel features. First, it represents the time series as a paired binary and continuous time series and utilizes a hurdle model that effectively handles sparsity in edge occurrence. Second, the model captures evolving network dependencies using node-specific latent variables governed by a dynamic shrinkage process. Third, it leverages a shared latent structure across the binary and continuous components, reflecting the fact that both networks involve the same nodes. Finally, the model employs a generalized logistic link function to relate edge occurrence to edge weight, allowing for a parsimonious and coherent hierarchical Bayesian framework that jointly models both network components. Compared to static or independent models, Hurdle-Net provides improved model selection, estimation, and prediction performance for analyzing international trade patterns. Its effectiveness is demonstrated through simulation studies and an application to bilateral trade flow data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21275v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandipan Pramanik, Raymond Robertson, Yang Ni</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis for Clustered Observational Studies with an Application to the Effectiveness of Magnet Nursing Hospitals</title>
      <link>https://arxiv.org/abs/2504.21617</link>
      <description>arXiv:2504.21617v1 Announce Type: cross 
Abstract: In a clustered observational study, treatment is assigned to groups and all units within the group are exposed to the treatment. Here, we use a clustered observational study (COS) design to estimate the effectiveness of Magnet Nursing certificates for emergency surgery patients. Recent research has introduced specialized weighting estimators for the COS design that balance baseline covariates at the unit and cluster level. These methods allow researchers to adjust for observed confounders, but are sensitive to unobserved confounding. In this paper, we develop new sensitivity analysis methods tailored to weighting estimators for COS designs. We provide several key contributions. First, we introduce a key bias decomposition, tailored to the specific confounding structure that arises in a COS. Second, we develop a sensitivity framework for weighted COS designs that constrain the error in the underlying weights. We introduce both a marginal sensitivity model and a variance-based sensitivity model, and extend both to accommodate multiple estimands. Finally, we propose amplification and benchmarking methods to better interpret the results. Throughout, we illustrate our proposed methods by analyzing the effectiveness of Magnet nursing hospitals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21617v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Melody Huang, Eli Ben-Michael, Matthew McHugh, Luke Keele</dc:creator>
    </item>
    <item>
      <title>While-alive regression analysis of composite survival endpoints</title>
      <link>https://arxiv.org/abs/2504.21710</link>
      <description>arXiv:2504.21710v2 Announce Type: cross 
Abstract: Composite endpoints, which combine two or more distinct outcomes, are frequently used in clinical trials to enhance the event rate and improve the statistical power. In the recent literature, the while-alive cumulative frequency measure offers a strong alternative to define composite survival outcomes, by relating the average event rate to the survival time. Although non-parametric methods have been proposed for two-sample comparisons between cumulative frequency measures in clinical trials, limited attention has been given to regression methods that directly address time-varying effects in while-alive measures for composite survival outcomes. Motivated by an individually randomized trial (HF-ACTION) and a cluster randomized trial (STRIDE), we address this gap by developing a regression framework for while-alive measures for composite survival outcomes that include a terminal component event. Our regression approach uses splines to model time-varying association between covariates and a while-alive loss rate of all component events, and can be applied to both independent and clustered data. We derive the asymptotic properties of the regression estimator in each setting and evaluate its performance through simulations. Finally, we apply our regression method to analyze data from the HF-ACTION individually randomized trial and the STRIDE cluster randomized trial. The proposed methods are implemented in the WAreg R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21710v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xi Fang, Hajime Uno, Fan Li</dc:creator>
    </item>
    <item>
      <title>Balancing Interpretability and Flexibility in Modeling Diagnostic Trajectories with an Embedded Neural Hawkes Process Model</title>
      <link>https://arxiv.org/abs/2504.21795</link>
      <description>arXiv:2504.21795v1 Announce Type: cross 
Abstract: The Hawkes process (HP) is commonly used to model event sequences with self-reinforcing dynamics, including electronic health records (EHRs). Traditional HPs capture self-reinforcement via parametric impact functions that can be inspected to understand how each event modulates the intensity of others. Neural network-based HPs offer greater flexibility, resulting in improved fit and prediction performance, but at the cost of interpretability, which is often critical in healthcare. In this work, we aim to understand and improve upon this tradeoff. We propose a novel HP formulation in which impact functions are modeled by defining a flexible impact kernel, instantiated as a neural network, in event embedding space, which allows us to model large-scale event sequences with many event types. This approach is more flexible than traditional HPs yet more interpretable than other neural network approaches, and allows us to explicitly trade flexibility for interpretability by adding transformer encoder layers to further contextualize the event embeddings. Results show that our method accurately recovers impact functions in simulations, achieves competitive performance on MIMIC-IV procedure dataset, and gains clinically meaningful interpretation on XX-EHR with children diagnosis dataset even without transformer layers. This suggests that our flexible impact kernel is often sufficient to capture self-reinforcing dynamics in EHRs and other data effectively, implying that interpretability can be maintained without loss of performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21795v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yuankang Zhao, Matthew Engelhard</dc:creator>
    </item>
    <item>
      <title>An Adaptive Learning Approach to Multivariate Time Forecasting in Industrial Processes</title>
      <link>https://arxiv.org/abs/2403.07554</link>
      <description>arXiv:2403.07554v2 Announce Type: replace 
Abstract: Industrial processes generate a massive amount of monitoring data that can be exploited to uncover hidden time losses in the system. This can be used to enhance the accuracy of maintenance policies and increase the effectiveness of the equipment. In this work, we propose a method for one-step probabilistic multivariate forecasting of time variables involved in a production process. The method is based on an Input-Output Hidden Markov Model (IO-HMM), in which the parameters of interest are the state transition probabilities and the parameters of the observations' joint density. The ultimate goal of the method is to predict operational process times in the near future, which enables the identification of hidden losses and the location of improvement areas in the process. The input stream in the IO-HMM model includes past values of the response variables and other process features, such as calendar variables, that can have an impact on the model's parameters. The discrete part of the IO-HMM models the operational mode of the process. The state transition probabilities are supposed to change over time and are updated using Bayesian principles. The continuous part of the IO-HMM models the joint density of the response variables. The estimate of the continuous model parameters is recursively computed through an adaptive algorithm that also admits a Bayesian interpretation. The adaptive algorithm allows for efficient updating of the current parameter estimates as soon as new information is available. We evaluate the method's performance using a real data set obtained from a company in a particular sector, and the results are compared with a collection of benchmark models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07554v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Miguelez, Josu Doncel, Maria Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>Flexible and Probabilistic Topology Tracking with Partial Optimal Transport</title>
      <link>https://arxiv.org/abs/2302.02895</link>
      <description>arXiv:2302.02895v3 Announce Type: replace-cross 
Abstract: In this paper, we present a flexible and probabilistic framework for tracking topological features in time-varying scalar fields using merge trees and partial optimal transport. Merge trees are topological descriptors that record the evolution of connected components in the sublevel sets of scalar fields. We present a new technique for modeling and comparing merge trees using tools from partial optimal transport. In particular, we model a merge tree as a measure network, that is, a network equipped with a probability distribution, and define a notion of distance on the space of merge trees inspired by partial optimal transport. Such a distance offers a new and flexible perspective for encoding intrinsic and extrinsic information in the comparative measures of merge trees. More importantly, it gives rise to a partial matching between topological features in time-varying data, thus enabling flexible topology tracking for scientific simulations. Furthermore, such partial matching may be interpreted as probabilistic coupling between features at adjacent time steps, which gives rise to probabilistic tracking graphs. We derive a stability result for our distance and provide numerous experiments indicating the efficacy of our framework in extracting meaningful feature tracks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02895v3</guid>
      <category>cs.CG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVCG.2025.3561300</arxiv:DOI>
      <dc:creator>Mingzhe Li, Xinyuan Yan, Lin Yan, Tom Needham, Bei Wang</dc:creator>
    </item>
    <item>
      <title>A comparison between copula-based, mixed model, and estimating equation methods for analysis of bivariate correlated data</title>
      <link>https://arxiv.org/abs/2410.11892</link>
      <description>arXiv:2410.11892v2 Announce Type: replace-cross 
Abstract: Regression analysis of non-normal correlated data is commonly performed using generalized linear mixed models (GLMM) and generalized estimating equations (GEE). The recent development of generalized joint regression models (GJRM) provide an alternative to these approaches by using copulas to flexibly model response variables and their dependence structures.
  This paper presents a simulation study comparing GJRM with alternative methods. We find that for the normal model with identity link, all models provide accurate estimates of marginal population parameters with comparable fit. However, for non-normal marginal distributions and when a non-identity link function is used, we highlight a major pitfall in the use of GLMMs: without significant adjustment they provide highly biased estimates of marginal population parameters. GLMM bias is more pronounced when the marginal distributions are more skewed or highly correlated. In addition, we highlight discrepancies between the estimates from different GLMM packages. In contrast, we find that GJRM provides unbiased estimates across all distributions with accurate standard errors when the copula is correctly specified. In addition, we highlight the advantages of the likelihood-based structure of the GJRM and show that it provides a model fit comparable, and often favorable to, GLMMs and GLMs. In a longitudinal study of doctor visits, we show that the GJRM provides better model fits than a comparable non-GAMLSS GLMM, GEE or GLM, due to its greater selection of marginal distributions. We conclude that the GJRM provides a superior approach to current popular models for regression of non-normal correlated data when population parameters are of interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11892v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aydin Sareff-Hibbert, Gillian Z. Heller</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Models for Multiple Raters: a General Statistical Framework</title>
      <link>https://arxiv.org/abs/2410.21498</link>
      <description>arXiv:2410.21498v3 Announce Type: replace-cross 
Abstract: Rating procedure is crucial in many applied fields (e.g., educational, clinical, emergency). It implies that a rater (e.g., teacher, doctor) rates a subject (e.g., student, doctor) on a rating scale. Given raters variability, several statistical methods have been proposed for assessing and improving the quality of ratings. Model estimation in the presence of heterogeneity has been one of the recent challenges in this research line. Consequently, several methods have been proposed to address this issue under a parametric multilevel modelling framework, in which strong distributional assumptions are made. We propose a more flexible model under the Bayesian nonparametric (BNP) framework, in which most of those assumptions are relaxed. By eliciting hierarchical discrete nonparametric priors, the model accommodates clusters among raters and subjects, naturally accounts for heterogeneity, and improves estimates accuracy. We propose a general BNP heteroscedastic framework to analyse continuous and coarse rating data and possible latent differences among subjects and raters. The estimated densities are used to make inferences about the rating process and the quality of the ratings. By exploiting a stick-breaking representation of the Dirichlet Process, a general class of Intraclass Correlation Coefficient (ICC) indices might be derived for these models. Our method allows us to independently identify latent similarities between subjects and raters and can be applied in precise education to improve personalised teaching programs or interventions. Theoretical results about the ICC are provided together with computational strategies. Simulations and a real-world application are presented, and possible future directions are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21498v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giuseppe Mignemi, Ioanna Manolopoulou</dc:creator>
    </item>
    <item>
      <title>Learning Disease Progression Models That Capture Health Disparities</title>
      <link>https://arxiv.org/abs/2412.16406</link>
      <description>arXiv:2412.16406v2 Announce Type: replace-cross 
Abstract: Disease progression models are widely used to inform the diagnosis and treatment of many progressive diseases. However, a significant limitation of existing models is that they do not account for health disparities that can bias the observed data. To address this, we develop an interpretable Bayesian disease progression model that captures three key health disparities: certain patient populations may (1) start receiving care only when their disease is more severe, (2) experience faster disease progression even while receiving care, or (3) receive follow-up care less frequently conditional on disease severity. We show theoretically and empirically that failing to account for any of these disparities can result in biased estimates of severity (e.g., underestimating severity for disadvantaged groups). On a dataset of heart failure patients, we show that our model can identify groups that face each type of health disparity, and that accounting for these disparities while inferring disease severity meaningfully shifts which patients are considered high-risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16406v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erica Chiang, Divya Shanmugam, Ashley N. Beecy, Gabriel Sayer, Deborah Estrin, Nikhil Garg, Emma Pierson</dc:creator>
    </item>
    <item>
      <title>A New Approach to Radiocarbon Summarisation: Rigorous Identification of Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a Poisson Process</title>
      <link>https://arxiv.org/abs/2501.15980</link>
      <description>arXiv:2501.15980v3 Announce Type: replace-cross 
Abstract: A commonly-used paradigm to estimate changes in the frequency of past events or the size of populations is to consider the occurrence rate of archaeological/environmental samples found at a site over time. The reliability of such a "dates-as-data" approach is highly dependent upon how the occurrence rates are estimated from the underlying samples, particularly when calendar age information for the samples is obtained from radiocarbon (14C). The most frequently used "14C-dates-as-data" approach of creating Summed Probability Distributions (SPDs) is not statistically valid, or coherent, and can provide highly misleading inference. Here, we provide an alternative method with a rigorous statistical underpinning that also provides valuable additional information on potential changepoints in the rate of events. Furthermore, unlike current SPD alternatives, our summarisation approach does not restrict users to pre-specified, rigid, summary formats (e.g., exponential or logistic growth) but instead flexibly adapts to the dates themselves. Our methodology ensures more reliable "14C-dates-as-data" analyses, allowing us to better assess and identify potential signals present. We model the occurrence of events, each assumed to leave a radiocarbon sample in the archaeological/environmental record, as an inhomogeneous Poisson process. The varying rate of samples over time is then estimated within a fully-Bayesian framework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set of radiocarbon samples, we reconstruct how their occurrence rate varies over calendar time and identify if that rate contains statistically-significant changes, i.e., specific times at which the rate of events abruptly changes. We illustrate our method with both a simulation study and a practical example concerning late-Pleistocene megafaunal population changes in Alaska and Yukon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15980v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J Heaton, Sara Al-assam, Edouard Bard</dc:creator>
    </item>
    <item>
      <title>Bhirkuti's Relative Efficiency (BRE): Examining its Performance in Psychometric Simulations</title>
      <link>https://arxiv.org/abs/2503.04775</link>
      <description>arXiv:2503.04775v3 Announce Type: replace-cross 
Abstract: Traditional Relative Efficiency (RE), based solely on variance, has limitations in evaluating estimator performance, particularly in planned missing data designs. We introduce Bhirkuti's Relative Efficiency (BRE), a novel metric that integrates precision and accuracy to provide a more robust assessment of efficiency. To compute BRE, we use interquartile range (IQR) overlap to measure precision and apply a bias adjustment factor based on the absolute median relative bias (AMRB). Monte Carlo simulations using a Latent Growth Model (LGM) with planned missing data illustrate that BRE maintains theoretically consistency and interpretability, avoiding paradoxes such as RE exceeding 100%. Visualizations via boxplots and ridgeline plots confirm that BRE provides a stable and meaningful estimator efficiency evaluation, making it a valuable advancement in psychometric and statistical modeling. By addressing fundamental weaknesses in traditional RE, BRE provides a superior, theoretically justified alternative for relative efficiency in psychometric modeling, structural equation modeling, and missing data research. This advancement enhances data-driven decision-making and offers a methodologically rigorous tool for researchers analyzing incomplete datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04775v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneel Bhusal, Todd D. Little</dc:creator>
    </item>
    <item>
      <title>A Novel Hybrid Approach Using an Attention-Based Transformer + GRU Model for Predicting Cryptocurrency Prices</title>
      <link>https://arxiv.org/abs/2504.17079</link>
      <description>arXiv:2504.17079v2 Announce Type: replace-cross 
Abstract: In this article, we introduce a novel deep learning hybrid model that integrates attention Transformer and Gated Recurrent Unit (GRU) architectures to improve the accuracy of cryptocurrency price predictions. By combining the Transformer's strength in capturing long-range patterns with the GRU's ability to model short-term and sequential trends, the hybrid model provides a well-rounded approach to time series forecasting. We apply the model to predict the daily closing prices of Bitcoin and Ethereum based on historical data that include past prices, trading volumes, and the Fear and Greed index. We evaluate the performance of our proposed model by comparing it with four other machine learning models: two are non-sequential feedforward models: Radial Basis Function Network (RBFN) and General Regression Neural Network (GRNN), and two are bidirectional sequential memory-based models: Bidirectional Long-Short-Term Memory (BiLSTM) and Bidirectional Gated Recurrent Unit (BiGRU). The performance of the model is assessed using several metrics, including Mean Squared Error (MSE), Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error (MAPE), along with statistical validation through the nonparametric Friedman test followed by a post hoc Wilcoxon signed rank test. The results demonstrate that our hybrid model consistently achieves superior accuracy, highlighting its effectiveness for financial prediction tasks. These findings provide valuable insights for improving real-time decision making in cryptocurrency markets and support the growing use of hybrid deep learning models in financial analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.17079v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esam Mahdi, C. Martin-Barreiro, X. Cabezas</dc:creator>
    </item>
  </channel>
</rss>
