<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 20 Jan 2025 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Adaptive Weighted Total Variation boosted by learning techniques in few-view tomographic imaging</title>
      <link>https://arxiv.org/abs/2501.09845</link>
      <description>arXiv:2501.09845v1 Announce Type: new 
Abstract: This study presents the development of a spatially adaptive weighting strategy for Total Variation regularization, aimed at addressing under-determined linear inverse problems. The method leverages the rapid computation of an accurate approximation of the true image (or its gradient magnitude) through a neural network. Our approach operates without requiring prior knowledge of the noise intensity in the data and avoids the iterative recomputation of weights. Additionally, the paper includes a theoretical analysis of the proposed method, establishing its validity as a regularization approach. This framework integrates advanced neural network capabilities within a regularization context, thereby making the results of the networks interpretable. The results are promising as they enable high-quality reconstructions from limited-view tomographic measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09845v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Morotti, Davide Evangelista, Andrea Sebastiani, Elena Loli Piccolomini</dc:creator>
    </item>
    <item>
      <title>Geometry-Preserving Encoder/Decoder in Latent Generative Models</title>
      <link>https://arxiv.org/abs/2501.09876</link>
      <description>arXiv:2501.09876v1 Announce Type: new 
Abstract: Generative modeling aims to generate new data samples that resemble a given dataset, with diffusion models recently becoming the most popular generative model. One of the main challenges of diffusion models is solving the problem in the input space, which tends to be very high-dimensional. Recently, solving diffusion models in the latent space through an encoder that maps from the data space to a lower-dimensional latent space has been considered to make the training process more efficient and has shown state-of-the-art results. The variational autoencoder (VAE) is the most commonly used encoder/decoder framework in this domain, known for its ability to learn latent representations and generate data samples. In this paper, we introduce a novel encoder/decoder framework with theoretical properties distinct from those of the VAE, specifically designed to preserve the geometric structure of the data distribution. We demonstrate the significant advantages of this geometry-preserving encoder in the training process of both the encoder and decoder. Additionally, we provide theoretical results proving convergence of the training process, including convergence guarantees for encoder training, and results showing faster convergence of decoder training when using the geometry-preserving encoder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09876v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wonjun Lee, Riley C. W. O'Neill, Dongmian Zou, Jeff Calder, Gilad Lerman</dc:creator>
    </item>
    <item>
      <title>On understanding and overcoming spectral biases of deep neural network learning methods for solving PDEs</title>
      <link>https://arxiv.org/abs/2501.09987</link>
      <description>arXiv:2501.09987v1 Announce Type: new 
Abstract: In this review, we survey the latest approaches and techniques developed to overcome the spectral bias towards low frequency of deep neural network learning methods in learning multiple-frequency solutions of partial differential equations. Open problems and future research directions are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09987v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi-Qin John Xu, Lulu Zhang, Wei Cai</dc:creator>
    </item>
    <item>
      <title>The optimal relaxation parameter for the SOR method applied to the Poisson equation on rectangular grids with different types of boundary conditions</title>
      <link>https://arxiv.org/abs/2501.09995</link>
      <description>arXiv:2501.09995v1 Announce Type: new 
Abstract: The Successive Over-Relaxation (SOR) method is a useful method for solving the sparse system of linear equations which arises from finite-difference discretization of the Poisson equation. Knowing the optimal value of the relaxation parameter is crucial for fast convergence. In this manuscript, we present the optimal relaxation parameter for the discretized Poisson equation with mixed and different types of boundary conditions on a rectangular grid with unequal mesh sizes in $x$- and $y$-directions ($\Delta x \neq \Delta y$) which does not addressed in the literature. The central second-order and high-order compact (HOC) schemes are considered for the discretization and the optimal relaxation parameter is obtained for both the point and line implementation of the SOR method. Furthermore, the obtained optimal parameters are verified by numerical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09995v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hossein Mahmoodi Darian</dc:creator>
    </item>
    <item>
      <title>Scaling-and-squaring method for computing the inverses of matrix $\varphi$-functions</title>
      <link>https://arxiv.org/abs/2501.10028</link>
      <description>arXiv:2501.10028v1 Announce Type: new 
Abstract: This paper aims to develop efficient numerical methods for computing the inverse of matrix $\varphi$-functions, $\psi_\ell(A) := (\varphi_\ell(A))^{-1}$, for $\ell =1,2,\ldots,$ when $A$ is a large and sparse matrix with eigenvalues in the open left half-plane. While $\varphi$-functions play a crucial role in the analysis and implementation of exponential integrators, their inverses arise in solving certain direct and inverse differential problems with non-local boundary conditions. We propose an adaptation of the standard scaling-and-squaring technique for computing $\psi_\ell(A)$, based on the Newton-Schulz iteration for matrix inversion. The convergence of this method is analyzed both theoretically and numerically. In addition, we derive and analyze Pad\'e approximants for approximating $\psi_1(A/2^s)$, where $s$ is a suitably chosen integer, necessary at the root of the squaring process. Numerical experiments demonstrate the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10028v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lidia Aceto, Luca Gemignani</dc:creator>
    </item>
    <item>
      <title>Accurate algorithms for Bessel matrices</title>
      <link>https://arxiv.org/abs/2501.10076</link>
      <description>arXiv:2501.10076v1 Announce Type: new 
Abstract: In this paper, we prove that any collocation matrix of Bessel polynomials at positive points is strictly totally positive, that is, all its minors are positive. Moreover, an accurate method to construct the bidiagonal factorization of these matrices is obtained and used to compute with high relative accuracy the eigenvalues, singular values and inverses. Similar results for the collocation matrices for the reverse Bessel polynomials are also obtained. Numerical examples illustrating the theoretical results are included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10076v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10915-019-00975-6</arxiv:DOI>
      <arxiv:journal_reference>J. Sci. Comput. 80 (2019), no. 2, 1264-1278</arxiv:journal_reference>
      <dc:creator>Jorge Delgado, H\'ector Orera, Juan Manuel Pe\~na</dc:creator>
    </item>
    <item>
      <title>Solving Random Hyperbolic Conservation Laws Using Linear Programming</title>
      <link>https://arxiv.org/abs/2501.10104</link>
      <description>arXiv:2501.10104v1 Announce Type: new 
Abstract: A novel structure-preserving numerical method to solve random hyperbolic systems of conservation laws is presented. The method uses a concept of generalized, measure-valued solutions to random conservation laws. This yields a linear partial differential equation with respect to the Young measure and allows to compute the approximation based on linear programming problems. We analyze the structure-preserving properties of the derived numerical method and discuss its advantages and disadvantages. Numerical results for one-dimensional Burgers equation and the isentropic Euler equations and comparisons with stochastic collocation method illustrate the behavior of the proposed numerical method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10104v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoshuai Chu, Michael Herty, Maria Lukacova-Medvid'ova, Yizhou Zhou</dc:creator>
    </item>
    <item>
      <title>Convex Physics Informed Neural Networks for the Monge-Amp\`ere Optimal Transport Problem</title>
      <link>https://arxiv.org/abs/2501.10162</link>
      <description>arXiv:2501.10162v1 Announce Type: new 
Abstract: Optimal transportation of raw material from suppliers to customers is an issue arising in logistics that is addressed here with a continuous model relying on optimal transport theory. A physics informed neuralnetwork method is advocated here for the solution of the corresponding generalized Monge-Amp`ere equation. Convex neural networks are advocated to enforce the convexity of the solution to the Monge-Amp\`ere equation and obtain a suitable approximation of the optimal transport map. A particular focus is set on the enforcement of transport boundary conditions in the loss function. Numerical experiments illustrate the solution to the optimal transport problem in several configurations, and sensitivity analyses are performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10162v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Caboussat, Anna Peruso</dc:creator>
    </item>
    <item>
      <title>AdagradLSPIA: Integrating adaptive optimization into least squares progressive iterative approximation</title>
      <link>https://arxiv.org/abs/2501.10170</link>
      <description>arXiv:2501.10170v1 Announce Type: new 
Abstract: This paper introduces the Adaptive Gradient Least Squares Progressive iterative Approximation (AdagradLSPIA), an accelerated version of the Least Squares Progressive Iterative Approximation (LSPIA) method, enhanced with adaptive optimization techniques inspired by the adaptive gradient (Adagrad) algorithm. By dynamically adjusting weights using historical gradient information, AdagradLSPIA achieves faster convergence compared to the standard LSPIA method. The method's effectiveness is demonstrated through tensor product B-spline surface fitting, where it consistently outperforms LSPIA in terms of accuracy, computational efficiency, and robustness to variations in global weight selection. Theoretical analysis confirms its convergence guarantees, and experimental results highlight its suitability for complex surface fitting tasks in geometric modeling and reverse engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10170v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Svaj\=unas Sajavi\v{c}ius</dc:creator>
    </item>
    <item>
      <title>Quantum simulation of a class of highly-oscillatory transport equations via Schr\"odingerisation</title>
      <link>https://arxiv.org/abs/2501.10176</link>
      <description>arXiv:2501.10176v1 Announce Type: new 
Abstract: In this paper, we present quantum algorithms for a class of highly-oscillatory transport equations, which arise in semiclassical computation of surface hopping problems and other related non-adiabatic quantum dynamics, based on the Born-Oppenheimer approximation. Our method relies on the classical nonlinear geometric optics method, and the recently developed Schr\"odingerisation approach for quantum simulation of partial differential equations. The Schr\"odingerisation technique can transform any linear ordinary and partial differential equations into Hamiltonian systems evolving under unitary dynamics, via a warped phase transformation that maps these equations to one higher dimension. We study possible paths for better recoveries of the solution to the original problem by shifting the bad eigenvalues in the Schr\"odingerized system. Our method ensures the uniform error estimates independent of the wave length, thus allowing numerical accuracy, in maximum norm, even without numerically resolving the physical oscillations. Various numerical experiments are performed to demonstrate the validity of this approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10176v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anjiao Gu, Shi Jin</dc:creator>
    </item>
    <item>
      <title>Surrogate-based multiscale analysis of experiments on thermoplastic composites under off-axis loading</title>
      <link>https://arxiv.org/abs/2501.10193</link>
      <description>arXiv:2501.10193v1 Announce Type: new 
Abstract: In this paper, we present a surrogate-based multiscale approach to model constant strain-rate and creep experiments on unidirectional thermoplastic composites under off-axis loading. In previous contributions, these experiments were modeled through a single-scale micromechanical simulation under the assumption of macroscopic homogeneity. Although efficient and accurate in many scenarios, simulations with low-off axis angles showed significant discrepancies with the experiments. It was hypothesized that the mismatch was caused by macroscopic inhomogeneity, which would require a multiscale approach to capture it. However, full-field multiscale simulations remain computationally prohibitive. To address this issue, we replace the micromodel with a Physically Recurrent Neural Network (PRNN), a surrogate model that combines data-driven components with embedded constitutive models to capture history-dependent behavior naturally. The explainability of the latent space of this network is also explored in a transfer learning strategy that requires no re-training. With the surrogate-based simulations, we confirm the hypothesis raised on the inhomogeneity of the macroscopic strain field and gain insights into the influence of adjustment of the experimental setup with oblique end-tabs. Results from the surrogate-based multiscale approach show better agreement with experiments than the single-scale micromechanical approach over a wide range of settings, although with limited accuracy on the creep experiments, where macroscopic test effects were implicitly taken into account in the material properties calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10193v1</guid>
      <category>math.NA</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. A. Maia, I. B. C. M. Rocha, D. Kova\v{c}evi\'c, F. P. van der Meer</dc:creator>
    </item>
    <item>
      <title>Mosaic-skeleton approximation is all you need for Smoluchowski equations</title>
      <link>https://arxiv.org/abs/2501.10206</link>
      <description>arXiv:2501.10206v1 Announce Type: new 
Abstract: In this work we demonstrate a surprising way of exploitation of the mosaic--skeleton approximations for efficient numerical solving of aggregation equations with many applied kinetic kernels. The complexity of the evaluation of the right-hand side with $M$ nonlinear differential equations basing on the use of the mosaic-skeleton approximations is $\mathcal{O}(M \log^2 M)$ operations instead of $\mathcal{O}(M^2)$ for the straightforward computation. The class of kernels allowing to make fast and accurate computations via our approach is wider than analogous set of kinetic coefficients for effective calculations with previously developed algorithms. This class covers the aggregation problems arising in modelling of sedimentation, supersonic effects, turbulent flows, etc. We show that our approach makes it possible to study the systems with $M=2^{20}$ nonlinear equations within a modest computing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10206v1</guid>
      <category>math.NA</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roman R. Dyachenko, Sergey A. Matveev, Bulat I. Valiakhmetov</dc:creator>
    </item>
    <item>
      <title>The worst-case root-convergence factor of GMRES(1)</title>
      <link>https://arxiv.org/abs/2501.10248</link>
      <description>arXiv:2501.10248v1 Announce Type: new 
Abstract: In this work, we analyze the asymptotic convergence factor of minimal residual iteration (MRI) (or GMRES(1)) for solving linear systems $Ax=b$ based on vector-dependent nonlinear eigenvalue problems. The worst-case root-convergence factor is derived for linear systems with $A$ being symmetric or $I-A$ being skew-symmetric. When $A$ is symmetric, the asymptotic convergence factor highly depends on the initial guess. While $M=I-A$ is skew-symmetric, GMRES(1) converges unconditionally and the worst-case root-convergence factor relies solely on the spectral radius of $M$. We also derive the q-linear convergence factor, which is the same as the worst-case root-convergence factor. Numerical experiments are presented to validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10248v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhui He</dc:creator>
    </item>
    <item>
      <title>Convergent Sixth-order Compact Finite Difference Method for Variable-Coefficient Elliptic PDEs in Curved Domains</title>
      <link>https://arxiv.org/abs/2501.10358</link>
      <description>arXiv:2501.10358v1 Announce Type: new 
Abstract: Finite difference methods (FDMs) are widely used for solving partial differential equations (PDEs) due to their relatively simple implementation. However, they face significant challenges when applied to non-rectangular domains and in establishing theoretical convergence, particularly for high-order schemes. In this paper, we focus on solving the elliptic equation $-\nabla \cdot (a\nabla u)=f$ in a two-dimensional curved domain $\Omega$, where the diffusion coefficient $a$ is variable and smooth. We propose a sixth-order $9$-point compact FDM that only utilizes the grid points in $(h \mathbb{Z}^2)\cap \Omega$ for any mesh size $h&gt;0$, without relying on ghost points or information outside $\overline{\Omega}$. All the boundary stencils near $\partial \Omega$ have at most $6$ different configurations and use at most $8$ grid points inside $\Omega$. We rigorously establish the sixth-order convergence of the numerically approximated solution $u_h$ in the $\infty$-norm. Additionally, we derive a gradient approximation $\nabla u$ directly from $u_h$ without solving auxiliary equations. This gradient approximation achieves proven accuracy of order $5+\frac{1}{q}$ in the $q$-norm for all $1\le q\le \infty$ (with a logarithmic factor $\log h$ for $1\le q&lt;2$). To validate our proposed sixth-order compact finite different method, we provide several numerical examples that illustrate the sixth-order accuracy and computational efficiency of both the numerical solution and the gradient approximation for solving elliptic PDEs in curved domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10358v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bin Han, Jiwoon Sim</dc:creator>
    </item>
    <item>
      <title>Study on a Fast Solver for Combined Field Integral Equations of 3D Conducting Bodies Based on Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2501.09923</link>
      <description>arXiv:2501.09923v1 Announce Type: cross 
Abstract: In this paper, we present a graph neural networks (GNNs)-based fast solver (GraphSolver) for solving combined field integral equations (CFIEs) of 3D conducting bodies. Rao-Wilton-Glisson (RWG) basis functions are employed to discretely and accurately represent the geometry of 3D conducting bodies. A concise and informative graph representation is then constructed by treating each RWG function as a node in the graph, enabling the flow of current between nodes. With the transformed graphs, GraphSolver is developed to directly predict real and imaginary parts of the x, y and z components of the surface current densities at each node (RWG function). Numerical results demonstrate the efficacy of GraphSolver in solving CFIEs for 3D conducting bodies with varying levels of geometric complexity, including basic 3D targets, missile-shaped targets, and airplane-shaped targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09923v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Shan, Xin Zhang, Di Wu</dc:creator>
    </item>
    <item>
      <title>A Family of Controllable Momentum Coefficients for Forward-Backward Accelerated Algorithms</title>
      <link>https://arxiv.org/abs/2501.10051</link>
      <description>arXiv:2501.10051v1 Announce Type: cross 
Abstract: Nesterov's accelerated gradient method (NAG) marks a pivotal advancement in gradient-based optimization, achieving faster convergence compared to the vanilla gradient descent method for convex functions. However, its algorithmic complexity when applied to strongly convex functions remains unknown, as noted in the comprehensive review by Chambolle and Pock [2016]. This issue, aside from the critical step size, was addressed by Li et al. [2024b], with the monotonic case further explored by Fu and Shi [2024]. In this paper, we introduce a family of controllable momentum coefficients for forward-backward accelerated methods, focusing on the critical step size $s=1/L$. Unlike traditional linear forms, the proposed momentum coefficients follow an $\alpha$-th power structure, where the parameter $r$ is adaptively tuned to $\alpha$. Using a Lyapunov function specifically designed for $\alpha$, we establish a controllable $O\left(1/k^{2\alpha} \right)$ convergence rate for the NAG-$\alpha$ method, provided that $r &gt; 2\alpha$. At the critical step size, NAG-$\alpha$ achieves an inverse polynomial convergence rate of arbitrary degree by adjusting $r$ according to $\alpha &gt; 0$. We further simplify the Lyapunov function by expressing it in terms of the iterative sequences $x_k$ and $y_k$, eliminating the need for phase-space representations. This simplification enables us to extend the controllable $O \left(1/k^{2\alpha} \right)$ rate to the monotonic variant, M-NAG-$\alpha$, thereby enhancing optimization efficiency. Finally, by leveraging the fundamental inequality for composite functions, we extended the controllable $O\left(1/k^{2\alpha} \right)$ rate to proximal algorithms, including the fast iterative shrinkage-thresholding algorithm (FISTA-$\alpha$) and its monotonic counterpart (M-FISTA-$\alpha$).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10051v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mingwei Fu, Bin Shi</dc:creator>
    </item>
    <item>
      <title>Convergence analysis of BDDC preconditioners for composite DG discretizations of the cardiac cell-by-cell model</title>
      <link>https://arxiv.org/abs/2212.12295</link>
      <description>arXiv:2212.12295v3 Announce Type: replace 
Abstract: A Balancing Domain Decomposition by Constraints (BDDC) preconditioner is constructed and analyzed for the solution of composite Discontinuous Galerkin discretizations of reaction-diffusion systems of ordinary and partial differential equations arising in cardiac cell-by-cell models. Unlike classical Bidomain and Monodomain cardiac models, which rely on homogenized descriptions of cardiac tissue at the macroscopic level, the cell-by-cell models enable the representation of individual cardiac cells, cell aggregates, damaged tissues, and nonuniform distributions of ion channels on the cell membrane. The resulting discrete cell-by-cell models exhibit discontinuous global solutions across the cell boundaries. Therefore, the proposed BDDC preconditioner employs appropriate dual and primal spaces with additional constraints to transfer information between cells (subdomains) without affecting the overall discontinuity of the global solution. A scalable convergence rate bound is proved for the resulting BDDC cell-by-cell preconditioned operator, while numerical tests validate this bound and investigate its dependence on the discretization parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.12295v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/22M1542532</arxiv:DOI>
      <dc:creator>Ngoc Mai Monica Huynh, Fatemeh Chegini, Luca Franco Pavarino, Martin Weiser, Simone Scacchi</dc:creator>
    </item>
    <item>
      <title>A Newton method for solving locally definite multiparameter eigenvalue problems by multiindex</title>
      <link>https://arxiv.org/abs/2404.04194</link>
      <description>arXiv:2404.04194v2 Announce Type: replace 
Abstract: We present a new approach to compute eigenvalues and eigenvectors of locally definite multiparameter eigenvalue problems by its signed multiindex. The method has the interpretation of a semismooth Newton method applied to certain functions that have a unique zero. We can therefore show local quadratic convergence, and for certain extreme eigenvalues even global linear convergence of the method. Local definiteness is a weaker condition than right and left definiteness, which is often considered for multiparameter eigenvalue problems. These conditions are naturally satisfied for multiparameter Sturm-Liouville problems that arise when separation of variables can be applied to multidimensional boundary eigenvalue problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04194v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henrik Eisenmann</dc:creator>
    </item>
    <item>
      <title>Change of Measure for Bayesian Field Inversion with Hierarchical Hyperparameters Sampling</title>
      <link>https://arxiv.org/abs/2404.12688</link>
      <description>arXiv:2404.12688v2 Announce Type: replace 
Abstract: This paper proposes an effective treatment of hyperparameters in the Bayesian inference of a scalar field from indirect observations. Obtaining the joint posterior distribution of the field and its hyperparameters is challenging. The infinite dimensionality of the field requires a finite parametrization that usually involves hyperparameters to reflect the limited prior knowledge. In the present work, we consider a Karhunen-Lo{\`e}ve(KL) decomposition for the random field and hyperparameters to account for the lack of prior knowledge of its autocovariance function. The hyperparameters must be inferred. To efficiently sample jointly the KL coordinates of the field and the autocovariance hyperparameters, we introduce a change of measure to reformulate the joint posterior distribution into a hierarchical Bayesian form. The likelihood depends only onthe field's coordinates in a fixed KL basis, with a prior conditioned on the hyperparameters. We exploit this structure to derive an efficient Markov Chain Monte Carlo (MCMC) sampling scheme based on an adapted Metropolis-Hasting algorithm. We rely on surrogate models (Polynomial Chaos expansions) of the forward model predictions to further accelerate the MCMC sampling. A first application to a transient diffusionproblem shows that our method is consistent with other approaches based on a change of coordinates (Sraj et al., 2016). A second application to a seismic traveltime tomography highlights the importance of inferring the hyperparameters. A third application to a 2D anisotropic groundwater flow problem illustrates the method on a more complex geometry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12688v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nad\`ege Polette (CEA/DAM, GEOSCIENCES), Olivier Le Ma\^itre (PLATON, CMAP), Pierre Sochala (CEA/DAM), Alexandrine Gesret (GEOSCIENCES)</dc:creator>
    </item>
    <item>
      <title>A comparison of mixed precision iterative refinement approaches for least-squares problems</title>
      <link>https://arxiv.org/abs/2405.18363</link>
      <description>arXiv:2405.18363v2 Announce Type: replace 
Abstract: Various approaches to iterative refinement (IR) for least-squares problems have been proposed in the literature and it may not be clear which approach is suitable for a given problem. We consider three approaches to IR for least-squares problems when two precisions are used and review their theoretical guarantees, known shortcomings and when the method can be expected to recognize that the correct solution has been found, and extend uniform precision analysis for an IR approach based on the semi-normal equations to the two-precision case. We focus on the situation where it is desired to refine the solution to the working precision level. It is shown that the IR methods exhibit different sensitivities to the conditioning of the problem and the size of the least-squares residual, which should be taken into account when choosing the IR approach. We also discuss a new approach that is based on solving multiple least-squares problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18363v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erin Carson, Ieva Dau\v{z}ickait\.e</dc:creator>
    </item>
    <item>
      <title>Global Solver based on the Sperner-Lemma and Mazurkewicz-Knaster-Kuratowski-Lemma based proof of the Brouwer Fixed-Point Theorem</title>
      <link>https://arxiv.org/abs/2407.18816</link>
      <description>arXiv:2407.18816v4 Announce Type: replace 
Abstract: In this paper a fixed-point solver for mappings from a Simplex into itself that is gradient-free, global and requires $d$ function evaluations for halvening the error is presented, where $d$ is the dimension. It is based on topological arguments and uses the constructive proof of the Mazurkewicz-Knaster-Kuratowski lemma as used as part of the proof for Brouwers Fixed-Point theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18816v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Thilo Moshagen</dc:creator>
    </item>
    <item>
      <title>A convergent scheme for the Bayesian filtering problem based on the Fokker--Planck equation and deep splitting</title>
      <link>https://arxiv.org/abs/2409.14585</link>
      <description>arXiv:2409.14585v2 Announce Type: replace 
Abstract: A numerical scheme for approximating the nonlinear filtering density is introduced and its convergence rate is established, theoretically under a parabolic H\"ormander condition, and empirically in two numerical examples. For the prediction step, between the noisy and partial measurements at discrete times, the scheme approximates the Fokker--Planck equation with a deep splitting scheme, combined with an exact update through Bayes' formula. This results in a classical prediction-update filtering algorithm that operates online for new observation sequences post-training. The algorithm employs a sampling-based Feynman--Kac approach, designed to mitigate the curse of dimensionality. The convergence proof relies on stochastic integration by parts from the Malliavin calculus. As a corollary we obtain the convergence rate for the approximation of the Fokker--Planck equation alone, disconnected from the filtering problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14585v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kasper B{\aa}gmark, Adam Andersson, Stig Larsson, Filip Rydin</dc:creator>
    </item>
    <item>
      <title>Three-precision iterative refinement with parameter regularization and prediction for solving large sparse linear systems</title>
      <link>https://arxiv.org/abs/2501.04229</link>
      <description>arXiv:2501.04229v2 Announce Type: replace 
Abstract: This study presents a novel mixed-precision iterative refinement algorithm, GADI-IR, within the general alternating-direction implicit (GADI) framework, designed for efficiently solving large-scale sparse linear systems. By employing low-precision arithmetic, particularly half-precision (FP16), for computationally intensive inner iterations, the method achieves substantial acceleration while maintaining high numerical accuracy. Key challenges such as overflow in FP16 and convergence issues for low precision are addressed through careful backward error analysis and the application of a regularization parameter $\alpha$. Furthermore, the integration of low-precision arithmetic into the parameter prediction process, using Gaussian process regression (GPR), significantly reduces computational time without degrading performance. The method is particularly effective for large-scale linear systems arising from discretized partial differential equations and other high-dimensional problems, where both accuracy and efficiency are critical. Numerical experiments demonstrate that the use of FP16 and mixed-precision strategies not only accelerates computation but also ensures robust convergence, making the approach advantageous for various applications. The results highlight the potential of leveraging lower-precision arithmetic to achieve superior computational efficiency in high-performance computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04229v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jifeng Ge, Juan Zhang</dc:creator>
    </item>
    <item>
      <title>Stochastic gradient descent for streaming linear and rectified linear systems with adversarial corruptions</title>
      <link>https://arxiv.org/abs/2403.01204</link>
      <description>arXiv:2403.01204v2 Announce Type: replace-cross 
Abstract: We propose SGD-exp, a stochastic gradient descent approach for linear and ReLU regressions under Massart noise (adversarial semi-random corruption model) for the fully streaming setting. We show novel nearly linear convergence guarantees of SGD-exp to the true parameter with up to $50\%$ Massart corruption rate, and with any corruption rate in the case of symmetric oblivious corruptions. This is the first convergence guarantee result for robust ReLU regression in the streaming setting, and it shows the improved convergence rate over previous robust methods for $L_1$ linear regression due to a choice of an exponentially decaying step size, known for its efficiency in practice. Our analysis is based on the drift analysis of a discrete stochastic process, which could also be interesting on its own.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.01204v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Halyun Jeong, Deanna Needell, Elizaveta Rebrova</dc:creator>
    </item>
    <item>
      <title>Solving Monge problem by Hilbert space embeddings of probability measures</title>
      <link>https://arxiv.org/abs/2412.03478</link>
      <description>arXiv:2412.03478v3 Announce Type: replace-cross 
Abstract: We propose deep learning methods for classical Monge's optimal mass transportation problems, where where the distribution constraint is treated as penalty terms defined by the maximum mean discrepancy in the theory of Hilbert space embeddings of probability measures. We prove that the transport maps given by the proposed methods converge to optimal transport maps in the problem with $L^2$ cost. Several numerical experiments validate our methods. In particular, we show that our methods are applicable to large-scale Monge problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03478v3</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 20 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takafumi Saito, Yumiharu Nakano</dc:creator>
    </item>
  </channel>
</rss>
