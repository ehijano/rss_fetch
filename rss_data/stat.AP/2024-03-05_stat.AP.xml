<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Mar 2024 05:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 04 Mar 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Fractional Model for Earthquakes</title>
      <link>https://arxiv.org/abs/2403.00142</link>
      <description>arXiv:2403.00142v1 Announce Type: new 
Abstract: This paper extends the existing fractional Hawkes process to better model mainshock-aftershock sequences of earthquakes. The fractional Hawkes process is a self-exciting point process model with temporal decay kernel being a Mittag-Leffler function. A maximum likelihood estimation scheme is developed and its consistency is checked. It is then compared to the ETAS model on three earthquake sequences in Southern California. The fractional Hawkes process performs favourably against the ETAS model. Additionally, two parameters in the fractional Hawkes process may have a fixed geophysical meaning dependent on the study zone and the stage of the seismic cycle the zone is in.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00142v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Davis, Boris Baeumer, Ting Wang</dc:creator>
    </item>
    <item>
      <title>Thematic agreement assessment of gridded, multi-modal geospatial datasets of different semantics and spatial granularities</title>
      <link>https://arxiv.org/abs/2403.00161</link>
      <description>arXiv:2403.00161v1 Announce Type: new 
Abstract: This paper presents a method for thematic agreement assessment of geospatial data products of different semantics and spatial granularities, which may be affected by spatial offsets between test and reference data. The proposed method uses a multi-scale framework allowing for a probabilistic evaluation whether thematic disagreement between datasets is induced by spatial offsets due to different nature of the datasets or not. We test our method using real-estate derived settlement locations and remote-sensing derived building footprint data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00161v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes H. Uhl, Stefan Leyk</dc:creator>
    </item>
    <item>
      <title>Risk Twin: Real-time Risk Visualization and Control for Structural Systems</title>
      <link>https://arxiv.org/abs/2403.00283</link>
      <description>arXiv:2403.00283v1 Announce Type: new 
Abstract: Digital twinning in structural engineering is a rapidly evolving technology that aims to eliminate the gap between physical systems and their digital models through real-time sensing, visualization, and control techniques. Although digital twins can offer dynamic insights into physical systems, their accuracy is inevitably compromised by uncertainties in sensing, modeling, simulation, and controlling. This paper proposes a specialized digital twin formulation, named Risk Twin, designed for real-time risk visualization and risk-informed control of structural systems. Integrating structural reliability and Bayesian inference methods with digital twining techniques, Risk Twin can analyze and visualize the reliability indices for structural components in real-time. To facilitate real-time inference and reliability updating, a "simulation-free" scheme is proposed, leveraging precomputed quantities prepared during an offline phase for rapid inference in the online phase. Proof-of-concept numerical and real-world Risk Twins are constructed to showcase the proposed concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00283v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zeyu Wang, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Creating area level indices of behaviours impacting cancer in Australia with a Bayesian generalised shared component model</title>
      <link>https://arxiv.org/abs/2403.00319</link>
      <description>arXiv:2403.00319v1 Announce Type: new 
Abstract: This study develops a model-based index creation approach called the Generalized Shared Component Model (GSCM) by drawing on the large field of factor models. The proposed fully Bayesian approach accommodates heteroscedastic model error, multiple shared factors and flexible spatial priors. Moreover, our model, unlike previous index approaches, provides indices with uncertainty. Focusing on Australian risk factor data, the proposed GSCM is used to develop the Area Indices of Behaviors Impacting Cancer product - representing the first area level cancer risk factor index in Australia. This advancement aids in identifying communities with elevated cancer risk, facilitating targeted health interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00319v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Hogg, Susanna Cramb, Jessica Cameron, Peter Baade, Kerrie Mengersen</dc:creator>
    </item>
    <item>
      <title>Frailty or Frailties: Exploring Frailty Index Subdimensions in the English Longitudinal Study of Ageing</title>
      <link>https://arxiv.org/abs/2403.00472</link>
      <description>arXiv:2403.00472v1 Announce Type: new 
Abstract: Background: Frailty, a state of increased vulnerability to adverse health outcomes, has garnered significant attention in research and clinical practice. Existing constructs aggregate clinical features or health deficits into a single score. While simple and interpretable, this approach may overlook the complexity of frailty and not capture the full range of variation between individuals.
  Methods: Exploratory factor analysis was used to infer latent dimensions of a frailty index constructed using survey data from the English Longitudinal Study of Ageing (ELSA), wave 9. The dataset included 58 self-reported health deficits in a representative sample of community-dwelling adults aged 65+ (N = 4971). Deficits encompassed chronic disease, general health status, mobility, independence with activities of daily living, psychological wellbeing, memory and cognition. Multiple linear regression examined associations with CASP-19 quality of life scores.
  Results: Factor analysis revealed four frailty subdimensions. Based on the component deficits with the highest loading values, these factors were labelled "Mobility Impairment and Physical Morbidity", "Difficulties in Daily Activities", "Mental Health" and "Disorientation in Time". The four subdimensions were a better predictor of quality of life than frailty index scores.
  Conclusions: Distinct subdimensions of frailty can be identified from standard index scores. A decomposed approach to understanding frailty has potential to provide a more nuanced understanding of an individual's state of health across multiple deficits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00472v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lara Johnson, Bruce Guthrie, Paul A T Kelly, Atul Anand, Alan Marshall, Sohan Seth</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal modeling for record-breaking temperature events in Spain</title>
      <link>https://arxiv.org/abs/2403.00080</link>
      <description>arXiv:2403.00080v1 Announce Type: cross 
Abstract: Record-breaking temperature events are now very frequently in the news, viewed as evidence of climate change. With this as motivation, we undertake the first substantial spatial modeling investigation of temperature record-breaking across years for any given day within the year. We work with a dataset consisting of over sixty years (1960-2021) of daily maximum temperatures across peninsular Spain. Formal statistical analysis of record-breaking events is an area that has received attention primarily within the probability community, dominated by results for the stationary record-breaking setting with some additional work addressing trends. Such effort is inadequate for analyzing actual record-breaking data. Effective analysis requires rich modeling of the indicator events which define record-breaking sequences. Resulting from novel and detailed exploratory data analysis, we propose hierarchical conditional models for the indicator events. After suitable model selection, we discover explicit trend behavior, necessary autoregression, significance of distance to the coast, useful interactions, helpful spatial random effects, and very strong daily random effects. Illustratively, the model estimates that global warming trends have increased the number of records expected in the past decade almost two-fold, 1.93 (1.89,1.98), but also estimates highly differentiated climate warming rates in space and by season.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00080v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Castillo-MateoUniversity of Zaragoza, Alan E. GelfandDuke University, Zeus Gracia-TabuencaUniversity of Zaragoza, Jes\'us As\'inUniversity of Zaragoza, Ana C. Cebri\'anUniversity of Zaragoza</dc:creator>
    </item>
    <item>
      <title>ForTune: Running Offline Scenarios to Estimate Impact on Business Metrics</title>
      <link>https://arxiv.org/abs/2403.00133</link>
      <description>arXiv:2403.00133v1 Announce Type: cross 
Abstract: Making ideal decisions as a product leader in a web-facing company is extremely difficult. In addition to navigating the ambiguity of customer satisfaction and achieving business goals, one must also pave a path forward for ones' products and services to remain relevant, desirable, and profitable. Data and experimentation to test product hypotheses are key to informing product decisions. Online controlled experiments by A/B testing may provide the best data to support such decisions with high confidence, but can be time-consuming and expensive, especially when one wants to understand impact to key business metrics such as retention or long-term value. Offline experimentation allows one to rapidly iterate and test, but often cannot provide the same level of confidence, and cannot easily shine a light on impact on business metrics. We introduce a novel, lightweight, and flexible approach to investigating hypotheses, called scenario analysis, that aims to support product leaders' decisions using data about users and estimates of business metrics. Its strengths are that it can provide guidance on trade-offs that are incurred by growing or shifting consumption, estimate trends in long-term outcomes like retention and other important business metrics, and can generate hypotheses about relationships between metrics at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00133v1</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georges DupretLeo, Konstantin SozinovLeo, Carmen Barcena GonzalezLeo, Ziggy ZacksLeo, Amber YuanLeo, Benjamin CarteretteLeo, Manuel MaiLeo, Shubham BansalLeo, Gwo LiangLeo,  Lien, Andrey Gatash, Roberto Sanchis Ojeda, Mounia Lalmas</dc:creator>
    </item>
    <item>
      <title>Classical and Bayesian statistical methods for low-level metrology</title>
      <link>https://arxiv.org/abs/2403.00385</link>
      <description>arXiv:2403.00385v1 Announce Type: cross 
Abstract: This document presents the statistical methods used to process low-level measurements in the presence of noise. These methods can be classical or Bayesian. The question is placed in the general framework of the problem of nuisance parameters, one of the canonical problems of statistical inference. By using a simple criterion proposed by Bolstad (2007), it is possible to define statistically significant results during a measurement process (act of measuring in the vocabulary of metrology). This result is similar for a classic paradigm (called "frequentist") or Bayesian: the presence of zero in the interval considered (confidence or credibility). It is shown that in the case of homoskedastic Gaussians, the commonly used results are found. The case of Poisson distributions is then considered. In the case of heteroscedastic Gaussians, which is that of radioactivity measurement, we can consider them as Poisson laws in the limit of large counts. The results are different from those commonly used, and in particular those from standards (ISO 11929). Their statistical performances, characterized by simulation, are better and are well verified experimentally. This is confirmed theoretically by the use of the Neyman-Pearson lemma which makes it possible to formally determine the statistical tests with the best performances. These results also make it possible to understand the paradox of the possible divergence of the detection limit. It is also formally shown that the confidence intervals thus calculated by getting rid of the nuisance parameter according to established methods result in the commonly used confidence interval. To our knowledge, this constitutes the first formal derivation of these confidence intervals. This method is based on keeping the measurement results whether they are significant or not (not censoring them). This is recommended in several standards or documents, is compatible with the ISO 11929 standard and is in line with recent proposals in the field of statistics. On the other hand, all the information necessary to determine whether a measurement result is significant or not remains available. The conservation and restitution of all results is currently applied in the USA. The textbook case of the WIPP incident makes it possible to ensure favorable public perception. The implications and applications of this method in different fields are finally discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00385v1</guid>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume ManificatIRSN</dc:creator>
    </item>
    <item>
      <title>Changepoint problem with angular data using a measure of variation based on the intrinsic geometry of torus</title>
      <link>https://arxiv.org/abs/2403.00508</link>
      <description>arXiv:2403.00508v1 Announce Type: cross 
Abstract: In many temporally ordered data sets, it is observed that the parameters of the underlying distribution change abruptly at unknown times. The detection of such changepoints is important for many applications. While this problem has been studied substantially in the linear data setup, not much work has been done for angular data. In this article, we utilize the intrinsic geometry of a torus to introduce the notion of the `square of an angle' and use it to propose a new measure of variation, called the `curved variance', of an angular random variable. Using the above ideas, we propose new tests for the existence of changepoint(s) in the concentration, mean direction, and/or both of these. The limiting distributions of the test statistics are derived and their powers are obtained using extensive simulation. It is seen that the tests have better power than the corresponding existing tests. The proposed methods have been implemented on three real-life data sets revealing interesting insights. In particular, our method when used to detect simultaneous changes in mean direction and concentration for hourly wind direction measurements of the cyclonic storm `Amphan' identified changepoints that could be associated with important meteorological events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00508v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Models to Mitigate Systematic Disparities in Prediction with Proxy Outcomes</title>
      <link>https://arxiv.org/abs/2403.00639</link>
      <description>arXiv:2403.00639v1 Announce Type: cross 
Abstract: Label bias occurs when the outcome of interest is not directly observable and instead modeling is performed with proxy labels. When the difference between the true outcome and the proxy label is correlated with predictors, this can yield systematic disparities in predictions for different groups of interest. We propose Bayesian hierarchical measurement models to address these issues. Through practical examples, we demonstrate how our approach improves accuracy and helps with algorithmic fairness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00639v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Mikhaeil, Andrew Gelman, Philip Greengard</dc:creator>
    </item>
    <item>
      <title>Shrinkage estimators in zero-inflated Bell regression model with application</title>
      <link>https://arxiv.org/abs/2403.00749</link>
      <description>arXiv:2403.00749v1 Announce Type: cross 
Abstract: We propose Stein-type estimators for zero-inflated Bell regression models by incorporating information on model parameters. These estimators combine the advantages of unrestricted and restricted estimators. We derive the asymptotic distributional properties, including bias and mean squared error, for the proposed shrinkage estimators. Monte Carlo simulations demonstrate the superior performance of our shrinkage estimators across various scenarios. Furthermore, we apply the proposed estimators to analyze a real dataset, showcasing their practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00749v1</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Solmaz Seifollahi, Hossein Bevrani, Zakariya Yahya Algamal</dc:creator>
    </item>
    <item>
      <title>An improved BISG for inferring race from surname and geolocation</title>
      <link>https://arxiv.org/abs/2304.09126</link>
      <description>arXiv:2304.09126v3 Announce Type: replace 
Abstract: Bayesian Improved Surname Geocoding (BISG) is a ubiquitous tool for predicting race and ethnicity using an individual's geolocation and surname. Here we demonstrate that statistical dependence of surname and geolocation within racial/ethnic categories in the United States results in biases for minority subpopulations, and we introduce a raking-based improvement. Our method augments the data used by BISG--distributions of race by geolocation and race by surname--with the distribution of surname by geolocation obtained from state voter files. We validate our algorithm on state voter registration lists that contain self-identified race/ethnicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09126v3</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philip Greengard, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>lpcde: Estimation and Inference for Local Polynomial Conditional Density Estimators</title>
      <link>https://arxiv.org/abs/2204.10375</link>
      <description>arXiv:2204.10375v2 Announce Type: replace-cross 
Abstract: This paper discusses the R package lpcde, which stands for local polynomial conditional density estimation. It implements the kernel-based local polynomial smoothing methods introduced in Cattaneo, Chandak, Jansson, Ma (2024( for statistical estimation and inference of conditional distributions, densities, and derivatives thereof. The package offers mean square error optimal bandwidth selection and associated point estimators, as well as uncertainty quantification based on robust bias correction both pointwise (e.g., confidence intervals) and uniformly (e.g., confidence bands) over evaluation points. The methods implemented are boundary adaptive whenever the data is compactly supported. The package also implements regularized conditional density estimation methods, ensuring the resulting density estimate is non-negative and integrates to one. We contrast the functionalities of lpcde with existing R packages for conditional density estimation, and showcase its main features using simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10375v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Rajita Chandak, Michael Jansson, Xinwei Ma</dc:creator>
    </item>
    <item>
      <title>Self-Supervised Learning for Time Series Analysis: Taxonomy, Progress, and Prospects</title>
      <link>https://arxiv.org/abs/2306.10125</link>
      <description>arXiv:2306.10125v3 Announce Type: replace-cross 
Abstract: Self-supervised learning (SSL) has recently achieved impressive performance on various time series tasks. The most prominent advantage of SSL is that it reduces the dependence on labeled data. Based on the pre-training and fine-tuning strategy, even a small amount of labeled data can achieve high performance. Compared with many published self-supervised surveys on computer vision and natural language processing, a comprehensive survey for time series SSL is still missing. To fill this gap, we review current state-of-the-art SSL methods for time series data in this article. To this end, we first comprehensively review existing surveys related to SSL and time series, and then provide a new taxonomy of existing time series SSL methods by summarizing them from three perspectives: generative-based, contrastive-based, and adversarial-based. These methods are further divided into ten subcategories with detailed reviews and discussions about their key intuitions, main frameworks, advantages and disadvantages. To facilitate the experiments and validation of time series SSL methods, we also summarize datasets commonly used in time series forecasting, classification, anomaly detection, and clustering tasks. Finally, we present the future directions of SSL for time series analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10125v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Zhang, Qingsong Wen, Chaoli Zhang, Rongyao Cai, Ming Jin, Yong Liu, James Zhang, Yuxuan Liang, Guansong Pang, Dongjin Song, Shirui Pan</dc:creator>
    </item>
    <item>
      <title>Is your data alignable? Principled and interpretable alignability testing and integration of single-cell data</title>
      <link>https://arxiv.org/abs/2308.01839</link>
      <description>arXiv:2308.01839v2 Announce Type: replace-cross 
Abstract: Single-cell data integration can provide a comprehensive molecular view of cells, and many algorithms have been developed to remove unwanted technical or biological variations and integrate heterogeneous single-cell datasets. Despite their wide usage, existing methods suffer from several fundamental limitations. In particular, we lack a rigorous statistical test for whether two high-dimensional single-cell datasets are alignable (and therefore should even be aligned). Moreover, popular methods can substantially distort the data during alignment, making the aligned data and downstream analysis difficult to interpret. To overcome these limitations, we present a spectral manifold alignment and inference (SMAI) framework, which enables principled and interpretable alignability testing and structure-preserving integration of single-cell data with the same type of features. SMAI provides a statistical test to robustly assess the alignability between datasets to avoid misleading inference, and is justified by high-dimensional statistical theory. On a diverse range of real and simulated benchmark datasets, it outperforms commonly used alignment methods. Moreover, we show that SMAI improves various downstream analyses such as identification of differentially expressed genes and imputation of single-cell spatial transcriptomics, providing further biological insights. SMAI's interpretability also enables quantification and a deeper understanding of the sources of technical confounders in single-cell data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01839v2</guid>
      <category>q-bio.QM</category>
      <category>cs.CV</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2313719121</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the National Academy of Sciences, 2024, 121(10) e2313719121</arxiv:journal_reference>
      <dc:creator>Rong Ma, Eric D. Sun, David Donoho, James Zou</dc:creator>
    </item>
    <item>
      <title>The `Why' behind including `Y' in your imputation model</title>
      <link>https://arxiv.org/abs/2310.17434</link>
      <description>arXiv:2310.17434v2 Announce Type: replace-cross 
Abstract: Missing data is a common challenge when analyzing epidemiological data, and imputation is often used to address this issue. Here, we investigate the scenario where a covariate used in an analysis has missingness and will be imputed. There are recommendations to include the outcome from the analysis model in the imputation model for missing covariates, but it is not necessarily clear if this recommendation always holds and why this is sometimes true. We examine deterministic imputation (i.e., single imputation with fixed values) and stochastic imputation (i.e., single or multiple imputation with random values) methods and their implications for estimating the relationship between the imputed covariate and the outcome. We mathematically demonstrate that including the outcome variable in imputation models is not just a recommendation but a requirement to achieve unbiased results when using stochastic imputation methods. Moreover, we dispel common misconceptions about deterministic imputation models and demonstrate why the outcome should not be included in these models. This paper aims to bridge the gap between imputation in theory and in practice, providing mathematical derivations to explain common statistical recommendations. We offer a better understanding of the considerations involved in imputing missing covariates and emphasize when it is necessary to include the outcome variable in the imputation model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17434v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan, Sarah C. Lotspeich, Staci A. Hepler</dc:creator>
    </item>
  </channel>
</rss>
