<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 03:26:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Evaluating a Novel Neuroevolution and Neural Architecture Search System</title>
      <link>https://arxiv.org/abs/2503.10869</link>
      <description>arXiv:2503.10869v1 Announce Type: new 
Abstract: The choice of neural network features can have a large impact on both the accuracy and speed of the network. Despite the current industry shift towards large transformer models, specialized binary classifiers remain critical for numerous practical applications where computational efficiency and low latency are essential. Neural network features tend to be developed homogeneously, resulting in slower or less accurate networks when testing against multiple datasets. In this paper, we show the effectiveness of Neuvo NAS+ a novel Python implementation of an extended Neural Architecture Search (NAS+) which allows the user to optimise the training parameters of a network as well as the network's architecture. We provide an in-depth analysis of the importance of catering a network's architecture to each dataset. We also describe the design of the Neuvo NAS+ system that selects network features on a task-specific basis including network training hyper-parameters such as the number of epochs and batch size. Results show that the Neuvo NAS+ task-specific approach significantly outperforms several machine learning approaches such as Naive Bayes, C4.5, Support Vector Machine and a standard Artificial Neural Network for solving a range of binary classification problems in terms of accuracy. Our experiments demonstrate substantial diversity in evolved network architectures across different datasets, confirming the value of task-specific optimization. Additionally, Neuvo NAS+ outperforms other evolutionary algorithm optimisers in terms of both accuracy and computational efficiency, showing that properly optimized binary classifiers can match or exceed the performance of more complex models while requiring significantly fewer computational resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10869v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin David Winter, William John Teahan</dc:creator>
    </item>
    <item>
      <title>Task-Specific Activation Functions for Neuroevolution using Grammatical Evolution</title>
      <link>https://arxiv.org/abs/2503.10879</link>
      <description>arXiv:2503.10879v1 Announce Type: new 
Abstract: Activation functions play a critical role in the performance and behaviour of neural networks, significantly impacting their ability to learn and generalise. Traditional activation functions, such as ReLU, sigmoid, and tanh, have been widely used with considerable success. However, these functions may not always provide optimal performance for all tasks and datasets. In this paper, we introduce Neuvo GEAF - an innovative approach leveraging grammatical evolution (GE) to automatically evolve novel activation functions tailored to specific neural network architectures and datasets. Experiments conducted on well-known binary classification datasets show statistically significant improvements in F1-score (between 2.4% and 9.4%) over ReLU using identical network architectures. Notably, these performance gains were achieved without increasing the network's parameter count, supporting the trend toward more efficient neural networks that can operate effectively on resource-constrained edge devices. This paper's findings suggest that evolved activation functions can provide significant performance improvements for compact networks while maintaining energy efficiency during both training and inference phases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10879v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin David Winter, William John Teahan</dc:creator>
    </item>
    <item>
      <title>Ecological Neural Architecture Search</title>
      <link>https://arxiv.org/abs/2503.10908</link>
      <description>arXiv:2503.10908v1 Announce Type: new 
Abstract: When employing an evolutionary algorithm to optimize a neural networks architecture, developers face the added challenge of tuning the evolutionary algorithm's own hyperparameters - population size, mutation rate, cloning rate, and number of generations. This paper introduces Neuvo Ecological Neural Architecture Search (ENAS), a novel method that incorporates these evolutionary parameters directly into the candidate solutions' phenotypes, allowing them to evolve dynamically alongside architecture specifications. Experimental results across four binary classification datasets demonstrate that ENAS not only eliminates manual tuning of evolutionary parameters but also outperforms competitor NAS methodologies in convergence speed (reducing computational time by 18.3%) and accuracy (improving classification performance in 3 out of 4 datasets). By enabling "greedy individuals" to optimize resource allocation based on fitness, ENAS provides an efficient, self-regulating approach to neural architecture search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10908v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin David Winter, William J. Teahan</dc:creator>
    </item>
    <item>
      <title>Configuration Design of Mechanical Assemblies using an Estimation of Distribution Algorithm and Constraint Programming</title>
      <link>https://arxiv.org/abs/2503.11002</link>
      <description>arXiv:2503.11002v1 Announce Type: new 
Abstract: A configuration design problem in mechanical engineering involves finding an optimal assembly of components and joints that realizes some desired performance criteria. Such a problem is a discrete, constrained, and black-box optimization problem. A novel method is developed to solve the problem by applying Bivariate Marginal Distribution Algorithm (BMDA) and constraint programming (CP). BMDA is a type of Estimation of Distribution Algorithm (EDA) that exploits the dependency knowledge learned between design variables without requiring too many fitness evaluations, which tend to be expensive for the current application. BMDA is extended with adaptive chi-square testing to identify dependencies and Gibbs sampling to generate new solutions. Also, repair operations based on CP are used to deal with infeasible solutions found during search. The method is applied to a vehicle suspension design problem and is found to be more effective in converging to good solutions than a genetic algorithm and other EDAs. These contributions are significant steps towards solving the difficult problem of configuration design in mechanical engineering with evolutionary computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11002v1</guid>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CEC.2019.8789944</arxiv:DOI>
      <arxiv:journal_reference>2019 IEEE Congress on Evolutionary Computation (CEC)</arxiv:journal_reference>
      <dc:creator>Hyunmin Cheong, Mehran Ebrahimi, Adrian Butscher, Francesco Iorio</dc:creator>
    </item>
    <item>
      <title>Underdamped Particle Swarm Optimization</title>
      <link>https://arxiv.org/abs/2503.11524</link>
      <description>arXiv:2503.11524v1 Announce Type: new 
Abstract: This article presents Underdamped Particle Swarm Optimization (UEPS), a novel metaheuristic inspired by both the Particle Swarm Optimization (PSO) algorithm and the dynamic behavior of an underdamped system. The underdamped motion acts as an intermediate solution between undamped systems, which oscillate indefinitely, and overdamped systems, which stabilize without oscillation. In the context of optimization, this type of motion allows particles to explore the search space dynamically, alternating between exploration and exploitation, with the ability to overshoot the optimal solution to explore new regions and avoid getting trapped in local optima.
  First, we review the concept of damped vibrations, an essential physical principle that describes how a system oscillates while losing energy over time, behaving in an underdamped, overdamped, or critically damped manner. This understanding forms the foundation for applying these concepts to optimization, ensuring a balanced management of exploration and exploitation. Furthermore, the classical PSO algorithm is discussed, highlighting its fundamental features and limitations, providing the necessary context to understand how the underdamped behavior improves PSO performance.
  The proposed metaheuristic is evaluated using benchmark functions and classic engineering problems, demonstrating its high robustness and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11524v1</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mat\'ias Ezequiel Hern\'andez Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Synthetic Categorical Restructuring large Or How AIs Gradually Extract Efficient Regularities from Their Experience of the World</title>
      <link>https://arxiv.org/abs/2503.10643</link>
      <description>arXiv:2503.10643v1 Announce Type: cross 
Abstract: How do language models segment their internal experience of the world of words to progressively learn to interact with it more efficiently? This study in the neuropsychology of artificial intelligence investigates the phenomenon of synthetic categorical restructuring, a process through which each successive perceptron neural layer abstracts and combines relevant categorical sub-dimensions from the thought categories of its previous layer. This process shapes new, even more efficient categories for analyzing and processing the synthetic system's own experience of the linguistic external world to which it is exposed. Our genetic neuron viewer, associated with this study, allows visualization of the synthetic categorical restructuring phenomenon occurring during the transition from perceptron layer 0 to 1 in GPT2-XL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10643v1</guid>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Michael Pichat, William Pogrund, Paloma Pichat, Armanouche Gasparian, Samuel Demarchi, Martin Corbet, Alois Georgeon, Theo Dasilva, Michael Veillet-Guillem</dc:creator>
    </item>
    <item>
      <title>Untapped Potential in Self-Optimization of Hopfield Networks: The Creativity of Unsupervised Learning</title>
      <link>https://arxiv.org/abs/2501.04007</link>
      <description>arXiv:2501.04007v2 Announce Type: replace 
Abstract: The Self-Optimization (SO) model can be considered as the third operational mode of the classical Hopfield Network, leveraging the power of associative memory to enhance optimization performance. Moreover, it has been argued to express characteristics of minimal agency, which renders it useful for the study of artificial life. In this article, we draw attention to another facet of the SO model: its capacity for creativity. Drawing on creativity studies, we argue that the model satisfies the necessary and sufficient conditions of a creative process. Moreover, we show that learning is needed to find creative outcomes above chance probability. Furthermore, we demonstrate that modifying the learning parameters in the SO model gives rise to four different regimes that can account for both creative products and inconclusive outcomes, thus providing a framework for studying and understanding the emergence of creative behaviors in artificial systems that learn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04007v2</guid>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalya Weber, Christian Guckelsberger, Tom Froese</dc:creator>
    </item>
  </channel>
</rss>
