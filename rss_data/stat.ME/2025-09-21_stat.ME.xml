<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Sep 2025 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Kernel Model Validation: How To Do It, And Why You Should Care</title>
      <link>https://arxiv.org/abs/2509.15244</link>
      <description>arXiv:2509.15244v1 Announce Type: new 
Abstract: Gaussian Process (GP) models are popular tools in uncertainty quantification (UQ) because they purport to furnish functional uncertainty estimates that can be used to represent model uncertainty. It is often difficult to state with precision what probabilistic interpretation attaches to such an uncertainty, and in what way is it calibrated. Without such a calibration statement, the value of such uncertainty estimates is quite limited and qualitative. We motivate the importance of proper probabilistic calibration of GP predictions by describing how GP predictive calibration failures can cause degraded convergence properties in a target optimization algorithm called Targeted Adaptive Design (TAD). We discuss the interpretation of GP-generated uncertainty intervals in UQ, and how one may learn to trust them, through a formal procedure for covariance kernel validation that exploits the multivariate normal nature of GP predictions. We give simple examples of GP regression misspecified 1-dimensional models, and discuss the situation with respect to higher-dimensional models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15244v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.61278/itea.46.3.1004</arxiv:DOI>
      <dc:creator>Carlo Graziani, Marieme Ngom</dc:creator>
    </item>
    <item>
      <title>Bayesian Mixture Models for Heterogeneous Extremes</title>
      <link>https://arxiv.org/abs/2509.15359</link>
      <description>arXiv:2509.15359v1 Announce Type: new 
Abstract: The conventional use of the Generalized Extreme Value (GEV) distribution to model block maxima may be inappropriate when extremes are actually structured into multiple heterogeneous groups. This can result in inaccurate risk estimation of extreme events based on return levels. In this work, we propose a novel approach for describing the behavior of extreme values in the presence of such heterogeneity. Rather than defaulting to the GEV distribution simply because it arises as a theoretical limit, we show that alternative block-maxima-based models can also align with the extremal types theorem while providing improved robustness and flexibility in practice. Our formulation leads us to a mixture model that has a Bayesian nonparametric interpretation as a Dirichlet process mixture of GEV distributions. The use of an infinite number of components enables the characterization of every possible block behavior, while at the same time defining similarities between observations based on their extremal behavior. By employing a Dirichlet process prior on the mixing measure, we can capture the complex structure of the data without the need to pre-specify the number of mixture components. The application of the proposed model is illustrated using both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15359v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Viviana Carcaiso, Miguel de Carvalho, Ilaria Prosdocimi, Isadora Antoniano-Villalobos</dc:creator>
    </item>
    <item>
      <title>KoMbine: Propagating Statistical and Systematic Errors to Kaplan--Meier Curves</title>
      <link>https://arxiv.org/abs/2509.15371</link>
      <description>arXiv:2509.15371v1 Announce Type: new 
Abstract: Kaplan--Meier curves are widely used in medical research to evaluate the performance of biomarkers and predict patient outcomes. These curves are often shown without error bands, and even when error bands are provided, they typically only account for the statistical uncertainty resulting from the finite number of patients in the study. In reality, other sources of uncertainty affect the measurements as well. As datasets grow, the statistical uncertainty on the number of patients no longer dominates the overall uncertainty, and other uncertainties are increasingly important to model. The KoMbine package, developed based on procedures used in particle physics, provides the first method to propagate both statistical and systematic uncertainties through the Kaplan--Meier curve estimation processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15371v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Roskes</dc:creator>
    </item>
    <item>
      <title>Joint Learning of Panel VAR models with Low Rank and Sparse Structure</title>
      <link>https://arxiv.org/abs/2509.15402</link>
      <description>arXiv:2509.15402v1 Announce Type: new 
Abstract: Panel vector auto-regressive (VAR) models are widely used to capture the dynamics of multivariate time series across different subpopulations, where each subpopulation shares a common set of variables. In this work, we propose a panel VAR model with a shared low-rank structure, modulated by subpopulation-specific weights, and complemented by idiosyncratic sparse components. To ensure parameter identifiability, we impose structural constraints that lead to a nonsmooth, nonconvex optimization problem. We develop a multi-block Alternating Direction Method of Multipliers (ADMM) algorithm for parameter estimation and establish its convergence under mild regularity conditions. Furthermore, we derive consistency guarantees for the proposed estimators under high-dimensional scaling. The effectiveness of the proposed modeling framework and estimators is demonstrated through experiments on both synthetic data and a real-world neuroscience data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15402v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Xu, George Michailidis</dc:creator>
    </item>
    <item>
      <title>Leveraging the group structure of hypotheses for more powerful multiple testing with FDR control for the filtered rejection set</title>
      <link>https://arxiv.org/abs/2509.15444</link>
      <description>arXiv:2509.15444v1 Announce Type: new 
Abstract: Modern biological studies often involve testing many hypotheses organized in a group or a hierarchical structure, such as a directed acyclic graph (DAG). In these studies, researchers often wish to control the false discovery rate (FDR) after filtering the discoveries to obtain interpretable results. For addressing this goal, Katsevich, Sabatti, and Bogomolov (2023, Journal of the American Statistical Association, 118(541), 165-176) developed a general method, Focused BH, that guarantees FDR control for the filtered rejection set for a pre-specified filter, under certain assumptions. We propose improving the power of Focused BH by adapting it to group or hierarchical structures of hypotheses using data-dependent weights. The general method incorporating such weights is referred to as Weighted Focused BH (WFBH). For DAG-structured hypotheses, we propose a variant of WFBH, which can gain power by being adaptive to the DAG structure, and by exploiting the logical relationships among the hypotheses. We prove that WFBH with weights that were proposed to adapt the Benjamini-Hochberg procedure to different group structures, as well as its proposed variant for testing DAG-structured hypotheses, control the post-filtering FDR under certain assumptions. Through simulations, we demonstrate that the latter variant is robust to deviations from these assumptions and can be considerably more powerful than comparable methods. Finally, we elucidate its practical use by applying it to real datasets from microbiome and gene expression studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15444v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Bogomolov, Shinjini Nandi</dc:creator>
    </item>
    <item>
      <title>A tree-based kernel for densities and its applications in clustering DNase-seq profiles</title>
      <link>https://arxiv.org/abs/2509.15480</link>
      <description>arXiv:2509.15480v1 Announce Type: new 
Abstract: Modeling multiple sampling densities within a hierarchical framework enables borrowing of information across samples. These density random effects can act as kernels in latent variable models to represent exchangeable subgroups or clusters. A key feature of these kernels is the (functional) covariance they induce, which determines how densities are grouped in mixture models. Our motivating problem is clustering chromatin accessibility profiles from high-throughput DNase-seq experiments to detect transcription factor (TF) binding. TF binding typically produces footprint profiles with spatial patterns, creating long-range dependency across genomic locations. Existing nonparametric hierarchical models impose restrictive covariance assumptions and cannot accommodate such dependencies, often leading to biologically uninformative clusters. We propose a nonparametric density kernel flexible enough to capture diverse covariance structures and adaptive to various spatial patterns of TF footprints. The kernel specifies dyadic tree splitting probabilities via a multivariate logit-normal model with a sparse precision matrix. Bayesian inference for latent variable models using this kernel is implemented through Gibbs sampling with Polya-Gamma augmentation. Extensive simulations show that our kernel substantially improves clustering accuracy. We apply the proposed mixture model to DNase-seq data from the ENCODE project, which results in biologically meaningful clusters corresponding to binding events of two common TFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15480v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yuliang Xu, Kaixuan Luo, Li Ma</dc:creator>
    </item>
    <item>
      <title>Efficient Estimation of Unfactorizable Systematic Uncertainties</title>
      <link>https://arxiv.org/abs/2509.15500</link>
      <description>arXiv:2509.15500v1 Announce Type: new 
Abstract: Accurate assessment of systematic uncertainties is an increasingly vital task in physics studies, where large, high-dimensional datasets, like those collected at the Large Hadron Collider, hold the key to new discoveries. Common approaches to assessing systematic uncertainties rely on simplifications, such as assuming that the impact of the various sources of uncertainty factorizes. In this paper, we provide realistic example scenarios in which this assumption fails. We introduce an algorithm that uses Gaussian process regression to estimate the impact of systematic uncertainties \textit{without} assuming factorization. The Gaussian process models are enhanced with derivative information, which increases the accuracy of the regression without increasing the number of samples. In addition, we present a novel sampling strategy based on Bayesian experimental design, which is shown to be more efficient than random and grid sampling in our example scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15500v1</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Romero, Kyle Cranmer, Daniel Whiteson</dc:creator>
    </item>
    <item>
      <title>Modelling time series of counts with hysteresis</title>
      <link>https://arxiv.org/abs/2509.15508</link>
      <description>arXiv:2509.15508v1 Announce Type: new 
Abstract: In this article, we propose a novel model for time series of counts called the hysteretic Poisson autoregressive (HPART) model with thresholds by extending the linear Poisson autoregressive model into a nonlinear model. Unlike other approaches that bear the adjective ``hysteretic", our model incorporates a scientifically relevant controlling factor that produces genuine hysteresis. Further, we re-analyse the buffered Poisson autoregressive (BPART) model with thresholds. Although the two models share the convenient piecewise linear structure, the HPART model probes deeper into the intricate dynamics that governs regime switching. We study the maximum likelihood estimation of the parameters of both models and their asymptotic properties in a unified manner, establish tests of separate families of hypotheses for the non-nested case involving a BPART model and a HPART model, and demonstrate the finite-sample efficacy of parameter estimation and tests with Monte Carlo simulation. We showcase advantages of the HPART model with two real time series, including plausible interpretations and improved out-of-sample predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15508v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xintong Ma, Dong Li, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Beyond the Average: Distributional Causal Inference under Imperfect Compliance</title>
      <link>https://arxiv.org/abs/2509.15594</link>
      <description>arXiv:2509.15594v1 Announce Type: new 
Abstract: We study the estimation of distributional treatment effects in randomized experiments with imperfect compliance. When participants do not adhere to their assigned treatments, we leverage treatment assignment as an instrumental variable to identify the local distributional treatment effect-the difference in outcome distributions between treatment and control groups for the subpopulation of compliers. We propose a regression-adjusted estimator based on a distribution regression framework with Neyman-orthogonal moment conditions, enabling robustness and flexibility with high-dimensional covariates. Our approach accommodates continuous, discrete, and mixed discrete-continuous outcomes, and applies under a broad class of covariate-adaptive randomization schemes, including stratified block designs and simple random sampling. We derive the estimator's asymptotic distribution and show that it achieves the semiparametric efficiency bound. Simulation results demonstrate favorable finite-sample performance, and we demonstrate the method's practical relevance in an application to the Oregon Health Insurance Experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15594v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui</dc:creator>
    </item>
    <item>
      <title>Strong uniform consistency of nonparametric estimation for quantile-based entropy function under length-biased sampling</title>
      <link>https://arxiv.org/abs/2509.15734</link>
      <description>arXiv:2509.15734v1 Announce Type: new 
Abstract: For studies in reliability, biometry, and survival analysis, the length-biased distribution is often well-suited for certain natural sampling plans. In this paper, we study the strong uniform consistency of two nonparametric estimators for the quantile-based Shannon entropy in the context of length-biased data. A simulation study is conducted to examine the behavior of the estimators in finite samples, followed by a comparative analysis with existing estimators. Furthermore, the usefulness of the proposed estimators is evaluated using a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15734v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vaishnavi Pavithradas, Rajesh G</dc:creator>
    </item>
    <item>
      <title>Transfer learning under latent space model</title>
      <link>https://arxiv.org/abs/2509.15797</link>
      <description>arXiv:2509.15797v1 Announce Type: new 
Abstract: Latent space model plays a crucial role in network analysis, and accurate estimation of latent variables is essential for downstream tasks such as link prediction. However, the large number of parameters to be estimated presents a challenge, especially when the latent space dimension is not exceptionally small. In this paper, we propose a transfer learning method that leverages information from networks with latent variables similar to those in the target network, thereby improving the estimation accuracy for the target. Given transferable source networks, we introduce a two-stage transfer learning algorithm that accommodates differences in node numbers between source and target networks. In each stage, we derive sufficient identification conditions and design tailored projected gradient descent algorithms for estimation. Theoretical properties of the resulting estimators are established. When the transferable networks are unknown, a detection algorithm is introduced to identify suitable source networks. Simulation studies and analyses of two real datasets demonstrate the effectiveness of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15797v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuangnan Fang, Ruixuan Qin, Xinyan Fan</dc:creator>
    </item>
    <item>
      <title>Deep learning based doubly robust test for Granger causality</title>
      <link>https://arxiv.org/abs/2509.15798</link>
      <description>arXiv:2509.15798v1 Announce Type: new 
Abstract: Granger causality is popular for analyzing time series data in many applications from natural science to social science including genomics, neuroscience, economics, and finance. Consequently, the Granger causality test has become one of the main concerns of the econometrician for decades. Taking advantage of the theoretical breakthroughs in deep learning in recent years, we propose a doubly robust Granger causality test (DRGCT). Our method offers several key advantages. The first and most direct benefit is for the users, DRGCT allows them to handle large lag orders while alleviating the curse of dimensionality that traditional nonlinear Granger causality tests usually face. Second, introducing a doubly robust test statistic for time series based on neural networks that achieves a parametric convergence rate not only suggests a new paradigm for nonparametric inference in econometrics, but also broadens the application scope of deep learning. Third, a multiplier bootstrap method, combined with the doubly robust approach, provides an efficient way to obtain critical values, effectively reducing computational time and avoiding redundant calculations. We prove that the test asymptotically controls the type I error, while achieving power approaches one, and validate the effectiveness of our test through numerical simulations. In real data analysis, we apply DRGCT to revisit the price-volume relationship problem in the stock markets of America, China, and Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15798v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongchang Hui, Chijin Liu, Xiaojun Song</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Estimation of Continuous Outcomes under Multiple Treatment Levels via GPS, CBPS, and Penalized Empirical Likelihood</title>
      <link>https://arxiv.org/abs/2509.15846</link>
      <description>arXiv:2509.15846v1 Announce Type: new 
Abstract: This paper develops a unified framework for estimating continuous outcomes under multiple treatment levels in observational studies. We integrate the Generalized Propensity Score (GPS), Covariate Balancing Propensity Score (CBPS), and outcome regression into a Penalized Empirical Likelihood (PEL) formulation. The GPS is parameterized by $\boldsymbol{\beta}$ and denoted $\pi_{\boldsymbol{\beta}}(\mathbf{X})$, while CBPS imposes moment conditions to ensure covariate balance. Outcome regression flexibly models the continuous response $Y$, and doubly robust estimation ensures consistency under either correct model specification. PEL allows simultaneous estimation and variable selection using general estimating equations. Simulation results and comparisons with state-of-the-art meta-learners confirm the effectiveness of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15846v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Bi-dendrograms for clustering the categories of a multivariate categorical data set</title>
      <link>https://arxiv.org/abs/2509.15939</link>
      <description>arXiv:2509.15939v1 Announce Type: new 
Abstract: The clustering of categories in a multivariate categorical data set is investigated, where the problem separates into that of merging categories of the same variables (i.e., within-variable categories), and combining categories of different variables (i.e., between-variable categories). For the within-variable problem, the objective is to arrive at fewer categories (and, consequently, lower data dimensionality) without affecting the essential features of the data set, thereby simplifying the interpretation of any analysis using the categorical variables. The categories can be of an ordinal or nominal nature, and this property is respected in the clustering, where only adjacent categories of ordinal variables can be combined. For the between-variable problem, the objective is to arrive at asmall number of category clusters that typify the observations in the data set. In this latter problem there is no restriction on which categories can combine, as long as they do not combine within the same variable. In each of these problems, results are given in the form of a pair of dendrograms stacked one on top of the other, called a bi-dendrogram. For the within-variable problem, once all categories within each variable have been merged, the second stage is to cluster the variables themselves. For the between-variable problem, the second stage is to cluster groups of respondents that fall into the response sets arrived at in the first stage of clustering. The approach is illustrated using a sociological survey data set from the International Social Survey Program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15939v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Greenacre, Maurizio Vichi</dc:creator>
    </item>
    <item>
      <title>Studying Optimal Designs for Multivariate Crossover Trials</title>
      <link>https://arxiv.org/abs/2509.16076</link>
      <description>arXiv:2509.16076v1 Announce Type: new 
Abstract: This article discusses $A$-, $D$- and $E$-optimality results for multivariate crossover designs, where more than one response is measured from every period for each subject. The motivation for these multivariate designs comes from a $3 \times 3$ crossover trial that investigates how an oral drug affects biomarkers of mucosal inflammation, by analyzing the various gene profiles from each participant. A multivariate response crossover model with fixed effects including direct and carryover effects, and with heteroscedastic error terms is considered to fit the multiple responses measured. It is assumed all throughout the article that there is no correlation between responses but there is presence of correlation within responses. Corresponding to the direct effects, we obtain the information matrix in a multiple response setup. Various results regarding this information matrix are studied. For $p$ periods and $t$ treatments, orthogonal array design of type $I$ and strength $2$ is proved as $A$-, $D$- and $E$-optimal, when $p=t \geq 3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16076v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shubham Niphadkar, Siuli Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Estimating systematic errors in Bayesian inversion using transport maps</title>
      <link>https://arxiv.org/abs/2509.16116</link>
      <description>arXiv:2509.16116v1 Announce Type: new 
Abstract: In indirect measurements, the measurand is determined by solving an inverse problem which requires a model of the measurement process. Such models are often approximations and introduce systematic errors leading to a bias of the posterior distribution in Bayesian inversion. We propose a unified framework that combines transport maps from a reference distribution to the posterior distribution with the model error approach. This leads to an adaptive algorithm that jointly estimates the posterior distribution of the measurand and the model error. The efficiency and accuracy of the method are demonstrated on two model problems, showing that the approach effectively corrects biases while enabling fast sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16116v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maren Casfor, Philipp Trunschke, Sebastian Heidenreich, Nando Hegemann</dc:creator>
    </item>
    <item>
      <title>Exact P-values for Network Interference</title>
      <link>https://arxiv.org/abs/1506.02084</link>
      <description>arXiv:1506.02084v1 Announce Type: cross 
Abstract: We study the calculation of exact p-values for a large class of non-sharp null hypotheses about treatment effects in a setting with data from experiments involving members of a single connected network. The class includes null hypotheses that limit the effect of one unit's treatment status on another according to the distance between units; for example, the hypothesis might specify that the treatment status of immediate neighbors has no effect, or that units more than two edges away have no effect. We also consider hypotheses concerning the validity of sparsification of a network (for example based on the strength of ties) and hypotheses restricting heterogeneity in peer effects (so that, for example, only the number or fraction treated among neighboring units matters). Our general approach is to define an artificial experiment, such that the null hypothesis that was not sharp for the original experiment is sharp for the artificial experiment, and such that the randomization analysis for the artificial experiment is validated by the design of the original experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:1506.02084v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susan Athey, Dean Eckles, Guido Imbens</dc:creator>
    </item>
    <item>
      <title>A Latent Principal Stratification Method to Address One-Sided Cluster and Individual Noncompliance in Cluster RCTs</title>
      <link>https://arxiv.org/abs/2509.15280</link>
      <description>arXiv:2509.15280v1 Announce Type: cross 
Abstract: In pragmatic cluster randomized controlled trials (PCRCTs), the unit of randomization may be the healthcare provider. In these studies, noncompliance can occur at both the patient and cluster levels. Some studies measure cluster-level implementation using multiple continuous metrics while documenting individual binary compliance. The complier average causal effect estimates the intervention effects among individuals that comply with the assigned intervention. However, it does not account for compliance metrics at the cluster level. When compliance with the intervention is influenced by both providers and individuals, it can be scientifically beneficial to describe the effects of the intervention between all levels of compliance. We propose a Bayesian method for PCRCTs with one-sided binary noncompliance at the individual level and one-sided partial compliance at the cluster level. Our Bayesian model classifies clusters into latent compliance strata based on pretreatment characteristics, partial compliance status, and individual outcomes. Because compliance is only observed in the treatment arm, the method imputes unobserved compliance for control clusters and the individuals within them. This approach estimates finite and super-population estimands within strata defined by both cluster- and individual-level compliance. We apply this method to the METRIcAL trial, a multi-part, pragmatic cluster randomized trial evaluating the effects of a personalized music intervention on agitation in nursing home residents with dementia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15280v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anthony Sisti, Ellen McCreedy, Roee Gutman</dc:creator>
    </item>
    <item>
      <title>Inference on the Distribution of Individual Treatment Effects in Nonseparable Triangular Models</title>
      <link>https://arxiv.org/abs/2509.15401</link>
      <description>arXiv:2509.15401v1 Announce Type: cross 
Abstract: In this paper, we develop inference methods for the distribution of heterogeneous individual treatment effects (ITEs) in the nonseparable triangular model with a binary endogenous treatment and a binary instrument of Vuong and Xu (2017) and Feng, Vuong, and Xu (2019). We focus on the estimation of the cumulative distribution function (CDF) of the ITE, which can be used to address a wide range of practically important questions such as inference on the proportion of individuals with positive ITEs, the quantiles of the distribution of ITEs, and the interquartile range as a measure of the spread of the ITEs, as well as comparison of the ITE distributions across sub-populations. Moreover, our CDF-based approach can deliver more precise results than density-based approach previously considered in the literature. We establish weak convergence to tight Gaussian processes for the empirical CDF and quantile function computed from nonparametric ITE estimates of Feng, Vuong, and Xu (2019). Using those results, we develop bootstrap-based nonparametric inferential methods, including uniform confidence bands for the CDF and quantile function of the ITE distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15401v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Ma, Vadim Marmer, Zhengfei Yu</dc:creator>
    </item>
    <item>
      <title>Interpretable Network-assisted Random Forest+</title>
      <link>https://arxiv.org/abs/2509.15611</link>
      <description>arXiv:2509.15611v1 Announce Type: cross 
Abstract: Machine learning algorithms often assume that training samples are independent. When data points are connected by a network, the induced dependency between samples is both a challenge, reducing effective sample size, and an opportunity to improve prediction by leveraging information from network neighbors. Multiple methods taking advantage of this opportunity are now available, but many, including graph neural networks, are not easily interpretable, limiting their usefulness for understanding how a model makes its predictions. Others, such as network-assisted linear regression, are interpretable but often yield substantially worse prediction performance. We bridge this gap by proposing a family of flexible network-assisted models built upon a generalization of random forests (RF+), which achieves highly-competitive prediction accuracy and can be interpreted through feature importance measures. In particular, we develop a suite of interpretation tools that enable practitioners to not only identify important features that drive model predictions, but also quantify the importance of the network contribution to prediction. Importantly, we provide both global and local importance measures as well as sample influence measures to assess the impact of a given observation. This suite of tools broadens the scope and applicability of network-assisted machine learning for high-impact problems where interpretability and transparency are essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.15611v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiffany M. Tang, Elizaveta Levina, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>What is a good matching of probability measures? A counterfactual lens on transport maps</title>
      <link>https://arxiv.org/abs/2509.16027</link>
      <description>arXiv:2509.16027v1 Announce Type: cross 
Abstract: Coupling probability measures lies at the core of many problems in statistics and machine learning, from domain adaptation to transfer learning and causal inference. Yet, even when restricted to deterministic transports, such couplings are not identifiable: two atomless marginals admit infinitely many transport maps. The common recourse to optimal transport, motivated by cost minimization and cyclical monotonicity, obscures the fact that several distinct notions of multivariate monotone matchings coexist. In this work, we first carry a comparative analysis of three constructions of transport maps: cyclically monotone, quantile-preserving and triangular monotone maps. We establish necessary and sufficient conditions for their equivalence, thereby clarifying their respective structural properties. In parallel, we formulate counterfactual reasoning within the framework of structural causal models as a problem of selecting transport maps between fixed marginals, which makes explicit the role of untestable assumptions in counterfactual reasoning. Then, we are able to connect these two perspectives by identifying conditions on causal graphs and structural equations under which counterfactual maps coincide with classical statistical transports. In this way, we delineate the circumstances in which causal assumptions support the use of a specific structure of transport map. Taken together, our results aim to enrich the theoretical understanding of families of transport maps and to clarify their possible causal interpretations. We hope this work contributes to establishing new bridges between statistical transport and causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16027v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas De Lara, Luca Ganassali</dc:creator>
    </item>
    <item>
      <title>Measuring agreement among several raters classifying subjects into one or more (hierarchical) categories: A generalization of Fleiss' kappa</title>
      <link>https://arxiv.org/abs/2303.12502</link>
      <description>arXiv:2303.12502v2 Announce Type: replace 
Abstract: Cohen's and Fleiss' kappa are well-known measures of inter-rater agreement, but they restrict each rater to selecting only one category per subject. This limitation is consequential in contexts where subjects may belong to multiple categories, such as psychiatric diagnoses involving multiple disorders or classifying interview snippets into multiple codes of a codebook. We propose a generalized version of Fleiss' kappa, which accommodates multiple raters assigning subjects to one or more nominal categories. Our proposed $\kappa$ statistic can incorporate category weights based on their importance and account for hierarchical category structures, such as primary disorders with sub-disorders. The new $\kappa$ statistic can also manage missing data and variations in the number of raters per subject or category. We review existing methods that allow for multiple category assignments and detail the derivation of our measure, proving its equivalence to Fleiss' kappa when raters select a single category per subject. The paper discusses the assumptions, premises, and potential paradoxes of the new measure, as well as the range of possible values and guidelines for interpretation. The measure was developed to investigate the reliability of a new mathematics assessment method, of which an example is elaborated. The paper concludes with a worked-out example of psychiatrists diagnosing patients with multiple disorders. All calculations are provided as R script and an Excel sheet to facilitate access to the new $\kappa$ tatistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.12502v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3758/s13428-025-02746-8</arxiv:DOI>
      <arxiv:journal_reference>Behavior Research Methods, 57, 287 (2025)</arxiv:journal_reference>
      <dc:creator>Filip Moons, Ellen Vandervieren</dc:creator>
    </item>
    <item>
      <title>Causal inference for the expected number of recurrent events in the presence of a terminal event</title>
      <link>https://arxiv.org/abs/2306.16571</link>
      <description>arXiv:2306.16571v2 Announce Type: replace 
Abstract: While recurrent event analyses have been extensively studied, limited attention has been given to causal inference within the framework of recurrent event analysis. We develop a multiply robust estimation framework for causal inference in recurrent event data with a terminal failure event. We define our estimand as the vector comprising both the expected number of recurrent events and the failure survival function evaluated along a sequence of landmark times. We show that the estimand can be identified under a weaker condition than conditionally independent censoring and derive the associated class of influence functions under general censoring and failure distributions (i.e., without assuming absolute continuity). We propose a particular estimator within this class for further study, conduct comprehensive simulation studies to evaluate the small-sample performance of our estimator, and illustrate the proposed estimator using a large Medicare dataset to assess the causal effect of PM$_{2.5}$ on recurrent cardiovascular hospitalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16571v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin R. Baer, Trang Bui, Daniel Mork, Robert L. Strawderman, Ashkan Ertefaie</dc:creator>
    </item>
    <item>
      <title>JEL ratio test for independence between a continuous and a categorical random variable</title>
      <link>https://arxiv.org/abs/2402.18105</link>
      <description>arXiv:2402.18105v3 Announce Type: replace 
Abstract: The categorical Gini covariance is a dependence measure between a numerical variable and a categorical variable. The Gini covariance measures dependence by quantifying the difference between the conditional and unconditional distributional functions. The categorical Gini covariance equals zero if and only if the numerical variable and the categorical variable are independent. We propose a non-parametric test for testing the independence between a numerical and categorical variable using a modified categorical Gini covariance. We used the theory of U-statistics to find the test statistics and study the properties. The test has an asymptotic normal distribution. Since the implementation of a normal-based test is difficult, we develop a jackknife empirical likelihood (JEL) ratio test for testing independence. Extensive Monte Carlo simulation studies are carried out to validate the performance of the proposed JEL ratio test. We illustrate the test procedure using Iris flower data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18105v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saparya Suresh, Sudheesh K. Kattumannil</dc:creator>
    </item>
    <item>
      <title>Bayesian Penalized Transformation Models: Structured Additive Location-Scale Regression for Arbitrary Conditional Distributions</title>
      <link>https://arxiv.org/abs/2404.07440</link>
      <description>arXiv:2404.07440v4 Announce Type: replace 
Abstract: Penalized transformation models (PTMs) are a semiparametric location-scale regression family that estimate a response's conditional distribution directly from the data, and model the location and scale through structured additive predictors. The core of the model is a monotonically increasing transformation function that relates the response distribution to a reference distribution. The transformation function is equipped with a smoothness prior that regularizes how much the estimated distribution diverges from the reference. PTMs can be seen as a bridge between conditional transformation models and generalized additive models for location, scale and shape. Markov chain Monte Carlo inference for PTMs offers straightforward uncertainty quantification for the conditional distribution as well as for the covariate effects. A simulation study demonstrates the effectiveness of the approach and includes comparisons to many alternative methods. Applications to the Fourth Dutch Growth Study and the Framingham Heart Study illustrate the usage and practical utility. A full-featured implementation is available as a Python library. Supplementary material for this article is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07440v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Brachem, Paul F. V. Wiemann, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>Bayesian Benchmarking Small Area Estimation via Entropic Tilting</title>
      <link>https://arxiv.org/abs/2407.17848</link>
      <description>arXiv:2407.17848v2 Announce Type: replace 
Abstract: Benchmarking estimation and its risk evaluation is a practically important issue in small area estimation. While Bayesian methods have been widely adopted in small area estimation, existing benchmarking approaches are often ad-hoc, such as projecting each MCMC draw to satisfy the constraint. In contrast, our work provides a unified Bayesian formulation based on entropic tilting, which offers a more principled way to define the benchmarked posterior distribution. This approach yields benchmarked point estimates together with coherent uncertainty quantification. We first introduce general Monte Carlo methods for obtaining a benchmarked posterior under hierarchical Bayesian approaches and then show that the benchmarked posterior under empirical Bayesian frameworks can be obtained in an analytical form for some small area models. We demonstrate the usefulness of the proposed method through simulation and empirical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17848v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shonosuke Sugasawa, Genya Kobayashi, Yuki Kawakubo</dc:creator>
    </item>
    <item>
      <title>Semiparametric principal stratification analysis beyond monotonicity</title>
      <link>https://arxiv.org/abs/2501.17514</link>
      <description>arXiv:2501.17514v2 Announce Type: replace 
Abstract: Intercurrent events, common in clinical trials and observational studies, affect the existence or interpretation of final outcomes. Principal stratification addresses this challenge by defining local average treatment effect estimands within subpopulations, but often relies on restrictive assumptions such as monotonicity and counterfactual intermediate independence. To overcome these limitations, we propose a semiparametric framework for principal stratification analysis leveraging a margin-free, conditional odds ratio sensitivity parameter. Under principal ignorability, we derive nonparametric identification formulas and efficient estimation methods, including a conditionally doubly robust parametric estimator and a debiased machine learning estimator with data-adaptive nuisance learners. Our simulations show that incorrectly assuming monotonicity can frequently lead to biased inference, but incorrectly assuming non-monotonicity when monotonicity holds may maintain approximately valid inference. We demonstrate our methods in the context of a critical care trial, where monotonicity is unlikely to be valid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17514v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiaqi Tong, Brennan Kahan, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Addressing Positivity Violations in Continuous Interventions through Data-Adaptive Strategies</title>
      <link>https://arxiv.org/abs/2502.14566</link>
      <description>arXiv:2502.14566v2 Announce Type: replace 
Abstract: Positivity violations pose a key challenge in the estimation of causal effects, particularly for continuous interventions. Current approaches for addressing this issue include the use of weights or modified treatment policies. While effective in many contexts, these methods can result in estimands that do not always align well with the original research question, thereby compromising interpretability. In this paper, we introduce a novel diagnostic tool-the non-overlap ratio-to detect positivity violations. To address these violations while maintaining interpretability, we propose a data-adaptive solution, specifically the "most feasible" intervention strategy. Our strategy operates on a unit-specific basis. For a given intervention of interest, we first assess whether the intervention value is feasible for each unit. For units with sufficient support-conditional on confounders-we adhere to the intervention of interest. However, for units lacking sufficient support, we do not assign the actual intervention value of interest. Instead, we assign the closest feasible value within the support region. The non-overlap ratio provides a diagnostic summary of such support across the population. We propose an estimator using g-computation coupled with flexible conditional density estimation to identify high- and low-support regions and to estimate this new estimand. Through simulations, we demonstrate that our method effectively reduces bias across various scenarios by addressing positivity violations. Moreover, when positivity violations are absent, the method successfully recovers the standard estimand. We further validate its practical utility using real-world data from the CHAPAS-3 trial, which enrolled HIV-positive children in Zambia and Uganda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14566v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Han Bao, Michael Schomaker</dc:creator>
    </item>
    <item>
      <title>Dealing with Logs and Zeros in Regression Models</title>
      <link>https://arxiv.org/abs/2203.11820</link>
      <description>arXiv:2203.11820v3 Announce Type: replace-cross 
Abstract: The log transformation is widely used in linear regression, mainly because coefficients are interpretable as proportional effects. Yet this practice has fundamental limitations, most notably that the log is undefined at zero, creating an identification problem. We propose a new estimator, iterated OLS (iOLS), which targets the normalized average treatment effect, preserving the percentage-change interpretation while addressing these limitations. Our procedure is the theoretically justified analogue of the ad-hoc log(1+Y) transformation and delivers a consistent and asymptotically normal estimator of the parameters of the exponential conditional mean model. iOLS is computationally efficient, globally convergent, and free of the incidental-parameter bias, while extending naturally to endogenous regressors through iterated 2SLS. We illustrate the methods with simulations and revisit three influential publications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11820v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Benatia, Christophe Bell\'ego, Louis Pape</dc:creator>
    </item>
    <item>
      <title>Modelling with Sensitive Variables</title>
      <link>https://arxiv.org/abs/2403.15220</link>
      <description>arXiv:2403.15220v2 Announce Type: replace-cross 
Abstract: The paper deals with models in which the dependent variable, some explanatory variables, or both represent sensitive data. We introduce a novel discretization method that preserves data privacy when working with such variables. A multiple discretization method is proposed that utilizes information from the different discretization schemes. We show convergence in distribution for the unobserved variable and derive the asymptotic properties of the OLS estimator for linear models. Monte Carlo simulation experiments presented support our theoretical findings. Finally, we contrast our method with a differential privacy method to estimate the Australian gender wage gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15220v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Chan, Laszlo Matyas, Agoston Reguly</dc:creator>
    </item>
    <item>
      <title>Spatio-temporal Dynamical Indices for Complex Systems</title>
      <link>https://arxiv.org/abs/2412.10069</link>
      <description>arXiv:2412.10069v2 Announce Type: replace-cross 
Abstract: Complex systems span multiple spatial and temporal scales, making their dynamics challenging to understand and predict. This challenge is especially daunting when one wants to study localized and/or rare events. Advances in dynamical systems theory, including the development of state-dependent dynamical indices, namely local dimension and persistence, have provided powerful tools for studying these phenomena. However, existing applications of such indices rely on a predefined and fixed spatial domain, that provides a single scalar quantity for the entire region of interest. This aspect prevents understanding the spatially localized dynamical behavior of the system. In this work, we introduce Spatio-temporal Dynamical Indices (SDIs), that leverage the existing framework of state-dependent local dimension and persistence. SDIs are obtained via a sliding window approach, enabling the exploration of space-dependent properties in spatio-temporal data. As an example, we show that, through this framework, we are able to reconcile previously different perspectives on European summertime heatwaves. This result showcases the importance of accounting for spatial scales when performing scale-dependent dynamical analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10069v2</guid>
      <category>physics.ao-ph</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <pubDate>Mon, 22 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenyu Dong, Gabriele Messori, Davide Faranda, Adriano Gualandi, Valerio Lucarini, Gianmarco Mengaldo</dc:creator>
    </item>
  </channel>
</rss>
