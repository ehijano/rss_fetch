<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Apr 2025 01:47:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Conformal Anomaly Detection for Functional Data with Elastic Distance Metrics</title>
      <link>https://arxiv.org/abs/2504.01172</link>
      <description>arXiv:2504.01172v1 Announce Type: new 
Abstract: This paper considers the problem of outlier detection in functional data analysis focusing particularly on the more difficult case of shape outliers. We present an inductive conformal anomaly detection method based on elastic functional distance metrics. This method is evaluated and compared to similar conformal anomaly detection methods for functional data using simulation experiments. The method is also used in the analysis of two real exemplar data sets that show its utility in practical applications. The results demonstrate the efficacy of the proposed method for detecting both magnitude and shape outliers in two distinct outlier detection scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01172v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason Adams, Brandon Berman, Joshua Michalenko, J. Derek Tucker</dc:creator>
    </item>
    <item>
      <title>Bivariate Simplex Distribution</title>
      <link>https://arxiv.org/abs/2504.01210</link>
      <description>arXiv:2504.01210v1 Announce Type: new 
Abstract: This article proposes a bivariate Simplex distribution for modeling continuous outcomes constrained to the interval $(0,1)$, which can represent proportions, rates, or indices. We derive analytical expressions to calculate the dependence between the variables and examine its relationship with the association parameter. Parameters are estimated using the maximum likelihood method, and their performance is assessed through Monte Carlo simulations. The simulations explore various aspects of the bivariate distribution, including different surfaces and contour graphs. To illustrate the proposed model's methodology and properties, we present an application in the Jurimetric area. A user-friendly package, BSimplex, is also available in the R software.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01210v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emerson Amaral, Lucas S. Vieira, Lizandra C. Fabio, Vanessa Barros, Jalmar M. F. Carrasco</dc:creator>
    </item>
    <item>
      <title>Stock Return Prediction based on a Functional Capital Asset Pricing Model</title>
      <link>https://arxiv.org/abs/2504.01239</link>
      <description>arXiv:2504.01239v1 Announce Type: new 
Abstract: The capital asset pricing model (CAPM) is readily used to capture a linear relationship between the daily returns of an asset and a market index. We extend this model to an intraday high-frequency setting by proposing a functional CAPM estimation approach. The functional CAPM is a stylized example of a function-on-function linear regression with a bivariate functional regression coefficient. The two-dimensional regression coefficient measures the cross-covariance between cumulative intraday asset returns and market returns. We apply it to the Standard and Poor's 500 index and its constituent stocks to demonstrate its practicality. We investigate the functional CAPM's in-sample goodness-of-fit and out-of-sample prediction for an asset's cumulative intraday return. The findings suggest that the proposed functional CAPM methods have superior model goodness-of-fit and forecast accuracy compared to the traditional CAPM empirical estimation. In particular, the functional methods produce better model goodness-of-fit and prediction accuracy for stocks traditionally considered less price-efficient or more information-opaque.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01239v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ufuk Beyaztas, Kaiying Ji, Han Lin Shang, Eliza Wu</dc:creator>
    </item>
    <item>
      <title>Analyzing Functional Data with a Mixture of Covariance Structures Using a Curved-Based Sampling Scheme</title>
      <link>https://arxiv.org/abs/2504.01313</link>
      <description>arXiv:2504.01313v1 Announce Type: new 
Abstract: Motivated by distinct walking patterns in real-world free-living gait data, this paper proposes an innovative curve-based sampling scheme for the analysis of functional data characterized by a mixture of covariance structures. Traditional approaches often fail to adequately capture inherent complexities arising from heterogeneous covariance patterns across distinct subsets of the data. We introduce a unified Bayesian framework that integrates a nonlinear regression function with a continuous-time hidden Markov model, enabling the identification and utilization of varying covariance structures. One of the key contributions is the development of a computationally efficient curve-based sampling scheme for hidden state estimation, addressing the sampling complexities associated with high-dimensional, conditionally dependent data. This paper details the Bayesian inference procedure, examines the asymptotic properties to ensure the structural consistency of the model, and demonstrates its effectiveness through simulated and real-world examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01313v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yian Yu, Bo Wang, Jian Qing Shi</dc:creator>
    </item>
    <item>
      <title>A Practical Guide to Estimating Conditional Marginal Effects: Modern Approaches</title>
      <link>https://arxiv.org/abs/2504.01355</link>
      <description>arXiv:2504.01355v1 Announce Type: new 
Abstract: This Element offers a practical guide to estimating conditional marginal effects-how treatment effects vary with a moderating variable-using modern statistical methods. Commonly used approaches, such as linear interaction models, often suffer from unclarified estimands, limited overlap, and restrictive functional forms. This guide begins by clearly defining the estimand and presenting the main identification results. It then reviews and improves upon existing solutions, such as the semiparametric kernel estimator, and introduces robust estimation strategies, including augmented inverse propensity score weighting with Lasso selection (AIPW-Lasso) and double machine learning (DML) with modern algorithms. Each method is evaluated through simulations and empirical examples, with practical recommendations tailored to sample size and research context. All tools are implemented in the accompanying interflex package for R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01355v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiehan Liu, Ziyi Liu, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Adaptive adequacy testing of high-dimensional factor-augmented regression model</title>
      <link>https://arxiv.org/abs/2504.01432</link>
      <description>arXiv:2504.01432v2 Announce Type: new 
Abstract: In this paper, we investigate the adequacy testing problem of high-dimensional factor-augmented regression model. Existing test procedures perform not well under dense alternatives. To address this critical issue, we introduce a novel quadratic-type test statistic which can efficiently detect dense alternative hypotheses. We further propose an adaptive test procedure to remain powerful under both sparse and dense alternative hypotheses. Theoretically, under the null hypothesis, we establish the asymptotic normality of the proposed quadratic-type test statistic and asymptotic independence of the newly introduced quadratic-type test statistic and a maximum-type test statistic. We also prove that our adaptive test procedure is powerful to detect signals under either sparse or dense alternative hypotheses. Simulation studies and an application to an FRED-MD macroeconomics dataset are carried out to illustrate the merits of our introduced procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01432v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanmei Shi, Leheng Cai, Xu Guo, Shurong Zheng</dc:creator>
    </item>
    <item>
      <title>Time-to-event prediction for grouped variables using Exclusive Lasso</title>
      <link>https://arxiv.org/abs/2504.01520</link>
      <description>arXiv:2504.01520v1 Announce Type: new 
Abstract: The integration of high-dimensional genomic data and clinical data into time-to-event prediction models has gained significant attention due to the growing availability of these datasets. Traditionally, a Cox regression model is employed, concatenating various covariate types linearly. Given that much of the data may be redundant or irrelevant, feature selection through penalization is often desirable. A notable characteristic of these datasets is their organization into blocks of distinct data types, such as methylation and clinical predictors, which requires selecting a subset of covariates from each group due to high intra-group correlations. For this reason, we propose utilizing Exclusive Lasso regularization in place of standard Lasso penalization. We apply our methodology to a real-life cancer dataset, demonstrating enhanced survival prediction performance compared to the conventional Cox regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01520v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dayasri Ravi, Andreas Groll</dc:creator>
    </item>
    <item>
      <title>On the limitations for causal inference in Cox models with time-varying treatment</title>
      <link>https://arxiv.org/abs/2504.01524</link>
      <description>arXiv:2504.01524v1 Announce Type: new 
Abstract: When using the Cox model to analyze the effect of a time-varying treatment on a survival outcome, treatment is commonly included, using only the current level as a time-dependent covariate. Such a model does not necessarily assume that past treatment is not associated with the outcome (the Markov property), since it is possible to model the hazard conditional on only the current treatment value. However, modeling the hazard conditional on the full treatment history is required in order to interpret the results causally, and such a full model assumes the Markov property when only including current treatment. This is, for example, common in marginal structural Cox models. We demonstrate that relying on the Markov property is problematic, since it only holds in unrealistic settings or if the treatment has no causal effect. This is the case even if there are no confounders and the true causal effect of treatment really only depends on its current level. Further, we provide an example of a scenario where the Markov property is not fulfilled, but the Cox model that includes only current treatment as a covariate is correctly specified. Transforming the result to the survival scale does not give the true intervention-specific survival probabilities, showcasing that it is unclear how to make causal statements from such models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01524v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark B. Knudsen, Erin E. Gabriel, Torben Martinussen, Helene C. W. Rytgaard, Arvid Sj\"olander</dc:creator>
    </item>
    <item>
      <title>Non-parametric Quantile Regression and Uniform Inference with Unknown Error Distribution</title>
      <link>https://arxiv.org/abs/2504.01761</link>
      <description>arXiv:2504.01761v1 Announce Type: new 
Abstract: This paper studies the non-parametric estimation and uniform inference for the conditional quantile regression function (CQRF) with covariates exposed to measurement errors. We consider the case that the distribution of the measurement error is unknown and allowed to be either ordinary or super smooth. We estimate the density of the measurement error by the repeated measurements and propose the deconvolution kernel estimator for the CQRF. We derive the uniform Bahadur representation of the proposed estimator and construct the uniform confidence bands for the CQRF, uniformly in the sense for all covariates and a set of quantile indices, and establish the theoretical validity of the proposed inference. A data-driven approach for selecting the tuning parameter is also included. Monte Carlo simulations and a real data application demonstrate the usefulness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01761v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haoze Hou, Wei Huang, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>A New Approach to the Nonparametric Behrens-Fisher Problem with Compatible Confidence Intervals</title>
      <link>https://arxiv.org/abs/2504.01796</link>
      <description>arXiv:2504.01796v1 Announce Type: new 
Abstract: We propose a new test to address the nonparametric Behrens-Fisher problem involving different distribution functions in the two samples. Our procedure tests the null hypothesis $\mathcal{H}_0: \theta = \frac{1}{2}$, where $\theta = P(X&lt;Y) + \frac{1}{2}P(X=Y)$ denotes the Mann-Whitney effect. No restrictions on the underlying distributions of the data are imposed with the trivial exception of one-point distributions. The method is based on evaluating the ratio of the variance $\sigma_N^2$ of the Mann-Whitney effect estimator $\widehat{\theta}$ to its theoretical maximum, as derived from the Birnbaum-Klose inequality. Through simulations, we demonstrate that the proposed test effectively controls the type-I error rate under various conditions, including small sample sizes, unbalanced designs, and different data-generating mechanisms. Notably, it provides better control of the type-1 error rate compared to the widely used Brunner-Munzel test, particularly at small significance levels such as $\alpha \in \{0.01, 0.005\}$. Additionally, we derive range-preserving compatible confidence intervals, showing that they offer improved coverage over those compatible to the Brunner-Munzel test. Finally, we illustrate the application of our method in a clinical trial example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01796v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stephen Sch\"u\"urhuis, Frank Konietschke, Edgar Brunner</dc:creator>
    </item>
    <item>
      <title>Comparison of Bayesian methods for extrapolation of treatment effects: a large scale simulation study</title>
      <link>https://arxiv.org/abs/2504.01949</link>
      <description>arXiv:2504.01949v1 Announce Type: new 
Abstract: Extrapolating treatment effects from related studies is a promising strategy for designing and analyzing clinical trials in situations where achieving an adequate sample size is challenging. Bayesian methods are well-suited for this purpose, as they enable the synthesis of prior information through the use of prior distributions. While the operating characteristics of Bayesian approaches for borrowing data from control arms have been extensively studied, methods that borrow treatment effects -- quantities derived from the comparison between two arms -- remain less well understood. In this paper, we present the findings of an extensive simulation study designed to address this gap. We evaluate the frequentist operating characteristics of these methods, including the probability of success, mean squared error, bias, precision, and credible interval coverage. Our results provide insights into the strengths and limitations of existing methods in the context of confirmatory trials. In particular, we show that the Conditional Power Prior and the Robust Mixture Prior perform better overall, while the test-then-pool variants and the p-value-based power prior display suboptimal performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01949v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tristan Fauvel, Julien Tanniou, Pascal Godbillot, Billy Amzal</dc:creator>
    </item>
    <item>
      <title>Addressing an extreme positivity violation to distinguish the causal effects of surgery and anesthesia via separable effects</title>
      <link>https://arxiv.org/abs/2504.01171</link>
      <description>arXiv:2504.01171v1 Announce Type: cross 
Abstract: The U.S. Food and Drug Administration has cautioned that prenatal exposure to anesthetic drugs during the third trimester may have neurotoxic effects; however, there is limited clinical evidence available to substantiate this recommendation. One major scientific question of interest is whether such neurotoxic effects might be due to surgery, anesthesia, or both. Isolating the effects of these two exposures is challenging because they are observationally equivalent, thereby inducing an extreme positivity violation. To address this, we adopt the separable effects framework of Robins and Richardson (2010) to identify the effect of anesthesia (alone) by blocking effects through variables that are assumed to completely mediate the causal pathway from surgery to the outcome. We apply this approach to data from the nationwide Medicaid Analytic eXtract (MAX) from 1999 through 2013, which linked 16,778,281 deliveries to mothers enrolled in Medicaid during pregnancy. Furthermore, we assess the sensitivity of our results to violations of our key identification assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01171v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amy J. Pitts, Ling Guo, Caleb Ing, Caleb H. Miles</dc:creator>
    </item>
    <item>
      <title>Cause or Trigger? From Philosophy to Causal Modeling</title>
      <link>https://arxiv.org/abs/2504.01398</link>
      <description>arXiv:2504.01398v1 Announce Type: cross 
Abstract: Not much has been written about the role of triggers in the literature on causal reasoning, causal modeling, or philosophy. In this paper, we focus on describing triggers and causes in the metaphysical sense and on characterizations that differentiate them from each other. We carry out a philosophical analysis of these differences. From this, we formulate a definition that clearly differentiates triggers from causes and can be used for causal reasoning in natural sciences. We propose a mathematical model and the Cause-Trigger algorithm, which, based on given data to observable processes, is able to determine whether a process is a cause or a trigger of an effect. The possibility to distinguish triggers from causes directly from data makes the algorithm a useful tool in natural sciences using observational data, but also for real-world scenarios. For example, knowing the processes that trigger causes of a tropical storm could give politicians time to develop actions such as evacuation the population. Similarly, knowing the triggers of processes that cause global warming could help politicians focus on effective actions. We demonstrate our algorithm on the climatological data of two recent cyclones, Freddy and Zazu. The Cause-Trigger algorithm detects processes that trigger high wind speed in both storms during their cyclogenesis. The findings obtained agree with expert knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01398v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kate\v{r}ina Hlav\'a\v{c}kov\'a-Schindler, Rainer W\"o\ss, Vera Pecorino, Philip Schindler</dc:creator>
    </item>
    <item>
      <title>On Robust Empirical Likelihood for Nonparametric Regression with Application to Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2504.01535</link>
      <description>arXiv:2504.01535v1 Announce Type: cross 
Abstract: Empirical likelihood serves as a powerful tool for constructing confidence intervals in nonparametric regression and regression discontinuity designs (RDD). The original empirical likelihood framework can be naturally extended to these settings using local linear smoothers, with Wilks' theorem holding only when an undersmoothed bandwidth is selected. However, the generalization of bias-corrected versions of empirical likelihood under more realistic conditions is non-trivial and has remained an open challenge in the literature. This paper provides a satisfactory solution by proposing a novel approach, referred to as robust empirical likelihood, designed for nonparametric regression and RDD. The core idea is to construct robust weights which simultaneously achieve bias correction and account for the additional variability introduced by the estimated bias, thereby enabling valid confidence interval construction without extra estimation steps involved. We demonstrate that the Wilks' phenomenon still holds under weaker conditions in nonparametric regression, sharp and fuzzy RDD settings. Extensive simulation studies confirm the effectiveness of our proposed approach, showing superior performance over existing methods in terms of coverage probabilities and interval lengths. Moreover, the proposed procedure exhibits robustness to bandwidth selection, making it a flexible and reliable tool for empirical analyses. The practical usefulness is further illustrated through applications to two real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01535v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qin Fang, Shaojun Guo, Yang Hong, Xinghao Qiao</dc:creator>
    </item>
    <item>
      <title>Multi-Marker Similarity enables reduced-reference and interpretable image quality assessment in optical microscopy</title>
      <link>https://arxiv.org/abs/2504.01537</link>
      <description>arXiv:2504.01537v1 Announce Type: cross 
Abstract: Optical microscopy contributes to the ever-increasing progress in biological and biomedical studies, as it allows the implementation of minimally invasive experimental pipelines to translate the data of measured samples into valuable knowledge. Within these pipelines, reliable quality assessment must be ensured to validate the generated results. Image quality assessment is often applied with full-reference methods to estimate the similarity between the ground truth and the output images. However, current methods often show poor agreement with visual perception and lead to the generation of various full-reference metrics tailored to specific applications. Additionally, they rely on pixel-wise comparisons, emphasizing local intensity similarity while often overlooking comprehensive and interpretable image quality assessment. To address these issues, we have developed a multi-marker similarity method that compares standard quality markers, such as resolution, signal-to-noise ratio, contrast, and high frequency components. The method computes a similarity score between the image and the ground truth for each marker, then combines these scores into an overall similarity estimate. This provides a full-reference estimate of image quality while extracting global quality features and detecting experimental artifacts. Multi-marker similarity provides a reliable and interpretable method for image quality assessment and the generation of quality rankings. By focusing on the comparison of quality markers rather than direct image distances, the method enables reduced reference implementations, where a single field of view is used as a benchmark for multiple measurements. This opens the way for reliable automatic evaluation of big datasets, typical of large biomedical studies, when manual assessment of single images and defining the ground truth for each field of view is not feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01537v1</guid>
      <category>q-bio.QM</category>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Corbetta, Thomas Bocklitz</dc:creator>
    </item>
    <item>
      <title>Density estimation via mixture discrepancy and moments</title>
      <link>https://arxiv.org/abs/2504.01570</link>
      <description>arXiv:2504.01570v1 Announce Type: cross 
Abstract: With the aim of generalizing histogram statistics to higher dimensional cases, density estimation via discrepancy based sequential partition (DSP) has been proposed [D. Li, K. Yang, W. Wong, Advances in Neural Information Processing Systems (2016) 1099-1107] to learn an adaptive piecewise constant approximation defined on a binary sequential partition of the underlying domain, where the star discrepancy is adopted to measure the uniformity of particle distribution. However, the calculation of the star discrepancy is NP-hard and it does not satisfy the reflection invariance and rotation invariance either. To this end, we use the mixture discrepancy and the comparison of moments as a replacement of the star discrepancy, leading to the density estimation via mixture discrepancy based sequential partition (DSP-mix) and density estimation via moments based sequential partition (MSP), respectively. Both DSP-mix and MSP are computationally tractable and exhibit the reflection and rotation invariance. Numerical experiments in reconstructing the $d$-D mixture of Gaussians and Betas with $d=2, 3, \dots, 6$ demonstrate that DSP-mix and MSP both run approximately ten times faster than DSP while maintaining the same accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01570v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengyang Lei, Sihong Shao</dc:creator>
    </item>
    <item>
      <title>A Causal Inference Framework for Data Rich Environments</title>
      <link>https://arxiv.org/abs/2504.01702</link>
      <description>arXiv:2504.01702v1 Announce Type: cross 
Abstract: We propose a formal model for counterfactual estimation with unobserved confounding in "data-rich" settings, i.e., where there are a large number of units and a large number of measurements per unit. Our model provides a bridge between the structural causal model view of causal inference common in the graphical models literature with that of the latent factor model view common in the potential outcomes literature. We show how classic models for potential outcomes and treatment assignments fit within our framework. We provide an identification argument for the average treatment effect, the average treatment effect on the treated, and the average treatment effect on the untreated. For any estimator that has a fast enough estimation error rate for a certain nuisance parameter, we establish it is consistent for these various causal parameters. We then show principal component regression is one such estimator that leads to consistent estimation, and we analyze the minimal smoothness required of the potential outcomes function for consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01702v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Abadie, Anish Agarwal, Devavrat Shah</dc:creator>
    </item>
    <item>
      <title>Robust Universal Inference For Misspecified Models</title>
      <link>https://arxiv.org/abs/2307.04034</link>
      <description>arXiv:2307.04034v2 Announce Type: replace 
Abstract: In statistical inference, it is rarely realistic that the hypothesized statistical model is well-specified, and consequently it is important to understand the effects of misspecification on inferential procedures. When the hypothesized statistical model is misspecified, the natural target of inference is a projection of the data generating distribution onto the model. We present a general method for constructing valid confidence sets for such projections, under weak regularity conditions, despite possible model misspecification. Our method builds upon the universal inference method of Wasserman et al. (2020) and is based on inverting a family of split-sample tests of relative fit. We study settings in which our methods yield either exact or approximate, finite-sample valid confidence sets for various projection distributions. We study rates at which the resulting confidence sets shrink around the target of inference and complement these results with a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04034v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomjo Park, Sivaraman Balakrishnan, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Conditionally Affinely Invariant Rerandomization and its Admissible Complete Class</title>
      <link>https://arxiv.org/abs/2402.11336</link>
      <description>arXiv:2402.11336v2 Announce Type: replace 
Abstract: Rerandomization utilizes modern computing ability to improve covariate balance while adhering to the randomization principle originally advocated by RA Fisher. Affinely invariant rerandomization has the ``Equal Percent Variance Reducing'' (EPVR) property. When dealing with covariates of varying importance and/or mixed types, the conditionally EPVR property is often more desired. We discuss a general class of conditionally affinely invariant rerandomization methods and obtain their conditionally EPVR property. In addition, we set up a decision-theoretical framework to evaluate balance criteria for rerandomization. Popular rerandomization methods, such as the covariate balance table check, are found to be inadmissible. We suggest an admissible complete class of conditionally affinely invariant balance criteria, which can be applied to experimental designs involving tiers of covariates, stratification, and multiple treatment arms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11336v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhen Zhong, Donald B. Rubin</dc:creator>
    </item>
    <item>
      <title>Integer Programming for Learning Directed Acyclic Graphs from Non-identifiable Gaussian Models</title>
      <link>https://arxiv.org/abs/2404.12592</link>
      <description>arXiv:2404.12592v3 Announce Type: replace 
Abstract: We study the problem of learning directed acyclic graphs from continuous observational data, generated according to a linear Gaussian structural equation model. State-of-the-art structure learning methods for this setting have at least one of the following shortcomings: i) they cannot provide optimality guarantees and can suffer from learning sub-optimal models; ii) they rely on the stringent assumption that the noise is homoscedastic, and hence the underlying model is fully identifiable. We overcome these shortcomings and develop a computationally efficient mixed-integer programming framework for learning medium-sized problems that accounts for arbitrary heteroscedastic noise. We present an early stopping criterion under which we can terminate the branch-and-bound procedure to achieve an asymptotically optimal solution and establish the consistency of this approximate solution. In addition, we show via numerical experiments that our method outperforms state-of-the-art algorithms and is robust to noise heteroscedasticity, whereas the performance of some competing methods deteriorates under strong violations of the identifiability assumption. The software implementation of our method is available as the Python package \emph{micodag}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12592v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Biometrika,2025</arxiv:journal_reference>
      <dc:creator>Tong Xu, Armeen Taeb, Simge K\"u\c{c}\"ukyavuz, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Likelihood Based Inference in Fully and Partially Observed Exponential Family Graphical Models with Intractable Normalizing Constants</title>
      <link>https://arxiv.org/abs/2404.17763</link>
      <description>arXiv:2404.17763v2 Announce Type: replace 
Abstract: Probabilistic graphical models that encode an underlying Markov random field are fundamental building blocks of generative modeling to learn latent representations in modern multivariate data sets with complex dependency structures. Among these, the exponential family graphical models are especially popular, given their fairly well-understood statistical properties and computational scalability to high-dimensional data based on pseudo-likelihood methods. These models have been successfully applied in many fields, such as the Ising model in statistical physics and count graphical models in genomics. Another strand of models allows some nodes to be latent, so as to allow the marginal distribution of the observable nodes to depart from exponential family to capture more complex dependence. These approaches form the basis of generative models in artificial intelligence, such as the Boltzmann machines and their restricted versions. A fundamental barrier to likelihood-based (i.e., both maximum likelihood and fully Bayesian) inference in both fully and partially observed cases is the intractability of the likelihood. The usual workaround is via adopting pseudo-likelihood based approaches, following the pioneering work of Besag (1974). The goal of this paper is to demonstrate that full likelihood based analysis of these models is feasible in a computationally efficient manner. The chief innovation lies in using a technique of Geyer (1991) utilizing the tractable independence model underlying an intractable graphical model, to estimate the normalizing constant, as well as its gradient. Extensive numerical results, supporting theory and comparisons with pseudo-likelihood based approaches demonstrate the applicability of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17763v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yujie Chen, Anindya Bhadra, Antik Chakraborty</dc:creator>
    </item>
    <item>
      <title>Dependence-based fuzzy clustering of functional time series</title>
      <link>https://arxiv.org/abs/2405.04904</link>
      <description>arXiv:2405.04904v2 Announce Type: replace 
Abstract: Time series clustering is essential in scientific applications, yet methods for functional time series, collections of infinite-dimensional curves treated as random elements in a Hilbert space, remain underdeveloped. This work presents clustering approaches for functional time series that combine the fuzzy $C$-medoids and fuzzy $C$-means procedures with a novel dissimilarity measure tailored for functional data. This dissimilarity is based on an extension of the quantile autocorrelation to the functional context. Our methods effectively groups time series with similar dependence structures, achieving high accuracy and computational efficiency in simulations. The practical utility of the approach is demonstrated through case studies on high-frequency financial stock data and multi-country age-specific mortality improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04904v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Angel Lopez-Oriona, Ying Sun, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>Angular Co-variance using intrinsic geometry of torus: Non-parametric change points detection in meteorological data</title>
      <link>https://arxiv.org/abs/2409.08838</link>
      <description>arXiv:2409.08838v2 Announce Type: replace 
Abstract: In many temporal datasets, the parameters of the underlying distribution may change abruptly at unknown times. Detecting these changepoints is crucial for numerous applications. While this problem has been extensively studied for linear data, there has been remarkably less research on bivariate angular data. For the first time, we address the changepoint problem for the mean direction of toroidal and spherical data, which are types of bivariate angular data. By leveraging the intrinsic geometry of a curved torus, we introduce the concept of the ``square'' of an angle. This leads us to define the ``curved dispersion matrix'' for bivariate angular random variables, analogous to the dispersion matrix for bivariate linear random variables. Using this analogous measure of the ``Mahalanobis distance,'' we develop two new non-parametric tests to identify changes in the mean direction parameters for toroidal and spherical distributions. We derive the limiting distributions of the test statistics and evaluate their power surface and contours through extensive simulations. We also apply the proposed methods to detect changes in mean direction for hourly wind-wave direction measurements and the path of the cyclonic storm ``Biporjoy,'' which occurred between 6th and 19th June 2023 over the Arabian Sea, western coast of India.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08838v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Surojit Biswas, Buddhananda Banerjee, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>Bootstrap-based goodness-of-fit test for parametric families of conditional distributions</title>
      <link>https://arxiv.org/abs/2409.20262</link>
      <description>arXiv:2409.20262v5 Announce Type: replace 
Abstract: In this paper, we introduce a consistent goodness-of-fit test for distributional regression. The test statistic is based on a process that traces the difference between a nonparametric and a semi-parametric estimate of the marginal distribution function of Y. As its asymptotic null distribution is not distribution-free, a parametric bootstrap method is used to determine critical values. Empirical results suggest that, in certain scenarios, the test outperforms existing specification tests by achieving a higher power and thereby offering greater sensitivity to deviations from the assumed parametric distribution family. Notably, the proposed test does not involve any hyperparameters and can easily be applied to individual datasets using the gofreg-package in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20262v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gitte Kremling, Gerhard Dikta</dc:creator>
    </item>
    <item>
      <title>Assessing mediation in cross-sectional stepped wedge cluster randomized trials</title>
      <link>https://arxiv.org/abs/2410.15596</link>
      <description>arXiv:2410.15596v3 Announce Type: replace 
Abstract: Mediation analysis has been comprehensively studied for independent data but relatively little work has been done for correlated data, especially for the increasingly adopted stepped wedge cluster randomized trials (SW-CRTs). Motivated by challenges in underlying the effect mechanisms in pragmatic and implementation science clinical trials, we develop new methods for mediation analysis in SW-CRTs. Specifically, based on a linear and generalized linear mixed models, we demonstrate how to estimate the natural indirect effect and mediation proportion in typical SW-CRTs with four data types, including both continuous and binary mediators and outcomes. Furthermore, to address the emerging challenges in exposure-time treatment effect heterogeneity, we derive the mediation expressions in SW-CRTs when the total effect varies as a function of the exposure time. The cluster jackknife approach is considered for inference across all data types and treatment effect structures. We conduct extensive simulations to evaluate the finite-sample performances of proposed mediation estimators and demonstrate the proposed approach in a real data example. A user-friendly R package mediateSWCRT has been developed to facilitate the practical implementation of the estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15596v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhiqiang Cao, Fan Li</dc:creator>
    </item>
    <item>
      <title>A Doubly Robust Framework for Addressing Outcome-Dependent Selection Bias in Multi-Cohort EHR Studies</title>
      <link>https://arxiv.org/abs/2412.00228</link>
      <description>arXiv:2412.00228v2 Announce Type: replace 
Abstract: Selection bias can hinder accurate estimation of association parameters in binary disease risk models using non-probability samples like electronic health records (EHRs). The issue is compounded when participants are recruited from multiple clinics/centers with varying selection mechanisms that may depend on the disease/outcome of interest. Traditional inverse-probability-weighted (IPW) methods, based on constructed parametric selection models, often struggle with misspecifications when selection mechanisms vary across cohorts. This paper introduces a new Joint Augmented Inverse Probability Weighted (JAIPW) method, which integrates individual-level data from multiple cohorts collected under potentially outcome-dependent selection mechanisms, with data from an external probability sample. JAIPW offers double robustness by incorporating a flexible auxiliary score model to address potential misspecifications in the selection models. We outline the asymptotic properties of the JAIPW estimator, and our simulations reveal that JAIPW achieves up to five times lower relative bias and three times lower root mean square error (RMSE) compared to the best performing joint IPW methods under scenarios with misspecified selection models. Applying JAIPW to the Michigan Genomics Initiative (MGI), a multi-clinic EHR-linked biobank, combined with external national probability samples, resulted in cancer-sex association estimates closely aligned with national benchmark estimates. We also analysed the association between cancer and polygenic risk scores (PRS) in MGI to illustrate a situation where the exposure variable is not measured in the external probability sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00228v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritoban Kundu, Xu Shi, Michael Kleinsasser, Lars G. Fritsche, Maxwell Salvatore, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Data-Driven Knowledge Transfer in Batch $Q^*$ Learning</title>
      <link>https://arxiv.org/abs/2404.15209</link>
      <description>arXiv:2404.15209v2 Announce Type: replace-cross 
Abstract: In data-driven decision-making in marketing, healthcare, and education, it is desirable to utilize a large amount of data from existing ventures to navigate high-dimensional feature spaces and address data scarcity in new ventures. We explore knowledge transfer in dynamic decision-making by concentrating on batch stationary environments and formally defining task discrepancies through the lens of Markov decision processes (MDPs). We propose a framework of Transferred Fitted $Q$-Iteration algorithm with general function approximation, enabling the direct estimation of the optimal action-state function $Q^*$ using both target and source data. We establish the relationship between statistical performance and MDP task discrepancy under sieve approximation, shedding light on the impact of source and target sample sizes and task discrepancy on the effectiveness of knowledge transfer. We show that the final learning error of the $Q^*$ function is significantly improved from the single task rate both theoretically and empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15209v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elynn Chen, Xi Chen, Wenbo Jing</dc:creator>
    </item>
    <item>
      <title>A Primer on the Analysis of Randomized Experiments and a Survey of some Recent Advances</title>
      <link>https://arxiv.org/abs/2405.03910</link>
      <description>arXiv:2405.03910v2 Announce Type: replace-cross 
Abstract: The past two decades have witnessed a surge of new research in the analysis of randomized experiments. The emergence of this literature may seem surprising given the widespread use and long history of experiments as the "gold standard" in program evaluation, but this body of work has revealed many subtle aspects of randomized experiments that may have been previously unappreciated. This article provides an overview of some of these topics, primarily focused on stratification, regression adjustment, and cluster randomization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03910v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Azeem M. Shaikh, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Method-of-Moments Inference for GLMs and Doubly Robust Functionals under Proportional Asymptotics</title>
      <link>https://arxiv.org/abs/2408.06103</link>
      <description>arXiv:2408.06103v2 Announce Type: replace-cross 
Abstract: In this paper, we consider the estimation of regression coefficients and signal-to-noise (SNR) ratio in high-dimensional Generalized Linear Models (GLMs), and explore their implications in inferring popular estimands such as average treatment effects in high-dimensional observational studies. Under the ``proportional asymptotic'' regime and Gaussian covariates with known (population) covariance $\Sigma$, we derive Consistent and Asymptotically Normal (CAN) estimators of our targets of inference through a Method-of-Moments type of estimators that bypasses estimation of high dimensional nuisance functions and hyperparameter tuning altogether. Additionally, under non-Gaussian covariates, we demonstrate universality of our results under certain additional assumptions on the regression coefficients and $\Sigma$. We also demonstrate that knowing $\Sigma$ is not essential to our proposed methodology when the sample covariance matrix estimator is invertible. Finally, we complement our theoretical results with numerical experiments and comparisons with existing literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06103v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingyu Chen, Lin Liu, Rajarshi Mukherjee</dc:creator>
    </item>
  </channel>
</rss>
