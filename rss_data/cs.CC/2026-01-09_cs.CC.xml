<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 05:00:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Depth-Three Circuits for Inner Product</title>
      <link>https://arxiv.org/abs/2601.04446</link>
      <description>arXiv:2601.04446v1 Announce Type: new 
Abstract: Optimal depth 3 circuits for IP - arXiv abstract
  We show that Inner Product in $2n$ variables, $\mathbf{IP}_n(x, y) = x_1y_1 \oplus \ldots \oplus x_ny_n$, can be computed by depth-3 bottom fan-in 2 circuits of size $\mathsf{poly}(n)\cdot (9/5)^n$, matching the lower bound of G\"o\"os, Guan, and Mosnoi (Inform. Comput.'24). Our construction is given via the following steps.
  - We provide a general template for constructing optimal depth-3 circuits with bottom fan-in $k$ for an arbitrary function $f$. We do this in two steps. First, we partition the accepting inputs to $f$ into several carefully defined orbits. Second, for each orbit, we construct one $k$-CNF that (a) accepts the largest number of inputs from that orbit and (b) rejects all inputs rejected by $f$. This partitioning of inputs into orbits is based on the symmetries of $f$.
  - We instantiate the template for $\mathbf{IP}_n$ and $k = 2$. Guided by a modularity principle that these optimal 2-CNFs may be constructed by composing variable-disjoint copies of small formulas, we use computer search to identify a small set of 2-CNFs each on at most 4 variables which we call building blocks.
  - We then use analytic combinatorial techniques to determine optimal ways to combine these building blocks and construct these optimal 2-CNFs.
  We believe that the above steps can be applied to a wide range of functions to determine their depth-3 complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04446v1</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohit Gurumukhani, Daniel Kleber, Ramamohan Paturi, Christopher Rosin, Navid Talebanfard</dc:creator>
    </item>
    <item>
      <title>Efficient Compression in Semigroups</title>
      <link>https://arxiv.org/abs/2601.04747</link>
      <description>arXiv:2601.04747v1 Announce Type: cross 
Abstract: Straight-line programs are a central tool in several areas of computer science, including data compression, algebraic complexity theory, and the algorithmic solution of algebraic equations. In the algebraic setting, where straight-line programs can be interpreted as circuits over algebraic structures such as semigroups or groups, they have led to deep insights in computational complexity.
  A key result by Babai and Szemer\'edi (1984) showed that finite groups afford efficient compression via straight-line programs, enabling the design of a black-box computation model for groups. Building on their result, Fleischer (2019) placed the Cayley table membership problem for certain classes (pseudovarieties) of finite semigroups in NPOLYLOGTIME, and in some cases even in FOLL. He also provided a complete classification of pseudovarieties of finite monoids affording efficient compression.
  In this work, we complete this classification program initiated by Fleischer, characterizing precisely those pseudovarieties of finite semigroups that afford efficient compression via straight-line programs. Along the way, we also improve several known bounds on the length and width of straight-line programs over semigroups, monoids, and groups. These results lead to new upper bounds for the membership problem in the Cayley table model: for all pseudovarieties that afford efficient compression and do not contain any nonsolvable group, we obtain FOLL algorithms. In particular, we resolve a conjecture of Barrington, Kadau, Lange, and McKenzie (2001), showing that the membership problem for all solvable groups is in FOLL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04747v1</guid>
      <category>math.RA</category>
      <category>cs.CC</category>
      <category>math.GR</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Thumm, Armin Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>An Invitation to "Fine-grained Complexity of NP-Complete Problems"</title>
      <link>https://arxiv.org/abs/2601.05044</link>
      <description>arXiv:2601.05044v1 Announce Type: cross 
Abstract: Assuming that P is not equal to NP, the worst-case run time of any algorithm solving an NP-complete problem must be super-polynomial. But what is the fastest run time we can get? Before one can even hope to approach this question, a more provocative question presents itself: Since for many problems the naive brute-force baseline algorithms are still the fastest ones, maybe their run times are already optimal?
  The area that we call in this survey "fine-grained complexity of NP-complete problems" studies exactly this question. We invite the reader to catch up on selected classic results as well as delve into exciting recent developments in a riveting tour through the area passing by (among others) algebra, complexity theory, extremal and additive combinatorics, cryptography, and, of course, last but not least, algorithm design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05044v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jesper Nederlof</dc:creator>
    </item>
    <item>
      <title>Limitations of Affine Integer Relaxations for Solving Constraint Satisfaction Problems</title>
      <link>https://arxiv.org/abs/2407.09097</link>
      <description>arXiv:2407.09097v3 Announce Type: replace 
Abstract: We show that various recent algorithms for finite-domain constraint satisfaction problems (CSP), which are based on solving their affine integer relaxations, do not solve all tractable and not even all Maltsev CSPs. This rules them out as candidates for a universal polynomial-time CSP algorithm. The algorithms are $\mathbb{Z}$-affine $k$-consistency, BLP+AIP, BA$^{k}$, and CLAP. We thereby answer a question by Brakensiek, Guruswami, Wrochna, and \v{Z}ivn\'y whether BLP+AIP solves all tractable CSPs in the negative. We also refute a conjecture by Dalmau and Opr\v{s}al (LICS 2024) that every CSP is either solved by $\mathbb{Z}$-affine $k$-consistency or admits a Datalog reduction from 3-colorability. For the cohomological $k$-consistency algorithm, that is also based on affine relaxations, we show that it correctly solves our counterexample but fails on an NP-complete template.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09097v3</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Lichter, Benedikt Pago</dc:creator>
    </item>
    <item>
      <title>Dichotomy for orderings?</title>
      <link>https://arxiv.org/abs/2504.13268</link>
      <description>arXiv:2504.13268v2 Announce Type: replace 
Abstract: Fagin defined the class $NP$ by the means of Existential Second-Order logic. Feder and Vardi expressed it (up to polynomial equivalence) by special fragments of Existential Second-Order logic (SNP), while the authors used forbidden expanded substructures (cf. lifts and shadows). Consequently, for such problems there is no dichotomy, unlike for CSPs.
  We prove that ordering problems for graphs defined by finitely many forbidden ordered subgraphs capture the full power of the class $NP$, that is, any language in the class $NP$ is polynomially equivalent to an ordering problem. In particular, we refute a conjecture of Hell, Mohar and Rafiey that dichotomy holds for this class. On the positive side, we confirm the conjecture of Duffus, Ginn and R\"odl that ordering problems defined by a single obstruction which is a biconnected ordered graph are $NP$-complete if the graph is not complete.
  We initiate the study of meta-theorems for classes which have the full power of the class $NP$. For example, homomorphism problems (or CSPs) do not have full power (similarly to coloring problems). On the other hand, we show that problems defined by the existence of an ordering, which avoids certain ordered patterns, have full power. We find it surprising that such simple structures can express the full power of $NP$.
  A principal tool for obtaining these results is the Sparse Incomparability Lemma in many of its variants, which are classical results in the theory of homomorphisms of graphs and structures. We prove it here in the setting of ordered stuctures as a Temporal Sparse Incomparability Lemma. This is a non-trivial result, even in the random setting, and a deterministic algorithm requires more effort. Interestingly, our proof involves the Lov\'asz Local Lemma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13268v2</guid>
      <category>cs.CC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G\'abor Kun, Jaroslav Ne\v{s}et\v{r}il</dc:creator>
    </item>
    <item>
      <title>Toward P vs NP: An Observer-Theoretic Separation via SPDP Rank and a ZFC-Equivalent Foundation within the N-Frame Model</title>
      <link>https://arxiv.org/abs/2512.11820</link>
      <description>arXiv:2512.11820v5 Announce Type: replace 
Abstract: We present a self-contained separation framework for P vs NP developed entirely within ZFC. The approach consists of: (i) a deterministic, radius-1 compilation from uniform polynomial-time Turing computation to local sum-of-squares (SoS) polynomials with polylogarithmic contextual entanglement width (CEW); (ii) a formal Width-to-Rank upper bound for the resulting SPDP matrices at matching parameters; (iii) an NP-side identity-minor lower bound in the same encoding; and (iv) a rank-monotone, instance-uniform extraction map from the compiled P-side polynomials to the NP family. Together these yield a contradiction under the assumption P = NP, establishing a separation.
  We develop a correspondence between CEW, viewed as a quantitative measure of computational contextuality, and SPDP rank, yielding a unified criterion for complexity separation. We prove that bounded-CEW observers correspond to polynomial-rank computations (the class P), while unbounded CEW characterizes the class NP. This implies that exponential SPDP rank for #3SAT and related hard families forces P != NP within the standard framework of complexity theory.
  Key technical components include: (1) constructive lower bounds on SPDP rank via Ramanujan-Tseitin expander families; (2) a non-circular reduction from Turing-machine computation to low-rank polynomial evaluation; (3) a codimension-collapse lemma ensuring that rank amplification cannot occur within polynomial resources; and (4) proofs of barrier immunity against relativization, natural proofs, and algebrization. The result is a complete ZFC proof architecture whose primitives and compositions are fully derived, with community verification and machine-checked formalization left as future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11820v5</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darren J. Edwards</dc:creator>
    </item>
    <item>
      <title>Polynomial-Time Classical Simulation of Noisy Quantum Circuits with Naturally Fault-Tolerant Gates</title>
      <link>https://arxiv.org/abs/2411.02535</link>
      <description>arXiv:2411.02535v3 Announce Type: replace-cross 
Abstract: We construct a polynomial-time classical algorithm that samples from the output distribution of noisy geometrically local Clifford circuits with any product-state input and single-qubit measurements in any basis. Our results apply to circuits with nearest-neighbor gates on an $O(1)$-D architecture with depolarizing noise after each gate. Importantly, we assume that the circuit does not contain qubit resets or mid-circuit measurements. This class of circuits includes Clifford-magic circuits and Conjugated-Clifford circuits, which are important candidates for demonstrating quantum advantage using non-universal gates. Additionally, our results can be extended to the case of IQP circuits augmented with CNOT gates, which is another class of non-universal circuits that are relevant to current experiments. Importantly, these results do not require randomness assumptions over the circuit families considered (such as anticoncentration properties) and instead hold for every circuit in each class as long as the depth is above a constant threshold. This allows us to rule out the possibility of fault-tolerance in these circuit models. As a key technical step, we prove that interspersed noise causes a decay of long-range entanglement at depths beyond a critical threshold. To prove our results, we merge techniques from percolation theory and Pauli path analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02535v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jon Nelson, Joel Rajakumar, Dominik Hangleiter, Michael J. Gullans</dc:creator>
    </item>
    <item>
      <title>Effective Versions of Strong Measure Zero</title>
      <link>https://arxiv.org/abs/2506.02031</link>
      <description>arXiv:2506.02031v2 Announce Type: replace-cross 
Abstract: Effective versions of strong measure zero sets are developed for various levels of complexity and computability. It is shown that the sets can be equivalently defined using a generalization of supermartingales called odds supermartingales, success rates on supermartingales, predictors, and coverings. We show Borel's conjecture of a set having strong measure zero if and only if it is countable holds in the time and space bounded setting. At the level of computability this does not hold. We show the computable level contains sequences at arbitrary levels of the hyperarithmetical hierarchy by proving a correspondence principle yielding a condition for the sets of computable strong measure zero to agree with the classical sets of strong measure zero. An algorithmic version of strong measure zero using lower semicomputability is defined. We show that this notion is equivalent to the set of NCR reals studied by Reimann and Slaman, thereby giving new characterizations of this set. Effective strong packing dimension zero is investigated requiring success with respect to the limit inferior instead of the limit superior. It is proven that every sequence in the corresponding algorithmic class is decidable. At the level of computability, the sets coincide with a notion of weak countability that we define.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02031v2</guid>
      <category>math.LO</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Rayman</dc:creator>
    </item>
    <item>
      <title>Taming Barren Plateaus in Arbitrary Parameterized Quantum Circuits without Sacrificing Expressibility</title>
      <link>https://arxiv.org/abs/2511.13408</link>
      <description>arXiv:2511.13408v3 Announce Type: replace-cross 
Abstract: Quantum algorithms based on parameterized quantum circuits (PQCs) have enabled a wide range of applications on near-term quantum devices. However, existing PQC architectures face several challenges, among which the ``barren plateaus" phenomenon is particularly prominent. In such cases, the loss function concentrates exponentially with increasing system size, thereby hindering effective parameter optimization. To address this challenge, we propose a general and hardware-efficient method for eliminating barren plateaus in an arbitrary PQC. Specifically, our approach achieves this by inserting a layer of easily implementable quantum channels into the original PQC, each channel requiring only one ancilla qubit and four additional gates, yielding a modified PQC (MPQC) that is provably at least as expressive as the original PQC and, under mild assumptions, is guaranteed to be free from barren plateaus. Furthermore, by appropriately adjusting the structure of MPQCs, we rigorously prove that any parameter in the original PQC can be made trainable. Importantly, the absence of barren plateaus in MPQCs is robust against realistic noise, making our approach directly applicable to near-term quantum hardware. Numerical simulations demonstrate that MPQC effectively eliminates barren plateaus in PQCs for preparing thermal states of systems with up to 100 qubits and 2400 layers. Furthermore, in end-to-end simulations, MPQC significantly outperforms PQC in finding the ground-state energy of a complex Hamiltonian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13408v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyu Chen, Yuguo Shao, Zhengwei Liu, Zhaohui Wei</dc:creator>
    </item>
  </channel>
</rss>
