<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 21 Mar 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>kDGLM: a R package for Bayesian analysis of Generalized Dynamic Linear Models</title>
      <link>https://arxiv.org/abs/2403.13069</link>
      <description>arXiv:2403.13069v1 Announce Type: new 
Abstract: This paper introduces kDGLM, an R package designed for Bayesian analysis of Generalized Dynamic Linear Models (GDLM), with a primary focus on both uni- and multivariate exponential families. Emphasizing sequential inference for time series data, the kDGLM package provides comprehensive support for fitting, smoothing, monitoring, and feed-forward interventions. The methodology employed by kDGLM, as proposed in Alves et al. (2024), seamlessly integrates with well-established techniques from the literature, particularly those used in (Gaussian) Dynamic Models. These include discount strategies, autoregressive components, transfer functions, and more. Leveraging key properties of the Kalman filter and smoothing, kDGLM exhibits remarkable computational efficiency, enabling virtually instantaneous fitting times that scale linearly with the length of the time series. This characteristic makes it an exceptionally powerful tool for the analysis of extended time series. For example, when modeling monthly hospital admissions in Brazil due to gastroenteritis from 2010 to 2022, the fitting process took a mere 0.11s. Even in a spatial-time variant of the model (27 outcomes, 110 latent states, and 156 months, yielding 17,160 parameters), the fitting time was only 4.24s. Currently, the kDGLM package supports a range of distributions, including univariate Normal (unknown mean and observational variance), bivariate Normal (unknown means, observational variances, and correlation), Poisson, Gamma (known shape and unknown mean), and Multinomial (known number of trials and unknown event probabilities). Additionally, kDGLM allows the joint modeling of multiple time series, provided each series follows one of the supported distributions. Ongoing efforts aim to continuously expand the supported distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13069v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Silvaneo V. dos Santos Jr., Mariane Branco Alves, Helio S. Migon</dc:creator>
    </item>
    <item>
      <title>Spatial Autoregressive Model on a Dirichlet Distribution</title>
      <link>https://arxiv.org/abs/2403.13076</link>
      <description>arXiv:2403.13076v1 Announce Type: new 
Abstract: Compositional data find broad application across diverse fields due to their efficacy in representing proportions or percentages of various components within a whole. Spatial dependencies often exist in compositional data, particularly when the data represents different land uses or ecological variables. Ignoring the spatial autocorrelations in modelling of compositional data may lead to incorrect estimates of parameters. Hence, it is essential to incorporate spatial information into the statistical analysis of compositional data to obtain accurate and reliable results. However, traditional statistical methods are not directly applicable to compositional data due to the correlation between its observations, which are constrained to lie on a simplex. To address this challenge, the Dirichlet distribution is commonly employed, as its support aligns with the nature of compositional vectors. Specifically, the R package DirichletReg provides a regression model, termed Dirichlet regression, tailored for compositional data. However, this model fails to account for spatial dependencies, thereby restricting its utility in spatial contexts. In this study, we introduce a novel spatial autoregressive Dirichlet regression model for compositional data, adeptly integrating spatial dependencies among observations. We construct a maximum likelihood estimator for a Dirichlet density function augmented with a spatial lag term. We compare this spatial autoregressive model with the same model without spatial lag, where we test both models on synthetic data as well as two real datasets, using different metrics. By considering the spatial relationships among observations, our model provides more accurate and reliable results for the analysis of compositional data. The model is further evaluated against a spatial multinomial regression model for compositional data, and their relative effectiveness is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13076v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teo Nguyen, Sarat Moka, Kerrie Mengersen, Benoit Liquet</dc:creator>
    </item>
    <item>
      <title>Modal Analysis of Spatiotemporal Data via Multivariate Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2403.13118</link>
      <description>arXiv:2403.13118v1 Announce Type: new 
Abstract: Modal analysis has become an essential tool to understand the coherent structure of complex flows. The classical modal analysis methods, such as dynamic mode decomposition (DMD) and spectral proper orthogonal decomposition (SPOD), rely on a sufficient amount of data that is regularly sampled in time. However, often one needs to deal with sparse temporally irregular data, e.g., due to experimental measurements and simulation algorithm. To overcome the limitations of data scarcity and irregular sampling, we propose a novel modal analysis technique using multi-variate Gaussian process regression (MVGPR). We first establish the connection between MVGPR and the existing modal analysis techniques, DMD and SPOD, from a linear system identification perspective. Next, leveraging this connection, we develop a MVGPR-based modal analysis technique that addresses the aforementioned limitations. The capability of MVGPR is endowed by its judiciously designed kernel structure for correlation function, that is derived from the assumed linear dynamics. Subsequently, the proposed MVGPR method is benchmarked against DMD and SPOD on a range of examples, from academic and synthesized data to unsteady airfoil aerodynamics. The results demonstrate MVGPR as a promising alternative to classical modal analysis methods, especially in the scenario of scarce and temporally irregular data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13118v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>math.SP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiwoo Song, Daning Huang</dc:creator>
    </item>
    <item>
      <title>Robust inference of cooperative behaviour of multiple ion channels in voltage-clamp recordings</title>
      <link>https://arxiv.org/abs/2403.13197</link>
      <description>arXiv:2403.13197v1 Announce Type: new 
Abstract: Recent experimental studies have shed light on the intriguing possibility that ion channels exhibit cooperative behaviour. However, a comprehensive understanding of such cooperativity remains elusive, primarily due to limitations in measuring separately the response of each channel. Rather, only the superimposed channel response can be observed, challenging existing data analysis methods. To address this gap, we propose IDC (Idealisation, Discretisation, and Cooperativity inference), a robust statistical data analysis methodology that requires only voltage-clamp current recordings of an ensemble of ion channels. The framework of IDC enables us to integrate recent advancements in idealisation techniques and coupled Markov models. Further, in the cooperativity inference phase of IDC, we introduce a minimum distance estimator and establish its statistical guarantee in the form of asymptotic consistency. We demonstrate the effectiveness and robustness of IDC through extensive simulation studies. As an application, we investigate gramicidin D channels. Our findings reveal that these channels act independently, even at varying applied voltages during voltage-clamp experiments. An implementation of IDC is available from GitLab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13197v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robin Requadt, Manuel Fink, Patrick Kubica, Claudia Steinem, Axel Munk, Housen Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Trees for Principal Causal Effects</title>
      <link>https://arxiv.org/abs/2403.13256</link>
      <description>arXiv:2403.13256v1 Announce Type: new 
Abstract: Principal stratification analysis evaluates how causal effects of a treatment on a primary outcome vary across strata of units defined by their treatment effect on some intermediate quantity. This endeavor is substantially challenged when the intermediate variable is continuously scaled and there are infinitely many basic principal strata. We employ a Bayesian nonparametric approach to flexibly evaluate treatment effects across flexibly-modeled principal strata. The approach uses Bayesian Causal Forests (BCF) to simultaneously specify two Bayesian Additive Regression Tree models; one for the principal stratum membership and one for the outcome, conditional on principal strata. We show how the capability of BCF for capturing treatment effect heterogeneity is particularly relevant for assessing how treatment effects vary across the surface defined by continuously-scaled principal strata, in addition to other benefits relating to targeted selection and regularization-induced confounding. The capabilities of the proposed approach are illustrated with a simulation study, and the methodology is deployed to investigate how causal effects of power plant emissions control technologies on ambient particulate pollution vary as a function of the technologies' impact on sulfur dioxide emissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13256v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chanmin Kim, Corwin Zigler</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Selecting Relevant External Data (BASE): Application to a study of Long-Term Outcomes in a Hemophilia Gene Therapy Trial</title>
      <link>https://arxiv.org/abs/2403.13260</link>
      <description>arXiv:2403.13260v1 Announce Type: new 
Abstract: Gene therapies aim to address the root causes of diseases, particularly those stemming from rare genetic defects that can be life-threatening or severely debilitating. While there has been notable progress in the development of gene therapies in recent years, understanding their long-term effectiveness remains challenging due to a lack of data on long-term outcomes, especially during the early stages of their introduction to the market. To address the critical question of estimating long-term efficacy without waiting for the completion of lengthy clinical trials, we propose a novel Bayesian framework. This framework selects pertinent data from external sources, often early-phase clinical trials with more comprehensive longitudinal efficacy data that could lead to an improved inference of the long-term efficacy outcome. We apply this methodology to predict the long-term factor IX (FIX) levels of HEMGENIX (etranacogene dezaparvovec), the first FDA-approved gene therapy to treat adults with severe Hemophilia B, in a phase 3 study. Our application showcases the capability of the framework to estimate the 5-year FIX levels following HEMGENIX therapy, demonstrating sustained FIX levels induced by HEMGENIX infusion. Additionally, we provide theoretical insights into the methodology by establishing its posterior convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13260v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyu Pan, Xiang Zhang, Weining Shen, Ting Ye</dc:creator>
    </item>
    <item>
      <title>Forecasting density-valued functional panel data</title>
      <link>https://arxiv.org/abs/2403.13340</link>
      <description>arXiv:2403.13340v1 Announce Type: new 
Abstract: We introduce a statistical method for modeling and forecasting functional panel data, where each element is a density. Density functions are nonnegative and have a constrained integral and thus do not constitute a linear vector space. We implement a center log-ratio transformation to transform densities into unconstrained functions. These functions exhibit cross-sectionally correlation and temporal dependence. Via a functional analysis of variance decomposition, we decompose the unconstrained functional panel data into a deterministic trend component and a time-varying residual component. To produce forecasts for the time-varying component, a functional time series forecasting method, based on the estimation of the long-range covariance, is implemented. By combining the forecasts of the time-varying residual component with the deterministic trend component, we obtain h-step-ahead forecast curves for multiple populations. Illustrated by age- and sex-specific life-table death counts in the United States, we apply our proposed method to generate forecasts of the life-table death counts for 51 states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13340v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian F. Jim\'enez-Var\'on, Ying Sun, Han Lin Shang</dc:creator>
    </item>
    <item>
      <title>A unified framework for bounding causal effects on the always-survivor and other populations</title>
      <link>https://arxiv.org/abs/2403.13398</link>
      <description>arXiv:2403.13398v1 Announce Type: new 
Abstract: We investigate the bounding problem of causal effects in experimental studies in which the outcome is truncated by death, meaning that the subject dies before the outcome can be measured. Causal effects cannot be point identified without instruments and/or tight parametric assumptions but can be bounded under mild restrictions. Previous work on partial identification under the principal stratification framework has primarily focused on the `always-survivor' subpopulation. In this paper, we present a novel nonparametric unified framework to provide sharp bounds on causal effects on discrete and continuous square-integrable outcomes. These bounds are derived on the `always-survivor', `protected', and `harmed' subpopulations and on the entire population with/without assumptions of monotonicity and stochastic dominance. The main idea depends on rewriting the optimization problem in terms of the integrated tail probability expectation formula using a set of conditional probability distributions. The proposed procedure allows for settings with any type and number of covariates, and can be extended to incorporate average causal effects and complier average causal effects. Furthermore, we present several simulation studies conducted under various assumptions as well as the application of the proposed approach to a real dataset from the National Supported Work Demonstration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13398v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aixian Chen, Xia Cui, Guangren Yang</dc:creator>
    </item>
    <item>
      <title>A class of bootstrap based residuals for compositional data</title>
      <link>https://arxiv.org/abs/2403.13544</link>
      <description>arXiv:2403.13544v1 Announce Type: new 
Abstract: Regression models for compositional data are common in several areas of knowledge. As in other classes of regression models, it is desirable to perform diagnostic analysis in these models using residuals that are approximately standard normally distributed. However, for regression models for compositional data, there has not been any multivariate residual that meets this requirement. In this work, we introduce a class of asymptotically standard normally distributed residuals for compositional data based on bootstrap. Monte Carlo simulation studies indicate that the distributions of the residuals of this class are well approximated by the standard normal distribution in small samples. An application to simulated data also suggests that one of the residuals of the proposed class is better to identify model misspecification than its competitors. Finally, the usefulness of the best residual of the proposed class is illustrated through an application on sleep stages. The class of residuals proposed here can also be used in other classes of multivariate regression models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13544v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gustavo H. A. Pereira, Jianwen Cai</dc:creator>
    </item>
    <item>
      <title>Scalable Scalar-on-Image Cortical Surface Regression with a Relaxed-Thresholded Gaussian Process Prior</title>
      <link>https://arxiv.org/abs/2403.13628</link>
      <description>arXiv:2403.13628v1 Announce Type: new 
Abstract: In addressing the challenge of analysing the large-scale Adolescent Brain Cognition Development (ABCD) fMRI dataset, involving over 5,000 subjects and extensive neuroimaging data, we propose a scalable Bayesian scalar-on-image regression model for computational feasibility and efficiency. Our model employs a relaxed-thresholded Gaussian process (RTGP), integrating piecewise-smooth, sparse, and continuous functions capable of both hard- and soft-thresholding. This approach introduces additional flexibility in feature selection in scalar-on-image regression and leads to scalable posterior computation by adopting a variational approximation and utilising the Karhunen-Lo\`eve expansion for Gaussian processes. This advancement substantially reduces the computational costs in vertex-wise analysis of cortical surface data in large-scale Bayesian spatial models. The model's parameter estimation and prediction accuracy and feature selection performance are validated through extensive simulation studies and an application to the ABCD study. Here, we perform regression analysis correlating intelligence scores with task-based functional MRI data, taking into account confounding factors including age, sex, and parental education level. This validation highlights our model's capability to handle large-scale neuroimaging data while maintaining computational feasibility and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13628v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anna Menacher, Thomas E. Nichols, Timothy D. Johnson, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Data integration of non-probability and probability samples with predictive mean matching</title>
      <link>https://arxiv.org/abs/2403.13750</link>
      <description>arXiv:2403.13750v1 Announce Type: new 
Abstract: In this paper we study predictive mean matching mass imputation estimators to integrate data from probability and non-probability samples. We consider two approaches: matching predicted to observed ($\hat{y}-y$ matching) or predicted to predicted ($\hat{y}-\hat{y}$ matching) values. We prove the consistency of two semi-parametric mass imputation estimators based on these approaches and derive their variance and estimators of variance. Our approach can be employed with non-parametric regression techniques, such as kernel regression, and the analytical expression for variance can also be applied in nearest neighbour matching for non-probability samples. We conduct extensive simulation studies in order to compare the properties of this estimator with existing approaches, discuss the selection of $k$-nearest neighbours, and study the effects of model mis-specification. The paper finishes with empirical study in integration of job vacancy survey and vacancies submitted to public employment offices (admin and online data). Open source software is available for the proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13750v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chlebicki Piotr, {\L}ukasz Chrostowski, Maciej Ber\k{e}sewicz</dc:creator>
    </item>
    <item>
      <title>Tensor Time Series Imputation through Tensor Factor Modelling</title>
      <link>https://arxiv.org/abs/2403.13153</link>
      <description>arXiv:2403.13153v1 Announce Type: cross 
Abstract: We propose tensor time series imputation when the missing pattern in the tensor data can be general, as long as any two data positions along a tensor fibre are both observed for enough time points. The method is based on a tensor time series factor model with Tucker decomposition of the common component. One distinguished feature of the tensor time series factor model used is that there can be weak factors in the factor loadings matrix for each mode. This reflects reality better when real data can have weak factors which drive only groups of observed variables, for instance, a sector factor in financial market driving only stocks in a particular sector. Using the data with missing entries, asymptotic normality is derived for rows of estimated factor loadings, while consistent covariance matrix estimation enables us to carry out inferences. As a first in the literature, we also propose a ratio-based estimator for the rank of the core tensor under general missing patterns. Rates of convergence are spelt out for the imputations from the estimated tensor factor models. We introduce a new measure for gauging imputation performances, and simulation results show that our imputation procedure works well, with asymptotic normality and corresponding inferences also demonstrated. Re-imputation performances are also gauged when we demonstrate that using slightly larger rank then estimated gives superior re-imputation performances. An NYC taxi traffic data set is also analyzed by imposing general missing patterns and gauging the imputation performances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13153v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Quadratic Point Estimate Method for Probabilistic Moments Computation</title>
      <link>https://arxiv.org/abs/2403.13203</link>
      <description>arXiv:2403.13203v1 Announce Type: cross 
Abstract: This paper presents in detail the originally developed Quadratic Point Estimate Method (QPEM), aimed at efficiently and accurately computing the first four output moments of probabilistic distributions, using 2n^2+1 sample (or sigma) points, with n, the number of input random variables. The proposed QPEM particularly offers an effective, superior, and practical alternative to existing sampling and quadrature methods for low- and moderately-high-dimensional problems. Detailed theoretical derivations are provided proving that the proposed method can achieve a fifth or higher-order accuracy for symmetric input distributions. Various numerical examples, from simple polynomial functions to nonlinear finite element analyses with random field representations, support the theoretical findings and further showcase the validity, efficiency, and applicability of the QPEM, from low- to high-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13203v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minhyeok Ko, Konstantinos G. Papakonstantinou</dc:creator>
    </item>
    <item>
      <title>Antithetic Multilevel Methods for Elliptic and Hypo-Elliptic Diffusions with Applications</title>
      <link>https://arxiv.org/abs/2403.13489</link>
      <description>arXiv:2403.13489v1 Announce Type: cross 
Abstract: In this paper, we present a new antithetic multilevel Monte Carlo (MLMC) method for the estimation of expectations with respect to laws of diffusion processes that can be elliptic or hypo-elliptic. In particular, we consider the case where one has to resort to time discretization of the diffusion and numerical simulation of such schemes. Motivated by recent developments, we introduce a new MLMC estimator of expectations, which does not require simulation of intractable L\'evy areas but has a weak error of order 2 and achieves the optimal computational complexity. We then show how this approach can be used in the context of the filtering problem associated to partially observed diffusions with discrete time observations. We illustrate with numerical simulations that our new approaches provide efficiency gains for several problems relative to some existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13489v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuga Iguchi, Ajay Jasra, Mohamed Maama, Alexandros Beskos</dc:creator>
    </item>
    <item>
      <title>AdaTrans: Feature-wise and Sample-wise Adaptive Transfer Learning for High-dimensional Regression</title>
      <link>https://arxiv.org/abs/2403.13565</link>
      <description>arXiv:2403.13565v1 Announce Type: cross 
Abstract: We consider the transfer learning problem in the high dimensional setting, where the feature dimension is larger than the sample size. To learn transferable information, which may vary across features or the source samples, we propose an adaptive transfer learning method that can detect and aggregate the feature-wise (F-AdaTrans) or sample-wise (S-AdaTrans) transferable structures. We achieve this by employing a novel fused-penalty, coupled with weights that can adapt according to the transferable structure. To choose the weight, we propose a theoretically informed, data-driven procedure, enabling F-AdaTrans to selectively fuse the transferable signals with the target while filtering out non-transferable signals, and S-AdaTrans to obtain the optimal combination of information transferred from each source sample. The non-asymptotic rates are established, which recover existing near-minimax optimal rates in special cases. The effectiveness of the proposed method is validated using both synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.13565v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zelin He, Ying Sun, Jingyuan Liu, Runze Li</dc:creator>
    </item>
    <item>
      <title>A Bayesian hierarchical model for disease mapping that accounts for scaling and heavy-tailed latent effects</title>
      <link>https://arxiv.org/abs/2109.10330</link>
      <description>arXiv:2109.10330v2 Announce Type: replace 
Abstract: In disease mapping, the relative risk of a disease is commonly estimated across different areas within a region of interest. The number of cases in an area is often assumed to follow a Poisson distribution whose mean is decomposed as the product between an offset and the logarithm of the disease's relative risk. The log risk may be written as the sum of fixed effects and latent random effects. The BYM2 model decomposes each latent effect into a weighted sum of independent and spatial effects. We build on the BYM2 model to allow for heavy-tailed latent effects and accommodate potentially outlying risks, after accounting for the fixed effects. We assume a scale mixture structure wherein the variance of the latent process changes across areas and allows for outlier identification. We propose two prior specifications for this scale mixture parameter. These are compared through simulation studies and in the analysis of Zika cases from the first (2015-2016) epidemic in Rio de Janeiro city, Brazil. The simulation studies show that, in terms of the model assessment criterion WAIC and outlier detection, the two proposed parametrisations perform better than the model proposed by Congdon (2017) to capture outliers. In particular, the proposed parametrisations are more efficient, in terms of outlier detection, than Congdon's when outliers are neighbours. Our analysis of Zika cases finds 19 out of 160 districts of Rio as potential outliers, after accounting for the socio-development index. Our proposed model may help prioritise interventions and identify potential issues in the recording of cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.10330v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victoire Michal, La\'is Picinini Freitas, Alexandra M. Schmidt, Oswaldo Gon\c{c}alves Cruz</dc:creator>
    </item>
    <item>
      <title>Forward selection and post-selection inference in factorial designs</title>
      <link>https://arxiv.org/abs/2301.12045</link>
      <description>arXiv:2301.12045v2 Announce Type: replace 
Abstract: Ever since the seminal work of R. A. Fisher and F. Yates, factorial designs have been an important experimental tool to simultaneously estimate the effects of multiple treatment factors. In factorial designs, the number of treatment combinations grows exponentially with the number of treatment factors, which motivates the forward selection strategy based on the sparsity, hierarchy, and heredity principles for factorial effects. Although this strategy is intuitive and has been widely used in practice, its rigorous statistical theory has not been formally established. To fill this gap, we establish design-based theory for forward factor selection in factorial designs based on the potential outcome framework. We not only prove a consistency property for the factor selection procedure but also discuss statistical inference after factor selection. In particular, with selection consistency, we quantify the advantages of forward selection based on asymptotic efficiency gain in estimating factorial effects. With inconsistent selection in higher-order interactions, we propose two strategies and investigate their impact on subsequent inference. Our formulation differs from the existing literature on variable selection and post-selection inference because our theory is based solely on the physical randomization of the factorial design and does not rely on a correctly specified outcome model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.12045v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lei Shi, Jingshen Wang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Incorporating Subsampling into Bayesian Models for High-Dimensional Spatial Data</title>
      <link>https://arxiv.org/abs/2305.13221</link>
      <description>arXiv:2305.13221v3 Announce Type: replace 
Abstract: Additive spatial statistical models with weakly stationary process assumptions have become standard in spatial statistics. However, one disadvantage of such models is the computation time, which rapidly increases with the number of data points. The goal of this article is to apply an existing subsampling strategy to standard spatial additive models and to derive the spatial statistical properties. We call this strategy the ''spatial data subset model'' (SDSM) approach, which can be applied to big datasets in a computationally feasible way. Our approach has the advantage that one does not require any additional restrictive model assumptions. That is, computational gains increase as model assumptions are removed when using our model framework. This provides one solution to the computational bottlenecks that occur when applying methods such as Kriging to ''big data''. We provide several properties of this new spatial data subset model approach in terms of moments, sill, nugget, and range under several sampling designs. An advantage of our approach is that it subsamples without throwing away data, and can be implemented using datasets of any size that can be stored. We present the results of the spatial data subset model approach on simulated datasets, and on a large dataset consists of 150,000 observations of daytime land surface temperatures measured by the MODIS instrument onboard the Terra satellite.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13221v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sudipto Saha, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Spatial data fusion adjusting for preferential sampling using INLA and SPDE</title>
      <link>https://arxiv.org/abs/2309.03316</link>
      <description>arXiv:2309.03316v2 Announce Type: replace 
Abstract: Spatially misaligned data can be fused by using a Bayesian melding model that assumes that underlying all observations there is a spatially continuous Gaussian random field process. This model can be used, for example, to predict air pollution levels by combining point data from monitoring stations and areal data from satellite imagery.
  However, if the data presents preferential sampling, that is, if the observed point locations are not independent of the underlying spatial process, the inference obtained from models that ignore such a dependence structure might not be valid.
  In this paper, we present a Bayesian spatial model for the fusion of point and areal data that takes into account preferential sampling. The model combines the Bayesian melding specification and a model for the stochastically dependent sampling and underlying spatial processes.
  Fast Bayesian inference is performed using the integrated nested Laplace approximation (INLA) and the stochastic partial differential equation (SPDE) approaches. The performance of the model is assessed using simulated data in a range of scenarios and sampling strategies that can appear in real settings. The model is also applied to predict air pollution in the USA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03316v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiman Zhong, Andr\'e Victor Ribeiro Amaral, Paula Moraga</dc:creator>
    </item>
    <item>
      <title>Detecting Multiple Change Points in Distributional Sequences Derived from Structural Health Monitoring Data: An Application to Bridge Damage Detection</title>
      <link>https://arxiv.org/abs/2312.12823</link>
      <description>arXiv:2312.12823v2 Announce Type: replace 
Abstract: Detecting damage in critical structures using monitored data is a fundamental task of structural health monitoring, which is extremely important for maintaining structures' safety and life-cycle management. Based on statistical pattern recognition paradigm, damage detection can be conducted by assessing changes in the distribution of properly extracted damage-sensitive features (DSFs). This can be naturally formulated as a distributional change-point detection problem. A good change-point detector for damage detection should be scalable to large DSF datasets, applicable to different types of changes, and capable of controlling for false-positive indications. This study proposes a new distributional change-point detection method for damage detection to address these challenges. We embed the elements of a DSF distributional sequence into the Wasserstein space and construct a moving sum (MOSUM) multiple change-point detector based on Fr\'echet statistics and establish theoretical properties. Extensive simulation studies demonstrate the superiority of our proposed approach against other competitors to address the aforementioned practical requirements. We apply our method to the cable-tension measurements monitored from a long-span cable-stayed bridge for cable damage detection. We conduct a comprehensive change-point analysis for the extracted DSF data, and reveal interesting patterns from the detected changes, which provides valuable insights into cable system damage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12823v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Lei, Zhicheng Chen</dc:creator>
    </item>
    <item>
      <title>Functional Gaussian Graphical Regression Models</title>
      <link>https://arxiv.org/abs/2401.10196</link>
      <description>arXiv:2401.10196v2 Announce Type: replace 
Abstract: Functional data describe a wide range of processes encountered in practice, such as growth curves and spectral absorption. Functional regression considers a version of regression, where both the response and covariates are functional data. Evaluating both the functional relatedness between the response and covariates and the relatedness of a multivariate response function can be challenging. In this paper, we propose a solution for both these issues, by means of a functional Gaussian graphical regression model. It extends the notion of conditional Gaussian graphical models to partially separable functions. For inference, we propose a double-penalized estimator. Additionally, we present a novel adaptation of Kullback-Leibler cross-validation tailored for graph estimators which accounts for precision and regression matrices when the population presents one or more sub-groups, named joint Kullback-Leibler cross-validation. Evaluation of model performance is done in terms of Kullback-Leibler divergence and graph recovery power. We illustrate the method on a air pollution dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10196v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rita Fici, Luigi Augugliaro, Ernst-Jan Camiel Wit</dc:creator>
    </item>
    <item>
      <title>A novel methodological framework for the analysis of health trajectories and survival outcomes in heart failure patients</title>
      <link>https://arxiv.org/abs/2403.03138</link>
      <description>arXiv:2403.03138v2 Announce Type: replace 
Abstract: Heart failure (HF) contributes to circa 200,000 annual hospitalizations in France. With the increasing age of HF patients, elucidating the specific causes of inpatient mortality became a public health problematic. We introduce a novel methodological framework designed to identify prevalent health trajectories and investigate their impact on death. The initial step involves applying sequential pattern mining to characterize patients' trajectories, followed by an unsupervised clustering algorithm based on a new metric for measuring the distance between hospitalization diagnoses. Finally, a survival analysis is conducted to assess survival outcomes. The application of this framework to HF patients from a representative sample of the French population demonstrates its methodological significance in enhancing the analysis of healthcare trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03138v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juliette Murris, Tristan Amadei, Tristan Kirscher, Antoine Klein, Anne-Isabelle Tropeano, Sandrine Katsahian</dc:creator>
    </item>
    <item>
      <title>Non-Independent Components Analysis</title>
      <link>https://arxiv.org/abs/2206.13668</link>
      <description>arXiv:2206.13668v4 Announce Type: replace-cross 
Abstract: A seminal result in the ICA literature states that for $AY = \varepsilon$, if the components of $\varepsilon$ are independent and at most one is Gaussian, then $A$ is identified up to sign and permutation of its rows (Comon, 1994). In this paper we study to which extent the independence assumption can be relaxed by replacing it with restrictions on higher order moment or cumulant tensors of $\varepsilon$. We document new conditions that establish identification for several non-independent component models, e.g. common variance models, and propose efficient estimation methods based on the identification results. We show that in situations where independence cannot be assumed the efficiency gains can be significant relative to methods that rely on independence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.13668v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geert Mesters, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks for Causal Inference Under Network Confounding</title>
      <link>https://arxiv.org/abs/2211.07823</link>
      <description>arXiv:2211.07823v3 Announce Type: replace-cross 
Abstract: This paper studies causal inference with observational network data. A challenging aspect of this setting is the possibility of interference in both potential outcomes and selection into treatment, for example due to peer effects in either stage. We therefore consider a nonparametric setup in which both stages are reduced forms of simultaneous-equations models. This results in high-dimensional network confounding, where the network and covariates of all units constitute sources of selection bias. The literature predominantly assumes that confounding can be summarized by a known, low-dimensional function of these objects, and it is unclear what selection models justify common choices of functions. We show that graph neural networks (GNNs) are well suited to adjust for high-dimensional network confounding. We establish a network analog of approximate sparsity under primitive conditions on interference. This demonstrates that the model has low-dimensional structure that makes estimation feasible and justifies the use of shallow GNN architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.07823v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung, Pantelis Loupos</dc:creator>
    </item>
    <item>
      <title>Extracting the Multiscale Causal Backbone of Brain Dynamics</title>
      <link>https://arxiv.org/abs/2311.00118</link>
      <description>arXiv:2311.00118v2 Announce Type: replace-cross 
Abstract: The bulk of the research effort on brain connectivity revolves around statistical associations among brain regions, which do not directly relate to the causal mechanisms governing brain dynamics. Here we propose the multiscale causal backbone (MCB) of brain dynamics, shared by a set of individuals across multiple temporal scales, and devise a principled methodology to extract it.
  Our approach leverages recent advances in multiscale causal structure learning and optimizes the trade-off between the model fit and its complexity. Empirical assessment on synthetic data shows the superiority of our methodology over a baseline based on canonical functional connectivity networks. When applied to resting-state fMRI data, we find sparse MCBs for both the left and right brain hemispheres. Thanks to its multiscale nature, our approach shows that at low-frequency bands, causal dynamics are driven by brain regions associated with high-level cognitive functions; at higher frequencies instead, nodes related to sensory processing play a crucial role. Finally, our analysis of individual multiscale causal structures confirms the existence of a causal fingerprint of brain connectivity, thus supporting the existing extensive research in brain connectivity fingerprinting from a causal perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00118v2</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriele D'Acunto, Francesco Bonchi, Gianmarco De Francisci Morales, Giovanni Petri</dc:creator>
    </item>
    <item>
      <title>Are Ensembles Getting Better all the Time?</title>
      <link>https://arxiv.org/abs/2311.17885</link>
      <description>arXiv:2311.17885v2 Announce Type: replace-cross 
Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform as well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the average loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabilities that may be of independent interest. We illustrate our results on a medical prediction problem (diagnosing melanomas using neural nets) and a "wisdom of crowds" experiment (guessing the ratings of upcoming movies).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17885v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre-Alexandre Mattei, Damien Garreau</dc:creator>
    </item>
    <item>
      <title>Causal Interpretation of Estimands Defined by Exposure Mappings</title>
      <link>https://arxiv.org/abs/2403.08183</link>
      <description>arXiv:2403.08183v2 Announce Type: replace-cross 
Abstract: In settings with interference, researchers commonly define estimands using exposure mappings to summarize neighborhood variation in treatment assignments. This paper studies the causal interpretation of these estimands under weak restrictions on interference. We demonstrate that the estimands can exhibit unpalatable sign reversals under conventional identification conditions. This motivates the formulation of sign preservation criteria for causal interpretability. To satisfy preferred criteria, it is necessary to impose restrictions on interference, either in potential outcomes or selection into treatment. We provide sufficient conditions and show that they can be satisfied by nonparametric models with interference in both the outcome and selection stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08183v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Leung</dc:creator>
    </item>
  </channel>
</rss>
