<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Jul 2025 04:02:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>One-dimensional Discrete Models of Maximum Likelihood Degree One</title>
      <link>https://arxiv.org/abs/2507.18686</link>
      <description>arXiv:2507.18686v1 Announce Type: new 
Abstract: We settle a conjecture by Bik and Marigliano stating that the degree of a one-dimensional discrete model with rational maximum likelihood estimator is bounded above by a linear function in the size of its support, therefore showing that there are only finitely many fundamental such models for any given number of states. We study these models from a combinatorial perspective with regard to their existence and enumeration. In particular, sharp models, those whose degree attains the maximal bound, enjoy special properties and have been studied as monomial maps between unit spheres. In this way, we present a novel link between Cauchy-Riemann geometry and algebraic statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18686v1</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>math.CO</category>
      <category>math.CV</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Am\'endola, Viet Duc Nguyen, Janike Oldekop</dc:creator>
    </item>
    <item>
      <title>Robust Tail Index Estimation under Random Censoring via Minimum Density Power Divergence</title>
      <link>https://arxiv.org/abs/2507.18737</link>
      <description>arXiv:2507.18737v1 Announce Type: new 
Abstract: Based on the minimum density power divergence approach, we propose a robust estimator of the tail index for randomly right-censored data from a Pareto-type distribution. We establish its consistency and asymptotic normality. An extensive simulation study is performed to assess the finite-sample behavior of the estimator in comparison with existing ones. The methodology is further illustrated through an application to a real AIDS survival dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18737v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nour Elhouda Guesmia, Abdelhakim Necir, Djamel Meraghni</dc:creator>
    </item>
    <item>
      <title>Tree-structured Ising models under mean parameterization</title>
      <link>https://arxiv.org/abs/2507.18749</link>
      <description>arXiv:2507.18749v1 Announce Type: new 
Abstract: We assess advantages of expressing tree-structured Ising models via their mean parameterization rather than their commonly chosen canonical parameterization. This includes fixedness of marginal distributions, often convenient for dependence modeling, and the dispelling of the intractable normalizing constant otherwise hindering Ising models. We derive an analytic expression for the joint probability generating function of mean-parameterized tree-structured Ising models, conferring efficient computation methods for the distribution of the sum of its constituent random variables. The mean parameterization also allows for a stochastic representation of Ising models, providing straightforward sampling methods. We furthermore show that Markov random fields with fixed Poisson marginal distributions may act as an efficient and accurate approximation for tree-structured Ising models, in the spirit of Poisson approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18749v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin C\^ot\'e, H\'el\`ene Cossette, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>A Better Linear Unbiased Estimator for Averages over Discrete Structures</title>
      <link>https://arxiv.org/abs/2507.19294</link>
      <description>arXiv:2507.19294v1 Announce Type: new 
Abstract: Given an i.i.d. sample drawn from some probability distribution on a finite set, the best (in the sense of least variance) linear unbiased estimator (BLUE) of the average of any quantity with respect to that distribution is the sample average of the quantity. Here we consider the situation in which, together with the sample, also the probability mass (possibly unnormalized) at each sample point is provided. We show that with that information BLUE can be systematically improved. The proposed procedure is expected to have applications in statistical physics, where it is common to have a closed-form specification of the relevant (unnormalized) probability distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19294v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bastiaan J. Braams</dc:creator>
    </item>
    <item>
      <title>Riesz representers for the rest of us</title>
      <link>https://arxiv.org/abs/2507.19413</link>
      <description>arXiv:2507.19413v1 Announce Type: new 
Abstract: The application of semiparametric efficient estimators, particularly those that leverage machine learning, is rapidly expanding within epidemiology and causal inference. Much of the recent methodological literature on these estimators relies heavily on the Riesz representation theorem and Riesz regression. This paper aims to introduce the Riesz representation theorem to an applied audience, explaining why and how Riesz regression is becoming widely used in the semiparametric estimator statistical literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19413v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicholas T. Williams, Oliver J. Hines, Kara E. Rudolph</dc:creator>
    </item>
    <item>
      <title>Bayesian Inverse Problems on Metric Graphs</title>
      <link>https://arxiv.org/abs/2507.18951</link>
      <description>arXiv:2507.18951v1 Announce Type: cross 
Abstract: This paper studies the formulation, well-posedness, and numerical solution of Bayesian inverse problems on metric graphs, in which the edges represent one-dimensional wires connecting vertices. We focus on the inverse problem of recovering the diffusion coefficient of a (fractional) elliptic equation on a metric graph from noisy measurements of the solution. Well-posedness hinges on both stability of the forward model and an appropriate choice of prior. We establish the stability of elliptic and fractional elliptic forward models using recent regularity theory for differential equations on metric graphs. For the prior, we leverage modern Gaussian Whittle--Mat\'ern process models on metric graphs with sufficiently smooth sample paths. Numerical results demonstrate accurate reconstruction and effective uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18951v1</guid>
      <category>math.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Wenwen Li, Daniel Sanz-Alonso</dc:creator>
    </item>
    <item>
      <title>Weak convergence of predictive distributions</title>
      <link>https://arxiv.org/abs/2507.19169</link>
      <description>arXiv:2507.19169v1 Announce Type: cross 
Abstract: Let $(X_n)$ be a sequence of random variables with values in a standard Borel space $S$. We investigate the condition \begin{gather}\label{x56w1q} E\bigl\{f(X_{n+1})\mid X_1,\ldots,X_n\bigr\}\,\quad\text{converges in probability,}\tag{*} \\\text{as }n\rightarrow\infty,\text{ for each bounded Borel function }f:S\rightarrow\mathbb{R}.\notag \end{gather} Some consequences of \eqref{x56w1q} are highlighted and various sufficient conditions for it are obtained. In particular, \eqref{x56w1q} is characterized in terms of stable convergence. Since \eqref{x56w1q} holds whenever $(X_n)$ is conditionally identically distributed, three weak versions of the latter condition are investigated as well. For each of such versions, our main goal is proving (or disproving) that \eqref{x56w1q} holds. Several counterexamples are given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19169v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabrizio Leisen, Luca Pratelli, Pietro Rigo</dc:creator>
    </item>
    <item>
      <title>On a $T_1$ Transport inequality for the adapted Wasserstein distance</title>
      <link>https://arxiv.org/abs/2507.19215</link>
      <description>arXiv:2507.19215v1 Announce Type: cross 
Abstract: The $L^1$ transport-entropy inequality (or $T_1$ inequality), which bounds the $1$-Wasserstein distance in terms of the relative entropy, is known to characterize Gaussian concentration. To extend the $T_1$ inequality to laws of discrete-time processes while preserving their temporal structure, we investigate the adapted $T_1$ inequality which relates the $1$-adapted Wasserstein distance to the relative entropy. Building on the Bolley--Villani inequality, we establish the adapted $T_1$ inequality under the same moment assumption as the classical $T_1$ inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19215v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonghwa Park</dc:creator>
    </item>
    <item>
      <title>Finding a dense submatrix of a random matrix. Sharp bounds for online algorithms</title>
      <link>https://arxiv.org/abs/2507.19259</link>
      <description>arXiv:2507.19259v1 Announce Type: cross 
Abstract: We consider the problem of finding a dense submatrix of a matrix with i.i.d. Gaussian entries, where density is measured by average value. This problem arose from practical applications in biology and social sciences \cites{madeira-survey,shabalin2009finding} and is known to exhibit a computation-to-optimization gap between the optimal value and best values achievable by existing polynomial time algorithms. In this paper we consider the class of online algorithms, which includes the best known algorithm for this problem, and derive a tight approximation factor ${4\over 3\sqrt{2}}$ for this class. The result is established using a simple implementation of recently developed Branching-Overlap-Gap-Property \cite{huang2025tight}. We further extend our results to $(\mathbb R^n)^{\otimes p}$ tensors with i.i.d. Gaussian entries, for which the approximation factor is proven to be ${2\sqrt{p}/(1+p)}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19259v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shankar Bhamidi, David Gamarnik, Shuyang Gong</dc:creator>
    </item>
    <item>
      <title>Perfect Clustering in Very Sparse Diverse Multiplex Networks</title>
      <link>https://arxiv.org/abs/2507.19423</link>
      <description>arXiv:2507.19423v1 Announce Type: cross 
Abstract: The paper studies the DIverse MultiPLEx Signed Generalized Random Dot Product Graph (DIMPLE-SGRDPG) network model (Pensky (2024)), where all layers of the network have the same collection of nodes. In addition, all layers can be partitioned into groups such that the layers in the same group are embedded in the same ambient subspace but otherwise matrices of connection probabilities can be all different. This setting includes majority of multilayer network models as its particular cases. The key task in this model is to recover the groups of layers with unique subspace structures, since the case where all layers of the network are embedded in the same subspace has been fairly well studied. Until now, clustering of layers in such networks was based on the layer-per-layer analysis, which required the multilayer network to be sufficiently dense. Nevertheless, in this paper we succeeded in pooling information in all layers together and providing a tensor-based methodology that ensures perfect clustering for a much sparser network. Our theoretical results, established under intuitive non-restrictive assumptions, assert that the new technique achieves perfect clustering under sparsity conditions that, up to logarithmic factors, coincide with the computational lower bound derived for a much simpler model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19423v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianna Pensky</dc:creator>
    </item>
    <item>
      <title>An extended latent factor framework for ill-posed linear regression</title>
      <link>https://arxiv.org/abs/2307.08377</link>
      <description>arXiv:2307.08377v2 Announce Type: replace 
Abstract: In many applications, particularly in the natural sciences, the available high-dimensional set of features may contain variables that are not correlated with the response under consideration. Such irrelevant features can, in certain cases, hinder both the accurate estimation and meaningful interpretation of the effects of the relevant features on the response. At the same time, the relevant features may also be well-approximated within a low-dimensional linear subspace, rendering the problem ill-posed. These observations motivate an extension of the classical latent factor model for linear regression. In this extended framework, it is assumed that, up to an unknown orthogonal transformation, the feature set comprises two subsets: one relevant and one irrelevant to the response. A joint low-dimensionality is imposed solely on the relevant features and the response variable. This setting enables the analysis of arbitrary linear dimensionality reduction techniques under a random design setting. In particular, it is demonstrated why principal component regression (PCR) is generally unsuitable for most applications. The framework also allows for a comprehensive analysis of the partial least squares (PLS) algorithm under random design. High-probability convergence rates are established for the sample PLS estimator with respect to an oracle latent coefficient vector, along with the corresponding linear prediction risk. Additionally, it is shown that early stopping can be guided by the empirical condition numbers of the projected design matrix. The theoretical results are validated through numerical studies on both real and simulated datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.08377v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca Finocchio, Tatyana Krivobokova</dc:creator>
    </item>
    <item>
      <title>Average partial effect estimation using double machine learning</title>
      <link>https://arxiv.org/abs/2308.09207</link>
      <description>arXiv:2308.09207v3 Announce Type: replace 
Abstract: Single-parameter summaries of variable effects in regression settings are desirable for ease of interpretation. However (partially) linear models for example, which would deliver these, may fit poorly to the data. On the other hand, an interpretable summary of the contribution of a given predictor is provided by the so-called average partial effect: the average slope of the regression function with respect to the predictor of interest. Although one can construct a doubly robust procedure for estimating this quantity, it entails estimating the derivative of the conditional mean and also the conditional score of the predictor of interest given all others, tasks which can be very challenging in moderate dimensions: in particular, popular decision tree based regression methods cannot be used. In this work we introduce an approach for estimating the average partial effect whose accuracy depends primarily on the estimation of certain regression functions, which may be performed by user-chosen machine learning methods that produce potentially non-differentiable estimates. Our procedure involves resmoothing a given first-stage regression estimator to produce a differentiable version, and modelling the conditional distribution of the predictor of interest through a location--scale model. We show that with the latter assumption, surprisingly the overall error in estimating the conditional score is controlled by a sum of errors of estimating the conditional mean and conditional standard deviation, and the estimation error in a much more tractable univariate score estimation problem. Our theory makes use of a new result on the sub-Gaussianity of Lipschitz score functions that may be of independent interest. We demonstrate the attractive numerical performance of our approach in a variety of settings including ones with misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09207v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harvey Klyne, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>On determinantal point processes with nonsymmetric kernels</title>
      <link>https://arxiv.org/abs/2406.03360</link>
      <description>arXiv:2406.03360v3 Announce Type: replace 
Abstract: Determinantal point processes (DPPs for short) are a class of repulsive point processes. They have found some statistical applications to model spatial point pattern datasets with repulsion between close points. In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric. While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties. In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels. We also generalize various common results on DPPs. We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03360v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poinas Arnaud</dc:creator>
    </item>
    <item>
      <title>On unbiased estimators for functions of the rate parameter of the exponential distribution</title>
      <link>https://arxiv.org/abs/2506.20005</link>
      <description>arXiv:2506.20005v2 Announce Type: replace 
Abstract: In this paper, we explicitly derive unbiased estimators for various functions of the rate parameter of the exponential distribution in the absence of a location parameter, including powers of the rate parameter, the $q$th quantile, the $p$th moment, the survival function, the maximum, minimum, probability density function, mean past lifetime, moment generating function, and others. This work non-trivially complements established formulas for unbiased estimators of functions of parameters of the location-rate exponential distribution. Additionally, we establish a result demonstrating the asymptotic normality of the proposed unbiased estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20005v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Eduardo Yoshio Nakano</dc:creator>
    </item>
    <item>
      <title>Bernoulli amputation</title>
      <link>https://arxiv.org/abs/2407.18572</link>
      <description>arXiv:2407.18572v2 Announce Type: replace-cross 
Abstract: An approach to amputation, the process of introducing missing values to a complete dataset, is presented. It allows to construct missingness indicators in a flexible and principled way via copulas and Bernoulli margins and to incorporate dependence in missingness patterns. Besides more classical missingness models such as missing completely at random, missing at random, and missing not at random, the approach is able to model structured missingness such as block missingness and, via mixtures, monotone missingness, which are patterns of missing data frequently found in real-life datasets. Properties such as joint missingness probabilities or missingness correlation are derived mathematically. The approach is demonstrated with mathematical examples and empirical illustrations in terms of a well-known dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18572v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Hofert, James Jackson, Niels Hagenbuch</dc:creator>
    </item>
    <item>
      <title>Lower Bounds on the Size of Markov Equivalence Classes</title>
      <link>https://arxiv.org/abs/2506.20933</link>
      <description>arXiv:2506.20933v2 Announce Type: replace-cross 
Abstract: Causal discovery algorithms typically recover causal graphs only up to their Markov equivalence classes unless additional parametric assumptions are made. The sizes of these equivalence classes reflect the limits of what can be learned about the underlying causal graph from purely observational data. Under the assumptions of acyclicity, causal sufficiency, and a uniform model prior, Markov equivalence classes are known to be small on average. In this paper, we show that this is no longer the case when any of these assumptions is relaxed. Specifically, we prove exponentially large lower bounds for the expected size of Markov equivalence classes in three settings: sparse random directed acyclic graphs, uniformly random acyclic directed mixed graphs, and uniformly random directed cyclic graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20933v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Jahn, Frederick Eberhardt, Leonard J. Schulman</dc:creator>
    </item>
    <item>
      <title>A Fisher's exact test justification of the TF-IDF term-weighting scheme</title>
      <link>https://arxiv.org/abs/2507.15742</link>
      <description>arXiv:2507.15742v2 Announce Type: replace-cross 
Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably the most celebrated mathematical expression in the history of information retrieval. Conceived as a simple heuristic quantifying the extent to which a given term's occurrences are concentrated in any one given document out of many, TF-IDF and its many variants are routinely used as term-weighting schemes in diverse text analysis applications. There is a growing body of scholarship dedicated to placing TF-IDF on a sound theoretical foundation. Building on that tradition, this paper justifies the use of TF-IDF to the statistics community by demonstrating how the famed expression can be understood from a significance testing perspective. We show that the common TF-IDF variant TF-ICF is, under mild regularity conditions, closely related to the negative logarithm of the $p$-value from a one-tailed version of Fisher's exact test of statistical significance. As a corollary, we establish a connection between TF-IDF and the said negative log-transformed $p$-value under certain idealized assumptions. We further demonstrate, as a limiting case, that this same quantity converges to TF-IDF in the limit of an infinitely large document collection. The Fisher's exact test justification of TF-IDF equips the working statistician with a ready explanation of the term-weighting scheme's long-established effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15742v2</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Sheridan, Zeyad Ahmed, Aitazaz A. Farooque</dc:creator>
    </item>
  </channel>
</rss>
