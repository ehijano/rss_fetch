<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 05:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Robust Mean Estimation With Auxiliary Samples</title>
      <link>https://arxiv.org/abs/2501.18095</link>
      <description>arXiv:2501.18095v1 Announce Type: new 
Abstract: In data-driven learning and inference tasks, the high cost of acquiring samples from the target distribution often limits performance. A common strategy to mitigate this challenge is to augment the limited target samples with data from a more accessible "auxiliary" distribution. This paper establishes fundamental limits of this approach by analyzing the improvement in the mean square error (MSE) when estimating the mean of the target distribution. Using the Wasserstein-2 metric to quantify the distance between distributions, we derive expressions for the worst-case MSE when samples are drawn (with labels) from both a target distribution and an auxiliary distribution within a specified Wasserstein-2 distance from the target distribution. We explicitly characterize the achievable MSE and the optimal estimator in terms of the problem dimension, the number of samples from the target and auxiliary distributions, the Wasserstein-2 distance, and the covariance of the target distribution. We note that utilizing samples from the auxiliary distribution effectively improves the MSE when the squared radius of the Wasserstein-2 uncertainty ball is small compared to the variance of the true distribution and the number of samples from the true distribution is limited. Numerical simulations in the Gaussian location model illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18095v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barron Han, Danil Akhtiamov, Reza Ghane, Babak Hassibi</dc:creator>
    </item>
    <item>
      <title>A theory of shape regularity for local regression maps</title>
      <link>https://arxiv.org/abs/2501.18204</link>
      <description>arXiv:2501.18204v1 Announce Type: new 
Abstract: We introduce the concept of shape-regular regression maps as a framework to derive optimal rates of convergence for various non-parametric local regression estimators. Using Vapnik-Chervonenkis theory, we establish upper and lower bounds on the pointwise and the sup-norm estimation error, even when the localization procedure depends on the full data sample, and under mild conditions on the regression model. Our results demonstrate that the shape regularity of regression maps is not only sufficient but also necessary to achieve an optimal rate of convergence for Lipschitz regression functions. To illustrate the theory, we establish new concentration bounds for many popular local regression methods such as nearest neighbors algorithm, CART-like regression trees and several purely random trees including Mondrian trees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18204v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\'er\'emy Bettinger (ENSAI, CREST), Fran\c{c}ois Portier (CREST, ENSAI), Adrien Saumard (CREST, ENSAI)</dc:creator>
    </item>
    <item>
      <title>The No-Underrun Sampler: A Locally-Adaptive, Gradient-Free MCMC Method</title>
      <link>https://arxiv.org/abs/2501.18548</link>
      <description>arXiv:2501.18548v1 Announce Type: new 
Abstract: In this work, we introduce the No-Underrun Sampler (NURS): a locally-adaptive, gradient-free Markov chain Monte Carlo method that combines elements of Hit-and-Run and the No-U-Turn Sampler. NURS dynamically adapts to the local geometry of the target distribution without requiring gradient evaluations, making it especially suitable for applications where gradients are unavailable or costly. We establish key theoretical properties, including reversibility, formal connections to Hit-and-Run and Random Walk Metropolis, Wasserstein contraction comparable to Hit-and-Run in Gaussian targets, and bounds on the total variation distance between the transition kernels of Hit-and-Run and NURS. Finally, we demonstrate - through empirical experiments supported by theoretical insights - that NURS can effectively sample Neal's funnel, a challenging multi-scale distribution from Bayesian hierarchical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18548v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nawaf Bou-Rabee, Bob Carpenter, Sifan Liu, Stefan Oberd\"orster</dc:creator>
    </item>
    <item>
      <title>Universal Inference for Incomplete Discrete Choice Models</title>
      <link>https://arxiv.org/abs/2501.17973</link>
      <description>arXiv:2501.17973v1 Announce Type: cross 
Abstract: A growing number of empirical models exhibit set-valued predictions. This paper develops a tractable inference method with finite-sample validity for such models. The proposed procedure uses a robust version of the universal inference framework by Wasserman et al. (2020) and avoids using moment selection tuning parameters, resampling, or simulations. The method is designed for constructing confidence intervals for counterfactual objects and other functionals of the underlying parameter. It can be used in applications that involve model incompleteness, discrete and continuous covariates, and parameters containing nuisance components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17973v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hiroaki Kaido, Yi Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal Survey Design for Private Mean Estimation</title>
      <link>https://arxiv.org/abs/2501.18121</link>
      <description>arXiv:2501.18121v1 Announce Type: cross 
Abstract: This work identifies the first privacy-aware stratified sampling scheme that minimizes the variance for general private mean estimation under the Laplace, Discrete Laplace (DLap) and Truncated-Uniform-Laplace (TuLap) mechanisms within the framework of differential privacy (DP). We view stratified sampling as a subsampling operation, which amplifies the privacy guarantee; however, to have the same final privacy guarantee for each group, different nominal privacy budgets need to be used depending on the subsampling rate. Ignoring the effect of DP, traditional stratified sampling strategies risk significant variance inflation. We phrase our optimal survey design as an optimization problem, where we determine the optimal subsampling sizes for each group with the goal of minimizing the variance of the resulting estimator. We establish strong convexity of the variance objective, propose an efficient algorithm to identify the integer-optimal design, and offer insights on the structure of the optimal design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18121v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yu-Wei Chen, Raghu Pasupathy, Jordan A. Awan</dc:creator>
    </item>
    <item>
      <title>Proofs for Folklore Theorems on the Radon-Nikodym Derivative</title>
      <link>https://arxiv.org/abs/2501.18374</link>
      <description>arXiv:2501.18374v1 Announce Type: cross 
Abstract: Rigorous statements and formal proofs are presented for both foundational and advanced folklore theorems on the Radon-Nikodym derivative. The cases of product and marginal measures are carefully considered; and the hypothesis under which the statements hold are rigorously enumerated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18374v1</guid>
      <category>cs.IT</category>
      <category>math.HO</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaiza Bermudez, Gaetan Bisson, I\~naki Esnaola, Samir M. Perlaza</dc:creator>
    </item>
    <item>
      <title>One-Bit Distributed Mean Estimation with Unknown Variance</title>
      <link>https://arxiv.org/abs/2501.18502</link>
      <description>arXiv:2501.18502v1 Announce Type: cross 
Abstract: In this work, we study the problem of distributed mean estimation with $1$-bit communication constraints when the variance is unknown. We focus on the specific case where each user has access to one i.i.d. sample drawn from a distribution that belongs to a scale-location family, and is limited to sending just a single bit of information to a central server whose goal is to estimate the mean. We propose non-adaptive and adaptive estimators that are shown to be asymptotically normal. We derive bounds on the asymptotic (in the number of users) Mean Squared Error (MSE) achieved by these estimators. For a class of symmetric log-concave distributions, we derive matching lower bounds for the MSE achieved by adaptive estimators, proving the optimality of our scheme. We show that non-adaptive estimators can be strictly suboptimal by deriving a lower bound on the MSE achieved by any non-adaptive estimator for Gaussian distributions and demonstrating a positive gap between this and the MSE achieved by our adaptive scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18502v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ritesh Kumar, Shashank Vatedka</dc:creator>
    </item>
    <item>
      <title>Layered Models can "Automatically" Regularize and Discover Low-Dimensional Structures via Feature Learning</title>
      <link>https://arxiv.org/abs/2310.11736</link>
      <description>arXiv:2310.11736v3 Announce Type: replace 
Abstract: Layered models like neural networks appear to extract key features from data through empirical risk minimization, yet the theoretical understanding for this process remains unclear. Motivated by these observations, we study a two-layer nonparametric regression model where the input undergoes a linear transformation followed by a nonlinear mapping to predict the output, mirroring the structure of two-layer neural networks. In our model, both layers are optimized jointly through empirical risk minimization, with the nonlinear layer modeled by a reproducing kernel Hilbert space induced by a rotation and translation invariant kernel, regularized by a ridge penalty.
  Our main result shows that the two-layer model can "automatically" induce regularization and facilitate feature learning. Specifically, the two-layer model promotes dimensionality reduction in the linear layer and identifies a parsimonious subspace of relevant features -- even without applying any norm penalty on the linear layer. Notably, this regularization effect arises directly from the model's layered structure, independent of optimization dynamics.
  More precisely, assuming the covariates have nonzero explanatory power for the response only through a low dimensional subspace (central mean subspace), the linear layer consistently estimates both the subspace and its dimension. This demonstrates that layered models can inherently discover low-complexity solutions relevant for prediction, without relying on conventional regularization methods. Real-world data experiments further demonstrate the persistence of this phenomenon in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11736v3</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunlu Chen, Yang Li, Keli Liu, Feng Ruan</dc:creator>
    </item>
    <item>
      <title>Interference Among First-Price Pacing Equilibria: A Bias and Variance Analysis</title>
      <link>https://arxiv.org/abs/2402.07322</link>
      <description>arXiv:2402.07322v3 Announce Type: replace 
Abstract: Online A/B testing is widely used in the internet industry to inform decisions on new feature roll-outs. For online marketplaces (such as advertising markets), standard approaches to A/B testing may lead to biased results when buyers operate under a budget constraint, as budget consumption in one arm of the experiment impacts performance of the other arm. To counteract this interference, one can use a budget-split design where the budget constraint operates on a per-arm basis and each arm receives an equal fraction of the budget, leading to ``budget-controlled A/B testing.'' Despite clear advantages of budget-controlled A/B testing, performance degrades when budget are split too small, limiting the overall throughput of such systems. In this paper, we propose a parallel budget-controlled A/B testing design where we use market segmentation to identify submarkets in the larger market, and we run parallel experiments on each submarket.
  Our contributions are as follows: First, we introduce and demonstrate the effectiveness of the parallel budget-controlled A/B test design with submarkets in a large online marketplace environment. Second, we formally define market interference in first-price auction markets using the first price pacing equilibrium (FPPE) framework. Third, we propose a debiased surrogate that eliminates the first-order bias of FPPE, drawing upon the principles of sensitivity analysis in mathematical programs. Fourth, we derive a plug-in estimator for the surrogate and establish its asymptotic normality. Fifth, we provide an estimation procedure for submarket parallel budget-controlled A/B tests. Finally, we present numerical examples on semi-synthetic data, confirming that the debiasing technique achieves the desired coverage properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07322v3</guid>
      <category>math.ST</category>
      <category>cs.GT</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luofeng Liao, Christian Kroer, Sergei Leonenkov, Okke Schrijvers, Liang Shi, Nicolas Stier-Moses, Congshan Zhang</dc:creator>
    </item>
    <item>
      <title>Weighted past and paired dynamic varentropy measures, their properties and usefulness</title>
      <link>https://arxiv.org/abs/2405.06428</link>
      <description>arXiv:2405.06428v2 Announce Type: replace 
Abstract: We introduce two uncertainty measures, say weighted past varentropy (WPVE) and weighted paired dynamic varentropy (WPDVE). Several properties of these proposed measures, including their effect under the monotone transformations are studied. An upper bound of the WPVE using the weighted past Shannon entropy and a lower bound of the WPVE are obtained. Further, the WPVE is studied for the proportional reversed hazard rate (PRHR) models. Upper and lower bounds of the WPDVE are derived. In addition, the non-parametric kernel estimates of the WPVE and WPDVE are proposed. Furthermore, the maximum likelihood estimation technique is employed to estimate WPVE and WPDVE for an exponential population. A numerical simulation is provided to observe the behaviour of the proposed estimates. A real data set is analysed, and then the estimated values of WPVE are obtained. Based on the bootstrap samples generated from the real data set, the performance of the non-parametric and parametric estimators of the WPVE and WPDVE is compared in terms of the absolute bias and mean squared error (MSE). Finally, we have reported an application of WPVE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06428v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shital Saha, Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>Multivariate change estimation for a stochastic heat equation from local measurements</title>
      <link>https://arxiv.org/abs/2409.15059</link>
      <description>arXiv:2409.15059v2 Announce Type: replace 
Abstract: We study a stochastic heat equation with piecewise constant diffusivity $\theta$ having a jump at a hypersurface $\Gamma$ that splits the underlying space $[0,1]^d$, $d\geq2,$ into two disjoint sets $\Lambda_-\cup\Lambda_+.$ Based on multiple spatially localized measurement observations on a regular $\delta$-grid of $[0,1]^d$, we propose a joint M-estimator for the diffusivity values and the set $\Lambda_+$ that is inspired by statistical image reconstruction methods. We study convergence of the domain estimator $\hat{\Lambda}_+$ in the vanishing resolution level regime $\delta \to 0$ and with respect to the expected symmetric difference pseudometric. As a first main finding we give a characterization of the convergence rate for $\hat{\Lambda}_+$ in terms of the complexity of $\Gamma$ measured by the number of intersecting hypercubes from the regular $\delta$-grid. Furthermore, for the special case of domains $\Lambda_+$ that are built from hypercubes from the $\delta$-grid, we demonstrate that perfect identification with overwhelming probability is possible with a slight modification of the estimation approach. Implications of our general results are discussed under two specific structural assumptions on $\Lambda_+$. For a $\beta$-H\"older smooth boundary fragment $\Gamma$, the set $\Lambda_+$ is estimated with rate $\delta^\beta$. If we assume $\Lambda_+$ to be convex, we obtain a $\delta$-rate. While our approach only aims at optimal domain estimation rates, we also demonstrate consistency of our diffusivity estimators, which is strengthened to a CLT at minimax optimal rate for sets $\Lambda_+$ anchored on the $\delta$-grid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15059v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Tiepner, Lukas Trottner</dc:creator>
    </item>
    <item>
      <title>Phase transitions for the existence of unregularized M-estimators in single index models</title>
      <link>https://arxiv.org/abs/2501.03163</link>
      <description>arXiv:2501.03163v2 Announce Type: replace 
Abstract: This paper studies phase transitions for the existence of unregularized M-estimators under proportional asymptotics where the sample size $n$ and feature dimension $p$ grow proportionally with $n/p \to \delta \in (1, \infty)$. We study the existence of M-estimators in single-index models where the response $y_i$ depends on covariates $x_i \sim N(0, I_p)$ through an unknown index ${w} \in \mathbb{R}^p$ and an unknown link function. An explicit expression is derived for the critical threshold $\delta_\infty$ that determines the phase transition for the existence of the M-estimator, generalizing the results of Cand\'es &amp; Sur (2020) for binary logistic regression to other single-index models.
  Furthermore, we investigate the existence of a solution to the nonlinear system of equations governing the asymptotic behavior of the M-estimator when it exists. The existence of solution to this system for $\delta &gt; \delta_\infty$ remains largely unproven outside the global null in binary logistic regression. We address this gap with a proof that the system admits a solution if and only if $\delta &gt; \delta_\infty$, providing a comprehensive theoretical foundation for proportional asymptotic results that require as a prerequisite the existence of a solution to the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03163v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre C. Bellec, Takuya Koriyama</dc:creator>
    </item>
    <item>
      <title>Dynamic treatment effects: high-dimensional inference under model misspecification</title>
      <link>https://arxiv.org/abs/2111.06818</link>
      <description>arXiv:2111.06818v3 Announce Type: replace-cross 
Abstract: Estimating dynamic treatment effects is crucial across various disciplines, providing insights into the time-dependent causal impact of interventions. However, this estimation poses challenges due to time-varying confounding, leading to potentially biased estimates. Furthermore, accurately specifying the growing number of treatment assignments and outcome models with multiple exposures appears increasingly challenging to accomplish. Double robustness, which permits model misspecification, holds great value in addressing these challenges. This paper introduces a novel "sequential model doubly robust" estimator. We develop novel moment-targeting estimates to account for confounding effects and establish that root-$N$ inference can be achieved as long as at least one nuisance model is correctly specified at each exposure time, despite the presence of high-dimensional covariates. Although the nuisance estimates themselves do not achieve root-$N$ rates, the carefully designed loss functions in our framework ensure final root-$N$ inference for the causal parameter of interest. Unlike off-the-shelf high-dimensional methods, which fail to deliver robust inference under model misspecification even within the doubly robust framework, our newly developed loss functions address this limitation effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.06818v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqian Zhang, Weijie Ji, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>Second-Order Regular Variation and Second-Order Approximation of Hawkes Processes</title>
      <link>https://arxiv.org/abs/2311.02655</link>
      <description>arXiv:2311.02655v2 Announce Type: replace-cross 
Abstract: This paper provides and extends second-order versions of several fundamental theorems on first-order regularly varying functions such as Karamata's theorem/representation and Tauberian's theorem. Our results are used to establish second-order approximations for the mean and variance of Hawkes processes with general kernels. Our approximations provide novel insights into the asymptotic behavior of Hawkes processes. They are also of key importance when establishing functional limit theorems for Hawkes processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02655v2</guid>
      <category>math.PR</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 31 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulrich Horst, Wei Xu</dc:creator>
    </item>
  </channel>
</rss>
