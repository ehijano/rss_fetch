<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Feb 2026 02:48:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ACE-RTL: When Agentic Context Evolution Meets RTL-Specialized LLMs</title>
      <link>https://arxiv.org/abs/2602.10218</link>
      <description>arXiv:2602.10218v1 Announce Type: new 
Abstract: Recent advances in large language models (LLMs) have sparked growing interest in applying them to hardware design automation, particularly for accurate RTL code generation. Prior efforts follow two largely independent paths: (i) training domain-adapted RTL models to internalize hardware semantics, (ii) developing agentic systems that leverage frontier generic LLMs guided by simulation feedback. However, these two paths exhibit complementary strengths and weaknesses. In this work, we present ACE-RTL that unifies both directions through Agentic Context Evolution (ACE). ACE-RTL integrates an RTL-specialized LLM, trained on a large-scale dataset of 1.7 million RTL samples, with a frontier reasoning LLM through three synergistic components: the generator, reflector, and coordinator. These components iteratively refine RTL code toward functional correctness. We further introduce a parallel scaling strategy that significantly reduces the number of iterations required to reach correct solutions. On the Comprehensive Verilog Design Problems (CVDP) benchmark, ACE-RTL achieves up to a 44.87% pass rate improvement over 14 competitive baselines while requiring only four iterations on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10218v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenhui Deng, Zhongzhi Yu, Guan-Ting Liu, Nathaniel Pinckney, Haoxing Ren</dc:creator>
    </item>
    <item>
      <title>Area-Efficient In-Memory Computing for Mixture-of-Experts via Multiplexing and Caching</title>
      <link>https://arxiv.org/abs/2602.10254</link>
      <description>arXiv:2602.10254v1 Announce Type: new 
Abstract: Mixture-of-Experts (MoE) layers activate a subset of model weights, dubbed experts, to improve model performance. MoE is particularly promising for deployment on process-in-memory (PIM) architectures, because PIM can naturally fit experts separately and provide great benefits for energy efficiency. However, PIM chips often suffer from large area overhead, especially in the peripheral circuits. In this paper, we propose an area-efficient in-memory computing architecture for MoE transformers. First, to reduce area, we propose a crossbar-level multiplexing strategy that exploits MoE sparsity: experts are deployed on crossbars and multiple crossbars share the same peripheral circuits. Second, we propose expert grouping and group-wise scheduling methods to alleviate the load imbalance and contention overhead caused by sharing. In addition, to address the problem that the expert choice router requires access to all hidden states during generation, we propose a gate-output (GO)cache to store necessary results and bypass expensive additional computation. Experiments show that our approaches improve the area efficiency of the MoE part by up to 2.2x compared to a SOTA architecture. During generation, the cache improves performance and energy efficiency by 4.2x and 10.1x, respectively, compared to the baseline when generating 8 tokens. The total performance density achieves 15.6 GOPS/W/mm2. The code is open source at https://github.com/superstarghy/MoEwithPIM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10254v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanyuan Gao, Xiaoxuan Yang</dc:creator>
    </item>
    <item>
      <title>DRAMPyML: A Formal Description of DRAM Protocols with Timed Petri Nets</title>
      <link>https://arxiv.org/abs/2602.10654</link>
      <description>arXiv:2602.10654v1 Announce Type: new 
Abstract: The JEDEC committee defines various domain-specific DRAM standards. These standards feature increasingly complex and evolving protocol specifications, which are detailed in timing diagrams and command tables. Understanding these protocols is becoming progressively challenging as new features and complex device hierarchies are difficult to comprehend without an expressive model. While each JEDEC standard features a simplified state machine, this state machine fails to reflect the parallel operation of memory banks.
  In this paper, we present an evolved modeling approach based on timed Petri nets and Python. This model provides a more accurate representation of DRAM protocols, making them easier to understand and directly executable, which enables the evaluation of interesting metrics and the verification of controller RTL models, DRAM logic and memory simulators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10654v1</guid>
      <category>cs.AR</category>
      <category>cs.FL</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derek Christ, Thomas Zimmermann, Philippe Barbie, Dmitri Saberi, Yao Yin, Matthias Jung</dc:creator>
    </item>
    <item>
      <title>Fault Tolerant Design of IGZO-based Binary Search ADCs</title>
      <link>https://arxiv.org/abs/2602.10790</link>
      <description>arXiv:2602.10790v1 Announce Type: new 
Abstract: Thin-film technologies such as Indium Gallium Zinc Oxide (IGZO) enable Flexible Electronics (FE) for emerging applications in wearable sensing, personal health monitoring, and large-area systems. Analog-to-digital converters (ADCs) serve as critical sensor interfaces in these systems. Yet, their vulnerability to manufacturing defects remains poorly understood despite unipolar technologies' inherently high defect densities and process variations compared to mature CMOS technologies. We present a hierarchical fault injection framework to characterize defect sensitivity in Binary Search ADCs implemented in n-type only technologies. Our methodology combines transistor-level defect characterization with system-level fault propagation analysis, enabling efficient exploration of both single and multiple fault scenarios across the conversion hierarchy. The framework identifies critical fault-sensitive circuit components and enables selective redundancy strategies targeting only the most sensitive components. The resulting defect-tolerant designs improve fault coverage from 60% to 92% under single-fault injections and from 34% to 77.6% under multi-fault injection, while incurring only 4.2% area overhead and 6% power increase. While validated on IGZO-TFTs, the methodology applies to all emerging unipolar technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10790v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paula Carolina Lozano Duarte, Sule Ozev, Mehdi Tahoori</dc:creator>
    </item>
    <item>
      <title>From Buffers to Registers: Unlocking Fine-Grained FlashAttention with Hybrid-Bonded 3D NPU Co-Design</title>
      <link>https://arxiv.org/abs/2602.11016</link>
      <description>arXiv:2602.11016v1 Announce Type: new 
Abstract: Transformer-based models dominate modern AI workloads but exacerbate memory bottlenecks due to their quadratic attention complexity and ever-growing model sizes. Existing accelerators, such as Groq and Cerebras, mitigate off-chip traffic with large on-chip caches, while algorithmic innovations such as FlashAttention fuse operators to avoid materializing large attention matrices. However, as off-chip traffic decreases, our measurements show that on-chip SRAM accesses account for over 60% of energy in long-sequence workloads, making cache access the new bottleneck. We propose 3D-Flow, a hybrid-bonded, 3D-stacked spatial accelerator that enables register-to-register communication across vertically partitioned PE tiers. Unlike 2D multi-array architectures limited by NoC-based router-to-router transfers, 3D-Flow leverages sub-10 um vertical TSVs to sustain cycle-level operator pipelining with minimal overhead. On top of this architecture, we design 3D-FlashAttention, a fine-grained scheduling method that balances latency across tiers, forming a bubble-free vertical dataflow without on-chip SRAM roundtrips. Evaluations on Transformer workloads (OPT and QWEN models) show that our 3D spatial accelerator reduces 46-93% energy consumption and achieves 1.4x-7.6x speedups compared to state-of-the-art 2D and 3D designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11016v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinxin Yu, Yudong Pan, Mengdi Wang, Huawei Li, Yinhe Han, Xiaowei Li, Ying Wang</dc:creator>
    </item>
    <item>
      <title>Execution-Centric Characterization of FP8 Matrix Cores, Asynchronous Execution, and Structured Sparsity on AMD MI300A</title>
      <link>https://arxiv.org/abs/2602.10262</link>
      <description>arXiv:2602.10262v1 Announce Type: cross 
Abstract: The AMD MI300A APU integrates CDNA3 GPUs with high-bandwidth memory and advanced accelerator features: FP8 matrix cores, asynchronous compute engines (ACE), and 2:4 structured sparsity. These capabilities are increasingly relied upon by modern HPC and HPC-AI workloads, yet their execution characteristics and system-level implications remain insufficiently understood. In this paper, we present an execution-centric characterization of FP8 matrix execution, ACE concurrency, and structured sparsity on MI300A using targeted microbenchmarks. We quantify occupancy thresholds, fairness, throughput trade-offs under concurrent execution, and context-dependent sparsity benefits. We evaluate representative case studies - transformer-style, concurrent, and mixed-precision kernels - to show how these effects translate into application-level performance and predictability. Our results provide practical guidance for occupancy-aware scheduling, concurrency decisions, and sparsity enablement on MI300A-class unified nodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10262v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aaron Jarmusch, Connor Vitz, Sunita Chandrasekaran</dc:creator>
    </item>
    <item>
      <title>Scaling Routers with In-Package Optics and High-Bandwidth Memories</title>
      <link>https://arxiv.org/abs/2602.10505</link>
      <description>arXiv:2602.10505v1 Announce Type: cross 
Abstract: This paper aims to apply two major scaling transformations from the computing packaging industry to internet routers: the heterogeneous integration of high-bandwidth memories (HBMs) and chiplets, as well as in-package optics. We propose a novel internet router architecture that employs these technologies to achieve a petabit/sec router within a single integrated package. At the top-level, we introduce a novel split-parallel switch architecture that spatially divides (without processing) the incoming fibers and distributes them across smaller independent switches without intermediate OEO conversions or fine-tuned per-packet load-balancing. This passive spatial division enables scaling at the cost of a coarser traffic load balancing. Yet, through extensive evaluations of backbone network traffic, we demonstrate that differences with fine-tuned approaches are small. In addition, we propose a novel HBM-based shared-memory architecture for the implementation of the smaller independent switches, and we introduce a novel parallel frame interleaving algorithm that packs traffic into frames so that HBM banks are accessed at peak HBM data rates in a cyclical interleaving manner. We further discuss why these new technologies represent a paradigm shift in the design of future internet routers. Finally, we emphasize that power consumption may constitute the primary bottleneck to scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10505v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Keslassy, Ilay Yavlovich, Jose Yallouz, Tzu-Chien Hsueh, Yeshaiahu Fainman, Bill Lin</dc:creator>
    </item>
    <item>
      <title>LOREN: Low Rank-Based Code-Rate Adaptation in Neural Receivers</title>
      <link>https://arxiv.org/abs/2602.10770</link>
      <description>arXiv:2602.10770v1 Announce Type: cross 
Abstract: Neural network based receivers have recently demonstrated superior system-level performance compared to traditional receivers. However, their practicality is limited by high memory and power requirements, as separate weight sets must be stored for each code rate. To address this challenge, we propose LOREN, a Low Rank-Based Code-Rate Adaptation Neural Receiver that achieves adaptability with minimal overhead. LOREN integrates lightweight low rank adaptation adapters (LOREN adapters) into convolutional layers, freezing a shared base network while training only small adapters per code rate. An end-to-end training framework over 3GPP CDL channels ensures robustness across realistic wireless environments. LOREN achieves comparable or superior performance relative to fully retrained base neural receivers. The hardware implementation of LOREN in 22nm technology shows more than 65% savings in silicon area and up to 15% power reduction when supporting three code rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.10770v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>eess.SP</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bram Van Bolderik (Technische Universiteit Eindhoven, The Netherlands), Vlado Menkovski (Technische Universiteit Eindhoven, The Netherlands), Sonia Heemstra de Groot (Eindhoven Technical University, The Netherlands), Manil Dev Gomony (Eindhoven University of Technology, The Netherlands)</dc:creator>
    </item>
    <item>
      <title>Vulnerabilities in Partial TEE-Shielded LLM Inference with Precomputed Noise</title>
      <link>https://arxiv.org/abs/2602.11088</link>
      <description>arXiv:2602.11088v1 Announce Type: cross 
Abstract: The deployment of large language models (LLMs) on third-party devices requires new ways to protect model intellectual property. While Trusted Execution Environments (TEEs) offer a promising solution, their performance limits can lead to a critical compromise: using a precomputed, static secret basis to accelerate cryptographic operations. We demonstrate that this mainstream design pattern introduces a classic cryptographic flaw, the reuse of secret keying material, into the system's protocol. We prove its vulnerability with two distinct attacks: First, our attack on a model confidentiality system achieves a full confidentiality break by recovering its secret permutations and model weights. Second, our integrity attack completely bypasses the integrity checks of systems like Soter and TSQP. We demonstrate the practicality of our attacks against state-of-the-art LLMs, recovering a layer's secrets from a LLaMA-3 8B model in about 6 minutes and showing the attack scales to compromise 405B-parameter LLMs across a variety of configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11088v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhishek Saini, Haolin Jiang, Hang Liu</dc:creator>
    </item>
    <item>
      <title>Reed-Muller Error-Correction Code Encoder for SFQ-to-CMOS Interface Circuits</title>
      <link>https://arxiv.org/abs/2602.11140</link>
      <description>arXiv:2602.11140v1 Announce Type: cross 
Abstract: Data transmission from superconducting digital electronics such as single flux quantum (SFQ) logic to semiconductor (CMOS) circuits is subject to bit errors due to, e.g., flux trapping, process parameter variations (PPV), and fabrication defects. In this paper, a lightweight hardware-efficient error-correction code encoder is designed and analyzed. Particularly, a Reed-Muller code RM(1,3) encoder is implemented with SFQ digital logic. The proposed RM(1,3) encoder converts a 4-bit message into an 8-bit codeword and can detect and correct up to 3- and 1-bit errors, respectively. This encoder circuit is designed using MIT-LL SFQ5ee process and SuperTools/ColdFlux RSFQ cell library. A simulation framework integrating JoSIM simulator and MATLAB script for automated data collection and analysis, is proposed to study the performance of RM(1,3) encoder. The proposed encoder improves the probability of having no bit errors by 6.7% as compared to an encoder-less design under $\pm20\%$ PPV. With $\pm15\%$ and lower PPV, the proposed encoder could correct all errors with at least 99.1% probability. The impact of fabrication defects such as open circuit faults on the encoder circuit is also studied using the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11140v1</guid>
      <category>eess.SP</category>
      <category>cond-mat.supr-con</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yerzhan Mustafa, Berker Pek\"oz, Sel\c{c}uk K\"ose</dc:creator>
    </item>
    <item>
      <title>Device-Level Optimization Techniques for Solid-State Drives: A Survey</title>
      <link>https://arxiv.org/abs/2507.10573</link>
      <description>arXiv:2507.10573v2 Announce Type: replace 
Abstract: Solid-state drives (SSDs) have revolutionized data storage with their high performance, energy efficiency, and reliability. However, as storage demands grow, SSDs face critical challenges in scalability, endurance, latency, and security. This survey provides a comprehensive analysis of SSD architecture, key challenges, and device-level optimization techniques. We first examine the fundamental components of SSDs, including NAND flash memory structures, SSD controller functionalities (e.g., address mapping, garbage collection, wear leveling), and host interface protocols. Next, we discuss major challenges such as reliability degradation, endurance limitations, latency variations, and security threats. We then explore advanced optimization techniques, including error correction mechanisms, flash translation layer (FTL) enhancements, and emerging architectures like zoned namespace (ZNS) SSDs and flexible data placement (FDP). Finally, we highlight open research challenges, such as QLC/PLC NAND scalability, performance-reliability trade-offs, and SSD optimizations for AI/LLM workloads. This survey aims to guide future research in the development of next-generation SSDs that balance performance, endurance, and security in evolving storage ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10573v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyu Ren, Yajuan Du, Jinhua Cui, Yina Lv, Qiao Li, Chun Jason Xue</dc:creator>
    </item>
    <item>
      <title>EmbBERT: Attention Under 2 MB Memory</title>
      <link>https://arxiv.org/abs/2502.10001</link>
      <description>arXiv:2502.10001v2 Announce Type: replace-cross 
Abstract: Transformer architectures based on the attention mechanism have revolutionized natural language processing (NLP), driving major breakthroughs across virtually every NLP task. However, their substantial memory and computational requirements still hinder deployment on ultra-constrained devices such as wearables and Internet-of-Things (IoT) units, where available memory is limited to just a few megabytes. To address this challenge, we introduce EmbBERT, a tiny language model (TLM) architecturally designed for extreme efficiency. The model integrates a compact embedding layer, streamlined feed-forward blocks, and an efficient attention mechanism that together enable optimal performance under strict memory budgets. Through this redesign for the extreme edge, we demonstrate that highly simplified transformer architectures remain remarkably effective under tight resource constraints. EmbBERT requires only 2 MB of total memory, and achieves accuracy performance comparable to the ones of state-of-the-art (SotA) models that require a $\mathbf{10\times}$ memory budget. Extensive experiments on the curated TinyNLP benchmark and the GLUE suite confirm that EmbBERT achieves competitive accuracy, comparable to that of larger SotA models, and consistently outperforms downsized versions of BERT and MAMBA of similar size. Furthermore, we demonstrate the model resilience to 8-bit quantization, which further reduces memory usage to just 781 kB , and the scalability of the EmbBERT architecture across the sub-megabyte to tens-of-megabytes range. Finally, we perform an ablation study demonstrating the positive contributions of all components and the pre-training procedure. All code, scripts, and checkpoints are publicly released to ensure reproducibility: https://github.com/RiccardoBravin/tiny-LLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10001v2</guid>
      <category>cs.CL</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riccardo Bravin, Massimo Pavan, Hazem Hesham Yousef Shalby, Fabrizio Pittorino, Manuel Roveri</dc:creator>
    </item>
    <item>
      <title>Optimizing Task Scheduling in Fog Computing with Deadline Awareness</title>
      <link>https://arxiv.org/abs/2509.07378</link>
      <description>arXiv:2509.07378v4 Announce Type: replace-cross 
Abstract: The rise of Internet of Things (IoT) devices has led to the development of numerous time-sensitive applications that require quick responses and low latency. Fog computing has emerged as a solution for processing these IoT applications, but it faces challenges such as resource allocation and job scheduling. Therefore, it is crucial to determine how to assign and schedule tasks on Fog nodes. This work aims to schedule tasks in IoT while minimizing the total energy consumption of nodes and enhancing the Quality of Service (QoS) requirements of IoT tasks, taking into account task deadlines. This paper classifies Fog nodes into two categories based on their traffic level: low and high. It schedules short-deadline tasks on low-traffic nodes using an Improved Golden Eagle Optimization (IGEO) algorithm, an enhancement that utilizes genetic operators for discretization. Long-deadline tasks are processed on high-traffic nodes using reinforcement learning (RL). This combined approach is called the Reinforcement Improved Golden Eagle Optimization (RIGEO) algorithm. Experimental results demonstrate that RIGEO achieves up to a 29% reduction in energy consumption, up to an 86% improvement in response time, and up to a 19% reduction in deadline violations compared to state-of-the-art algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07378v4</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Sadegh Sirjani, Mohammad Ahmad, Amir Mousavi, Erfan Nourbakhsh</dc:creator>
    </item>
    <item>
      <title>ReLANCE: A Resource-Efficient Low-Latency Cortical Neural Acceleration Engine</title>
      <link>https://arxiv.org/abs/2510.17392</link>
      <description>arXiv:2510.17392v2 Announce Type: replace-cross 
Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed, resource-efficient CORDIC based Hodgkin-Huxley (RCHH) neuron model. Unlike shared CORDIC-based DNN approaches, the proposed neuron leverages modular and performance-optimised CORDIC stages with a latency-area trade-off. We introduce a novel Constraint-Aware Modular Parallelism (CAMP) with Precision &amp; Stability handling to leverage maximum speedup and utilisation of hardware through hardware software co-design. The FPGA implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved speed, compared to SoTA designs, with 70% better normalised root mean square error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69 GOPS) than a functionally equivalent CORDIC-based DNN engine, with only a 0.35% accuracy drop relative to the DNN counterpart on the MNIST dataset. The overall results indicate that the design shows biologically accurate, low-resource spiking neural network implementations for resource-constrained edge AI applications. The reproducibility codes are publicly available at https://github.com/mukullokhande99/CNP RCHH, facilitating rapid integration and further development by researchers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17392v2</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <pubDate>Thu, 12 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sonu Kumar, Arjun S. Nair, Bhawna Chaudhary, Mukul Lokhande, Santosh Kumar Vishvakarma</dc:creator>
    </item>
  </channel>
</rss>
