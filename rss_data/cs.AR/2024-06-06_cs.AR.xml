<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 01:48:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>HASS: Hardware-Aware Sparsity Search for Dataflow DNN Accelerator</title>
      <link>https://arxiv.org/abs/2406.03088</link>
      <description>arXiv:2406.03088v1 Announce Type: new 
Abstract: Deep Neural Networks (DNNs) excel in learning hierarchical representations from raw data, such as images, audio, and text. To compute these DNN models with high performance and energy efficiency, these models are usually deployed onto customized hardware accelerators. Among various accelerator designs, dataflow architecture has shown promising performance due to its layer-pipelined structure and its scalability in data parallelism.
  Exploiting weights and activations sparsity can further enhance memory storage and computation efficiency. However, existing approaches focus on exploiting sparsity in non-dataflow accelerators, which cannot be applied onto dataflow accelerators because of the large hardware design space introduced. As such, this could miss opportunities to find an optimal combination of sparsity features and hardware designs.
  In this paper, we propose a novel approach to exploit unstructured weights and activations sparsity for dataflow accelerators, using software and hardware co-optimization. We propose a Hardware-Aware Sparsity Search (HASS) to systematically determine an efficient sparsity solution for dataflow accelerators. Over a set of models, we achieve an efficiency improvement ranging from 1.3$\times$ to 4.2$\times$ compared to existing sparse designs, which are either non-dataflow or non-hardware-aware. Particularly, the throughput of MobileNetV3 can be optimized to 4895 images per second. HASS is open-source: \url{https://github.com/Yu-Zhewen/HASS}</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03088v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhewen Yu, Sudarshan Sreeram, Krish Agrawal, Junyi Wu, Alexander Montgomerie-Corcoran, Cheng Zhang, Jianyi Cheng, Christos-Savvas Bouganis, Yiren Zhao</dc:creator>
    </item>
    <item>
      <title>Soft GPGPU versus IP cores: Quantifying and Reducing the Performance Gap</title>
      <link>https://arxiv.org/abs/2406.03227</link>
      <description>arXiv:2406.03227v1 Announce Type: new 
Abstract: eGPU, a recently-reported soft GPGPU for FPGAs, has demonstrated very high clock frequencies (more than 750 MHz) and small footprint. This means that for the first time, commercial soft processors may be competitive for the kind of heavy numerical computations common in FPGA-based digital signal processing. In this paper we take a deep dive into the performance of the eGPU family on FFT computation, in order to quantify the performance gap between state-of-the-art soft processors and commercial IP cores specialized for this task. In the process, we propose two novel architectural features for the eGPU that improve the efficiency of the design by 50\% when executing the FFTs. The end-result is that our modified GPGPU takes only 3 times the performance-area product of a specialized IP core, yet as a programmable processor is able to execute arbitrary software-defined algorithms. Further comparison to Nvidia A100 GPGPUs demonstrates the superior efficiency of eGPU on FFTs of the size studied (256 to 4096-point).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03227v1</guid>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Langhammer (Intel Corporation, Imperial College London), George A. Constantinides (Imperial College London)</dc:creator>
    </item>
    <item>
      <title>Llumnix: Dynamic Scheduling for Large Language Model Serving</title>
      <link>https://arxiv.org/abs/2406.03243</link>
      <description>arXiv:2406.03243v1 Announce Type: new 
Abstract: Inference serving for large language models (LLMs) is the key to unleashing their potential in people's daily lives. However, efficient LLM serving remains challenging today because the requests are inherently heterogeneous and unpredictable in terms of resource and latency requirements, as a result of the diverse applications and the dynamic execution nature of LLMs. Existing systems are fundamentally limited in handling these characteristics and cause problems such as severe queuing delays, poor tail latencies, and SLO violations.
  We introduce Llumnix, an LLM serving system that reacts to such heterogeneous and unpredictable requests by runtime rescheduling across multiple model instances. Similar to context switching across CPU cores in modern operating systems, Llumnix reschedules requests to improve load balancing and isolation, mitigate resource fragmentation, and differentiate request priorities and SLOs. Llumnix implements the rescheduling with an efficient and scalable live migration mechanism for requests and their in-memory states, and exploits it in a dynamic scheduling policy that unifies the multiple rescheduling scenarios elegantly. Our evaluations show that Llumnix improves tail latencies by an order of magnitude, accelerates high-priority requests by up to 1.5x, and delivers up to 36% cost savings while achieving similar tail latencies, compared against state-of-the-art LLM serving systems. Llumnix is publicly available at https://github.com/AlibabaPAI/llumnix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03243v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Biao Sun, Ziming Huang, Hanyu Zhao, Wencong Xiao, Xinyi Zhang, Yong Li, Wei Lin</dc:creator>
    </item>
    <item>
      <title>An Open-Source Framework for Efficient Numerically-Tailored Computations</title>
      <link>https://arxiv.org/abs/2406.02579</link>
      <description>arXiv:2406.02579v1 Announce Type: cross 
Abstract: We present a versatile open-source framework designed to facilitate efficient, numerically-tailored Matrix-Matrix Multiplications (MMMs). The framework offers two primary contributions: first, a fine-tuned, automated pipeline for arithmetic datapath generation, enabling highly customizable systolic MMM kernels; second, seamless integration of the generated kernels into user code, irrespective of the programming language employed, without necessitating modifications.
  The framework demonstrates a systematic enhancement in accuracy per energy cost across diverse High Performance Computing (HPC) workloads displaying a variety of numerical requirements, such as Artificial Intelligence (AI) inference and Sea Surface Height (SSH) computation. For AI inference, we consider a set of state-of-the-art neural network models, namely ResNet18, ResNet34, ResNet50, DenseNet121, DenseNet161, DenseNet169, and VGG11, in conjunction with two datasets, two computer formats, and 27 distinct intermediate arithmetic datapaths. Our approach consistently reduces energy consumption across all cases, with a notable example being the reduction by factors of $3.3\times$ for IEEE754-32 and $1.4\times$ for Bfloat16 during ImageNet inference with ResNet50. This is accomplished while maintaining accuracies of $82.3\%$ and $86\%$, comparable to those achieved with conventional Floating-Point Units (FPUs). In the context of SSH computation, our method achieves fully-reproducible results using double-precision words, surpassing the accuracy of conventional double- and quad-precision arithmetic in FPUs. Our approach enhances SSH computation accuracy by a minimum of $5\times$ and $27\times$ compared to IEEE754-64 and IEEE754-128, respectively, resulting in $5.6\times$ and $15.1\times$ improvements in accuracy per power cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02579v1</guid>
      <category>cs.MS</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/FPL60245.2023.00011</arxiv:DOI>
      <arxiv:journal_reference>International Conference on Field Programmable Logic and Applications 2023</arxiv:journal_reference>
      <dc:creator>Louis Ledoux, Marc Casas</dc:creator>
    </item>
    <item>
      <title>Floorplanning with I/O assignment via feasibility-seeking and superiorization methods</title>
      <link>https://arxiv.org/abs/2406.03165</link>
      <description>arXiv:2406.03165v1 Announce Type: cross 
Abstract: The feasibility-seeking approach offers a systematic framework for managing and resolving intricate constraints in continuous problems, making it a promising avenue to explore in the context of floorplanning problems with increasingly heterogeneous constraints. The classic legality constraints can be expressed as the union of convex sets. In implementation, we introduce a resetting strategy aimed at effectively reducing the problem of algorithmic divergence in the projection-based method used for the feasibility-seeking formulation. Furthermore, we introduce the novel application of the superiorization method (SM) to floorplanning, which bridges the gap between feasibility-seeking and constrained optimization. The SM employs perturbations to steer the iterations of the feasibility-seeking algorithm towards feasible solutions with reduced (not necessarily minimal) total wirelength. To evaluate the performance of Per-RMAP, we conduct comprehensive experiments on the MCNC benchmarks and GSRC benchmarks. The results demonstrate that we can obtain legal floorplanning results 166 times faster than the branch-and-bound (B&amp;B) method while incurring only a 5% wirelength increase compared to the optimal results. Furthermore, we evaluate the effectiveness of the algorithmic flow that considers the I/O assignment constraints, which achieves an 6% improvement in wirelength. Besides, considering the soft modules with a larger feasible solution space, we obtain 15% improved runtime compared with PeF, the state-of-the-art analytical method. Moreover, we compared our method with Parquet-4 and Fast-SA on GSRC benchmarks which include larger-scale instances. The results highlight the ability of our approach to maintain a balance between floorplanning quality and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03165v1</guid>
      <category>math.OC</category>
      <category>cs.AR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TCAD.2024.3408106.</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems (2024)</arxiv:journal_reference>
      <dc:creator>Shan Yu, Yair Censor, Guojie Luo</dc:creator>
    </item>
    <item>
      <title>Improving Memory Dependence Prediction with Static Analysis</title>
      <link>https://arxiv.org/abs/2403.08056</link>
      <description>arXiv:2403.08056v3 Announce Type: replace-cross 
Abstract: This paper explores the potential of communicating information gained by static analysis from compilers to Out-of-Order (OoO) machines, focusing on the memory dependence predictor (MDP). The MDP enables loads to issue without all in-flight store addresses being known, with minimal memory order violations. We use LLVM to find loads with no dependencies and label them via their opcode. These labelled loads skip making lookups into the MDP, improving prediction accuracy by reducing false dependencies. We communicate this information in a minimally intrusive way, i.e.~without introducing additional hardware costs or instruction bandwidth, providing these improvements without any additional overhead in the CPU. We find that in select cases in Spec2017, a significant number of load instructions can skip interacting with the MDP and lead to a performance gain. These results point to greater possibilities for static analysis as a source of near zero cost performance gains in future CPU designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08056v3</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Panayi, Rohan Gandhi, Jim Whittaker, Vassilios Chouliaras, Martin Berger, Paul Kelly</dc:creator>
    </item>
  </channel>
</rss>
