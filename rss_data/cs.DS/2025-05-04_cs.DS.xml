<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 May 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Faster All-Pairs Optimal Electric Car Routing</title>
      <link>https://arxiv.org/abs/2505.00728</link>
      <description>arXiv:2505.00728v1 Announce Type: new 
Abstract: We present a randomized $\tilde{O}(n^{3.5})$-time algorithm for computing \emph{optimal energetic paths} for an electric car between all pairs of vertices in an $n$-vertex directed graph with positive and negative \emph{costs}. The optimal energetic paths are finite and well-defined even if the graph contains negative-cost cycles. This makes the problem much more challenging than standard shortest paths problems.
  More specifically, for every two vertices $s$ and~$t$ in the graph, the algorithm computes $\alpha_B(s,t)$, the maximum amount of charge the car can reach~$t$ with, if it starts at~$s$ with full battery, i.e., with charge~$B$, where~$B$ is the capacity of the battery. In the presence of negative-cost cycles, optimal paths are not necessarily simple. For dense graphs, our new $\tilde{O}(n^{3.5})$ time algorithm improves on a previous $\tilde{O}(mn^{2})$-time algorithm of Dorfman et al. [ESA 2023] for the problem.
  The \emph{cost} of an arc is the amount of charge taken from the battery of the car when traversing the arc. The charge in the battery can never exceed the capacity~$B$ of the battery and can never be negative. An arc of negative cost may correspond, for example, to a downhill road segment, while an arc with a positive cost may correspond to an uphill segment. A negative-cost cycle, if one exists, can be used in certain cases to charge the battery to its capacity. This makes the problem more interesting and more challenging. Negative-cost cycles may arise when certain road segments have magnetic charging strips, or when the electric car has solar panels.
  Combined with a result of Dorfman et al. [SOSA 2024], this also provides a randomized $\tilde{O}(n^{3.5})$-time algorithm for computing \emph{minimum-cost paths} between all pairs of vertices in an $n$-vertex graph when the battery can be externally recharged, at varying costs, at intermediate vertices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00728v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dani Dorfman, Haim Kaplan, Robert E. Tarjan, Mikkel Thorup, Uri Zwick</dc:creator>
    </item>
    <item>
      <title>Lower Bounds for Non-adaptive Local Computation Algorithms</title>
      <link>https://arxiv.org/abs/2505.00915</link>
      <description>arXiv:2505.00915v1 Announce Type: new 
Abstract: We study *non-adaptive* Local Computation Algorithms (LCA). A reduction of Parnas and Ron (TCS'07) turns any distributed algorithm into a non-adaptive LCA. Plugging known distributed algorithms, this leads to non-adaptive LCAs for constant approximations of maximum matching (MM) and minimum vertex cover (MVC) with complexity $\Delta^{O(\log \Delta / \log \log \Delta)}$, where $\Delta$ is the maximum degree of the graph. Allowing adaptivity, this bound can be significantly improved to $\text{poly}(\Delta)$, but is such a gap necessary or are there better non-adaptive LCAs?
  Adaptivity as a resource has been studied extensively across various areas. Beyond this, we further motivate the study of non-adaptive LCAs by showing that even a modest improvement over the Parnas-Ron bound for the MVC problem would have major implications in the Massively Parallel Computation (MPC) setting; It would lead to faster truly sublinear space MPC algorithms for approximate MM, a major open problem of the area. Our main result is a lower bound that rules out this avenue for progress.
  We prove that $\Delta^{\Omega(\log \Delta / \log \log \Delta)}$ queries are needed for any non-adaptive LCA computing a constant approximation of MM or MVC. This is the first separation between non-adaptive and adaptive LCAs, and already matches (up to constants in the exponent) the algorithm obtained by the black-box reduction of Parnas and Ron.
  Our proof blends techniques from two separate lines of work: sublinear time lower bounds and distributed lower bounds. Particularly, we adopt techniques such as couplings over acyclic subgraphs from the recent sublinear time lower bounds of Behnezhad, Roghani, and Rubinstein (STOC'23, FOCS'23, STOC'24). We apply these techniques to a very different instance, (a modified version of) the construction of Kuhn, Moscibroda and Wattenhoffer (JACM'16) from distributed computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00915v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amir Azarmehr, Soheil Behnezhad, Alma Ghafari, Madhu Sudan</dc:creator>
    </item>
    <item>
      <title>Cluster deletion and clique partitioning in graphs with bounded clique number</title>
      <link>https://arxiv.org/abs/2505.00922</link>
      <description>arXiv:2505.00922v1 Announce Type: new 
Abstract: The Cluster Deletion problem takes a graph $G$ as input and asks for a minimum size set of edges $X$ such that $G-X$ is the disjoint union of complete graphs. An equivalent formulation is the Clique Partition problem, which asks to find a partition of $V(G)$ into cliques such that the total number of edges is maximized.
  We begin by giving a much simpler proof of a theorem of Gao, Hare, and Nastos that Cluster Deletion is efficiently solvable on the class of cographs. We then investigate Cluster Deletion and Clique Partition on permutation graphs, which are a superclass of cographs. Our findings suggest that Cluster Deletion may be NP-hard on permutation graphs.
  Finally, we prove that for graphs with clique number at most $c$, there is a $\frac{2\binom{c}{2}}{\binom{c}{2}+1}$-approximation algorithm for Clique Partition. This is the first polynomial time algorithm which achieves an approximation ratio better than 2 for graphs with bounded clique number. More generally, our algorithm runs in polynomial time on any graph class for which Maximum Clique can be computed in polynomial time. We also provide a class of examples which shows that our approximation ratio is best possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00922v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicola Galesi, Tony Huynh, Fariba Ranjbar</dc:creator>
    </item>
    <item>
      <title>LZD-style Compression Scheme with Truncation and Repetitions</title>
      <link>https://arxiv.org/abs/2505.00970</link>
      <description>arXiv:2505.00970v1 Announce Type: new 
Abstract: Lempel-Ziv-Double (LZD) is a variation of the LZ78 compression scheme that achieves better compression on repetitive datasets. Nevertheless, prior research has identified computational inefficiencies and a weakness in its compressibility for certain datasets. In this paper, we introduce LZD+, an enhancement of LZD, which enables expected linear-time online compression by allowing truncated references. To avoid the compressibility weakness exhibited by a lower bound example, we propose LZDR (LZD-runlength compressed), a further enhancement on top of LZD+, which introduces a repetition-based factorization rule while maintaining linear expected time complexity. The both time bounds can be de-randomized by a lookup data structure like a balanced search tree with a logarithmic dependency on the alphabet size. Additionally, we present three flexible parsing variants of LZDR that yield fewer factors in practice. Comprehensive benchmarking on standard corpora reveals that LZD+, LZDR, and its flexible variants outperform existing LZ-based methods in the number of factors while keeping competitive runtime efficiency. However, we note that the difference in the number of factors becomes marginal for large datasets like those of the Pizza&amp;Chili corpus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00970v1</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linus G\"otz, Dominik K\"oppl</dc:creator>
    </item>
    <item>
      <title>Shuffling Cards When You Are of Very Little Brain: Low Memory Generation of Permutations</title>
      <link>https://arxiv.org/abs/2505.01287</link>
      <description>arXiv:2505.01287v1 Announce Type: new 
Abstract: How can we generate a permutation of the numbers $1$ through $n$ so that it is hard to guess the next element given the history so far? The twist is that the generator of the permutation (the ``Dealer") has limited memory, while the ``Guesser" has unlimited memory. With unbounded memory (actually $n$ bits suffice), the Dealer can generate a truly random permutation where~$\ln n$ is the expected number of correct guesses.
  Our main results are tight bounds for the relationship between the guessing probability and the memory required to generate the permutation. We suggest a method for the Dealer that requires~$m$ bits of storage, constant time for each turn and makes any Guesser pick correctly only $O(n/m+\log m)$ cards in expectation. The method does not require any secrecy from the dealer, i.e. it is ``open book" or ``whitebox". On the other hand, we show that this bound is the best possible, even for Dealers with secret memory: For any $m$-bit Dealer there is a (computationally powerful) guesser that makes $\Omega(n/m+\log m)$ correct guesses in expectation.
  We also give an $O(n)$ bit memory Dealer that generates perfectly random permutations and operates in constant time per turn.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01287v1</guid>
      <category>cs.DS</category>
      <category>cs.CR</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boaz Menuhin, Moni Naor</dc:creator>
    </item>
    <item>
      <title>Maximum list $r$-colorable induced subgraphs in $kP_3$-free graphs</title>
      <link>https://arxiv.org/abs/2505.00412</link>
      <description>arXiv:2505.00412v1 Announce Type: cross 
Abstract: We show that, for every fixed positive integers $r$ and $k$, \textsc{Max-Weight List $r$-Colorable Induced Subgraph} admits a polynomial-time algorithm on $kP_3$-free graphs. This problem is a common generalization of \textsc{Max-Weight Independent Set}, \textsc{Odd Cycle Transversal} and \textsc{List $r$-Coloring}, among others. Our result has several consequences.
  First, it implies that, for every fixed $r \geq 5$, assuming $\mathsf{P}\neq \mathsf{NP}$, \textsc{Max-Weight List $r$-Colorable Induced Subgraph} is polynomial-time solvable on $H$-free graphs if and only if $H$ is an induced subgraph of either $kP_3$ or $P_5+kP_1$, for some $k \geq 1$. Second, it makes considerable progress toward a complexity dichotomy for \textsc{Odd Cycle Transversal} on $H$-free graphs, allowing to answer a question of Agrawal, Lima, Lokshtanov, Rz{\k{a}}{\.z}ewski, Saurabh, and Sharma [TALG 2024]. Third, it gives a short and self-contained proof of the known result of Chudnovsky, Hajebi, and Spirkl [Combinatorica 2024] that \textsc{List $r$-Coloring} on $kP_3$-free graphs is polynomial-time solvable for every fixed $r$ and $k$.
  We also consider two natural distance-$d$ generalizations of \textsc{Max-Weight Independent Set} and \textsc{List $r$-Coloring} and provide polynomial-time algorithms on $kP_3$-free graphs for every fixed integers $r$, $k$, and $d \geq 6$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00412v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esther Galby, Paloma T. Lima, Andrea Munaro, Amir Nikabadi</dc:creator>
    </item>
    <item>
      <title>The tape reconfiguration problem and its consequences for dominating set reconfiguration</title>
      <link>https://arxiv.org/abs/2505.00988</link>
      <description>arXiv:2505.00988v1 Announce Type: cross 
Abstract: A dominating set of a graph $G=(V,E)$ is a set of vertices $D \subseteq V$ whose closed neighborhood is $V$, i.e., $N[D]=V$. We view a dominating set as a collection of tokens placed on the vertices of $D$. In the token sliding variant of the Dominating Set Reconfiguration problem (TS-DSR), we seek to transform a source dominating set into a target dominating set in $G$ by sliding tokens along edges, and while maintaining a dominating set all along the transformation.
  TS-DSR is known to be PSPACE-complete even restricted to graphs of pathwidth $w$, for some non-explicit constant $w$ and to be XL-complete parameterized by the size $k$ of the solution. The first contribution of this article consists in using a novel approach to provide the first explicit constant for which the TS-DSR problem is PSPACE-complete, a question that was left open in the literature.
  From a parameterized complexity perspective, the token jumping variant of DSR, i.e., where tokens can jump to arbitrary vertices, is known to be FPT when parameterized by the size of the dominating sets on nowhere dense classes of graphs. But, in contrast, no non-trivial result was known about TS-DSR. We prove that DSR is actually much harder in the sliding model since it is XL-complete when restricted to bounded pathwidth graphs and even when parameterized by $k$ plus the feedback vertex set number of the graph. This gives, for the first time, a difference of behavior between the complexity under token sliding and token jumping for some problem on graphs of bounded treewidth. All our results are obtained using a brand new method, based on the hardness of the so-called Tape Reconfiguration problem, a problem we believe to be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00988v1</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Bousquet, Quentin Deschamps, Arnaud Mary, Amer E. Mouawad, Th\'eo Pierron</dc:creator>
    </item>
    <item>
      <title>ConflictSync: Bandwidth Efficient Synchronization of Divergent State</title>
      <link>https://arxiv.org/abs/2505.01144</link>
      <description>arXiv:2505.01144v1 Announce Type: cross 
Abstract: State-based Conflict-free Replicated Data Types (CRDTs) are widely used in distributed systems to ensure high availability without coordination. However, their naive synchronization strategy - transmitting the full state - incurs high communication costs. Existing optimizations like delta-CRDTs reduce this overhead but rely on external metadata that must be garbage collected to prevent unbounded growth, at the cost of full state transmissions after network partitions.
  This paper presents ConflictSync, the first digest-driven synchronization algorithm for state-based CRDTs. We reduce synchronization to the set reconciliation of irredundant join decompositions and build on existing work in rateless set reconciliation. To support CRDTs, we generalize set reconciliation to variable-sized elements, and further introduce a novel combination of Bloom filters with Rateless Invertible Bloom Lookup Tables to address inefficiencies at low similarity levels.
  Our evaluation shows that ConflictSync reduces total data transfer by up to 18 times compared to traditional state-based synchronization. Bloom filter prefiltering reduces overhead by up to 50% compared to pure rateless reconciliation at 0% similarity, while pure rateless reconciliation performs better above 93% similarity. We characterize the trade-off between similarity level and Bloom filter size, identifying optimal configurations for different synchronization scenarios.
  Although developed for CRDTs, ConflictSync applies to any synchronization problem where states can be decomposed into sets of constituent components, analogous to join decompositions, making it suitable for a wide range of distributed data models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01144v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pedro Silva Gomes, Miguel Boaventura Rodrigues, Carlos Baquero</dc:creator>
    </item>
    <item>
      <title>Negative Stepsizes Make Gradient-Descent-Ascent Converge</title>
      <link>https://arxiv.org/abs/2505.01423</link>
      <description>arXiv:2505.01423v1 Announce Type: cross 
Abstract: Efficient computation of min-max problems is a central question in optimization, learning, games, and controls. Arguably the most natural algorithm is gradient-descent-ascent (GDA). However, since the 1970s, conventional wisdom has argued that GDA fails to converge even on simple problems. This failure spurred an extensive literature on modifying GDA with additional building blocks such as extragradients, optimism, momentum, anchoring, etc. In contrast, we show that GDA converges in its original form by simply using a judicious choice of stepsizes.
  The key innovation is the proposal of unconventional stepsize schedules (dubbed slingshot stepsize schedules) that are time-varying, asymmetric, and periodically negative. We show that all three properties are necessary for convergence, and that altogether this enables GDA to converge on the classical counterexamples (e.g., unconstrained convex-concave problems). All of our results apply to the last iterate of GDA, as is typically desired in practice.
  The core algorithmic intuition is that although negative stepsizes make backward progress, they de-synchronize the min and max variables (overcoming the cycling issue of GDA), and lead to a slingshot phenomenon in which the forward progress in the other iterations is overwhelmingly larger. This results in fast overall convergence. Geometrically, the slingshot dynamics leverage the non-reversibility of gradient flow: positive/negative steps cancel to first order, yielding a second-order net movement in a new direction that leads to convergence and is otherwise impossible for GDA to move in. We interpret this as a second-order finite-differencing algorithm and show that, intriguingly, it approximately implements consensus optimization, an empirically popular algorithm for min-max problems involving deep neural networks (e.g., training GANs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01423v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Henry Shugart, Jason M. Altschuler</dc:creator>
    </item>
    <item>
      <title>Differentially Private High-Dimensional Approximate Range Counting, Revisited</title>
      <link>https://arxiv.org/abs/2409.07187</link>
      <description>arXiv:2409.07187v2 Announce Type: replace 
Abstract: Locality Sensitive Filters are known for offering a quasi-linear space data structure with rigorous guarantees for the Approximate Near Neighbor search (ANN) problem. Building on Locality Sensitive Filters, we derive a simple data structure for the Approximate Near Neighbor Counting (ANNC) problem under differential privacy (DP). Moreover, we provide a simple analysis leveraging a connection with concomitant statistics and extreme value theory. Our approach produces a simple data structure with a tunable parameter that regulates a trade-off between space-time and utility. Through this trade-off, our data structure achieves the same performance as the recent findings of Andoni et al. (NeurIPS 2023) while offering better utility at the cost of higher space and query time. In addition, we provide a more efficient algorithm under pure $\varepsilon$-DP and elucidate the connection between ANN and differentially private ANNC. As a side result, the paper provides a more compact description and analysis of Locality Sensitive Filters for Fair Near Neighbor Search, improving a previous result in Aum\"{u}ller et al. (TODS 2022).</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07187v2</guid>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Aum\"uller, Fabrizio Boninsegna, Francesco Silvestri</dc:creator>
    </item>
    <item>
      <title>The Complexity of Minimum-Envy House Allocation Over Graphs</title>
      <link>https://arxiv.org/abs/2505.00296</link>
      <description>arXiv:2505.00296v2 Announce Type: replace 
Abstract: In this paper, we study a generalization of the House Allocation problem. In our problem, agents are represented by vertices of a graph $\GG_{\mathcal{A}} = (\AA, E_\AA)$, and each agent $a \in \AA$ is associated with a set of preferred houses $\PP_a \subseteq \HH$, where $\AA$ is the set of agents and $\HH$ is the set of houses. A house allocation is an injective function $\phi: \AA \rightarrow \HH$, and an agent $a$ envies a neighbour $a' \in N_{\GG_\AA}(a)$ under $\phi$ if $\phi(a) \notin \PP_a$ and $\phi(a') \in \PP_a$. We study two natural objectives: the first problem called \ohaa, aims to compute an allocation that minimizes the number of envious agents; the second problem called \ohaah aims to maximize, among all minimum-envy allocations, the number of agents who are assigned a house they prefer. These two objectives capture complementary notions of fairness and individual satisfaction.
  We design polynomial time algorithms for both problems for the variant when each agent prefers exactly one house. On the other hand, when the list of preferred houses for each agent has size at most $2$ then we show that both problems are \NP-hard even when the agent graph $\GG_\AA$ is a complete bipartite graph. We also show that both problems are \NP-hard even when the number $|\mathcal H|$ of houses is equal to the number $|\mathcal A|$ of agents. This is in contrast to the classical {\sc House Allocation} problem, where the problem is polynomial time solvable when $|\mathcal H| = |\mathcal A|$. The two problems are also \NP-hard when the agent graph has a small vertex cover. On the positive side, we design exact algorithms that exploit certain structural properties of $\GG_{\AA}$ such as sparsity, existence of balanced separators or existence of small-sized vertex covers, and perform better than the naive brute-force algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00296v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Palash Dey, Anubhav Dhar, Ashlesha Hota, Sudeshna Kolay</dc:creator>
    </item>
    <item>
      <title>Exponential Quantum Advantage for Pathfinding in Regular Sunflower Graphs</title>
      <link>https://arxiv.org/abs/2407.14398</link>
      <description>arXiv:2407.14398v2 Announce Type: replace-cross 
Abstract: Finding problems that allow for superpolynomial quantum speedup is one of the most important tasks in quantum computation. A key challenge is identifying problem structures that can only be exploited by quantum mechanics. In this paper, we find a class of graphs that allows for exponential quantum-classical separation for the pathfinding problem with the adjacency list oracle, and this class of graphs is named regular sunflower graphs. We prove that, with high probability, a regular sunflower graph of degree at least $7$ is a mild expander graph, that is, the spectral gap of the graph Laplacian is at least inverse polylogarithmic in the graph size.
  We provide an efficient quantum algorithm to find an $s$-$t$ path in the regular sunflower graph while any classical algorithm takes exponential time. This quantum advantage is achieved by efficiently preparing a $0$-eigenstate of the adjacency matrix of the regular sunflower graph as a quantum superposition state over the vertices, and this quantum state contains enough information to help us efficiently find an $s$-$t$ path in the regular sunflower graph.
  Because the security of an isogeny-based cryptosystem depends on the hardness of finding an $s$-$t$ path in an expander graph \cite{Charles2009}, a quantum speedup of the pathfinding problem on an expander graph is of significance. Our result represents a step towards this goal as the first provable exponential speedup for pathfinding in a mild expander graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14398v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianqiang Li, Yu Tong</dc:creator>
    </item>
    <item>
      <title>An Efficient Matrix Multiplication Algorithm for Accelerating Inference in Binary and Ternary Neural Networks</title>
      <link>https://arxiv.org/abs/2411.06360</link>
      <description>arXiv:2411.06360v3 Announce Type: replace-cross 
Abstract: Despite their tremendous success and versatility, Deep Neural Networks (DNNs) such as Large Language Models (LLMs) suffer from inference inefficiency and rely on advanced computational infrastructure. To address these challenges and make these models more accessible and cost-effective, in this paper, we propose algorithms to improve the inference time and memory efficiency of DNNs with binary and ternary weight matrices. Particularly focusing on matrix multiplication as the bottleneck operation of inference, we observe that, once trained, the weight matrices of a model no longer change. This allows us to preprocess these matrices and create indices that help reduce the storage requirements by a logarithmic factor while enabling our efficient inference algorithms. Specifically, for a $n\times n$ weight matrix, our efficient algorithm guarantees a time complexity of $O(\frac{n^2}{\log n})$, a logarithmic factor improvement over the standard vector-matrix multiplication. Besides theoretical analysis, we conduct extensive experiments to evaluate the practical efficiency of our algorithms. Our results confirm the superiority of our approach both with respect to time and memory, as we observed a reduction in the multiplication time up to 29x and memory usage up to 6x. When applied to LLMs, our experiments show up to a 5.24x speedup in the inference time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06360v3</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Dehghankar, Mahdi Erfanian, Abolfazl Asudeh</dc:creator>
    </item>
    <item>
      <title>Provably faster randomized and quantum algorithms for $k$-means clustering via uniform sampling</title>
      <link>https://arxiv.org/abs/2504.20982</link>
      <description>arXiv:2504.20982v2 Announce Type: replace-cross 
Abstract: The $k$-means algorithm (Lloyd's algorithm) is a widely used method for clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that each iteration requires time linear in the number of data points, which can be expensive in big data applications. This was improved in recent works proposing quantum and quantum-inspired classical algorithms to approximate the $k$-means algorithm locally, in time depending only logarithmically on the number of data points (along with data dependent parameters) [$q$-means: A quantum algorithm for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash, NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this work, we describe a simple randomized mini-batch $k$-means algorithm and a quantum algorithm inspired by the classical algorithm. We prove worse-case guarantees that significantly improve upon the bounds for previous algorithms. Our improvements are due to a careful use of uniform sampling, which preserves certain symmetries of the $k$-means problem that are not preserved in previous algorithms that use data norm-based sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20982v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Chen, Archan Ray, Akshay Seshadri, Dylan Herman, Bao Bach, Pranav Deshpande, Abhishek Som, Niraj Kumar, Marco Pistoia</dc:creator>
    </item>
  </channel>
</rss>
