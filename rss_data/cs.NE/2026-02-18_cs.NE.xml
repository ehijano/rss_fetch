<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 19 Feb 2026 02:34:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Quantum-inspired Hybrid Swarm Intelligence and Decision-Making for Multi-Criteria ADAS Calibration</title>
      <link>https://arxiv.org/abs/2602.15043</link>
      <description>arXiv:2602.15043v1 Announce Type: new 
Abstract: The tuning of Advanced Driver Assistance Systems (ADAS) involves resolving trade-offs among several competing objectives, including operational safety, system responsiveness, energy usage, and passenger comfort. This work introduces a novel optimization framework based on Quantum-Inspired Hybrid Swarm Intelligence (QiHSI), in which quantum-inspired mechanisms are embedded within a multi-objective salp swarm optimization process to strengthen global search capability and preserve population diversity in complex, high-dimensional decision spaces. In addition, a decision-maker-in-the-loop strategy is incorporated to incorporate adaptive expert guidance, enabling the optimization process to respond dynamically to changing design priorities and system constraints. The effectiveness of QiHSI is assessed using established multi-objective benchmark problems as well as a practical ADAS calibration scenario. Experimental comparisons with several state-of-the-art evolutionary and swarm-based algorithms, including MSSA, MOPSO, MOEA/D, SPEA2, NSGA-III, and RVEA, show that the proposed method consistently produces well-distributed Pareto-optimal solutions with faster convergence and improved adaptability. These findings demonstrate that QiHSI offers a reliable and scalable approach for intelligent ADAS calibration, supporting the development of more responsive, efficient, and safety-oriented autonomous driving technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15043v1</guid>
      <category>cs.NE</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanjai Pathak, Ashish Mani, Amlan Chatterjee</dc:creator>
    </item>
    <item>
      <title>An effective Genetic Programming Hyper-Heuristic for Uncertain Agile Satellite Scheduling</title>
      <link>https://arxiv.org/abs/2602.15070</link>
      <description>arXiv:2602.15070v1 Announce Type: new 
Abstract: This paper investigates a novel problem, namely the Uncertain Agile Earth Observation Satellite Scheduling Problem (UAEOSSP). Unlike the static AEOSSP, it takes into account a range of uncertain factors (e.g., task profit, resource consumption, and task visibility) in order to reflect the reality that the actual information is inherently unknown beforehand. An effective Genetic Programming Hyper-Heuristic (GPHH) is designed to automate the generation of scheduling policies. The evolved scheduling policies can be utilized to adjust plans in real time and perform exceptionally well. Experimental results demonstrate that evolved scheduling policies significantly outperform both well-designed Look-Ahead Heuristics (LAHs) and Manually Designed Heuristics (MDHs). Specifically, the policies generated by GPHH achieve an average improvement of 5.03% compared to LAHs and 8.14% compared to MDHs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15070v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuning Chen, Junhua Xue, Wangqi Gu, Mingyan Shao</dc:creator>
    </item>
    <item>
      <title>Reverse Delegated Training and Private Inference via Perfectly-Secure Quantum Homomorphic Encryption</title>
      <link>https://arxiv.org/abs/2602.12712</link>
      <description>arXiv:2602.12712v1 Announce Type: cross 
Abstract: Quantum machine learning in cloud environments requires protecting sensitive data while enabling remote computation. Here we demonstrate the first realistic implementations of a perfectly-secure quantum homomorphic encryption (QHE) scheme applied to quantum neural networks (QNN). Using efficient Clifford+$T$ decomposition, we implement quantum convolutional neural networks for two complementary scenarios: (i) reverse delegated training, where encrypted data from multiple providers trains a user's network via federated aggregation; (ii) private inference, where users process encrypted data with remote quantum networks. Moreover, analysis of server circuit privacy reveals probabilistic model protection through Pauli gate concealment. These results establish perfectly-secure QHE as a practical framework for multi-party quantum machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12712v1</guid>
      <category>quant-ph</category>
      <category>cs.NE</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sergio A. Ortega, Miguel A. Martin-Delgado</dc:creator>
    </item>
    <item>
      <title>CDRL: A Reinforcement Learning Framework Inspired by Cerebellar Circuits and Dendritic Computational Strategies</title>
      <link>https://arxiv.org/abs/2602.15367</link>
      <description>arXiv:2602.15367v1 Announce Type: cross 
Abstract: Reinforcement learning (RL) has achieved notable performance in high-dimensional sequential decision-making tasks, yet remains limited by low sample efficiency, sensitivity to noise, and weak generalization under partial observability. Most existing approaches address these issues primarily through optimization strategies, while the role of architectural priors in shaping representation learning and decision dynamics is less explored. Inspired by structural principles of the cerebellum, we propose a biologically grounded RL architecture that incorporate large expansion, sparse connectivity, sparse activation, and dendritic-level modulation. Experiments on noisy, high-dimensional RL benchmarks show that both the cerebellar architecture and dendritic modulation consistently improve sample efficiency, robustness, and generalization compared to conventional designs. Sensitivity analysis of architectural parameters suggests that cerebellum-inspired structures can offer optimized performance for RL with constrained model parameters. Overall, our work underscores the value of cerebellar structural priors as effective inductive biases for RL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.15367v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sibo Zhang, Rui Jing, Liangfu Lv, Jian Zhang, Yunliang Zang</dc:creator>
    </item>
    <item>
      <title>Morephy-Net: An Evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed Neural Operator Learning Networks</title>
      <link>https://arxiv.org/abs/2509.00663</link>
      <description>arXiv:2509.00663v2 Announce Type: replace-cross 
Abstract: We propose an evolutionary Multi-objective Optimization for Replica-Exchange-based Physics-informed operator-learning Networks (Morephy-Net) to solve parametric partial differential equations (PDEs) in noisy data regimes, for both forward prediction and inverse identification. Existing physics-informed neural networks and operator-learning models (e.g., DeepONets and Fourier neural operators) often face three coupled challenges: (i) balancing data/operator and physics residual losses, (ii) maintaining robustness under noisy or sparse observations, and (iii) providing reliable uncertainty quantification. Morephy-Net addresses these issues by integrating: (i) evolutionary multi-objective optimization that treats data/operator and physics residual terms as separate objectives and searches the Pareto front, thereby avoiding ad hoc loss weighting; (ii) replica-exchange stochastic gradient Langevin dynamics to enhance global exploration and stabilize training in non-convex landscapes; and (iii) Bayesian uncertainty quantification obtained from stochastic sampling. We validate Morephy-Net on representative forward and inverse problems, including the one-dimensional Burgers equation and the time-fractional mixed diffusion--wave equation. The results demonstrate consistent improvements in accuracy, noise robustness, and calibrated uncertainty estimates over standard operator-learning baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00663v2</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binghang Lu, Changhong Mou, Guang Lin</dc:creator>
    </item>
    <item>
      <title>Online Fine-Tuning of Pretrained Controllers for Autonomous Driving via Real-Time Recurrent RL</title>
      <link>https://arxiv.org/abs/2602.02236</link>
      <description>arXiv:2602.02236v3 Announce Type: replace-cross 
Abstract: Deploying pretrained policies in real-world applications presents substantial challenges that fundamentally limit the practical applicability of learning-based control systems. When autonomous systems encounter environmental changes in system dynamics, sensor drift, or task objectives, fixed policies rapidly degrade in performance. We show that employing Real-Time Recurrent Reinforcement Learning (RTRRL), a biologically plausible algorithm for online adaptation, can effectively fine-tune a pretrained policy to improve autonomous agents' performance on driving tasks. We further show that RTRRL synergizes with a recent biologically inspired recurrent network model, the Liquid-Resistance Liquid-Capacitance RNN. We demonstrate the effectiveness of this closed-loop approach in a simulated CarRacing environment and in a real-world line-following task with a RoboRacer car equipped with an event camera.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02236v3</guid>
      <category>cs.RO</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 18 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Lemmel, Felix Resch, M\'onika Farsang, Ramin Hasani, Daniela Rus, Radu Grosu</dc:creator>
    </item>
  </channel>
</rss>
