<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Streamlining Compliance And Risk Management with Regtech Solutions</title>
      <link>https://arxiv.org/abs/2501.18910</link>
      <description>arXiv:2501.18910v1 Announce Type: new 
Abstract: RegTech is a rapidly rising financial services sector focused on using cutting-edge technology to improve the process of regulatory compliance. RegTech solutions are characterized by numerous features and benefits that can considerably contribute to helping organizations operate effectively in the increasingly regulated environment, when it comes to compliance and risk management. This paper sheds light on why RegTech will be one of the most promising markets, driven by the rising cost of compliance and the growing reliance on technology in crisis management. Moreover, this paper will examine the advantages of using such solutions to strike a balance between compliance and operational efficiencies. This paper will deepen the understanding of regulatory compliance, introduce RegTech, and examine the benefits of using these solutions to achieve compliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18910v1</guid>
      <category>cs.CY</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5121/ijcsit.2024.16302</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Computer Science &amp; Information Technology (IJCSIT) Vol 16, No 3, June 2024</arxiv:journal_reference>
      <dc:creator>Chintamani Bagwe</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review of Computer Science MOOCs for K-12 education</title>
      <link>https://arxiv.org/abs/2501.18986</link>
      <description>arXiv:2501.18986v1 Announce Type: new 
Abstract: Computer science (CS) is increasingly becoming part of the curricula of K-12 education in different countries. However, there are few K-12 CS teachers, and tools to offer K-12 CS education are often limited. Massive Open Online Courses (MOOCs) might help to temporarily address these challenges, and enable more schools to offer CS education. The goal of this systematic review is to give an overview of how CS MOOCs have been used in K-12 education. Nineteen papers from 2014 to May 2024 were included, describing thirteen different MOOCs. This review summarizes the research performed with these MOOCs and discusses directions for future research. Our findings show that most CS MOOCs target only part of the CS curriculum. When using a MOOC, a classroom teacher has an important role in supporting and managing students as they work in the MOOC. Research evaluating MOOCs is diverse, both in aims and in methods. In conclusion, MOOCs can play a valuable role in K-12 CS education, although additional teacher training to support students might be required. Moreover, additional learning material is needed to cover the full curriculum, as most MOOCs focus on programming and computational thinking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18986v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>L. M. van der Lubbe, S. P van Borkulo, J. T. Jeuring</dc:creator>
    </item>
    <item>
      <title>Position: Contextual Integrity Washing for Language Models</title>
      <link>https://arxiv.org/abs/2501.19173</link>
      <description>arXiv:2501.19173v1 Announce Type: new 
Abstract: Machine learning community is discovering Contextual Integrity (CI) as a useful framework to assess the privacy implications of large language models (LLMs). This is an encouraging development. The CI theory emphasizes sharing information in accordance with privacy norms and can bridge the social, legal, political, and technical aspects essential for evaluating privacy in LLMs. However, this is also a good point to reflect on use of CI for LLMs. This position paper argues that existing literature adopts CI for LLMs without embracing the theory's fundamental tenets, essentially amounting to a form of "CI-washing." CI-washing could lead to incorrect conclusions and flawed privacy-preserving designs. We clarify the four fundamental tenets of CI theory, systematize prior work on whether they deviate from these tenets, and highlight overlooked issues in experimental hygiene for LLMs (e.g., prompt sensitivity, positional bias).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19173v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Shvartzshnaider, Vasisht Duddu</dc:creator>
    </item>
    <item>
      <title>The geography of inequalities in access to healthcare across England: the role of bus travel time variability</title>
      <link>https://arxiv.org/abs/2501.19231</link>
      <description>arXiv:2501.19231v1 Announce Type: new 
Abstract: Fair access to healthcare facilities is fundamental to achieving social equity. Traditional travel time-based accessibility measures often overlook the dynamic nature of travel times resulting from different departure times, which compromises the accuracy of these measures in reflecting the true accessibility experienced by individuals. This study examines public transport-based accessibility to healthcare facilities across England from the perspective of travel time variability (TTV). Using comprehensive bus timetable data from the Bus Open Data Service (BODS), we calculated hourly travel times from each Lower Layer Super Output Area (LSOA) to the nearest hospitals and general practices and developed a TTV metric for each LSOA and analysed its geographical inequalities across various spatial scales. Our analysis reveals notable spatial-temporal patterns of TTV and average travel times, including an urban-rural divide, clustering of high and low TTV regions, and distinct outliers. Furthermore, we explored the relationship between TTV and deprivation, categorising LSOAs into four groups based on their unique characteristics, which provides valuable insights for designing targeted interventions. Our study also highlights the limitations of using theoretical TTV derived from timetable data and emphasises the potential of using real-time operational data to capture more realistic accessibility measures. By offering a more dynamic perspective on accessibility, our findings complement existing travel time-based metrics and pave way for future research on TTV-based accessibility using real-time data. This evidence-based approach can inform efforts to ``level up" public transport services, addressing geographical inequalities and promoting equitable access to essential healthcare services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19231v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Chen, Federico Botta</dc:creator>
    </item>
    <item>
      <title>From Assistance to Autonomy -- A Researcher Study on the Potential of AI Support for Qualitative Data Analysis</title>
      <link>https://arxiv.org/abs/2501.19275</link>
      <description>arXiv:2501.19275v1 Announce Type: new 
Abstract: The advent of AI tools, such as Large Language Models, has introduced new possibilities for Qualitative Data Analysis (QDA), offering both opportunities and challenges. To help navigate the responsible integration of AI into QDA, we conducted semi-structured interviews with 15 HCI researchers experienced in QDA. While our participants were open to AI support in their QDA workflows, they expressed concerns about data privacy, autonomy, and the quality of AI outputs. In response, we developed a framework that spans from minimal to high AI involvement, providing tangible scenarios for integrating AI into HCI researchers' QDA practices while addressing their needs and concerns. Aligned with real-life QDA workflows, we identify potentials for AI tools in areas such as data pre-processing, researcher onboarding, or mediation. Our framework aims to provoke further discussion on the development of AI-supported QDA and to help establish community standards for their responsible use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19275v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisabeth Kirsten, Annalina Buckmann, Leona Lassak, Nele Borgert, Abraham Mhaidli, Steffen Becker</dc:creator>
    </item>
    <item>
      <title>The Value of Prediction in Identifying the Worst-Off</title>
      <link>https://arxiv.org/abs/2501.19334</link>
      <description>arXiv:2501.19334v1 Announce Type: new 
Abstract: Machine learning is increasingly used in government programs to identify and support the most vulnerable individuals, prioritizing assistance for those at greatest risk over optimizing aggregate outcomes. This paper examines the welfare impacts of prediction in equity-driven contexts, and how they compare to other policy levers, such as expanding bureaucratic capacity. Through mathematical models and a real-world case study on long-term unemployment amongst German residents, we develop a comprehensive understanding of the relative effectiveness of prediction in surfacing the worst-off. Our findings provide clear analytical frameworks and practical, data-driven tools that empower policymakers to make principled decisions when designing these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19334v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Unai Fischer-Abaigar, Christoph Kern, Juan Carlos Perdomo</dc:creator>
    </item>
    <item>
      <title>We're Different, We're the Same: Creative Homogeneity Across LLMs</title>
      <link>https://arxiv.org/abs/2501.19361</link>
      <description>arXiv:2501.19361v1 Announce Type: new 
Abstract: Numerous powerful large language models (LLMs) are now available for use as writing support tools, idea generators, and beyond. Although these LLMs are marketed as helpful creative assistants, several works have shown that using an LLM as a creative partner results in a narrower set of creative outputs. However, these studies only consider the effects of interacting with a single LLM, begging the question of whether such narrowed creativity stems from using a particular LLM -- which arguably has a limited range of outputs -- or from using LLMs in general as creative assistants. To study this question, we elicit creative responses from humans and a broad set of LLMs using standardized creativity tests and compare the population-level diversity of responses. We find that LLM responses are much more similar to other LLM responses than human responses are to each other, even after controlling for response structure and other key variables. This finding of significant homogeneity in creative outputs across the LLMs we evaluate adds a new dimension to the ongoing conversation about creativity and LLMs. If today's LLMs behave similarly, using them as a creative partners -- regardless of the model used -- may drive all users towards a limited set of "creative" outputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19361v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emily Wenger, Yoed Kenett</dc:creator>
    </item>
    <item>
      <title>AI Biases Towards Rich and Powerful Surnames</title>
      <link>https://arxiv.org/abs/2501.19407</link>
      <description>arXiv:2501.19407v1 Announce Type: new 
Abstract: Surnames often convey implicit markers of social status, wealth, and lineage, shaping perceptions in ways that can perpetuate systemic biases. This study investigates whether and how surnames influence AI-driven decision-making, focusing on their effects across key areas such as hiring recommendations, leadership appointments, and loan approvals. Drawing on 600 surnames from the United States and Thailand, countries with differing sociohistorical dynamics and surname conventions, we categorize names into Rich, Legacy, Normal, and phonetically similar Variant groups. Our findings reveal that elite surnames consistently predict AI-generated perceptions of power, intelligence, and wealth, leading to significant consequences for decisions in high-stakes situations. Mediation analysis highlights perceived intelligence as a crucial pathway through which surname biases operate. Providing objective qualifications alongside the surnames reduces, but does not eliminate, these biases, especially in contexts with uniformly low credentials. These results call for fairness-aware algorithms and robust policy interventions to mitigate the reinforcement of inherited inequalities by AI systems. Our work also urges a reexamination of algorithmic accountability and its societal impact, particularly in systems designed for meritocratic outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19407v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pat Pataranutaporn, Nattavudh Powdthavee, Pattie Maes</dc:creator>
    </item>
    <item>
      <title>The Effect of Covid-19 Lockdown on Human Behaviour Using Analytical Hierarchy Process</title>
      <link>https://arxiv.org/abs/2501.18603</link>
      <description>arXiv:2501.18603v1 Announce Type: cross 
Abstract: The coronavirus pandemic corresponds to a serious global health crisis which not only changed the way people used to live but also how people behaved in their daily lives. Information from social and behavioural sciences can help in modifying human behaviour to comply with the recommendations of health officials, as the pandemic requires large-scale behaviour change and puts significant mental stress on individuals. The aim of this paper is to examine the changes in human behaviour brought about by the COVID-19 pandemic, which has caused a global health crisis and altered the way people live and interact. The collection of data has been done through online mode and the behaviour of the people is observed, and the results were finally analysed using the Analytical Hierarchy Process (AHP) which is a multi-criteria decision-making method to rank the factors that had the greatest impact on the changes in human behaviour. During the study, parameters taken under consideration were the ones which were most likely to affect the human behaviour as an impact of COVID-19 lockdown on health, relationship with family and friends, overall lifestyle, online education and work from home, screen time etc. The paper explains each criterion and how it affected human behaviour the most.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18603v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rashi Jain, Mansi Yadav</dc:creator>
    </item>
    <item>
      <title>Gender assignment in doctoral theses: revisiting Teseo with a method based on cultural consensus theory</title>
      <link>https://arxiv.org/abs/2501.18607</link>
      <description>arXiv:2501.18607v1 Announce Type: cross 
Abstract: This study critically evaluates gender assignment methods within academic contexts, employing a comparative analysis of diverse techniques, including a SVM classifier, gender-guesser, genderize.io, and a Cultural Consensus Theory based classifier. Emphasizing the significance of transparency, data sources, and methodological considerations, the research introduces nomquamgender, a cultural consensus-based method, and applies it to Teseo, a Spanish dissertation database. The results reveal a substantial reduction in the number of individuals with unknown gender compared to traditional methods relying on INE data. The nuanced differences in gender distribution underscore the importance of methodological choices in gender studies, urging for transparent, comprehensive, and freely accessible methods to enhance the accuracy and reliability of gender assignment in academic research. After reevaluating the problem of gender imbalances in the doctoral system we can conclude that it's still evident although the trend is clearly set for its reduction. Finaly, specific problems related to some disciplines, including STEM fields and seniority roles are found to be worth of attention in the near future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18607v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11192-024-05079-z</arxiv:DOI>
      <arxiv:journal_reference>Matias-Rayme, N., Botezan, I., Suarez-Figueroa, M.C. and Sanchez-Jimenez, R. Gender assignment in doctoral theses: revisiting Teseo with a method based on cultural theory. Scientometrics 129, 4553-4572 (2024)</arxiv:journal_reference>
      <dc:creator>Nataly Matias-Rayme, Iuliana Botezan, Mari Carmen Su\'arez-Figueroa, Rodrigo S\'anchez-Jim\'enez</dc:creator>
    </item>
    <item>
      <title>Indiana Jones: There Are Always Some Useful Ancient Relics</title>
      <link>https://arxiv.org/abs/2501.18628</link>
      <description>arXiv:2501.18628v1 Announce Type: cross 
Abstract: This paper introduces Indiana Jones, an innovative approach to jailbreaking Large Language Models (LLMs) by leveraging inter-model dialogues and keyword-driven prompts. Through orchestrating interactions among three specialised LLMs, the method achieves near-perfect success rates in bypassing content safeguards in both white-box and black-box LLMs. The research exposes systemic vulnerabilities within contemporary models, particularly their susceptibility to producing harmful or unethical outputs when guided by ostensibly innocuous prompts framed in historical or contextual contexts. Experimental evaluations highlight the efficacy and adaptability of Indiana Jones, demonstrating its superiority over existing jailbreak methods. These findings emphasise the urgent need for enhanced ethical safeguards and robust security measures in the development of LLMs. Moreover, this work provides a critical foundation for future studies aimed at fortifying LLMs against adversarial exploitation while preserving their utility and flexibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18628v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junchen Ding, Jiahao Zhang, Yi Liu, Ziqi Ding, Gelei Deng, Yuekang Li</dc:creator>
    </item>
    <item>
      <title>Divergent Emotional Patterns in Disinformation on Social Media? An Analysis of Tweets and TikToks about the DANA in Valencia</title>
      <link>https://arxiv.org/abs/2501.18640</link>
      <description>arXiv:2501.18640v1 Announce Type: cross 
Abstract: This study investigates the dissemination of disinformation on social media platforms during the DANA event (DANA is a Spanish acronym for Depresion Aislada en Niveles Altos, translating to high-altitude isolated depression) that resulted in extremely heavy rainfall and devastating floods in Valencia, Spain, on October 29, 2024. We created a novel dataset of 650 TikTok and X posts, which was manually annotated to differentiate between disinformation and trustworthy content. Additionally, a Few-Shot annotation approach with GPT-4o achieved substantial agreement (Cohen's kappa of 0.684) with manual labels. Emotion analysis revealed that disinformation on X is mainly associated with increased sadness and fear, while on TikTok, it correlates with higher levels of anger and disgust. Linguistic analysis using the LIWC dictionary showed that trustworthy content utilizes more articulate and factual language, whereas disinformation employs negations, perceptual words, and personal anecdotes to appear credible. Audio analysis of TikTok posts highlighted distinct patterns: trustworthy audios featured brighter tones and robotic or monotone narration, promoting clarity and credibility, while disinformation audios leveraged tonal variation, emotional depth, and manipulative musical elements to amplify engagement. In detection models, SVM+TF-IDF achieved the highest F1-Score, excelling with limited data. Incorporating audio features into roberta-large-bne improved both Accuracy and F1-Score, surpassing its text-only counterpart and SVM in Accuracy. GPT-4o Few-Shot also performed well, showcasing the potential of large language models for automated disinformation detection. These findings demonstrate the importance of leveraging both textual and audio features for improved disinformation detection on multimodal platforms like TikTok.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18640v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 17th International Conference on Agents and Artificial Intelligence (ICAART 2025), Porto, Portugal, February 23-25, 2025</arxiv:journal_reference>
      <dc:creator>Iv\'an Arcos, Paolo Rosso, Ram\'on Salaverr\'ia</dc:creator>
    </item>
    <item>
      <title>The Pitfalls of "Security by Obscurity" And What They Mean for Transparent AI</title>
      <link>https://arxiv.org/abs/2501.18669</link>
      <description>arXiv:2501.18669v1 Announce Type: cross 
Abstract: Calls for transparency in AI systems are growing in number and urgency from diverse stakeholders ranging from regulators to researchers to users (with a comparative absence of companies developing AI). Notions of transparency for AI abound, each addressing distinct interests and concerns.
  In computer security, transparency is likewise regarded as a key concept. The security community has for decades pushed back against so-called security by obscurity -- the idea that hiding how a system works protects it from attack -- against significant pressure from industry and other stakeholders. Over the decades, in a community process that is imperfect and ongoing, security researchers and practitioners have gradually built up some norms and practices around how to balance transparency interests with possible negative side effects. This paper asks: What insights can the AI community take from the security community's experience with transparency?
  We identify three key themes in the security community's perspective on the benefits of transparency and their approach to balancing transparency against countervailing interests. For each, we investigate parallels and insights relevant to transparency in AI. We then provide a case study discussion on how transparency has shaped the research subfield of anonymization. Finally, shifting our focus from similarities to differences, we highlight key transparency issues where modern AI systems present challenges different from other kinds of security-critical systems, raising interesting open questions for the security and AI communities alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18669v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Peter Hall, Olivia Mundahl, Sunoo Park</dc:creator>
    </item>
    <item>
      <title>Logical Modalities within the European AI Act: An Analysis</title>
      <link>https://arxiv.org/abs/2501.19112</link>
      <description>arXiv:2501.19112v1 Announce Type: cross 
Abstract: The paper presents a comprehensive analysis of the European AI Act in terms of its logical modalities, with the aim of preparing its formal representation, for example, within the logic-pluralistic Knowledge Engineering Framework and Methodology (LogiKEy). LogiKEy develops computational tools for normative reasoning based on formal methods, employing Higher-Order Logic (HOL) as a unifying meta-logic to integrate diverse logics through shallow semantic embeddings. This integration is facilitated by Isabelle/HOL, a proof assistant tool equipped with several automated theorem provers. The modalities within the AI Act and the logics suitable for their representation are discussed. For a selection of these logics, embeddings in HOL are created, which are then used to encode sample paragraphs. Initial experiments evaluate the suitability of these embeddings for automated reasoning, and highlight key challenges on the way to more robust reasoning capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19112v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LO</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lara Lawniczak, Christoph Benzm\"uller</dc:creator>
    </item>
    <item>
      <title>The Atlas for the Aspiring Network Scientist</title>
      <link>https://arxiv.org/abs/2101.00863</link>
      <description>arXiv:2101.00863v3 Announce Type: replace 
Abstract: Network science is the field dedicated to the investigation and analysis of complex systems via their representations as networks. We normally model such networks as graphs: sets of nodes connected by sets of edges and a number of node and edge attributes. This deceptively simple object is the starting point of never-ending complexity, due to its ability to represent almost every facet of reality: chemical interactions, protein pathways inside cells, neural connections inside the brain, scientific collaborations, financial relations, citations in art history, just to name a few examples. If we hope to make sense of complex networks, we need to master a large analytic toolbox: graph and probability theory, linear algebra, statistical physics, machine learning, combinatorics, and more.
  This book aims at providing the first access to all these tools. It is intended as an "Atlas", because its interest is not in making you a specialist in using any of these techniques. Rather, after reading this book, you will have a general understanding about the existence and the mechanics of all these approaches. You can use such an understanding as the starting point of your own career in the field of network science. This has been, so far, an interdisciplinary endeavor. The founding fathers of this field come from many different backgrounds: mathematics, sociology, computer science, physics, history, digital humanities, and more. This Atlas is charting your path to be something different from all of that: a pure network scientist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.00863v3</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michele Coscia</dc:creator>
    </item>
    <item>
      <title>AI, insurance, discrimination and unfair differentiation. An overview and research agenda</title>
      <link>https://arxiv.org/abs/2401.11892</link>
      <description>arXiv:2401.11892v4 Announce Type: replace 
Abstract: Insurers underwrite risks: they calculate risks and decide on the insurance price. Insurers seem captivated by two trends enabled by Artificial Intelligence (AI). (i) First, insurers could use AI for analysing more and new types of data to assess risks more precisely: data-intensive underwriting. (ii) Second, insurers could use AI to monitor the behaviour of individual consumers in real-time: behaviour-based insurance. For example, some car insurers offer a discount if the consumer agrees to being tracked by the insurer and drives safely. While the two trends bring many advantages, they may also have discriminatory effects on society. This paper focuses on the following question. Which effects related to discrimination and unfair differentiation may occur if insurers use data-intensive underwriting and behaviour-based insurance?</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11892v4</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marvin van Bekkum, Frederik Zuiderveen Borgesius, Tom Heskes</dc:creator>
    </item>
    <item>
      <title>Sociotechnical Approach to Enterprise Generative Artificial Intelligence (E-GenAI)</title>
      <link>https://arxiv.org/abs/2409.17408</link>
      <description>arXiv:2409.17408v2 Announce Type: replace 
Abstract: In this theoretical article, a sociotechnical approach is proposed to characterize. First, the business ecosystem, focusing on the relationships among Providers, Enterprise, and Customers through SCM, ERP, and CRM platforms to align: (1) Business Intelligence (BI), Fuzzy Logic (FL), and TRIZ (Theory of Inventive Problem Solving), through the OID model, and (2) Knowledge Management (KM) and Imperfect Knowledge Management (IKM), through the OIDK model. Second, the article explores the E-GenAI business ecosystem, which integrates GenAI-based platforms for SCM, ERP, and CRM with GenAI-based platforms for BI, FL, TRIZ, KM, and IKM, to align Large Language Models (LLMs) through the E-GenAI (OID) model. Finally, to understand the dynamics of LLMs, we utilize finite automata to model the relationships between Followers and Followees. This facilitates the construction of LLMs that can identify specific characteristics of users on a social media platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17408v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leoncio Jimenez, Francisco Venegas</dc:creator>
    </item>
    <item>
      <title>Characterizing the MrDeepFakes Sexual Deepfake Marketplace</title>
      <link>https://arxiv.org/abs/2410.11100</link>
      <description>arXiv:2410.11100v3 Announce Type: replace 
Abstract: The prevalence of sexual deepfake material has exploded over the past several years. Attackers create and utilize deepfakes for many reasons: to seek sexual gratification, to harass and humiliate targets, or to exert power over an intimate partner. In part enabling this growth, several markets have emerged to support the buying and selling of sexual deepfake material. In this paper, we systematically characterize the most prominent and mainstream marketplace, MrDeepFakes. We analyze the marketplace economics, the targets of created media, and user discussions of how to create deepfakes, which we use to understand the current state-of-the-art in deepfake creation. Our work uncovers little enforcement of posted rules (e.g., limiting targeting to well-established celebrities), previously undocumented attacker motivations, and unexplored attacker tactics for acquiring resources to create sexual deepfakes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11100v3</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Catherine Han, Anne Li, Deepak Kumar, Zakir Durumeric</dc:creator>
    </item>
    <item>
      <title>Regulating radiology AI medical devices that evolve in their lifecycle</title>
      <link>https://arxiv.org/abs/2412.20498</link>
      <description>arXiv:2412.20498v3 Announce Type: replace 
Abstract: Over time, the distribution of medical image data drifts due to factors such as shifts in patient demographics, acquisition devices, and disease manifestations. While human radiologists can adjust their expertise to accommodate such variations, deep learning models cannot. In fact, such models are highly susceptible to even slight variations in image characteristics. Consequently, manufacturers must conduct regular updates to ensure that they remain safe and effective. Performing such updates in the United States and European Union required, until recently, obtaining re-approval. Given the time and financial burdens associated with these processes, updates were infrequent, and obsolete systems remained in operation for too long. During 2024, several regulatory developments promised to streamline the safe rollout of model updates: The European Artificial Intelligence Act came into effect last August, and the Food and Drug Administration (FDA) issued final marketing submission recommendations for a Predetermined Change Control Plan (PCCP) in December. We provide an overview of these developments and outline the key building blocks necessary for successfully deploying dynamic systems. At the heart of these regulations - and as prerequisites for manufacturers to conduct model updates without re-approval - are clear descriptions of data collection and re-training processes, coupled with robust real-world quality monitoring mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20498v3</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camila Gonz\'alez, Moritz Fuchs, Daniel Pinto dos Santos, Philipp Matthies, Manuel Trenz, Maximilian Gr\"uning, Akshay Chaudhari, David B. Larson, Ahmed Othman, Moon Kim, Felix Nensa, Anirban Mukhopadhyay</dc:creator>
    </item>
    <item>
      <title>Polarized Patterns of Language Toxicity and Sentiment of Debunking Posts on Social Media</title>
      <link>https://arxiv.org/abs/2501.06274</link>
      <description>arXiv:2501.06274v2 Announce Type: replace 
Abstract: The rise of misinformation and fake news in online political discourse poses significant challenges to democratic processes and public engagement. While debunking efforts aim to counteract misinformation and foster fact-based dialogue, these discussions often involve language toxicity and emotional polarization. We examined over 86 million debunking tweets and more than 4 million Reddit debunking comments to investigate the relationship between language toxicity, pessimism, and social polarization in debunking efforts. Focusing on discussions of the 2016 and 2020 U.S. presidential elections and the QAnon conspiracy theory, our analysis reveals three key findings: (1) peripheral participants (1-degree users) play a disproportionate role in shaping toxic discourse, driven by lower community accountability and emotional expression; (2) platform mechanisms significantly influence polarization, with Twitter amplifying partisan differences and Reddit fostering higher overall toxicity due to its structured, community-driven interactions; and (3) a negative correlation exists between language toxicity and pessimism, with increased interaction reducing toxicity, especially on Reddit. We show that platform architecture affects informational complexity of user interactions, with Twitter promoting concentrated, uniform discourse and Reddit encouraging diverse, complex communication. Our findings highlight the importance of user engagement patterns, platform dynamics, and emotional expressions in shaping polarization in debunking discourse. This study offers insights for policymakers and platform designers to mitigate harmful effects and promote healthier online discussions, with implications for understanding misinformation, hate speech, and political polarization in digital environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06274v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wentao Xu, Wenlu Fan, Shiqian Lu, Tenghao Li, Bin Wang</dc:creator>
    </item>
    <item>
      <title>Complex Dynamic Systems in Education: Beyond the Static, the Linear and the Causal Reductionism</title>
      <link>https://arxiv.org/abs/2501.10386</link>
      <description>arXiv:2501.10386v2 Announce Type: replace 
Abstract: Traditional methods in educational research often fail to capture the complex and evolving nature of learning processes. This chapter examines the use of complex systems theory in education to address these limitations. The chapter covers the main characteristics of complex systems such as non-linear relationships, emergent properties, and feedback mechanisms to explain how educational phenomena unfold. Some of the main methodological approaches are presented, such as network analysis and recurrence quantification analysis to study relationships and patterns in learning. These have been operationalized by existing education research to study self-regulation, engagement, and academic emotions, among other learning-related constructs. Lastly, the chapter describes data collection methods that are suitable for studying learning processes from a complex systems' lens.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10386v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Saqr, Daryn Dever, Sonsoles L\'opez-Pernas, Christophe Gernigon, Gwen Marchand, Avi Kaplan</dc:creator>
    </item>
    <item>
      <title>IsolateGPT: An Execution Isolation Architecture for LLM-Based Agentic Systems</title>
      <link>https://arxiv.org/abs/2403.04960</link>
      <description>arXiv:2403.04960v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) extended as systems, such as ChatGPT, have begun supporting third-party applications. These LLM apps leverage the de facto natural language-based automated execution paradigm of LLMs: that is, apps and their interactions are defined in natural language, provided access to user data, and allowed to freely interact with each other and the system. These LLM app ecosystems resemble the settings of earlier computing platforms, where there was insufficient isolation between apps and the system. Because third-party apps may not be trustworthy, and exacerbated by the imprecision of natural language interfaces, the current designs pose security and privacy risks for users. In this paper, we evaluate whether these issues can be addressed through execution isolation and what that isolation might look like in the context of LLM-based systems, where there are arbitrary natural language-based interactions between system components, between LLM and apps, and between apps. To that end, we propose IsolateGPT, a design architecture that demonstrates the feasibility of execution isolation and provides a blueprint for implementing isolation, in LLM-based systems. We evaluate IsolateGPT against a number of attacks and demonstrate that it protects against many security, privacy, and safety issues that exist in non-isolated LLM-based systems, without any loss of functionality. The performance overhead incurred by IsolateGPT to improve security is under 30% for three-quarters of tested queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04960v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Network and Distributed System Security (NDSS) Symposium 2025</arxiv:journal_reference>
      <dc:creator>Yuhao Wu, Franziska Roesner, Tadayoshi Kohno, Ning Zhang, Umar Iqbal</dc:creator>
    </item>
    <item>
      <title>The Smart Buildings Control Suite: A Diverse Open Source Benchmark to Evaluate and Scale HVAC Control Policies for Sustainability</title>
      <link>https://arxiv.org/abs/2410.03756</link>
      <description>arXiv:2410.03756v2 Announce Type: replace-cross 
Abstract: Commercial buildings account for 17% of U.S. carbon emissions, with roughly half of that from Heating, Ventilation, and Air Conditioning (HVAC). HVAC devices form a complex thermodynamic system, and while Model Predictive Control and Reinforcement Learning have been used to optimize control policies, scaling to thousands of buildings remains a significant unsolved challenge. Most current algorithms are over-optimized for specific buildings and rely on proprietary data or hard-to-configure simulations. We present the Smart Buildings Control Suite, the first open source interactive HVAC control benchmark with a focus on solutions that scale. It consists of 3 components: real-world telemetric data extracted from 11 buildings over 6 years, a lightweight data-driven simulator for each building, and a modular Physically Informed Neural Network (PINN) building model as a simulator alternative. The buildings span a variety of climates, management systems, and sizes, and both the simulator and PINN easily scale to new buildings, ensuring solutions using this benchmark are robust to these factors and only reliant on fully scalable building models. This represents a major step towards scaling HVAC optimization from the lab to buildings everywhere. To facilitate use, our benchmark is compatible with the Gym standard, and our data is part of TensorFlow Datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03756v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Judah Goldfeder, Victoria Dean, Zixin Jiang, Xuezheng Wang, Bing dong, Hod Lipson, John Sipple</dc:creator>
    </item>
    <item>
      <title>SafetyAnalyst: Interpretable, transparent, and steerable safety moderation for AI behavior</title>
      <link>https://arxiv.org/abs/2410.16665</link>
      <description>arXiv:2410.16665v2 Announce Type: replace-cross 
Abstract: The ideal AI safety moderation system would be both structurally interpretable (so its decisions can be reliably explained) and steerable (to align to safety standards and reflect a community's values), which current systems fall short on. To address this gap, we present SafetyAnalyst, a novel AI safety moderation framework. Given an AI behavior, SafetyAnalyst uses chain-of-thought reasoning to analyze its potential consequences by creating a structured "harm-benefit tree," which enumerates harmful and beneficial actions and effects the AI behavior may lead to, along with likelihood, severity, and immediacy labels that describe potential impact on any stakeholders. SafetyAnalyst then aggregates all harmful and beneficial effects into a harmfulness score using fully interpretable weight parameters, which can be aligned to particular safety preferences. We applied this conceptual framework to develop, test, and release an open-source LLM prompt safety classification system, distilled from 18.5 million harm-benefit features generated by frontier LLMs on 19k prompts. On a comprehensive set of prompt safety benchmarks, we show that SafetyReporter (average F1=0.81) outperforms existing LLM safety moderation systems (average F1$&lt;$0.72) on prompt safety classification, while offering the additional advantages of interpretability, transparency, and steerability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16665v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing-Jing Li, Valentina Pyatkin, Max Kleiman-Weiner, Liwei Jiang, Nouha Dziri, Anne G. E. Collins, Jana Schaich Borg, Maarten Sap, Yejin Choi, Sydney Levine</dc:creator>
    </item>
  </channel>
</rss>
