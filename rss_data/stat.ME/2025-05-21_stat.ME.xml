<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 May 2025 04:00:02 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stochastic Processes with Modified Lognormal Distribution Featuring Flexible Upper Tail</title>
      <link>https://arxiv.org/abs/2505.14713</link>
      <description>arXiv:2505.14713v1 Announce Type: new 
Abstract: Asymmetric, non-Gaussian probability distributions are often observed in the analysis of natural and engineering datasets. The lognormal distribution is a standard model for data with skewed frequency histograms and fat tails. However, the lognormal law severely restricts the asymptotic dependence of the probability density and the hazard function for high values. Herein we present a family of three-parameter non-Gaussian probability density functions that are based on generalized kappa-exponential and kappa-logarithm functions and investigate its mathematical properties. These kappa-lognormal densities represent continuous deformations of the lognormal with lighter right tails, controlled by the parameter kappa. In addition, bimodal distributions are obtained for certain parameter combinations. We derive closed-form analytic expressions for the main statistical functions of the kappa-lognormal distribution. For the moments, we derive bounds that are based on hypergeometric functions as well as series expansions. Explicit expressions for the gradient and Hessian of the negative log-likelihood are obtained to facilitate numerical maximum-likelihood estimates of the kappa-lognormal parameters from data. We also formulate a joint probability density function for kappa-lognormal stochastic processes by applying Jacobi's multivariate theorem to a latent Gaussian process. Estimation of the kappa-lognormal distribution based on synthetic and real data is explored. Furthermore, we investigate applications of kappa-lognormal processes with different covariance kernels in time series forecasting and spatial interpolation using warped Gaussian process regression. Our results are of practical interest for modeling skewed distributions in various scientific and engineering fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14713v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dionissios T. Hristopulos, Anastassia Baxevani, Giorgio Kaniadakis</dc:creator>
    </item>
    <item>
      <title>A kernel-based framework for covariate significance tests in nonparametric regression</title>
      <link>https://arxiv.org/abs/2505.14851</link>
      <description>arXiv:2505.14851v1 Announce Type: new 
Abstract: It is well known that nonparametric regression estimation and inference procedures are subject to the curse of dimensionality. Moreover, model interpretability usually decreases with the data dimension. Therefore, model-free variable selection procedures and, in particular, covariate significance tests, are invaluable tools for regression modelling as they help to remove irrelevant covariates. In this contribution, we provide a general framework, based on recent developments in the theory of kernel-based characterizations of null conditional expectations, for testing the significance of a subgroup of Hilbert space-valued covariates in a nonparametric regression model. Moreover, we propose a test designed to be robust against the curse of dimensionality and we provide some asymptotic results regarding the distribution of the test statistic under the null hypothesis of non-significant covariates as well as under fixed and local alternatives. Regarding the test calibration, we present and prove the consistency of a multiplier bootstrap scheme. An extensive simulation study is conducted to assess the finite sample performance of the test. We also apply our test in a real data scenario</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14851v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Diz-Castro, Manuel Febrero-Bande, Wenceslao Gonz\'alez-Manteiga</dc:creator>
    </item>
    <item>
      <title>Envelope-based partial least squares in functional regression</title>
      <link>https://arxiv.org/abs/2505.14876</link>
      <description>arXiv:2505.14876v1 Announce Type: new 
Abstract: In this article, we extend predictor envelope models to settings with multivariate outcomes and multiple, functional predictors. We propose a two-step estimation strategy, which first projects the function onto a finite-dimensional Euclidean space before fitting the model using existing approaches to envelope models. We first develop an estimator under a linear model with continuous outcomes and then extend this procedure to the more general class of generalized linear models, which allow for a variety of outcome types. We provide asymptotic theory for these estimators showing that they are root-$n$ consistent and asymptotically normal when the regression coefficient is finite-rank. Additionally we show that consistency can be obtained even when the regression coefficient has rank that grows with the sample size. Extensive simulation studies confirm our theoretical results and show strong prediction performance of the proposed estimators. Additionally, we provide multiple data analyses showing that the proposed approach performs well in real-world settings under a variety of outcome types compared with existing dimension reduction approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14876v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxuan Wu, Joseph Antonelli, Zhihua Su</dc:creator>
    </item>
    <item>
      <title>Modeling and prediction of mutation fitness on protein functionality with structural information using high-dimensional Potts model</title>
      <link>https://arxiv.org/abs/2505.14958</link>
      <description>arXiv:2505.14958v1 Announce Type: new 
Abstract: Quantifying the effects of amino acid mutations in proteins presents a significant challenge due to the vast combinations of residue sites and amino acid types, making experimental approaches costly and time-consuming. The Potts model has been used to address this challenge, with parameters capturing evolutionary dependency between residue sites within a protein family. However, existing methods often use the mean-field approximation to reduce computational demands, which lacks provable guarantees and overlooks critical structural information for assessing mutation effects. We propose a new framework for analyzing protein sequences using the Potts model with node-wise high-dimensional multinomial regression. Our method identifies key residue interactions and important amino acids, quantifying mutation effects through evolutionary energy derived from model parameters. It encourages sparsity in both site-wise and amino acid-wise dependencies through element-wise and group sparsity. We have established, for the first time to our knowledge, the $\ell_2$ convergence rate for estimated parameters in the high-dimensional Potts model using sparse group Lasso, matching the existing minimax lower bound for high-dimensional linear models with a sparse group structure, up to a factor depending only on the multinomial nature of the Potts model. This theoretical guarantee enables accurate quantification of estimated energy changes. Additionally, we incorporate structural data into our model by applying penalty weights across site pairs. Our method outperforms others in predicting mutation fitness, as demonstrated by comparisons with high-throughput mutagenesis experiments across 12 protein families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14958v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Bingying Dai, Yinan Lin, Kejue Jia, Zhao Ren, Wen Zhou</dc:creator>
    </item>
    <item>
      <title>Pre-validation Revisited</title>
      <link>https://arxiv.org/abs/2505.14985</link>
      <description>arXiv:2505.14985v1 Announce Type: new 
Abstract: Pre-validation is a way to build prediction model with two datasets of significantly different feature dimensions. Previous work showed that the asymptotic distribution of test statistic for the pre-validated predictor deviated from a standard Normal, hence will lead to issues in hypothesis tests. In this paper, we revisited the pre-validation procedure and extended the problem formulation without any independence assumption on the two feature sets. We proposed not only an analytical distribution of the test statistics for pre-validated predictor under certain models, but also a generic bootstrap procedure to conduct inference. We showed properties and benefits of pre-validation in prediction, inference and error estimation by simulation and various applications, including analysis of a breast cancer study and a synthetic GWAS example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14985v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Shang, Sourav Chatterjee, Trevor Hastie, Robert Tibshirani</dc:creator>
    </item>
    <item>
      <title>Orthogonal Arrays: A Review</title>
      <link>https://arxiv.org/abs/2505.15032</link>
      <description>arXiv:2505.15032v1 Announce Type: new 
Abstract: Orthogonal arrays are arguably one of the most fascinating and important statistical tools for efficient data collection. They have a simple, natural definition, desirable properties when used as fractional factorials, and a rich and beautiful mathematical theory. Their connections with combinatorics, finite fields, geometry, and error-correcting codes are profound. Orthogonal arrays have been widely used in agriculture, engineering, manufacturing, and high-technology industries for quality and productivity improvement experiments. In recent years, they have drawn rapidly growing interest from various fields such as computer experiments, integration, visualization, optimization, big data, machine learning/artificial intelligence through successful applications in those fields. We review the fundamental concepts and statistical properties and report recent developments. Discussions of recent applications and connections with various fields are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15032v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/wics.70029</arxiv:DOI>
      <dc:creator>C. Devon Lin, John Stufken</dc:creator>
    </item>
    <item>
      <title>A novel framework for detecting multiple change points in functional data sequences</title>
      <link>https://arxiv.org/abs/2505.15188</link>
      <description>arXiv:2505.15188v1 Announce Type: new 
Abstract: Detecting multiple change points in functional data sequences has been increasingly popular and critical in various scientific fields. In this article, we propose a novel two-stage framework for detecting multiple change points in functional data sequences, named as detection by Group Selection and Partial F-test (GS-PF). The detection problem is firstly transformed into a high-dimensional sparse estimation problem via functional basis expansion, and the penalized group selection is applied to estimate the number and locations of candidate change points in the first stage. To further circumvent the issue of overestimating the true number of change points in practice, a partial F-test is applied in the second stage to filter redundant change points so that the false discovery rate of the F-test for multiple change points is controlled. Additionally, in order to reduce complexity of the proposed GS-PF method, a link parameter is adopted to generate candidate sets of potential change points, which greatly reduces the number of detected change points and improves the efficiency. Asymptotic results are established and validated to guarantee detection consistency of the proposed GS-PF method, and its performance is evaluated through intensive simulations and real data analysis, compared with the state-of-the-art detecting methods. Our findings indicate that the proposed GS-PF method exhibits detection consistency in different scenarios, which endows our method with the capability for efficient and robust detection of multiple change points in functional data sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15188v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqing Fang, Xin Liu</dc:creator>
    </item>
    <item>
      <title>Testing for Replicated Signals Across Multiple Studies with Side Information</title>
      <link>https://arxiv.org/abs/2505.15328</link>
      <description>arXiv:2505.15328v1 Announce Type: new 
Abstract: Partial conjunction (PC) $p$-values and side information provided by covariates can be used to detect signals that replicate across multiple studies investigating the same set of features, all while controlling the false discovery rate (FDR). However, when many features are present, the extent of multiplicity correction required for FDR control, along with the inherently limited power of PC $p$-values$\unicode{x2013}$especially when replication across all studies is demanded$\unicode{x2013}$often inhibits the number of discoveries made. To address this problem, we develop a $p$-value-based covariate-adaptive methodology that revolves around partitioning studies into smaller groups and borrowing information between them to filter out unpromising features. This filtering strategy: 1) reduces the multiplicity correction required for FDR control, and 2) allows independent hypothesis weights to be trained on data from filtered-out features to enhance the power of the PC $p$-values in the rejection rule. Our methodology has finite-sample FDR control under minimal distributional assumptions, and we demonstrate its competitive performance through simulation studies and a real-world case study on gene expression and the immune system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15328v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ninh Tran, Dennis Leung</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach for Inference on Mixed Graphical Models</title>
      <link>https://arxiv.org/abs/2505.15464</link>
      <description>arXiv:2505.15464v1 Announce Type: new 
Abstract: Mixed data refers to a type of data in which variables can be of multiple types, such as continuous, discrete, or categorical. This data is routinely collected in various fields, including healthcare and social sciences. A common goal in the analysis of such data is to identify dependence relationships between variables, for an understanding of their associations. In this paper, we propose a Bayesian pairwise graphical model that estimates conditional independencies between any type of data. We implement a flexible modeling construction, that includes zero-inflated count data and can also handle missing data. We show that the model maintains both global and local Markov properties. We employ a spike-and-slab prior for the estimation of the graph and implement an MCMC algorithm for posterior inference based on conditional likelihoods. We assess performances on four simulation scenarios with distinct dependence structures, that also include cases with data missing at random, and compare results with existing methods. Finally, we present an analysis of real data from adolescents diagnosed with an eating disorder. Estimated graphs show differences in the associations estimated at intake and discharge, suggesting possible effects of the treatment on cognitive and behavioral measures in the adolescents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15464v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mauro Florez, Anna Gottard, Carrie McAdams, Michele Guindani, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Meta-analytic-predictive priors based on a single study</title>
      <link>https://arxiv.org/abs/2505.15502</link>
      <description>arXiv:2505.15502v1 Announce Type: new 
Abstract: Meta-analytic-predictive (MAP) priors have been proposed as a generic approach to deriving informative prior distributions, where external empirical data are processed to learn about certain parameter distributions. The use of MAP priors is also closely related to shrinkage estimation (also sometimes referred to as dynamic borrowing). A potentially odd situation arises when the external data consist only of a single study. Conceptually this is not a problem, it only implies that certain prior assumptions gain in importance and need to be specified with particular care. We outline this important, not uncommon special case and demonstrate its implementation and interpretation based on the normal-normal hierarchical model. The approach is illustrated using example applications in clinical medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15502v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian R\"over, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Modular Jump Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.15557</link>
      <description>arXiv:2505.15557v1 Announce Type: new 
Abstract: Gaussian processes (GPs) furnish accurate nonlinear predictions with well-calibrated uncertainty. However, the typical GP setup has a built-in stationarity assumption, making it ill-suited for modeling data from processes with sudden changes, or "jumps" in the output variable. The "jump GP" (JGP) was developed for modeling data from such processes, combining local GPs and latent "level" variables under a joint inferential framework. But joint modeling can be fraught with difficulty. We aim to simplify by suggesting a more modular setup, eschewing joint inference but retaining the main JGP themes: (a) learning optimal neighborhood sizes that locally respect manifolds of discontinuity; and (b) a new cluster-based (latent) feature to capture regions of distinct output levels on both sides of the manifold. We show that each of (a) and (b) separately leads to dramatic improvements when modeling processes with jumps. In tandem (but without requiring joint inference) that benefit is compounded, as illustrated on real and synthetic benchmark examples from the recent literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15557v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna R. Flowers, Christopher T. Franck, Micka\"el Binois, Chiwoo Park, Robert B. Gramacy</dc:creator>
    </item>
    <item>
      <title>Quantifying structural uncertainty in chemical reaction network inference</title>
      <link>https://arxiv.org/abs/2505.15653</link>
      <description>arXiv:2505.15653v1 Announce Type: new 
Abstract: Dynamical systems in chemistry and biology are complex, and one often does not have comprehensive knowledge about the interactions involved. Chemical reaction network (CRN) inference aims to identify, from observing species concentrations over time, the unknown reactions between the species. Existing approaches largely focus on identifying a single, most likely CRN, without addressing uncertainty about the resulting network structure. However, it is important to quantify structural uncertainty to have confidence in our inference and predictions. In this work, we do so by constructing an approximate posterior distribution over CRN structures. This is done by keeping a large set of suboptimal solutions found in an optimisation framework with sparse regularisation, in contrast to existing optimisation approaches which discard suboptimal solutions. We find that inducing reaction sparsity with nonconvex penalty functions results in more parsimonious CRNs compared to the popular lasso regularisation. In a real-data example where multiple CRNs have been previously suggested, we simultaneously recover reactions proposed from different literature under our approach to structural uncertainty. We demonstrate how posterior correlations between reactions help identify where structural ambiguities are present. This can be translated into alternative reaction pathways suggested by the available data, thus guiding the efforts of future experimental design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15653v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yong See Foo, Adriana Zanca, Jennifer A. Flegg, Ivo Siekmann</dc:creator>
    </item>
    <item>
      <title>Estimating Associations Between Cumulative Exposure and Health via Generalized Distributed Lag Non-Linear Models using Penalized Splines</title>
      <link>https://arxiv.org/abs/2505.15759</link>
      <description>arXiv:2505.15759v1 Announce Type: new 
Abstract: Quantifying associations between short-term exposure to ambient air pollution and health outcomes is an important public health priority. Many studies have investigated the association considering delayed effects within the past few days. Adaptive cumulative exposure distributed lag non-linear models (ACE-DLNMs) quantify associations between health outcomes and cumulative exposure that is specified in a data-adaptive way. While the ACE-DLNM framework is highly interpretable, it is limited to continuous outcomes and does not scale well to large datasets. Motivated by a large analysis of daily pollution and respiratory hospitalization counts in Canada between 2001 and 2018, we propose a generalized ACE-DLNM incorporating penalized splines, improving upon existing ACE-DLNM methods to accommodate general response types. We then develop a computationally efficient estimation strategy based on profile likelihood and Laplace approximate marginal likelihood with Newton-type methods. We demonstrate the performance and practical advantages of the proposed method through simulations. In application to the motivating analysis, the proposed method yields more stable inferences compared to generalized additive models with fixed exposures, while retaining interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15759v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianyi Pan, Hwashin Hyun Shin, Glen McGee, Alex Stringer</dc:creator>
    </item>
    <item>
      <title>Assimilative Causal Inference</title>
      <link>https://arxiv.org/abs/2505.14825</link>
      <description>arXiv:2505.14825v1 Announce Type: cross 
Abstract: Causal inference determines cause-and-effect relationships between variables and has broad applications across disciplines. Traditional time-series methods often reveal causal links only in a time-averaged sense, while ensemble-based information transfer approaches detect the time evolution of short-term causal relationships but are typically limited to low-dimensional systems. In this paper, a new causal inference framework, called assimilative causal inference (ACI), is developed. Fundamentally different from the state-of-the-art methods, ACI uses a dynamical system and a single realization of a subset of the state variables to identify instantaneous causal relationships and the dynamic evolution of the associated causal influence range (CIR). Instead of quantifying how causes influence effects as done traditionally, ACI solves an inverse problem via Bayesian data assimilation, thus tracing causes backward from observed effects with an implicit Bayesian hypothesis. Causality is determined by assessing whether incorporating the information of the effect variables reduces the uncertainty in recovering the potential cause variables. ACI has several desirable features. First, it captures the dynamic interplay of variables, where their roles as causes and effects can shift repeatedly over time. Second, a mathematically justified objective criterion determines the CIR without empirical thresholds. Third, ACI is scalable to high-dimensional problems by leveraging computationally efficient Bayesian data assimilation techniques. Finally, ACI applies to short time series and incomplete datasets. Notably, ACI does not require observations of candidate causes, which is a key advantage since potential drivers are often unknown or unmeasured. The effectiveness of ACI is demonstrated by complex dynamical systems showcasing intermittency and extreme events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14825v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marios Andreou, Nan Chen, Erik Bollt</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariate Approach to Subnational mortality graduation with Age-Varying Smoothness</title>
      <link>https://arxiv.org/abs/2505.14955</link>
      <description>arXiv:2505.14955v1 Announce Type: cross 
Abstract: This work introduces a Bayesian smoothing approach for the joint graduation of mortality rates across multiple populations. In particular, dynamical linear models are used to induce smoothness across ages through structured dependence, analogously to how temporal correlation is accommodated in state-space time-indexed models. An essential issue in subnational mortality probabilistic modelling is the lack or sparseness of information for some subpopulations. For many countries, mortality data is severely limited, and approaches based on a single population model can result in high uncertainty in the adjusted mortality tables. Here, we recognize the interdependence within a group of mortality data and pursue the pooling of information across several curves that ideally share common characteristics, such as the influence of epidemics or major economic shifts. Our proposal considers multivariate Bayesian dynamical models with common parameters, allowing for borrowing of information across mortality tables and enabling tests of convergence across populations. We also employ discount factors, typical in DLMs, to regulate smoothness, with varying discounting across ages, ensuring less smoothness at younger ages and greater stability at adult ages. This setup implies a trade-off between stability and adaptability. The discount parameter controls the responsiveness of the fit at older ages to new data. The estimation is fully Bayesian, accommodating all uncertainties in modelling and prediction. To illustrate the effectiveness of our model, we analyse male and female mortality data from England and Wales between 2010 and 2012, obtained from the Office for National Statistics. In scenarios with simulated missing data, our approach showed strong performance and flexibility in pooling information from related populations with more complete data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.14955v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luiz F. V. Figueiredo, Viviana G. R. Lobo, Mariane B. Alves, Thais C. O. Fonseca</dc:creator>
    </item>
    <item>
      <title>Clustering and Pruning in Causal Data Fusion</title>
      <link>https://arxiv.org/abs/2505.15215</link>
      <description>arXiv:2505.15215v1 Announce Type: cross 
Abstract: Data fusion, the process of combining observational and experimental data, can enable the identification of causal effects that would otherwise remain non-identifiable. Although identification algorithms have been developed for specific scenarios, do-calculus remains the only general-purpose tool for causal data fusion, particularly when variables are present in some data sources but not others. However, approaches based on do-calculus may encounter computational challenges as the number of variables increases and the causal graph grows in complexity. Consequently, there exists a need to reduce the size of such models while preserving the essential features. For this purpose, we propose pruning (removing unnecessary variables) and clustering (combining variables) as preprocessing operations for causal data fusion. We generalize earlier results on a single data source and derive conditions for applying pruning and clustering in the case of multiple data sources. We give sufficient conditions for inferring the identifiability or non-identifiability of a causal effect in a larger graph based on a smaller graph and show how to obtain the corresponding identifying functional for identifiable causal effects. Examples from epidemiology and social science demonstrate the use of the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15215v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Otto Tabell, Santtu Tikka, Juha Karvanen</dc:creator>
    </item>
    <item>
      <title>SplitWise Regression: Stepwise Modeling with Adaptive Dummy Encoding</title>
      <link>https://arxiv.org/abs/2505.15423</link>
      <description>arXiv:2505.15423v1 Announce Type: cross 
Abstract: Capturing nonlinear relationships without sacrificing interpretability remains a persistent challenge in regression modeling. We introduce SplitWise, a novel framework that enhances stepwise regression. It adaptively transforms numeric predictors into threshold-based binary features using shallow decision trees, but only when such transformations improve model fit, as assessed by the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). This approach preserves the transparency of linear models while flexibly capturing nonlinear effects. Implemented as a user-friendly R package, SplitWise is evaluated on both synthetic and real-world datasets. The results show that it consistently produces more parsimonious and generalizable models than traditional stepwise and penalized regression techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15423v1</guid>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcell T. Kurbucz, Nikolaos Tzivanakis, Nilufer Sari Aslam, Adam M. Sykulski</dc:creator>
    </item>
    <item>
      <title>Bayesian Ensembling: Insights from Online Optimization and Empirical Bayes</title>
      <link>https://arxiv.org/abs/2505.15638</link>
      <description>arXiv:2505.15638v1 Announce Type: cross 
Abstract: We revisit the classical problem of Bayesian ensembles and address the challenge of learning optimal combinations of Bayesian models in an online, continual learning setting. To this end, we reinterpret existing approaches such as Bayesian model averaging (BMA) and Bayesian stacking through a novel empirical Bayes lens, shedding new light on the limitations and pathologies of BMA. Further motivated by insights from online optimization, we propose Online Bayesian Stacking (OBS), a method that optimizes the log-score over predictive distributions to adaptively combine Bayesian models. A key contribution of our work is establishing a novel connection between OBS and portfolio selection, bridging Bayesian ensemble learning with a rich, well-studied theoretical framework that offers efficient algorithms and extensive regret analysis. We further clarify the relationship between OBS and online BMA, showing that they optimize related but distinct cost functions. Through theoretical analysis and empirical evaluation, we identify scenarios where OBS outperforms online BMA and provide principled guidance on when practitioners should prefer one approach over the other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15638v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Waxman, Fernando Llorente, Petar M. Djuri\'c</dc:creator>
    </item>
    <item>
      <title>Sequential Bayesian Registration for Functional Data</title>
      <link>https://arxiv.org/abs/2203.12005</link>
      <description>arXiv:2203.12005v2 Announce Type: replace 
Abstract: In many modern applications, discretely-observed data may be naturally understood as a set of functions. Functional data often exhibit two confounded sources of variability: amplitude (y-axis) and phase (x-axis). The extraction of amplitude and phase, a process known as registration, is essential in exploring the underlying structure of functional data in a variety of areas, from environmental monitoring to medical imaging. Critically, such data are often gathered sequentially with new functional observations arriving over time. Despite this, existing registration procedures do not sequentially update inference based on the new data, requiring model refitting. To address these challenges, we introduce a Bayesian framework for sequential registration of functional data, which updates statistical inference as new sets of functions are assimilated. This Bayesian model-based sequential learning approach utilizes sequential Monte Carlo sampling to recursively update the alignment of observed functions while accounting for associated uncertainty. Distributed computing significantly reduces computational cost relative to refitting the model using an iterative method such as Markov chain Monte Carlo on the full data. Simulation studies and comparisons reveal that the proposed approach performs well even when the target posterior distribution has a challenging structure. We apply the proposed method to three real datasets: (1) functions of annual drought intensity near Kaweah River in California, (2) annual sea surface salinity functions near Null Island, and (3) a sequence of repeated patterns in electrocardiogram signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.12005v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-025-10640-8</arxiv:DOI>
      <dc:creator>Yoonji Kim, Oksana A. Chkrebtii, Sebastian A. Kurtek</dc:creator>
    </item>
    <item>
      <title>hmmTMB: hidden Markov models with flexible covariate effects in R</title>
      <link>https://arxiv.org/abs/2211.14139</link>
      <description>arXiv:2211.14139v3 Announce Type: replace 
Abstract: Hidden Markov models (HMMs) are widely applied in studies where a discrete-valued process of interest is observed indirectly. They have for example been used to model behaviour from human and animal tracking data, disease status from medical data, and financial market volatility from stock prices. The model has two main sets of parameters: transition probabilities, which drive the latent state process, and observation parameters, which characterise the state-dependent distributions of observed variables. One particularly useful extension of HMMs is the inclusion of covariates on those parameters, to investigate the drivers of state transitions or to implement Markov-switching regression models. We present the new R package hmmTMB for HMM analyses, with flexible covariate models in both the hidden state and observation parameters. In particular, non-linear effects are implemented using penalised splines, including multiple univariate and multivariate splines, with automatic smoothness selection. The package allows for various random effect formulations (including random intercepts and slopes), to capture between-group heterogeneity. hmmTMB can be applied to multivariate observations, and it accommodates various types of response data, including continuous (bounded or not), discrete, and binary variables. Parameter constraints can be used to implement non-standard dependence structures, such as semi-Markov, higher-order Markov, and autoregressive models. Here, we summarise the relevant statistical methodology, we describe the structure of the package, and we present an example analysis of animal tracking data to showcase the workflow of the package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.14139v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Michelot</dc:creator>
    </item>
    <item>
      <title>Robust Nonparametric Regression for Compositional Data: the Simplicial--Real case</title>
      <link>https://arxiv.org/abs/2405.12924</link>
      <description>arXiv:2405.12924v2 Announce Type: replace 
Abstract: Statistical analysis on compositional data has gained a lot of attention due to their great potential of applications. A feature of these data is that they are multivariate vectors that lie in the simplex, that is, the components of each vector are positive and sum up a constant value. This fact poses a challenge to the analyst due to the internal dependency of the components which exhibit a spurious negative correlation. Since classical multivariate techniques are not appropriate in this scenario, it is necessary to endow the simplex of a suitable algebraic-geometrical structure, which is a starting point to develop adequate methodology and strategies to handle compositions. We centered our attention on regression problems with real responses and compositional covariates and we adopt a nonparametric approach due to the flexibility it provides. Aware of the potential damage that outliers may produce, we introduce a robust estimator in the framework of nonparametric regression for compositional data. The performance of the estimators is investigated by means of a numerical study where different contamination schemes are simulated. Through a real data analysis the advantages of using a robust procedure is illustrated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12924v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ana M. Bianco, Graciela Boente, Wenceslao Gonz\'alez--Manteiga, Francisco Gude Sampedro, Ana P\'erez--Gonz\'alez</dc:creator>
    </item>
    <item>
      <title>Mixture of Directed Graphical Models for Discrete Spatial Random Fields</title>
      <link>https://arxiv.org/abs/2406.15700</link>
      <description>arXiv:2406.15700v4 Announce Type: replace 
Abstract: Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). Notably, the MDGM is not proposed as an approximation to an MRF, but rather shares the same default, underlying graphical representation of spatial dependence as an MRF. However, in the case that the data generating mechanism of the latent spatial field is known to be an MRF, we find that posterior inferences under an MDGM prior better approximate the posterior of the model with a correctly specified MRF prior. We introduce the concept of compatibility to show how an undirected graph can be used as a template for the dependencies between areal units to create sets of DAGs which, as a collection, preserve the dependencies represented in the template undirected graph. Lastly, we compare highlighted classes of MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15700v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Brandon Carter, Catherine A. Calder</dc:creator>
    </item>
    <item>
      <title>Non-centering for discrete-valued state transition models: an application to ESBL-producing E. coli transmission in Malawi</title>
      <link>https://arxiv.org/abs/2504.11836</link>
      <description>arXiv:2504.11836v2 Announce Type: replace 
Abstract: Infectious disease transmission is often modelled by discrete-valued stochastic state-transition processes. Due to a lack of complete data, Bayesian inference for these models often relies on data-augmentation techniques. These techniques are often inefficient or time consuming to implement. We introduce a novel data-augmentation Markov chain Monte Carlo method for discrete-time individual-based epidemic models, which we call the Rippler algorithm. This method uses the transmission model in the proposal step of the Metropolis-Hastings algorithm, rather than in the accept-reject step. We test the Rippler algorithm on simulated data and apply it to data on extended-spectrum beta-lactamase (ESBL)-producing E. coli collected in Blantyre, Malawi. We compare the Rippler algorithm to two other commonly used Bayesian inference methods for partially observed epidemic data, and find that it has a good balance between mixing speed and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11836v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Neill, Rebecca Lester, Winnie Bakali, Gareth Roberts, Nicholas Feasey, Lloyd A. C. Chapman, Chris Jewell</dc:creator>
    </item>
    <item>
      <title>Covariate balancing estimation and model selection for difference-in-differences approach</title>
      <link>https://arxiv.org/abs/2504.13057</link>
      <description>arXiv:2504.13057v2 Announce Type: replace 
Abstract: Remarkable progress has been made in difference-in-differences (DID) approaches to causal inference that estimate the average effect of a treatment on the treated (ATT). Of these, the semiparametric DID (SDID) approach incorporates a propensity score analysis into the DID setup. Supposing that the ATT is a function of covariates, we estimate it by weighting the inverse of the propensity score. In this study, as one way to make the estimation robust to the propensity score modeling, we incorporate covariate balancing. Then, by attentively constructing the moment conditions used in the covariate balancing, we show that the proposed estimator is doubly robust. In addition to the estimation, we also address model selection. In practice, covariate selection is an essential task in statistical analysis, but even in the basic setting of the SDID approach, there are no reasonable information criteria. Here, we derive a model selection criterion as an asymptotically bias-corrected estimator of risk based on the loss function used in the SDID estimation. We show that a penalty term can be derived that is considerably different from almost twice the number of parameters that often appears in AIC-type information criteria. Numerical experiments show that the proposed method estimates the ATT more robustly compared with the method using propensity scores given by maximum likelihood estimation, and that the proposed criterion clearly reduces the risk targeted in the SDID approach in comparison with the intuitive generalization of the existing information criterion. In addition, real data analysis confirms that there is a large difference between the results of the proposed method and those of the existing method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13057v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takamichi Baba, Yoshiyuki Ninomiya</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v5 Announce Type: replace 
Abstract: We introduce a design-based framework for causal inference that accommodates random potential outcomes, thereby extending the classical Neyman-Rubin model in which outcomes are treated as fixed. Each unit's potential outcome is modelled as a structural mapping $\tilde{y}_i(z, \omega)$, where $z$ denotes the treatment assignment and \(\omega\) represents latent outcome-level randomness. Inspired by recent connections between design-based inference and the Riesz representation theorem, we embed potential outcomes in a Hilbert space and define treatment effects as linear functionals, yielding estimators constructed via their Riesz representers. This approach preserves the core identification logic of randomised assignment while enabling valid inference under stochastic outcome variation. We establish large-sample properties under local dependence and develop consistent variance estimators that remain valid under weaker structural assumptions, including partially known dependence. A simulation study illustrates the robustness and finite-sample behaviour of the estimators. Overall, the framework unifies design-based reasoning with stochastic outcome modelling, broadening the scope of causal inference in complex experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
    <item>
      <title>Accelerated inference for stochastic compartmental models with over-dispersed partial observations</title>
      <link>https://arxiv.org/abs/2505.06935</link>
      <description>arXiv:2505.06935v2 Announce Type: replace 
Abstract: An assumed density approximate likelihood is derived for a class of partially observed stochastic compartmental models which permit observational over-dispersion. This is achieved by treating time-varying reporting probabilities as latent variables and integrating them out using Laplace approximations within Poisson Approximate Likelihoods (LawPAL), resulting in a fast deterministic approximation to the marginal likelihood and filtering distributions. We derive an asymptotically exact filtering result in the large population regime, demonstrating the approximation's ability to recover latent disease states and reporting probabilities. Through simulations we: 1) demonstrate favorable behavior of the maximum approximate likelihood estimator in the large population and time horizon regime in terms of ground truth recovery; 2) demonstrate order of magnitude computational speed gains over a sequential Monte Carlo likelihood based approach, and explore the statistical compromises our approximation implicitly makes. We conclude by embedding our methodology within the probabilistic programming language Stan for automated Bayesian inference to develop a model of practical interest using data from the Covid-19 outbreak in Switzerland.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06935v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Whitehouse</dc:creator>
    </item>
    <item>
      <title>Optimal Transport on Categorical Data for Counterfactuals using Compositional Data and Dirichlet Transport</title>
      <link>https://arxiv.org/abs/2501.15549</link>
      <description>arXiv:2501.15549v2 Announce Type: replace-cross 
Abstract: Recently, optimal transport-based approaches have gained attention for deriving counterfactuals, e.g., to quantify algorithmic discrimination. However, in the general multivariate setting, these methods are often opaque and difficult to interpret. To address this, alternative methodologies have been proposed, using causal graphs combined with iterative quantile regressions (Ple\v{c}ko and Meinshausen (2020)) or sequential transport (Fernandes Machado et al. (2025)) to examine fairness at the individual level, often referred to as ``counterfactual fairness.'' Despite these advancements, transporting categorical variables remains a significant challenge in practical applications with real datasets. In this paper, we propose a novel approach to address this issue. Our method involves (1) converting categorical variables into compositional data and (2) transporting these compositions within the probabilistic simplex of $\mathbb{R}^d$. We demonstrate the applicability and effectiveness of this approach through an illustration on real-world data, and discuss limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15549v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 22 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Agathe Fernandes Machado, Arthur Charpentier, Ewen Gallic</dc:creator>
    </item>
  </channel>
</rss>
