<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 30 May 2025 01:42:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Improved Approximation Algorithms for Chromatic and Pseudometric-Weighted Correlation Clustering</title>
      <link>https://arxiv.org/abs/2505.21939</link>
      <description>arXiv:2505.21939v1 Announce Type: new 
Abstract: Correlation Clustering (CC) is a foundational problem in unsupervised learning that models binary similarity relations using labeled graphs. While classical CC has been widely studied, many real-world applications involve more nuanced relationships, either multi-class categorical interactions or varying confidence levels in edge labels. To address these, two natural generalizations have been proposed: Chromatic Correlation Clustering (CCC), which assigns semantic colors to edge labels, and pseudometric-weighted CC, which allows edge weights satisfying the triangle inequality. In this paper, we develop improved approximation algorithms for both settings. Our approach leverages LP-based pivoting techniques combined with problem-specific rounding functions. For the pseudometric-weighted correlation clustering problem, we present a tight $10/3$-approximation algorithm, matching the best possible bound achievable within the framework of standard LP relaxation combined with specialized rounding. For the Chromatic Correlation Clustering (CCC) problem, we improve the approximation ratio from the previous best of $2.5$ to $2.15$, and we establish a lower bound of $2.11$ within the same analytical framework, highlighting the near-optimality of our result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21939v1</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dahoon Lee, Chenglin Fan, Euiwoong Lee</dc:creator>
    </item>
    <item>
      <title>(Near)-Optimal Algorithms for Sparse Separable Convex Integer Programs</title>
      <link>https://arxiv.org/abs/2505.22212</link>
      <description>arXiv:2505.22212v1 Announce Type: new 
Abstract: We study the general integer programming (IP) problem of optimizing a separable convex function over the integer points of a polytope: $\min \{f(\mathbf{x}) \mid A\mathbf{x} = \mathbf{b}, \, \mathbf{l} \leq \mathbf{x} \leq \mathbf{u}, \, \mathbf{x} \in \mathbb{Z}^n\}$. The number of variables $n$ is a variable part of the input, and we consider the regime where the constraint matrix $A$ has small coefficients $\|A\|_\infty$ and small primal or dual treedepth $\mathrm{td}_P(A)$ or $\mathrm{td}_D(A)$, respectively. Equivalently, we consider block-structured matrices, in particular $n$-fold, tree-fold, $2$-stage and multi-stage matrices.
  We ask about the possibility of near-linear time algorithms in the general case of (non-linear) separable convex functions. The techniques of previous works for the linear case are inherently limited to it; in fact, no strongly-polynomial algorithm may exist due to a simple unconditional information-theoretic lower bound of $n \log \|\mathbf{u}-\mathbf{l}\|_\infty$, where $\mathbf{l}, \mathbf{u}$ are the vectors of lower and upper bounds. Our first result is that with parameters $\mathrm{td}_P(A)$ and $\|A\|_\infty$, this lower bound can be matched (up to dependency on the parameters). Second, with parameters $\mathrm{td}_D(A)$ and $\|A\|_\infty$, the situation is more involved, and we design an algorithm with time complexity $g(\mathrm{td}_D(A), \|A\|_\infty) n \log n \log \|\mathbf{u}-\mathbf{l}\|_\infty$ where $g$ is some computable function. We conjecture that a stronger lower bound is possible in this regime, and our algorithm is in fact optimal.
  Our algorithms combine ideas from scaling, proximity, and sensitivity of integer programs, together with a new dynamic data structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22212v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christoph Hunkenschr\"oder, Martin Kouteck\'y, Asaf Levin, Tung Anh Vu</dc:creator>
    </item>
    <item>
      <title>Finding $d$-Cuts in Probe $H$-Free Graphs</title>
      <link>https://arxiv.org/abs/2505.22351</link>
      <description>arXiv:2505.22351v1 Announce Type: new 
Abstract: For an integer $d\geq 1$, the $d$-Cut problem is that of deciding whether a graph has an edge cut in which each vertex is adjacent to at most $d$ vertices on the opposite side of the cut. The $1$-Cut problem is the well-known Matching Cut problem. The $d$-Cut problem has been extensively studied for $H$-free graphs. We extend these results to the probe graph model, where we do not know all the edges of the input graph. For a graph $H$, a partitioned probe $H$-free graph $(G,P,N)$ consists of a graph $G=(V,E)$, together with a set $P\subseteq V$ of probes and an independent set $N=V\setminus P$ of non-probes such that we can change $G$ into an $H$-free graph by adding zero or more edges between vertices in $N$. For every graph $H$ and every integer $d\geq 1$, we completely determine the complexity of $d$-Cut on partitioned probe $H$-free graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22351v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konrad K. Dabrowski, Tala Eagling-Vose, Matthew Johnson, Giacomo Paesani, Dani\"el Paulusma</dc:creator>
    </item>
    <item>
      <title>Exact Algorithms and Lower Bounds for Forming Coalitions of Constrained Maximum Size</title>
      <link>https://arxiv.org/abs/2505.22384</link>
      <description>arXiv:2505.22384v1 Announce Type: new 
Abstract: Imagine we want to split a group of agents into teams in the most \emph{efficient} way, considering that each agent has their own preferences about their teammates. This scenario is modeled by the extensively studied \textsc{Coalition Formation} problem. Here, we study a version of this problem where each team must additionally be of bounded size.
  We conduct a systematic algorithmic study, providing several intractability results as well as multiple exact algorithms that scale well as the input grows (FPT), which could prove useful in practice.
  Our main contribution is an algorithm that deals efficiently with tree-like structures (bounded \emph{treewidth}) for ``small'' teams. We complement this result by proving that our algorithm is asymptotically optimal. Particularly, there can be no algorithm that vastly outperforms the one we present, under reasonable theoretical assumptions, even when considering star-like structures (bounded \emph{vertex cover number}).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22384v1</guid>
      <category>cs.DS</category>
      <category>cs.AI</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Foivos Fioravantes, Harmender Gahlawat, Nikolaos Melissinos</dc:creator>
    </item>
    <item>
      <title>Faster Convolutions: Yates and Strassen Revisited</title>
      <link>https://arxiv.org/abs/2505.22410</link>
      <description>arXiv:2505.22410v1 Announce Type: new 
Abstract: Given two vectors $u,v \in \mathbb{Q}^D$ over a finite domain $D$ and a function $f : D\times D\to D$, the convolution problem asks to compute the vector $w \in \mathbb{Q}^D$ whose entries are defined by $w(d) = \sum_{\substack{x,y \in D \\ f(x,y)=d}} u(x)v(y).$ In parameterized and exponential-time algorithms, convolutions on product domains are particularly prominent: Here, a finite domain $B$ and a function $h : B \times B \to B$ are fixed, and convolution is done over the product domain $D = B^k$, using the function $h^k :D \times D\to D$ that applies $h$ coordinate-wise to its input tuples.
  We present a new perspective on product-domain convolutions through multilinear algebra. This viewpoint streamlines the presentation and analysis of existing algorithms, such as those by van Rooij et al. (ESA 2009). Moreover, using established results from the theory of fast matrix multiplication, we derive improved $O^\ast(|B|^{2\omega/3 \cdot k}) = O(|D|^{1.582})$ time algorithms, improving upon previous upper bounds by Esmer et al. (Algorithmica 86(1), 2024) of the form $c^k |B|^{2k}$ for $c &lt; 1$. Using the setup described in this note, Strassen's asymptotic rank conjecture from algebraic complexity theory would imply quasi-linear $|D|^{1+o(1)}$ time algorithms. This conjecture has recently gained attention in the algorithms community. (Bj\"orklund-Kaski and Pratt, STOC 2024, Bj\"orklund et al., SODA 2025)
  Our paper is intended as a self-contained exposition for an algorithms audience, and it includes all essential mathematical prerequisites with explicit coordinate-based notation. In particular, we assume no knowledge in abstract algebra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22410v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cornelius Brand, Radu Curticapean, Baitian Li, Kevin Pratt</dc:creator>
    </item>
    <item>
      <title>Efficient Diffusion Models for Symmetric Manifolds</title>
      <link>https://arxiv.org/abs/2505.21640</link>
      <description>arXiv:2505.21640v1 Announce Type: cross 
Abstract: We introduce a framework for designing efficient diffusion models for $d$-dimensional symmetric-space Riemannian manifolds, including the torus, sphere, special orthogonal group and unitary group. Existing manifold diffusion models often depend on heat kernels, which lack closed-form expressions and require either $d$ gradient evaluations or exponential-in-$d$ arithmetic operations per training step. We introduce a new diffusion model for symmetric manifolds with a spatially-varying covariance, allowing us to leverage a projection of Euclidean Brownian motion to bypass heat kernel computations. Our training algorithm minimizes a novel efficient objective derived via Ito's Lemma, allowing each step to run in $O(1)$ gradient evaluations and nearly-linear-in-$d$ ($O(d^{1.19})$) arithmetic operations, reducing the gap between diffusions on symmetric manifolds and Euclidean space. Manifold symmetries ensure the diffusion satisfies an "average-case" Lipschitz condition, enabling accurate and efficient sample generation. Empirically, our model outperforms prior methods in training speed and improves sample quality on synthetic datasets on the torus, special orthogonal group, and unitary group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21640v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oren Mangoubi, Neil He, Nisheeth K. Vishnoi</dc:creator>
    </item>
    <item>
      <title>Adaptive Frontier Exploration on Graphs with Applications to Network-Based Disease Testing</title>
      <link>https://arxiv.org/abs/2505.21671</link>
      <description>arXiv:2505.21671v1 Announce Type: cross 
Abstract: We study a sequential decision-making problem on a $n$-node graph $G$ where each node has an unknown label from a finite set $\mathbf{\Sigma}$, drawn from a joint distribution $P$ that is Markov with respect to $G$. At each step, selecting a node reveals its label and yields a label-dependent reward. The goal is to adaptively choose nodes to maximize expected accumulated discounted rewards. We impose a frontier exploration constraint, where actions are limited to neighbors of previously selected nodes, reflecting practical constraints in settings such as contact tracing and robotic exploration. We design a Gittins index-based policy that applies to general graphs and is provably optimal when $G$ is a forest. Our implementation runs in $O(n^2 \cdot |\mathbf{\Sigma}|^2)$ time while using $O(n \cdot |\mathbf{\Sigma}|^2)$ oracle calls to $P$ and $O(n^2 \cdot |\mathbf{\Sigma}|)$ space. Experiments on synthetic and real-world graphs show that our method consistently outperforms natural baselines, including in non-tree, budget-limited, and undiscounted settings. For example, in HIV testing simulations on real-world sexual interaction networks, our policy detects nearly all positive cases with only half the population tested, substantially outperforming other baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21671v1</guid>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davin Choo, Yuqi Pan, Tonghan Wang, Milind Tambe, Alastair van Heerden, Cheryl Johnson</dc:creator>
    </item>
    <item>
      <title>Counting Small Induced Subgraphs: Scorpions Are Easy but Not Trivial</title>
      <link>https://arxiv.org/abs/2505.22300</link>
      <description>arXiv:2505.22300v1 Announce Type: cross 
Abstract: We consider the parameterized problem $\#$IndSub$(\Phi)$ for fixed graph properties $\Phi$: Given a graph $G$ and an integer $k$, this problem asks to count the number of induced $k$-vertex subgraphs satisfying $\Phi$. D\"orfler et al. [Algorithmica 2022] and Roth et al. [SICOMP 2024] conjectured that $\#$IndSub$(\Phi)$ is $\#$W[1]-hard for all non-meager properties $\Phi$, i.e., properties that are nontrivial for infinitely many $k$. This conjecture has been confirmed for several restricted types of properties, including all hereditary properties [STOC 2022] and all edge-monotone properties [STOC 2024].
  In this work, we refute this conjecture by showing that scorpion graphs, certain $k$-vertex graphs which were introduced more than 50 years ago in the context of the evasiveness conjecture, can be counted in time $O(n^4)$ for all $k$. A simple variant of this construction results in graph properties that achieve arbitrary intermediate complexity assuming ETH.
  We formulate an updated conjecture on the complexity of $\#$IndSub$(\Phi)$ that correctly captures the complexity status of scorpions and related constructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22300v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Curticapean, Simon D\"oring, Daniel Neuen</dc:creator>
    </item>
    <item>
      <title>Private Lossless Multiple Release</title>
      <link>https://arxiv.org/abs/2505.22449</link>
      <description>arXiv:2505.22449v1 Announce Type: cross 
Abstract: Koufogiannis et al. (2016) showed a $\textit{gradual release}$ result for Laplace noise-based differentially private mechanisms: given an $\varepsilon$-DP release, a new release with privacy parameter $\varepsilon' &gt; \varepsilon$ can be computed such that the combined privacy loss of both releases is at most $\varepsilon'$ and the distribution of the latter is the same as a single release with parameter $\varepsilon'$. They also showed gradual release techniques for Gaussian noise, later also explored by Whitehouse et al. (2022).
  In this paper, we consider a more general $\textit{multiple release}$ setting in which analysts hold private releases with different privacy parameters corresponding to different access/trust levels. These releases are determined one by one, with privacy parameters in arbitrary order. A multiple release is $\textit{lossless}$ if having access to a subset $S$ of the releases has the same privacy guarantee as the least private release in $S$, and each release has the same distribution as a single release with the same privacy parameter. Our main result is that lossless multiple release is possible for a large class of additive noise mechanisms. For the Gaussian mechanism we give a simple method for lossless multiple release with a short, self-contained analysis that does not require knowledge of the mathematics of Brownian motion. We also present lossless multiple release for the Laplace and Poisson mechanisms. Finally, we consider how to efficiently do gradual release of sparse histograms, and present a mechanism with running time independent of the number of dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22449v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joel Daniel Andersson, Lukas Retschmeier, Boel Nelson, Rasmus Pagh</dc:creator>
    </item>
    <item>
      <title>Fully Packed and Ready to Go: High-Density, Rearrangement-Free, Grid-Based Storage and Retrieval</title>
      <link>https://arxiv.org/abs/2505.22497</link>
      <description>arXiv:2505.22497v1 Announce Type: cross 
Abstract: Grid-based storage systems with uniformly shaped loads (e.g., containers, pallets, totes) are commonplace in logistics, industrial, and transportation domains. A key performance metric for such systems is the maximization of space utilization, which requires some loads to be placed behind or below others, preventing direct access to them. Consequently, dense storage settings bring up the challenge of determining how to place loads while minimizing costly rearrangement efforts necessary during retrieval. This paper considers the setting involving an inbound phase, during which loads arrive, followed by an outbound phase, during which loads depart. The setting is prevalent in distribution centers, automated parking garages, and container ports. In both phases, minimizing the number of rearrangement actions results in more optimal (e.g., fast, energy-efficient, etc.) operations. In contrast to previous work focusing on stack-based systems, this effort examines the case where loads can be freely moved along the grid, e.g., by a mobile robot, expanding the range of possible motions. We establish that for a range of scenarios, such as having limited prior knowledge of the loads' arrival sequences or grids with a narrow opening, a (best possible) rearrangement-free solution always exists, including when the loads fill the grid to its capacity. In particular, when the sequences are fully known, we establish an intriguing characterization showing that rearrangement can always be avoided if and only if the open side of the grid (used to access the storage) is at least 3 cells wide. We further discuss useful practical implications of our solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22497v1</guid>
      <category>cs.RO</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tzvika Geft, Kostas Bekris, Jingjin Yu</dc:creator>
    </item>
    <item>
      <title>The APX-hardness of the Traveling Tournament Problem</title>
      <link>https://arxiv.org/abs/2308.14124</link>
      <description>arXiv:2308.14124v2 Announce Type: replace 
Abstract: The Traveling Tournament Problem (TTP-$k$) is a well-known benchmark problem in sports scheduling, which asks us to design a double round-robin schedule such that each pair of teams plays one game in each other's home venue, no pair of teams plays each other on two consecutive days, each team plays at most $k$ consecutive home games or away games, and the total traveling distance of all the $n$ teams is minimized. TTP-$k$ allows a polynomial-time approximation scheme when $k=2$ and becomes APX-hard when $k\geq n-1$. In this paper, we reduce the gap by showing that TTP-$k$ is APX-hard for any fixed $k\geq3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14124v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Zhao, Mingyu Xiao</dc:creator>
    </item>
    <item>
      <title>Learning Safe Strategies for Value Maximizing Buyers in Uniform Price Auctions</title>
      <link>https://arxiv.org/abs/2406.03674</link>
      <description>arXiv:2406.03674v2 Announce Type: replace 
Abstract: We study the bidding problem in repeated uniform price multi-unit auctions from the perspective of a single value-maximizing buyer who aims to maximize their cumulative value over $T$ rounds while adhering to return-on-investment (RoI) constraints in each round. Buyers adopt $m$-uniform bidding format, where they submit $m$ bid-quantity pairs $(b_i, q_i)$ to demand $q_i$ units at bid $b_i$. We introduce safe bidding strategies as those that satisfy RoI constraints in every auction, regardless of competing bids. We show that these strategies depend only on the valuation curve of the bidder, and the bidder can focus on a finite subset of this class without loss of generality. While the number of strategies in this subset is exponential in $m$, we develop a polynomial-time algorithm to learn the optimal safe strategy that achieves sublinear regret in the online setting, where regret is measured against a clairvoyant benchmark that knows the competing bids a priori and selects a fixed hindsight optimal safe strategy. We then evaluate the performance of safe strategies against a clairvoyant that selects the optimal strategy from a richer class of strategies in the online setting. In this scenario, we compute the richness ratio, $\alpha\in(0, 1]$ for the class of strategies chosen by the clairvoyant and show that our algorithm, designed to learn safe strategies, achieves $\alpha$-approximate sublinear regret against these stronger benchmarks. Experiments on semi-synthetic data from real-world auctions show that safe strategies substantially outperform the derived theoretical bounds, making them quite appealing in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03674v2</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Negin Golrezaei, Sourav Sahoo</dc:creator>
    </item>
    <item>
      <title>Spanning Trees Minimizing Branching Costs</title>
      <link>https://arxiv.org/abs/2407.10571</link>
      <description>arXiv:2407.10571v2 Announce Type: replace 
Abstract: The Minimum Branch Vertices Spanning Tree problem aims to find a spanning tree $T$ in a given graph $G$ with the fewest branch vertices, defined as vertices with a degree three or more in $T$. This problem, known to be NP-hard, has attracted significant attention due to its importance in network design and optimization. Extensive research has been conducted on the algorithmic and combinatorial aspects of this problem, with recent studies delving into its fixed-parameter tractability.
  In this paper, we focus primarily on the parameter modular-width. We demonstrate that finding a spanning tree with the minimum number of branch vertices is Fixed-Parameter Tractable (FPT) when considered with respect to modular-width. Additionally, in cases where each vertex in the input graph has an associated cost for serving as a branch vertex, we prove that the problem of finding a spanning tree with the minimum branch cost (i.e., minimizing the sum of the costs of branch vertices) is FPT with respect to neighborhood diversity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10571v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luisa Gargano, Adele A. Rescigno</dc:creator>
    </item>
    <item>
      <title>Fast Geographic Routing in Fixed-Growth Graphs</title>
      <link>https://arxiv.org/abs/2502.03663</link>
      <description>arXiv:2502.03663v2 Announce Type: replace 
Abstract: In the 1960s, the social scientist Stanley Milgram performed his famous "small-world" experiments where he found that people in the US who are far apart geographically are nevertheless connected by remarkably short chains of acquaintances. Since then, there has been considerable work to design networks that accurately model the phenomenon that Milgram observed. One well-known approach was Barab{\'a}si and Albert's preferential attachment model, which has small diameter yet lacks an algorithm that can efficiently find those short connections between nodes. Jon Kleinberg, in contrast, proposed a small-world graph formed from an $n \times n$ lattice that guarantees that greedy routing can navigate between any two nodes in $\mathcal{O}(\log^2 n)$ time with high probability. Further work by Goodrich and Ozel and by Gila, Goodrich, and Ozel present a hybrid technique that combines elements from these previous approaches to improve greedy routing time to $\mathcal{O}(\log n)$ hops. These are important theoretical results, but we believe that their reliance on the square lattice limits their application in the real world. In this work, we generalize the model of Gila, Ozel, and Goodrich to any class of what we call fixed-growth graphs of dimensionality $\alpha$, a subset of bounded-growth graphs introduced in several prior papers. We prove tight bounds for greedy routing and diameter in these graphs, both in expectation and with high probability. We then apply our model to the U.S. road network to show that by modeling the network as a fixed-growth graph rather than as a lattice, we are able to improve greedy routing performance over all 50 states. We also show empirically that the optimal clustering exponent for the U.S. road network is much better modeled by the dimensionality of the network $\alpha$ than by the network's size, as was conjectured in a previous work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03663v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-92935-9_10</arxiv:DOI>
      <arxiv:journal_reference>Lect.Notes Comput.Sci. 15680 (2025) 151-167 Lect.Notes Comput.Sci. 15680 (2025) 151-167 Lect.Notes Comput.Sci. 15680 (2025) 151-167</arxiv:journal_reference>
      <dc:creator>Ofek Gila, Michael T. Goodrich, Abraham M. Illickan, Vinesh Sridhar</dc:creator>
    </item>
    <item>
      <title>Optimal Approximations for the Requirement Cut Problem on Sparse Graph Classes</title>
      <link>https://arxiv.org/abs/2505.21433</link>
      <description>arXiv:2505.21433v2 Announce Type: replace 
Abstract: We study the Requirement Cut problem, a generalization of numerous classical graph partitioning problems including Multicut, Multiway Cut, $k$-Cut, and Steiner Multicut among others. Given a graph with edge costs, terminal groups $(S_1, ..., S_g)$ and integer requirements $(r_1,... , r_g)$; the goal is to compute a minimum-cost edge cut that separates each group $S_i$ into at least $r_i$ connected components. Despite many efforts, the best known approximation for Requirement Cut yields a double-logarithmic $O(\log(g).\log(n))$ approximation ratio as it relies on embedding general graphs into trees and solving the tree instance.
  In this paper, we explore two largely unstudied structural parameters in order to obtain single-logarithmic approximation ratios: (1) the number of minimal Steiner trees in the instance, which in particular is upper-bounded by the number of spanning trees of the graphs multiplied by $g$, and (2) the depth of series-parallel graphs. Specifically, we show that if the number of minimal Steiner trees is polynomial in $n$, then a simple LP-rounding algorithm yields an $O(\log n)$-approximation, and if the graph is series-parallel with a constant depth then a refined analysis of a known probabilistic embedding yields a $O(depth.\log(g))$-approximation on series-parallel graphs of bounded depth. Both results extend the known class of graphs that have a single-logarithmic approximation ratio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21433v2</guid>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nadym Mallek, Kirill Simonov</dc:creator>
    </item>
    <item>
      <title>Learning in Stackelberg Games with Non-myopic Agents</title>
      <link>https://arxiv.org/abs/2208.09407</link>
      <description>arXiv:2208.09407v3 Announce Type: replace-cross 
Abstract: We study Stackelberg games where a principal repeatedly interacts with a non-myopic long-lived agent, without knowing the agent's payoff function. Although learning in Stackelberg games is well-understood when the agent is myopic, dealing with non-myopic agents poses additional complications. In particular, non-myopic agents may strategize and select actions that are inferior in the present in order to mislead the principal's learning algorithm and obtain better outcomes in the future.
  We provide a general framework that reduces learning in presence of non-myopic agents to robust bandit optimization in the presence of myopic agents. Through the design and analysis of minimally reactive bandit algorithms, our reduction trades off the statistical efficiency of the principal's learning algorithm against its effectiveness in inducing near-best-responses. We apply this framework to Stackelberg security games (SSGs), pricing with unknown demand curve, general finite Stackelberg games, and strategic classification. In each setting, we characterize the type and impact of misspecifications present in near-best responses and develop a learning algorithm robust to such misspecifications.
  On the way, we improve the state-of-the-art query complexity of learning in SSGs with $n$ targets from $O(n^3)$ to a near-optimal $\widetilde{O}(n)$ by uncovering a fundamental structural property of these games. The latter result is of independent interest beyond learning with non-myopic agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.09407v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nika Haghtalab, Thodoris Lykouris, Sloan Nietert, Alexander Wei</dc:creator>
    </item>
    <item>
      <title>Gibbs state preparation for commuting Hamiltonian: Mapping to classical Gibbs sampling</title>
      <link>https://arxiv.org/abs/2410.04909</link>
      <description>arXiv:2410.04909v3 Announce Type: replace-cross 
Abstract: Gibbs state preparation, or Gibbs sampling, is a key computational technique extensively used in physics, statistics, and other scientific fields. Recent efforts for designing fast mixing Gibbs samplers for quantum Hamiltonians have largely focused on commuting local Hamiltonians (CLHs), a non-trivial subclass of Hamiltonians which include highly entangled systems such as the Toric code and quantum double model. Most previous Gibbs samplers relied on simulating the Davies generator, which is a Lindbladian associated with the thermalization process in nature.
  Instead of using the Davies generator, we design a different Gibbs sampler for various CLHs by giving a reduction to classical Hamiltonians, in the sense that one can efficiently prepare the Gibbs state for some CLH $H$ on a quantum computer as long as one can efficiently do classical Gibbs sampling for the corresponding classical Hamiltonian $H^{(c)}$. We demonstrate that our Gibbs sampler is able to replicate state-of-the-art results as well as prepare the Gibbs state in regimes which were previously unknown, such as the low temperature region, as long as there exists fast mixing Gibbs samplers for the corresponding classical Hamiltonians. Our reductions are as follows.
  - If $H$ is a 2-local qudit CLH, then $H^{(c)}$ is a 2-local qudit classical Hamiltonian.
  - If $H$ is a 4-local qubit CLH on 2D lattice and there are no classical qubits, then $H^{(c)}$ is a 2-local qudit classical Hamiltonian on a planar graph. As an example, our algorithm can prepare the Gibbs state for the (defected) Toric code at any non-zero temperature in $\mathcal O(n^2)$ time.
  - If $H$ is a 4-local qubit CLH on 2D lattice and there are classical qubits, assuming that quantum terms are uniformly correctable, then $H^{(c)}$ is a constant-local classical Hamiltonian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04909v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Thu, 29 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeongwoo Hwang, Jiaqing Jiang</dc:creator>
    </item>
  </channel>
</rss>
