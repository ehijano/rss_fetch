<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 01:38:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Distributed Asynchronous Primal-Dual Optimization for Supply-Chain Networks</title>
      <link>https://arxiv.org/abs/2506.08024</link>
      <description>arXiv:2506.08024v1 Announce Type: new 
Abstract: Distributed supply-chain optimization demands algorithms that can cope with unreliable communication, unbounded messaging delays, and geographically dispersed agents while still guaranteeing convergence with provable rates. In this work, we introduce DAPD-SCO (Distributed Asynchronous Primal-Dual Optimization for Supply-Chain Networks), a fully asynchronous primal-dual scheme for network flow allocation over directed acyclic supply-chain graphs. Each edge agent independently updates its local flow by projected gradient descent, and each retailer agent independently updates its dual multiplier by projected gradient ascent, using only potentially stale information whose delays can grow sublinearly. Under standard convexity and Slater's conditions and without any global synchronization or bounded-delay assumptions, we prove almost-sure convergence to a saddle point and establish an ergodic duality gap rate of $O(K^{-1/2})$, matching centralized lower bounds. Our analysis uses a Lyapunov-based decomposition that isolates delay-induced errors and handles time-varying communication topologies, bounded noise, and slowly drifting cost or capacity parameters. Extensive simulations on realistic three-tier networks show that DAPD-SCO outperforms synchronous primal-dual methods, Asynchronous Distributed Decision-Making (ADDM), and gradient-push, achieving faster convergence, lower communication overhead, and robust performance under packet loss and high staleness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08024v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laksh Patel, Neel Shanbhag</dc:creator>
    </item>
    <item>
      <title>Mean-Field-Type Game Theory with Rosenblatt Noise</title>
      <link>https://arxiv.org/abs/2506.08025</link>
      <description>arXiv:2506.08025v1 Announce Type: new 
Abstract: We study the integration of Rosenblatt noise into stochastic systems, control theory, and mean-field-type game theory, addressing the limitations of traditional Gaussian and Markovian models. Empirical evidence from various domains, including water demand, e-commerce, power grid operations, wireless channels, and agricultural supply chains, demonstrates the prevalence of non-Gaussian characteristics such as skews, heavy tails and strong long-range dependencies. The Rosenblatt process, a non-Gaussian non-Markovian, self-similar process, offers a baseline framework for capturing some the behaviors observed in real data. We develop novel stochastic calculus formulas for Rosenblatt processes, apply these to dynamical systems, and analyze optimal control problems, revealing the suboptimality of traditional noise approximation methods. We extend game-theoretic analysis to environments driven by Rosenblatt noise, establishing conditions for saddle-point equilibria in zero-sum games and identifying state-feedback Nash equilibria in non-zero-sum games. Our findings underscore the importance of incorporating non-Gaussian noise into predictive analytics and control strategies, enhancing the accuracy and robustness of models in real-world applications. These findings represent a significant advancement in mean-field-type game theory with variance-awareness, offering new insights and tools for managing interactive systems influenced by Rosenblatt noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08025v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hamidou Tembine, Tyrone E. Duncan, Bozenna Pasik-Duncan</dc:creator>
    </item>
    <item>
      <title>MOSS: Multi-Objective Optimization for Stable Rule Sets</title>
      <link>https://arxiv.org/abs/2506.08030</link>
      <description>arXiv:2506.08030v1 Announce Type: new 
Abstract: We present MOSS, a multi-objective optimization framework for constructing stable sets of decision rules. MOSS incorporates three important criteria for interpretability: sparsity, accuracy, and stability, into a single multi-objective optimization framework. Importantly, MOSS allows a practitioner to rapidly evaluate the trade-off between accuracy and stability in sparse rule sets in order to select an appropriate model. We develop a specialized cutting plane algorithm in our framework to rapidly compute the Pareto frontier between these two objectives, and our algorithm scales to problem instances beyond the capabilities of commercial optimization solvers. Our experiments show that MOSS outperforms state-of-the-art rule ensembles in terms of both predictive performance and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08030v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian Liu, Rahul Mazumder</dc:creator>
    </item>
    <item>
      <title>Stochastic Krasnosel skii-Mann Iterations in Banach Spaces with Bregman Distances</title>
      <link>https://arxiv.org/abs/2506.08031</link>
      <description>arXiv:2506.08031v1 Announce Type: new 
Abstract: We propose a generalization of the stochastic Krasnoselskil-Mann $(SKM)$ algorithm to reflexive Banach spaces endowed with Bregman distances. Under standard martingale-difference noise assumptions in the dual space and mild conditions on the distance-generating function, we establish almost-sure convergence to a fixed point and derive non-asymptotic residual bounds that depend on the uniform convexity modulus of the generating function. Extensions to adaptive Bregman geometries and robust noise models are also discussed. Numerical experiments on entropy-regularized reinforcement learning and mirror-descent illustrate the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08031v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeed Hashemi Sababe, Ehsan Lotfali Ghasab</dc:creator>
    </item>
    <item>
      <title>From Local Updates to Global Balance: A Framework for Distributed Matrix Scaling</title>
      <link>https://arxiv.org/abs/2506.08035</link>
      <description>arXiv:2506.08035v1 Announce Type: new 
Abstract: This paper investigates matrix scaling processes in the context of local normalization algorithms and their convergence behavior. Starting from the classical Sinkhorn algorithm, the authors introduce a generalization where only a single row or column is normalized at each step, without restrictions on the update order. They extend Birkhoff's theorem to characterize the convergence properties of these algorithms, especially when the normalization sequence is arbitrary. A novel application is explored in the form of a Decentralized Random Walk (DRW) on directed graphs, where agents modify edge weights locally without global knowledge or memory. The paper shows that such local updates lead to convergence towards a doubly stochastic matrix, ensuring a uniform stationary distribution across graph vertices. These results not only deepen the theoretical understanding of matrix scaling but also open avenues for distributed and agent-based models in networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08035v1</guid>
      <category>math.OC</category>
      <category>cs.DC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Giovanni Naldi</dc:creator>
    </item>
    <item>
      <title>Continuous Policy and Value Iteration for Stochastic Control Problems and Its Convergence</title>
      <link>https://arxiv.org/abs/2506.08121</link>
      <description>arXiv:2506.08121v1 Announce Type: new 
Abstract: We introduce a continuous policy-value iteration algorithm where the approximations of the value function of a stochastic control problem and the optimal control are simultaneously updated through Langevin-type dynamics. This framework applies to both the entropy-regularized relaxed control problems and the classical control problems, with infinite horizon. We establish policy improvement and demonstrate convergence to the optimal control under the monotonicity condition of the Hamiltonian. By utilizing Langevin-type stochastic differential equations for continuous updates along the policy iteration direction, our approach enables the use of distribution sampling and non-convex learning techniques in machine learning to optimize the value function and identify the optimal control simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08121v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qi Feng, Gu Wang</dc:creator>
    </item>
    <item>
      <title>An In-situ Solid Fuel Ramjet Thrust Monitoring and Regulation Framework Using Neural Networks and Adaptive Control</title>
      <link>https://arxiv.org/abs/2506.08157</link>
      <description>arXiv:2506.08157v1 Announce Type: new 
Abstract: Controlling the complex combustion dynamics within solid fuel ramjets (SFRJs) remains a critical challenge limiting deployment at scale. This paper proposes the use of a neural network model to process in-situ measurements for monitoring and regulating SFRJ thrust with a learning-based adaptive controller. A neural network is trained to estimate thrust from synthetic data generated by a feed-forward quasi-one-dimensional SFRJ model with variable inlet control. An online learning controller based on retrospective cost optimization is integrated with the quasi-one-dimensional SFRJ model to regulate the thrust. Sensitivity studies are conducted on both the neural network and adaptive controller to identify optimal hyperparameters. Numerical simulation results indicate that the combined neural network and learning control framework can effectively regulate the thrust produced by the SFRJ model using limited in-situ data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08157v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan DeBoskey, Parham Oveissi, Venkat Narayanaswamy, Ankit Goel</dc:creator>
    </item>
    <item>
      <title>A Cubic Regularization Method for Multiobjective Optimization</title>
      <link>https://arxiv.org/abs/2506.08181</link>
      <description>arXiv:2506.08181v1 Announce Type: new 
Abstract: This work introduces a new cubic regularization method for nonconvex unconstrained multiobjective optimization problems. At each iteration of the method, a model associated with the cubic regularization of each component of the objective function is minimized. This model allows approximations for the first- and second-order derivatives, which must satisfy suitable error conditions. One interesting feature of the proposed algorithm is that the regularization parameter of the model and the accuracy of the derivative approximations are jointly adjusted using a nonmonotone line search criterion. Implementations of the method, where derivative information is computed using finite difference strategies, are discussed. It is shown that, under the assumption that the Hessians of the objectives are globally Lipschitz continuous, the method requires at most $\mathcal{O}(C\epsilon^{-3/2})$ iterations to generate an $\epsilon$-approximate Pareto critical. In particular, if the first- and second-order derivative information is computed using finite differences based solely on function values, the method requires at most $\mathcal{O}(n^{1-\beta}\epsilon^{-3/2})$ iterations, corresponding to $\mathcal{O}\left(mn^{3-\beta}\varepsilon^{-\frac{3}{2}}\right)$ function evaluations, where \(n\) is the dimension of the domain of the objective function, $m$ is the number of objectives, and $\beta \in [0,1]$ is a constant associated with the stepsize used in the finite-difference approximation. We further discuss the global convergence and local convergence rate of the method. Specifically, under the local convexity assumption, we show that the method achieves superlinear convergence when the first derivative is computed exactly, and quadratic convergence when both first- and second-order derivatives are exact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08181v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas S. Gon\c{c}alves, Max L. N. Gon\c{c}alves, Jefferson G. Melo</dc:creator>
    </item>
    <item>
      <title>Solving Convex-Concave Problems with $\tilde{\mathcal{O}}(\epsilon^{-4/7})$ Second-Order Oracle Complexity</title>
      <link>https://arxiv.org/abs/2506.08362</link>
      <description>arXiv:2506.08362v1 Announce Type: new 
Abstract: Previous algorithms can solve convex-concave minimax problems $\min_{x \in \mathcal{X}} \max_{y \in \mathcal{Y}} f(x,y)$ with $\mathcal{O}(\epsilon^{-2/3})$ second-order oracle calls using Newton-type methods. This result has been speculated to be optimal because the upper bound is achieved by a natural generalization of the optimal first-order method. In this work, we show an improved upper bound of $\tilde{\mathcal{O}}(\epsilon^{-4/7})$ by generalizing the optimal second-order method for convex optimization to solve the convex-concave minimax problem. We further apply a similar technique to lazy Hessian algorithms and show that our proposed algorithm can also be seen as a second-order ``Catalyst'' framework (Lin et al., JMLR 2018) that could accelerate any globally convergent algorithms for solving minimax problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08362v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesi Chen, Chengchang Liu, Luo Luo, Jingzhao Zhang</dc:creator>
    </item>
    <item>
      <title>Composite Optimization with Indicator Functions: Stationary Duality and a Semismooth Newton Method</title>
      <link>https://arxiv.org/abs/2506.08374</link>
      <description>arXiv:2506.08374v1 Announce Type: new 
Abstract: Indicator functions of taking values of zero or one are essential to numerous applications in machine learning and statistics. The corresponding primal optimization model has been researched in several recent works. However, its dual problem is a more challenging topic that has not been well addressed. One possible reason is that the Fenchel conjugate of any indicator function is finite only at the origin. This work aims to explore the dual optimization for the sum of a strongly convex function and a composite term with indicator functions on positive intervals. For the first time, a dual problem is constructed by extending the classic conjugate subgradient property to the indicator function. This extension further helps us establish the equivalence between the primal and dual solutions. The dual problem turns out to be a sparse optimization with a $\ell_0$ regularizer and a nonnegative constraint. The proximal operator of the sparse regularizer is used to identify a dual subspace to implement gradient and/or semismooth Newton iteration with low computational complexity. This gives rise to a dual Newton-type method with both global convergence and local superlinear (or quadratic) convergence rate under mild conditions. Finally, when applied to AUC maximization and sparse multi-label classification, our dual Newton method demonstrates satisfactory performance on computational speed and accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08374v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Penghe Zhang, Naihua Xiu, Houduo Qi</dc:creator>
    </item>
    <item>
      <title>Sharper Convergence Rates for Nonconvex Optimisation via Reduction Mappings</title>
      <link>https://arxiv.org/abs/2506.08428</link>
      <description>arXiv:2506.08428v1 Announce Type: new 
Abstract: Many high-dimensional optimisation problems exhibit rich geometric structures in their set of minimisers, often forming smooth manifolds due to over-parametrisation or symmetries. When this structure is known, at least locally, it can be exploited through reduction mappings that reparametrise part of the parameter space to lie on the solution manifold. These reductions naturally arise from inner optimisation problems and effectively remove redundant directions, yielding a lower-dimensional objective. In this work, we introduce a general framework to understand how such reductions influence the optimisation landscape. We show that well-designed reduction mappings improve curvature properties of the objective, leading to better-conditioned problems and theoretically faster convergence for gradient-based methods. Our analysis unifies a range of scenarios where structural information at optimality is leveraged to accelerate convergence, offering a principled explanation for the empirical gains observed in such optimisation algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08428v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evan Markou, Thalaiyasingam Ajanthan, Stephen Gould</dc:creator>
    </item>
    <item>
      <title>BESS Participation Planning for Provision of Grid Services in Energy and Regulation Markets</title>
      <link>https://arxiv.org/abs/2506.08472</link>
      <description>arXiv:2506.08472v1 Announce Type: new 
Abstract: This paper presents a stochastic optimization model for planning the participation of battery energy storage systems (BESSs) in energy and regulation markets. The proposed model quantifies and compares the business value of single and multi-market BESS services by accounting for their bid submission and acceptance procedures, pricing mechanisms and revenue streams, and penalty payments and degradation costs. By modelling the price uncertainties using market price scenarios (MPSs) and considering representative days for different price seasonalities, the model outputs the percentage of hours each market service can be targeted to maximize profits. This helps in quantifying the impact of operational and market requirements of different services on choice of markets for BESSs. It also helps in determining the approximate costs and revenues that may be accrued by the BESS owners by choosing combinations of available services under different price conditions. The model thus overcomes the key limitations of previous studies that were mainly conducted from a controller design viewpoint and were thus more focused on the operational control of BESSs. The proposed model is generalizable and extendable to BESS service provision in multiple markets of different regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08472v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeenat Hameed, Chresten Traeholt</dc:creator>
    </item>
    <item>
      <title>Complexity Analysis of Convex Majorization Schemes for Nonconvex Constrained Optimization</title>
      <link>https://arxiv.org/abs/2506.08506</link>
      <description>arXiv:2506.08506v1 Announce Type: new 
Abstract: We introduce and study various algorithms for solving nonconvex minimization with inequality constraints, based on the construction of convex surrogate envelopes that majorize the objective and the constraints. In the case where the objective and constraint functions are gradient H\"{o}lderian continuous, the surrogate functions can be readily constructed and the solution method can be efficiently implemented. The surrogate envelopes are extended to the settings where the second-order information is available, and the convex subproblems are further represented by Dikin ellipsoids using the self-concordance of the convex surrogate constraints. Iteration complexities have been developed for both convex and nonconvex optimization models. The numerical results show promising potential of the proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08506v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nuozhou Wang, Junyu Zhang, Shuzhong Zhang</dc:creator>
    </item>
    <item>
      <title>Gradient flow in the kernel learning problem</title>
      <link>https://arxiv.org/abs/2506.08550</link>
      <description>arXiv:2506.08550v1 Announce Type: new 
Abstract: This is a sequel to our paper `On the kernel learning problem'. We identify a canonical choice of Riemannian gradient flow, to find the stationary points in the kernel learning problem. In the presence of Gaussian noise variables, this flow enjoys the remarkable property of having a continuous family of Lyapunov functionals, and the interpretation is the automatic reduction of noise.
  PS. We include an extensive discussion in the postcript explaining the comparison with the 2-layer neural networks. Readers looking for additional motivations are encouraged to read the postscript immediately following the introduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08550v1</guid>
      <category>math.OC</category>
      <category>math.DG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Li, Feng Ruan</dc:creator>
    </item>
    <item>
      <title>Optimization over Sparse Support-Preserving Sets: Two-Step Projection with Global Optimality Guarantees</title>
      <link>https://arxiv.org/abs/2506.08558</link>
      <description>arXiv:2506.08558v2 Announce Type: new 
Abstract: In sparse optimization, enforcing hard constraints using the $\ell_0$ pseudo-norm offers advantages like controlled sparsity compared to convex relaxations. However, many real-world applications demand not only sparsity constraints but also some extra constraints. While prior algorithms have been developed to address this complex scenario with mixed combinatorial and convex constraints, they typically require the closed form projection onto the mixed constraints which might not exist, and/or only provide local guarantees of convergence which is different from the global guarantees commonly sought in sparse optimization. To fill this gap, in this paper, we study the problem of sparse optimization with extra support-preserving constraints commonly encountered in the literature. We present a new variant of iterative hard-thresholding algorithm equipped with a two-step consecutive projection operator customized for these mixed constraints, serving as a simple alternative to the Euclidean projection onto the mixed constraint. By introducing a novel trade-off between sparsity relaxation and sub-optimality, we provide global guarantees in objective value for the output of our algorithm, in the deterministic, stochastic, and zeroth-order settings, under the conventional restricted strong-convexity/smoothness assumptions. As a fundamental contribution in proof techniques, we develop a novel extension of the classic three-point lemma to the considered two-step non-convex projection operator, which allows us to analyze the convergence in objective value in an elegant way that has not been possible with existing techniques. In the zeroth-order case, such technique also improves upon the state-of-the-art result from de Vazelhes et. al. (2022), even in the case without additional constraints, by allowing us to remove a non-vanishing system error present in their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08558v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William de Vazelhes, Xiao-Tong Yuan, Bin Gu</dc:creator>
    </item>
    <item>
      <title>SDP bounds on the stability number via ADMM and intermediate levels of the Lasserre hierarchy</title>
      <link>https://arxiv.org/abs/2506.08648</link>
      <description>arXiv:2506.08648v1 Announce Type: new 
Abstract: We consider the Lasserre hierarchy for computing bounds on the stability number of graphs. The semidefinite programs (SDPs) arising from this hierarchy involve large matrix variables and many linear constraints, which makes them difficult to solve using interior-point methods. We propose solving these SDPs using the alternating direction method of multipliers (ADMM). When the second level of the Lasserre hierarchy for a given graph is intractable for the ADMM, we consider an intermediate-level relaxation of the hierarchy. To warm-start the ADMM, we use an optimal solution from the first level of the Lasserre hierarchy, which is equivalent to the well-known Lov\'asz theta function. Additionally, we use this solution to determine which degree two monomials to add in the Lasserre hierarchy relaxation to obtain an intermediate level between 1 and 2. Computational results demonstrate that our approach yields strong bounds on the stability number, which are computable within reasonable running times. We provide the best-known bounds on the stability number of various graphs from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08648v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lennart Sinjorgo, Renata Sotirov, Juan C. Vera</dc:creator>
    </item>
    <item>
      <title>Characterizations of Faces of Convex Sets in Infinite-dimensional Vector Spaces</title>
      <link>https://arxiv.org/abs/2506.08742</link>
      <description>arXiv:2506.08742v1 Announce Type: new 
Abstract: In the paper three different characterizations of faces of convex sets, belonging to infinite-dimensional real vector spaces, are presented. The first one is formulated in the terms of generalized semispaces, the second -- in the terms of compatible complete (total) preorders, and the third -- in the terms of step-affine functions. All three characterization are equivalent each other and extend to infinite-dimensional vector spaces the lexicographical characterization of faces established in finite-dimensional settings by Martinez-Legaz J.-E. (Acta Mathematica Vietnamica. 1997. Vol. 22, No.~1, P. 207--211).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08742v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Valentin V. Gorokhovik</dc:creator>
    </item>
    <item>
      <title>On the Batch Size Selection in Stochastic Gradient Methods Using No-Replacement Sampling</title>
      <link>https://arxiv.org/abs/2506.08758</link>
      <description>arXiv:2506.08758v1 Announce Type: new 
Abstract: Recent stochastic gradient methods that have appeared in the literature base their efficiency and global convergence properties on a suitable control of the variance of the gradient batch estimate. This control is typically achieved by dynamically increasing the batch size during the iterations of the algorithm. However, in the existing methods the statistical analysis often relies on sampling with replacement. This particular batch selection appears unrealistic in practice. In this paper, we consider a more realistic approach to batch size selection based on sampling without replacement. The consequent statistical analysis is compared to that of sampling with replacement. The new batch size selection method, while still ensuring global convergence, provides a more accurate representation of the variance reduction observed in practice, leading to a smoother and more efficient batch size update scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08758v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Boresta, Alberto De Santis, Stefano Lucidi</dc:creator>
    </item>
    <item>
      <title>An Efficient Augmented Lagrangian Method for Dynamic Optimal Transport on Surfaces Based on Second-Order Cone Programming</title>
      <link>https://arxiv.org/abs/2506.08988</link>
      <description>arXiv:2506.08988v1 Announce Type: new 
Abstract: This paper proposes an efficient numerical optimization approach for solving dynamic optimal transport (DOT) problems on general smooth surfaces, computing both the quadratic Wasserstein distance and the associated transportation path. Building on the convex DOT model of Benamou and Brenier, we first properly reformulate its dual problem, discretized on a triangular mesh for space together with a staggered grid for time, to a linear second-order cone programming. Then the resulting finite-dimensional convex optimization problem is solved via an inexact semi-proximal augmented Lagrangian method with a highly efficient numerical implementation, and the algorithm is guaranteed to converge to a Karush-Kuhn-Tucker solution without imposing any additional assumptions. Finally, we implement the proposed methodology as an open-source software package. The effectiveness, robustness, and computational efficiency of the software are demonstrated through extensive numerical experiments across diverse datasets, where it consistently outperforms state-of-the-art solvers by several times in speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08988v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Chen, Youyicun Lin, Yuxuan Zhou</dc:creator>
    </item>
    <item>
      <title>Superlinear Drift in Consensus-Based Optimization with Condensation Phenomena</title>
      <link>https://arxiv.org/abs/2506.09001</link>
      <description>arXiv:2506.09001v1 Announce Type: new 
Abstract: Consensus-based optimization (CBO) is a class of metaheuristic algorithms designed for global optimization problems. In the many-particle limit, classical CBO dynamics can be rigorously connected to mean-field equations that ensure convergence toward global minimizers under suitable conditions. In this work, we draw inspiration from recent extensions of the Kaniadakis--Quarati model for indistinguishable bosons to develop a novel CBO method governed by a system of SDEs with superlinear drift and nonconstant diffusion. The resulting mean-field formulation in one dimension exhibits condensation-like phenomena, including finite-time blow-up and loss of $L^2$-regularity. To avoid the curse of dimensionality a marginal based formulation which permits to leverage the one-dimensional results to multiple dimensions is proposed. We support our approach with numerical experiments that highlight both its consistency and potential performance improvements compared to classical CBO methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09001v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jonathan Franceschi, Lorenzo Pareschi, Mattia Zanella</dc:creator>
    </item>
    <item>
      <title>Characterization of Hilbertizable spaces via convex functions</title>
      <link>https://arxiv.org/abs/2506.04686</link>
      <description>arXiv:2506.04686v1 Announce Type: cross 
Abstract: We show that the existence of a strongly convex function with a Lipschitz derivative on a Banach space already implies that the space is isomorphic to a Hilbert space. Similarly, if both a function and its convex conjugate are $C^2$ then the underlying space is also isomorphic to a Hilbert space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04686v1</guid>
      <category>math.FA</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicolas Borchard, Gerd Wachsmuth</dc:creator>
    </item>
    <item>
      <title>Modified K-means Algorithm with Local Optimality Guarantees</title>
      <link>https://arxiv.org/abs/2506.06990</link>
      <description>arXiv:2506.06990v2 Announce Type: cross 
Abstract: The K-means algorithm is one of the most widely studied clustering algorithms in machine learning. While extensive research has focused on its ability to achieve a globally optimal solution, there still lacks a rigorous analysis of its local optimality guarantees. In this paper, we first present conditions under which the K-means algorithm converges to a locally optimal solution. Based on this, we propose simple modifications to the K-means algorithm which ensure local optimality in both the continuous and discrete sense, with the same computational complexity as the original K-means algorithm. As the dissimilarity measure, we consider a general Bregman divergence, which is an extension of the squared Euclidean distance often used in the K-means algorithm. Numerical experiments confirm that the K-means algorithm does not always find a locally optimal solution in practice, while our proposed methods provide improved locally optimal solutions with reduced clustering loss. Our code is available at https://github.com/lmingyi/LO-K-means.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06990v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyi Li, Michael R. Metel, Akiko Takeda</dc:creator>
    </item>
    <item>
      <title>Massive parallelization of projection-based depths</title>
      <link>https://arxiv.org/abs/2506.08262</link>
      <description>arXiv:2506.08262v1 Announce Type: cross 
Abstract: This article introduces a novel methodology for the massive parallelization of projection-based depths, addressing the computational challenges of data depth in high-dimensional spaces. We propose an algorithmic framework based on Refined Random Search (RRS) and demonstrate significant speedup (up to 7,000 times faster) on GPUs. Empirical results on synthetic data show improved precision and reduced runtime, making the method suitable for large-scale applications. The RRS algorithm (and other depth functions) are available in the Python-library data-depth (https://data-depth.github.io/) with ready-to-use tools to implement and to build upon this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08262v1</guid>
      <category>stat.CO</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leonardo Leone, Pavlo Mozharovskyi, David Bounie</dc:creator>
    </item>
    <item>
      <title>Online Learning-guided Learning Rate Adaptation via Gradient Alignment</title>
      <link>https://arxiv.org/abs/2506.08419</link>
      <description>arXiv:2506.08419v1 Announce Type: cross 
Abstract: The performance of an optimizer on large-scale deep learning models depends critically on fine-tuning the learning rate, often requiring an extensive grid search over base learning rates, schedules, and other hyperparameters. In this paper, we propose a principled framework called GALA (Gradient Alignment-based Learning rate Adaptation), which dynamically adjusts the learning rate by tracking the alignment between consecutive gradients and using a local curvature estimate. Guided by the convergence analysis, we formulate the problem of selecting the learning rate as a one-dimensional online learning problem. When paired with an online learning algorithm such as Follow-the-Regularized-Leader, our method produces a flexible, adaptive learning rate schedule that tends to increase when consecutive gradients are aligned and decrease otherwise. We establish a data-adaptive convergence rate for normalized SGD equipped with GALA in the smooth, nonconvex setting. Empirically, common optimizers such as SGD and Adam, when augmented with GALA, demonstrate robust performance across a wide range of initial learning rates and perform competitively without the need for tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08419v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruichen Jiang, Ali Kavis, Aryan Mokhtari</dc:creator>
    </item>
    <item>
      <title>Robust Evolutionary Multi-Objective Network Architecture Search for Reinforcement Learning (EMNAS-RL)</title>
      <link>https://arxiv.org/abs/2506.08533</link>
      <description>arXiv:2506.08533v1 Announce Type: cross 
Abstract: This paper introduces Evolutionary Multi-Objective Network Architecture Search (EMNAS) for the first time to optimize neural network architectures in large-scale Reinforcement Learning (RL) for Autonomous Driving (AD). EMNAS uses genetic algorithms to automate network design, tailored to enhance rewards and reduce model size without compromising performance. Additionally, parallelization techniques are employed to accelerate the search, and teacher-student methodologies are implemented to ensure scalable optimization. This research underscores the potential of transfer learning as a robust framework for optimizing performance across iterative learning processes by effectively leveraging knowledge from earlier generations to enhance learning efficiency and stability in subsequent generations. Experimental results demonstrate that tailored EMNAS outperforms manually designed models, achieving higher rewards with fewer parameters. The findings of these strategies contribute positively to EMNAS for RL in autonomous driving, advancing the field toward better-performing networks suitable for real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08533v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nihal Acharya Adde, Alexandra Gianzina, Hanno Gottschalk, Andreas Ebert</dc:creator>
    </item>
    <item>
      <title>sparseGeoHOPCA: A Geometric Solution to Sparse Higher-Order PCA Without Covariance Estimation</title>
      <link>https://arxiv.org/abs/2506.08670</link>
      <description>arXiv:2506.08670v1 Announce Type: cross 
Abstract: We propose sparseGeoHOPCA, a novel framework for sparse higher-order principal component analysis (SHOPCA) that introduces a geometric perspective to high-dimensional tensor decomposition. By unfolding the input tensor along each mode and reformulating the resulting subproblems as structured binary linear optimization problems, our method transforms the original nonconvex sparse objective into a tractable geometric form. This eliminates the need for explicit covariance estimation and iterative deflation, enabling significant gains in both computational efficiency and interpretability, particularly in high-dimensional and unbalanced data scenarios. We theoretically establish the equivalence between the geometric subproblems and the original SHOPCA formulation, and derive worst-case approximation error bounds based on classical PCA residuals, providing data-dependent performance guarantees. The proposed algorithm achieves a total computational complexity of $O\left(\sum_{n=1}^{N} (k_n^3 + J_n k_n^2)\right)$, which scales linearly with tensor size. Extensive experiments demonstrate that sparseGeoHOPCA accurately recovers sparse supports in synthetic settings, preserves classification performance under 10$\times$ compression, and achieves high-quality image reconstruction on ImageNet, highlighting its robustness and versatility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08670v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Renjie Xu, Chong Wu, Maolin Che, Zhuoheng Ran, Yimin Wei, Hong Yan</dc:creator>
    </item>
    <item>
      <title>Introduction to Nonlinear Spectral Analysis</title>
      <link>https://arxiv.org/abs/2506.08754</link>
      <description>arXiv:2506.08754v1 Announce Type: cross 
Abstract: These notes are meant as an introduction to the theory of nonlinear spectral theory. We will discuss the variational form of nonlninear eigenvalue problems and the corresponding non-linear Euler--Lagrange equations, as well as connections with gradient flows. For the latter ones, we will give precise conditions for finite time extinction and discuss convergence rates. We will use this theory to study asymptotic behaviour of nonlinear PDEs and present applications in $L^\infty$ variational problems. Finally we will discuss numerical methods for solving gradient flows and computing nonlinear eigenfunctions based on a nonlinear power method. Our main tools are convex analysis and calculus of variations, necessary background on which will be provided. It is expected that the reader is familiar with Hilbert spaces; familiarity with Banach spaces is beneficial but not strictly necessary. The notes are based on the lectures taught by the authors at the universities of Bonn and Cambridge in 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08754v1</guid>
      <category>math.SP</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Leon Bungert, Yury Korolev</dc:creator>
    </item>
    <item>
      <title>Optimal hedging of an informed broker facing many traders</title>
      <link>https://arxiv.org/abs/2506.08992</link>
      <description>arXiv:2506.08992v1 Announce Type: cross 
Abstract: This paper investigates the optimal hedging strategies of an informed broker interacting with multiple traders in a financial market. We develop a theoretical framework in which the broker, possessing exclusive information about the drift of the asset's price, engages with traders whose trading activities impact the market price. Using a mean-field game approach, we derive the equilibrium strategies for both the broker and the traders, illustrating the intricate dynamics of their interactions. The broker's optimal strategy involves a Stackelberg equilibrium, where the broker leads and the traders follow. Our analysis also addresses the mean field limit of finite-player models and shows the convergence to the mean-field solution as the number of traders becomes large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08992v1</guid>
      <category>q-fin.TR</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Bergault, Pierre Cardaliaguet, Wenbin Yan</dc:creator>
    </item>
    <item>
      <title>A bilevel approach for compensation and routing decisions in last-mile delivery</title>
      <link>https://arxiv.org/abs/2304.09170</link>
      <description>arXiv:2304.09170v4 Announce Type: replace 
Abstract: In last-mile delivery logistics, peer-to-peer logistic platforms play an important role in connecting senders, customers, and independent carriers to fulfill delivery requests. Since the carriers are not under the platform's control, the platform has to anticipate their reactions, while deciding how to allocate the delivery operations. Indeed, carriers' decisions largely affect the platform's revenue. In this paper, we model this problem using bilevel programming. At the upper level, the platform decides how to assign the orders to the carriers; at the lower level, each carrier solves a profitable tour problem to determine which offered requests to accept, based on her own profit maximization. Possibly, the platform can influence carriers' decisions by determining also the compensation paid for each accepted request. The two considered settings result in two different formulations: the bilevel profitable tour problem with fixed compensation margins and with margin decisions, respectively. For each of them, we propose single-level reformulations and alternative formulations where the lower-level routing variables are projected out. A branch-and-cut algorithm is proposed to solve the bilevel models, with a tailored warm-start heuristic used to speed up the solution process. Extensive computational tests are performed to compare the proposed formulations and analyze solution characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09170v4</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/trsc.2023.0129</arxiv:DOI>
      <arxiv:journal_reference>Transportation Science, 58(5), pages 1076-1100 (2024)</arxiv:journal_reference>
      <dc:creator>Martina Cerulli, Claudia Archetti, Elena Fernandez, Ivana Ljubic</dc:creator>
    </item>
    <item>
      <title>Contest for system observability as an infinitely repeated game</title>
      <link>https://arxiv.org/abs/2306.13570</link>
      <description>arXiv:2306.13570v3 Announce Type: replace 
Abstract: This paper studies a system security problem in the context of observability based on a two-person noncooperative infinitely repeated game. Both the attacker and the defender have means to modify the dimension of the unobservable subspace, which is set as the value function. Utilizing tools from geometric control, we construct the best response sets considering one-step and two-step optimality respectively to maximize or minimize the value function. We establish a unified necessary-and-sufficient condition for Nash equilibrium that holds for both one-step and two-step optimizations. Our analysis further uncovers two evolutionary patterns, lock and loop modes, and shows an asymmetry between defense and attack. The defender can lock the game into equilibrium, whereas the attacker can disrupt it by sacrificing short-term utility for longer-term advantage. Six representative numerical examples corroborate the theoretical results and highlight the complexity of possible game outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13570v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyue Xu, Panpan Zhou, Lin Wang, Zhixin Liu, Xiaoming Hu</dc:creator>
    </item>
    <item>
      <title>Convex semi-infinite programming algorithms with inexact separation oracles</title>
      <link>https://arxiv.org/abs/2307.14181</link>
      <description>arXiv:2307.14181v3 Announce Type: replace 
Abstract: Solving convex Semi-Infinite Programming (SIP) problems is challenging when the separation problem, i.e., the problem of finding the most violated constraint, is computationally hard. We propose to tackle this difficulty by solving the separation problem approximately, i.e., by using an inexact oracle. Our focus lies in two algorithms for SIP, namely the Cutting-Planes (CP) and the Inner-Outer Approximation (IOA) algorithms. We prove the CP convergence rate to be in O(1/k), where k is the number of calls to the limited-accuracy oracle, if the objective function is strongly convex. Compared to the CP algorithm, the advantage of the IOA algorithm is the feasibility of its iterates. In the case of a semi-infinite program with Quadratically Constrained Quadratic Programming separation problem, we prove the convergence of the IOA algorithm toward an optimal solution of the SIP problem despite the oracle's inexactness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.14181v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11590-024-02148-3</arxiv:DOI>
      <arxiv:journal_reference>Optimization Letters, 19(3), Pages 437-462 (2025)</arxiv:journal_reference>
      <dc:creator>Antoine Oustry, Martina Cerulli</dc:creator>
    </item>
    <item>
      <title>Advanced Kernel Search approach for the MST Problem with conflicts involving affinity detection and initial solution construction</title>
      <link>https://arxiv.org/abs/2401.02222</link>
      <description>arXiv:2401.02222v2 Announce Type: replace 
Abstract: The Minimum Spanning Tree Problem with Conflicts consists in finding the minimum conflict-free spanning tree of a graph, i.e., the spanning tree of minimum cost, including no pairs of edges that are in conflict. In this paper, we solve this problem using an enhanced Kernel Search method, which iteratively solves refined problem restrictions. Our approach addresses two central open questions in the kernel search literature: (1) how to determine the affinity between variables to ensure that the restricted problem contains variables that are as compatible as possible, meaning they are more likely to appear together in a feasible solution, and (2) how to construct an initial feasible solution quickly. To this end, we integrate the computation of independent sets from the conflict graph within the algorithm to detect affinities and effectively manage conflicts. Furthermore, we heuristically construct an initial starting point, significantly accelerating the computational process. Although our methodology is designed for MSTC, its principles could be extended to other combinatorial optimization problems with conflicts. Experimental results on benchmark instances demonstrate the efficiency and competitiveness of our approach compared to existing methods in the literature, achieving 17 new best-known values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02222v2</guid>
      <category>math.OC</category>
      <category>cs.CR</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Carrabs, Martina Cerulli, Domenico Serra</dc:creator>
    </item>
    <item>
      <title>Data-Driven Discovery of PDEs via the Adjoint Method</title>
      <link>https://arxiv.org/abs/2401.17177</link>
      <description>arXiv:2401.17177v4 Announce Type: replace 
Abstract: In this work, we present an adjoint-based method for discovering the underlying governing partial differential equations (PDEs) given data. The idea is to consider a parameterized PDE in a general form and formulate a PDE-constrained optimization problem aimed at minimizing the error of the PDE solution from data. Using variational calculus, we obtain an evolution equation for the Lagrange multipliers (adjoint equations) allowing us to compute the gradient of the objective function with respect to the parameters of PDEs given data in a straightforward manner. In particular, we consider a family of parameterized PDEs encompassing linear, nonlinear, and spatial derivative candidate terms, and elegantly derive the corresponding adjoint equations. We show the efficacy of the proposed approach in identifying the form of the PDE up to machine accuracy, enabling the accurate discovery of PDEs from data. We also compare its performance with the famous PDE Functional Identification of Nonlinear Dynamics method known as PDE-FIND (Rudy et al., 2017), on both smooth and noisy data sets. Even though the proposed adjoint method relies on forward/backward solvers, it outperforms PDE-FIND for large data sets thanks to the analytic expressions for gradients of the cost function with respect to each PDE parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17177v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohsen Sadr, Tony Tohme, Kamal Youcef-Toumi</dc:creator>
    </item>
    <item>
      <title>Aircraft Conflict Resolution: A Benchmark Generator</title>
      <link>https://arxiv.org/abs/2405.12836</link>
      <description>arXiv:2405.12836v2 Announce Type: replace 
Abstract: Aircraft conflict resolution is one of the major tasks of computer-aided air traffic management and represents a challenging optimization problem. Many models and methods have been proposed to assist trajectory regulation to avoid conflicts. However, the question of testing the different mathematical optimization approaches against each other is still open. Standard benchmarks include unrealistic scenarios in which all the flights move toward a common point or completely random generated instances. There is a lack of a common set of test instances that allows comparison of the available methods under a variety of heterogeneous and representative scenarios. We present a flight deconfliction benchmark generator that allows the user to choose between (i) different predefined scenario inspired by existing benchmarks in the literature; (ii) pseudo-random traffic meeting certain congestion measurements; (iii) and randomly generated traffic. The proposed setting can account for different levels of difficulty in the deconfliction of the aircraft and allows to explore and compare the real limitations of optimization approaches for aircraft conflict resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12836v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1287/ijoc.2022.1265</arxiv:DOI>
      <arxiv:journal_reference>INFORMS Journal on Computing, Volume 35, Number 2, Pages 274-285 (2023)</arxiv:journal_reference>
      <dc:creator>Mercedes Pelegrin, Martina Cerulli</dc:creator>
    </item>
    <item>
      <title>Distributionally and Adversarially Robust Logistic Regression via Intersecting Wasserstein Balls</title>
      <link>https://arxiv.org/abs/2407.13625</link>
      <description>arXiv:2407.13625v4 Announce Type: replace 
Abstract: Adversarially robust optimization (ARO) has emerged as the *de facto* standard for training models that hedge against adversarial attacks in the test stage. While these models are robust against adversarial attacks, they tend to suffer severely from overfitting. To address this issue, some successful methods replace the empirical distribution in the training stage with alternatives including *(i)* a worst-case distribution residing in an ambiguity set, resulting in a distributionally robust (DR) counterpart of ARO; *(ii)* a mixture of the empirical distribution with a distribution induced by an auxiliary (*e.g.*, synthetic, external, out-of-domain) dataset. Inspired by the former, we study the Wasserstein DR counterpart of ARO for logistic regression and show it admits a tractable convex optimization reformulation. Adopting the latter setting, we revise the DR approach by intersecting its ambiguity set with another ambiguity set built using the auxiliary dataset, which offers a significant improvement whenever the Wasserstein distance between the data generating and auxiliary distributions can be estimated. We study the underlying optimization problem, develop efficient solution algorithms, and demonstrate that the proposed method outperforms benchmark approaches on standard datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13625v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aras Selvi, Eleonora Kreacic, Mohsen Ghassemi, Vamsi Potluru, Tucker Balch, Manuela Veloso</dc:creator>
    </item>
    <item>
      <title>Domain decomposition for integer optimal control with total variation regularization</title>
      <link>https://arxiv.org/abs/2410.15672</link>
      <description>arXiv:2410.15672v2 Announce Type: replace 
Abstract: Total variation integer optimal control problems admit solutions and necessary optimality conditions via geometric variational analysis. In spite of the existence of said solutions, algorithms which solve the discretized objective suffer from high numerical cost associated with the combinatorial nature of integer programming. Hence, such methods are often limited to small- and medium-sized problems.
  We propose a globally convergent, coordinate descent-inspired algorithm that allows tractable subproblem solutions restricted to a partition of the domain. Our decomposition method solves relatively small trust-region subproblems that modify the control variable on a subdomain only. Given nontrivial subdomain overlap, we prove that a global first-order necessary optimality condition is equivalent to a first-order necessary optimality condition per subdomain. We additionally show that sufficient decrease is achieved on a single subdomain by way of a trust-region subproblem solver using geometric measure theoretic arguments, which we integrate with a greedy patch selection to prove convergence of our algorithm. We demonstrate the practicality of our algorithm on a benchmark large-scale, PDE-constrained integer optimal control problem, and find that our method is faster than the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15672v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Baraldi, Paul Manns</dc:creator>
    </item>
    <item>
      <title>Convex Data-Driven Contraction With Riemannian Metrics</title>
      <link>https://arxiv.org/abs/2412.20283</link>
      <description>arXiv:2412.20283v3 Announce Type: replace 
Abstract: The growing complexity of dynamical systems and advances in data collection necessitates robust data-driven control strategies without explicit system identification and robust synthesis. Data-driven stability has been explored in linear and nonlinear systems, often by turning the problem into a linear or positive semidefinite program. This paper focuses on a new emerging property called contractivity, which refers to the exponential convergence of all system trajectories toward each other under a specified metric. Data-driven closed loop contractivity has been studied for the case of the 2-norm and assuming nonlinearities are Lipschitz bounded in subsets of n dimensional euclidean space. We extend the analysis by considering Riemannian metrics for polynomial dynamics. The key to our derivation is to leverage the convex criteria for closed-loop contraction and duality results to efficiently check infinite dimensional membership constraints. Numerical examples demonstrate the effectiveness of the proposed method for both linear and nonlinear systems, highlighting its potential for robust data-driven contraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20283v3</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andreas Oliveira, Jian Zheng, Mario Sznaier</dc:creator>
    </item>
    <item>
      <title>Secant Line Search for Frank-Wolfe Algorithms</title>
      <link>https://arxiv.org/abs/2501.18775</link>
      <description>arXiv:2501.18775v2 Announce Type: replace 
Abstract: We present a new step-size strategy based on the secant method for Frank-Wolfe algorithms. This strategy, which requires mild assumptions about the function under consideration, can be applied to any Frank-Wolfe algorithm. It is as effective as full line search and, in particular, allows for adapting to the local smoothness of the function, such as in Pedregosa et al 2018, but comes with a significantly reduced computational cost, leading to higher effective rates of convergence. We provide theoretical guarantees and demonstrate the effectiveness of the strategy through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18775v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deborah Hendrych, Mathieu Besan\c{c}on, David Mart\'inez-Rubio, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Interior Point Differential Dynamic Programming, Redux</title>
      <link>https://arxiv.org/abs/2504.08278</link>
      <description>arXiv:2504.08278v2 Announce Type: replace 
Abstract: We present IPDDP2, a structure-exploiting algorithm for solving discrete-time, finite-horizon optimal control problems (OCPs) with nonlinear constraints. Inequality constraints are handled using a primal-dual interior point formulation and step acceptance for equality constraints follows a line-search filter approach. The iterates of the algorithm are derived under the Differential Dynamic Programming (DDP) framework. A proof of local quadratic convergence of the IPDDP2 iterates is provided. Our numerical experiments evaluate IPDDP2 on over 500 OCPs derived from five different classes of robotic motion planning problems, three of which are contact-implicit trajectory optimisation problems. IPDDP2 demonstrates improvements in robustness against existing constrained DDP algorithms for contact-implicit planning, while being significantly faster than general-purpose solver IPOPT. We provide a full implementation of IPDDP2 in the Julia programming language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08278v2</guid>
      <category>math.OC</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ming Xu, Stephen Gould, Iman Shames</dc:creator>
    </item>
    <item>
      <title>The Weierstrass necessary condition for fractional calculus of variations</title>
      <link>https://arxiv.org/abs/2506.06741</link>
      <description>arXiv:2506.06741v2 Announce Type: replace 
Abstract: In this paper, we study problems of minimization of a functional depending on the fractional Caputo derivative of order $0&lt;\alpha \leq 1$ and the fractional Riemann- Liouville integral of order $\beta &gt; 0$ at fixed endpoints. A fractional analogue of the Du Bois-Reymond lemma is proved, and the Euler-Lagrange conditions are proved for the simplest problem of fractional variational calculus with fixed ends and for the fractional isoperimetric problem. An approach is proposed to obtain the necessary first-order conditions for the strong and weak extrema, and the necessary optimality conditions are obtained. From these necessary conditions, as a consequence, we obtain the Weierstrass condition and its local modification.
  It should be noted that some papers in the literature claim that the standard proof of the Legendre condition in the classical case $\alpha=1$ cannot be adapted to the fractional case $0&lt; \alpha &lt;1$ with final constraints. Despite this, we prove the Legendre conditions by the standard classical method via the Weierstrass condition. In addition, the necessary Weierstrass-Erdmann conditions at the corner points are obtained. Examples are provided to illustrate the significance of the main results obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06741v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shakir Sh. Yusubov, Shikhi Sh. Yusubov, Elimhan N. Mahmudov</dc:creator>
    </item>
    <item>
      <title>General Loss Functions Lead to (Approximate) Interpolation in High Dimensions</title>
      <link>https://arxiv.org/abs/2303.07475</link>
      <description>arXiv:2303.07475v2 Announce Type: replace-cross 
Abstract: We provide a unified framework that applies to a general family of convex losses across binary and multiclass settings in the overparameterized regime to approximately characterize the implicit bias of gradient descent in closed form. Specifically, we show that the implicit bias is approximated (but not exactly equal to) the minimum-norm interpolation in high dimensions, which arises from training on the squared loss. In contrast to prior work, which was tailored to exponentially-tailed losses and used the intermediate support-vector-machine formulation, our framework directly builds on the primal-dual analysis of Ji and Telgarsky (2021), allowing us to provide new approximate equivalences for general convex losses through a novel sensitivity analysis. Our framework also recovers existing exact equivalence results for exponentially-tailed losses across binary and multiclass settings. Finally, we provide evidence for the tightness of our techniques and use our results to demonstrate the effect of certain loss functions designed for out-of-distribution problems on the closed-form solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07475v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuo-Wei Lai, Vidya Muthukumar</dc:creator>
    </item>
    <item>
      <title>EquivaMap: Leveraging LLMs for Automatic Equivalence Checking of Optimization Formulations</title>
      <link>https://arxiv.org/abs/2502.14760</link>
      <description>arXiv:2502.14760v2 Announce Type: replace-cross 
Abstract: A fundamental problem in combinatorial optimization is identifying equivalent formulations. Despite the growing need for automated equivalence checks -- driven, for example, by optimization copilots, which generate problem formulations from natural language descriptions -- current approaches rely on simple heuristics that fail to reliably check formulation equivalence. Inspired by Karp reductions, in this work we introduce Quasi-Karp equivalence, a formal criterion for determining when two optimization formulations are equivalent based on the existence of a mapping between their decision variables. We propose EquivaMap, a framework that leverages large language models to automatically discover such mappings for scalable, reliable equivalence checking, with a verification stage that ensures mapped solutions preserve feasibility and optimality without additional solver calls. To evaluate our approach, we construct EquivaFormulation, the first open-source dataset of equivalent optimization formulations, generated by applying transformations such as adding slack variables or valid inequalities to existing formulations. Empirically, EquivaMap significantly outperforms existing methods, achieving substantial improvements in correctly identifying formulation equivalence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14760v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haotian Zhai, Connor Lawless, Ellen Vitercik, Liu Leqi</dc:creator>
    </item>
    <item>
      <title>On long-duration storage, weather uncertainty and limited foresight</title>
      <link>https://arxiv.org/abs/2505.12538</link>
      <description>arXiv:2505.12538v3 Announce Type: replace-cross 
Abstract: Long-duration energy storage (LDES) is a key component for fully renewable, sector-coupled energy systems based on wind and solar. While capacity expansion planning has begun to take into account interannual weather variability, it often ignores weather uncertainty and limited foresight in capacity and operational decisions. We build a stochastic capacity expansion model for fully decarbonized energy systems with LDES in Europe accounting for weather uncertainty - isolating the effect of limited foresight by comparing it to a perfect foresight benchmark. Under limited foresight, LDES acts as a hedge against extreme system states operating defensively and exhibiting a stockpiling effect absent under perfect foresight. Solar PV gains in system value for its higher predictability with up to 25% higher capacities versus the benchmark while onshore wind capacities are lower. We shed light on the underlying mechanisms by deriving implicit LDES bidding curves. We show that LDES bids reflect the costs and the weather-dependent probability of extreme system states conditional on the current system state. This has important implications for the price formation on renewable electricity markets, as a wide and continuous range of probabilistic LDES bids alleviates concerns of extreme price disparity at high renewable shares.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12538v3</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 11 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Schmidt</dc:creator>
    </item>
  </channel>
</rss>
