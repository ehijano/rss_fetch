<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Jul 2025 01:28:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Monotone Circuit Complexity of Matching</title>
      <link>https://arxiv.org/abs/2507.16105</link>
      <description>arXiv:2507.16105v1 Announce Type: new 
Abstract: We show that the perfect matching function on $n$-vertex graphs requires monotone circuits of size $\smash{2^{n^{\Omega(1)}}}$. This improves on the $n^{\Omega(\log n)}$ lower bound of Razborov (1985). Our proof uses the standard approximation method together with a new sunflower lemma for matchings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16105v1</guid>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bruno Cavalar, Mika G\"o\"os, Artur Riazanov, Anastasia Sofronova, Dmitry Sokolov</dc:creator>
    </item>
    <item>
      <title>Constructing material network representations for intelligent amorphous alloys design</title>
      <link>https://arxiv.org/abs/2507.16336</link>
      <description>arXiv:2507.16336v1 Announce Type: cross 
Abstract: Designing high-performance amorphous alloys is demanding for various applications. But this process intensively relies on empirical laws and unlimited attempts. The high-cost and low-efficiency nature of the traditional strategies prevents effective sampling in the enormous material space. Here, we propose material networks to accelerate the discovery of binary and ternary amorphous alloys. The network topologies reveal hidden material candidates that were obscured by traditional tabular data representations. By scrutinizing the amorphous alloys synthesized in different years, we construct dynamical material networks to track the history of the alloy discovery. We find that some innovative materials designed in the past were encoded in the networks, demonstrating their predictive power in guiding new alloy design. These material networks show physical similarities with several real-world networks in our daily lives. Our findings pave a new way for intelligent materials design, especially for complex alloys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16336v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. -Y. Zhang, J. Tian, S. -L. Liu, H. -M. Zhang, H. -Y. Bai, Y. -C. Hu, W. -H. Wang</dc:creator>
    </item>
    <item>
      <title>An unconditional lower bound for the active-set method in convex quadratic maximization</title>
      <link>https://arxiv.org/abs/2507.16648</link>
      <description>arXiv:2507.16648v1 Announce Type: cross 
Abstract: We prove that the active-set method needs an exponential number of iterations in the worst-case to maximize a convex quadratic function subject to linear constraints, regardless of the pivot rule used. This substantially improves over the best previously known lower bound [IPCO 2025], which needs objective functions of polynomial degrees $\omega(\log d)$ in dimension $d$, to a bound using a convex polynomial of degree 2. In particular, our result firmly resolves the open question [IPCO 2025] of whether a constant degree suffices, and it represents significant progress towards linear objectives, where the active-set method coincides with the simplex method and a lower bound for all pivot rules would constitute a major breakthrough.
  Our result is based on a novel extended formulation, recursively constructed using deformed products. Its key feature is that it projects onto a polygonal approximation of a parabola while preserving all of its exponentially many vertices. We define a quadratic objective that forces the active-set method to follow the parabolic boundary of this projection, without allowing any shortcuts along chords corresponding to edges of its full-dimensional preimage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16648v1</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleon Bach, Yann Disser, Sophie Huiberts, Nils Mosis</dc:creator>
    </item>
    <item>
      <title>Computational aspects of the trace norm contraction coefficient</title>
      <link>https://arxiv.org/abs/2507.16737</link>
      <description>arXiv:2507.16737v1 Announce Type: cross 
Abstract: We show that approximating the trace norm contraction coefficient of a quantum channel within a constant factor is NP-hard. Equivalently, this shows that determining the optimal success probability for encoding a bit in a quantum system undergoing noise is NP-hard. This contrasts with the classical analogue of this problem that can clearly by solved efficiently. Our hardness results also hold for deciding if the contraction coefficient is equal to 1. As a consequence, we show that deciding if a non-commutative graph has an independence number of at least 2 is NP-hard. In addition, we establish a converging hierarchy of semidefinite programming upper bounds on the contraction coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16737v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Idris Delsol, Omar Fawzi, Jan Kochanowski, Akshay Ramachandran</dc:creator>
    </item>
    <item>
      <title>Closing the complexity gap of the double distance problem</title>
      <link>https://arxiv.org/abs/2411.01691</link>
      <description>arXiv:2411.01691v2 Announce Type: replace 
Abstract: Genome rearrangement has been an active area of research in computational comparative genomics for the last three decades. While initially mostly an interesting algorithmic endeavor, now the practical application by applying rearrangement distance methods and more advanced phylogenetic tasks is becoming common practice, given the availability of many completely sequenced genomes. Several genome rearrangement models have been developed over time, sometimes with surprising computational properties. A prominent example is the fact that computing the reversal distance of two signed permutations is possible in linear time, while for two unsigned permutations it is NP-hard. Therefore one has always to be careful about the precise problem formulation and complexity analysis of rearrangement problems in order not to be fooled. The double distance is the minimum number of genomic rearrangements between a singular and a duplicated genome that, in addition to rearrangements, are separated by a whole genome duplication. At the same time it allows to assign the genes of the duplicated genome to the two paralogous chromosome copies that existed right after the duplication event. Computing the double distance is another example of a tricky hardness landscape: If the distance measure underlying the double distance is the simple breakpoint distance, the problem can be solved in linear time, while with the more elaborate DCJ distance it is NP-hard. Indeed, there is a family of distance measures, parameterized by an even number k, between the breakpoint distance (k=2) and the DCJ distance (k=\infty). Little was known about the hardness border between these extremes; the problem complexity was known only for k=4 and k=6. In this paper, we close the gap, providing a full picture of the hardness landscape when computing the double distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01691v2</guid>
      <category>cs.CC</category>
      <category>math.CO</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lu\'is Cunha, Thiago Lopes, U\'everton Souza, Leonard Bohnenk\"amper, Mar\'ilia D. V. Braga, Jens Stoye</dc:creator>
    </item>
    <item>
      <title>Characterizing p-Simulation Between Theories</title>
      <link>https://arxiv.org/abs/2507.13576</link>
      <description>arXiv:2507.13576v2 Announce Type: replace 
Abstract: This paper characterizes when one axiomatic theory, as a proof system for tautologies, $p$-simulates another, by showing: (i)~if c.e. theory $\mathcal{S}$ efficiently interprets $\mathcal{S}{+}\phi$, then $\mathcal{S}$ $p$-simulates $\mathcal{S}{+}\phi$ (Je\v{r}\'abek in Pudl\'ak17 proved simulation), since the interpretation maps an $\mathcal{S}{+}\phi$-proof whose lines are all theorems into an $\mathcal{S}$-proof; (ii)~$\mathcal{S}$ proves ``$\mathcal{S}$ efficiently interprets $\mathcal{S}{+}\phi$'' iff $\mathcal{S}$ proves ``$\mathcal{S}$ $p$-simulates $\mathcal{S}{+}\phi$'' (if so, $\mathcal{S}$ already proves the $\Pi_1$ theorems of $\mathcal{S}{+}\phi$); and (iii)~no $\mathcal{S}$ $p$-simulates all theories. Result (iii) implies $\textbf{P}{\neq}\textbf{NP}{\neq}\textbf{coNP}$, using the nonrelativizing fact ``no c.e. theory interprets all c.e. theories'' (false for $\mathcal{S}$ with predicate for true sentences). To explore whether this framework resolves other open questions, the paper formulates conjectures stronger than ``no optimal proof system exists'' that imply Feige's Hypothesis, the existence of one-way functions, and circuit lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13576v2</guid>
      <category>cs.CC</category>
      <category>math.LO</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hunter Monroe</dc:creator>
    </item>
    <item>
      <title>Beating the natural Grover bound for low-energy estimation and state preparation</title>
      <link>https://arxiv.org/abs/2407.03073</link>
      <description>arXiv:2407.03073v3 Announce Type: replace-cross 
Abstract: Estimating ground state energies of many-body Hamiltonians is a central task in many areas of quantum physics. In this work, we give quantum algorithms which, given any $k$-body Hamiltonian $H$, compute an estimate for the ground state energy and prepare a quantum state achieving said energy, respectively. Specifically, for any $\varepsilon&gt;0$, our algorithms return, with high probability, an estimate of the ground state energy of $H$ within additive error $\varepsilon M$, or a quantum state with the corresponding energy. Here, $M$ is the total strength of all interaction terms, which in general is extensive in the system size. Our approach makes no assumptions about the geometry or spatial locality of interaction terms of the input Hamiltonian and thus handles even long-range or all-to-all interactions, such as in quantum chemistry, where lattice-based techniques break down. In this fully general setting, the runtime of our algorithms scales as $2^{cn/2}$ for $c&lt;1$, yielding the first quantum algorithms for low-energy estimation breaking a standard square root Grover speedup for unstructured search. The core of our approach is remarkably simple, and relies on showing that an extensive fraction of the interactions can be neglected with a controlled error. What this ultimately implies is that even arbitrary $k$-local Hamiltonians have structure in their low energy space, in the form of an exponential-dimensional low energy subspace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03073v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/29qw-bssx</arxiv:DOI>
      <arxiv:journal_reference>Physical Review Letters 135, 030601 (2025)</arxiv:journal_reference>
      <dc:creator>Harry Buhrman, Sevag Gharibian, Zeph Landau, Fran\c{c}ois Le Gall, Norbert Schuch, Suguru Tamaki</dc:creator>
    </item>
    <item>
      <title>On the Complexity of p-Order Cone Programs</title>
      <link>https://arxiv.org/abs/2501.09828</link>
      <description>arXiv:2501.09828v2 Announce Type: replace-cross 
Abstract: This manuscript explores novel complexity results for the feasibility problem over $p$-order cones, extending the foundational work of Porkolab and Khachiyan. By leveraging the intrinsic structure of $p$-order cones, we derive refined complexity bounds that surpass those obtained via standard semidefinite programming reformulations. Our analysis not only improves theoretical bounds but also provides practical insights into the computational efficiency of solving such problems. In addition to establishing complexity results, we derive explicit bounds for solutions when the feasibility problem admits one. For infeasible instances, we analyze their discrepancy quantifying the degree of infeasibility. Finally, we examine specific cases of interest, highlighting scenarios where the geometry of $p$-order cones or problem structure yields further computational simplifications. These findings contribute to both the theoretical understanding and practical tractability of optimization problems involving $p$-order cones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09828v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V\'ictor Blanco, Victor Magron, Miguel Mart\'inez-Ant\'on</dc:creator>
    </item>
    <item>
      <title>Feature Selection and Junta Testing are Statistically Equivalent</title>
      <link>https://arxiv.org/abs/2505.04604</link>
      <description>arXiv:2505.04604v2 Announce Type: replace-cross 
Abstract: For a function $f \colon \{0,1\}^n \to \{0,1\}$, the junta testing problem asks whether $f$ depends on only $k$ variables. If $f$ depends on only $k$ variables, the feature selection problem asks to find those variables. We prove that these two tasks are statistically equivalent. Specifically, we show that the ``brute-force'' algorithm, which checks for any set of $k$ variables consistent with the sample, is simultaneously sample-optimal for both problems, and the optimal sample size is \[ \Theta\left(\frac 1 \varepsilon \left( \sqrt{2^k \log {n \choose k}} + \log {n \choose k}\right)\right). \]</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04604v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Beretta, Nathaniel Harms, Caleb Koch</dc:creator>
    </item>
    <item>
      <title>Studying homing and synchronizing sequences for Timed Finite State Machines with output delays</title>
      <link>https://arxiv.org/abs/2507.14526</link>
      <description>arXiv:2507.14526v2 Announce Type: replace-cross 
Abstract: The paper introduces final state identification (synchronizing and homing) sequences for Timed Finite State Machines (TFSMs) with output delays and investigates their properties. We formally define the notions of homing sequences (HSs) and synchronizing sequences (SSs) for these TFSMs and demonstrate that several properties that hold for untimed machines do not necessarily apply to timed ones. Furthermore, we explore the applicability of various approaches for deriving SSs and HSs for Timed FSMs with output delays, such as truncated successor tree-based and FSM abstraction-based methods. Correspondingly, we identify the subclasses of TFSMs for which these approaches can be directly applied and those for which other methods are required. Additionally, we evaluate the complexity of existence check and derivation of (shortest) HSs / SSs for TFSMs with output delays.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.14526v2</guid>
      <category>cs.FL</category>
      <category>cs.CC</category>
      <pubDate>Wed, 23 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Evgenii Vinarskii, Jakub Ruszil, Adam Roman, Natalia Kushik</dc:creator>
    </item>
  </channel>
</rss>
