<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 02:35:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exploring the Role of LLMs for Supporting Older Adults: Opportunities and Concerns</title>
      <link>https://arxiv.org/abs/2411.08123</link>
      <description>arXiv:2411.08123v1 Announce Type: new 
Abstract: We explore some of the existing research in HCI around technology for older adults and examine the role of LLMs in enhancing it. We also discuss the digital divide and emphasize the need for inclusive technology design. At the same time, we also surface concerns regarding privacy, security, and the accuracy of information provided by LLMs, alongside the importance of user-centered design to make technology accessible and effective for the elderly. We show the transformative possibilities of LLM-supported interactions at the intersection of aging, technology, and human-computer interaction, advocating for further research and development in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08123v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sidharth Kaliappan, Abhay Sheel Anand, Koustuv Saha, Ravi Karkar</dc:creator>
    </item>
    <item>
      <title>WristSonic: Enabling Fine-grained Hand-Face Interactions on Smartwatches Using Active Acoustic Sensing</title>
      <link>https://arxiv.org/abs/2411.08217</link>
      <description>arXiv:2411.08217v1 Announce Type: new 
Abstract: Hand-face interactions play a key role in many everyday tasks, providing insights into user habits, behaviors, intentions, and expressions. However, existing wearable sensing systems often struggle to track these interactions in daily settings due to their reliance on multiple sensors or privacy-sensitive, vision-based approaches. To address these challenges, we propose WristSonic, a wrist-worn active acoustic sensing system that uses speakers and microphones to capture ultrasonic reflections from hand, arm, and face movements, enabling fine-grained detection of hand-face interactions with minimal intrusion. By transmitting and analyzing ultrasonic waves, WristSonic distinguishes a wide range of gestures, such as tapping the temple, brushing teeth, and nodding, using a Transformer-based neural network architecture. This approach achieves robust recognition of 21 distinct actions with a single, low-power, privacy-conscious wearable. Through two user studies with 15 participants in controlled and semi-in-the-wild settings, WristSonic demonstrates high efficacy, achieving macro F1-scores of 93.08% and 82.65%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08217v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saif Mahmud, Kian Mahmoodi, Chi-Jung Lee, Francois Guimbretiere, Cheng Zhang</dc:creator>
    </item>
    <item>
      <title>Virtual Buddy: Redefining Conversational AI Interactions for Individuals with Hand Motor Disabilities</title>
      <link>https://arxiv.org/abs/2411.08228</link>
      <description>arXiv:2411.08228v1 Announce Type: new 
Abstract: Advances in artificial intelligence have transformed the paradigm of human-computer interaction, with the development of conversational AI systems playing a pivotal role. These systems employ technologies such as natural language processing and machine learning to simulate intelligent and human-like conversations. Driven by the personal experience of an individual with a neuromuscular disease who faces challenges with leaving home and contends with limited hand-motor control when operating digital systems, including conversational AI platforms, we propose a method aimed at enriching their interaction with conversational AI. Our prototype allows the creation of multiple agent personas based on hobbies and interests, to support topic-based conversations. In contrast with existing systems, such as Replika, that offer a 1:1 relation with a virtual agent, our design enables one-to-many relationships, easing the process of interaction for this individual by reducing the need for constant data input. We can imagine our prototype potentially helping others who are in a similar situation with reduced typing/input ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08228v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3586182.3616680</arxiv:DOI>
      <arxiv:journal_reference>Adjunct Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology. 2023. (pp. 1-3)</arxiv:journal_reference>
      <dc:creator>Atieh Taheri, Purav Bhardwaj, Arthur Caetano, Alice Zhong, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Virtual Steps: The Experience of Walking for a Lifelong Wheelchair User in Virtual Reality</title>
      <link>https://arxiv.org/abs/2411.08229</link>
      <description>arXiv:2411.08229v1 Announce Type: new 
Abstract: Many people often take walking for granted, but for individuals with mobility disabilities, this seemingly simple act can feel out of reach. This reality can foster a sense of disconnect from the world since walking is a fundamental way in which people interact with each other and the environment. Advances in virtual reality and its immersive capabilities have made it possible to enable those who have never walked in their life to virtually experience walking. We co-designed a VR walking experience with a person with Spinal Muscular Atrophy who has been a lifelong wheelchair user. Over 9 days, we collected data on this person's experience through a diary study and analyzed this data to better understand the design elements required. Given that they had only ever seen others walking and had not experienced it first-hand, determining which design parameters must be considered in order to match the virtual experience to their idea of walking was challenging. Generally, we found the experience of walking to be quite positive, providing a perspective from a higher vantage point than what was available in a wheelchair. Our findings provide insights into the emotional complexities and evolving sense of agency accompanying virtual walking. These findings have implications for designing more inclusive and emotionally engaging virtual reality experiences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08229v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/VR58804.2024.00040</arxiv:DOI>
      <arxiv:journal_reference>In 2024 IEEE Conference Virtual Reality and 3D User Interfaces (VR) (pp. 168-178). IEEE</arxiv:journal_reference>
      <dc:creator>Atieh Taheri, Arthur Caetano, Misha Sra</dc:creator>
    </item>
    <item>
      <title>Proceedings of 6th International Conference AsiaHaptics 2024</title>
      <link>https://arxiv.org/abs/2411.08318</link>
      <description>arXiv:2411.08318v1 Announce Type: new 
Abstract: The sixth international conference AsiaHaptics 2024 took place at Sunway University, Malaysia on 28-30 October 2024. AsiaHaptics is an exhibition type of international conference dedicated to the haptics domain, engaging presentations accompanied by hands-on demonstrations. It presents the state-of-the-art of the diverse haptics (touch)-related research, including perception and illusion, development of haptics devices, and applications to a wide variety of fields such as education, medicine, telecommunication, navigation and entertainment. This proceedings volume is a valuable resource not only for active haptics researchers, but also for general readers wishing to understand the status quo in this interdisciplinary area of science and technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08318v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasutoshi Makino, Hsin-Ni Ho, Seokhee Jeon</dc:creator>
    </item>
    <item>
      <title>An Empirical Examination of the Evaluative AI Framework</title>
      <link>https://arxiv.org/abs/2411.08583</link>
      <description>arXiv:2411.08583v1 Announce Type: new 
Abstract: This study empirically examines the "Evaluative AI" framework, which aims to enhance the decision-making process for AI users by transitioning from a recommendation-based approach to a hypothesis-driven one. Rather than offering direct recommendations, this framework presents users pro and con evidence for hypotheses to support more informed decisions. However, findings from the current behavioral experiment reveal no significant improvement in decision-making performance and limited user engagement with the evidence provided, resulting in cognitive processes similar to those observed in traditional AI systems. Despite these results, the framework still holds promise for further exploration in future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08583v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaroslaw Kornowicz</dc:creator>
    </item>
    <item>
      <title>I Can Embrace and Avoid Vagueness Myself: Supporting the Design Process by Balancing Vagueness through Text-to-Image Generative AI</title>
      <link>https://arxiv.org/abs/2411.08588</link>
      <description>arXiv:2411.08588v1 Announce Type: new 
Abstract: This study examines the role of vagueness in the design process and its strategic management for the effective human-AI interaction. While vagueness in the generation of design ideas promotes diverse interpretations and prevents fixation, excessive vagueness can lead to scattered results. Designers attempt to use image search tools or generative AIs (e.g., Dall-E) for their work but often fail to achieve satisfactory results because the level of vagueness is not properly managed in these technologies. In this work, we identified how designers coordinate vagueness in their design process and applied key components of the process to the design of CLAY, an interactive system that balances vagueness through iterative prompt refinement by integrating the strengths of text-to-image generative AI. Results from our user study with 10 fashion designers showed that CLAY effectively supported their design process, reducing design time, and expanding creative possibilities compared to their existing practice, by allowing them to both embrace and avoid vagueness as needed. Our study highlights the importance of identifying key characteristics of the target user and domain, and exploring ways to incorporate them into the design of an AI-based interactive tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08588v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Myungjin Kim, Bogoan Kim, Kyungsik Han</dc:creator>
    </item>
    <item>
      <title>Thought Experiments in Design Fiction for Visualization</title>
      <link>https://arxiv.org/abs/2411.08621</link>
      <description>arXiv:2411.08621v1 Announce Type: new 
Abstract: Thought experiments are considered valuable tools in science, enabling the exploration of hypotheses and the examination of complex ideas in a conceptual, non-empirical framework. These thought experiments can be useful in design fiction for speculating future possibilities, examining existing and alternate scenarios in new ways or challenging current paradigms. In visualization, speculating future possibilities or exploring new ways of interpreting existing scenarios can provoke critical reflection and envision novel approaches. In this paper we present such thought experiments for visualization. We conceptualize and define a thought experiment to consist of a situation, a story, and a scenario. Situations are derived from different tools of thought experiments and visualization practice; a story is an AI-generated fiction based on the situation and the scenario is the grounding of the situation and story in visualization research. We present ten such thought experiments and demonstrate their utility in visualization by deriving critiques from them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08621v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swaroop Panda</dc:creator>
    </item>
    <item>
      <title>DipMe: Haptic Recognition of Granular Media for Tangible Interactive Applications</title>
      <link>https://arxiv.org/abs/2411.08641</link>
      <description>arXiv:2411.08641v1 Announce Type: new 
Abstract: While tangible user interface has shown its power in naturally interacting with rigid or soft objects, users cannot conveniently use different types of granular materials as the interaction media. We introduce DipMe as a smart device to recognize the types of granular media in real time, which can be used to connect the granular materials in the physical world with various virtual content. Other than vision-based solutions, we propose a dip operation of our device and exploit the haptic signals to recognize different types of granular materials. With modern machine learning tools, we find the haptic signals from different granular media are distinguishable by DipMe. With the online granular object recognition, we build several tangible interactive applications, demonstrating the effects of DipMe in perceiving granular materials and its potential in developing a tangible user interface with granular objects as the new media.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08641v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinkai Wang, Shuo Zhang, Ziyi Zhao, Lifeng Zhu, Aiguo Song</dc:creator>
    </item>
    <item>
      <title>Designing a Virtual Reality Training Apprenticeship for Cold Spray Advanced Manufacturing</title>
      <link>https://arxiv.org/abs/2411.08859</link>
      <description>arXiv:2411.08859v1 Announce Type: new 
Abstract: Apprenticeship and training programs in advanced manufacturing frequently encounter safety and accessibility concerns due to using heavy machinery. Virtual Reality (VR) training addresses such constraints while maintaining the spatial and procedural learning requirements of such training. However, designing effective VR training is challenging because advanced manufacturing processes are complex and require experts to train novices for a long time. This paper presents a VR Training Apprenticeship (VRTA) tailored for cold spray, which we carefully designed to teach novices step-by-step this particular advanced manufacturing process. To assess its effectiveness, we conducted an exploratory study ($n = 22$). We evaluated user experience (UX) measures in the form of quantitative scales, users' qualitative insights, and task performance with real-world machinery after the VR training. We discuss how the VRTA design contributed to the effectiveness and the challenges of considering VR training for advanced manufacturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08859v1</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahsa Nasri, Uttkarsh Narayan, Mustafa Feyyaz Sonbudak, Aubrey Simonson, Maria Chiu, Jason Donati, Mark Sivak, Mehmet Kosa, Casper Harteveld</dc:creator>
    </item>
    <item>
      <title>Audience Reach of Scientific Data Visualizations in Planetarium-Screened Films</title>
      <link>https://arxiv.org/abs/2411.08045</link>
      <description>arXiv:2411.08045v1 Announce Type: cross 
Abstract: Quantifying the global reach of planetarium dome shows presents significant challenges due to the lack of standardized viewership tracking mechanisms across diverse planetarium venues. We present an analysis of the global impact of dome shows, presenting data regarding four documentary films from a single visualization lab. Specifically, we designed and administered a viewership survey of four long-running shows that contained cinematic scientific visualizations. Reported survey data shows that between 1.2 - 2.6 million people have viewed these four films across the 68 responding planetariums (mean: 1.9 million). When we include estimates and extrapolate for the 315 planetariums that licensed these shows, we arrive at an estimate of 16.5 - 24.1 million people having seen these films (mean: 20.3 million).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08045v1</guid>
      <category>physics.pop-ph</category>
      <category>astro-ph.IM</category>
      <category>cs.CY</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kalina Borkiewicz, Eric Jensen, Yiwen Miao, Stuart Levy, J. P. Naiman, Jeff Carpenter, Katherine E. Isaacs</dc:creator>
    </item>
    <item>
      <title>Generalized Pose Space Embeddings for Training In-the-Wild using Anaylis-by-Synthesis</title>
      <link>https://arxiv.org/abs/2411.08603</link>
      <description>arXiv:2411.08603v1 Announce Type: cross 
Abstract: Modern pose estimation models are trained on large, manually-labelled datasets which are costly and may not cover the full extent of human poses and appearances in the real world. With advances in neural rendering, analysis-by-synthesis and the ability to not only predict, but also render the pose, is becoming an appealing framework, which could alleviate the need for large scale manual labelling efforts. While recent work have shown the feasibility of this approach, the predictions admit many flips due to a simplistic intermediate skeleton representation, resulting in low precision and inhibiting the acquisition of any downstream knowledge such as three-dimensional positioning. We solve this problem with a more expressive intermediate skeleton representation capable of capturing the semantics of the pose (left and right), which significantly reduces flips. To successfully train this new representation, we extend the analysis-by-synthesis framework with a training protocol based on synthetic data. We show that our representation results in less flips and more accurate predictions. Our approach outperforms previous models trained with analysis-by-synthesis on standard benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08603v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominik Borer, Jakob Buhmann, Martin Guay</dc:creator>
    </item>
    <item>
      <title>3D Modelling to Address Pandemic Challenges: A Project-Based Learning Methodology</title>
      <link>https://arxiv.org/abs/2411.08730</link>
      <description>arXiv:2411.08730v1 Announce Type: cross 
Abstract: The use of 3D modelling in medical education is a revolutionary tool during the learning process. In fact, this type of technology enables a more interactive teaching approach, making information retention more effective and enhancing students' understanding. 3D modelling allows for the creation of precise representations of the human body, as well as interaction with three-dimensional models, giving students a better spatial understanding of the different organs and systems and enabling simulations of surgical and technical procedures. This way, medical education is enriched with a more realistic and safe educational experience. The goal is to understand whether, when students and schools are challenged, they play an important role in addressing health issues in their community. School-led projects are directed towards educational scenarios that emphasize STEM education, tackling relevant public health problems through open-school initiatives. By implementing an educational scenario focused on 3D modelling and leveraging technology, we aim to raise community awareness on public health issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08730v1</guid>
      <category>cs.MM</category>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T\^ania Rocha, Ana Ribeiro, Joana Oliveira, Ricardo Nunes, Diana Carvalho, Hugo Paredes, Paulo Martins</dc:creator>
    </item>
    <item>
      <title>Information Visualization for Effective Altruism</title>
      <link>https://arxiv.org/abs/2209.00836</link>
      <description>arXiv:2209.00836v2 Announce Type: replace 
Abstract: Effective altruism is a movement whose goal it to use evidence and reason to figure out how to benefit others as much as possible. This movement is becoming influential, but effective altruists still lack tools to help them understand complex humanitarian trade-offs and make good decisions based on data. Visualization-the study of computer-supported, visual representations of data meant to support understanding, communication, and decision makingcan help alleviate this issue. Conversely, effective altruism provides a powerful thinking framework for visualization research that focuses on humanitarian applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00836v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Dragicevic</dc:creator>
    </item>
    <item>
      <title>TexSenseGAN: A User-Guided System for Optimizing Texture-Related Vibrotactile Feedback Using Generative Adversarial Network</title>
      <link>https://arxiv.org/abs/2407.11467</link>
      <description>arXiv:2407.11467v4 Announce Type: replace 
Abstract: Texture rendering has attracted significant attention as a means of creating realistic experiences in human-virtual object interactions. But in practical applications, many limited device conditions do not support the complete reproduction of spatial and temporal tactile stimuli. Different frequency components of designed vibrations can activate texture-related sensations owing to similar receptors. Therefore, we can utilize corresponding vibration signals to provide tactile feedback within the constraints of limited device environments. However, designing specific vibrations for numerous real-world materials is impractical. This study proposes a human-in-the-loop vibration generation model based on user preferences. To enable users to easily control the generation of vibration samples with large parameter spaces, we introduced an optimization model based on Differential Subspace Search (DSS) and Generative Adversarial Network (GAN). With DSS, users can employ a one-dimensional slider to easily modify the high-dimensional latent space to ensure that the GAN can generate desired vibrations. We trained the generative model using an open dataset of tactile vibration data and selected five types of vibrations as target samples for the generation experiment. Extensive user experiments were conducted using the generated and real samples. The results indicated that our system could generate distinguishable samples that matched the target characteristics. Moreover, we established a correlation between subjects' ability to distinguish real samples and their ability to distinguish generated samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11467v4</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxin Zhang, Shun Terui, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Proxona: Leveraging LLM-Driven Personas to Enhance Creators' Understanding of Their Audience</title>
      <link>https://arxiv.org/abs/2408.10937</link>
      <description>arXiv:2408.10937v2 Announce Type: replace 
Abstract: Creators are nothing without their audience, and thereby understanding their audience is the cornerstone of their professional achievement. Yet many creators feel lost while comprehending audiences with existing tools, which offer insufficient insights for tailoring content to audience needs. To address the challenges creators face in understanding their audience, we present Proxona, a system for defining and extracting representative audience personas from the comments. Creators converse with personas to gain insights into their preferences and engagement, solicit feedback, and implement evidence-based improvements to their content. Powered by large language models, Proxona analyzes audience comments, distilling the latent characteristics of audiences into tangible dimensions (classification categories) and values (category attributes). Proxona then clusters these into synthetic personas. Our technical evaluations demonstrated that our pipelines effectively generated relevant and distinct dimensions and values, enabling the deduction of audience-reflecting personas, while minimizing the likelihood of hallucinations in persona responses. Our user evaluation with 11 creators showed that Proxona supported creators to gain new insights about their audience, make informed decisions, and successfully complete content creation with high confidence. Proxona's data-driven audience personas empower creators to seamlessly integrate audience perspectives into their creative processes, fostering a collaborative approach to content creation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10937v2</guid>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoonseo Choi, Eun Jeong Kang, Seulgi Choi, Min Kyung Lee, Juho Kim</dc:creator>
    </item>
    <item>
      <title>SLYKLatent: A Learning Framework for Gaze Estimation Using Deep Facial Feature Learning</title>
      <link>https://arxiv.org/abs/2402.01555</link>
      <description>arXiv:2402.01555v2 Announce Type: replace-cross 
Abstract: In this research, we present SLYKLatent, a novel approach for enhancing gaze estimation by addressing appearance instability challenges in datasets due to aleatoric uncertainties, covariant shifts, and test domain generalization. SLYKLatent utilizes Self-Supervised Learning for initial training with facial expression datasets, followed by refinement with a patch-based tri-branch network and an inverse explained variance-weighted training loss function. Our evaluation on benchmark datasets achieves a 10.9% improvement on Gaze360, supersedes top MPIIFaceGaze results with 3.8%, and leads on a subset of ETH-XGaze by 11.6%, surpassing existing methods by significant margins. Adaptability tests on RAF-DB and Affectnet show 86.4% and 60.9% accuracies, respectively. Ablation studies confirm the effectiveness of SLYKLatent's novel components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01555v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>eess.IV</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Adebayo, Joost C. Dessing, Se\'an McLoone</dc:creator>
    </item>
    <item>
      <title>PEaRL: Personalized Privacy of Human-Centric Systems using Early-Exit Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2403.05864</link>
      <description>arXiv:2403.05864v2 Announce Type: replace-cross 
Abstract: In the evolving landscape of human-centric systems, personalized privacy solutions are becoming increasingly crucial due to the dynamic nature of human interactions. Traditional static privacy models often fail to meet the diverse and changing privacy needs of users. This paper introduces PEaRL, a system designed to enhance privacy preservation by tailoring its approach to individual behavioral patterns and preferences. While incorporating reinforcement learning (RL) for its adaptability, PEaRL primarily focuses on employing an early-exit strategy that dynamically balances privacy protection and system utility. This approach addresses the challenges posed by the variability and evolution of human behavior, which static privacy models struggle to handle effectively. We evaluate PEaRL in two distinct contexts: Smart Home environments and Virtual Reality (VR) Smart Classrooms. The empirical results demonstrate PEaRL's capability to provide a personalized tradeoff between user privacy and application utility, adapting effectively to individual user preferences. On average, across both systems, PEaRL enhances privacy protection by 31%, with a corresponding utility reduction of 24%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05864v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mojtaba Taherisadr, Salma Elmalaki</dc:creator>
    </item>
    <item>
      <title>Strike the Balance: On-the-Fly Uncertainty based User Interactions for Long-Term Video Object Segmentation</title>
      <link>https://arxiv.org/abs/2408.00169</link>
      <description>arXiv:2408.00169v2 Announce Type: replace-cross 
Abstract: In this paper, we introduce a variant of video object segmentation (VOS) that bridges interactive and semi-automatic approaches, termed Lazy Video Object Segmentation (ziVOS). In contrast, to both tasks, which handle video object segmentation in an off-line manner (i.e., pre-recorded sequences), we propose through ziVOS to target online recorded sequences. Here, we strive to strike a balance between performance and robustness for long-term scenarios by soliciting user feedback's on-the-fly during the segmentation process. Hence, we aim to maximize the tracking duration of an object of interest, while requiring minimal user corrections to maintain tracking over an extended period. We propose a competitive baseline, i.e., Lazy-XMem, as a reference for future works in ziVOS. Our proposed approach uses an uncertainty estimation of the tracking state to determine whether a user interaction is necessary to refine the model's prediction. To quantitatively assess the performance of our method and the user's workload, we introduce complementary metrics alongside those already established in the field. We evaluate our approach using the recently introduced LVOS dataset, which offers numerous long-term videos. Our code is publicly available at https://github.com/Vujas-Eteph/LazyXMem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00169v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>St\'ephane Vujasinovi\'c, Stefan Becker, Sebastian Bullinger, Norbert Scherer-Negenborn, Michael Arens, Rainer Stiefelhagen</dc:creator>
    </item>
    <item>
      <title>VoxelKeypointFusion: Generalizable Multi-View Multi-Person Pose Estimation</title>
      <link>https://arxiv.org/abs/2410.18723</link>
      <description>arXiv:2410.18723v2 Announce Type: replace-cross 
Abstract: In the rapidly evolving field of computer vision, the task of accurately estimating the poses of multiple individuals from various viewpoints presents a formidable challenge, especially if the estimations should be reliable as well. This work presents an extensive evaluation of the generalization capabilities of multi-view multi-person pose estimators to unseen datasets and presents a new algorithm with strong performance in this task. It also studies the improvements by additionally using depth information. Since the new approach can not only generalize well to unseen datasets, but also to different keypoints, the first multi-view multi-person whole-body estimator is presented. To support further research on those topics, all of the work is publicly accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18723v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Bermuth, Alexander Poeppel, Wolfgang Reif</dc:creator>
    </item>
  </channel>
</rss>
