<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Mar 2025 04:02:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The computation of average kernel with Gauss-Laguerre quadrature for double integrals</title>
      <link>https://arxiv.org/abs/2503.05726</link>
      <description>arXiv:2503.05726v1 Announce Type: new 
Abstract: The use of average kernel method based on the Laplace transformation can significantly simplify the procedure for obtaining approximate analytical solution of Smoluchowski equation. However, this method also has its own shortcomings, one of which is the higher computational complexity of the binary Laplace transformation for a nonlinear collision kernel. In this study, a universal algorithm based on the Gauss-Laguerre quadrature for treating the double integral is developed to obtain easily and quickly pre-exponential factor of the average kernel. Furthermore, the corresponding truncation error estimate also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05726v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kejun Pan, Mingliang Xie</dc:creator>
    </item>
    <item>
      <title>A Digital Twin-Driven Recommendation System for Adaptive Campus Course Timetabling</title>
      <link>https://arxiv.org/abs/2503.06109</link>
      <description>arXiv:2503.06109v1 Announce Type: new 
Abstract: Efficient and adaptive course timetabling for large, dynamic university campuses remains a significant challenge due to the complex interplay of hard and soft constraints. Traditional static optimization methods often fail to accommodate real-time disruptions, evolving user preferences, and the nuanced spatial-temporal relationships inherent in campus environments. This paper reconceptualizes the timetabling problem as a recommendation-based task and leverages the Texas A&amp;M Campus Digital Twin as a dynamic data platform. Our proposed framework integrates collaborative and content-based filtering techniques with iterative feedback mechanisms, thereby generating a ranked set of adaptive timetable recommendations. A composite scoring function, incorporating metrics for classroom occupancy, travel distance, travel time, and vertical transitions, enables the framework to systematically balance resource utilization with user-centric factors. Extensive experiments using real-world data from Texas A&amp;M University demonstrate that our approach effectively reduces travel inefficiencies, optimizes classroom utilization, and enhances overall user satisfaction. By coupling a recommendation-oriented paradigm with a digital twin environment, this study offers a robust and scalable blueprint for intelligent campus planning and resource allocation, with potential applications in broader urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06109v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keshu Wu, Xinyue Ye, Suphanut Jamonnak, Xin Feng</dc:creator>
    </item>
    <item>
      <title>Identifying Evidence Subgraphs for Financial Risk Detection via Graph Counterfactual and Factual Reasoning</title>
      <link>https://arxiv.org/abs/2503.06441</link>
      <description>arXiv:2503.06441v1 Announce Type: new 
Abstract: Company financial risks pose a significant threat to personal wealth and national economic stability, stimulating increasing attention towards the development of efficient andtimely methods for monitoring them. Current approaches tend to use graph neural networks (GNNs) to model the momentum spillover effect of risks. However, due to the black-box nature of GNNs, these methods leave much to be improved for precise and reliable explanations towards company risks. In this paper, we propose CF3, a novel Counterfactual and Factual learning method for company Financial risk detection, which generates evidence subgraphs on company knowledge graphs to reliably detect and explain company financial risks. Specifically, we first propose a meta-path attribution process based on Granger causality, selecting the meta-paths most relevant to the target node labels to construct an attribution subgraph. Subsequently, we propose anedge-type-aware graph generator to identify important edges, and we also devise a layer-based feature masker to recognize crucial node features. Finally, we utilize counterfactual-factual reasoning and a loss function based on attribution subgraphs to jointly guide the learning of the graph generator and feature masker. Extensive experiments on three real-world datasets demonstrate the superior performance of our method compared to state-of-the-art approaches in the field of financial risk detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06441v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huaming Du, Lei Yuan, Qing Yang, Xingyan Chen, Yu Zhao, Han Ji, Fuzhen Zhuang, Carl Yang, Gang Kou</dc:creator>
    </item>
    <item>
      <title>Hierarchical Multi-Objective Optimization for Precise Performance Design of Closed-Chain Legged Mechanisms</title>
      <link>https://arxiv.org/abs/2503.06533</link>
      <description>arXiv:2503.06533v1 Announce Type: new 
Abstract: Over the past decades, the performance design of closed-chain legged mechanisms (CLMs) has not been adequately addressed. Most existing design methodologies have predominantly relied on trajectory synthesis, which inadvertently prioritizes less critical performance aspects. This study proposes a hierarchical multi-objective optimization strategy to address this limitation. First, the numerical performance-trajectory mapping is derived based on a foot-ground contact model, aiming to decouple the performance characteristics. Subsequently, a hierarchical optimization strategy is employed for two CLM design scenarios: In trajectory shape-constrained scenarios, a coarse-to-fine optimization process, integrating Fourier descriptors, refines the design from overall shape to local features. In scenarios without trajectory shape constraints, a stepwise optimization process is proposed for reconfigurable CLMs to transition from primary motion to auxiliary motion. The robustness of the proposed design strategy is validated across three configurations and seven algorithms. The effectiveness of the proposed design strategy is verified by comparison with other existing CLM design methods. The applicability of the proposed strategy is confirmed through simulation and prototype experiments. The results demonstrate that the hierarchical strategy effectively addresses the challenges of precise performance design in CLMs. Our work provides a general framework for the CLM design and offers insights for the optimization design of other closed-chain linkages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06533v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Long Guo, Ying Zhang, Qi Qin, Guanjun Liu, Hanyu Chen, Yan-an Yao</dc:creator>
    </item>
    <item>
      <title>Energy-Adaptive Checkpoint-Free Intermittent Inference for Low Power Energy Harvesting Systems</title>
      <link>https://arxiv.org/abs/2503.06663</link>
      <description>arXiv:2503.06663v1 Announce Type: new 
Abstract: Deep neural network (DNN) inference in energy harvesting (EH) devices poses significant challenges due to resource constraints and frequent power interruptions. These power losses not only increase end-to-end latency, but also compromise inference consistency and accuracy, as existing checkpointing and restore mechanisms are prone to errors. Consequently, the quality of service (QoS) for DNN inference on EH devices is severely impacted. In this paper, we propose an energy-adaptive DNN inference mechanism capable of dynamically transitioning the model into a low-power mode by reducing computational complexity when harvested energy is limited. This approach ensures that end-to-end latency requirements are met. Additionally, to address the limitations of error-prone checkpoint-and-restore mechanisms, we introduce a checkpoint-free intermittent inference framework that ensures consistent, progress-preserving DNN inference during power failures in energy-harvesting systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06663v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sahidul Islam, Wei Wei, Jishnu Banarjee, Chen Pan</dc:creator>
    </item>
    <item>
      <title>Modular Photobioreactor Fa\c{c}ade Systems for Sustainable Architecture: Design, Fabrication, and Real-Time Monitoring</title>
      <link>https://arxiv.org/abs/2503.06769</link>
      <description>arXiv:2503.06769v1 Announce Type: new 
Abstract: This paper proposes an innovative solution to the growing issue of greenhouse gas emissions: a closed photobioreactor (PBR) fa\c{c}ade system to mitigate greenhouse gas (GHG) concentrations. With digital fabrication technology, this study explores the transition from traditional, single function building facades to multifunctional, integrated building systems. It introduces a photobioreactor (PBR) fa\c{c}ade system to mitigate greenhouse gas (GHG) concentrations while addressing the challenge of large-scale prefabricated components transportation. This research introduces a novel approach by designing the fa\c{c}ade system as modular, user-friendly and transportation-friendly bricks, enabling the creation of a user-customized and self-assembled photobioreactor (PBR) system. The single module in the system is proposed to be "neutralization bricks", which embedded with algae and equipped with an air circulation system, facilitating the photobioreactor (PBR)'s functionality. A connection system between modules allows for easy assembly by users, while a limited variety of brick styles ensures modularity in manufacturing without sacrificing customization and diversity. The system is also equipped with an advanced microalgae status detection algorithm, which allows users to monitor the condition of the microalgae using monocular camera. This functionality ensures timely alerts and notifications for users to replace the algae, thereby optimizing the operational efficiency and sustainability of the algae cultivation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06769v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiujin Liu</dc:creator>
    </item>
    <item>
      <title>Simulating programmable morphing of shape memory polymer beam systems with complex geometry and topology</title>
      <link>https://arxiv.org/abs/2503.07150</link>
      <description>arXiv:2503.07150v1 Announce Type: new 
Abstract: We propose a novel approach to the analysis of programmable geometrically exact shear deformable beam systems made of shape memory polymers. The proposed method combines the viscoelastic Generalized Maxwell model with the Williams, Landel and Ferry relaxation principle, enabling the reproduction of the shape memory effect of structural systems featuring complex geometry and topology. Very high efficiency is pursued by discretizing the differential problem in space through the isogeometric collocation (IGA-C) method. The method, in addition to the desirable attributes of isogeometric analysis (IGA), such as exactness of the geometric reconstruction of complex shapes and high-order accuracy, circumvents the need for numerical integration since it discretizes the problem in the strong form. Other distinguishing features of the proposed formulation are: i) ${\rm SO}(3)$-consistency for the linearization of the problem and for the time stepping; ii) minimal (finite) rotation parametrization, that means only three rotational unknowns are used; iii) no additional unknowns are needed to account for the rate-dependent material compared to the purely elastic case. Through different numerical applications involving challenging initial geometries, we show that the proposed formulation possesses all the sought attributes in terms of programmability of complex systems, geometric flexibility, and high order accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07150v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Ferri, Enzo Marino</dc:creator>
    </item>
    <item>
      <title>An Analytics-Driven Approach to Enhancing Supply Chain Visibility with Graph Neural Networks and Federated Learning</title>
      <link>https://arxiv.org/abs/2503.07231</link>
      <description>arXiv:2503.07231v1 Announce Type: new 
Abstract: In today's globalised trade, supply chains form complex networks spanning multiple organisations and even countries, making them highly vulnerable to disruptions. These vulnerabilities, highlighted by recent global crises, underscore the urgent need for improved visibility and resilience of the supply chain. However, data-sharing limitations often hinder the achievement of comprehensive visibility between organisations or countries due to privacy, security, and regulatory concerns. Moreover, most existing research studies focused on individual firm- or product-level networks, overlooking the multifaceted interactions among diverse entities that characterise real-world supply chains, thus limiting a holistic understanding of supply chain dynamics. To address these challenges, we propose a novel approach that integrates Federated Learning (FL) and Graph Convolutional Neural Networks (GCNs) to enhance supply chain visibility through relationship prediction in supply chain knowledge graphs. FL enables collaborative model training across countries by facilitating information sharing without requiring raw data exchange, ensuring compliance with privacy regulations and maintaining data security. GCNs empower the framework to capture intricate relational patterns within knowledge graphs, enabling accurate link prediction to uncover hidden connections and provide comprehensive insights into supply chain networks. Experimental results validate the effectiveness of the proposed approach, demonstrating its ability to accurately predict relationships within country-level supply chain knowledge graphs. This enhanced visibility supports actionable insights, facilitates proactive risk management, and contributes to the development of resilient and adaptive supply chain strategies, ensuring that supply chains are better equipped to navigate the complexities of the global economy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07231v1</guid>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ge Zheng, Alexandra Brintrup</dc:creator>
    </item>
    <item>
      <title>Early signs of stuck pipe detection based on Crossformer</title>
      <link>https://arxiv.org/abs/2503.07440</link>
      <description>arXiv:2503.07440v1 Announce Type: new 
Abstract: Stuck pipe incidents are one of the major challenges in drilling engineering,leading to massive time loss and additional costs.To address the limitations of insufficient long sequence modeling capability,the difficulty in accurately establishing warning threshold,and the lack of model interpretability in existing methods,we utilize Crossformer for early signs of detection indicating potential stuck events in order to provide guidance for on-site drilling engineers and prevent stuck pipe incidents.The sliding window technique is integrated into Crossformer to allow it to output and display longer outputs,the improved Crossformer model is trained using normal time series drilling data to generate predictions for various parameters at each time step.The relative reconstruction error of model is regard as the risk of stuck pipe,thereby considering data that the model can't predict as anomalies,which represent the early signs of stuck pipe incidents.The multi-step prediction capability of Crossformer and relative reconstruction error are combined to assess stuck pipe risk at each time step in advance.We partition the reconstruction error into modeling error and error due to anomalous data fluctuations,furthermore,the dynamic warning threshold and warning time for stuck pipe incidents are determined using the probability density function of reconstruction errors from normal drilling data.The results indicate that our method can effectively detect early signs of stuck pipe incidents during the drilling process.Crossformer exhibits superior modeling and predictive capabilities compared with other deep learning models.Transformer-based models with multi-step prediction capability are more suitable for stuck pipe prediction compared to the current single-step prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07440v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Cao, Yu Song, Jin Yang, Lei Li</dc:creator>
    </item>
    <item>
      <title>Simultaneous Energy Harvesting and Bearing Fault Detection using Piezoelectric Cantilevers</title>
      <link>https://arxiv.org/abs/2503.07462</link>
      <description>arXiv:2503.07462v1 Announce Type: new 
Abstract: Bearings are critical components in industrial machinery, yet their vulnerability to faults often leads to costly breakdowns. Conventional fault detection methods depend on continuous, high-frequency vibration sensing, digitising, and wireless transmission to the cloud-an approach that significantly drains the limited energy reserves of battery-powered sensors, accelerating their depletion and increasing maintenance costs. This work proposes a fundamentally different approach: rather than using instantaneous vibration data, we employ piezoelectric energy harvesters (PEHs) tuned to specific frequencies and leverage the cumulative harvested energy over time as the key diagnostic feature. By directly utilising the energy generated from the machinery's vibrations, we eliminate the need for frequent analog-to-digital conversions and data transmission, thereby reducing energy consumption at the sensor node and extending its operational lifetime. To validate this approach, we use a numerical PEH model and publicly available acceleration datasets, examining various PEH designs with different natural frequencies. We also consider the influence of the classification algorithm, the number of devices, and the observation window duration. The results demonstrate that the harvested energy reliably indicates bearing faults across a range of conditions and severities. By converting vibration energy into both a power source and a diagnostic feature, our solution offers a more sustainable, low-maintenance strategy for fault detection in smart machinery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07462v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>P. Peralta-Braz, M. M. Alamdari, C. T. Chou, M. Hassan, E. Atroshchenko</dc:creator>
    </item>
    <item>
      <title>zScore: A Universal Decentralised Reputation System for the Blockchain Economy</title>
      <link>https://arxiv.org/abs/2503.05718</link>
      <description>arXiv:2503.05718v1 Announce Type: cross 
Abstract: Modern society functions on trust. The onchain economy, however, is built on the founding principles of trustless peer-to-peer interactions in an adversarial environment without a centralised body of trust and needs a verifiable system to quantify credibility to minimise bad economic activity. We provide a robust framework titled zScore, a core primitive for reputation derived from a wallet's onchain behaviour using state-of-the-art AI neural network models combined with real-world credentials ported onchain through zkTLS. The initial results tested on retroactive data from lending protocols establish a strong correlation between a good zScore and healthy borrowing and repayment behaviour, making it a robust and decentralised alibi for creditworthiness; we highlight significant improvements from previous attempts by protocols like Cred showcasing its robustness. We also present a list of possible applications of our system in Section 5, thereby establishing its utility in rewarding actual value creation while filtering noise and suspicious activity and flagging malicious behaviour by bad actors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05718v1</guid>
      <category>cs.CY</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Himanshu Udupi, Ashutosh Sahoo, Akshay S. P., Gurukiran S., Parag Paul, Petrus C. Martens</dc:creator>
    </item>
    <item>
      <title>Mitigating Blockchain extractable value (BEV) threats by Distributed Transaction Sequencing in Blockchains</title>
      <link>https://arxiv.org/abs/2503.06279</link>
      <description>arXiv:2503.06279v1 Announce Type: cross 
Abstract: The rapid growth of Blockchain and Decentralized Finance (DeFi) has introduced new challenges and vulnerabilities that threaten the integrity and efficiency of the ecosystem. This study identifies critical issues such as Transaction Order Dependence (TOD), Blockchain Extractable Value (BEV), and Transaction Importance Diversity (TID), which collectively undermine the fairness and security of DeFi systems. BEV-related activities, including Sandwich attacks, Liquidations, and Transaction Replay, have emerged as significant threats, collectively generating $540.54 million in losses over 32 months across 11,289 addresses, involving 49,691 cryptocurrencies and 60,830 on-chain markets. These attacks exploit transaction mechanics to manipulate asset prices and extract value at the expense of other participants, with Sandwich attacks being particularly impactful. Additionally, the growing adoption of Blockchain in traditional finance highlights the challenge of TID, where high transaction volumes can strain systems and compromise time-sensitive operations. To address these pressing issues, we propose a novel Distributed Transaction Sequencing Strategy (DTSS), which combines forking mechanisms and the Analytic Hierarchy Process (AHP) to enforce fair and transparent transaction ordering in a decentralized manner. Our approach is further enhanced by an optimization framework and the introduction of the Normalized Allocation Disparity Metric (NADM), which ensures optimal parameter selection for transaction prioritization. Experimental evaluations demonstrate that DTSS effectively mitigates BEV risks, enhances transaction fairness, and significantly improves the security and transparency of DeFi ecosystems. This work is essential for protecting the future of decentralized finance and promoting its integration into global financial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06279v1</guid>
      <category>cs.CR</category>
      <category>cs.CE</category>
      <category>cs.DC</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiongfei Zhao, Hou-Wan Long, Zhengzhe Li, Jiangchuan Liu, Yain-Whar Si</dc:creator>
    </item>
    <item>
      <title>Personalized Class Incremental Context-Aware Food Classification for Food Intake Monitoring Systems</title>
      <link>https://arxiv.org/abs/2503.06647</link>
      <description>arXiv:2503.06647v1 Announce Type: cross 
Abstract: Accurate food intake monitoring is crucial for maintaining a healthy diet and preventing nutrition-related diseases. With the diverse range of foods consumed across various cultures, classic food classification models have limitations due to their reliance on fixed-sized food datasets. Studies show that people consume only a small range of foods across the existing ones, each consuming a unique set of foods. Existing class-incremental models have low accuracy for the new classes and lack personalization. This paper introduces a personalized, class-incremental food classification model designed to overcome these challenges and improve the performance of food intake monitoring systems. Our approach adapts itself to the new array of food classes, maintaining applicability and accuracy, both for new and existing classes by using personalization. Our model's primary focus is personalization, which improves classification accuracy by prioritizing a subset of foods based on an individual's eating habits, including meal frequency, times, and locations. A modified version of DSN is utilized to expand on the appearance of new food classes. Additionally, we propose a comprehensive framework that integrates this model into a food intake monitoring system. This system analyzes meal images provided by users, makes use of a smart scale to estimate food weight, utilizes a nutrient content database to calculate the amount of each macro-nutrient, and creates a dietary user profile through a mobile application. Finally, experimental evaluations on two new benchmark datasets FOOD101-Personal and VFN-Personal, personalized versions of well-known datasets for food classification, are conducted to demonstrate the effectiveness of our model in improving the classification accuracy of both new and existing classes, addressing the limitations of both conventional and class-incremental food classification models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06647v1</guid>
      <category>cs.CV</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hassan Kazemi Tehrani, Jun Cai, Abbas Yekanlou, Sylvia Santosa</dc:creator>
    </item>
    <item>
      <title>Effect of Selection Format on LLM Performance</title>
      <link>https://arxiv.org/abs/2503.06926</link>
      <description>arXiv:2503.06926v1 Announce Type: cross 
Abstract: This paper investigates a critical aspect of large language model (LLM) performance: the optimal formatting of classification task options in prompts. Through an extensive experimental study, we compared two selection formats -- bullet points and plain English -- to determine their impact on model performance. Our findings suggest that presenting options via bullet points generally yields better results, although there are some exceptions. Furthermore, our research highlights the need for continued exploration of option formatting to drive further improvements in model performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06926v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Han, Yucheng Wu, Jeffrey Willard</dc:creator>
    </item>
    <item>
      <title>A semi-Lagrangian method for the direct numerical simulation of crystallization and precipitation at the pore scale</title>
      <link>https://arxiv.org/abs/2409.05449</link>
      <description>arXiv:2409.05449v2 Announce Type: replace 
Abstract: This article introduces a new efficient particle method for the numerical simulation of crystallization and precipitation at the pore scale of real rock geometries extracted by X-Ray tomography. It is based on the coupling between superficial velocity models of porous media, Lagrangian description of chemistry using Transition-State-Theory, involving underlying grids. Its ability to successfully compute dissolution process has been established in the past and is presently generalized to precipitation and crystallization by means of adsorption modeling. Numerical simulations of mineral CO2 trapping are provided, showing evidence of clogging/non-clogging regimes, and one of the main results is the introduction of a new non-dimensional number needed for this characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05449v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3389/feart.2025.1493305</arxiv:DOI>
      <arxiv:journal_reference>Frontiers in Earth Science 13:1493305 (2025)</arxiv:journal_reference>
      <dc:creator>Sarah Perez, Jean-Matthieu Etancelin, Philippe Poncet</dc:creator>
    </item>
    <item>
      <title>A High-Resolution, US-scale Digital Similar of Interacting Livestock, Wild Birds, and Human Ecosystems with Applications to Multi-host Epidemic Spread</title>
      <link>https://arxiv.org/abs/2411.01386</link>
      <description>arXiv:2411.01386v2 Announce Type: replace 
Abstract: One Health issues, such as the spread of highly pathogenic avian influenza~(HPAI), present significant challenges at the human-animal-environmental interface. Recent H5N1 outbreaks underscore the need for comprehensive modeling efforts that capture the complex interactions between various entities in these interconnected ecosystems. To support such efforts, we develop a methodology to construct a synthetic spatiotemporal gridded dataset of livestock production and processing, human population, and wild birds for the contiguous United States, called a \emph{digital similar}. This representation is a result of fusing diverse datasets using statistical and optimization techniques, followed by extensive verification and validation. The livestock component includes farm-level representations of four major livestock types -- cattle, poultry, swine, and sheep -- including further categorization into subtypes such as dairy cows, beef cows, chickens, turkeys, ducks, etc. Weekly abundance data for wild bird species identified in the transmission of avian influenza are included. Gridded distributions of the human population, along with demographic and occupational features, capture the placement of agricultural workers and the general population. We demonstrate how the digital similar can be applied to evaluate spillover risk to dairy cows and poultry from wild bird population, then validate these results using historical H5N1 incidences. The resulting subtype-specific spatiotemporal risk maps identify hotspots of high risk from H5N1 infected wild bird population to dairy cattle and poultry operations, thus guiding surveillance efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01386v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhijin Adiga, Ayush Chopra, Mandy L. Wilson, S. S. Ravi, Dawen Xie, Samarth Swarup, Bryan Lewis, John Barnes, Ramesh Raskar, Madhav V. Marathe</dc:creator>
    </item>
    <item>
      <title>Extending the Lattice Boltzmann Method to Non-linear Solid Mechanics</title>
      <link>https://arxiv.org/abs/2502.00920</link>
      <description>arXiv:2502.00920v2 Announce Type: replace 
Abstract: This work outlines a Lattice Boltzmann Method (LBM) for geometrically and constitutively nonlinear solid mechanics to simulate large deformations under dynamic loading conditions. The method utilizes the moment chain approach, where the non-linear constitutive law is incorporated via a forcing term. Stress and deformation measures are expressed in the reference configuration. Finite difference schemes are employed for gradient and divergence computations, and Neumann- and Dirichlet-type boundary conditions are introduced.
  Numerical studies are performed to assess the proposed method and illustrate its capabilities. Benchmark tests for weakly dynamic uniaxial tension and simple shear across a range of Poisson's ratios demonstrate the feasibility of the scheme and serve as validation of the implementation. Furthermore, a dynamic test case involving the propagation of bending waves in a cantilever beam highlights the potential of the method to model complex dynamic phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00920v2</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henning M\"uller, Erik Faust, Alexander Schl\"uter, Ralf M\"uller</dc:creator>
    </item>
    <item>
      <title>A high-order augmented basis positivity-preserving discontinuous Galerkin method for a Linear Hyperbolic Equation</title>
      <link>https://arxiv.org/abs/2503.04039</link>
      <description>arXiv:2503.04039v2 Announce Type: replace 
Abstract: This paper designs a high-order positivity-preserving discontinuous Galerkin (DG) scheme for a linear hyperbolic equation. The scheme relies on augmenting the standard polynomial DG spaces with additional basis functions. The purpose of these augmented basis functions is to ensure the preservation of a positive cell average for the unmodulated DG solution. As such, the simple Zhang and Shu limiter~\cite{zhang2010maximum} can be applied with no loss of accuracy for smooth solutions, and the cell average remains unaltered. A key feature of the proposed scheme is its implicit generation of suitable augmented basis functions. Nonlinear optimization facilitates the design of these augmented basis functions. Several benchmarks and computational studies demonstrate that the method works well in two and three dimensions. \keywords{discontinuous Galerkin \and High-order \and Positivity-preserving</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04039v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maurice S. Fabien</dc:creator>
    </item>
    <item>
      <title>Deep Convolutional Neural Networks for Short-Term Multi-Energy Demand Prediction of Integrated Energy Systems</title>
      <link>https://arxiv.org/abs/2312.15497</link>
      <description>arXiv:2312.15497v2 Announce Type: replace-cross 
Abstract: Forecasting power consumptions of integrated electrical, heat or gas network systems is essential in order to operate more efficiently the whole energy network. Multi-energy systems are increasingly seen as a key component of future energy systems, and a valuable source of flexibility, which can significantly contribute to a cleaner and more sustainable whole energy system. Therefore, there is a stringent need for developing novel and performant models for forecasting multi-energy demand of integrated energy systems, which to account for the different types of interacting energy vectors and of the coupling between them. Previous efforts in demand forecasting focused mainly on the single electrical power consumption or, more recently, on the single heat or gas power consumptions. In order to address this gap, in this paper six novel prediction models based on Convolutional Neural Networks (CNNs) are developed, for either individual or joint prediction of multi-energy power consumptions: the single input/single output CNN model with determining the optimum number of epochs (CNN_1), the multiple input/single output CNN model (CNN_2), the single input/ single output CNN model with training/validation/testing datasets (CNN_3), the joint prediction CNN model (CNN_4), the multiple-building input/output CNN model (CNN_5) and the federated learning CNN model (CNN_6). All six novel CNN models are applied in a comprehensive manner on a novel integrated electrical, heat and gas network system, which only recently has started to be used for forecasting. The forecast horizon is short-term (next half an hour) and all the predictions results are evaluated in terms of the Signal to Noise Ratio (SNR) and the Normalized Root Mean Square Error (NRMSE), while the Mean Absolute Percentage Error (MAPE) is used for comparison purposes with other existent results from literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15497v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corneliu Arsene, Alessandra Parisio</dc:creator>
    </item>
    <item>
      <title>NLP-enabled Trajectory Map-matching in Urban Road Networks using a Transformer-based Encoder-decoder</title>
      <link>https://arxiv.org/abs/2404.12460</link>
      <description>arXiv:2404.12460v4 Announce Type: replace-cross 
Abstract: Vehicular trajectory data from geolocation telematics is vital for analyzing urban mobility patterns. Map-matching aligns noisy, sparsely sampled GPS trajectories with digital road maps to reconstruct accurate vehicle paths. Traditional methods rely on geometric proximity, topology, and shortest-path heuristics, but they overlook two key factors: (1) drivers may prefer routes based on local road characteristics rather than shortest paths, revealing learnable shared preferences, and (2) GPS noise varies spatially due to multipath effects. These factors can reduce the effectiveness of conventional methods in complex scenarios and increase the effort required for heuristic-based implementations. This study introduces a data-driven, deep learning-based map-matching framework, formulating the task as machine translation, inspired by NLP. Specifically, a transformer-based encoder-decoder model learns contextual representations of noisy GPS points to infer trajectory behavior and road structures in an end-to-end manner. Trained on large-scale trajectory data, the method improves path estimation accuracy. Experiments on synthetic trajectories show that this approach outperforms conventional methods by integrating contextual awareness. Evaluation on real-world GPS traces from Manhattan, New York, achieves 75% accuracy in reconstructing navigated routes. These results highlight the effectiveness of transformers in capturing drivers' trajectory behaviors, spatial dependencies, and noise patterns, offering a scalable, robust solution for map-matching. This work contributes to advancing trajectory-driven foundation models for geospatial modeling and urban mobility applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12460v4</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sevin Mohammadi, Andrew W. Smyth</dc:creator>
    </item>
    <item>
      <title>BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments</title>
      <link>https://arxiv.org/abs/2405.17631</link>
      <description>arXiv:2405.17631v3 Announce Type: replace-cross 
Abstract: Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17631v3</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.MA</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yusuf Roohani, Andrew Lee, Qian Huang, Jian Vora, Zachary Steinhart, Kexin Huang, Alexander Marson, Percy Liang, Jure Leskovec</dc:creator>
    </item>
    <item>
      <title>A Survey on Point-of-Interest Recommendation: Models, Architectures, and Security</title>
      <link>https://arxiv.org/abs/2410.02191</link>
      <description>arXiv:2410.02191v2 Announce Type: replace-cross 
Abstract: The widespread adoption of smartphones and Location-Based Social Networks has led to a massive influx of spatio-temporal data, creating unparalleled opportunities for enhancing Point-of-Interest (POI) recommendation systems. These advanced POI systems are crucial for enriching user experiences, enabling personalized interactions, and optimizing decision-making processes in the digital landscape. However, existing surveys tend to focus on traditional approaches and few of them delve into cutting-edge developments, emerging architectures, as well as security considerations in POI recommendations. To address this gap, our survey stands out by offering a comprehensive, up-to-date review of POI recommendation systems, covering advancements in models, architectures, and security aspects. We systematically examine the transition from traditional models to advanced techniques such as large language models. Additionally, we explore the architectural evolution from centralized to decentralized and federated learning systems, highlighting the improvements in scalability and privacy. Furthermore, we address the increasing importance of security, examining potential vulnerabilities and privacy-preserving approaches. Our taxonomy provides a structured overview of the current state of POI recommendation, while we also identify promising directions for future research in this rapidly advancing field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02191v2</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>TKDE 2025</arxiv:journal_reference>
      <dc:creator>Qianru Zhang, Peng Yang, Junliang Yu, Haixin Wang, Xingwei He, Siu-Ming Yiu, Hongzhi Yin</dc:creator>
    </item>
    <item>
      <title>Learning Object Properties Using Robot Proprioception via Differentiable Robot-Object Interaction</title>
      <link>https://arxiv.org/abs/2410.03920</link>
      <description>arXiv:2410.03920v2 Announce Type: replace-cross 
Abstract: Differentiable simulation has become a powerful tool for system identification. While prior work has focused on identifying robot properties using robot-specific data or object properties using object-specific data, our approach calibrates object properties by using information from the robot, without relying on data from the object itself. Specifically, we utilize robot joint encoder information, which is commonly available in standard robotic systems. Our key observation is that by analyzing the robot's reactions to manipulated objects, we can infer properties of those objects, such as inertia and softness. Leveraging this insight, we develop differentiable simulations of robot-object interactions to inversely identify the properties of the manipulated objects. Our approach relies solely on proprioception -- the robot's internal sensing capabilities -- and does not require external measurement tools or vision-based tracking systems. This general method is applicable to any articulated robot and requires only joint position information. We demonstrate the effectiveness of our method on a low-cost robotic platform, achieving accurate mass and elastic modulus estimations of manipulated objects with just a few seconds of computation on a laptop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03920v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Yichen Chen, Chao Liu, Pingchuan Ma, John Eastman, Daniela Rus, Dylan Randle, Yuri Ivanov, Wojciech Matusik</dc:creator>
    </item>
    <item>
      <title>Solving Differential Equations with Constrained Learning</title>
      <link>https://arxiv.org/abs/2410.22796</link>
      <description>arXiv:2410.22796v2 Announce Type: replace-cross 
Abstract: (Partial) differential equations (PDEs) are fundamental tools for describing natural phenomena, making their solution crucial in science and engineering. While traditional methods, such as the finite element method, provide reliable solutions, their accuracy is often tied to the use of computationally intensive fine meshes. Moreover, they do not naturally account for measurements or prior solutions, and any change in the problem parameters requires results to be fully recomputed. Neural network-based approaches, such as physics-informed neural networks and neural operators, offer a mesh-free alternative by directly fitting those models to the PDE solution. They can also integrate prior knowledge and tackle entire families of PDEs by simply aggregating additional training losses. Nevertheless, they are highly sensitive to hyperparameters such as collocation points and the weights associated with each loss. This paper addresses these challenges by developing a science-constrained learning (SCL) framework. It demonstrates that finding a (weak) solution of a PDE is equivalent to solving a constrained learning problem with worst-case losses. This explains the limitations of previous methods that minimize the expected value of aggregated losses. SCL also organically integrates structural constraints (e.g., invariances) and (partial) measurements or known solutions. The resulting constrained learning problems can be tackled using a practical algorithm that yields accurate solutions across a variety of PDEs, neural network architectures, and prior knowledge levels without extensive hyperparameter tuning and sometimes even at a lower computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22796v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Viggo Moro, Luiz F. O. Chamon</dc:creator>
    </item>
    <item>
      <title>IVE: Enhanced Probabilistic Forecasting of Intraday Volume Ratio with Transformers</title>
      <link>https://arxiv.org/abs/2411.10956</link>
      <description>arXiv:2411.10956v2 Announce Type: replace-cross 
Abstract: This paper presents a new approach to volume ratio prediction in financial markets, specifically targeting the execution of Volume-Weighted Average Price (VWAP) strategies. Recognizing the importance of accurate volume profile forecasting, our research leverages the Transformer architecture to predict intraday volume ratio at a one-minute scale. We diverge from prior models that use log-transformed volume or turnover rates, instead opting for a prediction model that accounts for the intraday volume ratio's high variability, stabilized via log-normal transformation. Our input data incorporates not only the statistical properties of volume but also external volume-related features, absolute time information, and stock-specific characteristics to enhance prediction accuracy. The model structure includes an encoder-decoder Transformer architecture with a distribution head for greedy sampling, optimizing performance on high-liquidity stocks across both Korean and American markets. We extend the capabilities of our model beyond point prediction by introducing probabilistic forecasting that captures the mean and standard deviation of volume ratios, enabling the anticipation of significant intraday volume spikes. Furthermore, an agent with a simple trading logic demonstrates the practical application of our model through live trading tests in the Korean market, outperforming VWAP benchmarks over a period of two and a half months. Our findings underscore the potential of Transformer-based probabilistic models for volume ratio prediction and pave the way for future research advancements in this domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10956v2</guid>
      <category>q-fin.CP</category>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hanwool Lee, Heehwan Park</dc:creator>
    </item>
    <item>
      <title>Data-driven inventory management for new products: An adjusted Dyna-$Q$ approach with transfer learning</title>
      <link>https://arxiv.org/abs/2501.08109</link>
      <description>arXiv:2501.08109v3 Announce Type: replace-cross 
Abstract: In this paper, we propose a novel reinforcement learning algorithm for inventory management of newly launched products with no historical demand information. The algorithm follows the classic Dyna-$Q$ structure, balancing the model-free and model-based approaches, while accelerating the training process of Dyna-$Q$ and mitigating the model discrepancy generated by the model-based feedback. Based on the idea of transfer learning, warm-start information from the demand data of existing similar products can be incorporated into the algorithm to further stabilize the early-stage training and reduce the variance of the estimated optimal policy. Our approach is validated through a case study of bakery inventory management with real data. The adjusted Dyna-$Q$ shows up to a 23.7\% reduction in average daily cost compared with $Q$-learning, and up to a 77.5\% reduction in training time within the same horizon compared with classic Dyna-$Q$. By using transfer learning, it can be found that the adjusted Dyna-$Q$ has the lowest total cost, lowest variance in total cost, and relatively low shortage percentages among all the benchmarking algorithms under a 30-day testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08109v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 11 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinye Qu, Longxiao Liu, Wenjie Huang</dc:creator>
    </item>
  </channel>
</rss>
