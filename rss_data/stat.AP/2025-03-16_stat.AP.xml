<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 17 Mar 2025 04:00:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quantifying sleep apnea heterogeneity using hierarchical Bayesian modeling</title>
      <link>https://arxiv.org/abs/2503.11599</link>
      <description>arXiv:2503.11599v1 Announce Type: new 
Abstract: Obstructive Sleep Apnea (OSA) is a breathing disorder during sleep that affects millions of people worldwide. The diagnosis of OSA often occurs through an overnight polysomnogram (PSG) sleep study that generates a massive amount of physiological data. However, despite the evidence of substantial heterogeneity in the expression and symptoms of OSA, diagnosis and scientific analysis of severity typically focus on a single summary statistic, the Apnea-Hypopnea Index (AHI). To address the limitations inherent in such analyses, we propose a hierarchical Bayesian modeling approach to analyze PSG data. Our approach produces an interpretable vector of random effect parameters for each patient that govern sleep-stage dynamics, rates of OSA events, and impacts of OSA events on subsequent sleep-stage dynamics. We propose a novel approach for using these random effects to produce a Bayes optimal cluster of patients under K-means loss. We use the proposed approach to analyze data from the APPLES study. This analysis produces clinically interesting groups of patients with sleep apnea and a novel finding of an association between OSA expression and cognitive performance that is missed by an AHI-based analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11599v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glenn Palmer, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Bayes factors for partial correlation</title>
      <link>https://arxiv.org/abs/2503.10787</link>
      <description>arXiv:2503.10787v1 Announce Type: cross 
Abstract: Partial correlation coefficients are widely applied in the social sciences to evaluate the relationship between two variables after accounting for the influence of others. In this article, we present Bayes Factor Functions (BFFs) for assessing the presence of partial correlation. BFFs represent Bayes factors derived from test statistics and are expressed as functions of a standardized effect size. While traditional frequentist methods based on $p$-values have been criticized for their inability to provide cumulative evidence in favor of the true hypothesis, Bayesian approaches are often challenged due to their computational demands and sensitivity to prior distributions. BFFs overcome these limitations and offer summaries of hypothesis tests as alternative hypotheses are varied over a range of prior distributions on standardized effects. They also enable the integration of evidence across multiple studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10787v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptati Datta, Valen E. Johnson</dc:creator>
    </item>
    <item>
      <title>Surviving the frailty of time to event analysis in massive datasets with Generalized Additive Models (and the help of Simon Laplace)</title>
      <link>https://arxiv.org/abs/2503.10823</link>
      <description>arXiv:2503.10823v1 Announce Type: cross 
Abstract: Analyses of time to event datasets have been invariably based on the Cox proportional hazards model (PHM). Reformulations of the PHM as a Poisson Generalized Additive Model (GAM) or as a Generalized Linear Mixed Model (GLMM) have been proposed in the literature, aiming to increase the flexibility of the PHM and allow its use in situations in which complex spatiotemporal relationships have to be taken into account when modeling survival. In this report, we provide a unified framework for considering these previous attempts and consider the implementation in software for GAM and GLMM in the R programming language. The connection between GAM/GLMM and the PHM is leveraged to provide computationally efficient implementations for a subclass of survival models that incorporate individual random effects ('frailty models'). Frailty models provide a unified method to address repeated events, correlated outcomes and also time varying visitation schedules when analyzing Electronic Health Record data. However the current implementation of frailty models in software facilities for the Cox model does not scale because of long computation times; conversely the direct implementation of individual random effects in GAM/GLMM software does not scale well with memory usage. We propose a two stage method for survival models with frailty based on the Laplace approximation. Using a D-optimal experimental design to simulate the performance of the proposed method across simulated datasets we illustrate that the proposed method can circumvent the limitations of existing implementations, opening up the possibility to model datasets of hundred of thousands to million individuals using high end workstations from within R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10823v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos Argyropoulos (Division of Nephrology Department of Internal Medicine University of New Mexico, Clinical and Translational Sciences Center Health Sciences Center University of New Mexico), Hamza Mir (Division of Nephrology Department of Internal Medicine University of New Mexico), Maria-Eleni Roumelioti (Division of Nephrology Department of Internal Medicine University of New Mexico), Pablo Garcia (Division of Nephrology Department of Internal Medicine University of New Mexico)</dc:creator>
    </item>
    <item>
      <title>Configuration Design of Mechanical Assemblies using an Estimation of Distribution Algorithm and Constraint Programming</title>
      <link>https://arxiv.org/abs/2503.11002</link>
      <description>arXiv:2503.11002v1 Announce Type: cross 
Abstract: A configuration design problem in mechanical engineering involves finding an optimal assembly of components and joints that realizes some desired performance criteria. Such a problem is a discrete, constrained, and black-box optimization problem. A novel method is developed to solve the problem by applying Bivariate Marginal Distribution Algorithm (BMDA) and constraint programming (CP). BMDA is a type of Estimation of Distribution Algorithm (EDA) that exploits the dependency knowledge learned between design variables without requiring too many fitness evaluations, which tend to be expensive for the current application. BMDA is extended with adaptive chi-square testing to identify dependencies and Gibbs sampling to generate new solutions. Also, repair operations based on CP are used to deal with infeasible solutions found during search. The method is applied to a vehicle suspension design problem and is found to be more effective in converging to good solutions than a genetic algorithm and other EDAs. These contributions are significant steps towards solving the difficult problem of configuration design in mechanical engineering with evolutionary computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11002v1</guid>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CEC.2019.8789944</arxiv:DOI>
      <arxiv:journal_reference>2019 IEEE Congress on Evolutionary Computation (CEC)</arxiv:journal_reference>
      <dc:creator>Hyunmin Cheong, Mehran Ebrahimi, Adrian Butscher, Francesco Iorio</dc:creator>
    </item>
    <item>
      <title>The pushed beta distribution and contaminated binary sampling</title>
      <link>https://arxiv.org/abs/2503.11128</link>
      <description>arXiv:2503.11128v1 Announce Type: cross 
Abstract: We examine a generalisation of the beta distribution that we call the pushed beta distribution. This is a continuous univariate distribution on the unit interval which generalises the beta distribution by "pushing" the density in a particular direction using an additional multiplicative term in the density kernel. We examine the properties of this distribution and compare it to the beta distribution. We also examine the use of this distribution in contaminated binary sampling using Bayesian inference. We find that this distribution arises as the appropriate posterior distribution for inference in certain kinds of contaminated binary models. We derive a broad range of properties of the distribution and we also establish some computational methods to compute various functions for the distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11128v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ben O'Neill</dc:creator>
    </item>
    <item>
      <title>CRPS-Based Targeted Sequential Design with Application in Chemical Space</title>
      <link>https://arxiv.org/abs/2503.11250</link>
      <description>arXiv:2503.11250v1 Announce Type: cross 
Abstract: Sequential design of real and computer experiments via Gaussian Process (GP) models has proven useful for parsimonious, goal-oriented data acquisition purposes. In this work, we focus on acquisition strategies for a GP model that needs to be accurate within a predefined range of the response of interest. Such an approach is useful in various fields including synthetic chemistry, where finding molecules with particular properties is essential for developing useful materials and effective medications. GP modeling and sequential design of experiments have been successfully applied to a plethora of domains, including molecule research. Our main contribution here is to use the threshold-weighted Continuous Ranked Probability Score (CRPS) as a basic building block for acquisition functions employed within sequential design. We study pointwise and integral criteria relying on two different weighting measures and benchmark them against competitors, demonstrating improved performance with respect to considered goals. The resulting acquisition strategies are applicable to a wide range of fields and pave the way to further developing sequential design relying on scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11250v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Friedli, Ath\'ena\"is Gautier, Anna Broccard, David Ginsbourger</dc:creator>
    </item>
    <item>
      <title>Data-Driven Construction of Age-Structured Contact Networks</title>
      <link>https://arxiv.org/abs/2503.11527</link>
      <description>arXiv:2503.11527v1 Announce Type: cross 
Abstract: Capturing the structure of a population and characterising contacts within the population are key to reliable projections of infectious disease. Two main elements of population structure -- contact heterogeneity and age -- have been repeatedly demonstrated to be key in infection dynamics, yet are rarely combined. Regarding individuals as nodes and contacts as edges within a network provides a powerful and intuitive method to fully realise this population structure. While there are a few key examples of contact networks being measured explicitly, in general we need to construct the appropriate networks from individual-level data. Here, using data from social contact surveys, we develop a generic and robust algorithm to generate an extrapolated network that preserves both age-structured mixing and heterogeneity in the number of contacts. We then use these networks to simulate the spread of infection through the population, constrained to have a given basic reproduction number ($R_0$) and hence a given early growth rate. Given the over-dominant role that highly connected nodes (`superspreaders') would otherwise play in early dynamics, we scale transmission by the average duration of contacts, providing a better match to surveillance data for numbers of secondary cases. This network-based model shows that, for COVID-like parameters, including both heterogeneity and age-structure reduces both peak height and epidemic size compared to models that ignore heterogeneity. Our robust methodology therefore allows for the inclusion of the full wealth of data commonly collected by surveys but frequently overlooked to be incorporated into more realistic transmission models of infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11527v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Murray Kearney, Emma L. Davis, Matt J. Keeling</dc:creator>
    </item>
    <item>
      <title>Model-free Sign Estimation for High-Throughput Screenings</title>
      <link>https://arxiv.org/abs/2208.01745</link>
      <description>arXiv:2208.01745v4 Announce Type: replace 
Abstract: In high-throughput screenings, it is common to estimate the effects of many treatments using a small number of independent trials of each. Because little is known about the distributional properties of the measurements from these trials, it is challenging to identify plausible assumptions that can serve as a basis for inferential statistics in this setting. In this article, we develop a method based on minimal assumptions to infer signs of treatment effects (positive or negative). The proposed method controls the number of misestimated signs by using the number of sign disagreements between measurements of the same treatment as a proxy for the number of sign errors. In simulations, the proposed method compares favorably with the Benjamini-Hochberg procedure applied to invalid $p$-values, which is currently considered best practice for many high-throughput screenings. For real data from the L1000 cell-perturbation platform, the proposed method outperforms existing practices, which fail to control error at the nominal level in some cases and are needlessly conservative in others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01745v4</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackson Loper, Jeffrey Regier</dc:creator>
    </item>
    <item>
      <title>Scientific productivity as a random walk</title>
      <link>https://arxiv.org/abs/2309.04414</link>
      <description>arXiv:2309.04414v2 Announce Type: replace 
Abstract: The expectation that scientific productivity follows regular patterns over a career underpins many scholarly evaluations, including hiring, promotion and tenure, awards, and grant funding. However, recent studies of individual productivity patterns reveal a puzzle: on the one hand, the average number of papers published per year robustly follows the "canonical trajectory" of a rapid rise to an early peak followed by a gradual decline, but on the other hand, only about 20% of individual productivity trajectories follow this pattern. We resolve this puzzle by modeling scientific productivity as a parameterized random walk, showing that the canonical pattern can be explained as a decrease in the variance in changes to productivity in the early-to-mid career. By empirically characterizing the variable structure of 2,085 productivity trajectories of computer science faculty at 205 PhD-granting institutions, spanning 29,119 publications over 1980--2016, we (i) discover remarkably simple patterns in both early-career and year-to-year changes to productivity, and (ii) show that a random walk model of productivity both reproduces the canonical trajectory in the average productivity and captures much of the diversity of individual-level trajectories. These results highlight the fundamental role of a panoply of contingent factors in shaping individual scientific productivity, opening up new avenues for characterizing how systemic incentives and opportunities can be directed for aggregate effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04414v2</guid>
      <category>stat.AP</category>
      <category>cs.DL</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sam Zhang, Nicholas LaBerge, Samuel F. Way, Daniel B. Larremore, Aaron Clauset</dc:creator>
    </item>
    <item>
      <title>The Loser's Curse and the Critical Role of the Utility Function</title>
      <link>https://arxiv.org/abs/2411.10400</link>
      <description>arXiv:2411.10400v3 Announce Type: replace 
Abstract: A longstanding question in the judgment and decision making literature is whether experts, even in high-stakes environments, exhibit the same cognitive biases observed in controlled experiments with inexperienced participants. In their seminal work, Massey and Thaler (2013) provide a notable example of bias and irrationality in expert decision making: general managers' behavior in the National Football League draft pick trade market. They argue that general managers systematically overvalue top draft picks, which generate less surplus value on average than later first-round picks, a phenomenon known as the loser's curse. Their conclusion hinges on the assumption that general managers should use expected surplus value as their utility function for evaluating draft picks. This assumption, however, is neither explicitly justified nor necessarily aligned with the strategic complexities of constructing a National Football League roster. In this paper, we challenge their framework by considering alternative utility functions, particularly those that emphasize the acquisition of transformational players--those capable of dramatically increasing a team's chances of winning the Super Bowl. Under a decision rule that prioritizes the probability of acquiring elite players, which we construct from a novel Bayesian hierarchical Beta regression model, general managers' draft trade behavior appears rational rather than systematically flawed. More broadly, our findings highlight the critical role of carefully specifying a utility function when evaluating the quality of decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10400v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan S. Brill, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Estimating the Causal Effect of Redlining on Present-day Air Pollution</title>
      <link>https://arxiv.org/abs/2501.16958</link>
      <description>arXiv:2501.16958v2 Announce Type: replace 
Abstract: Recent studies have shown associations between redlining policies (1935-1974) and present-day fine particulate matter (PM$_{2.5}$) and nitrogen dioxide (NO$_2$) air pollution concentrations. In this paper, we reevaluate these associations using spatial causal inference. Redlining policies enacted in the 1930s, so there is very limited documentation of pre-treatment covariates. Consequently, traditional methods fails to sufficiently account for unmeasured confounders, potentially biasing causal interpretations. By integrating historical redlining data with 2010 PM$_{2.5}$ and NO$_2$ concentrations, our study aims to discern whether a causal link exists. Our study addresses challenges with a novel spatial and non-spatial latent factor framework, using the unemployment rate, house rent and percentage of Black population in 1940 U.S. Census as proxies to reconstruct pre-treatment latent socio-economic status. We establish identification of a causal effect under broad assumptions, and use Bayesian Markov Chain Monte Carlo to quantify uncertainty. Our analysis indicates that historically redlined neighborhoods are exposed to notably higher NO$_2$ concentration. In contrast, the disparities in PM$_{2.5}$ between these neighborhoods are less pronounced. Among the cities analyzed, Los Angeles, CA, and Atlanta, GA, demonstrate the most significant effects for both NO$_2$ and PM$_{2.5}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16958v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaodan Zhou, Shu Yang, Brian J Reich</dc:creator>
    </item>
    <item>
      <title>Privacy Violations in Election Results</title>
      <link>https://arxiv.org/abs/2308.04100</link>
      <description>arXiv:2308.04100v5 Announce Type: replace-cross 
Abstract: After an election, should election officials release a copy of each anonymous ballot? Some policymakers have championed public disclosure to counter distrust, but others worry that it might undermine ballot secrecy. We introduce the term vote revelation to refer to the linkage of a vote on an anonymous ballot to the voter's name in the public voter file, and detail how such revelation could theoretically occur. Using the 2020 election in Maricopa County, Arizona, as a case study, we show that the release of individual ballot records would lead to no revelation of any vote choice for 99.83% of voters as compared to 99.95% under Maricopa's current practice of reporting aggregate results by precinct and method of voting. Further, revelation is overwhelmingly concentrated among the few voters who cast provisional ballots or federal-only ballots. We discuss the potential benefits of transparency, compare remedies to reduce or eliminate privacy violations, and highlight the privacy-transparency tradeoff inherent in all election reporting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.04100v5</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1126/sciadv.adt1512</arxiv:DOI>
      <arxiv:journal_reference>Science Advances (2025), vol 11, issue 11, adt1512</arxiv:journal_reference>
      <dc:creator>Shiro Kuriwaki, Jeffrey B. Lewis, Michael Morse</dc:creator>
    </item>
    <item>
      <title>Estimating Fold Changes from Partially Observed Outcomes with Applications in Microbial Metagenomics</title>
      <link>https://arxiv.org/abs/2402.05231</link>
      <description>arXiv:2402.05231v2 Announce Type: replace-cross 
Abstract: We consider the problem of estimating fold-changes in the expected value of a multivariate outcome observed with unknown sample-specific and category-specific perturbations. This challenge arises in high-throughput sequencing studies of the abundance of microbial taxa because microbes are systematically over- and under-detected relative to their true abundances. Our model admits a partially identifiable estimand, and we establish full identifiability by imposing interpretable parameter constraints. To reduce bias and guarantee the existence of estimators in the presence of sparse observations, we apply an asymptotically negligible and constraint-invariant penalty to our estimating function. We develop a fast coordinate descent algorithm for estimation, and an augmented Lagrangian algorithm for estimation under null hypotheses. We construct a model-robust score test and demonstrate valid inference even for small sample sizes and violated distributional assumptions. The flexibility of the approach and comparisons to related methods are illustrated through a meta-analysis of microbial associations with colorectal cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05231v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David S Clausen, Sarah Teichman, Amy D Willis</dc:creator>
    </item>
    <item>
      <title>Improving Pre-trained Self-Supervised Embeddings Through Effective Entropy Maximization</title>
      <link>https://arxiv.org/abs/2411.15931</link>
      <description>arXiv:2411.15931v2 Announce Type: replace-cross 
Abstract: A number of different architectures and loss functions have been applied to the problem of self-supervised learning (SSL), with the goal of developing embeddings that provide the best possible pre-training for as-yet-unknown, lightly supervised downstream tasks. One of these SSL criteria is to maximize the entropy of a set of embeddings in some compact space. But the goal of maximizing the embedding entropy often depends -- whether explicitly or implicitly -- upon high dimensional entropy estimates, which typically perform poorly in more than a few dimensions. In this paper, we motivate an effective entropy maximization criterion (E2MC), defined in terms of easy-to-estimate, low-dimensional constraints. We demonstrate that using it to continue training an already-trained SSL model for only a handful of epochs leads to a consistent and, in some cases, significant improvement in downstream performance. We perform careful ablation studies to show that the improved performance is due to the proposed add-on criterion. We also show that continued pre-training with alternative criteria does not lead to notable improvements, and in some cases, even degrades performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15931v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deep Chakraborty, Yann LeCun, Tim G. J. Rudner, Erik Learned-Miller</dc:creator>
    </item>
    <item>
      <title>Dilemmas and trade-offs in the diffusion of conventions</title>
      <link>https://arxiv.org/abs/2501.17300</link>
      <description>arXiv:2501.17300v2 Announce Type: replace-cross 
Abstract: Outside ideal settings, conventions are shaped by competing processes that can challenge the emergence of norms. This paper identifies three trade-offs challenging the diffusion of conventions: (I) the trade-off between the imperatives of social, sequential, and contextual consistency that individuals balance when choosing between conventions; (II) the competition between local and global coordination, depending on whether individuals coordinate their behavior via interactions throughout a social network or external factors transcending the network; and (III) the balance between decision optimality (e.g., collective satisfaction) and decision costs when collectives with conflicting preferences choose a convention. We develop a broadly applicable statistical physics framework for measuring each of these trade-offs, which we then apply to a sign convention in physics. Our method can recover the structure of the underlying coordination game, the networks of social interactions involved, and the processes through which conflicts are resolved in collaborations. We find that the purpose of conventions may exceed coordination, and that individual preferences towards conventions are concurrently shaped by cultural factors and multiple social networks. Additionally, we reveal the role of leadership in the resolution of conflicts. Finally, this work provides a generalization of Lewis' account of conventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17300v2</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Gautheron</dc:creator>
    </item>
  </channel>
</rss>
