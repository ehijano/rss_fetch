<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 29 Oct 2024 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Less is More: Efficient Time Series Dataset Condensation via Two-fold Modal Matching--Extended Version</title>
      <link>https://arxiv.org/abs/2410.20905</link>
      <description>arXiv:2410.20905v1 Announce Type: new 
Abstract: The expanding instrumentation of processes throughout society with sensors yields a proliferation of time series data that may in turn enable important applications, e.g., related to transportation infrastructures or power grids. Machine-learning based methods are increasingly being used to extract value from such data. We provide means of reducing the resulting considerable computational and data storage costs. We achieve this by providing means of condensing large time series datasets such that models trained on the condensed data achieve performance comparable to those trained on the original, large data. Specifically, we propose a time series dataset condensation framework, TimeDC, that employs two-fold modal matching, encompassing frequency matching and training trajectory matching. Thus, TimeDC performs time series feature extraction and decomposition-driven frequency matching to preserve complex temporal dependencies in the reduced time series. Further, TimeDC employs curriculum training trajectory matching to ensure effective and generalized time series dataset condensation. To avoid memory overflow and to reduce the cost of dataset condensation, the framework includes an expert buffer storing pre-computed expert trajectories. Extensive experiments on real data offer insight into the effectiveness and efficiency of the proposed solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20905v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hao Miao, Ziqiao Liu, Yan Zhao, Chenjuan Guo, Bin Yang, Kai Zheng, Christian S. Jensen</dc:creator>
    </item>
    <item>
      <title>Modeling and Monitoring of Indoor Populations using Sparse Positioning Data (Extension)</title>
      <link>https://arxiv.org/abs/2410.21142</link>
      <description>arXiv:2410.21142v1 Announce Type: new 
Abstract: In large venues like shopping malls and airports, knowledge on the indoor populations fuels applications such as business analytics, venue management, and safety control. In this work, we provide means of modeling populations in partitions of indoor space offline and of monitoring indoor populations continuously, by using indoor positioning data. However, the low-sampling rates of indoor positioning render the data temporally and spatially sparse, which in turn renders the offline capture of indoor populations challenging. It is even more challenging to continuously monitor indoor populations, as positioning data may be missing or not ready yet at the current moment. To address these challenges, we first enable probabilistic modeling of populations in indoor space partitions as Normal distributions. Based on that, we propose two learning-based estimators for on-the-fly prediction of population distributions. Leveraging the prediction-based schemes, we provide a unified continuous query processing framework for a type of query that enables continuous monitoring of populated partitions. The framework encompasses caching and result validity mechanisms to reduce cost and maintain monitoring effectiveness. Extensive experiments on two real data sets show that the proposed estimators are able to outperform the state-of-the-art alternatives and that the query processing framework is effective and efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21142v1</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiao Li, Huan Li, Hua Lu, Christian S. Jensen</dc:creator>
    </item>
    <item>
      <title>Differentially Private Learned Indexes</title>
      <link>https://arxiv.org/abs/2410.21164</link>
      <description>arXiv:2410.21164v1 Announce Type: new 
Abstract: In this paper, we address the problem of efficiently answering predicate queries on encrypted databases, those secured by Trusted Execution Environments (TEEs), which enable untrusted providers to process encrypted user data without revealing its contents. A common strategy in modern databases to accelerate predicate queries is the use of indexes, which map attribute values (keys) to their corresponding positions in a sorted data array. This allows for fast lookup and retrieval of data subsets that satisfy specific predicates. Unfortunately, indexes cannot be directly applied to encrypted databases due to strong data dependent leakages. Recent approaches apply differential privacy (DP) to construct noisy indexes that enable faster access to encrypted data while maintaining provable privacy guarantees. However, these methods often suffer from large storage costs, with index sizes typically scaling linearly with the key space. To address this challenge, we propose leveraging learned indexes, a trending technique that repurposes machine learning models as indexing structures, to build more compact DP indexes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21164v1</guid>
      <category>cs.DB</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianzhang Du, Tilak Mudgal, Rutvi Rahul Gadre, Yukui Luo, Chenghong Wang</dc:creator>
    </item>
    <item>
      <title>Geo-FuB: A Method for Constructing an Operator-Function Knowledge Base for Geospatial Code Generation Tasks Using Large Language Models</title>
      <link>https://arxiv.org/abs/2410.20975</link>
      <description>arXiv:2410.20975v1 Announce Type: cross 
Abstract: The rise of spatiotemporal data and the need for efficient geospatial modeling have spurred interest in automating these tasks with large language models (LLMs). However, general LLMs often generate errors in geospatial code due to a lack of domain-specific knowledge on functions and operators. To address this, a retrieval-augmented generation (RAG) approach, utilizing an external knowledge base of geospatial functions and operators, is proposed. This study introduces a framework to construct such a knowledge base, leveraging geospatial script semantics. The framework includes: Function Semantic Framework Construction (Geo-FuSE), Frequent Operator Combination Statistics (Geo-FuST), and Semantic Mapping (Geo-FuM). Techniques like Chain-of-Thought, TF-IDF, and the APRIORI algorithm are utilized to derive and align geospatial functions. An example knowledge base, Geo-FuB, built from 154,075 Google Earth Engine scripts, is available on GitHub. Evaluation metrics show a high accuracy, reaching 88.89% overall, with structural and semantic accuracies of 92.03% and 86.79% respectively. Geo-FuB's potential to optimize geospatial code generation through the RAG and fine-tuning paradigms is highlighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20975v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyang Hou, Anqi Zhao, Jianyuan Liang, Zhangxiao Shen, Huayi Wu</dc:creator>
    </item>
    <item>
      <title>Thunderbolt: Causal Concurrent Consensus and Execution</title>
      <link>https://arxiv.org/abs/2407.09409</link>
      <description>arXiv:2407.09409v2 Announce Type: replace 
Abstract: In the realm of blockchain, smart contracts have achieved widespread adoption due to their inherent programmability. However, smart contracts suffer from long execution delays, resulting from the analysis of the contract code. Consequently, the development of a system capable of facilitating high throughput and scalability holds paramount importance. Sharding represents a prevalent technique that enhances performance by horizontally scaling storage into individual shards. However, existing sharding methods rely on 2PC to handle cross-shard transactions through data locking, necessitating the provision of read/write sets in advance, which poses impractical challenges for smart contracts. This paper introduces Thunderbolt, a novel sharding architecture that integrates the Execute-Order-Validate(EOV) and Order-Execute(OE) models to manage single-shard transactions (Single-shard TXs) and cross-shard transactions (Cross-shard TXs) without coordinating the transactions by 2PC. Shards in Thunderbolt share all the replicas, and Thunderbolt assigns each replica as the shard submitter to propose the transactions. Each shard submitter employs the EOV model to execute Single-shard TXs concurrently while applying the OE model for executing Cross-shard TXs. We leverage the DAG-based protocol as the consensus protocol and modify the consensus logic to ensure correctness between Single-shard TXs and Cross-shard TXs. We implemented a concurrent executor to execute the Single-shard TXs locally to dynamically assign the scheduling order without any read/write set knowledge. Additionally, we introduce a novel shard reconfiguration to withstand censorship attacks by relocating the shards from the current DAG to a new DAG and rotating the shard submitters. Our comparison of the results on SmallBank with serial execution on Narwhal-Tusk revealed a remarkable 50x speedup with 64 replicas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09409v2</guid>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junchao Chen, Alberto Sonnino, Lefteris Kokoris-Kogias, Mohammad Sadoghi</dc:creator>
    </item>
    <item>
      <title>Large Language Models as Data Preprocessors</title>
      <link>https://arxiv.org/abs/2308.16361</link>
      <description>arXiv:2308.16361v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs), typified by OpenAI's GPT, have marked a significant advancement in artificial intelligence. Trained on vast amounts of text data, LLMs are capable of understanding and generating human-like text across a diverse range of topics. This study expands on the applications of LLMs, exploring their potential in data preprocessing, a critical stage in data mining and analytics applications. Aiming at tabular data, we delve into the applicability of state-of-the-art LLMs such as GPT-4 and GPT-4o for a series of preprocessing tasks, including error detection, data imputation, schema matching, and entity matching. Alongside showcasing the inherent capabilities of LLMs, we highlight their limitations, particularly in terms of computational expense and inefficiency. We propose an LLM-based framework for data preprocessing, which integrates cutting-edge prompt engineering techniques, coupled with traditional methods like contextualization and feature selection, to improve the performance and efficiency of these models. The effectiveness of LLMs in data preprocessing is evaluated through an experimental study spanning a variety of public datasets. GPT-4 emerged as a standout, achieving 100\% accuracy or F1 score on 4 of these datasets, suggesting LLMs' immense potential in these tasks. Despite certain limitations, our study underscores the promise of LLMs in this domain and anticipates future developments to overcome current hurdles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.16361v2</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haochen Zhang, Yuyang Dong, Chuan Xiao, Masafumi Oyamada</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence for the Internal Democracy of Political Parties</title>
      <link>https://arxiv.org/abs/2405.09529</link>
      <description>arXiv:2405.09529v2 Announce Type: replace-cross 
Abstract: The article argues that AI can enhance the measurement and implementation of democratic processes within political parties, known as Intra-Party Democracy (IPD). It identifies the limitations of traditional methods for measuring IPD, which often rely on formal parameters, self-reported data, and tools like surveys. Such limitations lead to the collection of partial data, rare updates, and significant demands on resources. To address these issues, the article suggests that specific data management and Machine Learning (ML) techniques, such as natural language processing and sentiment analysis, can improve the measurement (ML about) and practice (ML for) of IPD. The article concludes by considering some of the principal risks of ML for IPD, including concerns over data privacy, the potential for manipulation, and the dangers of overreliance on technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09529v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11023-024-09693-x</arxiv:DOI>
      <arxiv:journal_reference>Minds &amp; Machines 34, 36 (2024)</arxiv:journal_reference>
      <dc:creator>Claudio Novelli, Giuliano Formisano, Prathm Juneja, Giulia Sandri, Luciano Floridi</dc:creator>
    </item>
    <item>
      <title>Memento Filter: A Fast, Dynamic, and Robust Range Filter</title>
      <link>https://arxiv.org/abs/2408.05625</link>
      <description>arXiv:2408.05625v3 Announce Type: replace-cross 
Abstract: Range filters are probabilistic data structures that answer approximate range emptiness queries. They aid in avoiding processing empty range queries and have use cases in many application domains such as key-value stores and social web analytics. However, current range filter designs do not support dynamically changing and growing datasets. Moreover, several of these designs also exhibit impractically high false positive rates under correlated workloads, which are common in practice. These impediments restrict the applicability of range filters across a wide range of use cases. We introduce Memento filter, the first range filter to offer dynamicity, fast operations, and a robust false positive rate guarantee for any workload. Memento filter partitions the key universe and clusters its keys according to this partitioning. For each cluster, it stores a fingerprint and a list of key suffixes contiguously. The encoding of these lists makes them amenable to existing dynamic filter structures. Due to the well-defined one-to-one mapping from keys to suffixes, Memento filter supports inserts and deletes and can even expand to accommodate a growing dataset. We implement Memento filter on top of a Rank-and-Select Quotient filter and InfiniFilter and demonstrate that it achieves competitive false positive rates and performance with the state-of-the-art while also providing dynamicity. Due to its dynamicity, Memento filter is the first range filter applicable to B-Trees. We showcase this by integrating Memento filter into WiredTiger, a B-Tree-based key-value store. Memento filter doubles WiredTiger's range query throughput when 50% of the queries are empty while keeping all other cost metrics unharmed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05625v3</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3698820</arxiv:DOI>
      <dc:creator>Navid Eslami, Niv Dayan</dc:creator>
    </item>
  </channel>
</rss>
