<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Sep 2024 01:41:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Sparse Factor Analysis for Categorical Data with the Group-Sparse Generalized Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2409.11789</link>
      <description>arXiv:2409.11789v1 Announce Type: new 
Abstract: Correspondence analysis, multiple correspondence analysis and their discriminant counterparts (i.e., discriminant simple correspondence analysis and discriminant multiple correspondence analysis) are methods of choice for analyzing multivariate categorical data. In these methods, variables are integrated into optimal components computed as linear combinations whose weights are obtained from a generalized singular value decomposition (GSVD) that integrates specific metric constraints on the rows and columns of the original data matrix. The weights of the linear combinations are, in turn, used to interpret the components, and this interpretation is facilitated when components are 1) pairwise orthogonal and 2) when the values of the weights are either large or small but not intermediate-a pattern called a simple or a sparse structure. To obtain such simple configurations, the optimization problem solved by the GSVD is extended to include new constraints that implement component orthogonality and sparse weights. Because multiple correspondence analysis represents qualitative variables by a set of binary variables, an additional group constraint is added to the optimization problem in order to sparsify the whole set representing one qualitative variable. This new algorithm-called group-sparse GSVD (gsGSVD)-integrates these constraints via an iterative projection scheme onto the intersection of subspaces where each subspace implements a specific constraint. In this paper, we expose this new algorithm and show how it can be adapted to the sparsification of simple and multiple correspondence analysis, and illustrate its applications with the analysis of four different data sets-each illustrating the sparsification of a particular CA-based analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11789v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ju-Chi Yu (CAMH), Julie Le Borgne (RID-AGE, CHRU Lille), Anjali Krishnan (CUNY), Arnaud Gloaguen (CNRGH, JACOB), Cheng-Ta Yang (NCKU), Laura A Rabin (CUNY), Herv\'e Abdi (UT Dallas), Vincent Guillemot</dc:creator>
    </item>
    <item>
      <title>Asymptotics for conformal inference</title>
      <link>https://arxiv.org/abs/2409.12019</link>
      <description>arXiv:2409.12019v1 Announce Type: new 
Abstract: Conformal inference is a versatile tool for building prediction sets in regression or classification. In this paper, we consider the false coverage proportion (FCP) in a transductive setting with a calibration sample of n points and a test sample of m points. We identify the exact, distribution-free, asymptotic distribution of the FCP when both n and m tend to infinity. This shows in particular that FCP control can be achieved by using the well-known Kolmogorov distribution, and puts forward that the asymptotic variance is decreasing in the ratio n/m. We then provide a number of extensions by considering the novelty detection problem, weighted conformal inference and distribution shift between the calibration sample and the test sample. In particular, our asymptotical results allow to accurately quantify the asymptotical behavior of the errors when weighted conformal inference is used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12019v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ulysse Gazin</dc:creator>
    </item>
    <item>
      <title>Linear hypothesis testing in high-dimensional heteroscedastics via random integration</title>
      <link>https://arxiv.org/abs/2409.12066</link>
      <description>arXiv:2409.12066v1 Announce Type: new 
Abstract: In this paper, for the problem of heteroskedastic general linear hypothesis testing (GLHT) in high-dimensional settings, we propose a random integration method based on the reference L2-norm to deal with such problems. The asymptotic properties of the test statistic can be obtained under the null hypothesis when the relationship between data dimensions and sample size is not specified. The results show that it is more advisable to approximate the null distribution of the test using the distribution of the chi-square type mixture, and it is shown through some numerical simulations and real data analysis that our proposed test is powerful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12066v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingxiang Cao, Hongwei Zhang, Kai Xu, Daojiang He</dc:creator>
    </item>
    <item>
      <title>Cyclicity Analysis of the Ornstein-Uhlenbeck Process</title>
      <link>https://arxiv.org/abs/2409.12102</link>
      <description>arXiv:2409.12102v1 Announce Type: new 
Abstract: In this thesis, we consider an $N$-dimensional Ornstein-Uhlenbeck (OU) process satisfying the linear stochastic differential equation $d\mathbf x(t) = - \mathbf B\mathbf x(t) dt + \boldsymbol \Sigma d \mathbf w(t).$ Here, $\mathbf B$ is a fixed $N \times N$ circulant friction matrix whose eigenvalues have positive real parts, $\boldsymbol \Sigma$ is a fixed $N \times M$ matrix. We consider a signal propagation model governed by this OU process. In this model, an underlying signal propagates throughout a network consisting of $N$ linked sensors located in space. We interpret the $n$-th component of the OU process as the measurement of the propagating effect made by the $n$-th sensor. The matrix $\mathbf B$ represents the sensor network structure: if $\mathbf B$ has first row $(b_1 \ , \ \dots \ , \ b_N),$ where $b_1&gt;0$ and $b_2 \ , \ \dots \ ,\ b_N \le 0,$ then the magnitude of $b_p$ quantifies how receptive the $n$-th sensor is to activity within the $(n+p-1)$-th sensor. Finally, the $(m,n)$-th entry of the matrix $\mathbf D = \frac{\boldsymbol \Sigma \boldsymbol \Sigma^\text T}{2}$ is the covariance of the component noises injected into the $m$-th and $n$-th sensors. For different choices of $\mathbf B$ and $\boldsymbol \Sigma,$ we investigate whether Cyclicity Analysis enables us to recover the structure of network. Roughly speaking, Cyclicity Analysis studies the lead-lag dynamics pertaining to the components of a multivariate signal. We specifically consider an $N \times N$ skew-symmetric matrix $\mathbf Q,$ known as the lead matrix, in which the sign of its $(m,n)$-th entry captures the lead-lag relationship between the $m$-th and $n$-th component OU processes. We investigate whether the structure of the leading eigenvector of $\mathbf Q,$ the eigenvector corresponding to the largest eigenvalue of $\mathbf Q$ in modulus, reflects the network structure induced by $\mathbf B.$</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12102v1</guid>
      <category>math.ST</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vivek Kaushik</dc:creator>
    </item>
    <item>
      <title>An exponential inequality for Hilbert-valued U-statistics of i.i.d. data</title>
      <link>https://arxiv.org/abs/2409.11737</link>
      <description>arXiv:2409.11737v1 Announce Type: cross 
Abstract: In this paper, we establish an exponential inequality for U-statistics of i.i.d. data, varying kernel and taking values in a separable Hilbert space. The bound are expressed as a sum of an exponential term plus an other one involving the tail of a sum of squared norms. We start by the degenerate case. Then we provide applications to U-statistics of not necessarily degenerate fixed kernel, weighted U-statistics and incomplete U-statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11737v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davide Giraudo (IRMA)</dc:creator>
    </item>
    <item>
      <title>Incremental effects for continuous exposures</title>
      <link>https://arxiv.org/abs/2409.11967</link>
      <description>arXiv:2409.11967v1 Announce Type: cross 
Abstract: Causal inference problems often involve continuous treatments, such as dose, duration, or frequency. However, continuous exposures bring many challenges, both with identification and estimation. For example, identifying standard dose-response estimands requires that everyone has some chance of receiving any particular level of the exposure (i.e., positivity). In this work, we explore an alternative approach: rather than estimating dose-response curves, we consider stochastic interventions based on exponentially tilting the treatment distribution by some parameter $\delta$, which we term an incremental effect. This increases or decreases the likelihood a unit receives a given treatment level, and crucially, does not require positivity for identification. We begin by deriving the efficient influence function and semiparametric efficiency bound for these incremental effects under continuous exposures. We then show that estimation of the incremental effect is dependent on the size of the exponential tilt, as measured by $\delta$. In particular, we derive new minimax lower bounds illustrating how the best possible root mean squared error scales with an effective sample size of $n/\delta$, instead of usual sample size $n$. Further, we establish new convergence rates and bounds on the bias of double machine learning-style estimators. Our novel analysis gives a better dependence on $\delta$ compared to standard analyses, by using mixed supremum and $L_2$ norms, instead of just $L_2$ norms from Cauchy-Schwarz bounds. Finally, we show that taking $\delta \to \infty$ gives a new estimator of the dose-response curve at the edge of the support, and we give a detailed study of convergence rates in this regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11967v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Shuying Shen, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>On the Statistical Complexity of Sample Amplification</title>
      <link>https://arxiv.org/abs/2201.04315</link>
      <description>arXiv:2201.04315v2 Announce Type: replace 
Abstract: The ``sample amplification'' problem formalizes the following question: Given $n$ i.i.d. samples drawn from an unknown distribution $P$, when is it possible to produce a larger set of $n+m$ samples which cannot be distinguished from $n+m$ i.i.d. samples drawn from $P$? In this work, we provide a firm statistical foundation for this problem by deriving generally applicable amplification procedures, lower bound techniques and connections to existing statistical notions. Our techniques apply to a large class of distributions including the exponential family, and establish a rigorous connection between sample amplification and distribution learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.04315v2</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Axelrod, Shivam Garg, Yanjun Han, Vatsal Sharan, Gregory Valiant</dc:creator>
    </item>
    <item>
      <title>Kernel Stein Discrepancy on Lie Groups: Theory and Applications</title>
      <link>https://arxiv.org/abs/2305.12551</link>
      <description>arXiv:2305.12551v5 Announce Type: replace 
Abstract: Distributional approximation is a fundamental problem in machine learning with numerous applications across all fields of science and engineering and beyond. The key challenge in most approximation methods is the need to tackle the intractable normalization constant pertaining to the parametrized distributions used to model the data. In this paper, we present a novel Stein operator on Lie groups leading to a kernel Stein discrepancy (KSD) which is a normalization-free loss function. We present several theoretical results characterizing the properties of this new KSD on Lie groups and its minimizers namely, the minimum KSD estimator (MKSDE). Proof of several properties of MKSDE are presented, including strong consistency, CLT and a closed form of the MKSDE for the von Mises-Fisher distribution on SO(N). Finally, we present experimental evidence depicting advantages of minimizing KSD over maximum likelihood estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12551v5</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoda Qu, Xiran Fan, Baba C. Vemuri</dc:creator>
    </item>
    <item>
      <title>Posterior consistency in multi-response regression models with non-informative priors for the error covariance matrix in growing dimensions</title>
      <link>https://arxiv.org/abs/2305.13743</link>
      <description>arXiv:2305.13743v3 Announce Type: replace 
Abstract: The Inverse-Wishart (IW) distribution is a standard and popular choice of priors for covariance matrices and has attractive properties such as conditional conjugacy. However, the IW family of priors has crucial drawbacks, including the lack of effective choices for non-informative priors. Several classes of priors for covariance matrices that alleviate these drawbacks, while preserving computational tractability, have been proposed in the literature. These priors can be obtained through appropriate scale mixtures of IW priors. However, in the era of increasing dimensionality, the posterior consistency of models that incorporate such priors has not been investigated. We address this issue for the multi-response regression setting ($q$ responses, $n$ samples) under a wide variety of IW scale mixture priors for the error covariance matrix. Posterior consistency and contraction rates for both the regression coefficient matrix and the error covariance matrix are established in the ``large $q$, large $n$'' setting under mild assumptions on the true data-generating covariance matrix and relevant hyperparameters. In particular, the number of responses $q_n$ is allowed to grow with $n$, but with $q_n = o(n)$. Also, some results related to the inconsistency of the posterior distribution and posterior mean for $q_n/n \to \gamma$, where $\gamma \in (0,\infty)$ are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.13743v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Partha Sarkar, Kshitij Khare, Malay Ghosh</dc:creator>
    </item>
    <item>
      <title>Estimation for multistate models subject to reporting delays and incomplete event adjudication</title>
      <link>https://arxiv.org/abs/2311.04318</link>
      <description>arXiv:2311.04318v2 Announce Type: replace 
Abstract: Complete observation of event histories is often impossible due to sampling effects such as right-censoring and left-truncation, but also due to reporting delays and incomplete event adjudication. This is for example the case for health insurance claims and during interim stages of clinical trials. In this paper, we develop a parametric method that takes the aforementioned effects into account, treating the latter two as partially exogenous. The method, which takes the form of a two-step M-estimation procedure, is applicable to multistate models in general, including competing risks and recurrent event models. The effect of reporting delays is derived via thinning, offering an alternative to existing results for Poisson models. To address incomplete event adjudication, we propose an imputed likelihood approach which, compared to existing methods, has the advantage of allowing for dependencies between the event history and adjudication processes as well as allowing for unreported events and multiple event types. We establish consistency and asymptotic normality under standard identifiability, integrability, and smoothness conditions, and we demonstrate the validity of the percentile bootstrap. Finally, a simulation study shows favorable finite sample performance of our method compared to other alternatives, while an application to disability insurance data illustrates its practical potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04318v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Buchardt, C. Furrer, O. L. Sandqvist</dc:creator>
    </item>
    <item>
      <title>Computationally efficient reductions between some statistical models</title>
      <link>https://arxiv.org/abs/2402.07717</link>
      <description>arXiv:2402.07717v2 Announce Type: replace 
Abstract: We study the problem of approximately transforming a sample from a source statistical model to a sample from a target statistical model without knowing the parameters of the source model, and construct several computationally efficient such reductions between canonical statistical experiments. In particular, we provide computationally efficient procedures that approximately reduce uniform, Erlang, and Laplace location models to general target families. We illustrate our methodology by establishing nonasymptotic reductions between some canonical high-dimensional problems, spanning mixtures of experts, phase retrieval, and signal denoising. Notably, the reductions are structure-preserving and can accommodate missing data. We also point to a possible application in transforming one differentially private mechanism to another.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07717v2</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengqi Lou, Guy Bresler, Ashwin Pananjady</dc:creator>
    </item>
    <item>
      <title>Two step estimations via the Dantzig selector for models of stochastic processes with high-dimensional parameters</title>
      <link>https://arxiv.org/abs/2404.00888</link>
      <description>arXiv:2404.00888v2 Announce Type: replace 
Abstract: We consider the sparse estimation for stochastic processes with possibly infinite-dimensional nuisance parameters, by using the Dantzig selector which is a sparse estimation method similar to $Z$-estimation. When a consistent estimator for a nuisance parameter is obtained, it is possible to construct an asymptotically normal estimator for the parameter of interest under appropriate conditions. Motivated by this fact, we establish the asymptotic behavior of the Dantzig selector for models of ergodic stochastic processes with high-dimensional parameters of interest and possibly infinite-dimensional nuisance parameters. Moreover, we construct an asymptotically normal estimator by the two step estimation with help of the variable selection through the Dantzig selector and a consistent estimator of the nuisance parameter. Applications to ergodic time series models including integer-valued autoregressive models and ergodic diffusion processes are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00888v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kou Fujimori, Koji Tsukuda</dc:creator>
    </item>
    <item>
      <title>An Approximation Theory Framework for Measure-Transport Sampling Algorithms</title>
      <link>https://arxiv.org/abs/2302.13965</link>
      <description>arXiv:2302.13965v4 Announce Type: replace-cross 
Abstract: This article presents a general approximation-theoretic framework to analyze measure transport algorithms for probabilistic modeling. A primary motivating application for such algorithms is sampling -- a central task in statistical inference and generative modeling. We provide a priori error estimates in the continuum limit, i.e., when the measures (or their densities) are given, but when the transport map is discretized or approximated using a finite-dimensional function space. Our analysis relies on the regularity theory of transport maps and on classical approximation theory for high-dimensional functions. A third element of our analysis, which is of independent interest, is the development of new stability estimates that relate the distance between two maps to the distance~(or divergence) between the pushforward measures they define. We present a series of applications of our framework, where quantitative convergence rates are obtained for practical problems using Wasserstein metrics, maximum mean discrepancy, and Kullback--Leibler divergence. Specialized rates for approximations of the popular triangular Kn{\"o}the-Rosenblatt maps are obtained, followed by numerical experiments that demonstrate and extend our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13965v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ricardo Baptista, Bamdad Hosseini, Nikola B. Kovachki, Youssef M. Marzouk, Amir Sagiv</dc:creator>
    </item>
    <item>
      <title>About the Cost of Central Privacy in Density Estimation</title>
      <link>https://arxiv.org/abs/2306.14535</link>
      <description>arXiv:2306.14535v4 Announce Type: replace-cross 
Abstract: We study non-parametric density estimation for densities in Lipschitz and Sobolev spaces, and under central privacy. In particular, we investigate regimes where the privacy budget is not supposed to be constant. We consider the classical definition of central differential privacy, but also the more recent notion of central concentrated differential privacy. We recover the result of Barber and Duchi (2014) stating that histogram estimators are optimal against Lipschitz distributions for the L2 risk, and under regular differential privacy, and we extend it to other norms and notions of privacy. Then, we investigate higher degrees of smoothness, drawing two conclusions: First, and contrary to what happens with constant privacy budget (Wasserman and Zhou, 2010), there are regimes where imposing privacy degrades the regular minimax risk of estimation on Sobolev densities. Second, so-called projection estimators are near-optimal against the same classes of densities in this new setup with pure differential privacy, but contrary to the constant privacy budget case, it comes at the cost of relaxation. With zero concentrated differential privacy, there is no need for relaxation, and we prove that the estimation is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.14535v4</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.48550/</arxiv:DOI>
      <dc:creator>Cl\'ement Lalanne (ENS de Lyon, OCKHAM), Aur\'elien Garivier (UMPA-ENSL, MC2), R\'emi Gribonval (OCKHAM)</dc:creator>
    </item>
    <item>
      <title>A new robust graph for graph-based methods</title>
      <link>https://arxiv.org/abs/2307.15205</link>
      <description>arXiv:2307.15205v3 Announce Type: replace-cross 
Abstract: Graph-based two-sample tests and change-point detection are powerful tools for analyzing high-dimensional and non-Euclidean data, as they do not impose distributional assumptions and perform effectively across a wide range of scenarios. These methods utilize a similarity graph constructed from the observations, with $K$-nearest neighbor graphs or $K$-minimum spanning trees being the current state-of-the-art choices. However, in high-dimensional settings, these graphs tend to form hubs -- nodes with disproportionately large degrees -- and graph-based methods are sensitive to hubs. To address this issue, we propose a robust graph that is significantly less prone to forming hubs in high-dimensional settings. Incorporating this robust graph can substantially improve the power of graph-based methods across various scenarios. Furthermore, we establish a theoretical foundation for graph-based methods using the proposed robust graph, demonstrating its consistency under fixed alternatives in both low-dimensional and high-dimensional contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15205v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 19 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yejiong Zhu, Hao Chen</dc:creator>
    </item>
  </channel>
</rss>
