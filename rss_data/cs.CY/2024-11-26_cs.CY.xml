<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Nov 2024 05:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Opportunities of Reinforcement Learning in South Africa's Just Transition</title>
      <link>https://arxiv.org/abs/2411.15145</link>
      <description>arXiv:2411.15145v1 Announce Type: new 
Abstract: South Africa stands at a crucial juncture, grappling with interwoven socio-economic challenges such as poverty, inequality, unemployment, and the looming climate crisis. The government's Just Transition framework aims to enhance climate resilience, achieve net-zero greenhouse gas emissions by 2050, and promote social inclusion and poverty eradication. According to the Presidential Commission on the Fourth Industrial Revolution, artificial intelligence technologies offer significant promise in addressing these challenges. This paper explores the overlooked potential of Reinforcement Learning (RL) in supporting South Africa's Just Transition. It examines how RL can enhance agriculture and land-use practices, manage complex, decentralised energy networks, and optimise transportation and logistics, thereby playing a critical role in achieving a just and equitable transition to a low-carbon future for all South Africans. We provide a roadmap as to how other researchers in the field may be able to contribute to these pressing problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15145v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claude Formanek, Callum Rhys Tilbury, Jonathan P. Shock</dc:creator>
    </item>
    <item>
      <title>Delegating Responsibilities to Intelligent Autonomous Systems: Challenges and Benefits</title>
      <link>https://arxiv.org/abs/2411.15147</link>
      <description>arXiv:2411.15147v1 Announce Type: new 
Abstract: As AI systems increasingly operate with autonomy and adaptability, the traditional boundaries of moral responsibility in techno-social systems are being challenged. This paper explores the evolving discourse on the delegation of responsibilities to intelligent autonomous agents and the ethical implications of such practices. Synthesizing recent developments in AI ethics, including concepts of distributed responsibility and ethical AI by design, the paper proposes a functionalist perspective as a framework. This perspective views moral responsibility not as an individual trait but as a role within a socio-technical system, distributed among human and artificial agents. As an example of 'AI ethical by design,' we present Basti and Vitiello's implementation. They suggest that AI can act as artificial moral agents by learning ethical guidelines and using Deontic Higher-Order Logic to assess decisions ethically. Motivated by the possible speed and scale beyond human supervision and ethical implications, the paper argues for 'AI ethical by design', while acknowledging the distributed, shared, and dynamic nature of responsibility. This functionalist approach offers a practical framework for navigating the complexities of AI ethics in a rapidly evolving technological landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15147v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gordana Dodig-Crnkovic, Gianfranco Basti, Tobias Holstein</dc:creator>
    </item>
    <item>
      <title>The Fundamental Rights Impact Assessment (FRIA) in the AI Act: Roots, legal obligations and key elements for a model template</title>
      <link>https://arxiv.org/abs/2411.15149</link>
      <description>arXiv:2411.15149v1 Announce Type: new 
Abstract: What is the context which gave rise to the obligation to carry out a Fundamental Rights Impact Assessment (FRIA) in the AI Act? How has assessment of the impact on fundamental rights been framed by the EU legislator in the AI Act? What methodological criteria should be followed in developing the FRIA? These are the three main research questions that this article aims to address, through both legal analysis of the relevant provisions of the AI Act and discussion of various possible models for assessment of the impact of AI on fundamental rights. The overall objective of this article is to fill existing gaps in the theoretical and methodological elaboration of the FRIA, as outlined in the AI Act. In order to facilitate the future work of EU and national bodies and AI operators in placing this key tool for human-centric and trustworthy AI at the heart of the EU approach to AI design and development, this article outlines the main building blocks of a model template for the FRIA. While this proposal is consistent with the rationale and scope of the AI Act, it is also applicable beyond the cases listed in Article 27 and can serve as a blueprint for other national and international regulatory initiatives to ensure that AI is fully consistent with human rights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15149v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.clsr.2024.106020</arxiv:DOI>
      <arxiv:journal_reference>Computer Law &amp; Security Review, Volume 54, September 2024, 106020</arxiv:journal_reference>
      <dc:creator>Alessandro Mantelero</dc:creator>
    </item>
    <item>
      <title>Role of Data Mining in Nigerian Tertiary Education Sector</title>
      <link>https://arxiv.org/abs/2411.15152</link>
      <description>arXiv:2411.15152v1 Announce Type: new 
Abstract: Over a decade there has been a rapid growth in Nigerian educational system particularly higher education. Various institutions have come up both from public and private sector offering many of courses both under and post graduate students. Therefore, rates of students enroll for higher educational institutions in Nigeria have also increased. Hence it is very important to understand the roles play by data mining in analyzing the collected data of students and their academic progression. It is a concern for today's education system and this gap has to be identified and properly addressed to the learning community. Data Mining it helps in various ways to resolve issues face in predictions students and staff performances within Nigerian education system. This paperwork we discuss the roles of Data Mining tools and techniques which can be used effectively in resolving issues in some functional unit of Nigerian tertiary institutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15152v1</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dauda Abdu, Almustapha Abdullahi Wakili, Lawan Nasiru, Buhari Ubale</dc:creator>
    </item>
    <item>
      <title>Developing and Sustaining a Student-Driven Software Solutions Center -- An Experience Report</title>
      <link>https://arxiv.org/abs/2411.15153</link>
      <description>arXiv:2411.15153v1 Announce Type: new 
Abstract: This paper presents an experience report on the establishment and sustenance of a student-driven software solutions center named Information Technology Solutions Center (ITSC), a unit within the School of Information Technology at the University of Cincinnati. A student-driven solution center empowers students to drive the design, development, execution, and maintenance of software solutions for industrial clients. This exposes the students to real-world projects and ensures that students are fully prepared to meet the demands of the ever-changing industrial landscape. The ITSC was established over a decade ago, has trained over 100 students, and executes about 20 projects annually with several industrial partners including Fortune 500 companies, government institutions, and research agencies. This paper discusses the establishment and maintenance of the center with the goal of motivating and providing a clear blueprint for computing programs that want to establish a similar student-driven software solutions center.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15153v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saheed Popoola, Vineela Kunapareddi, Hazem Said</dc:creator>
    </item>
    <item>
      <title>Trading off performance and human oversight in algorithmic policy: evidence from Danish college admissions</title>
      <link>https://arxiv.org/abs/2411.15348</link>
      <description>arXiv:2411.15348v1 Announce Type: new 
Abstract: Student dropout is a significant concern for educational institutions due to its social and economic impact, driving the need for risk prediction systems to identify at-risk students before enrollment. We explore the accuracy of such systems in the context of higher education by predicting degree completion before admission, with potential applications for prioritizing admissions decisions. Using a large-scale dataset from Danish higher education admissions, we demonstrate that advanced sequential AI models offer more precise and fair predictions compared to current practices that rely on either high school grade point averages or human judgment. These models not only improve accuracy but also outperform simpler models, even when the simpler models use protected sociodemographic attributes. Importantly, our predictions reveal how certain student profiles are better matched with specific programs and fields, suggesting potential efficiency and welfare gains in public policy. We estimate that even the use of simple AI models to guide admissions decisions, particularly in response to a newly implemented nationwide policy reducing admissions by 10 percent, could yield significant economic benefits. However, this improvement would come at the cost of reduced human oversight and lower transparency. Our findings underscore both the potential and challenges of incorporating advanced AI into educational policymaking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15348v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Magnus Lindgaard Nielsen, Jonas Skjold Raaschou-Pedersen, Emil Chrisander, David Dreyer Lassen, Julien Grenet, Anna Rogers, Andreas Bjerre-Nielsen</dc:creator>
    </item>
    <item>
      <title>When Image Generation Goes Wrong: A Safety Analysis of Stable Diffusion Models</title>
      <link>https://arxiv.org/abs/2411.15516</link>
      <description>arXiv:2411.15516v1 Announce Type: new 
Abstract: Text-to-image models are increasingly popular and impactful, yet concerns regarding their safety and fairness remain. This study investigates the ability of ten popular Stable Diffusion models to generate harmful images, including NSFW, violent, and personally sensitive material. We demonstrate that these models respond to harmful prompts by generating inappropriate content, which frequently displays troubling biases, such as the disproportionate portrayal of Black individuals in violent contexts. Our findings demonstrate a complete lack of any refusal behavior or safety measures in the models observed. We emphasize the importance of addressing this issue as image generation technologies continue to become more accessible and incorporated into everyday applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15516v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Schneider, Thilo Hagendorff</dc:creator>
    </item>
    <item>
      <title>Gaps Between Research and Practice When Measuring Representational Harms Caused by LLM-Based Systems</title>
      <link>https://arxiv.org/abs/2411.15662</link>
      <description>arXiv:2411.15662v1 Announce Type: new 
Abstract: To facilitate the measurement of representational harms caused by large language model (LLM)-based systems, the NLP research community has produced and made publicly available numerous measurement instruments, including tools, datasets, metrics, benchmarks, annotation instructions, and other techniques. However, the research community lacks clarity about whether and to what extent these instruments meet the needs of practitioners tasked with developing and deploying LLM-based systems in the real world, and how these instruments could be improved. Via a series of semi-structured interviews with practitioners in a variety of roles in different organizations, we identify four types of challenges that prevent practitioners from effectively using publicly available instruments for measuring representational harms caused by LLM-based systems: (1) challenges related to using publicly available measurement instruments; (2) challenges related to doing measurement in practice; (3) challenges arising from measurement tasks involving LLM-based systems; and (4) challenges specific to measuring representational harms. Our goal is to advance the development of instruments for measuring representational harms that are well-suited to practitioner needs, thus better facilitating the responsible development and deployment of LLM-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15662v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Harvey, Emily Sheng, Su Lin Blodgett, Alexandra Chouldechova, Jean Garcia-Gathright, Alexandra Olteanu, Hanna Wallach</dc:creator>
    </item>
    <item>
      <title>Understanding Student Acceptance, Trust, and Attitudes Toward AI-Generated Images for Educational Purposes</title>
      <link>https://arxiv.org/abs/2411.15710</link>
      <description>arXiv:2411.15710v1 Announce Type: new 
Abstract: Recent advancements in artificial intelligence (AI) have broadened the applicability of AI-generated images across various sectors, including the creative industry and design. However, their utilization in educational contexts, particularly among undergraduate students in computer science and software engineering, remains underexplored. This study adopts an exploratory approach, employing questionnaires and interviews, to assess students' acceptance, trust, and positive attitudes towards AI-generated images for educational tasks such as presentations, reports, and web design. The results reveal high acceptance, trust, and positive attitudes among students who value the ease of use and potential academic benefits. However, concerns regarding the lack of technical precision, where the AI fails to accurately produce images as specified by prompts, moderately impact their practical application in detail-oriented educational tasks. These findings suggest a need for developing comprehensive guidelines that address ethical considerations and intellectual property issues, while also setting quality standards for AI-generated images to enhance their educational use. Enhancing the capabilities of AI tools to meet precise user specifications could foster creativity and improve educational outcomes in technical disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15710v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aung Pyae</dc:creator>
    </item>
    <item>
      <title>Achieving employees agile response in e-governance: Exploring the synergy of technology and group collaboration</title>
      <link>https://arxiv.org/abs/2411.15875</link>
      <description>arXiv:2411.15875v1 Announce Type: new 
Abstract: The transformation of technology and collaboration methods driven by the e-government system forces government employees to reconsider their daily workflow and collaboration with colleagues. Despite the extensive existing knowledge of technology usage and collaboration, there are limitations in explaining the synergy between technology usage and group collaboration in achieving agile response from the perspective of government employees, particularly in the e-government setting. To address these challenges, this study provides a holistic understanding of the successful pathway to agile response in e-governance from the perspective of government employees. This study explores a dual path to achieve agile response in e-governance through qualitative analysis, involving 34 in-depth semi-structured interviews with government employees in several government sectors in China. By employing three rounds of coding processes and adopting Interpretative Structural Modeling (ISM), this study identifies the five-layer mechanisms leading to agile response in e-governance, considering both government employee technology usage and group collaboration perspectives. Findings of this study provides suggestions and implications for achieving agile response in e-governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15875v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10726-024-09911-y</arxiv:DOI>
      <arxiv:journal_reference>Group Decision and Negotiation (2024)</arxiv:journal_reference>
      <dc:creator>Ying Bao, Xusen Cheng, Linlin Su, Alex Zarifis</dc:creator>
    </item>
    <item>
      <title>Recent insights into the impact of geopolitical tensions: Quantifying the structure of computer science professors of Chinese descent in the United States</title>
      <link>https://arxiv.org/abs/2411.15907</link>
      <description>arXiv:2411.15907v1 Announce Type: new 
Abstract: The geopolitical tensions between China and the US have dramatically reshaped the American scientific workforce's landscape. To gain a deeper understanding of this circumstance, this study selects the discipline of computer science as a representative case for empirical investigations, aiming to explore the current situation of US-based Chinese-descent computer science professors. One thousand and seventy-eight tenured or tenure-track professors of Chinese descent from the computer science departments of 108 prestigious US universities are profiled, in order to quantify their structure primarily along gender, schooling, and expertise lines. The findings presented in this paper suggest that China-US tensions have made it more difficult for the US higher education system to retain valuable computer science professors of Chinese descent, particularly those in their mid-to late career stages, and that nearly 50% of the existing professors have less than seven years of faculty experience. In addition, the deterioration in faculty retention varies across fields of research, education backgrounds, and gender groups. Specifically, among the professors we are concerned about, those who do not work on AI or Systems, those who lack study experience at US universities, and those who are women, are underrepresented, albeit in different forms and to varying degrees. In a nutshell, the focal professoriate has not only shrunk in size, as has been widely reported, but also lost some of its diversity in structure. This paper has policy implications for the mobility of scientific talent, especially in an era of geopolitical challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15907v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongzhen Wang</dc:creator>
    </item>
    <item>
      <title>Advancing Transformative Education: Generative AI as a Catalyst for Equity and Innovation</title>
      <link>https://arxiv.org/abs/2411.15971</link>
      <description>arXiv:2411.15971v1 Announce Type: new 
Abstract: Generative AI is transforming education by enabling personalized learning, enhancing administrative efficiency, and fostering creative engagement. This paper explores the opportunities and challenges these tools bring to pedagogy, proposing actionable frameworks to address existing equity gaps. Ethical considerations such as algorithmic bias, data privacy, and AI role in human centric education are emphasized. The findings underscore the need for responsible AI integration that ensures accessibility, equity, and innovation in educational systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15971v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chiranjeevi Bura, Praveen Kumar Myakala</dc:creator>
    </item>
    <item>
      <title>Using Drone Swarm to Stop Wildfire: A Predict-then-optimize Approach</title>
      <link>https://arxiv.org/abs/2411.16144</link>
      <description>arXiv:2411.16144v1 Announce Type: new 
Abstract: Drone swarms coupled with data intelligence can be the future of wildfire fighting. However, drone swarm firefighting faces enormous challenges, such as the highly complex environmental conditions in wildfire scenes, the highly dynamic nature of wildfire spread, and the significant computational complexity of drone swarm operations. We develop a predict-then-optimize approach to address these challenges to enable effective drone swarm firefighting. First, we construct wildfire spread prediction convex neural network (Convex-NN) models based on real wildfire data. Then, we propose a mixed-integer programming (MIP) model coupled with dynamic programming (DP) to enable efficient drone swarm task planning. We further use chance-constrained robust optimization (CCRO) to ensure robust firefighting performances under varying situations. The formulated model is solved efficiently using Benders Decomposition and Branch-and-Cut algorithms. After 75 simulated wildfire environments training, the MIP+CCRO approach shows the best performance among several testing sets, reducing movements by 37.3\% compared to the plain MIP. It also significantly outperformed the GA baseline, which often failed to fully extinguish the fire. Eventually, we will conduct real-world fire spread and quenching experiments in the next stage for further validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16144v1</guid>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Pan, Aoran Cheng, Yiqi Sun, Kai Kang, Cristobal Pais, Yulun Zhou, Zuo-Jun Max Shen</dc:creator>
    </item>
    <item>
      <title>The Critical Canvas--How to regain information autonomy in the AI era</title>
      <link>https://arxiv.org/abs/2411.16193</link>
      <description>arXiv:2411.16193v1 Announce Type: new 
Abstract: In the era of AI, recommendation algorithms and generative AI challenge information autonomy by creating echo chambers and blurring the line between authentic and fabricated content. The Critical Canvas addresses these challenges with a novel information exploration platform designed to restore balance between algorithmic efficiency and human agency. It employs three key mechanisms: multi-dimensional exploration across logical, temporal, and geographical perspectives; dynamic knowledge entry generation to capture complex relationships between concepts; and a phase space to evaluate the credibility of both the content and its sources. Particularly relevant to technical AI governance, where stakeholders must navigate intricate specifications and safety frameworks, the platform transforms overwhelming technical information into actionable insights. The Critical Canvas empowers users to regain autonomy over their information consumption through structured yet flexible exploration pathways, creative visualization, human-centric navigation, and transparent source evaluation. It fosters a comprehensive understanding of nuanced topics, enabling more informed decision-making and effective policy development in the age of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16193v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dong Chen</dc:creator>
    </item>
    <item>
      <title>Suspected Undeclared Use of Artificial Intelligence in the Academic Literature: An Analysis of the Academ-AI Dataset</title>
      <link>https://arxiv.org/abs/2411.15218</link>
      <description>arXiv:2411.15218v1 Announce Type: cross 
Abstract: Since generative artificial intelligence (AI) tools such as OpenAI's ChatGPT became widely available, researchers have used them in the writing process. The consensus of the academic publishing community is that such usage must be declared in the published article. Academ-AI documents examples of suspected undeclared AI usage in the academic literature, discernible primarily due to the appearance in research papers of idiosyncratic verbiage characteristic of large language model (LLM)-based chatbots. This analysis of the first 500 examples collected reveals that the problem is widespread, penetrating the journals and conference proceedings of highly respected publishers. Undeclared AI seems to appear in journals with higher citation metrics and higher article processing charges (APCs), precisely those outlets that should theoretically have the resources and expertise to avoid such oversights. An extremely small minority of cases are corrected post publication, and the corrections are often insufficient to rectify the problem. The 500 examples analyzed here likely represent a small fraction of the undeclared AI present in the academic literature, much of which may be undetectable. Publishers must enforce their policies against undeclared AI usage in cases that are detectable; this is the best defense currently available to the academic publishing community against the proliferation of undisclosed AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15218v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Glynn</dc:creator>
    </item>
    <item>
      <title>TPLogAD: Unsupervised Log Anomaly Detection Based on Event Templates and Key Parameters</title>
      <link>https://arxiv.org/abs/2411.15250</link>
      <description>arXiv:2411.15250v1 Announce Type: cross 
Abstract: Log-system is an important mechanism for recording the runtime status and events of Web service systems, and anomaly detection in logs is an effective method of detecting problems. However, manual anomaly detection in logs is inefficient, error-prone, and unrealistic. Existing log anomaly detection methods either use the indexes of event templates, or form vectors by embedding the fixed string part of the template as a sentence, or use time parameters for sequence analysis. However, log entries often contain features and semantic information that cannot be fully represented by these methods, resulting in missed and false alarms. In this paper, we propose TPLogAD, a universal unsupervised method for analyzing unstructured logs, which performs anomaly detection based on event templates and key parameters. The itemplate2vec and para2vec included in TPLogAD are two efficient and easy-to-implement semantic representation methods for logs, detecting anomalies in event templates and parameters respectively, which has not been achieved in previous work. Additionally, TPLogAD can avoid the interference of log diversity and dynamics on anomaly detection. Our experiments on four public log datasets show that TPLogAD outperforms existing log anomaly detection methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15250v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiawei Lu, Chengrong Wu</dc:creator>
    </item>
    <item>
      <title>Inducing Human-like Biases in Moral Reasoning Language Models</title>
      <link>https://arxiv.org/abs/2411.15386</link>
      <description>arXiv:2411.15386v1 Announce Type: cross 
Abstract: In this work, we study the alignment (BrainScore) of large language models (LLMs) fine-tuned for moral reasoning on behavioral data and/or brain data of humans performing the same task. We also explore if fine-tuning several LLMs on the fMRI data of humans performing moral reasoning can improve the BrainScore. We fine-tune several LLMs (BERT, RoBERTa, DeBERTa) on moral reasoning behavioral data from the ETHICS benchmark [Hendrycks et al., 2020], on the moral reasoning fMRI data from Koster-Hale et al. [2013], or on both. We study both the accuracy on the ETHICS benchmark and the BrainScores between model activations and fMRI data. While larger models generally performed better on both metrics, BrainScores did not significantly improve after fine-tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15386v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artem Karpov, Seong Hah Cho, Austin Meek, Raymond Koopmanschap, Lucy Farnik, Bogdan-Ionut Cirstea</dc:creator>
    </item>
    <item>
      <title>Towards Robust Evaluation of Unlearning in LLMs via Data Transformations</title>
      <link>https://arxiv.org/abs/2411.15477</link>
      <description>arXiv:2411.15477v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown to be a great success in a wide range of applications ranging from regular NLP-based use cases to AI agents. LLMs have been trained on a vast corpus of texts from various sources; despite the best efforts during the data pre-processing stage while training the LLMs, they may pick some undesirable information such as personally identifiable information (PII). Consequently, in recent times research in the area of Machine Unlearning (MUL) has become active, the main idea is to force LLMs to forget (unlearn) certain information (e.g., PII) without suffering from performance loss on regular tasks. In this work, we examine the robustness of the existing MUL techniques for their ability to enable leakage-proof forgetting in LLMs. In particular, we examine the effect of data transformation on forgetting, i.e., is an unlearned LLM able to recall forgotten information if there is a change in the format of the input? Our findings on the TOFU dataset highlight the necessity of using diverse data formats to quantify unlearning in LLMs more reliably.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15477v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abhinav Joshi, Shaswati Saha, Divyaksh Shukla, Sriram Vema, Harsh Jhamtani, Manas Gaur, Ashutosh Modi</dc:creator>
    </item>
    <item>
      <title>Transition Network Analysis: A Novel Framework for Modeling, Visualizing, and Identifying the Temporal Patterns of Learners and Learning Processes</title>
      <link>https://arxiv.org/abs/2411.15486</link>
      <description>arXiv:2411.15486v1 Announce Type: cross 
Abstract: This paper proposes a novel analytical framework: Transition Network Analysis (TNA), an approach that integrates Stochastic Process Mining and probabilistic graph representation to model, visualize, and identify transition patterns in the learning process data. Combining the relational and temporal aspects into a single lens offers capabilities beyond either framework, including centralities to capture important learning events, community finding to identify patterns of behavior, and clustering to reveal temporal patterns. This paper introduces the theoretical and mathematical foundations of TNA. To demonstrate the functionalities of TNA, we present a case study with students (n=191) engaged in small-group collaboration to map patterns of group dynamics using the theories of co-regulation and socially-shared regulated learning. The analysis revealed that TNA could reveal the regulatory processes and identify important events, temporal patterns and clusters. Bootstrap validation established the significant transitions and eliminated spurious transitions. In doing so, we showcase TNA's utility to capture learning dynamics and provide a robust framework for investigating the temporal evolution of learning processes. Future directions include advancing estimation methods, expanding reliability assessment, exploring longitudinal TNA, and comparing TNA networks using permutation tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15486v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammed Saqr, Sonsoles L\'opez-Pernas, Tiina T\"orm\"anen, Rogers Kaliisa, Kamila Misiejuk, Santtu Tikka</dc:creator>
    </item>
    <item>
      <title>Performance Evaluation of Self-Organizing Features in Wireless Sensor Networks</title>
      <link>https://arxiv.org/abs/2411.15690</link>
      <description>arXiv:2411.15690v1 Announce Type: cross 
Abstract: With recent advancements in microelectromechanical systems, low-power integrated circuits, and wireless communications, wireless sensor networks have gained immense significance [1][2]These distributed networks facilitate the efficient utilization of resources as well as create countless opportunities for everyday life applications: personal healthcare, home automation, environment monitoring, industrial automation, and defense surveillance to name a few. Sensor networks are vulnerable to environmental conditions of their deployment area and may experience damage. If a part of the network is destroyed, the network needs to be reconfigured or repaired. Due to a number of issues and adjustments in available resources, the mechanisms of the wireless sensor network must be self-organized. For example, in task allocation, cooperative communication, and dynamic data-collection activities, self-organization can enhance the capabilities of wireless sensor networks related to applications, network, and physical layer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15690v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Salwa M. Din, Michael Simon, Raja Jamal Chib</dc:creator>
    </item>
    <item>
      <title>Decoding Urban Industrial Complexity: Enhancing Knowledge-Driven Insights via IndustryScopeGPT</title>
      <link>https://arxiv.org/abs/2411.15758</link>
      <description>arXiv:2411.15758v1 Announce Type: cross 
Abstract: Industrial parks are critical to urban economic growth. Yet, their development often encounters challenges stemming from imbalances between industrial requirements and urban services, underscoring the need for strategic planning and operations. This paper introduces IndustryScopeKG, a pioneering large-scale multi-modal, multi-level industrial park knowledge graph, which integrates diverse urban data including street views, corporate, socio-economic, and geospatial information, capturing the complex relationships and semantics within industrial parks. Alongside this, we present the IndustryScopeGPT framework, which leverages Large Language Models (LLMs) with Monte Carlo Tree Search to enhance tool-augmented reasoning and decision-making in Industrial Park Planning and Operation (IPPO). Our work significantly improves site recommendation and functional planning, demonstrating the potential of combining LLMs with structured datasets to advance industrial park management. This approach sets a new benchmark for intelligent IPPO research and lays a robust foundation for advancing urban industrial development. The dataset and related code are available at https://github.com/Tongji-KGLLM/IndustryScope.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15758v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3664647.3681705</arxiv:DOI>
      <arxiv:journal_reference>In Proceedings of the 32nd ACM International Conference on Multimedia, pp. 4757-4765 (2024, October)</arxiv:journal_reference>
      <dc:creator>Siqi Wang, Chao Liang, Yunfan Gao, Yang Liu, Jing Li, Haofen Wang</dc:creator>
    </item>
    <item>
      <title>Diagnosis of diabetic retinopathy using machine learning &amp; deep learning technique</title>
      <link>https://arxiv.org/abs/2411.16250</link>
      <description>arXiv:2411.16250v1 Announce Type: cross 
Abstract: Fundus images are widely used for diagnosing various eye diseases, such as diabetic retinopathy, glaucoma, and age-related macular degeneration. However, manual analysis of fundus images is time-consuming and prone to errors. In this report, we propose a novel method for fundus detection using object detection and machine learning classification techniques. We use a YOLO_V8 to perform object detection on fundus images and locate the regions of interest (ROIs) such as optic disc, optic cup and lesions. We then use machine learning SVM classification algorithms to classify the ROIs into different DR stages based on the presence or absence of pathological signs such as exudates, microaneurysms, and haemorrhages etc. Our method achieves 84% accuracy and efficiency for fundus detection and can be applied for retinal fundus disease triage, especially in remote areas around the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16250v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Education and Society (2024)</arxiv:journal_reference>
      <dc:creator>Eric Shah, Jay Patel, Mr. Vishal Katheriya, Parth Pataliya</dc:creator>
    </item>
    <item>
      <title>Exploring Privacy and Security as Drivers for Environmental Sustainability in Cloud-Based Office Solutions (Extended Abstract)</title>
      <link>https://arxiv.org/abs/2411.16340</link>
      <description>arXiv:2411.16340v1 Announce Type: cross 
Abstract: This paper explores the intersection of privacy, cybersecurity, and environmental impacts, specifically energy consumption and carbon emissions, in cloud-based office solutions. We hypothesise that solutions that emphasise privacy and security are typically "greener" than solutions that are financed through data collection and advertising. To test our hypothesis, we first investigate how the underlying architectures and business models of these services, e.g., monetisation through (personalised) advertising, contribute to the services' environmental impact. We then explore commonly used methodologies and identify tools that facilitate environmental assessments of software systems. By combining these tools, we develop an approach to systematically assess the environmental footprint of the user-side of online services, which we apply to investigate and compare the influence of service design and ad-blocking technology on the emissions of common web-mail services. Our measurements of a limited selection of such services does not yet conclusively support or falsify our hypothesis regarding primary impacts. However, we are already able to identify the greener web-mail services on the user-side and continue the investigation towards conclusive assessment strategies for online office solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16340v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Kayembe, Iness Ben Guirat, Jan Tobias Muehlberg</dc:creator>
    </item>
    <item>
      <title>Recommender Systems for Good (RS4Good): Survey of Use Cases and a Call to Action for Research that Matters</title>
      <link>https://arxiv.org/abs/2411.16645</link>
      <description>arXiv:2411.16645v1 Announce Type: cross 
Abstract: In the area of recommender systems, the vast majority of research efforts is spent on developing increasingly sophisticated recommendation models, also using increasingly more computational resources. Unfortunately, most of these research efforts target a very small set of application domains, mostly e-commerce and media recommendation. Furthermore, many of these models are never evaluated with users, let alone put into practice. The scientific, economic and societal value of much of these efforts by scholars therefore remains largely unclear. To achieve a stronger positive impact resulting from these efforts, we posit that we as a research community should more often address use cases where recommender systems contribute to societal good (RS4Good). In this opinion piece, we first discuss a number of examples where the use of recommender systems for problems of societal concern has been successfully explored in the literature. We then proceed by outlining a paradigmatic shift that is needed to conduct successful RS4Good research, where the key ingredients are interdisciplinary collaborations and longitudinal evaluation approaches with humans in the loop.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16645v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dietmar Jannach, Alan Said, Marko Tkal\v{c}i\v{c}, Markus Zanker</dc:creator>
    </item>
    <item>
      <title>Curious Rhythms: Temporal Regularities of Wikipedia Consumption</title>
      <link>https://arxiv.org/abs/2305.09497</link>
      <description>arXiv:2305.09497v4 Announce Type: replace 
Abstract: Wikipedia, in its role as the world's largest encyclopedia, serves a broad range of information needs. Although previous studies have noted that Wikipedia users' information needs vary throughout the day, there is to date no large-scale, quantitative study of the underlying dynamics. The present paper fills this gap by investigating temporal regularities in daily consumption patterns in a large-scale analysis of billions of timezone-corrected page requests mined from English Wikipedia's server logs, with the goal of investigating how context and time relate to the kind of information consumed. First, we show that even after removing the global pattern of day-night alternation, the consumption habits of individual articles maintain strong diurnal regularities. Then, we characterize the prototypical shapes of consumption patterns, finding a particularly strong distinction between articles preferred during the evening/night and articles preferred during working hours. Finally, we investigate topical and contextual correlates of Wikipedia articles' access rhythms, finding that article topic, reader country, and access device (mobile vs. desktop) are all important predictors of daily attention patterns. These findings shed new light on how humans seek information on the Web by focusing on Wikipedia as one of the largest open platforms for knowledge and learning, emphasizing Wikipedia's role as a rich knowledge base that fulfills information needs spread throughout the day, with implications for understanding information seeking across the globe and for designing appropriate information systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.09497v4</guid>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Tiziano Piccardi, Martin Gerlach, Robert West</dc:creator>
    </item>
    <item>
      <title>Using Large Language Models for a standard assessment mapping for sustainable communities</title>
      <link>https://arxiv.org/abs/2411.00208</link>
      <description>arXiv:2411.00208v2 Announce Type: replace 
Abstract: This paper presents a new approach to urban sustainability assessment through the use of Large Language Models (LLMs) to streamline the use of the ISO 37101 framework to automate and standardise the assessment of urban initiatives against the six "sustainability purposes" and twelve "issues" outlined in the standard. The methodology includes the development of a custom prompt based on the standard definitions and its application to two different datasets: 527 projects from the Paris Participatory Budget and 398 activities from the PROBONO Horizon 2020 project. The results show the effectiveness of LLMs in quickly and consistently categorising different urban initiatives according to sustainability criteria. The approach is particularly promising when it comes to breaking down silos in urban planning by providing a holistic view of the impact of projects. The paper discusses the advantages of this method over traditional human-led assessments, including significant time savings and improved consistency. However, it also points out the importance of human expertise in interpreting results and ethical considerations. This study hopefully can contribute to the growing body of work on AI applications in urban planning and provides a novel method for operationalising standardised sustainability frameworks in different urban contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00208v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luc Jonveaux</dc:creator>
    </item>
    <item>
      <title>Deciphering Urban Morphogenesis: A Morphospace Approach</title>
      <link>https://arxiv.org/abs/2411.13771</link>
      <description>arXiv:2411.13771v2 Announce Type: replace 
Abstract: Cities emerged independently across different world regions and historical periods, raising fundamental questions: How did the first urban settlements develop? What social and spatial conditions enabled their emergence? Are these processes universal or context-dependent? Moreover, what distinguishes cities from other human settlements? This paper investigates the drivers behind the creation of cities through a hybrid approach that integrates urban theory, the biological concept of morphospace (the space of all possible configurations), and archaeological evidence. It explores the transition from sedentary hunter-gatherer communities to urban societies, highlighting fundamental forces converging to produce increasingly complex divisions of labour as a central driver of urbanization. Morphogenesis is conceptualized as a trajectory through morphospace, governed by structure-seeking selection processes that balance density, permeability, and information as critical dimensions. The study highlights the non-ergodic nature of urban morphogenesis, where configurations are progressively selected based on their fitness to support the diversifying interactions between mutually dependent agents. The morphospace framework effectively distinguishes between theoretical spatial configurations, non-urban and proto-urban settlements, and contemporary cities. This analysis supports the proposition that cities emerge and evolve as solutions balancing density, permeability, and informational organization, enabling them to support increasingly complex societal functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13771v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vini Netto, Caio Cacholas, Dries Daems, Fabiano Ribeiro, Howard Davis, Daniel Lenz</dc:creator>
    </item>
    <item>
      <title>Decision Making with Differential Privacy under a Fairness Lens</title>
      <link>https://arxiv.org/abs/2105.07513</link>
      <description>arXiv:2105.07513v2 Announce Type: replace-cross 
Abstract: Agencies, such as the U.S. Census Bureau, release data sets and statistics about groups of individuals that are used as input to a number of critical decision processes. To conform to privacy and confidentiality requirements, these agencies are often required to release privacy-preserving versions of the data. This paper studies the release of differentially private data sets and analyzes their impact on some critical resource allocation tasks under a fairness perspective. {The paper shows that, when the decisions take as input differentially private data}, the noise added to achieve privacy disproportionately impacts some groups over others. The paper analyzes the reasons for these disproportionate impacts and proposes guidelines to mitigate these effects. The proposed approaches are evaluated on critical decision problems that use differentially private census data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.07513v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferdinando Fioretto, Cuong Tran, Pascal Van Hentenryck</dc:creator>
    </item>
    <item>
      <title>Auditing for Human Expertise</title>
      <link>https://arxiv.org/abs/2306.01646</link>
      <description>arXiv:2306.01646v3 Announce Type: replace-cross 
Abstract: High-stakes prediction tasks (e.g., patient diagnosis) are often handled by trained human experts. A common source of concern about automation in these settings is that experts may exercise intuition that is difficult to model and/or have access to information (e.g., conversations with a patient) that is simply unavailable to a would-be algorithm. This raises a natural question whether human experts add value which could not be captured by an algorithmic predictor. We develop a statistical framework under which we can pose this question as a natural hypothesis test. Indeed, as our framework highlights, detecting human expertise is more subtle than simply comparing the accuracy of expert predictions to those made by a particular learning algorithm. Instead, we propose a simple procedure which tests whether expert predictions are statistically independent from the outcomes of interest after conditioning on the available inputs (`features'). A rejection of our test thus suggests that human experts may add value to any algorithm trained on the available data, and has direct implications for whether human-AI `complementarity' is achievable in a given prediction task. We highlight the utility of our procedure using admissions data collected from the emergency department of a large academic hospital system, where we show that physicians' admit/discharge decisions for patients with acute gastrointestinal bleeding (AGIB) appear to be incorporating information that is not available to a standard algorithmic screening tool. This is despite the fact that the screening tool is arguably more accurate than physicians' discretionary decisions, highlighting that -- even absent normative concerns about accountability or interpretability -- accuracy is insufficient to justify algorithmic automation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01646v3</guid>
      <category>stat.ML</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohan Alur, Loren Laine, Darrick K. Li, Manish Raghavan, Devavrat Shah, Dennis Shung</dc:creator>
    </item>
    <item>
      <title>How Far Are We From AGI: Are LLMs All We Need?</title>
      <link>https://arxiv.org/abs/2405.10313</link>
      <description>arXiv:2405.10313v2 Announce Type: replace-cross 
Abstract: The evolution of artificial intelligence (AI) has profoundly impacted human society, driving significant advancements in multiple sectors. AGI, distinguished by its ability to execute diverse real-world tasks with efficiency and effectiveness comparable to human intelligence, reflects a paramount milestone in AI evolution. While existing studies have reviewed specific advancements in AI and proposed potential paths to AGI, such as large language models (LLMs), they fall short of providing a thorough exploration of AGI's definitions, objectives, and developmental trajectories. Unlike previous survey papers, this work goes beyond summarizing LLMs by addressing key questions about our progress toward AGI and outlining the strategies essential for its realization through comprehensive analysis, in-depth discussions, and novel insights. We start by articulating the requisite capability frameworks for AGI, integrating the internal, interface, and system dimensions. As the realization of AGI requires more advanced capabilities and adherence to stringent constraints, we further discuss necessary AGI alignment technologies to harmonize these factors. Notably, we emphasize the importance of approaching AGI responsibly by first defining the key levels of AGI progression, followed by the evaluation framework that situates the status quo, and finally giving our roadmap of how to reach the pinnacle of AGI. Moreover, to give tangible insights into the ubiquitous impact of the integration of AI, we outline existing challenges and potential pathways toward AGI in multiple domains. In sum, serving as a pioneering exploration into the current state and future trajectory of AGI, this paper aims to foster a collective comprehension and catalyze broader public discussions among researchers and practitioners on AGI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10313v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Feng, Chuanyang Jin, Jingyu Liu, Kunlun Zhu, Haoqin Tu, Zirui Cheng, Guanyu Lin, Jiaxuan You</dc:creator>
    </item>
    <item>
      <title>The AI Alignment Paradox</title>
      <link>https://arxiv.org/abs/2405.20806</link>
      <description>arXiv:2405.20806v2 Announce Type: replace-cross 
Abstract: The field of AI alignment aims to steer AI systems toward human goals, preferences, and ethical principles. Its contributions have been instrumental for improving the output quality, safety, and trustworthiness of today's AI models. This perspective article draws attention to a fundamental challenge we see in all AI alignment endeavors, which we term the "AI alignment paradox": The better we align AI models with our values, the easier we may make it for adversaries to misalign the models. We illustrate the paradox by sketching three concrete example incarnations for the case of language models, each corresponding to a distinct way in which adversaries might exploit the paradox. With AI's increasing real-world impact, it is imperative that a broad community of researchers be aware of the AI alignment paradox and work to find ways to mitigate it, in order to ensure the beneficial use of AI for the good of humanity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20806v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert West, Roland Aydin</dc:creator>
    </item>
    <item>
      <title>A Systematic Review of Echo Chamber Research: Comparative Analysis of Conceptualizations, Operationalizations, and Varying Outcomes</title>
      <link>https://arxiv.org/abs/2407.06631</link>
      <description>arXiv:2407.06631v2 Announce Type: replace-cross 
Abstract: This systematic review synthesizes current research on echo chambers and filter bubbles to highlight the reasons for the dissent in echo chamber research on the existence, antecedents, and effects of the phenomenon. The review of 112 studies reveals that the lack of consensus in echo chamber research is based on different conceptualizations and operationalizations of echo chambers. While studies that have conceptualized echo chambers with homophily and utilized data-driven computational social science (CSS) methods have confirmed the echo chamber hypothesis and polarization effects in social media, content exposure studies and surveys that have explored the full spectrum of media exposure have rejected it.
  Most of these studies have been conducted in the United States, and the review emphasizes the need for a more comprehensive understanding of how echo chambers work in systems with more than two parties and outside the Global North. To advance our understanding of this phenomenon, future research should prioritize conducting more cross-platform studies, considering algorithmic filtering changes through continuous auditing, and examining the causal direction of the association between polarization, fragmentation, and the establishment of online echo chambers. The review also provides the advantages and disadvantages of different operationalizations and makes recommendations for studies in the European Union (EU), which will become possible with the upcoming Digital Services Act (DSA). Overall, this systematic review contributes to the ongoing scholarly discussion on the existence, antecedents, and effects of echo chambers and filter bubbles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06631v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Hartmann, Lena Pohlmann, Sonja Mei Wang, Bettina Berendt</dc:creator>
    </item>
    <item>
      <title>How ChatGPT Changed the Media's Narratives on AI: A Semi-Automated Narrative Analysis Through Frame Semantics</title>
      <link>https://arxiv.org/abs/2408.06120</link>
      <description>arXiv:2408.06120v2 Announce Type: replace-cross 
Abstract: We perform a mixed-method frame semantics-based analysis on a dataset of more than 49,000 sentences collected from 5846 news articles that mention AI. The dataset covers the twelve-month period centred around the launch of OpenAI's chatbot ChatGPT and is collected from the most visited open-access English-language news publishers. Our findings indicate that during the six months succeeding the launch, media attention rose tenfold$\unicode{x2014}$from already historically high levels. During this period, discourse has become increasingly centred around experts and political leaders, and AI has become more closely associated with dangers and risks. A deeper review of the data also suggests a qualitative shift in the types of threat AI is thought to represent, as well as the anthropomorphic qualities ascribed to it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06120v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11023-024-09705-w</arxiv:DOI>
      <arxiv:journal_reference>Minds &amp; Machines 35, 2 (2025)</arxiv:journal_reference>
      <dc:creator>Igor Ryazanov, Carl \"Ohman, Johanna Bj\"orklund</dc:creator>
    </item>
    <item>
      <title>AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios</title>
      <link>https://arxiv.org/abs/2410.19346</link>
      <description>arXiv:2410.19346v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) are increasingly leveraged to empower autonomous agents to simulate human beings in various fields of behavioral research. However, evaluating their capacity to navigate complex social interactions remains a challenge. Previous studies face limitations due to insufficient scenario diversity, complexity, and a single-perspective focus. To this end, we introduce AgentSense: Benchmarking Social Intelligence of Language Agents through Interactive Scenarios. Drawing on Dramaturgical Theory, AgentSense employs a bottom-up approach to create 1,225 diverse social scenarios constructed from extensive scripts. We evaluate LLM-driven agents through multi-turn interactions, emphasizing both goal completion and implicit reasoning. We analyze goals using ERG theory and conduct comprehensive experiments. Our findings highlight that LLMs struggle with goals in complex social scenarios, especially high-level growth needs, and even GPT-4o requires improvement in private information reasoning. Code and data are available at \url{https://github.com/ljcleo/agent_sense}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19346v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Mou, Jingcong Liang, Jiayu Lin, Xinnong Zhang, Xiawei Liu, Shiyue Yang, Rong Ye, Lei Chen, Haoyu Kuang, Xuanjing Huang, Zhongyu Wei</dc:creator>
    </item>
    <item>
      <title>Generative AI may backfire for counterspeech</title>
      <link>https://arxiv.org/abs/2411.14986</link>
      <description>arXiv:2411.14986v2 Announce Type: replace-cross 
Abstract: Online hate speech poses a serious threat to individual well-being and societal cohesion. A promising solution to curb online hate speech is counterspeech. Counterspeech is aimed at encouraging users to reconsider hateful posts by direct replies. However, current methods lack scalability due to the need for human intervention or fail to adapt to the specific context of the post. A potential remedy is the use of generative AI, specifically large language models (LLMs), to write tailored counterspeech messages. In this paper, we analyze whether contextualized counterspeech generated by state-of-the-art LLMs is effective in curbing online hate speech. To do so, we conducted a large-scale, pre-registered field experiment (N=2,664) on the social media platform Twitter/X. Our experiment followed a 2x2 between-subjects design and, additionally, a control condition with no counterspeech. On the one hand, users posting hateful content on Twitter/X were randomly assigned to receive either (a) contextualized counterspeech or (b) non-contextualized counterspeech. Here, the former is generated through LLMs, while the latter relies on predefined, generic messages. On the other hand, we tested two counterspeech strategies: (a) promoting empathy and (b) warning about the consequences of online misbehavior. We then measured whether users deleted their initial hateful posts and whether their behavior changed after the counterspeech intervention (e.g., whether users adopted a less toxic language). We find that non-contextualized counterspeech employing a warning-of-consequence strategy significantly reduces online hate speech. However, contextualized counterspeech generated by LLMs proves ineffective and may even backfire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14986v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 26 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominik B\"ar, Abdurahman Maarouf, Stefan Feuerriegel</dc:creator>
    </item>
  </channel>
</rss>
