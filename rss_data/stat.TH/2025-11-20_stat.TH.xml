<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Nov 2025 05:01:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Asymptotic Equivalence of Level-Based and Share-Based Loss Functions</title>
      <link>https://arxiv.org/abs/2511.14812</link>
      <description>arXiv:2511.14812v1 Announce Type: new 
Abstract: Level-based and share-based loss functions are asymptotically equivalent if, in the limit, their averages converge almost surely to a constant ratio. These loss functions take a target value and its realization as arguments and are often used to measure accuracy. The equivalence is proved for a large class of loss functions, the weighted exponentiated functions, when the weights are decomposable as a particular product form. An upshot is that when losses are averaged for a large number of units, differences in ratios and, hence, ranks, are negligible, when the average (or summed) difference between the target values and their realizations is around zero. This implies the almost sure asymptotic convergence of numerical and distributive accuracy when using these loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14812v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman</dc:creator>
    </item>
    <item>
      <title>Design-based finite-sample analysis for regression adjustment</title>
      <link>https://arxiv.org/abs/2511.15161</link>
      <description>arXiv:2511.15161v1 Announce Type: new 
Abstract: In randomized experiments, regression adjustment leverages covariates to improve the precision of average treatment effect (ATE) estimation without requiring a correctly specified outcome model. Although well understood in low-dimensional settings, its behavior in high-dimensional regimes -- where the number of covariates $p$ may exceed the number of observations $n$ -- remains underexplored. Furthermore, existing theory is largely asymptotic, providing limited guidance for finite-sample inference. We develop a design-based, non-asymptotic analysis of the regression-adjusted ATE estimator under complete randomization. Specifically, we derive finite-sample-valid confidence intervals with explicit, instance-adaptive widths that remain informative even when $p &gt; n$. These intervals rely on oracle (population-level) quantities, and we also outline data-driven envelopes that are computable from observed data. Our approach hinges on a refined swap sensitivity analysis: stochastic fluctuation is controlled via a variance-adaptive Doob martingale and Freedman's inequality, while design bias is bounded using Stein's method of exchangeable pairs. The analysis suggests how covariate geometry governs concentration and bias through leverages and cross-leverages, shedding light on when and how regression adjustment improves on the difference-in-means baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15161v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dogyoon Song</dc:creator>
    </item>
    <item>
      <title>Consistent Empirical Bayes Estimation of the Mean of a Mixing Distribution with Applications to Treatment of Nonresponse</title>
      <link>https://arxiv.org/abs/2511.15373</link>
      <description>arXiv:2511.15373v1 Announce Type: new 
Abstract: We consider a Nonparametric Empirical Bayes (NPEB) framework. Let $Y_i$ be random variables, $Y_i \sim f(y|\theta_i)$, $i=1,...,n$, where $\theta_i \sim G$, and $\theta_i \in \Theta$ are independent. The variables $Y_i $ are conditionally independent given $\theta_i, \; i=1,...,n$. The mixing distribution $G$ is unknown and assumed to belong to a nonparametric class
  $\{G \}$.
  Let $\eta(\theta)$ be a function of $\theta$. We address the problem of consistently estimating $E_G \eta(\theta) \equiv \eta_G$. This problem becomes particularly challenging when $G$ cannot be consistently estimated from the observed data.
  We motivate this problem, especially in contexts involving nonresponse and missing data. For such cases, a consistent estimation method is suggested and its performance is demonstrated through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15373v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eitan Greenshtein</dc:creator>
    </item>
    <item>
      <title>Singular Learning Theory for Factor Analysis</title>
      <link>https://arxiv.org/abs/2511.15419</link>
      <description>arXiv:2511.15419v1 Announce Type: new 
Abstract: Watanabe's singular learning theory provides a framework for asymptotic analysis of Bayesian model selection for statistical models with singularities, where traditional statistical regularity assumptions fail. Learning coefficients, also known as real log canonical thresholds, play a central role in singular learning, as they govern the asymptotic behavior of Bayesian marginal likelihood integrals in settings where the Laplace approximations used for regular statistical models are not applicable. Learning coefficients are algebraic invariants that quantify the geometric complexity of a model and reveal how the singular structure impacts the model's generalization properties. In this paper, we apply algebraic methods to study the learning coefficients of factor analysis models, which are widely used latent variable models for continuously distributed data. Our main results provide a general upper bound for the learning coefficients as well as exact formulas for specific cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15419v1</guid>
      <category>math.ST</category>
      <category>math.AG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mathias Drton, Elizabeth Gross, Dimitra Kosta, Anton Leykin, Seth Sullivant, Daniel Windisch</dc:creator>
    </item>
    <item>
      <title>Cartan meets Cram\'er-Rao</title>
      <link>https://arxiv.org/abs/2511.15612</link>
      <description>arXiv:2511.15612v1 Announce Type: new 
Abstract: This paper develops a jet bundle and Cartan geometric foundation for the curvature-aware refinements of the Cram\'er-Rao bound (CRB) introduced in our earlier work. We show that the extrinsic corrections to variance bounds, previously derived from the second fundamental form of the square root embedding $s_\theta=\sqrt{f(\cdot;\theta)}\in L^2(\mu)$ for model density $f(\cdot;\theta)$ with scalar parameter $\theta$, admit an intrinsic formulation within the Cartan prolongation framework. Starting from the canonical contact forms and total derivative on the finite jet bundle $J^m(\mathbb{R}\times \mathbb{R})$, we construct the Cartan distribution and the associated Ehresmann connection, whose non-integrability and torsion encode the geometric source of curvature corrections in statistical estimation. In the statistical jet bundle $E=\mathbb{R}\times L^2(\mu)$, we point out that the condition for an estimator error to lie in the span of derivatives of $s_\theta$ up to order $m$ is equivalent to the square root map satisfying a linear differential equation of order~$m$. The corresponding submanifold of $J^m(E)$ defined by this equation represents the locus of $m$-th order efficient models, and the prolonged section must form an integral curve of the restricted Cartan vector field. This establishes a one-to-one correspondence between algebraic projection conditions underlying CRB and Bhattacharyya-type bounds and geometric integrability conditions for the statistical section in the jet bundle hierarchy. The resulting framework links variance bounds, curvature, and estimator efficiency through the geometry of Cartan distributions, offering a new differential equation and connection-theoretic interpretation of higher-order information inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15612v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.DG</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunder Ram Krishnan</dc:creator>
    </item>
    <item>
      <title>Convex Clustering Redefined: Robust Learning with the Median of Means Estimator</title>
      <link>https://arxiv.org/abs/2511.14784</link>
      <description>arXiv:2511.14784v1 Announce Type: cross 
Abstract: Clustering approaches that utilize convex loss functions have recently attracted growing interest in the formation of compact data clusters. Although classical methods like k-means and its wide family of variants are still widely used, all of them require the number of clusters k to be supplied as input, and many are notably sensitive to initialization. Convex clustering provides a more stable alternative by formulating the clustering task as a convex optimization problem, ensuring a unique global solution. However, it faces challenges in handling high-dimensional data, especially in the presence of noise and outliers. Additionally, strong fusion regularization, controlled by the tuning parameter, can hinder effective cluster formation within a convex clustering framework. To overcome these challenges, we introduce a robust approach that integrates convex clustering with the Median of Means (MoM) estimator, thus developing an outlier-resistant and efficient clustering framework that does not necessitate prior knowledge of the number of clusters. By leveraging the robustness of MoM alongside the stability of convex clustering, our method enhances both performance and efficiency, especially on large-scale datasets. Theoretical analysis demonstrates weak consistency under specific conditions, while experiments on synthetic and real-world datasets validate the method's superior performance compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14784v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav De, Koustav Chowdhury, Bibhabasu Mandal, Sagar Ghosh, Swagatam Das, Debolina Paul, Saptarshi Chakraborty</dc:creator>
    </item>
    <item>
      <title>Neural Networks Learn Generic Multi-Index Models Near Information-Theoretic Limit</title>
      <link>https://arxiv.org/abs/2511.15120</link>
      <description>arXiv:2511.15120v1 Announce Type: cross 
Abstract: In deep learning, a central issue is to understand how neural networks efficiently learn high-dimensional features. To this end, we explore the gradient descent learning of a general Gaussian Multi-index model $f(\boldsymbol{x})=g(\boldsymbol{U}\boldsymbol{x})$ with hidden subspace $\boldsymbol{U}\in \mathbb{R}^{r\times d}$, which is the canonical setup to study representation learning. We prove that under generic non-degenerate assumptions on the link function, a standard two-layer neural network trained via layer-wise gradient descent can agnostically learn the target with $o_d(1)$ test error using $\widetilde{\mathcal{O}}(d)$ samples and $\widetilde{\mathcal{O}}(d^2)$ time. The sample and time complexity both align with the information-theoretic limit up to leading order and are therefore optimal. During the first stage of gradient descent learning, the proof proceeds via showing that the inner weights can perform a power-iteration process. This process implicitly mimics a spectral start for the whole span of the hidden subspace and eventually eliminates finite-sample noise and recovers this span. It surprisingly indicates that optimal results can only be achieved if the first layer is trained for more than $\mathcal{O}(1)$ steps. This work demonstrates the ability of neural networks to effectively learn hierarchical functions with respect to both sample and time efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15120v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bohan Zhang, Zihao Wang, Hengyu Fu, Jason D. Lee</dc:creator>
    </item>
    <item>
      <title>Beyond Uncertainty Sets: Leveraging Optimal Transport to Extend Conformal Predictive Distribution to Multivariate Settings</title>
      <link>https://arxiv.org/abs/2511.15146</link>
      <description>arXiv:2511.15146v1 Announce Type: cross 
Abstract: Conformal prediction (CP) constructs uncertainty sets for model outputs with finite-sample coverage guarantees. A candidate output is included in the prediction set if its non-conformity score is not considered extreme relative to the scores observed on a set of calibration examples. However, this procedure is only straightforward when scores are scalar-valued, which has limited CP to real-valued scores or ad-hoc reductions to one dimension. The problem of ordering vectors has been studied via optimal transport (OT), which provides a principled method for defining vector-ranks and multivariate quantile regions, though typically with only asymptotic coverage guarantees. We restore finite-sample, distribution-free coverage by conformalizing the vector-valued OT quantile region. Here, a candidate's rank is defined via a transport map computed for the calibration scores augmented with that candidate's score. This defines a continuum of OT problems for which we prove that the resulting optimal assignment is piecewise-constant across a fixed polyhedral partition of the score space. This allows us to characterize the entire prediction set tractably, and provides the machinery to address a deeper limitation of prediction sets: that they only indicate which outcomes are plausible, but not their relative likelihood. In one dimension, conformal predictive distributions (CPDs) fill this gap by producing a predictive distribution with finite-sample calibration. Extending CPDs beyond one dimension remained an open problem. We construct, to our knowledge, the first multivariate CPDs with finite-sample calibration, i.e., they define a valid multivariate distribution where any derived uncertainty region automatically has guaranteed coverage. We present both conservative and exact randomized versions, the latter resulting in a multivariate generalization of the classical Dempster-Hill procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15146v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eugene Ndiaye</dc:creator>
    </item>
    <item>
      <title>Variance-reduced extreme value index estimators using control variates in a semi-supervised setting</title>
      <link>https://arxiv.org/abs/2511.15561</link>
      <description>arXiv:2511.15561v1 Announce Type: cross 
Abstract: The estimation of the Extreme Value Index (EVI) is fundamental in extreme value analysis but suffers from high variance due to reliance on only a few extreme observations. We propose a control variates based transfer learning approach in a semi-supervised framework, where a small set of coupled target and source observations is combined with abundant unpaired source data. By expressing the Hill estimator of the target EVI as a ratio of means, we apply approximate control variates to both numerator and denominator, with jointly optimized coefficients that guarantee variance reduction without introducing bias. We show theoretically and through simulations that the asymptotic relative variance reduction of the transferred Hill estimator is proportional to the tail dependence between the target and source variables and independent of their EVI values. Thus, substantial variance reduction can be achieved even without similarity in tail heaviness of the target and source distributions. The proposed approach can be extended to other EVI estimators expressed with ratio of means, as demonstrated on the moment estimator. The practical value of the proposed method is illustrated on multi-fidelity water surge and ice accretion datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15561v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Louison Bocquet-Nouaille, J\'er\^ome Morio, Benjamin Bobbia</dc:creator>
    </item>
    <item>
      <title>Consistent Group selection using Global-local prior in High dimensional setup</title>
      <link>https://arxiv.org/abs/2302.04715</link>
      <description>arXiv:2302.04715v5 Announce Type: replace 
Abstract: We consider the problem of model selection when grouping structure is inherent within the regressors. Using a Bayesian approach, we model the mean vector by a one-group global-local shrinkage prior belonging to a broad class of such priors that includes the horseshoe prior. In the context of variable selection, this class of priors was studied by Tang et al. (2018). A modified form of the usual class of global-local shrinkage priors with polynomial tail on the group regression coefficients is proposed. The resulting threshold rule selects the active group if within a group, the ratio of the $L_2$ norm of the posterior mean of its group coefficient to that of the corresponding ordinary least square group estimate is greater than a half. In the theoretical part of this article, we have used the global shrinkage parameter either as a tuning one or an empirical Bayes estimate of it depending on the knowledge regarding the underlying sparsity of the model. When the proportion of active groups is known, using $\tau$ as a tuning parameter, we have proved that our method is oracle. In case this proportion is unknown, we propose an empirical Bayes estimate of $\tau$. Even if this empirical Bayes estimate is used, then also our half-thresholding rule captures the truly important groups and obtains optimal estimation rate of the group coefficients simultaneously. Though our theoretical works rely on a special form of the design matrix, for general design matrices also, our simulation results show that the half-thresholding rule yields results similar to that of Yang and Narisetty (2020). As a consequence of this, in a high dimensional sparse group selection problem, instead of using the so-called `gold standard' spike and slab prior, one can use the one-group global-local shrinkage priors with polynomial tail to obtain similar results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04715v5</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sayantan Paul, Prasenjit Ghosh, Arijit Chakrabarti</dc:creator>
    </item>
    <item>
      <title>Dyson Equation for Correlated Linearizations and Test Error of Random Features Regression</title>
      <link>https://arxiv.org/abs/2312.09194</link>
      <description>arXiv:2312.09194v3 Announce Type: replace 
Abstract: This paper develops some theory of the Dyson equation for correlated linearizations and uses it to solve a problem on asymptotic deterministic equivalent for the test error in random features regression. The theory developed for the correlated Dyson equation includes existence-uniqueness, spectral support bounds, and stability properties. This theory is new for constructing deterministic equivalents for pseudo-resolvents of a class of linearizations with correlated entries. In the application, this theory is used to give a deterministic equivalent of the test error in random features ridge regression, in a proportional scaling regime, wherein we have conditioned on both training and test datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.09194v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Latourelle-Vigeant, Elliot Paquette</dc:creator>
    </item>
    <item>
      <title>Reconciling common source, specific source, feature based and score based likelihood ratios</title>
      <link>https://arxiv.org/abs/2409.05403</link>
      <description>arXiv:2409.05403v2 Announce Type: replace 
Abstract: We show that the incorporation of any new piece of information allows for improved decision making in the sense that the expected costs of an optimal decision decrease (or, in boundary cases where no or not enough new information is incorporated, stays the same) whenever this is done by the appropriate update of the probabilities of the hypotheses. Versions of this result have been stated before. However, previous proofs rely on auxiliary constructions with proper scoring rules. We, instead, offer a direct and completely general proof by considering elementary properties of likelihood ratios only. We apply our results to make a contribution to the debates about the use of score based/feature based and common/specific source likelihood ratios. In the literature these are often presented as different ``LR-systems''. We argue that the difference between these is simply a matter which information is processed. There is no therefore no such thing as different ``LR-systems'', there are only differences in the processed information. In particular, despite claims to the contrary, scores can very well be used in forensic practice and we illustrate this with an extensive example in DNA kinship context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05403v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/lpr/mgaf008</arxiv:DOI>
      <arxiv:journal_reference>Law, Probability and Risk, 24, 1-19, 2025</arxiv:journal_reference>
      <dc:creator>Aafko Boonstra, Ronald Meester, Klaas Slooten</dc:creator>
    </item>
    <item>
      <title>Uniform Distributions on p-Balls and the Singular Role of $p=1,2,\infty$ in p-Norm Geometry</title>
      <link>https://arxiv.org/abs/2411.13567</link>
      <description>arXiv:2411.13567v2 Announce Type: replace 
Abstract: This paper studies the relationship between volume and surface uniform measures on n-dimensional p-balls under the p-norm. It is proved that for p=1, p=2 and p=infinity, and only for these values of p, radial projection maps a volumetrically uniform distribution to a surface-uniform distribution. Algorithms for uniform sampling on p-balls and p-spheres are provided, together with empirical illustrations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13567v2</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Pinz\'on</dc:creator>
    </item>
    <item>
      <title>Direct sampling from conditional distributions by sequential maximum likelihood estimations</title>
      <link>https://arxiv.org/abs/2502.00812</link>
      <description>arXiv:2502.00812v3 Announce Type: replace 
Abstract: We can directly sample from the conditional distribution of any log-affine model. The algorithm is a Markov chain on a bounded integer lattice, and its transition probability is the ratio of the UMVUE (uniformly minimum variance unbiased estimator) of the expected counts to the total number of counts. The computation of the UMVUE accounts for most of the computational cost, which makes the implementation challenging. Here, we investigated an approximate algorithm that replaces the UMVUE with the MLE (maximum likelihood estimator). Although it is generally not exact, it is efficient and easy to implement; no prior study is required, such as about the connection matrices of the holonomic ideal in the original algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00812v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2140/astat.2025.16.175</arxiv:DOI>
      <arxiv:journal_reference>Journal of Algebraic Statistics 16(2): 175-199 (2025)</arxiv:journal_reference>
      <dc:creator>Shuhei Mano</dc:creator>
    </item>
    <item>
      <title>Filtering of periodically correlated processes</title>
      <link>https://arxiv.org/abs/2511.00990</link>
      <description>arXiv:2511.00990v2 Announce Type: replace 
Abstract: The problem of optimal linear estimation of a linear functional depending on the unknown values of periodically correlated stochastic process from observations of the process with additive noise is considered. Formulas for calculating the mean square error and the spectral characteristic of the optimal linear estimate of the functional are proposed in the case where spectral densities are exactly known. Formulas that determine the least favorable spectral densities and the minimax (robust) spectral characteristics are proposed for a given class of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00990v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iryna Dubovets'ka, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Rank-based linkage I: triplet comparisons and oriented simplicial complexes</title>
      <link>https://arxiv.org/abs/2302.02200</link>
      <description>arXiv:2302.02200v4 Announce Type: replace-cross 
Abstract: Rank-based linkage is a new tool for summarizing a collection $S$ of objects according to their relationships. These objects are not mapped to vectors, and ``similarity'' between objects need be neither numerical nor symmetrical. All an object needs to do is rank nearby objects by similarity to itself, using a \texttt{Comparator} which is transitive, but need not be consistent with any metric on the whole set. Call this a ranking system on $S$. Rank-based linkage is applied to the $K$-nearest neighbor digraph derived from a ranking system. Computations occur on a 2-dimensional abstract oriented simplicial complex whose faces are among the points, edges, and triangles of the line graph of the undirected $K$-nearest neighbor graph on $S$. In $|S| K^2$ steps it builds an edge-weighted linkage graph $(S, \mathcal{L}, \sigma)$ where $\sigma(\{x, y\})$ is called the in-sway between objects $x$ and $y$. Take $\mathcal{L}_t$ to be the links whose in-sway is at least $t$, and partition $S$ into components of the graph $(S, \mathcal{L}_t)$, for varying $t$. Rank-based linkage is a functor from a category of ``out-ordered'' digraphs to a category of partitioned sets, with the practical consequence that augmenting the set of objects in a rank-respectful way gives a fresh clustering which does not ``rip apart'' the previous one. The same holds for single linkage clustering in the metric space context, but not for typical optimization-based methods. Orientation sheaves play in a fundamental role and ensure that partially overlapping data sets can be ``glued'' together. Open combinatorial problems are presented in the last section</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.02200v4</guid>
      <category>math.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. W. R. Darling, Will Grilliette, Adam Logan</dc:creator>
    </item>
    <item>
      <title>Concentration and moment inequalities for sums of independent heavy-tailed random matrices</title>
      <link>https://arxiv.org/abs/2407.12948</link>
      <description>arXiv:2407.12948v3 Announce Type: replace-cross 
Abstract: We prove Fuk-Nagaev and Rosenthal-type inequalities for sums of independent random matrices, focusing on the situation when the norms of the matrices possess finite moments of only low orders. Our bounds depend on the ``intrinsic'' dimensional characteristics such as the effective rank, as opposed to the dimension of the ambient space. We illustrate the advantages of such results through several applications, including new moment inequalities for sample covariance matrices and their eigenvectors when the underlying distribution is heavy-tailed. Moreover, we demonstrate that our techniques yield sharpened versions of moment inequalities for empirical processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.12948v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00440-025-01412-6</arxiv:DOI>
      <arxiv:journal_reference>Probabability Theory and Related Fields (2025), p. 1-28</arxiv:journal_reference>
      <dc:creator>Moritz Jirak, Stanislav Minsker, Yiqiu Shen, Martin Wahl</dc:creator>
    </item>
    <item>
      <title>Fast convergence of the Expectation Maximization algorithm under a logarithmic Sobolev inequality</title>
      <link>https://arxiv.org/abs/2407.17949</link>
      <description>arXiv:2407.17949v2 Announce Type: replace-cross 
Abstract: We present a new framework for analysing the Expectation Maximization (EM) algorithm. Drawing on recent advances in the theory of gradient flows over Euclidean-Wasserstein spaces, we extend techniques from alternating minimization in Euclidean spaces to the EM algorithm, via its representation as coordinate-wise minimization of the free energy. In so doing, we obtain finite sample error bounds and exponential convergence of the EM algorithm under a natural generalisation of the log-Sobolev inequality. We further show that this framework naturally extends to several variants of EM, offering a unified approach for studying such algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17949v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rocco Caprio, Adam M Johansen</dc:creator>
    </item>
    <item>
      <title>Testing Conditional Stochastic Dominance at Target Points</title>
      <link>https://arxiv.org/abs/2503.14747</link>
      <description>arXiv:2503.14747v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel test for conditional stochastic dominance (CSD) at specific values of the conditioning covariates, referred to as target points. The test is relevant for analyzing income inequality, evaluating treatment effects, and studying discrimination. We propose a Kolmogorov--Smirnov-type test statistic that utilizes induced order statistics from independent samples. Notably, the test features a data-independent critical value, eliminating the need for resampling techniques such as the bootstrap. Our approach avoids kernel smoothing and parametric assumptions, instead relying on a tuning parameter to select relevant observations. We establish the asymptotic properties of our test, showing that the induced order statistics converge to independent draws from the true conditional distributions and that the test is asymptotically of level $\alpha$ under weak regularity conditions. While our results apply to both continuous and discrete data, in the discrete case, the critical value only provides a valid upper bound. To address this, we propose a refined critical value that significantly enhances power, requiring only knowledge of the support size of the distributions. Additionally, we analyze the test's behavior in the limit experiment, demonstrating that it reduces to a problem analogous to testing unconditional stochastic dominance in finite samples. This framework allows us to prove the validity of permutation-based tests for stochastic dominance when the random variables are continuous. Monte Carlo simulations confirm the strong finite-sample performance of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14747v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federico A. Bugni, Ivan A. Canay, Deborah Kim</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation of General Indicators in Off-Census Years</title>
      <link>https://arxiv.org/abs/2510.10812</link>
      <description>arXiv:2510.10812v2 Announce Type: replace-cross 
Abstract: We propose small area estimators of general indicators in off-census years, which avoid the use of deprecated census microdata, but are nearly optimal in census years. The procedure is based on replacing the obsolete census file with a larger unit-level survey that adequately covers the areas of interest and contains the values of useful auxiliary variables. However, the minimal data requirement of the proposed method is a single survey with microdata on the target variable and suitable auxiliary variables for the period of interest. We also develop an estimator of the mean squared error (MSE) that accounts for the uncertainty introduced by the large survey used to replace the census of auxiliary information. Our empirical results indicate that the proposed predictors perform clearly better than the alternative predictors when census data are outdated, and are very close to optimal ones when census data are correct. They also illustrate that the proposed total MSE estimator corrects for the bias of purely model-based MSE estimators that do not account for the large survey uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10812v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Acero, Isabel Molina, J. Miguel Mar\'in</dc:creator>
    </item>
    <item>
      <title>From Global to Local Correlation: Geometric Decomposition of Statistical Inference</title>
      <link>https://arxiv.org/abs/2511.04599</link>
      <description>arXiv:2511.04599v4 Announce Type: replace-cross 
Abstract: Understanding feature-outcome associations in high-dimensional data remains
  challenging when relationships vary across subpopulations, yet standard
  methods assuming global associations miss context-dependent patterns, reducing
  statistical power and interpretability. We develop a geometric decomposition
  framework offering two strategies for partitioning inference problems into
  regional analyses on data-derived Riemannian graphs. Gradient flow
  decomposition uses path-monotonicity-validated discrete Morse theory to
  partition samples into gradient flow cells where outcomes exhibit monotonic
  behavior. Co-monotonicity decomposition utilizes vertex-level coefficients
  that provide context-dependent versions of the classical Pearson correlation:
  these coefficients measure edge-based directional concordance between outcome
  and features, or between feature pairs, defining embeddings of samples into
  association space. These embeddings induce Riemannian k-NN graphs on which
  biclustering identifies co-monotonicity cells (coherent regions) and feature
  modules. This extends naturally to multi-modal integration across multiple
  feature sets. Both strategies apply independently or jointly, with Bayesian
  posterior sampling providing credible intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04599v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 20 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pawel Gajer, Jacques Ravel</dc:creator>
    </item>
  </channel>
</rss>
