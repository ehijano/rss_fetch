<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 31 Jan 2025 02:30:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Realizing Hardware-Optimized General Tree-Based Data Structures for Heterogeneous System Classes</title>
      <link>https://arxiv.org/abs/2501.17434</link>
      <description>arXiv:2501.17434v1 Announce Type: new 
Abstract: Tree-based data structures are ubiquitous across applications. Therefore, a multitude of different tree implementations exist. However, while these implementations are diverse, they share a tree structure as the underlying data structure. As such, the access patterns inside these trees are very similar, following a path from the root of the tree towards a leaf node. Similarly, many distinct types of memory exist. These types of memory all have different characteristics. Some of these have an impact on the overall system performance. While the concrete types of memory are varied, their characteristics can often be abstracted to have a similar effect on the performance. We show how the characteristics of different types of memories can be used to improve the performance of tree-based data structures. By reordering the nodes of a tree inside memory, the characteristics of memory can be exploited to optimize the performance. To this end, this paper presents different strategies for reordering nodes inside memory as well as efficient algorithms for realizing these strategies. It additionally provides strategies to decide when such a reordering operation should be triggered during operation. Further, this paper conducts experiments showing the performance impact of the proposed strategies. The experiments show that the strategies can improve the performance of trees by up to 95\% as offline optimization and 75\% as online optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17434v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Biebert, Christian Hakert, Jian-Jia Chen</dc:creator>
    </item>
    <item>
      <title>Proteus: Achieving High-Performance Processing-Using-DRAM via Dynamic Precision Bit-Serial Arithmetic</title>
      <link>https://arxiv.org/abs/2501.17466</link>
      <description>arXiv:2501.17466v1 Announce Type: new 
Abstract: Processing-using-DRAM (PUD) is a paradigm where the analog operational properties of DRAM structures are used to perform bulk logic operations. While PUD promises high throughput at low energy and area cost, we uncover three limitations of existing PUD approaches that lead to significant inefficiencies: (i) static data representation, i.e., 2's complement with fixed bit-precision, leading to unnecessary computation over useless (i.e., inconsequential) data; (ii) support for only throughput-oriented execution, where the high latency of individual PUD operations can only be hidden in the presence of bulk data-level parallelism; and (iii) high latency for high-precision (e.g., 32-bit) operations. To address these issues, we propose Proteus, which builds on two key ideas. First, Proteus parallelizes the execution of independent primitives in a PUD operation by leveraging DRAM's internal parallelism. Second, Proteus reduces the bit-precision for PUD operations by leveraging narrow values (i.e., values with many leading zeros). We compare Proteus to different state-of-the-art computing platforms (CPU, GPU, and the SIMDRAM PUD architecture) for twelve real-world applications. Using a single DRAM bank, Proteus provides (i) 17x, 7.3x, and 10.2x the performance per mm2; and (ii) 90.3x, 21x, and 8.1x lower energy consumption than that of the CPU, GPU, and SIMDRAM, respectively, on average across twelve real-world applications. Proteus incurs low area cost on top of a DRAM chip (1.6%) and CPU die (0.03%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17466v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geraldo F. Oliveira, Mayank Kabra, Yuxin Guo, Kangqi Chen, A. Giray Ya\u{g}l{\i}k\c{c}{\i}, Melina Soysal, Mohammad Sadrosadati, Joaquin Olivares Bueno, Saugata Ghose, Juan G\'omez-Luna, Onur Mutlu</dc:creator>
    </item>
    <item>
      <title>Exploring the Potential of Wireless-enabled Multi-Chip AI Accelerators</title>
      <link>https://arxiv.org/abs/2501.17567</link>
      <description>arXiv:2501.17567v1 Announce Type: new 
Abstract: The insatiable appetite of Artificial Intelligence (AI) workloads for computing power is pushing the industry to develop faster and more efficient accelerators. The rigidity of custom hardware, however, conflicts with the need for scalable and versatile architectures capable of catering to the needs of the evolving and heterogeneous pool of Machine Learning (ML) models in the literature. In this context, multi-chiplet architectures assembling multiple (perhaps heterogeneous) accelerators are an appealing option that is unfortunately hindered by the still rigid and inefficient chip-to-chip interconnects. In this paper, we explore the potential of wireless technology as a complement to existing wired interconnects in this multi-chiplet approach. Using an evaluation framework from the state-of-the-art, we show that wireless interconnects can lead to speedups of 10% on average and 20% maximum. We also highlight the importance of load balancing between the wired and wireless interconnects, which will be further explored in future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17567v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Irabor, Mariam Musavi, Abhijit Das, Sergi Abadal</dc:creator>
    </item>
    <item>
      <title>Towards Reliable Systems: A Scalable Approach to AXI4 Transaction Monitoring</title>
      <link>https://arxiv.org/abs/2501.17605</link>
      <description>arXiv:2501.17605v1 Announce Type: new 
Abstract: In safety-critical SoC applications such as automotive and aerospace, reliable transaction monitoring is crucial for maintaining system integrity. This paper introduces a drop-in Transaction Monitoring Unit (TMU) for AXI4 subordinate endpoints that detects transaction failures including protocol violations or timeouts and triggers recovery by resetting the affected subordinates. Two TMU variants address different constraints: a Tiny-Counter solution for tightly area-constrained systems and a Full-Counter solution for critical subordinates in mixed-criticality SoCs. The Tiny-Counter employs a single counter per outstanding transaction, while the Full-Counter uses multiple counters to track distinct transaction stages, offering finer-grained monitoring and reducing detection latencies by up to hundreds of cycles at roughly 2.5x the area cost. The Full-Counter also provides detailed error logs for performance and bottleneck analysis. Evaluations at both IP and system levels confirm the TMU's effectiveness and low overhead. In GF12 technology, monitoring 16-32 outstanding transactions occupies 1330-2616 um2 for the Tiny-Counter and 3452-6787 um2 for the Full-Counter; moderate prescaler steps reduce these figures by 18-39% and 19-32%, respectively, with no loss of functionality. Results from a full-system integration demonstrate the TMU's robust and precise monitoring capabilities in safety-critical SoC environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17605v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoqun Liang, Thomas Benz, Alessandro Ottaviano, Angelo Garofalo, Luca Benini, Davide Rossi</dc:creator>
    </item>
    <item>
      <title>Functional ISS-Driven Verification of Superscalar RISC-V Processors</title>
      <link>https://arxiv.org/abs/2407.21192</link>
      <description>arXiv:2407.21192v2 Announce Type: replace 
Abstract: A time-efficient and comprehensive verification is a fundamental part of the design process for modern computing platforms, and it becomes ever more important and critical to optimize as the latter get ever more complex. SupeRFIVe is a methodology for the functional verification of superscalar processors that leverages an instruction set simulator to validate their correctness according to a simulation-based approach, interfacing a testbench for the design under test with the instruction set simulator by means of socket communication. We demonstrate the effectiveness of the SupeRFIVe methodology by applying it to verify the functional correctness of a RISC-V dual-issue superscalar CPU, leveraging the state-of-the-art RISC-V instruction set simulator Spike and executing a set of benchmark applications from the open literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21192v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICECS61496.2024.10848613</arxiv:DOI>
      <arxiv:journal_reference>2024 31st IEEE International Conference on Electronics, Circuits and Systems (ICECS), Nancy, France, 2024, pp. 1-4</arxiv:journal_reference>
      <dc:creator>Andrea Galimberti, Marco Vitali, Sebastiano Vittoria, Davide Zoni</dc:creator>
    </item>
    <item>
      <title>Blink: Fast Automated Design of Run-Time Power Monitors on FPGA-Based Computing Platforms</title>
      <link>https://arxiv.org/abs/2407.21367</link>
      <description>arXiv:2407.21367v2 Announce Type: replace 
Abstract: The current over-provisioned heterogeneous multi-cores require effective run-time optimization strategies, and the run-time power monitoring subsystem is paramount for their success. Several state-of-the-art methodologies address the design of a run-time power monitoring infrastructure for generic computing platforms. However, the power model's training requires time-consuming gate-level simulations that, coupled with the ever-increasing complexity of the modern heterogeneous platforms, dramatically hinder the usability of such solutions. This paper introduces Blink, a scalable framework for the fast and automated design of run-time power monitoring infrastructures targeting computing platforms implemented on FPGA. Blink optimizes the time-to-solution to deliver the run-time power monitoring infrastructure by replacing traditional methodologies' gate-level simulations and power trace computations with behavioral simulations and direct power trace measurements. Applying Blink to multiple designs mixing a set of HLS-generated accelerators from a state-of-the-art benchmark suite demonstrates an average time-to-solution speedup of 18 times without affecting the quality of the run-time power estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21367v2</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICECS61496.2024.10849220</arxiv:DOI>
      <arxiv:journal_reference>2024 31st IEEE International Conference on Electronics, Circuits and Systems (ICECS), Nancy, France, 2024, pp. 1-4</arxiv:journal_reference>
      <dc:creator>Andrea Galimberti, Michele Piccoli, Davide Zoni</dc:creator>
    </item>
    <item>
      <title>Event-based backpropagation on the neuromorphic platform SpiNNaker2</title>
      <link>https://arxiv.org/abs/2412.15021</link>
      <description>arXiv:2412.15021v3 Announce Type: replace-cross 
Abstract: Neuromorphic computing aims to replicate the brain's capabilities for energy efficient and parallel information processing, promising a solution to the increasing demand for faster and more efficient computational systems. Efficient training of neural networks on neuromorphic hardware requires the development of training algorithms that retain the sparsity of spike-based communication during training. Here, we report on the first implementation of event-based backpropagation on the SpiNNaker2 neuromorphic hardware platform. We use EventProp, an algorithm for event-based backpropagation in spiking neural networks (SNNs), to compute exact gradients using sparse communication of error signals between neurons. Our implementation computes multi-layer networks of leaky integrate-and-fire neurons using discretized versions of the differential equations and their adjoints, and uses event packets to transmit spikes and error signals between network layers. We demonstrate a proof-of-concept of batch-parallelized, on-chip training of SNNs using the Yin Yang dataset, and provide an off-chip implementation for efficient prototyping, hyper-parameter search, and hybrid training methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15021v3</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 30 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel B\'ena, Timo Wunderlich, Mahmoud Akl, Bernhard Vogginger, Christian Mayr, Hector Andres Gonzales</dc:creator>
    </item>
  </channel>
</rss>
