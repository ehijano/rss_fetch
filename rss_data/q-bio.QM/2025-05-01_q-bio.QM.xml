<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 02 May 2025 01:30:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>ProT-GFDM: A Generative Fractional Diffusion Model for Protein Generation</title>
      <link>https://arxiv.org/abs/2504.21092</link>
      <description>arXiv:2504.21092v1 Announce Type: new 
Abstract: This work introduces the generative fractional diffusion model for protein generation (ProT-GFDM), a novel generative framework that employs fractional stochastic dynamics for protein backbone structure modeling. This approach builds on the continuous-time score-based generative diffusion modeling paradigm, where data are progressively transformed into noise via a stochastic differential equation and reversed to generate structured samples. Unlike classical methods that rely on standard Brownian motion, ProT-GFDM employs a fractional stochastic process with superdiffusive properties to improve the capture of long-range dependencies in protein structures. Trained on protein fragments from the Protein Data Bank, ProT-GFDM outperforms conventional score-based models, achieving a 7.19% increase in density, a 5.66% improvement in coverage, and a 1.01% reduction in the Frechet inception distance. By integrating fractional dynamics with computationally efficient sampling, the proposed framework advances generative modeling for structured biological data, with implications for protein design and computational drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21092v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiao Liang, Wentao Ma, Eric Paquet, Herna Lydia Viktor, Wojtek Michalowski</dc:creator>
    </item>
    <item>
      <title>MovementVR: An open-source tool for the study of motor control and learning in virtual reality</title>
      <link>https://arxiv.org/abs/2504.21696</link>
      <description>arXiv:2504.21696v1 Announce Type: new 
Abstract: Virtual reality (VR) is increasingly used to enhance the ecological validity of motor control and learning studies by providing immersive, interactive environments with precise motion tracking. However, designing realistic VR-based motor tasks remains complex, requiring advanced programming skills and limiting accessibility in research and clinical settings. MovementVR is an open-source platform designed to address these challenges by enabling the creation of customizable, naturalistic reaching tasks in VR without coding expertise. It integrates physics-based hand-object interactions, real-time hand tracking, and flexible experimental paradigms, including motor adaptation and reinforcement learning. The intuitive graphical user interface (GUI) allows researchers to customize task parameters and paradigm structure. Unlike existing platforms, MovementVR eliminates the need for scripting while supporting extensive customization and preserving ecological validity and realism. In addition to reducing technical barriers, MovementVR lowers financial constraints by being compatible with consumer-grade VR headsets. It is freely available with comprehensive documentation, facilitating broader adoption in movement research and rehabilitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21696v1</guid>
      <category>q-bio.QM</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cristina Rossi, Rini Varghese, Amy J Bastian</dc:creator>
    </item>
    <item>
      <title>Vision-Language Model-Based Semantic-Guided Imaging Biomarker for Early Lung Cancer Detection</title>
      <link>https://arxiv.org/abs/2504.21344</link>
      <description>arXiv:2504.21344v1 Announce Type: cross 
Abstract: Objective: A number of machine learning models have utilized semantic features, deep features, or both to assess lung nodule malignancy. However, their reliance on manual annotation during inference, limited interpretability, and sensitivity to imaging variations hinder their application in real-world clinical settings. Thus, this research aims to integrate semantic features derived from radiologists' assessments of nodules, allowing the model to learn clinically relevant, robust, and explainable features for predicting lung cancer. Methods: We obtained 938 low-dose CT scans from the National Lung Screening Trial with 1,246 nodules and semantic features. The Lung Image Database Consortium dataset contains 1,018 CT scans, with 2,625 lesions annotated for nodule characteristics. Three external datasets were obtained from UCLA Health, the LUNGx Challenge, and the Duke Lung Cancer Screening. We finetuned a pretrained Contrastive Language-Image Pretraining model with a parameter-efficient fine-tuning approach to align imaging and semantic features and predict the one-year lung cancer diagnosis. Results: We evaluated the performance of the one-year diagnosis of lung cancer with AUROC and AUPRC and compared it to three state-of-the-art models. Our model demonstrated an AUROC of 0.90 and AUPRC of 0.78, outperforming baseline state-of-the-art models on external datasets. Using CLIP, we also obtained predictions on semantic features, such as nodule margin (AUROC: 0.81), nodule consistency (0.81), and pleural attachment (0.84), that can be used to explain model predictions. Conclusion: Our approach accurately classifies lung nodules as benign or malignant, providing explainable outputs, aiding clinicians in comprehending the underlying meaning of model predictions. This approach also prevents the model from learning shortcuts and generalizes across clinical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21344v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luoting Zhuang, Seyed Mohammad Hossein Tabatabaei, Ramin Salehi-Rad, Linh M. Tran, Denise R. Aberle, Ashley E. Prosper, William Hsu</dc:creator>
    </item>
    <item>
      <title>Advancing Precision Oncology Through Modeling of Longitudinal and Multimodal Data</title>
      <link>https://arxiv.org/abs/2502.07836</link>
      <description>arXiv:2502.07836v2 Announce Type: replace 
Abstract: Cancer evolves continuously over time through a complex interplay of genetic, epigenetic, microenvironmental, and phenotypic changes. This dynamic behavior drives uncontrolled cell growth, metastasis, immune evasion, and therapy resistance, posing challenges for effective monitoring and treatment. However, today's data-driven research in oncology has primarily focused on cross-sectional analysis using data from a single modality, limiting the ability to fully characterize and interpret the disease's dynamic heterogeneity. Advances in multiscale data collection and computational methods now enable the discovery of longitudinal multimodal biomarkers for precision oncology. Longitudinal data reveal patterns of disease progression and treatment response that are not evident from single-timepoint data, enabling timely abnormality detection and dynamic treatment adaptation. Multimodal data integration offers complementary information from diverse sources for more precise risk assessment and targeting of cancer therapy. In this review, we survey methods of longitudinal and multimodal modeling, highlighting their synergy in providing multifaceted insights for personalized care tailored to the unique characteristics of a patient's cancer. We summarize the current challenges and future directions of longitudinal multimodal analysis in advancing precision oncology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07836v2</guid>
      <category>q-bio.QM</category>
      <category>cs.LG</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luoting Zhuang, Stephen H. Park, Steven J. Skates, Ashley E. Prosper, Denise R. Aberle, William Hsu</dc:creator>
    </item>
    <item>
      <title>The Global Diffusion Limit for the Space Dependent Variable-Order Time-Fractional Diffusion Equation</title>
      <link>https://arxiv.org/abs/2504.18787</link>
      <description>arXiv:2504.18787v2 Announce Type: replace-cross 
Abstract: The diffusion equation and its time-fractional counterpart can be obtained via the diffusion limit of continuous-time random walks with exponential and heavy-tailed waiting time distributions. The space dependent variable-order time-fractional diffusion equation is a generalization of the time-fractional diffusion equation with a fractional exponent that varies over space, modelling systems with spatial heterogeneity. However, there has been limited work on defining a global diffusion limit and an underlying random walk for this macroscopic governing equation, which is needed to make meaningful interpretations of the parameters for applications. Here, we introduce continuous time and discrete time random walk models that limit to the variable-order fractional diffusion equation via a global diffusion limit and space- and time- continuum limits. From this, we show how the master equation of the discrete time random walk can be used to provide a numerical method for solving the variable-order fractional diffusion equation. The results in this work provide underlying random walks and an improved understanding of the diffusion limit for the variable-order fractional diffusion equation, which is critical for the development, calibration and validation of models for diffusion in spatially inhomogeneous media with traps and obstacles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18787v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 01 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christopher N. Angstmann, Daniel S. Han, Bruce I. Henry, Boris Z. Huang, Zhuang Xu</dc:creator>
    </item>
  </channel>
</rss>
