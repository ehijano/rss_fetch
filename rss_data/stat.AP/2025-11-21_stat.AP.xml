<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:02:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Models with Accelerated Failure Conditionals</title>
      <link>https://arxiv.org/abs/2511.15769</link>
      <description>arXiv:2511.15769v1 Announce Type: cross 
Abstract: Arnold and Arvanitis (2020) introduced a novel bivariate conditionally specified distribution, a distribution in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This novel conditioning regime was achieved through the use of survival functions, and the approach was termed the accelerated failure conditionals model. In their work, the conditioning framework was constructed using the exponential distribution. Although further generalization was proposed, challenges emerged in deriving the necessary and sufficient conditions for valid joint survival functions. The present study achieves such generalization, extending the conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, moving beyond distributional families whose marginal densities are non-increasing. The resulting models are fully specified through closed-form expressions for their moments, with simulations implemented using either a copula-based procedure or the Metropolis-Hastings algorithm. Empirical applications to two datasets, each featuring variables which are unimodal and skewed, demonstrate that the models with flexible, non-monotonic marginal densities yield a superior fit relative to those models with marginal densities restricted to monotonically decaying forms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15769v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani</dc:creator>
    </item>
    <item>
      <title>Multidimensional scaling of two-mode three-way asymmetric dissimilarities: finding archetypal profiles and clustering</title>
      <link>https://arxiv.org/abs/2511.15813</link>
      <description>arXiv:2511.15813v1 Announce Type: cross 
Abstract: Multidimensional scaling visualizes dissimilarities among objects and reduces data dimensionality. While many methods address symmetric proximity data, asymmetric and especially three-way proximity data (capturing relationships across multiple occasions) remain underexplored. Recent developments, such as the h-plot, enable the analysis of asymmetric and non-reflexive relationships by embedding dissimilarities in a Euclidean space, allowing further techniques like archetypoid analysis to identify representative extreme profiles. However, no existing methods extract archetypal profiles from three-way asymmetric proximity data. This work extends the h-plot methodology to three-way proximity data under both symmetric and asymmetric, conditional and unconditional frameworks. The proposed approach offers several advantages: intuitive interpretability through a unified Euclidean representation; an explicit, eigenvector-based analytical solution free from local minima; scale invariance under linear transformations; computational efficiency for large matrices; and a straightforward goodness-of-fit evaluation. Furthermore, it enables the identification of archetypal profiles and clustering structures for three-way asymmetric proximities. Its performance is compared with existing models for multidimensional scaling and clustering, and illustrated through a financial application. All data and code are provided to facilitate reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15813v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleix Alcacer, Rafael Benitez, Vicente J. Bolos, Irene Epifanio</dc:creator>
    </item>
    <item>
      <title>Bayesian semiparametric modelling of biomarker variability in joint models</title>
      <link>https://arxiv.org/abs/2511.15882</link>
      <description>arXiv:2511.15882v1 Announce Type: cross 
Abstract: There is growing interest in the role of within-individual variability (WIV) in biomarker trajectories for assessing disease risk and progression. A trajectory-based definition that has attracted recent attention characterises WIV as the curvature-based roughness of the latent biomarker trajectory (TB-WIV). To rigorously evaluate the association between TB-WIV and clinical outcomes and to perform dynamic risk prediction, joint models for longitudinal and time-to-event data (JM) are necessary. However, specifying the longitudinal trajectory is critical in this framework and poses methodological challenges. In this work, we investigate three Bayesian semiparametric approaches for longitudinal modelling and TB-WIV estimation within the JM framework to improve stability and accuracy over existing approaches. Two key methods are newly introduced: one based on Bayesian penalised splines (P-splines) and another on functional principal component analysis (FPCA). Using extensive simulation studies, we compare their performance under two important TB-WIV definitions against established approaches. Our results demonstrate overall inferential and predictive advantages of the proposed P-spline and FPCA-based approaches while also providing insights that guide method choice and interpretation of inference results. The proposed approaches are applied to data from the UK Cystic Fibrosis Registry, where, for the first time, we identify a significant positive association between lung function TB-WIV and mortality risk in patients with cystic fibrosis and demonstrate improved predictive performance for survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15882v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sida Chen, Jessica K. Barrett, Marco Palma, Jianxin Pan, Brian D. M. Tom</dc:creator>
    </item>
    <item>
      <title>Cross-Balancing for Data-Informed Design and Efficient Analysis of Observational Studies</title>
      <link>https://arxiv.org/abs/2511.15896</link>
      <description>arXiv:2511.15896v1 Announce Type: cross 
Abstract: Causal inference starts with a simple idea: compare groups that differ by treatment, not much else. Traditionally, similar groups are constructed using only observed covariates; however, it remains a long-standing challenge to incorporate available outcome data into the study design while preserving valid inference. In this paper, we study the general problem of covariate adjustment, effect estimation, and statistical inference when balancing features are constructed or selected with the aid of outcome information from the data. We propose cross-balancing, a method that uses sample splitting to separate the error in feature construction from the error in weight estimation. Our framework addresses two cases: one where the features are learned functions and one where they are selected from a potentially high-dimensional dictionary. In both cases, we establish mild and general conditions under which cross-balancing produces consistent, asymptotically normal, and efficient estimators. In the learned-function case, cross-balancing achieves finite-sample bias reduction relative to plug-in-type estimators, and is multiply robust when the learned features converge at slow rates. In the variable-selection case, cross-balancing only requires a product condition on how well the selected variables approximate true functions. We illustrate cross-balancing in extensive simulations and an observational study, showing that careful use of outcome information can substantially improve both estimation and inference while maintaining interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15896v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Jin, Jos\'e Zubizarreta</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation Methods for Multivariate Health and Demographic Outcomes using Complex Survey Data</title>
      <link>https://arxiv.org/abs/2511.15917</link>
      <description>arXiv:2511.15917v1 Announce Type: cross 
Abstract: Improving health in the most disadvantaged populations requires reliable estimates of health and demographic indicators to inform policy and interventions. Low- and middle-income countries with the largest burden of disease and disability tend to have the least comprehensive data, relying primarily on household surveys. Subnational estimates are increasingly used to inform targeted interventions and health policies. Producing reliable estimates from these data at fine geographical scales requires statistical modeling, and small area estimation models are commonly used in this context. Although most current methods model univariate outcomes, improved estimates may be attained by borrowing strength across related outcomes via multivariate modeling. In this paper, we develop classes of area- and unit-level multivariate shared component models using complex survey data. This framework jointly models multiple outcomes to improve accuracy of estimates compared to separately fitting univariate models. We conduct simulation studies to validate the methodology and use the proposed approach on survey data from Kenya in 2014; first, to jointly model height-for-age and weight-for-age in children, and second, to model three categories of contraceptive use in women. These models produce improved estimates compared to univariate and naive multivariate modeling approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15917v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin E Schumacher, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Robust Estimation under Outcome Dependent Right Censoring in Huntington Disease: Estimators for Low and High Censoring Rates</title>
      <link>https://arxiv.org/abs/2511.15929</link>
      <description>arXiv:2511.15929v1 Announce Type: cross 
Abstract: Across health applications, researchers model outcomes as a function of time to an event, but the event time is right-censored for participants who exit the study or otherwise do not experience the event during follow-up. When censoring depends on the outcome-as in neurodegenerative disease studies where dropout is potentially related to disease severity-standard regression estimators produce biased estimates. We develop three consistent estimators for this outcome-dependent censoring setting: two augmented inverse probability weighted (AIPW) estimators and one maximum likelihood estimator (MLE). We establish their asymptotic properties and derive their robust sandwich variance estimators that account for nuisance parameter estimation. A key contribution is demonstrating that the choice of estimator to use depends on the censoring rate-the MLE performs best under low censoring rates, while the AIPW estimators yield lower bias and a higher nominal coverage under high censoring rates. We apply our estimators to Huntington disease data to characterize health decline leading up to mild cognitive impairment onset. The AIPW estimator with robustness matrix provided clinically-backed estimates with improved precision over inverse probability weighting, while MLE exhibited bias. Our results provide practical guidance for estimator selection based on censoring rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15929v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesus E. Vazquez, Yanyuan Ma, Karen Marder, Tanya P. Garcia</dc:creator>
    </item>
    <item>
      <title>A Simple and Robust Multi-Fidelity Data Fusion Method for Effective Modeling of Citizen-Science Air Pollution Data</title>
      <link>https://arxiv.org/abs/2511.15942</link>
      <description>arXiv:2511.15942v1 Announce Type: cross 
Abstract: We propose a robust multi-fidelity Gaussian process for integrating sparse, high-quality reference monitors with dense but noisy citizen-science sensors. The approach replaces the Gaussian log-likelihood in the high-fidelity channel with a global Huber loss applied to precision-weighted residuals, yielding bounded influence on all parameters, including the cross-fidelity coupling, while retaining the flexibility of co-kriging. We establish attenuation and unbounded influence of the Gaussian maximum likelihood estimator under low-fidelity contamination and derive explicit finite bounds for the proposed estimator that clarify how whitening and mean-shift sensitivity determine robustness. Monte Carlo experiments with controlled contamination show that the robust estimator maintains stable MAE and RMSE as anomaly magnitude and frequency increase, whereas the Gaussian MLE deteriorates rapidly. In an empirical study of PM2.5 concentrations in Hamburg, combining UBA monitors with openSenseMap data, the method consistently improves cross-validated predictive accuracy and yields coherent uncertainty maps without relying on auxiliary covariates. The framework remains computationally scalable through diagonal or low-rank whitening and is fully reproducible with publicly available code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15942v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camilla Andreozzi, Pietro Colombo, Philipp Otto</dc:creator>
    </item>
    <item>
      <title>Achieving Skilled and Reliable Daily Probabilistic Forecasts of Wind Power at Subseasonal-to-Seasonal Timescales over France</title>
      <link>https://arxiv.org/abs/2511.16164</link>
      <description>arXiv:2511.16164v1 Announce Type: cross 
Abstract: Accurate and reliable wind power forecasts are crucial for grid stability, balancing supply and demand, and market risk management. Even though short-term weather forecasts have been thoroughly used to provide short-term renewable power predictions, forecasts involving longer prediction horizons still need investigations. Despite the recent progress in subseasonal-to-seasonal weather probabilistic forecasting, their use for wind power prediction usually involves both temporal and spatial aggregation achieve reasonable skill. In this study, we present a forecasting pipeline enabling to transform ECMWF subseasonal-to-seasonal weather forecasts into wind power forecasts for lead times ranging from 1 day to 46 days at daily resolution. This framework also include post-processing of the resulting power ensembles to account for the biases and lack of dispersion of the weather forecasts. We show that our method is able to outperform a climatological baseline by 50 % in terms of both Continuous Ranked Probability Skill Score and Ensemble Mean Squared Error while also providing near perfect calibration of the forecasts for lead times ranging from 15 to 46 days.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16164v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eloi Lindas, Yannig Goude, Philippe Ciais</dc:creator>
    </item>
    <item>
      <title>Targeted Parameter Estimation for Robust Empirical Bayes Ranking</title>
      <link>https://arxiv.org/abs/2511.16530</link>
      <description>arXiv:2511.16530v1 Announce Type: cross 
Abstract: Ordering the expected outcomes across a collection of clusters after performing a covariate adjustment commonly arises in many applied settings, such as healthcare provider evaluation. Regression parameters in such covariate adjustment models are frequently estimated by maximum likelihood or through other criteria that do not directly evaluate the quality of the rankings resulting from using a particular set of parameter estimates. In this article, we propose both a novel empirical Bayes ranking procedure and an associated estimation approach for finding the regression parameters of the covariate adjustment model. By building our ranking approach around estimating approximate percentiles of the covariate-adjusted cluster-level means, we are able to develop manageable expressions for the expected ranking squared-error loss associated with any choice of the covariate-adjustment model parameters, and we harness this to generate a novel unbiased estimator for this expected loss. Minimization of this unbiased estimator directly leads to a novel ranking procedure that is often more robust than conventional empirical Bayes ranking methods. Through a series of simulation studies, we show that our approach consistently delivers improved ranking squared-error performance relative to competing methods, such as posterior expected ranks and ranking the components of the best linear unbiased predictor. Estimating rankings using our method is illustrated with an example from a longitudinal study evaluating test scores across a large group of schools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16530v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas C. Henderson, Nicholas Hartman</dc:creator>
    </item>
    <item>
      <title>Toward Valid Generative Clinical Trial Data with Survival Endpoints</title>
      <link>https://arxiv.org/abs/2511.16551</link>
      <description>arXiv:2511.16551v1 Announce Type: cross 
Abstract: Clinical trials face mounting challenges: fragmented patient populations, slow enrollment, and unsustainable costs, particularly for late phase trials in oncology and rare diseases. While external control arms built from real-world data have been explored, a promising alternative is the generation of synthetic control arms using generative AI. A central challenge is the generation of time-to-event outcomes, which constitute primary endpoints in oncology and rare disease trials, but are difficult to model under censoring and small sample sizes. Existing generative approaches, largely GAN-based, are data-hungry, unstable, and rely on strong assumptions such as independent censoring. We introduce a variational autoencoder (VAE) that jointly generates mixed-type covariates and survival outcomes within a unified latent variable framework, without assuming independent censoring. Across synthetic and real trial datasets, we evaluate our model in two realistic scenarios: (i) data sharing under privacy constraints, where synthetic controls substitute for original data, and (ii) control-arm augmentation, where synthetic patients mitigate imbalances between treated and control groups. Our method outperforms GAN baselines on fidelity, utility, and privacy metrics, while revealing systematic miscalibration of type I error and power. We propose a post-generation selection procedure that improves calibration, highlighting both progress and open challenges for generative survival modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16551v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Machine Learning for Health (ML4H) 2025</arxiv:journal_reference>
      <dc:creator>Perrine Chassat, Van Tuan Nguyen, Lucas Ducrot, Emilie Lanoy, Agathe Guilloux</dc:creator>
    </item>
    <item>
      <title>Sensor Informativeness, Identifiability, and Uncertainty in Bayesian Inverse Problems for Structural Health Monitoring</title>
      <link>https://arxiv.org/abs/2511.16628</link>
      <description>arXiv:2511.16628v1 Announce Type: cross 
Abstract: In Structural Health Monitoring (SHM), the recovery of distributed mechanical parameters from sparse data is often ill-posed, raising critical questions about identifiability and the reliability of inferred states. While deterministic regularization methods such as Tikhonov stabilise the inversion, they provide little insight into the spatial limits of resolution or the inherent uncertainty of the solution. This paper presents a Bayesian inverse framework that rigorously quantifies these limits, using the identification of distributed flexural rigidity from rotation (tilt) influence lines as a primary case study. Fisher information is employed as a diagnostic metric to quantify sensor informativeness, revealing how specific sensor layouts and load paths constrain the recoverable spatial features of the parameter field.
  The methodology is applied to the full-scale openLAB research bridge (TU Dresden) using data from controlled vehicle passages. Beyond estimating the flexural rigidity profile, the Bayesian formulation produces credible intervals that expose regions of practical non-identifiability, which deterministic methods may obscure. The results demonstrate that while the measurement data carry high information content for the target parameters, their utility is spatially heterogeneous and strictly bounded by the experiment design. The proposed framework unifies identification with uncertainty quantification, providing a rigorous basis for optimising sensor placement and interpreting the credibility of SHM diagnostics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16628v1</guid>
      <category>cs.CE</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tammam Bakeer, Max Herbers, Steffen Marx</dc:creator>
    </item>
    <item>
      <title>Effect Size-Driven Pathway Meta-Analysis for Gene Expression Data</title>
      <link>https://arxiv.org/abs/2501.13583</link>
      <description>arXiv:2501.13583v2 Announce Type: replace 
Abstract: The proliferation of omics datasets in public repositories has created unprecedented opportunities for biomedical research but has also posed significant challenges for their integration, particularly due to missing genes and platform-specific discrepancies. Traditional gene expression metaanalysis often focuses on individual genes, leading to data loss and limited biological insights when there are missing genes across different studies. To address these limitations, we propose GSEMA (Gene Set Enrichment Meta-Analysis), a novel methodology that leverages singlesample enrichment scoring to aggregate gene expression data into pathway-level matrices. By applying meta-analysis techniques to enrichment scores, GSEMA preserves the magnitude and directionality of effects, enabling the definition of pathway activity across datasets. Using simulated data and case studies on Systemic Lupus Erythematosus (SLE) and Parkinson's Disease (PD), we demonstrate that GSEMA outperforms other methods in controlling false positive rates while providing meaningful biological interpretations. GSEMA methodology is implemented as an R package available on CRAN repository</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13583v2</guid>
      <category>stat.AP</category>
      <category>q-bio.GN</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Juan Antonio Villatoro-Garc\'ia, Pablo Pedro Jurado-Basc\'on, Pedro Carmona-S\'aez</dc:creator>
    </item>
    <item>
      <title>Robust a posteriori estimation of probit-lognormal seismic fragility curves via sequential design of experiments and constrained reference prior</title>
      <link>https://arxiv.org/abs/2503.07343</link>
      <description>arXiv:2503.07343v3 Announce Type: replace 
Abstract: A seismic fragility curve expresses the probability of failure of a structure conditional to an intensity measure (IM) derived from seismic signals. When only limited data is available, the practitioner often refers to the probit-lognormal model coupled with maximum likelihood estimation (MLE) to obtain estimates of these curves. This means that only a binary indicator of the state (BIS) of the structure is known, namely a failure or non-failure state indicator, when it is subjected to a seismic signal with an intensity measure IM. In this context, the objective of this work is to propose a method for optimally estimating such curves by obtaining the most precise estimate possible with the minimum of data. The novelty of our work is twofold. First, we present and show how to mitigate the likelihood degeneracy problem which is ubiquitous with small data sets and hampers frequentist approaches such as MLE. Second, we propose a novel strategy for sequential design of experiments (DoE) that selects seismic signals from a large database of synthetic or real signals via their IM values, to be applied to structures to evaluate the corresponding BISs. This strategy relies on a criterion based on information theory in a Bayesian framework. It therefore aims to sequentially designate the IM value such that the pair (IM, BIS) has on average, with respect to the BIS of the structure, the greatest impact on the posterior distribution of the fragility curve. The methodology is applied to a case study from the nuclear industry. The results demonstrate its ability to efficiently and robustly estimate the fragility curve, and to avoid degeneracy even with a limited amount of data, i.e., less than 100. Furthermore, we demonstrate that the estimates quickly reach the model bias induced by the probit-lognormal modeling. Eventually, two criteria are suggested to help the user stop the DoE algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.07343v3</guid>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Van Biesbroeck, Cl\'ement Gauchy, Cyril Feau, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>A survey of using EHR as real-world evidence for discovering and validating new drug indications</title>
      <link>https://arxiv.org/abs/2505.24767</link>
      <description>arXiv:2505.24767v2 Announce Type: replace 
Abstract: Electronic Health Records (EHRs) have been increasingly used as real-world evidence (RWE) to support the discovery and validation of new drug indications. This paper surveys current approaches to EHR-based drug repurposing, covering data sources, processing methodologies, and representation techniques. It discusses study designs and statistical frameworks for evaluating drug efficacy. Key challenges in validation are discussed, with emphasis on the role of large language models (LLMs) and target trial emulation. By synthesizing recent developments and methodological advances, this work provides a foundational resource for researchers aiming to translate real-world data into actionable drug-repurposing evidence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24767v2</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nabasmita Talukdar, Xiaodan Zhang, Shreya Paithankar, Hui Wang, Bin Chen</dc:creator>
    </item>
    <item>
      <title>The conditional saddlepoint approximation for fast and accurate large-scale hypothesis testing</title>
      <link>https://arxiv.org/abs/2407.08911</link>
      <description>arXiv:2407.08911v4 Announce Type: replace-cross 
Abstract: Large-scale testing in modern applications such as genomics often entails a trade-off between accuracy and speed: multiplicity corrections push cutoffs deep into the tails, where normal approximations can fail, while resampling is accurate but computationally expensive for large datasets. To resolve this impasse in the context of conditional independence testing, we introduce spaCRT, a closed-form saddlepoint approximation (SPA) for the distilled conditional randomization test (dCRT) that retains the statistical accuracy of dCRT's resampling while avoiding its computational cost. We prove that spaCRT's relative approximation error vanishes asymptotically by establishing a general theorem on the relative error of conditional SPAs. Because dCRT uses a plug-in nuisance regression, we specialize our guarantees to common choices: low-dimensional generalized linear model (GLM), high-dimensional GLM lasso, and kernel ridge regression. Our general theorem is, to our knowledge, the first rigorous technical tool for analyzing SPAs for resampling tests, which had previously been justified only heuristically. It extends beyond spaCRT, as we exemplify by justifying an SPA for the classical sign-flipping location test. Empirically, spaCRT matches dCRT's statistical performance by approximating its p-values with median error 1-12% across settings while delivering a 250x speedup on a single-cell CRISPR screen dataset with 85,000 hypotheses. Building on dCRT's versatility, spaCRT and its open-source R package enable fast and accurate large-scale testing across diverse applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08911v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Niu, Jyotishka Ray Choudhury, Eugene Katsevich</dc:creator>
    </item>
    <item>
      <title>CardioLab: Laboratory Values Estimation from Electrocardiogram Features - An Exploratory Study</title>
      <link>https://arxiv.org/abs/2407.18629</link>
      <description>arXiv:2407.18629v3 Announce Type: replace-cross 
Abstract: Laboratory value represents a cornerstone of medical diagnostics, but suffers from slow turnaround times, and high costs and only provides information about a single point in time. The continuous estimation of laboratory values from non-invasive data such as electrocardiogram (ECG) would therefore mark a significant frontier in healthcare monitoring. Despite its potential, this domain remains relatively underexplored. In this preliminary study, we used a publicly available dataset (MIMIC-IV-ECG) to investigate the feasibility of inferring laboratory values from ECG features and patient demographics using tree-based models (XGBoost). We define the prediction task as a binary problem of whether the lab value falls into low or high abnormalities. We assessed model performance with AUROC. Our findings demonstrate promising results in the estimation of laboratory values related to different organ systems. While further research and validation are warranted to fully assess the clinical utility and generalizability of the approach, our findings lay the groundwork for future investigations for laboratory value estimation using ECG data. Such advancements hold promise for revolutionizing predictive healthcare applications, offering faster, non-invasive, and more affordable means of patient monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18629v3</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Miguel Lopez Alcaraz, Nils Strodthoff</dc:creator>
    </item>
    <item>
      <title>Increasing competitiveness by imbalanced groups: The example of the 48-team FIFA World Cup</title>
      <link>https://arxiv.org/abs/2502.08565</link>
      <description>arXiv:2502.08565v3 Announce Type: replace-cross 
Abstract: A match played in a sports tournament can be called stakeless if at least one team is indifferent to its outcome because it already has qualified or has been eliminated. Such a game threatens fairness since teams may not exert full effort without incentives. This paper suggests a novel classification for stakeless matches based on their expected outcome: they are more costly if the indifferent team is more likely to win by playing honestly. Our approach is illustrated with the 2026 FIFA World Cup, the first edition of the competition with 48 teams. We propose a novel format based on imbalanced groups, which substantially reduces the probability of stakeless matches played by the strongest teams according to Monte Carlo simulations. The new design also increases the uncertainty of match outcomes and requires fewer matches. Governing bodies in sports are encouraged to consider our innovative idea in order to enhance the competitiveness of their tournaments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08565v3</guid>
      <category>math.OC</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ejor.2025.11.025</arxiv:DOI>
      <arxiv:journal_reference>European Journal of Operational Research, 2025, forthcoming</arxiv:journal_reference>
      <dc:creator>L\'aszl\'o Csat\'o, Andr\'as Gyimesi</dc:creator>
    </item>
    <item>
      <title>Towards modelling lifetime default risk: Exploring different subtypes of recurrent event Cox-regression models</title>
      <link>https://arxiv.org/abs/2505.01044</link>
      <description>arXiv:2505.01044v2 Announce Type: replace-cross 
Abstract: In the pursuit of modelling a loan's probability of default (PD) over its lifetime, repeat default events are often ignored when using Cox Proportional Hazard (PH) models. Excluding such events may produce biased and inaccurate PD-estimates, which can compromise financial buffers against future losses. Accordingly, we investigate a few subtypes of Cox-models that can incorporate recurrent default events. Using South African mortgage data, we explore both the Andersen-Gill (AG) and the Prentice-Williams-Peterson (PWP) spell-time models. These models are compared against a baseline that deliberately ignores recurrent events, called the time to first default (TFD) model. Models are evaluated using Harrell's c-statistic, adjusted Cox-Sell residuals, and a novel extension of time-dependent receiver operating characteristic (ROC) analysis. From these Cox-models, we demonstrate how to derive a portfolio-level term-structure of default risk, which is a series of marginal PD-estimates at each point of the average loan's lifetime. While the TFD- and PWP-models do not differ significantly across all diagnostics, the AG-model underperformed expectations. Depending on the prevalence of recurrent defaults, one may therefore safely ignore them when estimating lifetime default risk. Accordingly, our work enhances the current practice of using Cox-modelling in producing timeous and accurate PD-estimates under IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01044v2</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Bernard Scheepers</dc:creator>
    </item>
    <item>
      <title>Modeling temporal hypergraphs</title>
      <link>https://arxiv.org/abs/2506.01408</link>
      <description>arXiv:2506.01408v3 Announce Type: replace-cross 
Abstract: Networks representing social, biological, technological or other systems are often characterized by higher-order interaction involving any number of nodes. Temporal hypergraphs are given by ordered sequences of hyperedges representing sets of nodes interacting at given points in time. In this paper we discuss how a recently proposed model family for time-stamped hyperedges - relational hyperevent models (RHEM) - can be employed to define tailored null distributions for temporal hypergraphs and to test and control for complex dependencies in hypergraph dynamics. RHEM can be specified with a given vector of temporal hyperedge statistics - functions that quantify the structural position of hyperedges in the history of previous hyperedges - and equate expected values of these statistics with their empirically observed values. This allows, for instance, to analyze the overrepresentation or underrepresentation of temporal hyperedge configurations in a model that reproduces the observed distributions of possibly complex sub-configurations, including but going beyond node degrees. Concrete examples include, but are not limited to, preferential attachment, repetition of subsets of any given size, triadic closure, homophily, and degree assortativity for subsets of any order.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01408v3</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J\"urgen Lerner, Marian-Gabriel H\^ancean, Matjaz Perc</dc:creator>
    </item>
    <item>
      <title>On Focusing Statistical Power for Searches and Measurements in Particle Physics</title>
      <link>https://arxiv.org/abs/2507.17831</link>
      <description>arXiv:2507.17831v2 Announce Type: replace-cross 
Abstract: Particle physics experiments rely on the (generalised) likelihood ratio test (LRT) for searches and measurements, which consist of composite hypothesis tests. However, this test is not guaranteed to be optimal, as the Neyman-Pearson lemma pertains only to simple hypothesis tests. Any choice of test statistic thus implicitly determines how statistical power varies across the parameter space. An improvement in the core statistical testing methodology for general settings with composite tests would have widespread ramifications across experiments. We discuss an alternate test statistic that provides the data analyzer an ability to focus the power of the test on physics-motivated regions of the parameter space. We demonstrate the improvement from this technique compared to the LRT on a Higgs $\rightarrow\tau\tau$ dataset simulated by the ATLAS experiment and a dark matter dataset inspired by the LZ experiment. We also employ machine learning to efficiently perform the Neyman construction, which is essential to ensure statistically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17831v2</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Aishik Ghosh, Rafael Izbicki, Ann Lee, Luca Masserano, Daniel Whiteson</dc:creator>
    </item>
    <item>
      <title>How many patients could we save with LLM priors?</title>
      <link>https://arxiv.org/abs/2509.04250</link>
      <description>arXiv:2509.04250v2 Announce Type: replace-cross 
Abstract: Imagine a world where clinical trials need far fewer patients to achieve the same statistical power, thanks to the knowledge encoded in large language models (LLMs). We present a novel framework for hierarchical Bayesian modeling of adverse events in multi-center clinical trials, leveraging LLM-informed prior distributions. Unlike data augmentation approaches that generate synthetic data points, our methodology directly obtains parametric priors from the model. Our approach systematically elicits informative priors for hyperparameters in hierarchical Bayesian models using a pre-trained LLM, enabling the incorporation of external clinical expertise directly into Bayesian safety modeling. Through comprehensive temperature sensitivity analysis and rigorous cross-validation on real-world clinical trial data, we demonstrate that LLM-derived priors consistently improve predictive performance compared to traditional meta-analytical approaches. This methodology paves the way for more efficient and expert-informed clinical trial design, enabling substantial reductions in the number of patients required to achieve robust safety assessment and with the potential to transform drug safety monitoring and regulatory decision making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04250v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.IR</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shota Arai, David Selby, Andrew Vargo, Sebastian Vollmer</dc:creator>
    </item>
    <item>
      <title>On Assessing Overall Survival (OS) in Oncology Studies</title>
      <link>https://arxiv.org/abs/2510.07122</link>
      <description>arXiv:2510.07122v2 Announce Type: replace-cross 
Abstract: In assessing Overall Survival (OS) in oncology studies, it is essential for the efficacy measure to be Logic-respecting, for otherwise patients may be incorrectly targeted. This paper explains, while Time Ratio (TR) is Logic-respecting, Hazard Ratio (HR) is not Logic-respecting. With Time Ratio (TR) being recommended, a smooth transitioning strategy is suggested. The conclusion states: Logicality requires, and Subgroup Mixable Estimation (SME) delivers, an efficacy assessment for the overall population within the range of minimum and maximum efficacy in the subgroups, no matter how outcome is measured, whichever logic-respecting efficacy measure is chosen, the same efficacy assessment regardless of how subgroups are stratified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07122v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jason C. Hsu</dc:creator>
    </item>
  </channel>
</rss>
