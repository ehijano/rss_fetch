<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Oct 2025 04:00:30 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A mathematical theory for understanding when abstract representations emerge in neural networks</title>
      <link>https://arxiv.org/abs/2510.09816</link>
      <description>arXiv:2510.09816v1 Announce Type: new 
Abstract: Recent experiments reveal that task-relevant variables are often encoded in approximately orthogonal subspaces of the neural activity space. These disentangled low-dimensional representations are observed in multiple brain areas and across different species, and are typically the result of a process of abstraction that supports simple forms of out-of-distribution generalization. The mechanisms by which such geometries emerge remain poorly understood, and the mechanisms that have been investigated are typically unsupervised (e.g., based on variational auto-encoders). Here, we show mathematically that abstract representations of latent variables are guaranteed to appear in the last hidden layer of feedforward nonlinear networks when they are trained on tasks that depend directly on these latent variables. These abstract representations reflect the structure of the desired outputs or the semantics of the input stimuli. To investigate the neural representations that emerge in these networks, we develop an analytical framework that maps the optimization over the network weights into a mean-field problem over the distribution of neural preactivations. Applying this framework to a finite-width ReLU network, we find that its hidden layer exhibits an abstract representation at all global minima of the task objective. We further extend these analyses to two broad families of activation functions and deep feedforward architectures, demonstrating that abstract representations naturally arise in all these scenarios. Together, these results provide an explanation for the widely observed abstract representations in both the brain and artificial neural networks, as well as a mathematically tractable toolkit for understanding the emergence of different kinds of representations in task-optimized, feature-learning network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09816v1</guid>
      <category>q-bio.NC</category>
      <category>math.OC</category>
      <category>physics.bio-ph</category>
      <category>stat.ML</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bin Wang, W. Jeffrey Johnston, Stefano Fusi</dc:creator>
    </item>
    <item>
      <title>Egocentric Visual Navigation through Hippocampal Sequences</title>
      <link>https://arxiv.org/abs/2510.09951</link>
      <description>arXiv:2510.09951v1 Announce Type: new 
Abstract: Sequential activation of place-tuned neurons in an animal during navigation is typically interpreted as reflecting the sequence of input from adjacent positions along the trajectory. More recent theories about such place cells suggest sequences arise from abstract cognitive objectives like planning. Here, we propose a mechanistic and parsimonious interpretation to complement these ideas: hippocampal sequences arise from intrinsic recurrent circuitry that propagates activity without readily available input, acting as a temporal memory buffer for extremely sparse inputs.We implement a minimal sequence generator inspired by neurobiology and pair it with an actor-critic learner for egocentric visual navigation. Our agent reliably solves a continuous maze without explicit geometric cues, with performance depending on the length of the recurrent sequence. Crucially, the model outperforms LSTM cores under sparse input conditions (16 channels, ~2.5% activity), but not under dense input, revealing a strong interaction between representational sparsity and memory architecture.In contrast to LSTM agents, hidden sequence units develop localized place fields, distance-dependent spatial kernels, and task-dependent remapping, while inputs orthogonalize and spatial information increases across layers. These phenomena align with neurobiological data and are causal to performance. Together, our results show that sparse input synergizes with sequence-generating dynamics, providing both a mechanistic account of place cell sequences in the mammalian hippocampus and a simple inductive bias for reinforcement learning based on sparse egocentric inputs in navigation tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09951v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiao-Xiong Lin, Yuk Hoi Yiu, Christian Leibold</dc:creator>
    </item>
    <item>
      <title>Neural Hardware for the Language of Thought: New Rules for an Old Game</title>
      <link>https://arxiv.org/abs/2510.10251</link>
      <description>arXiv:2510.10251v1 Announce Type: new 
Abstract: The Language of Thought (LOT) hypothesis posits that at least some important cognitive processes involve language-like representations. These representations must be processed by appropriate hardware. Since the organ of biological cognition is the nervous system, whether biological cognition relies on a LOT depends on how neural hardware works. I distinguish between different versions of LOT, articulate their hardware requirements, and consider which versions of LOT are supported by empirical evidence. I argue that the Classical LOT hypothesis (Fodor 1975) is ruled out; the version of LOT that is best supported by empirical evidence is the Nonclassical LOT thesis that some neural representations mirror some of the structure of natural language and represent in a language-like way, yet they encode information nondigitally and are processed by ordinary (nondigital, and hence Nonclassical) neural computations that rely not only on syntactic structure but many other features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10251v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gualtiero Piccinini</dc:creator>
    </item>
    <item>
      <title>AI-Assisted Geometric Analysis of Cultured Neuronal Networks: Parallels with the Cosmic Web</title>
      <link>https://arxiv.org/abs/2510.10286</link>
      <description>arXiv:2510.10286v1 Announce Type: new 
Abstract: Building on evidence of structural parallels between brain networks and the cosmic web [1], we apply AI-based geometric analysis to cultured neuronal networks. Isolated neurons self-organize into dendritic lattices shaped by reproducible wiring rules. These lattices show non-random features-frequent dendritic convergence, hub nodes, small-world connectivity, and large voids. Synaptic contacts cluster and strengthen at hubs. Strikingly, these properties mirror the cosmic web: dendritic branches resemble cosmic filaments and synapses map to galaxies. Quantitative metrics align across systems, suggesting shared underlying geometric principles. We invite cross-disciplinary collaboration to interrogate and extend these parallels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10286v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Wolfgang Kurz, Danny Baranes</dc:creator>
    </item>
    <item>
      <title>Artificial intelligence as a surrogate brain: Bridging neural dynamical models and data</title>
      <link>https://arxiv.org/abs/2510.10308</link>
      <description>arXiv:2510.10308v1 Announce Type: new 
Abstract: Recent breakthroughs in artificial intelligence (AI) are reshaping the way we construct computational counterparts of the brain, giving rise to a new class of ``surrogate brains''. In contrast to conventional hypothesis-driven biophysical models, the AI-based surrogate brain encompasses a broad spectrum of data-driven approaches to solve the inverse problem, with the primary objective of accurately predicting future whole-brain dynamics with historical data. Here, we introduce a unified framework of constructing an AI-based surrogate brain that integrates forward modeling, inverse problem solving, and model evaluation. Leveraging the expressive power of AI models and large-scale brain data, surrogate brains open a new window for decoding neural systems and forecasting complex dynamics with high dimensionality, nonlinearity, and adaptability. We highlight that the learned surrogate brain serves as a simulation platform for dynamical systems analysis, virtual perturbation, and model-guided neurostimulation. We envision that the AI-based surrogate brain will provide a functional bridge between theoretical neuroscience and translational neuroengineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10308v1</guid>
      <category>q-bio.NC</category>
      <category>cs.NE</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yinuo Zhang, Demao Liu, Zhichao Liang, Jiani Cheng, Kexin Lou, Jinqiao Duan, Ting Gao, Bin Hu, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>Evidence of Physiological Co-Modulation During Human-Animal Interaction: A Systematic Review</title>
      <link>https://arxiv.org/abs/2510.10559</link>
      <description>arXiv:2510.10559v1 Announce Type: new 
Abstract: This review examines the evidence in the literature for physiological co-modulation during human-animal interaction. The aim of this work is to identify studies that assessed co-modulation via simultaneous measurement of physiological signals in both species, performing quantitative comparisons, and evaluate the consistency of the findings.\\ We searched PubMed, EM-BASE, Scopus, Google Scholar, Animal Studies Repository, and the ''Consensus app'' tool between June and August 2025 (last search: August 5, 2025). Risk of bias was assessed using an adapted version of the ROBINS-I V2 tool. Results were grouped by data analysis method, interaction context, and physiological parameter. Data were synthesised narratively, in structured tables and in barplots. Thirty-seven studies were included, primarily focusing on dogs (n=22) and horses (n=15), framed primarily within the interaction contexts of Animal-Assisted Therapy and Intervention (AAT and AAI) and companionship. Cardiac and hormonal measures were most frequently assessed. Most studies (n = 20) performed correlation analyses. Sample sizes ranged from less than 10 to more than 130 dyads. Co-modulation resulted significant in 22 studies, partial (limited to subsets of data) in 9, and absent in 6. Time-series coupling methods yielded more consistent evidence than discrete-time correlations. Many studies had small samples and did not explicitly test for significant co-modulation. Evidence, while not conclusive, supports physiological co-modulation during human-animal interactions. However, studies' heterogeneity limits generalizability: rather than indicating a universal phenomenon, findings suggest co-modulation may emerge under specific biological and methodological conditions. Future research should explicitly test its presence across contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10559v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>G. Bargigli, L. Frassineti, A. Lanata', P. Baragli, C. Scopa, A. Vignoli</dc:creator>
    </item>
    <item>
      <title>Does Re-referencing Matter? Large Laplacian Filter Optimizes Single-Trial P300 BCI Performance</title>
      <link>https://arxiv.org/abs/2510.10733</link>
      <description>arXiv:2510.10733v1 Announce Type: new 
Abstract: Electroencephalography (EEG) provides a non-invasive window into brain activity, enabling Brain-Computer Interfaces (BCIs) for communication and control. However, their performance is limited by signal fidelity issues, among which the choice of re-referencing strategy is a pervasive but often overlooked preprocessing bias. Addressing controversies about its necessity and optimal choice, we adopted a quantified approach to evaluate four strategies - no re-referencing, Common Average Reference (CAR), small Laplacian, and large Laplacian - using 62-channels EEG (31 subjects, 2,520 trials). To our knowledge, this is the first study systematically quantifying their impact on single-trial P300 classification accuracy. Our controlled pipeline isolated re-referencing effects for source-space reconstruction (eLORETA with Phase Lag Index) and anatomically constrained classification. The large Laplacian resolves distributed P3b networks while maintaining P3a specificity, achieving the best P300 peak classification accuracy (81.57% hybrid method; 75.97% majority regions of interest). Performance follows a consistent and statistically significant hierarchy: large Laplacian &gt; CAR &gt; no re-reference &gt; small Laplacian, providing a foundation for unified methodological evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10733v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eva Guttmann-Flury, Jian Zhao, Mohamad Sawan</dc:creator>
    </item>
    <item>
      <title>The Cost of Simplicity: How Reducing EEG Electrodes Affects Source Localization and BCI Accuracy</title>
      <link>https://arxiv.org/abs/2510.10770</link>
      <description>arXiv:2510.10770v1 Announce Type: new 
Abstract: Electrode density optimization in electroencephalography (EEG)-based Brain-Computer Interfaces (BCIs) requires balancing practical usability against signal fidelity, particularly for source localization. Reducing electrodes enhances portability but its effects on neural source reconstruction quality and source connectivity - treated as proxies to BCI performance - remain understudied. We address this gap through systematic evaluation of 62-, 32-, and 16-channel configurations using a fixed, fully automated processing pipeline applied to the well-characterized P300 potential. This approach's rationale is to minimize variability and bias inherent to EEG analysis by leveraging the P300's stimulus-locked reproducibility and pipeline standardization. Analyzing 63 sessions (31 subjects) from the Eye-BCI dataset with rigorous artifact correction and channel validation, we demonstrate: (1) Progressive degradation in source reconstruction quality with sparser configurations, including obscured deep neural generators and spatiotemporal distortions; (2) A novel sqrt(Re) scaling law linking electrode reduction ratio (Re) to localization accuracy - a previously unquantified relationship to the best of our knowledge; (3) While reduced configurations preserve basic P300 topography and may suffice for communicative BCIs, higher-density channels are essential for reliable deep source reconstruction. Overall, this study establishes a first step towards quantitative benchmarks for electrode selection, with critical implications for clinical BCIs requiring anatomical precision in applications like neurodegenerative disease monitoring, where compromised spatial resolution could mask pathological signatures. Most importantly, the sqrt(Re) scaling law may provide the first principled method to determine the minimal electrode density required based on acceptable error margins or expected effect sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10770v1</guid>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eva Guttmann-Flury, Yanyan Wei, Shan Zhao, Jian Zhao, Mohamad Sawan</dc:creator>
    </item>
    <item>
      <title>A compressed code for memory discrimination</title>
      <link>https://arxiv.org/abs/2510.10791</link>
      <description>arXiv:2510.10791v1 Announce Type: new 
Abstract: The ability to discriminate similar visual stimuli is an important index of memory function. This ability is widely thought to be supported by expanding the dimensionality of relevant neural codes, such that neural representations for similar stimuli are maximally distinct, or ``separated.'' An alternative hypothesis is that discrimination is supported by lossy compression of visual inputs, efficiently coding sensory information by discarding seemingly irrelevant details. A benefit of compression, relative to expansion, is that it allows individuals to retain fewer essential dimensions underlying stimulus variation -- a process linked to higher-order visual processing -- without hindering discrimination. Under this hypothesis, pattern separation is facilitated when more information from similar stimuli can be discarded, rather than preserved. We test the compression versus expansion hypotheses by predicting performance on the canonical mnemonic similarity task. We train neural networks to compress perceptual and semantic factors of stimuli, measuring lossiness using the mathematical framework underlying compression. Consistent with the compression hypothesis, and not the expansion hypothesis, greater lossiness predicts the ease and performance of lure discrimination, especially in deeper convolutional network layers that predict higher-order visual brain activity. We then confirm these predictions across two image sets, four behavioral datasets, and alternative lossiness metrics. Finally, using task fMRI, we identify signatures of lossy compression -- neural dimensionality reduction and information loss -- in higher-order visual regions V4 and IT and hippocampal DG/CA3 and CA1 linked to lure discrimination. These results suggest lossy compression supports mnemonic discrimination by discarding redundant and overlapping information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10791v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dale Zhou, Sharon Mina Noh, Nora C Harhen, Nidhi V Banavar, C. Brock Kirwan, Michael A Yassa, Aaron M Bornstein</dc:creator>
    </item>
    <item>
      <title>People use fast, flat goal-directed simulation to reason about novel problems</title>
      <link>https://arxiv.org/abs/2510.11503</link>
      <description>arXiv:2510.11503v1 Announce Type: new 
Abstract: Games have long been a microcosm for studying planning and reasoning in both natural and artificial intelligence, especially with a focus on expert-level or even super-human play. But real life also pushes human intelligence along a different frontier, requiring people to flexibly navigate decision-making problems that they have never thought about before. Here, we use novice gameplay to study how people make decisions and form judgments in new problem settings. We show that people are systematic and adaptively rational in how they play a game for the first time, or evaluate a game (e.g., how fair or how fun it is likely to be) before they have played it even once. We explain these capacities via a computational cognitive model that we call the "Intuitive Gamer". The model is based on mechanisms of fast and flat (depth-limited) goal-directed probabilistic simulation--analogous to those used in Monte Carlo tree-search models of expert game-play, but scaled down to use very few stochastic samples, simple goal heuristics for evaluating actions, and no deep search. In a series of large-scale behavioral studies with over 1000 participants and 121 two-player strategic board games (almost all novel to our participants), our model quantitatively captures human judgments and decisions varying the amount and kind of experience people have with a game--from no experience at all ("just thinking"), to a single round of play, to indirect experience watching another person and predicting how they should play--and does so significantly better than much more compute-intensive expert-level models. More broadly, our work offers new insights into how people rapidly evaluate, act, and make suggestions when encountering novel problems, and could inform the design of more flexible and human-like AI systems that can determine not just how to solve new tasks, but whether a task is worth thinking about at all.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11503v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katherine M. Collins, Cedegao E. Zhang, Lionel Wong, Mauricio Barba da Costa, Graham Todd, Adrian Weller, Samuel J. Cheyette, Thomas L. Griffiths, Joshua B. Tenenbaum</dc:creator>
    </item>
    <item>
      <title>Proprioceptive Misestimation of Hand Speed</title>
      <link>https://arxiv.org/abs/2510.11664</link>
      <description>arXiv:2510.11664v1 Announce Type: new 
Abstract: The accuracy with which the human proprioceptive system estimates hand speed is not well understood. To investigate this, we designed an experiment using hobby-grade mechatronics parts and integrated it as a laboratory exercise in a large remote laboratory course. In a simple joint position reproduction task, participants (N = 191) grasped a servomotor-driven shaft with one hand as it followed a randomized trajectory composed of sinusoidal submovements. They simultaneously attempted to reproduce the movement by turning the shaft of a potentiometer with the other hand. Focusing on the first movement of the trajectory, we found that participants consistently overestimated the speed of the slowest rotations by ~45% and underestimated the speed of the fastest rotations also by ~30%. Speed estimation errors were near zero for trajectories with peak velocities ~63 deg/s. Participants' movements also overshot slow trajectories and undershot fast trajectories. We show that these trajectory errors can be explained by a model in which the proprioceptive system integrates velocity misestimates to infer position.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11664v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitlin Callaghan, David J Reinkensmeyer</dc:creator>
    </item>
    <item>
      <title>Lost in the Middle: An Emergent Property from Information Retrieval Demands in LLMs</title>
      <link>https://arxiv.org/abs/2510.10276</link>
      <description>arXiv:2510.10276v1 Announce Type: cross 
Abstract: The performance of Large Language Models (LLMs) often degrades when crucial information is in the middle of a long context, a "lost-in-the-middle" phenomenon that mirrors the primacy and recency effects in human memory. We propose that this behavior is not simply a flaw indicative of information loss but an adaptation to different information retrieval demands during pre-training: some tasks require uniform recall across the entire input (a long-term memory demand), while others prioritize the most recent information (a short-term memory demand). Consistent with this view, we show that this U-shaped performance curve emerges when LLMs (GPT-2 and Llama variants) are trained from scratch on two simple human memory paradigms simulating long-term and short-term memory demands. Our analysis reveals that while the recency effect directly aligns with short-term memory demand in the training data, the primacy effect is induced by the uniform long-term memory demand and is additionally influenced by the model's autoregressive properties and the formation of attention sinks. Our main findings from simple human memory paradigms also generalize to a sequence completion task, which more closely resembles the next-token prediction process in LLM pre-training. Together, our findings reveal how information retrieval demands, model architecture, and structural attention dynamics during model training can jointly produce positional bias observed in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10276v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikolaus Salvatore, Hao Wang, Qiong Zhang</dc:creator>
    </item>
    <item>
      <title>Optimal monophasic, asymmetric electric field pulses for selective transcranial magnetic stimulation (TMS) with minimised power and coil heating</title>
      <link>https://arxiv.org/abs/2510.10289</link>
      <description>arXiv:2510.10289v1 Announce Type: cross 
Abstract: Transcranial magnetic stimulation (TMS) with asymmetric electric field pulses, such as monophasic, offers directional selectivity for neural activation but requires excessive energy. Previous pulse shape optimisation has been limited to symmetric pulses or heavily constrained variations of conventional waveforms without achieving general optimality in energy efficiency or neural selectivity. We implemented an optimisation framework that incorporates neuron model activation constraints and flexible control of pulse asymmetry. The optimised electric field waveforms achieved up to 92 % and 88 % reduction in energy loss and thus coil heating respectively compared to conventional monophasic pulses and previously improved monophasic-equivalent pulses. In the human experiments, OUR pulses showed similar motor thresholds to monophasic pulses in both AP and PA directions with significantly lower energy loss, particularly in the AP direction. Moreover, there was a significant MEP latency difference of (1.79 +/- 0.41) ms between AP and PA direction with OUR pulses, which suggests directional selectivity. Our framework successfully identified highly energy-efficient asymmetric pulses for directionally-selective neural engagement. These pulses can enable selective rapid-rate repetitive TMS protocols with reduced power consumption and coil heating, with potential benefits for precision and potency of neuro-modulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10289v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Ma, Andrey Vlasov, Zeynep B. Simsek, Jinshui Zhang, Yiru Li, Boshuo Wang, David L. K. Murphy, Jessica Y. Choi, Maya E. Clinton, Noreen Bukhari-Parlakturk, Angel V. Peterchev, Stephan M. Goetz</dc:creator>
    </item>
    <item>
      <title>The algorithmic regulator</title>
      <link>https://arxiv.org/abs/2510.10300</link>
      <description>arXiv:2510.10300v1 Announce Type: cross 
Abstract: The regulator theorem states that, under certain conditions, any optimal controller must embody a model of the system it regulates, grounding the idea that controllers embed, explicitly or implicitly, internal models of the controlled. This principle underpins neuroscience and predictive brain theories like the Free-Energy Principle or Kolmogorov/Algorithmic Agent theory. However, the theorem is only proven in limited settings. Here, we treat the deterministic, closed, coupled world-regulator system $(W,R)$ as a single self-delimiting program $p$ via a constant-size wrapper that produces the world output string~$x$ fed to the regulator. We analyze regulation from the viewpoint of the algorithmic complexity of the output, $K(x)$. We define $R$ to be a \emph{good algorithmic regulator} if it \emph{reduces} the algorithmic complexity of the readout relative to a null (unregulated) baseline $\varnothing$, i.e., \[ \Delta = K\big(O_{W,\varnothing}\big) - K\big(O_{W,R}\big) &gt; 0. \] We then prove that the larger $\Delta$ is, the more world-regulator pairs with high mutual algorithmic information are favored. More precisely, a complexity gap $\Delta &gt; 0$ yields \[ \Pr\big((W,R)\mid x\big) \le C\,2^{\,M(W{:}R)}\,2^{-\Delta}, \] making low $M(W{:}R)$ exponentially unlikely as $\Delta$ grows. This is an AIT version of the idea that ``the regulator contains a model of the world.'' The framework is distribution-free, applies to individual sequences, and complements the Internal Model Principle. Beyond this necessity claim, the same coding-theorem calculus singles out a \emph{canonical scalar objective} and implicates a \emph{planner}. On the realized episode, a regulator behaves \emph{as if} it minimized the conditional description length of the readout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10300v1</guid>
      <category>cs.CC</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giulio Ruffini</dc:creator>
    </item>
    <item>
      <title>Compositional Symmetry as Compression: Lie Pseudogroup Structure in Algorithmic Agents</title>
      <link>https://arxiv.org/abs/2510.10586</link>
      <description>arXiv:2510.10586v1 Announce Type: cross 
Abstract: In the algorithmic (Kolmogorov) view, agents are programs that track and compress sensory streams using generative programs. We propose a framework where the relevant structural prior is simplicity (Solomonoff) understood as \emph{compositional symmetry}: natural streams are well described by (local) actions of finite-parameter Lie pseudogroups on geometrically and topologically complex low-dimensional configuration manifolds (latent spaces). Modeling the agent as a generic neural dynamical system coupled to such streams, we show that accurate world-tracking imposes (i) \emph{structural constraints} -- equivariance of the agent's constitutive equations and readouts -- and (ii) \emph{dynamical constraints}: under static inputs, symmetry induces conserved quantities (Noether-style labels) in the agent dynamics and confines trajectories to reduced invariant manifolds; under slow drift, these manifolds move but remain low-dimensional. This yields a hierarchy of reduced manifolds aligned with the compositional factorization of the pseudogroup, providing a geometric account of the ``blessing of compositionality'' in deep models. We connect these ideas to the Spencer formalism for Lie pseudogroups and formulate a symmetry-based, self-contained version of predictive coding in which higher layers receive only \emph{coarse-grained residual transformations} (prediction-error coordinates) along symmetry directions unresolved at lower layers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.10586v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giulio Ruffini</dc:creator>
    </item>
    <item>
      <title>Emergence of hybrid computational dynamics through reinforcement learning</title>
      <link>https://arxiv.org/abs/2510.11162</link>
      <description>arXiv:2510.11162v1 Announce Type: cross 
Abstract: Understanding how learning algorithms shape the computational strategies that emerge in neural networks remains a fundamental challenge in machine intelligence. While network architectures receive extensive attention, the role of the learning paradigm itself in determining emergent dynamics remains largely unexplored. Here we demonstrate that reinforcement learning (RL) and supervised learning (SL) drive recurrent neural networks (RNNs) toward fundamentally different computational solutions when trained on identical decision-making tasks. Through systematic dynamical systems analysis, we reveal that RL spontaneously discovers hybrid attractor architectures, combining stable fixed-point attractors for decision maintenance with quasi-periodic attractors for flexible evidence integration. This contrasts sharply with SL, which converges almost exclusively to simpler fixed-point-only solutions. We further show that RL sculpts functionally balanced neural populations through a powerful form of implicit regularization -- a structural signature that enhances robustness and is conspicuously absent in the more heterogeneous solutions found by SL-trained networks. The prevalence of these complex dynamics in RL is controllably modulated by weight initialization and correlates strongly with performance gains, particularly as task complexity increases. Our results establish the learning algorithm as a primary determinant of emergent computation, revealing how reward-based optimization autonomously discovers sophisticated dynamical mechanisms that are less accessible to direct gradient-based optimization. These findings provide both mechanistic insights into neural computation and actionable principles for designing adaptive AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11162v1</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roman A. Kononov, Nikita A. Pospelov, Konstantin V. Anokhin, Vladimir V. Nekorkin, Oleg V. Maslennikov</dc:creator>
    </item>
    <item>
      <title>Auditory steady-state response and gamma oscillations in an excitatory-inhibitory balanced neuronal network</title>
      <link>https://arxiv.org/abs/2504.04329</link>
      <description>arXiv:2504.04329v2 Announce Type: replace 
Abstract: This study introduces a novel auditory neuronal network model that integrates speech signal input, cochlear processing, and a cortical excitatory-inhibitory (E-I) balanced network. Our findings reveal that increasing noise intensity attenuates the auditory steady-state responses in gamma oscillations, a mechanism validated by public EEG data. Moreover, enhancing the brain's E-I balance significantly improves auditory attention during speech recognition. This work not only elucidates the neural basis of selective attention in noisy environments but also offers a promising therapeutic strategy for auditory attention disorders, marking a significant advancement in the field of computational neuroscience and auditory processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04329v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Duoyu Feng, Jiajia Li</dc:creator>
    </item>
    <item>
      <title>Task-Optimized Convolutional Recurrent Networks Align with Tactile Processing in the Rodent Brain</title>
      <link>https://arxiv.org/abs/2505.18361</link>
      <description>arXiv:2505.18361v4 Announce Type: replace 
Abstract: Tactile sensing remains far less understood in neuroscience and less effective in artificial systems compared to more mature modalities such as vision and language. We bridge these gaps by introducing a novel Encoder-Attender-Decoder (EAD) framework to systematically explore the space of task-optimized temporal neural networks trained on realistic tactile input sequences from a customized rodent whisker-array simulator. We identify convolutional recurrent neural networks (ConvRNNs) as superior encoders to purely feedforward and state-space architectures for tactile categorization. Crucially, these ConvRNN-encoder-based EAD models achieve neural representations closely matching rodent somatosensory cortex, saturating the explainable neural variability and revealing a clear linear relationship between supervised categorization performance and neural alignment. Furthermore, contrastive self-supervised ConvRNN-encoder-based EADs, trained with tactile-specific augmentations, match supervised neural fits, serving as an ethologically-relevant, label-free proxy.
  For neuroscience, our findings highlight nonlinear recurrent processing as important for general-purpose tactile representations in somatosensory cortex, providing the first quantitative characterization of the underlying inductive biases in this system. For embodied AI, our results emphasize the importance of recurrent EAD architectures to handle realistic tactile inputs, along with tailored self-supervised learning methods for achieving robust tactile perception with the same type of sensors animals use to sense in unstructured environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18361v4</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trinity Chung, Yuchen Shen, Nathan C. L. Kong, Aran Nayebi</dc:creator>
    </item>
    <item>
      <title>Amplitude equations of associative memory patterns in spatially distributed systems</title>
      <link>https://arxiv.org/abs/2506.13576</link>
      <description>arXiv:2506.13576v2 Announce Type: replace 
Abstract: Evolution equations of the amplitudes of heterogeneous states (associative memories) stored in the connectivity of distributed systems with non-local interactions are derived. The resulting system of coupled amplitude equations describes the spatio-temporal dynamics of memory recall. It is shown that these types of distributed systems perform pattern completion and selection, and that short-range connections afford spatio-temporal memory pattern dynamics taking the form of patterning fronts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13576v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.PS</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akke Mats Houben</dc:creator>
    </item>
    <item>
      <title>Alpha-Z divergence unveils further distinct phenotypic traits of human brain connectivity fingerprint</title>
      <link>https://arxiv.org/abs/2507.23116</link>
      <description>arXiv:2507.23116v2 Announce Type: replace 
Abstract: The accurate identification of individuals from functional connectomes (FCs) is critical for advancing individualized assessments in neuropsychiatric research. Traditional methods, such as Pearson's correlation, have limitations in capturing the complex, non-Euclidean geometry of FC data, leading to suboptimal performance in identification performance. Recent developments have introduced geodesic distance as a more robust metric; however, its performance is highly sensitive to regularization choices, which vary by spatial scale and task condition. To address these challenges, we propose a novel divergence-based distance metric, the Alpha-Z Bures-Wasserstein divergence, which provides a more flexible and geometry-aware framework for FC comparison. Unlike prior methods, our approach does not require meticulous parameter tuning and maintains strong identification performance across multiple task conditions and spatial resolutions. We evaluate our approach against both traditional (e.g., Euclidean, Pearson) and state-of-the-art manifold-based distances (e.g., affine-invariant, log-Euclidean, Bures-Wasserstein), and systematically investigate how varying regularization strengths affect geodesic distance performance on the Human Connectome Project dataset. Our results show that the proposed method significantly improves identification rates over traditional and geodesic distances, particularly when optimized regularization is applied, and especially in high-dimensional settings where matrix rank deficiencies degrade existing metrics. We further validate its generalizability across resting-state and task-based fMRI, using multiple parcellation schemes. These findings suggest that the new divergence provides a more reliable and generalizable framework for functional connectivity analysis, offering enhanced sensitivity in linking FC patterns to cognitive and behavioral outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23116v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Kaosar Uddin, Nghi Nguyen, Huajun Huang, Duy Duong-Tran, Jingyi Zheng</dc:creator>
    </item>
    <item>
      <title>NSPDI-SNN: An efficient lightweight SNN based on nonlinear synaptic pruning and dendritic integration</title>
      <link>https://arxiv.org/abs/2508.21566</link>
      <description>arXiv:2508.21566v2 Announce Type: replace 
Abstract: Spiking neural networks (SNNs) are artificial neural networks based on simulated biological neurons and have attracted much attention in recent artificial intelligence technology studies. The dendrites in biological neurons have efficient information processing ability and computational power; however, the neurons of SNNs rarely match the complex structure of the dendrites. Inspired by the nonlinear structure and highly sparse properties of neuronal dendrites, in this study, we propose an efficient, lightweight SNN method with nonlinear pruning and dendritic integration (NSPDI-SNN). In this method, we introduce nonlinear dendritic integration (NDI) to improve the representation of the spatiotemporal information of neurons. We implement heterogeneous state transition ratios of dendritic spines and construct a new and flexible nonlinear synaptic pruning (NSP) method to achieve the high sparsity of SNN. We conducted systematic experiments on three benchmark datasets (DVS128 Gesture, CIFAR10-DVS, and CIFAR10) and extended the evaluation to two complex tasks (speech recognition and reinforcement learning-based maze navigation task). Across all tasks, NSPDI-SNN consistently achieved high sparsity with minimal performance degradation. In particular, our method achieved the best experimental results on all three event stream datasets. Further analysis showed that NSPDI significantly improved the efficiency of synaptic information transfer as sparsity increased. In conclusion, our results indicate that the complex structure and nonlinear computation of neuronal dendrites provide a promising approach for developing efficient SNN methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21566v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wuque Cai, Hongze Sun, Jiayi He, Qianqian Liao, Yunliang Zang, Duo Chen, Dezhong Yao, Daqing Guo</dc:creator>
    </item>
    <item>
      <title>On a Geometry of Interbrain Networks</title>
      <link>https://arxiv.org/abs/2509.10650</link>
      <description>arXiv:2509.10650v2 Announce Type: replace 
Abstract: Effective analysis in neuroscience benefits significantly from robust conceptual frameworks. Traditional metrics of interbrain synchrony in social neuroscience typically depend on fixed, correlation-based approaches, restricting their explanatory capacity to descriptive observations. Inspired by the successful integration of geometric insights in network science, we propose leveraging discrete geometry to examine the dynamic reconfigurations in neural interactions during social exchanges. Unlike conventional synchrony approaches, our method interprets inter-brain connectivity changes through the evolving geometric structures of neural networks. This geometric framework is realized through a pipeline that identifies critical transitions in network connectivity using entropy metrics derived from curvature distributions. By doing so, we significantly enhance the capacity of hyperscanning methodologies to uncover underlying neural mechanisms in interactive social behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10650v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicol\'as Hinrichs, Noah Guzm\'an, Melanie Weber</dc:creator>
    </item>
    <item>
      <title>Human-inspired Episodic Memory for Infinite Context LLMs</title>
      <link>https://arxiv.org/abs/2407.09450</link>
      <description>arXiv:2407.09450v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have shown remarkable capabilities, but still struggle with processing extensive contexts, limiting their ability to maintain coherence and accuracy over long sequences. In contrast, the human brain excels at organising and retrieving episodic experiences across vast temporal scales, spanning a lifetime. In this work, we introduce EM-LLM, a novel approach that integrates key aspects of human episodic memory and event cognition into LLMs with no fine-tuning, enabling them to handle practically infinite context lengths while maintaining computational efficiency. EM-LLM organises sequences of tokens into coherent episodic events using a combination of Bayesian surprise and graph-theoretic boundary refinement in an online fashion. When needed, these events are retrieved through a two-stage memory process, combining similarity-based and temporally contiguous retrieval for efficient, human-inspired access to relevant information. Experiments on the LongBench and $\infty$-Bench benchmarks demonstrate EM-LLM's superior performance, consistently outperforming the state-of-the-art retrieval model InfLLM across various baseline LLMs. In addition, EM-LLM outperforms its popular counterpart, RAG, in a wide range of tasks, while requiring similar resources. Notably, EM-LLM's performance even surpasses full-context models in most tasks, while successfully performing retrieval across 10 million tokens -- a scale computationally infeasible for such models. Finally, our analysis reveals strong correlations between EM-LLM's event segmentation and human-perceived events, suggesting parallels between this artificial system and its biological counterpart, thereby offering a novel computational framework for exploring human memory mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09450v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proc. International Conference on Learning Representations (ICLR), 2025</arxiv:journal_reference>
      <dc:creator>Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, Jun Wang</dc:creator>
    </item>
    <item>
      <title>The role of oscillations in grid cells' toroidal topology</title>
      <link>https://arxiv.org/abs/2501.19262</link>
      <description>arXiv:2501.19262v2 Announce Type: replace-cross 
Abstract: Persistent homology applied to the activity of grid cells in the Medial Entorhinal Cortex suggests that this activity lies on a toroidal manifold. By analyzing real data and a simple model, we show that neural oscillations play a key role in the appearance of this toroidal topology. To quantitatively monitor how changes in spike trains influence the topology of the data, we first define a robust measure for the degree of toroidality of a dataset. Using this measure, we find that small perturbations ($\sim$ 100 ms) of spike times have little influence on both the toroidality and the hexagonality of the ratemaps. Jittering spikes by $\sim$ 100-500 ms, however, destroys the toroidal topology, while still having little impact on grid scores. These critical jittering time scales fall in the range of the periods of oscillations between the theta and eta bands. We thus hypothesized that these oscillatory modulations of neuronal spiking play a key role in the appearance and robustness of toroidal topology and the hexagonal spatial selectivity is not sufficient. We confirmed this hypothesis using a simple model for the activity of grid cells, consisting of an ensemble of independent rate-modulated Poisson processes. When these rates were modulated by oscillations, the network behaved similarly to the real data in exhibiting toroidal topology, even when the position of the fields were perturbed. In the absence of oscillations, this similarity was substantially lower. Furthermore, we find that the experimentally recorded spike trains indeed exhibit temporal modulations at the eta and theta bands, and that the ratio of the power in the eta band to that of the theta band, $A_{\eta}/A_{\theta}$, correlates with the critical jittering time at which the toroidal topology disappears.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19262v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 14 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pcbi.1012776</arxiv:DOI>
      <arxiv:journal_reference>di Sarra G, Jha S, Roudi Y (2025) The role of oscillations in grid cells' toroidal topology. PLOS Computational Biology 21(1):e1012776</arxiv:journal_reference>
      <dc:creator>Giovanni di Sarra, Siddharth Jha, Yasser Roudi</dc:creator>
    </item>
  </channel>
</rss>
