<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NE</link>
    <description>cs.NE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 20 Feb 2026 05:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Learning under noisy supervision is governed by a feedback-truth gap</title>
      <link>https://arxiv.org/abs/2602.16829</link>
      <description>arXiv:2602.16829v1 Announce Type: cross 
Abstract: When feedback is absorbed faster than task structure can be evaluated, the learner will favor feedback over truth. A two-timescale model shows this feedback-truth gap is inevitable whenever the two rates differ and vanishes only when they match. We test this prediction across neural networks trained with noisy labels (30 datasets, 2,700 runs), human probabilistic reversal learning (N = 292), and human reward/punishment learning with concurrent EEG (N = 25). In each system, truth is defined operationally: held-out labels, the objectively correct option, or the participant's pre-feedback expectation - the only non-circular reference decodable from post-feedback EEG. The gap appeared universally but was regulated differently: dense networks accumulated it as memorization; sparse-residual scaffolding suppressed it; humans generated transient over-commitment that was actively recovered. Neural over-commitment (~0.04-0.10) was amplified tenfold into behavioral commitment (d = 3.3-3.9). The gap is a fundamental constraint on learning under noisy supervision; its consequences depend on the regulation each system employs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16829v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elan Schonfeld, Elias Wisnia</dc:creator>
    </item>
    <item>
      <title>A Parameter-free Adaptive Resonance Theory-based Topological Clustering Algorithm Capable of Continual Learning</title>
      <link>https://arxiv.org/abs/2305.01507</link>
      <description>arXiv:2305.01507v3 Announce Type: replace 
Abstract: In general, a similarity threshold (i.e., a vigilance parameter) for a node learning process in Adaptive Resonance Theory (ART)-based algorithms has a significant impact on clustering performance. In addition, an edge deletion threshold in a topological clustering algorithm plays an important role in adaptively generating well-separated clusters during a self-organizing process. In this paper, we propose an ART-based topological clustering algorithm that integrates parameter estimation methods for both the similarity threshold and the edge deletion threshold. The similarity threshold is estimated using a determinantal point process-based criterion, while the edge deletion threshold is defined based on the age of edges. Experimental results with synthetic and real-world datasets show that the proposed algorithm has superior clustering performance to state-of-the-art clustering algorithms without requiring parameter specifications specific to the datasets. Source code is available at https://github.com/Masuyama-lab/CAE</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.01507v3</guid>
      <category>cs.NE</category>
      <category>cs.LG</category>
      <pubDate>Fri, 20 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s00521-026-11907-5</arxiv:DOI>
      <dc:creator>Naoki Masuyama, Takanori Takebayashi, Yusuke Nojima, Chu Kiong Loo, Hisao Ishibuchi, Stefan Wermter</dc:creator>
    </item>
  </channel>
</rss>
