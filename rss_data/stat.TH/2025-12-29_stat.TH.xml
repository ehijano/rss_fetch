<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 29 Dec 2025 05:01:19 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Thermodynamic Characterizations of Singular Bayesian Models: Specific Heat, Susceptibility, and Entropy Flow in Posterior Geometry</title>
      <link>https://arxiv.org/abs/2512.21411</link>
      <description>arXiv:2512.21411v1 Announce Type: new 
Abstract: Singular learning theory (SLT) \citep{watanabe2009algebraic,watanabe2018mathematical} provides a rigorous asymptotic framework for Bayesian models with non-identifiable parameterizations, yet the statistical meaning of its second-order invariant, the \emph{singular fluctuation}, has remained unclear. In this work, we show that singular fluctuation admits a precise and natural interpretation as a \emph{specific heat}: the second derivative of the Bayesian free energy with respect to temperature. Equivalently, it measures the posterior variance of the log-likelihood observable under the tempered Gibbs posterior. We further introduce a collection of related thermodynamic quantities, including entropy flow, prior susceptibility, and cross-susceptibility, that together provide a detailed geometric diagnosis of singular posterior structure. Through extensive numerical experiments spanning discrete symmetries, boundary singularities, continuous gauge freedoms, and piecewise (ReLU) models, we demonstrate that these thermodynamic signatures cleanly distinguish singularity types, exhibit stable finite-sample behavior, and reveal phase-transition--like phenomena as temperature varies. We also show empirically that the widely used WAIC estimator \citep{watanabe2010asymptotic, watanabe2013widely} is exactly twice the thermodynamic specific heat at unit temperature, clarifying its robustness in singular models.Our results establish a concrete bridge between singular learning theory and statistical mechanics, providing both theoretical insight and practical diagnostics for modern Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21411v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean Plummer</dc:creator>
    </item>
    <item>
      <title>Estimating axial symmetry using random projections</title>
      <link>https://arxiv.org/abs/2512.21417</link>
      <description>arXiv:2512.21417v1 Announce Type: new 
Abstract: This paper studies the problem of identifying directions of axial symmetry in multivariate distributions. Theoretical results are derived on how the measure or cardinality of the set of symmetry directions relates to spherical symmetry. The problem is framed using random projections, leading to a proof that in \(\RR^2\), agreement on two random projections is enough to identify the true axes of symmetry. A corresponding result for higher dimensions is conjectured. An estimator for the symmetry directions is proposed and proved to be consistent in the plane.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21417v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alejandro Cholaquidis, Ricardo Fraiman, Manuel Hern\'andez-Banadik, Stanislav Nagy</dc:creator>
    </item>
    <item>
      <title>Expected star discrepancy based on stratified sampling</title>
      <link>https://arxiv.org/abs/2512.21504</link>
      <description>arXiv:2512.21504v1 Announce Type: new 
Abstract: We present two main contributions to the expected star discrepancy theory. First, we derive a sharper expected upper bound for jittered sampling, improving the leading constants and logarithmic terms compared to the state-of-the-art [Doerr, 2022]. Second, we prove the strong partition principle for star discrepancy, showing that any equal-measure stratified sampling yields a strictly smaller expected discrepancy than simple random sampling, thereby resolving an open question in [Kiderlen and Pausinger, 2022]. Numerical simulations confirm our theoretical advances and illustrate the superiority of stratified sampling in low to moderate dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21504v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoda Xu, Jun Xian</dc:creator>
    </item>
    <item>
      <title>Rational Inattention: A Bayesian Predictive Approach</title>
      <link>https://arxiv.org/abs/2512.21639</link>
      <description>arXiv:2512.21639v1 Announce Type: new 
Abstract: We recast rational inattention as a Bayesian predictive decision problem in which the agent reports a predictive distribution and is evaluated by a proper local scoring rule. This yields a direct link to rate-distortion theory and shows that Shannon entropy emerges endogenously as the honest local utility for predictive refinement. Bernardo's characterization of proper local scoring rules together with Shannon's amalgamation invariance imply that the logarithmic score, and hence mutual information, is the unique information measure consistent with coherent prediction under refinement of the state space. Information costs, therefore, need not be assumed: they arise as expected predictive utility. Within this framework we establish a supported complete-class result: the optimal policies are Gibbs-Boltzmann channels, with the classical rational-inattention family recovered as a special case. Canonical models appear as geometric specializations of the same structure, including multinomial logit (and IIA) under entropic regularization, James-Stein shrinkage as optimal capacity allocation in Gaussian learning, and linear-quadratic-Gaussian control as the capacity-optimal Gaussian channel. Overall, the Bayesian predictive formulation reframes bounded rationality as an optimal design principle: finite information capacity is an endogenous solution to a well-posed predictive problem, and behaviors often attributed to cognitive frictions, soft choice, regularization, sparsity, and screening arise as rational responses to the geometry of predictive refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21639v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas G. Polson, Daniel Zantedeschi</dc:creator>
    </item>
    <item>
      <title>Inference in the $p_0$ model for directed networks under local differential privacy</title>
      <link>https://arxiv.org/abs/2512.21700</link>
      <description>arXiv:2512.21700v1 Announce Type: new 
Abstract: We explore the edge-flipping mechanism, a type of input perturbation, to release the directed graph under edge-local differential privacy. By using the noisy bi-degree sequence from the output graph, we construct the moment equations to estimate the unknown parameters in the $p_0$ model, which is an exponential family distribution with the bi-degree sequence as the natural sufficient statistic. We show that the resulting private estimator is asymptotically consistent and normally distributed under some conditions. In addition, we compare the performance of input and output perturbation mechanisms for releasing bi-degree sequences in terms of parameter estimation accuracy and privacy protection. Numerical studies demonstrate our theoretical findings and compare the performance of the private estimates obtained by different types of perturbation methods. We apply the proposed method to analyze the UC Irvine message network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21700v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueying Sun, Ting Yan, Binyan Jiang</dc:creator>
    </item>
    <item>
      <title>Optimal Robust Bounded Bias and Bounded Variance Designs</title>
      <link>https://arxiv.org/abs/2512.21806</link>
      <description>arXiv:2512.21806v1 Announce Type: new 
Abstract: Designs which are minimax in the presence of model misspecifications have been constructed so as to minimize the maximum, over classes of alternate response models, of the integrated mean squared error of the predicted values. This mean squared error decomposes into a term arising solely from variation, and a bias term arising from the model errors. Here we consider two associated problems: (i) design so as to minimize the variance, subject to a bound on the bias, and (ii) design so as to minimize the bias, subject to a bound on the variance. We show that solutions to both problems are given by the minimax designs, with appropriately chosen values of their tuning constant. Conversely, any minimax design solves both problems for appropriate choices of the bounds on the bias or variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21806v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Douglas P. Wiens</dc:creator>
    </item>
    <item>
      <title>A Sieve-based Estimator for Entropic Optimal Transport</title>
      <link>https://arxiv.org/abs/2512.21981</link>
      <description>arXiv:2512.21981v1 Announce Type: new 
Abstract: The entropically regularized optimal transport problem between probability measures on compact Euclidean subsets can be represented as an information projection with moment inequality constraints. This allows its Fenchel dual to be approximated by a sequence of convex, finite-dimensional problems using sieve methods, enabling tractable estimation of the primal value and dual optimizers from samples. Assuming only continuity of the cost function, I establish almost sure consistency of these estimators. I derive a finite-sample convergence rate for the primal value estimator, showing logarithmic dependence on sieve complexity, and quantify uncertainty for the dual optimal value estimator via matching stochastic bounds involving suprema of centered Gaussian processes. These results provide the first statistical guarantees for sieve-based estimators of entropic optimal transport, extending beyond the empirical Sinkhorn approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21981v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rami V. Tabri</dc:creator>
    </item>
    <item>
      <title>Exact inference via quasi-conjugacy in two-parameter Poisson-Dirichlet hidden Markov models</title>
      <link>https://arxiv.org/abs/2512.22098</link>
      <description>arXiv:2512.22098v1 Announce Type: cross 
Abstract: We introduce a nonparametric model for time-evolving, unobserved probability distributions from discrete-time data consisting of unlabelled partitions. The latent process is a two-parameter Poisson-Dirichlet diffusion, and observations arise via exchangeable sampling. Applications include social and genetic data where only aggregate clustering summaries are observed. To address the intractable likelihood, we develop a tractable inferential framework that avoids label enumeration and direct simulation of the latent state. We exploit a duality between the diffusion and a pure-death process on partitions, together with coagulation operators that encode the effect of new data. These yield closed-form, recursive updates for forward and backward inference. We compute exact posterior distributions of the latent state at arbitrary times and predictive distributions of future or interpolated partitions. This enables online and offline inference and forecasting with full uncertainty quantification, bypassing MCMC and sequential Monte Carlo. Compared to particle filtering, our method achieves higher accuracy, lower variance, and substantial computational gains. We illustrate the methodology with synthetic experiments and a social network application, recovering interpretable patterns in time-varying heterozygosity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.22098v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Dalla Pria, Matteo Ruggiero, Dario Span\`o</dc:creator>
    </item>
    <item>
      <title>On Learning-Curve Monotonicity for Maximum Likelihood Estimators</title>
      <link>https://arxiv.org/abs/2512.10220</link>
      <description>arXiv:2512.10220v2 Announce Type: replace 
Abstract: The property of learning-curve monotonicity, highlighted in a recent series of work by Loog, Mey and Viering, describes algorithms which only improve in average performance given more data, for any underlying data distribution within a given family. We establish the first nontrivial monotonicity guarantees for the maximum likelihood estimator in a variety of well-specified parametric settings. For sequential prediction with log loss, we show monotonicity (in fact complete monotonicity) of the forward KL divergence for Gaussian vectors with unknown covariance and either known or unknown mean, as well as for Gamma variables with unknown scale parameter. The Gaussian setting was explicitly highlighted as open in the aforementioned works, even in dimension 1. Finally we observe that for reverse KL divergence, a folklore trick yields monotonicity for very general exponential families.
  All results in this paper were derived by variants of GPT-5.2 Pro. Humans did not provide any proof strategies or intermediate arguments, but only prompted the model to continue developing additional results, and verified and transcribed its proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.10220v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark Sellke, Steven Yin</dc:creator>
    </item>
    <item>
      <title>Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models</title>
      <link>https://arxiv.org/abs/2209.15224</link>
      <description>arXiv:2209.15224v5 Announce Type: replace-cross 
Abstract: Unsupervised learning has been widely used in many real-world applications. One of the simplest and most important unsupervised learning models is the Gaussian mixture model (GMM). In this work, we study the multi-task learning problem on GMMs, which aims to leverage potentially similar GMM parameter structures among tasks to obtain improved learning performance compared to single-task learning. We propose a multi-task GMM learning procedure based on the EM algorithm that effectively utilizes unknown similarities between related tasks and is robust against a fraction of outlier tasks from arbitrary distributions. The proposed procedure is shown to achieve the minimax optimal rate of convergence for both parameter estimation error and the excess mis-clustering error, in a wide range of regimes. Moreover, we generalize our approach to tackle the problem of transfer learning for GMMs, where similar theoretical results are derived. Additionally, iterative unsupervised multi-task and transfer learning methods may suffer from an initialization alignment problem, and two alignment algorithms are proposed to resolve the issue. Finally, we demonstrate the effectiveness of our methods through simulations and real data examples. To the best of our knowledge, this is the first work studying multi-task and transfer learning on GMMs with theoretical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.15224v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Tian, Haolei Weng, Lucy Xia, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Asymptotic Analysis and Practical Evaluation of Jump Rate Estimators in Piecewise-Deterministic Markov Processes</title>
      <link>https://arxiv.org/abs/2502.14621</link>
      <description>arXiv:2502.14621v2 Announce Type: replace-cross 
Abstract: Piecewise-deterministic Markov processes (PDMPs) offer a powerful stochastic modeling framework that combines deterministic trajectories with random perturbations at random times. Estimating their local characteristics (particularly the jump rate) is an important yet challenging task. In recent years, non-parametric methods for jump rate inference have been developed, but these approaches often rely on distinct theoretical frameworks, complicating direct comparisons. In this paper, we propose a unified framework to standardize and consolidate state-of-the-art approaches. We establish new results on consistency and asymptotic normality within this framework, enabling rigorous theoretical comparisons of convergence rates and asymptotic variances. Notably, we demonstrate that no single method uniformly outperforms the others, even within the same model. These theoretical insights are validated through numerical simulations using a representative PDMP application: the TCP model. Furthermore, we extend the comparison to real-world data, focusing on cell growth and division dynamics in Escherichia coli. This work enhances the theoretical understanding of PDMP inference while offering practical insights into the relative strengths and limitations of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14621v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Romain Aza\"is, Solune Denis</dc:creator>
    </item>
    <item>
      <title>Optimal estimation for regression discontinuity design with binary outcomes</title>
      <link>https://arxiv.org/abs/2509.18857</link>
      <description>arXiv:2509.18857v2 Announce Type: replace-cross 
Abstract: We develop a finite-sample optimal estimator for regression discontinuity design when the outcomes are bounded, including binary outcomes as the leading case. Our estimator achieves minimax mean squared error among linear shrinkage estimators with nonnegative weights when the regression function lies in a Lipschitz class. Although the original minimax problem involves an iterative noncovex optimization problem, we show that our estimator is obtained by solving a convex optimization problem. A key advantage of the proposed estimator is that the Lipschitz constant is its only tuning parameter. We also propose a uniformly valid inference procedure without a large-sample approximation. In a simulation exercise for small samples, our estimator exhibits smaller mean squared errors and shorter confidence intervals than those of conventional large-sample techniques. In an empirical multi-cutoff design in which the sample size for each cutoff is small, our method yields informative confidence intervals, in contrast to the leading large-sample approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18857v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Masayuki Sawada, Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Inference in partially identified moment models via regularized optimal transport</title>
      <link>https://arxiv.org/abs/2512.18084</link>
      <description>arXiv:2512.18084v2 Announce Type: replace-cross 
Abstract: Partial identification often arises when the joint distribution of the data is known only up to its marginals. We consider the corresponding partially identified GMM model and develop a methodology for identification, estimation, and inference in this model. We characterize the sharp identified set for the parameter of interest via a support-function/optimal-transport (OT) representation. For estimation, we employ entropic regularization, which provides a smooth approximation to classical OT and can be computed efficiently by the Sinkhorn algorithm. We also propose a statistic for testing hypotheses and constructing confidence regions for the identified set. To derive the asymptotic distribution of this statistic, we establish a novel central limit theorem for the entropic OT value under general smooth costs. We then obtain valid critical values using the bootstrap for directionally differentiable functionals of Fang and Santos (2019). The resulting testing procedure controls size locally uniformly, including at parameter values on the boundary of the identified set. We illustrate its performance in a Monte Carlo simulation. Our methodology is applicable to a wide range of empirical settings, such as panels with attrition and refreshment samples, nonlinear treatment effects, nonparametric instrumental variables without large-support conditions, and Euler equations with repeated cross-sections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18084v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 29 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Grigory Franguridi, Laura Liu</dc:creator>
    </item>
  </channel>
</rss>
