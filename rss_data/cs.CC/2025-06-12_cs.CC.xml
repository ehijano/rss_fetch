<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Jun 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Complexity of Contextuality</title>
      <link>https://arxiv.org/abs/2506.09133</link>
      <description>arXiv:2506.09133v1 Announce Type: cross 
Abstract: Generalized contextuality is a hallmark of nonclassical theories like quantum mechanics. Yet, three fundamental computational problems concerning its decidability and complexity remain open. First, determining the complexity of deciding if a theory admits a noncontextual ontological model; Second, determining the complexity of deciding if such a model is possible for a specific dimension $k$; Third, efficiently computing the smallest such model when it exists, given that finding the smallest ontological model is NP-hard. We address the second problem by presenting an algorithm derived from a geometric formulation and its reduction to the intermediate simplex problem in computational geometry. We find that the complexity of deciding the existence of a noncontextual ontological model of dimension $k$ is at least exponential in the dimension of the theory and at most exponential in $k$. This, in turn, implies that computing the smallest noncontextual ontological model is inefficient in general. Finally, we demonstrate the fundamental difference between finding the smallest noncontextual ontological model and the smallest ontological model using an explicit example wherein the respective minimum ontic sizes are five and four.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09133v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theodoros Yianni, Farid Shahandeh</dc:creator>
    </item>
    <item>
      <title>Average-case deterministic query complexity of boolean functions with fixed weight</title>
      <link>https://arxiv.org/abs/2403.03530</link>
      <description>arXiv:2403.03530v3 Announce Type: replace 
Abstract: We study the $\textit{average-case deterministic query complexity}$ of boolean functions under a $\textit{uniform input distribution}$, denoted by $\mathrm{D}_\mathrm{ave}(f)$, the minimum average depth of zero-error decision trees that compute a boolean function $f$. This measure has found several applications across diverse fields, yet its understanding is limited. We study boolean functions with fixed weight, where weight is defined as the number of inputs on which the output is $1$. We prove $\mathrm{D}_\mathrm{ave}(f) \le \max \left\{ \log \frac{\mathrm{wt}(f)}{\log n} + O(\log \log \frac{\mathrm{wt}(f)}{\log n}), O(1) \right\}$ for every $n$-variable boolean function $f$, where $\mathrm{wt}(f)$ denotes the weight. For any $4\log n \le m(n) \le 2^{n-1}$, we prove the upper bound is tight up to an additive logarithmic term for almost all $n$-variable boolean functions with fixed weight $\mathrm{wt}(f) = m(n)$. H\r{a}stad's switching lemma or Rossman's switching lemma [Comput. Complexity Conf. 137, 2019] implies $\mathrm{D}_\mathrm{ave}(f) \leq n\left(1 - \frac{1}{O(w)}\right)$ or $\mathrm{D}_\mathrm{ave}(f) \le n\left(1 - \frac{1}{O(\log s)}\right)$ for CNF/DNF formulas of width $w$ or size $s$, respectively. We show there exists a DNF formula of width $w$ and size $\lceil 2^w / w \rceil$ such that $\mathrm{D}_\mathrm{ave}(f) = n \left(1 - \frac{\log n}{\Theta(w)}\right)$ for any $w \ge 2\log n$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03530v3</guid>
      <category>cs.CC</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Li, Haowei Wu, Yi Yang</dc:creator>
    </item>
    <item>
      <title>Lower Bounds for Learning Quantum States with Single-Copy Measurements</title>
      <link>https://arxiv.org/abs/2207.14438</link>
      <description>arXiv:2207.14438v3 Announce Type: replace-cross 
Abstract: We study the problems of quantum tomography and shadow tomography using measurements performed on individual, identical copies of an unknown $d$-dimensional state. We first revisit a known lower bound due to Haah et al. (2017) on quantum tomography with accuracy $\epsilon$ in trace distance, when the measurements choices are independent of previously observed outcomes (i.e., they are nonadaptive). We give a succinct proof of this result. This leads to stronger lower bounds when the learner uses measurements with a constant number of outcomes. In particular, this rigorously establishes the optimality of the folklore ``Pauli tomography" algorithm in terms of its sample complexity. We also derive novel bounds of $\Omega(r^2 d/\epsilon^2)$ and $\Omega(r^2 d^2/\epsilon^2)$ for learning rank $r$ states using arbitrary and constant-outcome measurements, respectively, in the nonadaptive case.
  In addition to the sample complexity, a resource of practical significance for learning quantum states is the number of different measurements used by an algorithm. We extend our lower bounds to the case where the learner performs possibly adaptive measurements from a fixed set of $\exp(O(d))$ measurements. This implies in particular that adaptivity does not give us any advantage using single-copy measurements that are efficiently implementable. We also obtain a similar bound in the case where the goal is to predict the expectation values of a given sequence of observables, a task known as shadow tomography. Finally, in the case of adaptive, single-copy measurements implementable with polynomial-size circuits, we prove that a straightforward strategy based on computing sample means of the given observables is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.14438v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3717450</arxiv:DOI>
      <arxiv:journal_reference>ACM Trans. Comput. Theory 17, 1, Article 7 (March 2025), 42 pages</arxiv:journal_reference>
      <dc:creator>Angus Lowe, Ashwin Nayak</dc:creator>
    </item>
    <item>
      <title>Computational Irreducibility as the Foundation of Agency: A Formal Model Connecting Undecidability to Autonomous Behavior in Complex Systems</title>
      <link>https://arxiv.org/abs/2505.04646</link>
      <description>arXiv:2505.04646v2 Announce Type: replace-cross 
Abstract: This article presents a formal model demonstrating that genuine autonomy, the ability of a system to self-regulate and pursue objectives, fundamentally implies computational unpredictability from an external perspective. we establish precise mathematical connections, proving that for any truly autonomous system, questions about its future behavior are fundamentally undecidable. this formal undecidability, rather than mere complexity, grounds a principled distinction between autonomous and non-autonomous systems. our framework integrates insights from computational theory and biology, particularly regarding emergent agency and computational irreducibility, to explain how novel information and purpose can arise within a physical universe. the findings have significant implications for artificial intelligence, biological modeling, and philosophical concepts like free will.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.04646v2</guid>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 12 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Poria Azadi</dc:creator>
    </item>
  </channel>
</rss>
