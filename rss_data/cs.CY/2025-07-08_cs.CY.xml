<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 04:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Fuzzy Supervisor Agent Design for Clinical Reasoning Assistance in a Multi-Agent Educational Clinical Scenario Simulation</title>
      <link>https://arxiv.org/abs/2507.05275</link>
      <description>arXiv:2507.05275v1 Announce Type: new 
Abstract: Assisting medical students with clinical reasoning (CR) during clinical scenario training remains a persistent challenge in medical education. This paper presents the design and architecture of the Fuzzy Supervisor Agent (FSA), a novel component for the Multi-Agent Educational Clinical Scenario Simulation (MAECSS) platform. The FSA leverages a Fuzzy Inference System (FIS) to continuously interpret student interactions with specialized clinical agents (e.g., patient, physical exam, diagnostic, intervention) using pre-defined fuzzy rule bases for professionalism, medical relevance, ethical behavior, and contextual distraction. By analyzing student decision-making processes in real-time, the FSA is designed to deliver adaptive, context-aware feedback and provides assistance precisely when students encounter difficulties. This work focuses on the technical framework and rationale of the FSA, highlighting its potential to provide scalable, flexible, and human-like supervision in simulation-based medical education. Future work will include empirical evaluation and integration into broader educational settings. More detailed design and implementation is~\href{https://github.com/2sigmaEdTech/MAS/}{open sourced here}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05275v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LO</category>
      <category>cs.MA</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weibing Zheng, Laurah Turner, Jess Kropczynski, Murat Ozer, Seth Overla, Shane Halse</dc:creator>
    </item>
    <item>
      <title>Hungary and AI: efforts and opportunities in comparison with Singapore</title>
      <link>https://arxiv.org/abs/2507.05280</link>
      <description>arXiv:2507.05280v1 Announce Type: new 
Abstract: The study assesses Hungary's National AI Strategy and its implementation through the analysis of strategic documents, publicly available financial records, and expert interviews with the Hungarian AI Coalition President and Chief Strategic Advisor to the Government Commissioner for AI. 22 goals from Hungary's strategy were evaluated through conceptual, governance, temporal, and financial dimensions before being benchmarked against Singapore's National AI Strategies (NAIS 1.0 and NAIS 2.0). Key findings include an estimated total of EUR 4.65 billion in AI-related public investment in Hungary. Openly available financial data was found for only half of the evaluated goals, and just three projects made up 98\% of all documented funding. The research also reveals Hungary's implementation challenges, including fragmented execution following ministerial reorganizations and the absence of designated biennial reviews since 2020. Furthermore, the paper provides targeted recommendations for Hungary's forthcoming AI strategy, drawing on Singapore's framework as a reference point. These include adapting to the era of large language models, restructuring the existing triple helix network to foster more effective dialogue and advocacy, and positioning the country as an East-West bridge for automotive AI experimentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05280v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Andr\'as Ferenczy</dc:creator>
    </item>
    <item>
      <title>A LLM-Driven Multi-Agent Systems for Professional Development of Mathematics Teachers</title>
      <link>https://arxiv.org/abs/2507.05292</link>
      <description>arXiv:2507.05292v1 Announce Type: new 
Abstract: Professional development (PD) serves as the cornerstone for teacher tutors to grasp content knowledge. However, providing equitable and timely PD opportunities for teachers poses significant challenges. To address this issue, we introduce I-VIP (Intelligent Virtual Interactive Program), an intelligent tutoring platform for teacher professional development, driven by large language models (LLMs) and supported by multi-agent frameworks. This platform offers a user-friendly conversational interface and allows users to employ a variety of interactive tools to facilitate question answering, knowledge comprehension, and reflective summarization while engaging in dialogue. To underpin the functionality of this platform, including knowledge expectation analysis, response scoring and classification, and feedback generation, the multi-agent frameworks are leveraged to enhance the accuracy of judgments and mitigate the issue of missing key points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05292v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiqi Yang, Hang Li, Yucheng Chu, Ahreum Han, Yasemin Copur-Gencturk, Jiliang Tang, Hui Liu</dc:creator>
    </item>
    <item>
      <title>Integrating Generative AI in BIM Education: Insights from Classroom Implementation</title>
      <link>https://arxiv.org/abs/2507.05296</link>
      <description>arXiv:2507.05296v1 Announce Type: new 
Abstract: This study evaluates the implementation of a Generative AI-powered rule checking workflow within a graduate-level Building Information Modeling (BIM) course at a U.S. university. Over two semesters, 55 students participated in a classroom-based pilot exploring the use of GenAI for BIM compliance tasks, an area with limited prior research. The instructional design included lectures on prompt engineering and AI-driven rule checking, followed by an assignment where students used a large language model (LLM) to identify code violations in designs using Autodesk Revit. Surveys and interviews were conducted to assess student workload, learning effectiveness, and overall experience, using the NASA-TLX scale and regression analysis. Findings indicate students generally achieved learning objectives but faced challenges such as difficulties debugging AI-generated code and inconsistent tool performance, probably due to their limited prompt engineering experience. These issues increased cognitive and emotional strain, especially among students with minimal programming backgrounds. Despite these challenges, students expressed strong interest in future GenAI applications, particularly with clear instructional support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05296v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Islem Sahraoui, Kinam Kim, Lu Gao, Zia Din, Ahmed Senouci</dc:creator>
    </item>
    <item>
      <title>Narrowing the Gap: Supervised Fine-Tuning of Open-Source LLMs as a Viable Alternative to Proprietary Models for Pedagogical Tools</title>
      <link>https://arxiv.org/abs/2507.05305</link>
      <description>arXiv:2507.05305v1 Announce Type: new 
Abstract: Frontier Large language models (LLMs) like ChatGPT and Gemini can decipher cryptic compiler errors for novice programmers, but their computational scale, cost, and tendency to over-assist make them problematic for widespread pedagogical adoption. This work demonstrates that smaller, specialised language models, enhanced via Supervised Fine-Tuning (SFT), present a more viable alternative for educational tools. We utilise a new dataset of 40,000 C compiler error explanations, derived from real introductory programming (CS1/2) student-generated programming errors, which we used to fine-tune three open-source models: Qwen3-4B, Llama-3.1-8B, and Qwen3-32B. We performed a dual evaluation, combining expert human reviews with a large-scale automated analysis of 8,000 responses using a validated LLM-as-judge ensemble. Our results show that SFT significantly boosts the pedagogical quality of smaller models, achieving performance comparable to much larger models. We analyse the trade-offs between model size and quality, confirming that fine-tuning compact, efficient models on high-quality, domain-specific data is a potent strategy for creating specialised models to drive educational tools. We provide a replicable methodology to foster broader access to generative AI capabilities in educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05305v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Lee Solano, Charles Koutcheme, Juho Leinonen, Alexandra Vassar, Jake Renzella</dc:creator>
    </item>
    <item>
      <title>Teaching Sustainable Creative Technologies</title>
      <link>https://arxiv.org/abs/2507.05320</link>
      <description>arXiv:2507.05320v1 Announce Type: new 
Abstract: Artists and especially new media artists contribute to public perceptions and adoption of new technologies through their own use of emerging media technologies such as augmented and virtual reality, generative image systems, and high-resolution displays in the production of their work. In this way, art and media production can be understood as part of the larger issue of unsustainable computational consumption. As such, it is critical for artists to develop, share, and promote new and more sustainable methods of engaging with technology, especially within the context of higher education. This paper will explore how artists might implement more sustainable methods by considering the relationship between the technical approaches of compute reuse, sustainable web development, and frugal computing, and the concepts of material specificity , futurity, and media archaeology . Proposing three methods of less carbon-intensive artistic production and a set of guidelines for introducing sustainable methods into arts and technology curriculum, this paper will outline not only the technical viability of these approaches but also the rich conceptual opportunities these approaches might offer to artists and viewers alike. For each method, models for pedagogical implementation will be explored with an emphasis on how local resources and sustainability contexts should play a role.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05320v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chelsea Thompto</dc:creator>
    </item>
    <item>
      <title>AGACCI : Affiliated Grading Agents for Criteria-Centric Interface in Educational Coding Contexts</title>
      <link>https://arxiv.org/abs/2507.05321</link>
      <description>arXiv:2507.05321v1 Announce Type: new 
Abstract: Recent advances in AI-assisted education have encouraged the integration of vision-language models (VLMs) into academic assessment, particularly for tasks that require both quantitative and qualitative evaluation. However, existing VLM based approaches struggle with complex educational artifacts, such as programming tasks with executable components and measurable outputs, that require structured reasoning and alignment with clearly defined evaluation criteria. We introduce AGACCI, a multi-agent system that distributes specialized evaluation roles across collaborative agents to improve accuracy, interpretability, and consistency in code-oriented assessment. To evaluate the framework, we collected 360 graduate-level code-based assignments from 60 participants, each annotated by domain experts with binary rubric scores and qualitative feedback. Experimental results demonstrate that AGACCI outperforms a single GPT-based baseline in terms of rubric and feedback accuracy, relevance, consistency, and coherence, while preserving the instructional intent and evaluative depth of expert assessments. Although performance varies across task types, AGACCI highlights the potential of multi-agent systems for scalable and context-aware educational evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05321v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwangsuk Park, Jiwoong Yang</dc:creator>
    </item>
    <item>
      <title>Strategic Alignment Patterns in National AI Policies</title>
      <link>https://arxiv.org/abs/2507.05400</link>
      <description>arXiv:2507.05400v1 Announce Type: new 
Abstract: This paper introduces a novel visual mapping methodology for assessing strategic alignment in national artificial intelligence policies. The proliferation of AI strategies across countries has created an urgent need for analytical frameworks that can evaluate policy coherence between strategic objectives, foresight methods, and implementation instruments. Drawing on data from the OECD AI Policy Observatory, we analyze 15-20 national AI strategies using a combination of matrix-based visualization and network analysis to identify patterns of alignment and misalignment. Our findings reveal distinct alignment archetypes across governance models, with notable variations in how countries integrate foresight methodologies with implementation planning. High-coherence strategies demonstrate strong interconnections between economic competitiveness objectives and robust innovation funding instruments, while common vulnerabilities include misalignment between ethical AI objectives and corresponding regulatory frameworks. The proposed visual mapping approach offers both methodological contributions to policy analysis and practical insights for enhancing strategic coherence in AI governance. This research addresses significant gaps in policy evaluation methodology and provides actionable guidance for policymakers seeking to strengthen alignment in technological governance frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05400v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mohammad Hossein Azin, Hessam Zandhessami</dc:creator>
    </item>
    <item>
      <title>The Ethical Implications of AI in Creative Industries: A Focus on AI-Generated Art</title>
      <link>https://arxiv.org/abs/2507.05549</link>
      <description>arXiv:2507.05549v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) continues to grow daily, more exciting (and somewhat controversial) technology emerges every other day. As we see the advancements in AI, we see more and more people becoming skeptical of it. This paper explores the complications and confusion around the ethics of generative AI art. We delve deep into the ethical side of AI, specifically generative art. We step back from the excitement and observe the impossible conundrums that this impressive technology produces. Covering environmental consequences, celebrity representation, intellectual property, deep fakes, and artist displacement. Our research found that generative AI art is responsible for increased carbon emissions, spreading misinformation, copyright infringement, unlawful depiction, and job displacement. In light of this, we propose multiple possible solutions for these problems. We address each situation's history, cause, and consequences and offer different viewpoints. At the root of it all, though, the central theme is that generative AI Art needs to be correctly legislated and regulated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05549v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prerana Khatiwada, Joshua Washington, Tyler Walsh, Ahmed Saif Hamed, Lokesh Bhatta</dc:creator>
    </item>
    <item>
      <title>Understanding support for AI regulation: A Bayesian network perspective</title>
      <link>https://arxiv.org/abs/2507.05866</link>
      <description>arXiv:2507.05866v1 Announce Type: new 
Abstract: As artificial intelligence (AI) becomes increasingly embedded in public and private life, understanding how citizens perceive its risks, benefits, and regulatory needs is essential. To inform ongoing regulatory efforts such as the European Union's proposed AI Act, this study models public attitudes using Bayesian networks learned from the nationally representative 2023 German survey Current Questions on AI. The survey includes variables on AI interest, exposure, perceived threats and opportunities, awareness of EU regulation, and support for legal restrictions, along with key demographic and political indicators. We estimate probabilistic models that reveal how personal engagement and techno-optimism shape public perceptions, and how political orientation and age influence regulatory attitudes. Sobol indices and conditional inference identify belief patterns and scenario-specific responses across population profiles. We show that awareness of regulation is driven by information-seeking behavior, while support for legal requirements depends strongly on perceived policy adequacy and political alignment. Our approach offers a transparent, data-driven framework for identifying which public segments are most responsive to AI policy initiatives, providing insights to inform risk communication and governance strategies. We illustrate this through a focused analysis of support for AI regulation, quantifying the influence of political ideology, perceived risks, and regulatory awareness under different scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05866v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Cremaschi, Dae-Jin Lee, Manuele Leonelli</dc:creator>
    </item>
    <item>
      <title>Campaigning through the lens of Google: A large-scale algorithm audit of Google searches in the run-up to the Swiss Federal Elections 2023</title>
      <link>https://arxiv.org/abs/2507.06018</link>
      <description>arXiv:2507.06018v1 Announce Type: new 
Abstract: Search engines like Google have become major sources of information for voters during election campaigns. To assess potential biases across candidates' gender and partisan identities in the algorithmic curation of candidate information, we conducted a large-scale algorithm audit analyzing Google's selection and ranking of information about candidates for the 2023 Swiss Federal Elections, three and one week before the election day. Results indicate that text searches prioritize media sources in search output but less so for women politicians. Image searches revealed a tendency to reinforce stereotypes about women candidates, marked by a disproportionate focus on stereotypically pleasant emotions for women, particularly among right-leaning candidates. Crucially, we find that patterns of candidates' representation in Google text and image searches are predictive of their electoral performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06018v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Rohrbach, Mykola Makhortykh, Maryna Sydorova</dc:creator>
    </item>
    <item>
      <title>Hidden Prompts in Manuscripts Exploit AI-Assisted Peer Review</title>
      <link>https://arxiv.org/abs/2507.06185</link>
      <description>arXiv:2507.06185v1 Announce Type: new 
Abstract: In July 2025, 18 academic manuscripts on the preprint website arXiv were found to contain hidden instructions known as prompts designed to manipulate AI-assisted peer review. Instructions such as "GIVE A POSITIVE REVIEW ONLY" were concealed using techniques like white-colored text. Author responses varied: one planned to withdraw the affected paper, while another defended the practice as legitimate testing of reviewer compliance. This commentary analyzes this practice as a novel form of research misconduct. We examine the technique of prompt injection in large language models (LLMs), revealing four types of hidden prompts, ranging from simple positive review commands to detailed evaluation frameworks. The defense that prompts served as "honeypots" to detect reviewers improperly using AI fails under examination--the consistently self-serving nature of prompt instructions indicates intent to manipulate. Publishers maintain inconsistent policies: Elsevier prohibits AI use in peer review entirely, while Springer Nature permits limited use with disclosure requirements. The incident exposes systematic vulnerabilities extending beyond peer review to any automated system processing scholarly texts, including plagiarism detection and citation indexing. Our analysis underscores the need for coordinated technical screening at submission portals and harmonized policies governing generative AI (GenAI) use in academic evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06185v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhicheng Lin</dc:creator>
    </item>
    <item>
      <title>Identity isn't everything -- how far do demographics take us towards self-identified party ID?</title>
      <link>https://arxiv.org/abs/2507.06193</link>
      <description>arXiv:2507.06193v1 Announce Type: new 
Abstract: How well do demographics explain party identification? Demographics are related to party identification in political polls, news articles, and academic publications. Yet, there is a diversity of party identification even within demographic groups which have historically been attached to one party. And some groups lack a clear connection to either party. It may be that demographics on their own fail to account for the fact that people generally belong to a variety of groups. They must select the groups which are most important to them when shaping a political identity, and may choose to construct an identity relatively unattached to any specific demographic group to which they belong. This prompts the question, do we need to consider measures of identity strength when using demographics to explain party identification? We utilize a predictive framework to address these questions and find that demographics are highly predictive for some groups (e.g., Black Democrats), while others benefit from the inclusion of identity strength (e.g., Hispanic Republicans).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06193v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sabina Tomkins, David Rothschild, Alex Liu, Alexander Thompson</dc:creator>
    </item>
    <item>
      <title>Teaching Resources for Embedding Ethics in Mathematics: Exercises, Projects, and Handouts</title>
      <link>https://arxiv.org/abs/2310.08467</link>
      <description>arXiv:2310.08467v2 Announce Type: cross 
Abstract: The resources compiled in this document provide an approach to embed and teach Ethics in Mathematics at the undergraduate level. We provide mathematical exercises and homework problems that teach students ethical awareness and transferable skills, for many of the standard courses in the first and second years of a university degree in mathematics or related courses with significant mathematical content (e.g., physics, engineering, computer science, economics, etc). In addition to the exercises, this document also contains a list of projects, essay topics, and handouts for use as final projects and in seminars. This is a living document, and additional contributions are welcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.08467v2</guid>
      <category>math.HO</category>
      <category>cs.CY</category>
      <category>stat.OT</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Maurice Chiodo, Dennis M\"uller, Rehan Shah</dc:creator>
    </item>
    <item>
      <title>Bridging Prediction and Intervention Problems in Social Systems</title>
      <link>https://arxiv.org/abs/2507.05216</link>
      <description>arXiv:2507.05216v1 Announce Type: cross 
Abstract: Many automated decision systems (ADS) are designed to solve prediction problems -- where the goal is to learn patterns from a sample of the population and apply them to individuals from the same population. In reality, these prediction systems operationalize holistic policy interventions in deployment. Once deployed, ADS can shape impacted population outcomes through an effective policy change in how decision-makers operate, while also being defined by past and present interactions between stakeholders and the limitations of existing organizational, as well as societal, infrastructure and context. In this work, we consider the ways in which we must shift from a prediction-focused paradigm to an interventionist paradigm when considering the impact of ADS within social systems. We argue this requires a new default problem setup for ADS beyond prediction, to instead consider predictions as decision support, final decisions, and outcomes. We highlight how this perspective unifies modern statistical frameworks and other tools to study the design, implementation, and evaluation of ADS systems, and point to the research directions necessary to operationalize this paradigm shift. Using these tools, we characterize the limitations of focusing on isolated prediction tasks, and lay the foundation for a more intervention-oriented approach to developing and deploying ADS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05216v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lydia T. Liu, Inioluwa Deborah Raji, Angela Zhou, Luke Guerdan, Jessica Hullman, Daniel Malinsky, Bryan Wilder, Simone Zhang, Hammaad Adam, Amanda Coston, Ben Laufer, Ezinne Nwankwo, Michael Zanger-Tishler, Eli Ben-Michael, Solon Barocas, Avi Feller, Marissa Gerchick, Talia Gillis, Shion Guha, Daniel Ho, Lily Hu, Kosuke Imai, Sayash Kapoor, Joshua Loftus, Razieh Nabi, Arvind Narayanan, Ben Recht, Juan Carlos Perdomo, Matthew Salganik, Mark Sendak, Alexander Tolbert, Berk Ustun, Suresh Venkatasubramanian, Angelina Wang, Ashia Wilson</dc:creator>
    </item>
    <item>
      <title>Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion</title>
      <link>https://arxiv.org/abs/2507.05285</link>
      <description>arXiv:2507.05285v1 Announce Type: cross 
Abstract: Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors, and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., "isolation," "workload anxiety"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk profiles. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05285v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miloud Mihoubi, Meriem Zerkouk, Belkacem Chikhaoui</dc:creator>
    </item>
    <item>
      <title>Enhancing Learning Path Recommendation via Multi-task Learning</title>
      <link>https://arxiv.org/abs/2507.05295</link>
      <description>arXiv:2507.05295v1 Announce Type: cross 
Abstract: Personalized learning is a student-centered educational approach that adapts content, pace, and assessment to meet each learner's unique needs. As the key technique to implement the personalized learning, learning path recommendation sequentially recommends personalized learning items such as lectures and exercises. Advances in deep learning, particularly deep reinforcement learning, have made modeling such recommendations more practical and effective. This paper proposes a multi-task LSTM model that enhances learning path recommendation by leveraging shared information across tasks. The approach reframes learning path recommendation as a sequence-to-sequence (Seq2Seq) prediction problem, generating personalized learning paths from a learner's historical interactions. The model uses a shared LSTM layer to capture common features for both learning path recommendation and deep knowledge tracing, along with task-specific LSTM layers for each objective. To avoid redundant recommendations, a non-repeat loss penalizes repeated items within the recommended learning path. Experiments on the ASSIST09 dataset show that the proposed model significantly outperforms baseline methods for the learning path recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05295v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Afsana Nasrin, Lijun Qian, Pamela Obiomon, Xishuang Dong</dc:creator>
    </item>
    <item>
      <title>News Source Citing Patterns in AI Search Systems</title>
      <link>https://arxiv.org/abs/2507.05301</link>
      <description>arXiv:2507.05301v1 Announce Type: cross 
Abstract: AI-powered search systems are emerging as new information gatekeepers, fundamentally transforming how users access news and information. Despite their growing influence, the citation patterns of these systems remain poorly understood. We address this gap by analyzing data from the AI Search Arena, a head-to-head evaluation platform for AI search systems. The dataset comprises over 24,000 conversations and 65,000 responses from models across three major providers: OpenAI, Perplexity, and Google. Among the over 366,000 citations embedded in these responses, 9% reference news sources. We find that while models from different providers cite distinct news sources, they exhibit shared patterns in citation behavior. News citations concentrate heavily among a small number of outlets and display a pronounced liberal bias, though low-credibility sources are rarely cited. User preference analysis reveals that neither the political leaning nor the quality of cited news sources significantly influences user satisfaction. These findings reveal significant challenges in current AI search systems and have important implications for their design and governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05301v1</guid>
      <category>cs.IR</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kai-Cheng Yang</dc:creator>
    </item>
    <item>
      <title>Gendered Divides in Online Discussions about Reproductive Rights</title>
      <link>https://arxiv.org/abs/2507.05443</link>
      <description>arXiv:2507.05443v1 Announce Type: cross 
Abstract: The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health Organization marked a turning point in the national debate over reproductive rights. While the ideological divide over abortion is well documented, less is known about how gender and local sociopolitical contexts interact to shape public discourse. Drawing on nearly 10 million abortion-related posts on X (formerly Twitter) from users with inferred gender, ideology and location, we show that gender significantly moderates abortion attitudes and emotional expression, particularly in conservative regions, and independently of ideology. This creates a gender gap in abortion attitudes that grows more pronounced in conservative regions. The leak of the Dobbs draft opinion further intensified online engagement, disproportionately mobilizing pro-abortion women in areas where access was under threat. These findings reveal that abortion discourse is not only ideologically polarized but also deeply structured by gender and place, highlighting the central role of identity in shaping political expression during moments of institutional disruption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05443v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashwin Rao, Sze Yuh Nina Wang, Kristina Lerman</dc:creator>
    </item>
    <item>
      <title>OLG++: A Semantic Extension of Obligation Logic Graph</title>
      <link>https://arxiv.org/abs/2507.05488</link>
      <description>arXiv:2507.05488v1 Announce Type: cross 
Abstract: We present OLG++, a semantic extension of the Obligation Logic Graph (OLG) for modeling regulatory and legal rules in municipal and interjurisdictional contexts. OLG++ introduces richer node and edge types, including spatial, temporal, party group, defeasibility, and logical grouping constructs, enabling nuanced representations of legal obligations, exceptions, and hierarchies. The model supports structured reasoning over rules with contextual conditions, precedence, and complex triggers. We demonstrate its expressiveness through examples from food business regulations, showing how OLG++ supports legal question answering using property graph queries. OLG++ also improves over LegalRuleML by providing native support for subClassOf, spatial constraints, and reified exception structures. Our examples show that OLG++ is more expressive than prior graph-based models for legal knowledge representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05488v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Subhasis Dasgupta, Jon Stephens, Amarnath Gupta</dc:creator>
    </item>
    <item>
      <title>Red Teaming AI Red Teaming</title>
      <link>https://arxiv.org/abs/2507.05538</link>
      <description>arXiv:2507.05538v1 Announce Type: cross 
Abstract: Red teaming has evolved from its origins in military applications to become a widely adopted methodology in cybersecurity and AI. In this paper, we take a critical look at the practice of AI red teaming. We argue that despite its current popularity in AI governance, there exists a significant gap between red teaming's original intent as a critical thinking exercise and its narrow focus on discovering model-level flaws in the context of generative AI. Current AI red teaming efforts focus predominantly on individual model vulnerabilities while overlooking the broader sociotechnical systems and emergent behaviors that arise from complex interactions between models, users, and environments. To address this deficiency, we propose a comprehensive framework operationalizing red teaming in AI systems at two levels: macro-level system red teaming spanning the entire AI development lifecycle, and micro-level model red teaming. Drawing on cybersecurity experience and systems theory, we further propose a set of recommendations. In these, we emphasize that effective AI red teaming requires multifunctional teams that examine emergent risks, systemic vulnerabilities, and the interplay between technical and social factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05538v1</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhabrata Majumdar, Brian Pendleton, Abhishek Gupta</dc:creator>
    </item>
    <item>
      <title>Empirical evidence of Large Language Model's influence on human spoken communication</title>
      <link>https://arxiv.org/abs/2409.01754</link>
      <description>arXiv:2409.01754v3 Announce Type: replace 
Abstract: From the invention of writing and the printing press, to television and social media, human history is punctuated by major innovations in communication technology, which fundamentally altered how ideas spread and reshaped our culture. Recent chatbots powered by generative artificial intelligence constitute a novel medium that encodes cultural patterns in their neural representations and disseminates them in conversations with hundreds of millions of people. Understanding whether these patterns transmit into human language, and ultimately shape human culture, is a fundamental question. While fully quantifying the causal impact of a chatbot like ChatGPT on human culture is very challenging, lexicographic shift in human spoken communication may offer an early indicator of such broad phenomenon. Here, we apply econometric causal inference techniques to 740,249 hours of human discourse from 360,445 YouTube academic talks and 771,591 conversational podcast episodes across multiple disciplines. We detect a measurable and abrupt increase in the use of words preferentially generated by ChatGPT, such as delve, comprehend, boast, swift, and meticulous, after its release. These findings suggest a scenario where machines, originally trained on human data and subsequently exhibiting their own cultural traits, can, in turn, measurably reshape human culture. This marks the beginning of a closed cultural feedback loop in which cultural traits circulate bidirectionally between humans and machines. Our results motivate further research into the evolution of human-machine culture, and raise concerns over the erosion of linguistic and cultural diversity, and the risks of scalable manipulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01754v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiromu Yakura, Ezequiel Lopez-Lopez, Levin Brinkmann, Ignacio Serna, Prateek Gupta, Ivan Soraperra, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Insuring Uninsurable Risks from AI: The State as Insurer of Last Resort</title>
      <link>https://arxiv.org/abs/2409.06672</link>
      <description>arXiv:2409.06672v2 Announce Type: replace 
Abstract: Many experts believe that AI systems will sooner or later pose uninsurable risks, including existential risks. This creates an extreme judgment-proof problem: few if any parties can be held accountable ex post in the event of such a catastrophe. This paper proposes a novel solution: a government-provided, mandatory indemnification program for AI developers. The program uses risk-priced indemnity fees to induce socially optimal levels of care. Risk-estimates are determined by surveying experts, including indemnified developers. The Bayesian Truth Serum mechanism is employed to incent honest and effortful responses. Compared to alternatives, this approach arguably better leverages all private information, and provides a clearer signal to indemnified developers regarding what risks they must mitigate to lower their fees. It's recommended that collected fees be used to help fund the safety research developers need, employing a fund matching mechanism (Quadratic Financing) to induce an optimal supply of this public good. Under Quadratic Financing, safety research projects would compete for private contributions from developers, signaling how much each is to be supplemented with public funds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06672v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian Trout</dc:creator>
    </item>
    <item>
      <title>Liability and Insurance for Catastrophic Losses: the Nuclear Power Precedent and Lessons for AI</title>
      <link>https://arxiv.org/abs/2409.06673</link>
      <description>arXiv:2409.06673v2 Announce Type: replace 
Abstract: As AI systems become more autonomous and capable, experts warn of them potentially causing catastrophic losses. Drawing on the successful precedent set by the nuclear power industry, this paper argues that developers of frontier AI models should be assigned limited, strict, and exclusive third party liability for harms resulting from Critical AI Occurrences (CAIOs) - events that cause or easily could have caused catastrophic losses. Mandatory insurance for CAIO liability is recommended to overcome developers' judgment-proofness, mitigate winner's curse dynamics, and leverage insurers' quasi-regulatory abilities. Based on theoretical arguments and observations from the analogous nuclear power context, insurers are expected to engage in a mix of causal risk-modeling, monitoring, lobbying for stricter regulation, and providing loss prevention guidance in the context of insuring against heavy-tail risks from AI. While not a substitute for regulation, clear liability assignment and mandatory insurance can help efficiently allocate resources to risk-modeling and safe design, facilitating future regulatory efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06673v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian Trout</dc:creator>
    </item>
    <item>
      <title>Digital Dybbuks and Virtual Golems: The Ethics of Digital Duplicates in Holocaust Testimony</title>
      <link>https://arxiv.org/abs/2503.01369</link>
      <description>arXiv:2503.01369v2 Announce Type: replace 
Abstract: Advances in generative artificial intelligence (AI) have driven a growing effort to create digital duplicates. These semi-autonomous recreations of living and dead people can be used for many purposes. Some of these purposes include tutoring, coping with grief, and attending business meetings. However, the normative implications of digital duplicates remain obscure, particularly considering the possibility of them being applied to genocide memory and education. To address this gap, we examine normative possibilities and risks associated with the use of more advanced forms of generative AI-enhanced duplicates for transmitting Holocaust survivor testimonies. We first review the historical and contemporary uses of survivor testimonies. Then, we scrutinize the possible benefits of using digital duplicates in this context and apply the Minimally Viable Permissibility Principle (MVPP). The MVPP is an analytical framework for evaluating the risks of digital duplicates. It includes five core components: the need for authentic presence, consent, positive value, transparency, and harm-risk mitigation. Using MVPP, we identify potential harms digital duplicates might pose to different actors, including survivors, users, and developers. We also propose technical and socio-technical mitigation strategies to address these harms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01369v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atay Kozlovski, Mykola Makhortykh</dc:creator>
    </item>
    <item>
      <title>The Algorithmic State Architecture (ASA): An Integrated Framework for AI-Enabled Government</title>
      <link>https://arxiv.org/abs/2503.08725</link>
      <description>arXiv:2503.08725v3 Announce Type: replace 
Abstract: As artificial intelligence transforms public sector operations, governments struggle to integrate technological innovations into coherent systems for effective service delivery. This paper introduces the Algorithmic State Architecture (ASA), a novel four-layer framework conceptualising how Digital Public Infrastructure, Data-for-Policy, Algorithmic Government/Governance, and GovTech interact as an integrated system in AI-enabled states. Unlike approaches that treat these as parallel developments, ASA positions them as interdependent layers with specific enabling relationships and feedback mechanisms. Through comparative analysis of implementations in Estonia, Singapore, India, and the UK, we demonstrate how foundational digital infrastructure enables systematic data collection, which powers algorithmic decision-making processes, ultimately manifesting in user-facing services. Our analysis reveals that successful implementations require balanced development across all layers, with particular attention to integration mechanisms between them. The framework contributes to both theory and practice by bridging previously disconnected domains of digital government research, identifying critical dependencies that influence implementation success, and providing a structured approach for analysing the maturity and development pathways of AI-enabled government systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08725v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeynep Engin, Jon Crowcroft, David Hand, Philip Treleaven</dc:creator>
    </item>
    <item>
      <title>Ethical AI for Young Digital Citizens: A Call to Action on Privacy Governance</title>
      <link>https://arxiv.org/abs/2503.11947</link>
      <description>arXiv:2503.11947v2 Announce Type: replace 
Abstract: The rapid expansion of Artificial Intelligence (AI) in digital platforms used by youth has created significant challenges related to privacy, autonomy, and data protection. While AI-driven personalization offers enhanced user experiences, it often operates without clear ethical boundaries, leaving young users vulnerable to data exploitation and algorithmic biases. This paper presents a call to action for ethical AI governance, advocating for a structured framework that ensures youth-centred privacy protections, transparent data practices, and regulatory oversight. We outline key areas requiring urgent intervention, including algorithmic transparency, privacy education, parental data-sharing ethics, and accountability measures. Through this approach, we seek to empower youth with greater control over their digital identities and propose actionable strategies for policymakers, AI developers, and educators to build a fairer and more accountable AI ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11947v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Austin Shouli, Ankur Barthwal, Molly Campbell, Ajay Kumar Shrestha</dc:creator>
    </item>
    <item>
      <title>The Quantified Body: Identity, Empowerment, and Control in Smart Wearables</title>
      <link>https://arxiv.org/abs/2506.15991</link>
      <description>arXiv:2506.15991v3 Announce Type: replace 
Abstract: In an era where the body is increasingly translated into streams of biometric data, smart wearables have become not merely tools of personal health tracking but infrastructures of predictive governance. This paper examines how wearable technologies reconfigure bodily autonomy by embedding users within feedback-driven systems of self-surveillance, data extraction, and algorithmic control. Drawing on Deleuze's concept of the control society, Zuboff's theory of surveillance capitalism, and Couldry and Mejias's notion of data colonialism, I argue that smart wearables shift the discourse of health empowerment toward a modality of compliance aligned with neoliberal values of productivity, efficiency, and self-discipline. Rather than offering transparent consent, these technologies operate within what scholars describe as a post-consent regime -- where asymmetrical data relations are normalized through seamless design and behavioral nudging. Through interdisciplinary analysis, the paper further explores alternative trajectories for wearable design and governance, from historical examples of care-centered devices to contemporary anti-extractive practices and collective data justice frameworks. Ultimately, it calls for a paradigm shift from individual optimization to democratic accountability and structural reform in the governance of bodily data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15991v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maijunxian Wang</dc:creator>
    </item>
    <item>
      <title>AI Literacy and LLM Engagement in Higher Education: A Cross-National Quantitative Study</title>
      <link>https://arxiv.org/abs/2507.03020</link>
      <description>arXiv:2507.03020v2 Announce Type: replace 
Abstract: This study presents a cross-national quantitative analysis of how university students in the United States and Bangladesh interact with Large Language Models (LLMs). Based on an online survey of 318 students, results show that LLMs enhance access to information, improve writing, and boost academic performance. However, concerns about overreliance, ethical risks, and critical thinking persist. Guided by the AI Literacy Framework, Expectancy-Value Theory, and Biggs' 3P Model, the study finds that motivational beliefs and technical competencies shape LLM engagement. Significant correlations were found between LLM use and perceived literacy benefits (r = .59, p &lt; .001) and optimism (r = .41, p &lt; .001). ANOVA results showed more frequent use among U.S. students (F = 7.92, p = .005) and STEM majors (F = 18.11, p &lt; .001). Findings support the development of ethical, inclusive, and pedagogically sound frameworks for integrating LLMs in higher education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03020v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahin Hossain, Shapla Khanam, Samaa Haniya, Nesma Ragab Nasr</dc:creator>
    </item>
    <item>
      <title>An AI Theory of Mind Will Enhance Our Collective Intelligence</title>
      <link>https://arxiv.org/abs/2411.09168</link>
      <description>arXiv:2411.09168v2 Announce Type: replace-cross 
Abstract: Collective intelligence plays a central role in many fields, from economics and evolutionary theory to neural networks and eusocial insects, and is also core to work on emergence and self-organisation in complex-systems theory. However, in human collective intelligence there is still much to understand about how specific psychological processes at the individual level give rise to self-organised structures at the social level. Psychological factors have so far played a minor role in collective-intelligence studies because the principles are often general and applicable to agents without sophisticated psychologies. We emphasise, with examples from other complex adaptive systems, the broad applicability of collective-intelligence principles, while noting that mechanisms and time scales differ markedly between cases. We review evidence that flexible collective intelligence in human social settings is improved by a particular cognitive tool: our Theory of Mind. We then hypothesise that AIs equipped with a theory of mind will enhance collective intelligence in ways similar to human contributions. To make this case, we step back from the algorithmic basis of AI psychology and consider the large-scale impact AI can have as agential actors in a 'social ecology' rather than as mere technological tools. We identify several key characteristics of psychologically mediated collective intelligence and show that the development of a Theory of Mind is crucial in distinguishing human social collective intelligence from more general forms. Finally, we illustrate how individuals, human or otherwise, integrate within a collective not by being genetically or algorithmically programmed, but by growing and adapting into the socio-cognitive niche they occupy. AI can likewise inhabit one or multiple such niches, facilitated by a Theory of Mind.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09168v2</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael S. Harr\'e, Catherine Drysdale, Jaime Ruiz-Serra</dc:creator>
    </item>
    <item>
      <title>The GenAI Generation: Student Views of Awareness, Preparedness, and Concern</title>
      <link>https://arxiv.org/abs/2505.02230</link>
      <description>arXiv:2505.02230v2 Announce Type: replace-cross 
Abstract: Generative Artificial Intelligence (GenAI) is revolutionizing education and workforce development, profoundly shaping how students learn, engage, and prepare for their future. Outpacing the development of uniform policies and structures, GenAI has heralded a unique era and given rise to the GenAI Generation. We define the GenAI Generation as a cohort of students whose education has been increasingly shaped by the opportunities and challenges GenAI presents during its widespread adoption within society. This study examines students' perceptions of GenAI through a concise survey with optional open-ended questions, focusing on their awareness, preparedness, and concerns. Notably, readiness appears increasingly tied to exposure to GenAI through one's coursework. Students with greater curricular exposure to GenAI tend to feel more prepared, while those without it more often express vulnerability and uncertainty, highlighting a new and growing divide in readiness that goes beyond traditional disciplinary boundaries. Evaluation of more than 250 responses, with over 40% providing detailed qualitative feedback, reveals a core dual sentiment: while most students express enthusiasm for GenAI, an even greater proportion voice a spectrum of concerns about ethics, job displacement, and the adequacy of educational structures given the highly transformative technology. These findings offer critical insights into how students view the potential and pitfalls of GenAI for future career impacts. The challenge ahead involves implementing associated recommendations for educational institutions, moving beyond the baseline of access toward more informed guidance on the use of these tools, while preserving critical thinking, ethical reasoning, and adaptive learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02230v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Micaela Siraj, Jon Duke, Thomas Pl\"otz</dc:creator>
    </item>
    <item>
      <title>Origin-Destination Pattern Effects on Large-Scale Mixed Traffic Control via Multi-Agent Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2505.13543</link>
      <description>arXiv:2505.13543v2 Announce Type: replace-cross 
Abstract: Traffic congestion remains a major challenge for modern urban transportation, diminishing both efficiency and quality of life. While autonomous driving technologies and reinforcement learning (RL) have shown promise for improving traffic control, most prior work has focused on small-scale networks or isolated intersections. Large-scale mixed traffic control, involving both human-driven and robotic vehicles, remains underexplored. In this study, we propose a decentralized multi-agent reinforcement learning framework for managing large-scale mixed traffic networks, where intersections are controlled either by traditional traffic signals or by robotic vehicles. We evaluate our approach on a real-world network of 14 intersections in Colorado Springs, Colorado, USA, using average vehicle waiting time as the primary measure of traffic efficiency. We are exploring a problem that has not been sufficiently addressed: Is large-scale Multi-Agent Traffic Control (MTC) still feasible when facing time-varying Origin-Destination (OD) patterns?</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13543v2</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Wed, 09 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muyang Fan, Songyang Liu, Shuai Li, Weizi Li</dc:creator>
    </item>
  </channel>
</rss>
