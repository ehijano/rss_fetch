<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Jun 2025 01:39:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ratio of Mediator Probability Weighting for Estimating Natural Direct and Indirect Effects</title>
      <link>https://arxiv.org/abs/2506.03284</link>
      <description>arXiv:2506.03284v1 Announce Type: new 
Abstract: Decomposing a total causal effect into natural direct and indirect effects is central to revealing causal mechanisms. Conventional methods achieve the decomposition by specifying an outcome model as a linear function of the treatment, the mediator, and the observed covariates under identification assumptions including the assumption of no interaction between treatment and mediator. Recent statistical advances relax this assumption typically within the linear or nonlinear regression framework. I propose a non-parametric approach that also relaxes the assumption of no treatment-mediator interaction while avoiding the problems of outcome model specification that become particularly acute in the presence of a large number of covariates. The key idea is to estimate the marginal mean of each counterfactual outcome by assigning a weight to every experimental unit such that the weighted distribution of the mediator under the experimental condition approximates the counterfactual mediator distribution under the control condition. The weight is a ratio of the conditional probability of a mediator value under the control condition to that of the same mediator value under the experimental condition. A non-parametric approach to estimating the weight on the basis of propensity score stratification promises to increase the robustness of the direct and indirect effect estimates. The outcome is modeled as a function of the direct and indirect effects with minimal model-based assumptions. This method applies regardless of the distribution of the outcome or the functional relationship between the outcome and the mediator, and is suitable for handling a large number of pretreatment covariates. RMPW software packages are available in Stata (https://ideas.repec.org/c/boc/bocode/s458301.html) and R (https://cran.r-project.org/web/packages/rmpw/index.html).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03284v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>JSM Proceedings, Biometrics Section. Alexandria, VA: American Statistical Association, pp.2401-2415 (2010)</arxiv:journal_reference>
      <dc:creator>Guanglei Hong</dc:creator>
    </item>
    <item>
      <title>Constrained mixtures of generalized normal distributions</title>
      <link>https://arxiv.org/abs/2506.03285</link>
      <description>arXiv:2506.03285v1 Announce Type: new 
Abstract: This work introduces a family of univariate constrained mixtures of generalized normal distributions (CMGND) where the location, scale, and shape parameters can be constrained to be equal across any subset of mixture components. An expectation conditional maximisation (ECM) algorithm with Newton-Raphson updates is used to estimate the model parameters under the constraints. Simulation studies demonstrate that imposing correct constraints leads to more accurate parameter estimation compared to unconstrained mixtures, especially when components substantially overlap. Constrained models also exhibit competitive performance in capturing key characteristics of the marginal distribution, such as kurtosis. On a real dataset of daily stock index returns, CMGND models outperform constrained mixtures of normals and Student's t distributions based on the BIC criterion, highlighting their flexibility in modelling nonnormal features. The proposed constrained approach enhances interpretability and can improve parametric efficiency without compromising distributional flexibility for complex data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03285v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pierdomenico Duttilo, Stefano Antonio Gattone, Alfred Kume</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Missing Exposures, Missing Outcomes, and Dependence</title>
      <link>https://arxiv.org/abs/2506.03336</link>
      <description>arXiv:2506.03336v1 Announce Type: new 
Abstract: Missing data are ubiquitous in public health research. The missing-completely-at-random (MCAR) assumption is often unrealistic and can lead to meaningful bias when violated. The missing-at-random (MAR) assumption tends to be more reasonable, but guidance on conducting causal analyses under MAR is limited when there is missingness on multiple variables. We present a series of causal graphs and identification results to demonstrate the handling of missing exposures and outcomes in observational studies. For estimation and inference, we highlight the use of targeted minimum loss-based estimation (TMLE) with Super Learner to flexibly and robustly address confounding, missing data, and dependence. Our work is motivated by SEARCH-TB's investigation of the effect of alcohol consumption on the risk of incident tuberculosis (TB) infection in rural Uganda. This study posed notable challenges due to confounding, missingness on the exposure (alcohol use), missingness on the baseline outcome (defining who was at risk of TB), missingness on the outcome at follow-up (capturing who acquired TB), and clustering within households. Application to real data from SEARCH-TB highlighted the real-world consequences of the discussed methods. Estimates from TMLE suggested that alcohol use was associated with a 49% increase in the relative risk (RR) of incident TB infection (RR=1.49, 95%CI: 1.39-1.59). These estimates were notably larger and more precise than estimates from inverse probability weighting (RR=1.13, 95%CI: 1.00-1.27) and unadjusted, complete case analyses (RR=1.18, 95%CI: 0.89-1.57). Our work demonstrates the utility of causal models for describing the missing data mechanism and TMLE for flexible inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03336v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kirsten E. Landsiedel, Rachel Abbott, Atukunda Mucunguzi, Florence Mwangwa, Elijah Kakande, Edwin D. Charlebois, Carina Marquez, Moses R. Kamya, Laura B. Balzer</dc:creator>
    </item>
    <item>
      <title>Constructing g-computation estimators: two case studies in selection bias</title>
      <link>https://arxiv.org/abs/2506.03347</link>
      <description>arXiv:2506.03347v1 Announce Type: new 
Abstract: G-computation is a useful estimation method that can be adapted to address various biases in epidemiology. However, these adaptations may not be obvious for some complex causal structures. This challenge is an example of the much wider issue of translating a causal diagram into a novel estimation strategy. To highlight these challenges, we consider two recent cases from the selection bias literature: treatment-induced selection and co-occurrence of biases that lack a joint adjustment set. For each case study, we show how g-computation can be adapted, described how to implement that adaptation, show some general statistical properties, and illustrate the estimator using simulation. To simplify both the theoretical study and practical application of our estimators, we express the proposed g-computation estimators as stacked estimating equations. These examples illustrate how epidemiologists can translate identification results into an estimation strategy and study the theoretical and finite-sample properties of a novel estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03347v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Haidong Lu</dc:creator>
    </item>
    <item>
      <title>Jointly modeling multiple endpoints for efficient treatment effect estimation in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2506.03393</link>
      <description>arXiv:2506.03393v1 Announce Type: new 
Abstract: Randomized controlled trials are the gold standard for evaluating the efficacy of an intervention. However, there is often a trade-off between selecting the most scientifically relevant primary endpoint versus a less relevant, but more powerful, endpoint. For example, in the context of tobacco regulatory science many trials evaluate cigarettes per day as the primary endpoint instead of abstinence from smoking due to limited power. Additionally, it is often of interest to consider subgroup analyses to answer additional questions; such analyses are rarely adequately powered. In practice, trials often collect multiple endpoints. Heuristically, if multiple endpoints demonstrate a similar treatment effect we would be more confident in the results of this trial. However, there is limited research on leveraging information from secondary endpoints besides using composite endpoints which can be difficult to interpret. In this paper, we develop an estimator for the treatment effect on the primary endpoint based on a joint model for primary and secondary efficacy endpoints. This estimator gains efficiency over the standard treatment effect estimator when the model is correctly specified but is robust to model misspecification via model averaging. We illustrate our approach by estimating the effect of very low nicotine content cigarettes on the proportion of Black people who smoke who achieve abstinence and find our approach reduces the standard error by 27%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03393v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jack M. Wolf, Joseph S. Koopmeiners, David M. Vock</dc:creator>
    </item>
    <item>
      <title>Online multi-layer FDR control</title>
      <link>https://arxiv.org/abs/2506.03406</link>
      <description>arXiv:2506.03406v1 Announce Type: new 
Abstract: When hypotheses are tested in a stream and real-time decision-making is needed, online sequential hypothesis testing procedures are needed. Furthermore, these hypotheses are commonly partitioned into groups by their nature. For example, the RNA nanocapsules can be partitioned based on therapeutic nucleic acids (siRNAs) being used, as well as the delivery nanocapsules. When selecting effective RNA nanocapsules, simultaneous false discovery rate control at multiple partition levels is needed. In this paper, we develop hypothesis testing procedures which controls false discovery rate (FDR) simultaneously for multiple partitions of hypotheses in an online fashion. We provide rigorous proofs on their FDR or modified FDR (mFDR) control properties and use extensive simulations to demonstrate their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03406v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runqiu Wang, Ran Dai</dc:creator>
    </item>
    <item>
      <title>ROC Curves for Spatial Point Patterns and Presence-Absence Data</title>
      <link>https://arxiv.org/abs/2506.03414</link>
      <description>arXiv:2506.03414v1 Announce Type: new 
Abstract: Receiver Operating Characteristic (ROC) curves have recently been used to evaluate the performance of models for spatial presence-absence or presence-only data. Applications include species distribution modelling and mineral prospectivity analysis. We clarify the interpretation of the ROC curve in this context. Contrary to statements in the literature, ROC does not measure goodness-of-fit of a spatial model, and its interpretation as a measure of predictive ability is weak; it is a measure of ranking ability, insensitive to the precise form of the model. To gain insight we draw connections between ROC and existing statistical techniques for spatial point pattern data. The area under the ROC curve (AUC) is related to hypothesis tests of the null hypothesis that the explanatory variables have no effect. The shape of the ROC curve has a diagnostic interpretation. This suggests several new techniques, which extend the scope of application of ROC curves for spatial data, to support variable selection and model selection, analysis of segregation between different types of points, adjustment for a baseline, and analysis of spatial case-control data. The new techniques are illustrated with several real example datasets. Open source R code implementing the techniques is available in the development version of our package spatstat [Baddeley and Turner, 2005, Baddeley et al., 2015] and will be included in the next public release.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03414v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Baddeley, Ege Rubak, Suman Rakshit, Gopalan Nair</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood for Logistic Regression Model with Incomplete and Hybrid-Type Covariates</title>
      <link>https://arxiv.org/abs/2506.03445</link>
      <description>arXiv:2506.03445v1 Announce Type: new 
Abstract: Logistic regression is a fundamental and widely used statistical method for modeling binary outcomes based on covariates. However, the presence of missing data, particularly in settings involving hybrid covariates (a mix of discrete and continuous variables), poses significant challenges. In this paper, we propose a novel Expectation-Maximization based algorithm tailored for parameter estimation in logistic regression models with missing hybrid covariates. The proposed method is specifically designed to handle these complexities, delivering efficient parameter estimates. Through comprehensive simulations and real-world application, we demonstrate that our approach consistently outperforms traditional methods, achieving superior accuracy and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03445v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohamed Cherifi, Xujia Zhu, Mohammed Nabil El Korso, Ammar Mesloub</dc:creator>
    </item>
    <item>
      <title>Robust domain selection for functional data via interval-wise testing and effect size mapping</title>
      <link>https://arxiv.org/abs/2506.03462</link>
      <description>arXiv:2506.03462v1 Announce Type: new 
Abstract: Among inferential problems in functional data analysis, domain selection is one of the practical interests aiming to identify sub-interval(s) of the domain where desired functional features are displayed. Motivated by applications in quantitative ultrasound signal analysis, we propose the robust domain selection method, particularly aiming to discover a subset of the domain presenting distinct behaviors on location parameters among different groups. By extending the interval testing approach, we propose to take into account multiple aspects of functional features simultaneously to detect the practically interpretable domain. To further handle potential outliers and missing segments on collected functional trajectories, we perform interval testing with a test statistic based on functional M-estimators for the inference. In addition, we introduce the effect size heatmap by calculating robustified effect sizes from the lowest to the largest scales over the domain to reflect dynamic functional behaviors among groups so that clinicians get a comprehensive understanding and select practically meaningful sub-interval(s). The performance of the proposed method is demonstrated through simulation studies and an application to motivating quantitative ultrasound measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03462v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeonjoo Park, Aiguo Han</dc:creator>
    </item>
    <item>
      <title>Mosaic inference on panel data</title>
      <link>https://arxiv.org/abs/2506.03599</link>
      <description>arXiv:2506.03599v1 Announce Type: new 
Abstract: Analysis of panel data via linear regression is widespread across disciplines. To perform statistical inference, such analyses typically assume that clusters of observations are jointly independent. For example, one might assume that observations in New York are independent of observations in New Jersey. Are such assumptions plausible? Might there be hidden dependencies between nearby clusters? This paper introduces a mosaic permutation test that can (i) test the cluster-independence assumption and (ii) produce confidence intervals for linear models without assuming the full cluster-independence assumption. The key idea behind our method is to apply a permutation test to carefully constructed residual estimates that obey the same invariances as the true errors. As a result, our method yields finite-sample valid inferences under a mild "local exchangeability" condition. This condition differs from the typical cluster-independence assumption, as neither assumption implies the other. Furthermore, our method is asymptotically valid under cluster-independence (with no exchangeability assumptions). Together, these results show our method is valid under assumptions that are arguably weaker than the assumptions underlying many classical methods. In experiments on well-studied datasets from the literature, we find that many existing methods produce variance estimates that are up to five times too small, whereas mosaic methods produce reliable results. We implement our methods in the python package mosaicperm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03599v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Asher Spector, Rina Foygel Barber, Emmanuel Cand\`es</dc:creator>
    </item>
    <item>
      <title>Geometric standardized mean difference and its application to meta-analysis</title>
      <link>https://arxiv.org/abs/2506.03639</link>
      <description>arXiv:2506.03639v1 Announce Type: new 
Abstract: The standardized mean difference (SMD) is a widely used measure of effect size, particularly common in psychology, clinical trials, and meta-analysis involving continuous outcomes. Traditionally, under the equal variance assumption, the SMD is defined as the mean difference divided by a common standard deviation. This approach is prevalent in meta-analysis but can be overly restrictive in clinical practice. To accommodate unequal variances, the conventional method averages the two variances arithmetically, which does not allow for an unbiased estimation of the SMD. Inspired by this, we propose a geometric approach to averaging the variances, resulting in a novel measure for standardizing the mean difference with unequal variances. We further propose the Cohen-type and Hedges-type estimators for the new SMD, and derive their statistical properties including the confidence intervals. Simulation results show that the Hedges-type estimator performs optimally across various scenarios, demonstrating lower bias, lower mean squared error, and improved coverage probability. A real-world meta-analysis also illustrates that our new SMD and its estimators provide valuable insights to the existing literature and can be highly recommended for practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03639v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiandong Shi, Xiaochen Zhang, Lu Lin, Hiu Yee Kwan, Tiejun Tong</dc:creator>
    </item>
    <item>
      <title>Adaptive Grid Designs for Classifying Monotonic Binary Deterministic Computer Simulations</title>
      <link>https://arxiv.org/abs/2506.03815</link>
      <description>arXiv:2506.03815v1 Announce Type: new 
Abstract: This research is motivated by the need for effective classification in ice-breaking dynamic simulations, aimed at determining the conditions under which an underwater vehicle will break through the ice. This simulation is extremely time-consuming and yields deterministic, binary, and monotonic outcomes. Detecting the critical edge between the negative-outcome and positive-outcome regions with minimal simulation runs necessitates an efficient experimental design for selecting input values. In this paper, we derive lower bounds on the number of functional evaluations needed to ensure a certain level of classification accuracy for arbitrary static and adaptive designs. We also propose a new class of adaptive designs called adaptive grid designs, which are sequences of grids with increasing resolution such that lower resolution grids are proper subsets of higher resolution grids. By prioritizing simulation runs at lower resolution points and skipping redundant runs, adaptive grid designs require the same order of magnitude of runs as the best possible adaptive design, which is an order of magnitude fewer than the best possible static design. Numerical results across test functions, the road crash simulation and the ice-breaking simulation validate the superiority of adaptive grid designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03815v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tian Bai, Dianpeng Wang, Kuangqi Chen, Xu He</dc:creator>
    </item>
    <item>
      <title>The Causal-Noncausal Tail Processes: An Introduction</title>
      <link>https://arxiv.org/abs/2506.04046</link>
      <description>arXiv:2506.04046v1 Announce Type: new 
Abstract: This paper considers one-dimensional mixed causal/noncausal autoregressive (MAR) processes with heavy tail, usually introduced to model trajectories with patterns including asymmetric peaks and throughs, speculative bubbles, flash crashes, or jumps. We especially focus on the extremal behaviour of these processes when at a given date the process is above a large threshold and emphasize the roles of pure causal and noncausal components of the tail process. We provide the dynamic of the tail process and explain how it can be updated during the life of a speculative bubble. In particular we discuss the prediction of the turning point(s) and introduce pure residual plots as a diagnostic for the bubble episodes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04046v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Gouri\'eroux, Yang Lu, Christian-Yann Robert</dc:creator>
    </item>
    <item>
      <title>Leveraging External Data for Testing Experimental Therapies with Biomarker Interactions in Randomized Clinical Trials</title>
      <link>https://arxiv.org/abs/2506.04128</link>
      <description>arXiv:2506.04128v1 Announce Type: new 
Abstract: In oncology the efficacy of novel therapeutics often differs across patient subgroups, and these variations are difficult to predict during the initial phases of the drug development process. The relation between the power of randomized clinical trials and heterogeneous treatment effects has been discussed by several authors. In particular, false negative results are likely to occur when the treatment effects concentrate in a subpopulation but the study design did not account for potential heterogeneous treatment effects. The use of external data from completed clinical studies and electronic health records has the potential to improve decision-making throughout the development of new therapeutics, from early-stage trials to registration. Here we discuss the use of external data to evaluate experimental treatments with potential heterogeneous treatment effects. We introduce a permutation procedure to test, at the completion of a randomized clinical trial, the null hypothesis that the experimental therapy does not improve the primary outcomes in any subpopulation. The permutation test leverages the available external data to increase power. Also, the procedure controls the false positive rate at the desired $\alpha$-level without restrictive assumptions on the external data, for example, in scenarios with unmeasured confounders, different pre-treatment patient profiles in the trial population compared to the external data, and other discrepancies between the trial and the external data. We illustrate that the permutation test is optimal according to an interpretable criteria and discuss examples based on asymptotic results and simulations, followed by a retrospective analysis of individual patient-level data from a collection of glioblastoma clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04128v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Boyu Ren, Federico Ferrari, Sandra Fortini, Steffen Ventz, Lorenzo Trippa</dc:creator>
    </item>
    <item>
      <title>Bayes Error Rate Estimation in Difficult Situations</title>
      <link>https://arxiv.org/abs/2506.03159</link>
      <description>arXiv:2506.03159v1 Announce Type: cross 
Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable generalizable classification accuracy of any machine learning model due to inherent uncertainty within the data. BER estimators offer insight into the difficulty of any classification problem and set expectations for optimal classification performance. In order to be useful, the estimators must also be accurate with a limited number of samples on multivariate problems with unknown class distributions. To determine which estimators meet the minimum requirements for "usefulness", an in-depth examination of their accuracy is conducted using Monte Carlo simulations with synthetic data in order to obtain their confidence bounds for binary classification. To examine the usability of the estimators on real-world applications, new test scenarios are introduced upon which 2500 Monte Carlo simulations per scenario are run over a wide range of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques, results show that kNN is overwhelmingly the more accurate non-parametric estimator. In order to reach the target of an under 5 percent range for the 95 percent confidence bounds, the minimum number of required samples per class is 1000. As more features are added, more samples are needed, so that 2500 samples per class are required at only 4 features. Other estimators do become more accurate than kNN as more features are added, but continuously fail to meet the target range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03159v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi</dc:creator>
    </item>
    <item>
      <title>Causal Discovery in Dynamic Fading Wireless Networks</title>
      <link>https://arxiv.org/abs/2506.03163</link>
      <description>arXiv:2506.03163v1 Announce Type: cross 
Abstract: Dynamic causal discovery in wireless networks is essential due to evolving interference, fading, and mobility, which complicate traditional static causal models. This paper addresses causal inference challenges in dynamic fading wireless environments by proposing a sequential regression-based algorithm with a novel application of the NOTEARS acyclicity constraint, enabling efficient online updates. We derive theoretical lower and upper bounds on the detection delay required to identify structural changes, explicitly quantifying their dependence on network size, noise variance, and fading severity. Monte Carlo simulations validate these theoretical results, demonstrating linear increases in detection delay with network size, quadratic growth with noise variance, and inverse-square dependence on the magnitude of structural changes. Our findings provide rigorous theoretical insights and practical guidelines for designing robust online causal inference mechanisms to maintain network reliability under nonstationary wireless conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03163v1</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oluwaseyi Giwa</dc:creator>
    </item>
    <item>
      <title>Large Neighborhood and Hybrid Genetic Search for Inventory Routing Problems</title>
      <link>https://arxiv.org/abs/2506.03172</link>
      <description>arXiv:2506.03172v1 Announce Type: cross 
Abstract: The inventory routing problem (IRP) focuses on jointly optimizing inventory and distribution operations from a supplier to retailers over multiple days. Compared to other problems from the vehicle routing family, the interrelations between inventory and routing decisions render IRP optimization more challenging and call for advanced solution techniques. A few studies have focused on developing large neighborhood search approaches for this class of problems, but this remains a research area with vast possibilities due to the challenges related to the integration of inventory and routing decisions. In this study, we advance this research area by developing a new large neighborhood search operator tailored for the IRP. Specifically, the operator optimally removes and reinserts all visits to a specific retailer while minimizing routing and inventory costs. We propose an efficient tailored dynamic programming algorithm that exploits preprocessing and acceleration strategies. The operator is used to build an effective local search routine, and included in a state-of-the-art routing algorithm, i.e., Hybrid Genetic Search (HGS). Through extensive computational experiments, we demonstrate that the resulting heuristic algorithm leads to solutions of unmatched quality up to this date, especially on large-scale benchmark instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03172v1</guid>
      <category>cs.NE</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyi Zhao, Claudia Archetti, Tuan Anh Pham, Thibaut Vidal</dc:creator>
    </item>
    <item>
      <title>Probabilistic Factorial Experimental Design for Combinatorial Interventions</title>
      <link>https://arxiv.org/abs/2506.03363</link>
      <description>arXiv:2506.03363v1 Announce Type: cross 
Abstract: A combinatorial intervention, consisting of multiple treatments applied to a single unit with potentially interactive effects, has substantial applications in fields such as biomedicine, engineering, and beyond. Given $p$ possible treatments, conducting all possible $2^p$ combinatorial interventions can be laborious and quickly becomes infeasible as $p$ increases. Here we introduce probabilistic factorial experimental design, formalized from how scientists perform lab experiments. In this framework, the experimenter selects a dosage for each possible treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. Additionally, the experimenter can carry out such experiments over multiple rounds, adapting the design in an active manner. We address the optimal experimental design problem within an intervention model that imposes bounded-degree interactions between treatments. In the passive setting, we provide a closed-form solution for the near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating any $k$-way interaction model, regardless of $k$, and imply that $O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate this model. For the multi-round setting, we provide a near-optimal acquisition function that can be numerically optimized. We also explore several extensions of the design problem and finally validate our findings through simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03363v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Shyamal, Jiaqi Zhang, Caroline Uhler</dc:creator>
    </item>
    <item>
      <title>Differentially Private Distribution Release of Gaussian Mixture Models via KL-Divergence Minimization</title>
      <link>https://arxiv.org/abs/2506.03467</link>
      <description>arXiv:2506.03467v1 Announce Type: cross 
Abstract: Gaussian Mixture Models (GMMs) are widely used statistical models for representing multi-modal data distributions, with numerous applications in data mining, pattern recognition, data simulation, and machine learning. However, recent research has shown that releasing GMM parameters poses significant privacy risks, potentially exposing sensitive information about the underlying data. In this paper, we address the challenge of releasing GMM parameters while ensuring differential privacy (DP) guarantees. Specifically, we focus on the privacy protection of mixture weights, component means, and covariance matrices. We propose to use Kullback-Leibler (KL) divergence as a utility metric to assess the accuracy of the released GMM, as it captures the joint impact of noise perturbation on all the model parameters. To achieve privacy, we introduce a DP mechanism that adds carefully calibrated random perturbations to the GMM parameters. Through theoretical analysis, we quantify the effects of privacy budget allocation and perturbation statistics on the DP guarantee, and derive a tractable expression for evaluating KL divergence. We formulate and solve an optimization problem to minimize the KL divergence between the released and original models, subject to a given $(\epsilon, \delta)$-DP constraint. Extensive experiments on both synthetic and real-world datasets demonstrate that our approach achieves strong privacy guarantees while maintaining high utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03467v1</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Liu, Anna Scaglione, Sean Peisert</dc:creator>
    </item>
    <item>
      <title>Adaptive tuning of Hamiltonian Monte Carlo methods</title>
      <link>https://arxiv.org/abs/2506.04082</link>
      <description>arXiv:2506.04082v1 Announce Type: cross 
Abstract: With the recently increased interest in probabilistic models, the efficiency of an underlying sampler becomes a crucial consideration. A Hamiltonian Monte Carlo (HMC) sampler is one popular option for models of this kind. Performance of HMC, however, strongly relies on a choice of parameters associated with an integration method for Hamiltonian equations, which up to date remains mainly heuristic or introduce time complexity. We propose a novel computationally inexpensive and flexible approach (we call it Adaptive Tuning or ATune) that, by analyzing the data generated during a burning stage of an HMC simulation, detects a system specific splitting integrator with a set of reliable HMC hyperparameters, including their credible randomization intervals, to be readily used in a production simulation. The method automatically eliminates those values of simulation parameters which could cause undesired extreme scenarios, such as resonance artifacts, low accuracy or poor sampling. The new approach is implemented in the in-house software package \textsf{HaiCS}, with no computational overheads introduced in a production simulation, and can be easily incorporated in any package for Bayesian inference with HMC. The tests on popular statistical models using original HMC and generalized Hamiltonian Monte Carlo (GHMC) reveal the superiority of adaptively tuned methods in terms of stability, performance and accuracy over conventional HMC tuned heuristically and coupled with the well-established integrators. We also claim that the generalized formulation of HMC, i.e. GHMC, is preferable for achieving high sampling performance. The efficiency of the new methodology is assessed in comparison with state-of-the-art samplers, e.g. the No-U-Turn-Sampler (NUTS), in real-world applications, such as endocrine therapy resistance in cancer, modeling of cell-cell adhesion dynamics and influenza epidemic outbreak.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04082v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elena Akhmatskaya, Lorenzo Nagar, Jose Antonio Carrillo, Leonardo Gavira Balmacz, Hristo Inouzhe, Mart\'in Parga Pazos, Mar\'ia Xos\'e Rodr\'iguez \'Alvarez</dc:creator>
    </item>
    <item>
      <title>What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness</title>
      <link>https://arxiv.org/abs/2506.04194</link>
      <description>arXiv:2506.04194v1 Announce Type: cross 
Abstract: Most of the widely used estimators of the average treatment effect (ATE) in causal inference rely on the assumptions of unconfoundedness and overlap. Unconfoundedness requires that the observed covariates account for all correlations between the outcome and treatment. Overlap requires the existence of randomness in treatment decisions for all individuals. Nevertheless, many types of studies frequently violate unconfoundedness or overlap, for instance, observational studies with deterministic treatment decisions -- popularly known as Regression Discontinuity designs -- violate overlap.
  In this paper, we initiate the study of general conditions that enable the identification of the average treatment effect, extending beyond unconfoundedness and overlap. In particular, following the paradigm of statistical learning theory, we provide an interpretable condition that is sufficient and nearly necessary for the identification of ATE. Moreover, this condition characterizes the identification of the average treatment effect on the treated (ATT) and can be used to characterize other treatment effects as well. To illustrate the utility of our condition, we present several well-studied scenarios where our condition is satisfied and, hence, we prove that ATE can be identified in regimes that prior works could not capture. For example, under mild assumptions on the data distributions, this holds for the models proposed by Tan (2006) and Rosenbaum (2002), and the Regression Discontinuity design model introduced by Thistlethwaite and Campbell (1960). For each of these scenarios, we also show that, under natural additional assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic insights and causal inference methodologies, particularly in observational studies with complex treatment mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04194v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cai, Alkis Kalavasis, Katerina Mamali, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Similarity-based Random Partition Distribution for Clustering Functional Data</title>
      <link>https://arxiv.org/abs/2308.01704</link>
      <description>arXiv:2308.01704v5 Announce Type: replace 
Abstract: Random partition distribution is a crucial tool for model-based clustering. This study advances the field of random partition in the context of functional spatial data, focusing on the challenges posed by hourly population data across various regions and dates. We propose an extension of the generalized Dirichlet process, named the similarity-based generalized Dirichlet process (SGDP)-type distribution, to address the limitations of simple random partition distributions (e.g., those induced by the Dirichlet process), such as an overabundance of clusters. This model prevents excess cluster production and incorporates pairwise similarity information to ensure accurate and meaningful clustering. The theoretical properties of the SGDP-type distribution are studied. Then, SGDP-type random partition is applied to a real-world dataset of hourly population flow in $500\text{m}$ meshes in the central part of Tokyo. In this empirical context, our method excels at detecting meaningful patterns in the data while accounting for spatial nuances. The results underscore the adaptability and utility of the method, showcasing its prowess in revealing intricate spatiotemporal dynamics. The proposed random partition will significantly contribute to urban planning, transportation, and policy-making and will be a helpful tool for understanding population dynamics and their implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01704v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoya Wakayama, Shonosuke Sugasawa, Genya Kobayashi</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Rerandomization using Quadratic Forms</title>
      <link>https://arxiv.org/abs/2403.12815</link>
      <description>arXiv:2403.12815v2 Announce Type: replace 
Abstract: When designing a randomized experiment, one way to ensure treatment and control groups exhibit similar covariate distributions is to randomize treatment until some prespecified level of covariate balance is satisfied; this strategy is known as rerandomization. Most rerandomization methods utilize balance metrics based on a quadratic form $\mathbf{v}^T \mathbf{A} \mathbf{v}$, where $\mathbf{v}$ is a vector of covariate mean differences and $\mathbf{A}$ is a positive semi-definite matrix. In this work, we derive general results for treatment-versus-control rerandomization schemes that employ quadratic forms for covariate balance. In addition to allowing researchers to quickly derive properties of rerandomization schemes not previously considered, our theoretical results provide guidance on how to choose $\mathbf{A}$ in practice. We find the Mahalanobis and Euclidean distances optimize different measures of covariate balance. Furthermore, we establish how the covariates' eigenstructure and their relationship to the outcomes dictates which matrix $\mathbf{A}$ yields the most precise difference-in-means estimator for the average treatment effect. We find the Euclidean distance is minimax optimal, in the sense that the difference-in-means estimator's precision is never too far from the optimal choice. We verify our theoretical results via simulation and a real data application, and demonstrate how the choice of $\mathbf{A}$ impacts the variance reduction of rerandomized experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12815v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Schindl, Zach Branson</dc:creator>
    </item>
    <item>
      <title>Adding variances of independent probability distributions to estimate probabilities of replication</title>
      <link>https://arxiv.org/abs/2403.16906</link>
      <description>arXiv:2403.16906v5 Announce Type: replace 
Abstract: If the prior probability distributions of all possible hypothetical true means and all possible observed means of a continuous variable are conditional on the universal set of all numbers (i.e., before the nature of a study is known and a Bayesian prior distribution can be estimated), that prior probability distribution will be uniform. It would follow that a Gaussian probability distribution and a Gaussian likelihood distribution based on the same data set are identical. Replication involves doing two independent studies and is thus modelled by adding the variance of the probability distribution based on the observed data to the variance of the expected probability distribution of the replicating study based on its proposed sample size. This allows an estimate to be made of the probability that the P value will be less than or equal to 0.05 two sided (or any other specified value) in any replicating study. The same model can be used to estimate sample sizes when planning the required power of the initial study. If this requires doubling the variance, then this will require double the sample size estimated using current power calculations, suggesting that studies using current methods are underpowered. These considerations might be used to explain the replication crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16906v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huw Llewelyn</dc:creator>
    </item>
    <item>
      <title>Sequential model confidence sets</title>
      <link>https://arxiv.org/abs/2404.18678</link>
      <description>arXiv:2404.18678v3 Announce Type: replace 
Abstract: In most prediction and estimation situations, scientists consider various statistical models for the same problem, and naturally want to select amongst the best. Hansen et al. (2011) provide a powerful solution to this problem by the so-called model confidence set, a subset of the original set of available models that contains the best models with a given level of confidence. Importantly, model confidence sets respect the underlying selection uncertainty by being flexible in size. However, they presuppose a fixed sample size which stands in contrast to the fact that model selection and forecast evaluation are inherently sequential tasks where we successively collect new data and where the decision to continue or conclude a study may depend on the previous outcomes. In this article, we extend model confidence sets sequentially over time by relying on sequential testing methods. Recently, e-processes and confidence sequences have been introduced as new, safe methods for assessing statistical evidence. Sequential model confidence sets allow to continuously monitor the models' performances and come with time-uniform, nonasymptotic coverage guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18678v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Arnold, Georgios Gavrilopoulos, Benedikt Schulz, Johanna Ziegel</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian optimization for Likelihood-Free inference in sequential sampling models of decision making</title>
      <link>https://arxiv.org/abs/2409.01735</link>
      <description>arXiv:2409.01735v4 Announce Type: replace 
Abstract: Statistical models are often defined by a generative process for simulating synthetic data, but this can lead to intractable likelihoods. Likelihood free inference (LFI) methods enable Bayesian inference to be performed in this case. Extending a popular approach to simulation-efficient LFI for single-source data, we propose Multi-objective Bayesian Optimization for Likelihood Free Inference (MOBOLFI) to perform LFI using multi-source data. MOBOLFI models a multi-dimensional discrepancy between observed and simulated data, using a separate discrepancy for each data source. The use of a multivariate discrepancy allows for approximations to individual data source likelihoods in addition to the joint likelihood, enabling detection of conflicting information and deeper understanding of the importance of different data sources in estimating individual parameters. The adaptive choice of simulation parameters using multi-objective Bayesian optimization ensures simulation efficient approximation of likelihood components for all data sources. We illustrate our approach in sequential sampling models (SSMs), which are widely used in psychology and consumer-behavior modeling. SSMs are often fitted using multi-source data, such as choice and response time. The advantages of our approach are illustrated in comparison with a single discrepancy for an SSM fitted to data assessing preferences of ride-hailing drivers in Singapore to rent electric vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01735v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Chen, Xinwei Li, Eui-Jin Kim, Prateek Bansal, David Nott</dc:creator>
    </item>
    <item>
      <title>On the Foundations of the Design-Based Approach</title>
      <link>https://arxiv.org/abs/2505.10519</link>
      <description>arXiv:2505.10519v2 Announce Type: replace 
Abstract: The design-based paradigm may be adopted in causal inference and survey sampling when we assume Rubin's stable unit treatment value assumption (SUTVA) or impose similar frameworks. While often taken for granted, such assumptions entail strong claims about the data generating process. We develop an alternative design-based approach: we first invoke a generalized, non-parametric model that allows for unrestricted forms of interference, such as spillover. We define a new set of inferential targets and discuss their interpretation under SUTVA and a weaker assumption that we call the No Unmodeled Revealable Variation Assumption (NURVA). We then reconstruct the standard paradigm, reconsidering SUTVA at the end rather than assuming it at the beginning. Despite its similarity to SUTVA, we demonstrate the practical insufficiency of NURVA for identifying substantively interesting quantities. In so doing, we provide clarity on the nature and importance of SUTVA for applied research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10519v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>P. M. Aronow, Austin Jang, Molly Offer-Westort</dc:creator>
    </item>
    <item>
      <title>Recover Experimental Data with Selection Bias using Counterfactual Logic</title>
      <link>https://arxiv.org/abs/2506.00335</link>
      <description>arXiv:2506.00335v2 Announce Type: replace 
Abstract: Selection bias, arising from the systematic inclusion or exclusion of certain samples, poses a significant challenge to the validity of causal inference. While Bareinboim et al. introduced methods for recovering unbiased observational and interventional distributions from biased data using partial external information, the complexity of the backdoor adjustment and the method's strong reliance on observational data limit its applicability in many practical settings. In this paper, we formally discover the recoverability of $P(Y^*_{x^*})$ under selection bias with experimental data. By explicitly constructing counterfactual worlds via Structural Causal Models (SCMs), we analyze how selection mechanisms in the observational world propagate to the counterfactual domain. We derive a complete set of graphical and theoretical criteria to determine that the experimental distribution remain unaffected by selection bias. Furthermore, we propose principled methods for leveraging partially unbiased observational data to recover $P(Y^*_{x^*})$ from biased experimental datasets. Simulation studies replicating realistic research scenarios demonstrate the practical utility of our approach, offering concrete guidance for mitigating selection bias in applied causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00335v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang He, Shuai Wang, Ang Li</dc:creator>
    </item>
    <item>
      <title>Robust and Agnostic Learning of Conditional Distributional Treatment Effects</title>
      <link>https://arxiv.org/abs/2205.11486</link>
      <description>arXiv:2205.11486v3 Announce Type: replace-cross 
Abstract: The conditional average treatment effect (CATE) is the best measure of individual causal effects given baseline covariates. However, the CATE only captures the (conditional) average, and can overlook risks and tail events, which are important to treatment choice. In aggregate analyses, this is usually addressed by measuring the distributional treatment effect (DTE), such as differences in quantiles or tail expectations between treatment groups. Hypothetically, one can similarly fit conditional quantile regressions in each treatment group and take their difference, but this would not be robust to misspecification or provide agnostic best-in-class predictions. We provide a new robust and model-agnostic methodology for learning the conditional DTE (CDTE) for a class of problems that includes conditional quantile treatment effects, conditional super-quantile treatment effects, and conditional treatment effects on coherent risk measures given by $f$-divergences. Our method is based on constructing a special pseudo-outcome and regressing it on covariates using any regression learner. Our method is model-agnostic in that it can provide the best projection of CDTE onto the regression model class. Our method is robust in that even if we learn these nuisances nonparametrically at very slow rates, we can still learn CDTEs at rates that depend on the class complexity and even conduct inferences on linear projections of CDTEs. We investigate the behavior of our proposal in simulations, as well as in a case study of 401(k) eligibility effects on wealth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.11486v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>PMLR 206 (2023) 6037-6060</arxiv:journal_reference>
      <dc:creator>Nathan Kallus, Miruna Oprescu</dc:creator>
    </item>
    <item>
      <title>Joint Spectral Clustering in Multilayer Degree-Corrected Stochastic Blockmodels</title>
      <link>https://arxiv.org/abs/2212.05053</link>
      <description>arXiv:2212.05053v3 Announce Type: replace-cross 
Abstract: Modern network datasets are often composed of multiple layers, either as different views, time-varying observations, or independent sample units, resulting in collections of networks over the same set of vertices but with potentially different connectivity patterns on each network. These data require models and methods that are flexible enough to capture local and global differences across the networks, while at the same time being parsimonious and tractable to yield computationally efficient and theoretically sound solutions that are capable of aggregating information across the networks. This paper considers the multilayer degree-corrected stochastic blockmodel, where a collection of networks share the same community structure, but degree-corrections and block connection probability matrices are permitted to be different. We establish the identifiability of this model and propose a spectral clustering algorithm for community detection in this setting. Our theoretical results demonstrate that the misclustering error rate of the algorithm improves exponentially with multiple network realizations, even in the presence of significant layer heterogeneity with respect to degree corrections, signal strength, and spectral properties of the block connection probability matrices. Simulation studies show that this approach improves on existing multilayer community detection methods in this challenging regime. Furthermore, in a case study of US airport data through January 2016 -- September 2021, we find that this methodology identifies meaningful community structure and trends in airport popularity influenced by pandemic impacts on travel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05053v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Agterberg, Zachary Lubberts, Jes\'us Arroyo</dc:creator>
    </item>
    <item>
      <title>A Novel Criterion for Interpreting Acoustic Emission Damage Signals Based on Cluster Onset Distribution</title>
      <link>https://arxiv.org/abs/2312.13416</link>
      <description>arXiv:2312.13416v2 Announce Type: replace-cross 
Abstract: Structural health monitoring (SHM) relies on non-destructive techniques such as acoustic emission (AE) that generate large amounts of data over the lifespan of systems. Clustering methods are used to interpret these data and gain insights into damage progression and mechanisms. Conventional methods for evaluating clustering results utilise clustering validity indices (CVI) that prioritise compact and separable clusters. This paper introduces a novel approach based on the temporal sequence of cluster onsets, indicating the initial appearance of potential damage and allowing for early detection of defect initiation. The proposed CVI is based on the Kullback-Leibler divergence and can incorporate prior information about damage onsets when available. Three experiments on real-world datasets validate the effectiveness of the proposed method. The first benchmark focuses on detecting the loosening of bolted plates under vibration, where the onset-based CVI outperforms the conventional approach in both cluster quality and the accuracy of bolt loosening detection. The results demonstrate not only superior cluster quality but also unmatched precision in identifying cluster onsets, whether during uniform or accelerated damage growth. The two additional applications stem from industrial contexts. The first focuses on micro-drilling of hard materials using electrical discharge machining, demonstrating, for the first time, that the proposed criterion can effectively retrieve electrode progression to the reference depth, thus validating the setting of the machine to ensure structural integrity. The final application involves damage understanding in a composite/metal hybrid joint structure, where the cluster timeline is used to establish a scenario leading to critical failure due to slippage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13416v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emmanuel Ramasso, Martin Mbarga Nkogo, Neha Chandarana, Gilles Bourbon, Patrice Le Moal, Quentin Lefebvre, Martial Personeni, Constantinos Soutis, Matthieu Gresil</dc:creator>
    </item>
    <item>
      <title>Adaptive Robust Confidence Intervals</title>
      <link>https://arxiv.org/abs/2410.22647</link>
      <description>arXiv:2410.22647v2 Announce Type: replace-cross 
Abstract: This paper studies the construction of adaptive confidence intervals under Huber's contamination model when the contamination proportion is unknown. For the robust confidence interval of a Gaussian mean, we show that the optimal length of an adaptive interval must be exponentially wider than that of a non-adaptive one. An optimal construction is achieved through simultaneous uncertainty quantification of quantiles at all levels. The results are further extended beyond the Gaussian location model by addressing a general family of robust hypothesis testing. In contrast to adaptive robust estimation, our findings reveal that the optimal length of an adaptive robust confidence interval critically depends on the distribution's shape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22647v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuetian Luo, Chao Gao</dc:creator>
    </item>
    <item>
      <title>Conditional Independence Test Based on Transport Maps</title>
      <link>https://arxiv.org/abs/2504.09567</link>
      <description>arXiv:2504.09567v2 Announce Type: replace-cross 
Abstract: Testing conditional independence between two random vectors given a third is a fundamental and challenging problem in statistics, particularly in multivariate nonparametric settings due to the complexity of conditional structures. We propose a innovative framework for testing conditional independence using transport maps. At the population level, we show that two well-defined transport maps can transform the conditional independence test into an unconditional independence test, this substantially simplifies the problem. These transport maps are estimated from data using conditional continuous normalizing flow models. Within this framework, we derive a test statistic and prove its asymptotic validity under both the null and alternative hypotheses. A permutation-based procedure is employed to evaluate the significance of the test. We validate the proposed method through extensive simulations and real-data analysis. Our numerical studies demonstrate the practical effectiveness of the proposed method for conditional independence testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09567v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenxuan He, Yuan Gao, Liping Zhu, Jian Huang</dc:creator>
    </item>
  </channel>
</rss>
