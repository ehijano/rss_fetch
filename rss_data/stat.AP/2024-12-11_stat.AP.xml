<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Dec 2024 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Demand Modeling for Advanced Air Mobility</title>
      <link>https://arxiv.org/abs/2412.06807</link>
      <description>arXiv:2412.06807v1 Announce Type: new 
Abstract: In recent years, the rapid pace of urbanization has posed profound challenges globally, exacerbating environmental concerns and escalating traffic congestion in metropolitan areas. To mitigate these issues, Advanced Air Mobility (AAM) has emerged as a promising transportation alternative. However, the effective implementation of AAM requires robust demand modeling. This study delves into the demand dynamics of AAM by analyzing employment based trip data across Tennessee's census tracts, employing statistical techniques and machine learning models to enhance accuracy in demand forecasting. Drawing on datasets from the Bureau of Transportation Statistics (BTS), the Internal Revenue Service (IRS), the Federal Aviation Administration (FAA), and additional sources, we perform cost, time, and risk assessments to compute the Generalized Cost of Trip (GCT). Our findings indicate that trips are more likely to be viable for AAM if air transportation accounts for over 70\% of the GCT and the journey spans more than 250 miles. The study not only refines the understanding of AAM demand but also guides strategic planning and policy formulation for sustainable urban mobility solutions. The data and code can be accessed on GitHub.{https://github.com/lotussavy/IEEEBigData-2024.git }</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06807v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamal Acharya, Mehul Lad, Liang Sun, Houbing Song</dc:creator>
    </item>
    <item>
      <title>A Scalable Bayesian Spatiotemporal Model for Water Level Predictions using a Nearest Neighbor Gaussian Process Approach</title>
      <link>https://arxiv.org/abs/2412.06934</link>
      <description>arXiv:2412.06934v1 Announce Type: new 
Abstract: Obtaining accurate water level predictions are essential for water resource management and implementing flood mitigation strategies. Several data-driven models can be found in the literature. However, there has been limited research with regard to addressing the challenges posed by large spatio-temporally referenced hydrological datasets, in particular, the challenges of maintaining predictive performance and uncertainty quantification. Gaussian Processes (GPs) are commonly used to capture complex space-time interactions. However, GPs are computationally expensive and suffer from poor scaling as the number of locations increases due to required covariance matrix inversions. To overcome the computational bottleneck, the Nearest Neighbor Gaussian Process (NNGP) introduces a sparse precision matrix providing scalability without having to make inferential compromises. In this work we introduce an innovative model in the hydrology field, specifically designed to handle large datasets consisting of a large number of spatial points across multiple hydrological basins, with daily observations over an extended period. We investigate the application of a Bayesian spatiotemporal NNGP model to a rich dataset of daily water levels of rivers located in Ireland. The dataset comprises a network of 301 stations situated in various basins across Ireland, measured over a period of 90 days. The proposed approach allows for prediction of water levels at future time points, as well as the prediction of water levels at unobserved locations through spatial interpolation, while maintaining the benefits of the Bayesian approach, such as uncertainty propagation and quantification. Our findings demonstrate that the proposed model outperforms competing approaches in terms of accuracy and precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06934v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Hugo Nagahama, James Sweeney, Niamh Cahill</dc:creator>
    </item>
    <item>
      <title>A Bayesian Mixture Model Approach to Examining Neighborhood Social Determinants of Health Disparities in Endometrial Cancer Care in Massachusetts</title>
      <link>https://arxiv.org/abs/2412.07134</link>
      <description>arXiv:2412.07134v1 Announce Type: new 
Abstract: Many studies have examined social determinants of health (SDoH) factors independently, overlooking their interconnected and intersectional nature. Our study takes a multifactorial approach to construct a neighborhood level measure of SDoH and explores how neighborhood residency impacts care received by endometrial cancer patients in Massachusetts. We used a Bayesian multivariate Bernoulli mixture model to create and characterize neighborhood SDoH (NSDoH) profiles using the 2015-2019 American Community Survey at the census tract level (n=1478), incorporating 18 variables across four domains: housing conditions and resources, economic security, educational attainment, and social and community context. We linked these profiles to Massachusetts Cancer Registry data to estimate the odds of receiving optimal care for endometrial cancer using Bayesian multivariate logistic regression. The model identified eight NSDoH profiles. Profiles 1 and 2 accounted for 27% and 25% of census tracts, respectively. Profile 1 featured neighborhoods with high homeownership, above median incomes, and high education, while Profile 2 showed higher probabilities of limited English proficiency, renters, lower education, and working class jobs. After adjusting for sociodemographic and clinical characteristics, we found no statistically significant association between NSDoH profiles and receipt of optimal care. However, compared to patients in NSDoH Profile 1, those in Profile 2 had lower odds of receiving optimal care, OR = 0.77, 95% CI (0.56, 1.07). Our results demonstrate the interconnected and multidimensional nature of NSDoH, underscoring the importance of modeling them accordingly. This study also highlights the need for targeted interventions at the neighborhood level to address underlying drivers of health disparities, ensure equitable healthcare delivery, and foster better outcomes for all patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07134v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carmen B. Rodriguez, Stephanie M. Wu, Stephanie Alimena, Alecia J McGregor, Briana JK Stephenson</dc:creator>
    </item>
    <item>
      <title>A novel Phase I clinical trial design with unequal cohort sizes</title>
      <link>https://arxiv.org/abs/2412.07635</link>
      <description>arXiv:2412.07635v1 Announce Type: new 
Abstract: This paper introduces a new Phase I design aimed at enhancing the performance of existing methods, including algorithm-based, model-based, and model-assisted designs. The design, developed by integrating the concept of Fisher information, is easily operationalized. The new design addresses the issue of the classical designs'slow dosage escalation. Simulation demonstrate that the proposed design markedly enhances performance in terms of efficiency, accuracy, and reliability. Moreover, the trial duration has been notably reduced with a large sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07635v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojun Zhu</dc:creator>
    </item>
    <item>
      <title>Probabilistic Modelling of Multiple Long-Term Condition Onset Times</title>
      <link>https://arxiv.org/abs/2412.07657</link>
      <description>arXiv:2412.07657v1 Announce Type: new 
Abstract: The co-occurrence of multiple long-term conditions (MLTC), or multimorbidity, in an individual can reduce their lifespan and severely impact their quality of life. Exploring the longitudinal patterns, e.g. clusters, of disease accrual can help better understand the genetic and environmental drivers of multimorbidity, and potentially identify individuals who may benefit from early targeted intervention. We introduce $\textit{probabilistic modelling of onset times}$, or $\texttt{ProMOTe}$, for clustering and forecasting MLTC trajectories. $\texttt{ProMOTe}$ seamlessly learns from incomplete and unreliable disease trajectories that is commonplace in Electronic Health Records but often ignored in existing longitudinal clustering methods. We analyse data from 150,000 individuals in the UK Biobank and identify 50 clusters showing patterns of disease accrual that have also been reported by some recent studies. We further discuss the forecasting capabilities of the model given the history of disease accrual.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07657v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kieran Richards, Kelly Fleetwood, Regina Prigge, Paolo Missier, Michael Barnes, Nick J. Reynolds, Bruce Guthrie, Sohan Seth</dc:creator>
    </item>
    <item>
      <title>Theoretical and Practical Limits of Signal Strength Estimate Precision for Kolmogorov-Zurbenko Periodograms with Dynamic Smoothing</title>
      <link>https://arxiv.org/abs/2412.07735</link>
      <description>arXiv:2412.07735v1 Announce Type: new 
Abstract: This investigation establishes the theoretical and practical limits of signal strength estimate precision for Kolmogorov-Zurbenko periodograms with dynamic smoothing and compares them to those of standard log-periodograms with static smoothing. Previous research has established the sensitivity, accuracy, resolution, and robustness of Kolmogorov-Zurbenko periodograms with dynamic smoothing in estimating signal frequencies. However, the precision with which they estimate signal strength has never been evaluated. To this point, the width of the confidence interval for a signal strength estimate can serve as a criterion for assessing the precision of such estimates: the narrower the confidence interval, the more precise the estimate. The statistical background for confidence intervals of periodograms is presented, followed by candidate functions to compute and plot them when using Kolmogorov-Zurbenko periodograms with dynamic smoothing. Given an identified signal frequency, a static smoothing window and its smoothing window width can be selected such that its confidence interval is narrower and, thus, its signal strength estimate more precise, than that of dynamic smoothing windows, all while maintaining a level of frequency resolution as good as or better than that of a dynamic smoothing window. These findings suggest the need for a two-step protocol in spectral analysis: computation of a Kolmogorov-Zurbenko periodogram with dynamic smoothing to detect, identify, and separate signal frequencies, followed by computation of a Kolmogorov-Zurbenko periodogram with static smoothing to precisely estimate signal strength and compute its confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07735v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Barry Loneck, Igor Zurbenko, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>I See, Therefore I Do: Estimating Causal Effects for Image Treatments</title>
      <link>https://arxiv.org/abs/2412.06810</link>
      <description>arXiv:2412.06810v1 Announce Type: cross 
Abstract: Causal effect estimation under observational studies is challenging due to the lack of ground truth data and treatment assignment bias. Though various methods exist in literature for addressing this problem, most of them ignore multi-dimensional treatment information by considering it as scalar, either continuous or discrete. Recently, certain works have demonstrated the utility of this rich yet complex treatment information into the estimation process, resulting in better causal effect estimation. However, these works have been demonstrated on either graphs or textual treatments. There is a notable gap in existing literature in addressing higher dimensional data such as images that has a wide variety of applications. In this work, we propose a model named NICE (Network for Image treatments Causal effect Estimation), for estimating individual causal effects when treatments are images. NICE demonstrates an effective way to use the rich multidimensional information present in image treatments that helps in obtaining improved causal effect estimates. To evaluate the performance of NICE, we propose a novel semi-synthetic data simulation framework that generates potential outcomes when images serve as treatments. Empirical results on these datasets, under various setups including the zero-shot case, demonstrate that NICE significantly outperforms existing models that incorporate treatment information for causal effect estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06810v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abhinav Thorat, Ravi Kolla, Niranjan Pedanekar</dc:creator>
    </item>
    <item>
      <title>Feature Group Tabular Transformer: A Novel Approach to Traffic Crash Modeling and Causality Analysis</title>
      <link>https://arxiv.org/abs/2412.06825</link>
      <description>arXiv:2412.06825v1 Announce Type: cross 
Abstract: Reliable and interpretable traffic crash modeling is essential for understanding causality and improving road safety. This study introduces a novel approach to predicting collision types by utilizing a comprehensive dataset fused from multiple sources, including weather data, crash reports, high-resolution traffic information, pavement geometry, and facility characteristics. Central to our approach is the development of a Feature Group Tabular Transformer (FGTT) model, which organizes disparate data into meaningful feature groups, represented as tokens. These group-based tokens serve as rich semantic components, enabling effective identification of collision patterns and interpretation of causal mechanisms. The FGTT model is benchmarked against widely used tree ensemble models, including Random Forest, XGBoost, and CatBoost, demonstrating superior predictive performance. Furthermore, model interpretation reveals key influential factors, providing fresh insights into the underlying causality of distinct crash types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06825v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Lares, Hao Zhen, Jidong J. Yang</dc:creator>
    </item>
    <item>
      <title>TransitGPT: A Generative AI-based framework for interacting with GTFS data using Large Language Models</title>
      <link>https://arxiv.org/abs/2412.06831</link>
      <description>arXiv:2412.06831v1 Announce Type: cross 
Abstract: This paper introduces a framework that leverages Large Language Models (LLMs) to answer natural language queries about General Transit Feed Specification (GTFS) data. The framework is implemented in a chatbot called TransitGPT with open-source code. TransitGPT works by guiding LLMs to generate Python code that extracts and manipulates GTFS data relevant to a query, which is then executed on a server where the GTFS feed is stored. It can accomplish a wide range of tasks, including data retrieval, calculations, and interactive visualizations, without requiring users to have extensive knowledge of GTFS or programming. The LLMs that produce the code are guided entirely by prompts, without fine-tuning or access to the actual GTFS feeds. We evaluate TransitGPT using GPT-4o and Claude-3.5-Sonnet LLMs on a benchmark dataset of 100 tasks, to demonstrate its effectiveness and versatility. The results show that TransitGPT can significantly enhance the accessibility and usability of transit data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06831v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saipraneeth Devunuri, Lewis Lehe</dc:creator>
    </item>
    <item>
      <title>GRUvader: Sentiment-Informed Stock Market Prediction</title>
      <link>https://arxiv.org/abs/2412.06836</link>
      <description>arXiv:2412.06836v1 Announce Type: cross 
Abstract: Stock price prediction is challenging due to global economic instability, high volatility, and the complexity of financial markets. Hence, this study compared several machine learning algorithms for stock market prediction and further examined the influence of a sentiment analysis indicator on the prediction of stock prices. Our results were two-fold. Firstly, we used a lexicon-based sentiment analysis approach to identify sentiment features, thus evidencing the correlation between the sentiment indicator and stock price movement. Secondly, we proposed the use of GRUvader, an optimal gated recurrent unit network, for stock market prediction. Our findings suggest that stand-alone models struggled compared with AI-enhanced models. Thus, our paper makes further recommendations on latter systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06836v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/math12233801</arxiv:DOI>
      <arxiv:journal_reference>Mathematics, 12(23), 3801 (2024)</arxiv:journal_reference>
      <dc:creator>Akhila Mamillapalli, Bayode Ogunleye, Sonia Timoteo Inacio, Olamilekan Shobayo</dc:creator>
    </item>
    <item>
      <title>Innovative Sentiment Analysis and Prediction of Stock Price Using FinBERT, GPT-4 and Logistic Regression: A Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2412.06837</link>
      <description>arXiv:2412.06837v1 Announce Type: cross 
Abstract: This study explores the comparative performance of cutting-edge AI models, i.e., Finaance Bidirectional Encoder representations from Transsformers (FinBERT), Generatice Pre-trained Transformer GPT-4, and Logistic Regression, for sentiment analysis and stock index prediction using financial news and the NGX All-Share Index data label. By leveraging advanced natural language processing models like GPT-4 and FinBERT, alongside a traditional machine learning model, Logistic Regression, we aim to classify market sentiment, generate sentiment scores, and predict market price movements. This research highlights global AI advancements in stock markets, showcasing how state-of-the-art language models can contribute to understanding complex financial data. The models were assessed using metrics such as accuracy, precision, recall, F1 score, and ROC AUC. Results indicate that Logistic Regression outperformed the more computationally intensive FinBERT and predefined approach of versatile GPT-4, with an accuracy of 81.83% and a ROC AUC of 89.76%. The GPT-4 predefined approach exhibited a lower accuracy of 54.19% but demonstrated strong potential in handling complex data. FinBERT, while offering more sophisticated analysis, was resource-demanding and yielded a moderate performance. Hyperparameter optimization using Optuna and cross-validation techniques ensured the robustness of the models. This study highlights the strengths and limitations of the practical applications of AI approaches in stock market prediction and presents Logistic Regression as the most efficient model for this task, with FinBERT and GPT-4 representing emerging tools with potential for future exploration and innovation in AI-driven financial analytics</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06837v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/bdcc8110143</arxiv:DOI>
      <arxiv:journal_reference>Shobayo O., Adeyemi-Longe S., Popoola O., &amp; Ogunleye B. (2024). Innovative Sentiment Analysis and Prediction of Stock Price Using FinBERT, GPT-4 and Logistic Regression: A Data-Driven Approach. Big Data and Cognitive Computing, 8(11), 143</arxiv:journal_reference>
      <dc:creator>Olamilekan Shobayo, Sidikat Adeyemi-Longe, Olusogo Popoola, Bayode Ogunleye</dc:creator>
    </item>
    <item>
      <title>On the Consistency of Bayesian Adaptive Testing under the Rasch Model</title>
      <link>https://arxiv.org/abs/2412.07170</link>
      <description>arXiv:2412.07170v1 Announce Type: cross 
Abstract: This study establishes the consistency of Bayesian adaptive testing methods under the Rasch model, addressing a gap in the literature on their large-sample guarantees. Although Bayesian approaches are recognized for their finite-sample performance and capability to circumvent issues such as the cold-start problem; however, rigorous proofs of their asymptotic properties, particularly in non-i.i.d. structures, remain lacking. We derive conditions under which the posterior distributions of latent traits converge to the true values for a sequence of given items, and demonstrate that Bayesian estimators remain robust under the mis-specification of the prior. Our analysis then extends to adaptive item selection methods in which items are chosen endogenously during the test. Additionally, we develop a Bayesian decision-theoretical framework for the item selection problem and propose a novel selection that aligns the test process with optimal estimator performance. These theoretical results provide a foundation for Bayesian methods in adaptive testing, complementing prior evidence of their finite-sample advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07170v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hau-Hung Yang, Chia-Min Wei, Yu-Chang Chen</dc:creator>
    </item>
    <item>
      <title>Dual Random Fields and their Application to Mineral Potential Mapping</title>
      <link>https://arxiv.org/abs/2412.07488</link>
      <description>arXiv:2412.07488v1 Announce Type: cross 
Abstract: In various geosciences branches, including mineral exploration, geometallurgical characterization on established mining operations, and remote sensing, the regionalized input variables are spatially well-sampled across the domain of interest, limiting the scope of spatial uncertainty quantification procedures. In turn, response outcomes such as the mineral potential in a given region, mining throughput, metallurgical recovery, or in-situ estimations from remote satellite imagery, are usually modeled from a much-restricted subset of testing samples, collected at certain locations due to accessibility restrictions and the high acquisition costs. Our limited understanding of these functions, in terms of the multi-dimensional complexity of causalities and unnoticed dependencies on inaccessible inputs, may lead to observing changes in such functions based on their geographical location. Pooling together different response functions across the domain is critical to correctly predict outcome responses, the uncertainty associated with these inferred values, and the significance of inputs in such predictions at unexplored areas. This paper introduces the notion of a dual random field (dRF), where the response function itself is considered a regionalized variable. In this way, different established response models across the geographic domain can be considered as observations of a dRF realization, enabling the spatial inference and uncertainty assessment of both response models and their predictions. We explain how dRFs inherit all the properties from classical random fields, allowing the use of standard Gaussian simulation procedures to simulate them. These models are combined to obtain a mineral potential response, providing an example of how to rigorously integrate machine learning approaches with geostatistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07488v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>\'Alvaro I. Riquelme</dc:creator>
    </item>
    <item>
      <title>Gaussian Process with dissolution spline kernel</title>
      <link>https://arxiv.org/abs/2412.07524</link>
      <description>arXiv:2412.07524v1 Announce Type: cross 
Abstract: In-vitro dissolution testing is a critical component in the quality control of manufactured drug products. The $\mathrm{f}_2$ statistic is the standard for assessing similarity between two dissolution profiles. However, the $\mathrm{f}_2$ statistic has known limitations: it lacks an uncertainty estimate, is a discrete-time metric, and is a biased measure, calculating the differences between profiles at discrete time points. To address these limitations, we propose a Gaussian Process (GP) with a dissolution spline kernel for dissolution profile comparison. The dissolution spline kernel is a new spline kernel using logistic functions as its basis functions, enabling the GP to capture the expected monotonic increase in dissolution curves. This results in better predictions of dissolution curves. This new GP model reduces bias in the $\mathrm{f}_2$ calculation by allowing predictions to be interpolated in time between observed values, and provides uncertainty quantification. We assess the model's performance through simulations and real datasets, demonstrating its improvement over a previous GP-based model introduced for dissolution testing. We also show that the new model can be adapted to include dissolution-specific covariates. Applying the model to real ibuprofen dissolution data under various conditions, we demonstrate its ability to extrapolate curve shapes across different experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07524v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fiona Murphy, Marina Navas Bachiller, Deirdre M. D'Arcy, Alessio Benavoli</dc:creator>
    </item>
    <item>
      <title>Privacy-Preserving Customer Support: A Framework for Secure and Scalable Interactions</title>
      <link>https://arxiv.org/abs/2412.07687</link>
      <description>arXiv:2412.07687v1 Announce Type: cross 
Abstract: The growing reliance on artificial intelligence (AI) in customer support has significantly improved operational efficiency and user experience. However, traditional machine learning (ML) approaches, which require extensive local training on sensitive datasets, pose substantial privacy risks and compliance challenges with regulations like the General Data Protection Regulation (GDPR) and California Consumer Privacy Act (CCPA). Existing privacy-preserving techniques, such as anonymization, differential privacy, and federated learning, address some concerns but face limitations in utility, scalability, and complexity. This paper introduces the Privacy-Preserving Zero-Shot Learning (PP-ZSL) framework, a novel approach leveraging large language models (LLMs) in a zero-shot learning mode. Unlike conventional ML methods, PP-ZSL eliminates the need for local training on sensitive data by utilizing pre-trained LLMs to generate responses directly. The framework incorporates real-time data anonymization to redact or mask sensitive information, retrieval-augmented generation (RAG) for domain-specific query resolution, and robust post-processing to ensure compliance with regulatory standards. This combination reduces privacy risks, simplifies compliance, and enhances scalability and operational efficiency. Empirical analysis demonstrates that the PP-ZSL framework provides accurate, privacy-compliant responses while significantly lowering the costs and complexities of deploying AI-driven customer support systems. The study highlights potential applications across industries, including financial services, healthcare, e-commerce, legal support, telecommunications, and government services. By addressing the dual challenges of privacy and performance, this framework establishes a foundation for secure, efficient, and regulatory-compliant AI applications in customer interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07687v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anant Prakash Awasthi, Chandraketu Singh, Rakshit Varma, Sanchit Sharma</dc:creator>
    </item>
    <item>
      <title>A Statistical Model of Bipartite Networks: Application to Cosponsorship in the United States Senate</title>
      <link>https://arxiv.org/abs/2305.05833</link>
      <description>arXiv:2305.05833v3 Announce Type: replace 
Abstract: Many networks in political and social research are bipartite, with edges connecting exclusively across two distinct types of nodes. A common example includes cosponsorship networks, in which legislators are connected indirectly through the bills they support. Yet most existing network models are designed for unipartite networks, where edges can arise between any pair of nodes. However, using a unipartite network model to analyze bipartite networks, as often done in practice, can result in aggregation bias and artificially high-clustering -- a particularly insidious problem when studying the role groups play in network formation. To address these methodological problems, we develop a statistical model of bipartite networks theorized to be generated through group interactions by extending the popular mixed-membership stochastic blockmodel. Our model allows researchers to identify the groups of nodes, within each node type in the bipartite structure, that share common patterns of edge formation. The model also incorporates both node and dyad-level covariates as the predictors of group membership and of observed dyadic relations. We develop an efficient computational algorithm for fitting the model, and apply it to cosponsorship data from the United States Senate. We show that legislators in a Senate that was perfectly split along party lines were able to remain productive and pass major legislation by forming non-partisan, power-brokering coalitions that found common ground through their collaboration on low-stakes bills. We also find evidence for norms of reciprocity, and uncover the substantial role played by policy expertise in the formation of cosponsorships between senators and legislation. We make an open-source software package available that makes it possible for other researchers to uncover similar insights from bipartite networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.05833v3</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Adeline Lo, Santiago Olivella, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>A computational framework for integrating Predictive processes with evidence Accumulation Models (PAM)</title>
      <link>https://arxiv.org/abs/2411.13203</link>
      <description>arXiv:2411.13203v2 Announce Type: replace 
Abstract: Evidence Accumulation Models (EAMs) have been widely used to investigate speeded decision-making processes, but they have largely neglected the role of predictive processes emphasized by theories of the predictive brain. In this paper, we present the Predictive evidence Accumulation Models (PAM), a novel computational framework that integrates predictive processes into EAMs. Grounded in the "observing the observer" framework, PAM combines models of Bayesian perceptual inference, such as the Hierarchical Gaussian Filter, with three established EAMs (the Diffusion Decision Model, Lognormal Race Model, and Race Diffusion Model) to model decision-making under uncertainty. We validate PAM through parameter recovery simulations, demonstrating its accuracy and computational efficiency across various decision-making scenarios. Additionally, we provide a step-by-step tutorial using real data to illustrate PAM's application and discuss its theoretical implications. PAM represents a significant advancement in the computational modeling of decision-making, bridging the gap between predictive brain theories and EAMs, and offers a promising tool for future empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13203v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonino Visalli, Francesco Maria Calistroni, Margherita Calderan, Francesco Donnarumma, Marco Zorzi, Ettore Ambrosini</dc:creator>
    </item>
    <item>
      <title>Efficient Inference on High-Dimensional Linear Models with Missing Outcomes</title>
      <link>https://arxiv.org/abs/2309.06429</link>
      <description>arXiv:2309.06429v3 Announce Type: replace-cross 
Abstract: This paper is concerned with inference on the regression function of a high-dimensional linear model when outcomes are missing at random. We propose an estimator which combines a Lasso pilot estimate of the regression function with a bias correction term based on the weighted residuals of the Lasso regression. The weights depend on estimates of the missingness probabilities (propensity scores) and solve a convex optimization program that trades off bias and variance optimally. Provided that the propensity scores can be pointwise consistently estimated at in-sample data points, our proposed estimator for the regression function is asymptotically normal and semi-parametrically efficient among all asymptotically linear estimators. Furthermore, the proposed estimator keeps its asymptotic properties even if the propensity scores are estimated by modern machine learning techniques. We validate the finite-sample performance of the proposed estimator through comparative simulation studies and the real-world problem of inferring the stellar masses of galaxies in the Sloan Digital Sky Survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06429v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Alexander Giessing, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Mitigating Consequences of Prestige in Citations of Publications</title>
      <link>https://arxiv.org/abs/2411.05584</link>
      <description>arXiv:2411.05584v2 Announce Type: replace-cross 
Abstract: For many public research organizations, funding creation of science and maximizing scientific output is of central interest. Typically, when evaluating scientific production for funding, citations are utilized as a proxy, although these are severely influenced by factors beyond scientific impact. This study aims to mitigate the consequences of the Matthew effect in citations, where prominent authors and prestigious journals receive more citations regardless of the scientific content of the publications. To this end, the study presents an approach to predicting citations of papers based solely on observable characteristics available at the submission stage of a double-blind peer-review process. Combining classical linear models, generalized linear models and utilizing large-scale data sets on biomedical papers based on the PubMed database, the results demonstrate that it is possible to make fairly accurate predictions of citations using only observable characteristics of papers excluding information on authors and journals, thereby mitigating the Matthew effect. Thus, the outcomes have important implications for the field of scientometrics, providing a more objective method for citation prediction by relying on pre-publication variables that are immune to manipulation by authors and journals, thereby enhancing the objectivity of the evaluation process. Our approach is thus important for government agencies responsible for funding the creation of high-quality scientific content rather than perpetuating prestige.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05584v2</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Balzer, Adhen Benlahlou</dc:creator>
    </item>
    <item>
      <title>Highest Posterior Density Intervals As Analogues to Profile Likelihood Ratio Confidence Intervals for Modes of Unimodal Distributions</title>
      <link>https://arxiv.org/abs/2412.06528</link>
      <description>arXiv:2412.06528v2 Announce Type: replace-cross 
Abstract: In Bayesian statistics, the highest posterior density (HPD) interval is often used to describe properties of a posterior distribution. As a method for estimating confidence intervals (CIs), the HPD has two main desirable properties. Firstly, it is the shortest interval to have a specified coverage probability. Secondly, every point inside the HPD interval has a density greater than every point outside the interval. However, it is sometimes criticized for being transformation invariant.
  We make the case that the HPD interval is a natural analog to the frequentist profile likelihood ratio confidence interval (LRCI). First we provide background on the HPD interval as well as the Likelihood Ratio Test statistic and its inversion to generate asymptotically-correct CIs. Our main result is to show that the HPD interval has similar desirable properties as the profile LRCI, such as transformation invariance with respect to the mode for monotonic functions. We then discuss an application of the main result, an example case which compares the profile LRCI for the binomial probability parameter p with the Bayesian HPD interval for the beta distribution density function, both of which are used to estimate population proportions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06528v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. X. Venu</dc:creator>
    </item>
  </channel>
</rss>
