<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Mar 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CodeTool: Enhancing Programmatic Tool Invocation of LLMs via Process Supervision</title>
      <link>https://arxiv.org/abs/2503.20840</link>
      <description>arXiv:2503.20840v1 Announce Type: new 
Abstract: Tool invocation significantly enhances the capabilities of Large Language Models (LLMs), yet challenges persist, particularly in complex task scenarios. Current methods, such as instruction-enhanced reasoning and supervised fine-tuning, often result in unnecessarily long reasoning paths and face difficulties in verifying the correctness of intermediate steps. In this paper, we propose CodeTool, a novel framework for stepwise code generation that improves LLM tool invocation by leveraging the concise and easily verifiable nature of code. CodeTool incorporates two distinct process rewards: the On-the-spot Reward, which provides immediate feedback on the accuracy of each tool invocation, and the Latent Reward, which assesses the contribution of each step toward overall task completion. By maximizing the cumulative reward of the On-the-spot and Latend Rewards at each step, LLMs are guided to follow efficient and accurate reasoning paths. Extensive experiments on StableToolBench and RestBench-TMDB demonstrate the superiority of CodeTool over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20840v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Lu, Fanghua Ye, Jian Li, Qiang Gao, Cheng Liu, Haibo Luo, Nan Du, Xiaolong Li, Feiliang Ren</dc:creator>
    </item>
    <item>
      <title>StepGrade: Grading Programming Assignments with Context-Aware LLMs</title>
      <link>https://arxiv.org/abs/2503.20851</link>
      <description>arXiv:2503.20851v1 Announce Type: new 
Abstract: Grading programming assignments is a labor-intensive and time-consuming process that demands careful evaluation across multiple dimensions of the code. To overcome these challenges, automated grading systems are leveraged to enhance efficiency and reduce the workload on educators. Traditional automated grading systems often focus solely on correctness, failing to provide interpretable evaluations or actionable feedback for students. This study introduces StepGrade, which explores the use of Chain-of-Thought (CoT) prompting with Large Language Models (LLMs) as an innovative solution to address these challenges. Unlike regular prompting, which offers limited and surface-level outputs, CoT prompting allows the model to reason step-by-step through the interconnected grading criteria, i.e., functionality, code quality, and algorithmic efficiency, ensuring a more comprehensive and transparent evaluation. This interconnectedness necessitates the use of CoT to systematically address each criterion while considering their mutual influence. To empirically validate the efficiency of StepGrade, we conducted a case study involving 30 Python programming assignments across three difficulty levels (easy, intermediate, and advanced). The approach is validated against expert human evaluations to assess its consistency, accuracy, and fairness. Results demonstrate that CoT prompting significantly outperforms regular prompting in both grading quality and interpretability. By reducing the time and effort required for manual grading, this research demonstrates the potential of GPT-4 with CoT prompting to revolutionize programming education through scalable and pedagogically effective automated grading systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20851v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Akyash, Kimia Zamiri Azar, Hadi Mardani Kamali</dc:creator>
    </item>
    <item>
      <title>Leveraging LLMs, IDEs, and Semantic Embeddings for Automated Move Method Refactoring</title>
      <link>https://arxiv.org/abs/2503.20934</link>
      <description>arXiv:2503.20934v1 Announce Type: new 
Abstract: MOVEMETHOD is a hallmark refactoring. Despite a plethora of research tools that recommend which methods to move and where, these recommendations do not align with how expert developers perform MOVEMETHOD. Given the extensive training of Large Language Models and their reliance upon naturalness of code, they should expertly recommend which methods are misplaced in a given class and which classes are better hosts. Our formative study of 2016 LLM recommendations revealed that LLMs give expert suggestions, yet they are unreliable: up to 80% of the suggestions are hallucinations. We introduce the first LLM fully powered assistant for MOVEMETHOD refactoring that automates its whole end-to-end lifecycle, from recommendation to execution. We designed novel solutions that automatically filter LLM hallucinations using static analysis from IDEs and a novel workflow that requires LLMs to be self-consistent, critique, and rank refactoring suggestions. As MOVEMETHOD refactoring requires global, projectlevel reasoning, we solved the limited context size of LLMs by employing refactoring-aware retrieval augment generation (RAG). Our approach, MM-assist, synergistically combines the strengths of the LLM, IDE, static analysis, and semantic relevance. In our thorough, multi-methodology empirical evaluation, we compare MM-assist with the previous state-of-the-art approaches. MM-assist significantly outperforms them: (i) on a benchmark widely used by other researchers, our Recall@1 and Recall@3 show a 1.7x improvement; (ii) on a corpus of 210 recent refactorings from Open-source software, our Recall rates improve by at least 2.4x. Lastly, we conducted a user study with 30 experienced participants who used MM-assist to refactor their own code for one week. They rated 82.8% of MM-assist recommendations positively. This shows that MM-assist is both effective and useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20934v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fraol Batole, Abhiram Bellur, Malinda Dilhara, Mohammed Raihan Ullah, Yaroslav Zharov, Timofey Bryksin, Kai Ishikawa, Haifeng Chen, Masaharu Morimoto, Shota Motoura, Takeo Hosomi, Tien N. Nguyen, Hridesh Rajan, Nikolaos Tsantalis, Danny Dig</dc:creator>
    </item>
    <item>
      <title>Less Noise, More Signal: DRR for Better Optimizations of SE Tasks</title>
      <link>https://arxiv.org/abs/2503.21086</link>
      <description>arXiv:2503.21086v1 Announce Type: new 
Abstract: SE analytics problems do not always need complex AI. Better and faster solutions can sometimes be obtained by matching the complexity of the problem to the complexity of the solution. This paper introduces the Dimensionality Reduction Ratio (DRR), a new metric for predicting when lightweight algorithms suffice. Analyzing SE optimization problems from software configuration to process decisions and open-source project health we show that DRR pinpoints "simple" tasks where costly methods like DEHB (a state-of-the-art evolutionary optimizer) are overkill. For high-DRR problems, simpler methods can be just as effective and run two orders of magnitude faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21086v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andre Lustosa, Tim Menzies</dc:creator>
    </item>
    <item>
      <title>The Promise and Pitfalls of WebAssembly: Perspectives from the Industry</title>
      <link>https://arxiv.org/abs/2503.21240</link>
      <description>arXiv:2503.21240v1 Announce Type: new 
Abstract: As JavaScript has been criticized for performance and security issues in web applications, WebAssembly (Wasm) was proposed in 2017 and is regarded as the complementation for JavaScript. Due to its advantages like compact-size, native-like speed, and portability, Wasm binaries are gradually used as the compilation target for industrial projects in other high-level programming languages and are responsible for computation-intensive tasks in browsers, e.g., 3D graphic rendering and video decoding. Intuitively, characterizing in-the-wild adopted Wasm binaries from different perspectives, like their metadata, relation with source programming language, existence of security threats, and practical purpose, is the prerequisite before delving deeper into the Wasm ecosystem and beneficial to its roadmap selection. However, currently, there is no work that conducts a large-scale measurement study on in-the-wild adopted Wasm binaries. To fill this gap, we collect the largest-ever dataset to the best of our knowledge, and characterize the status quo of them from industry perspectives. According to the different roles of people engaging in the community, i.e., web developers, Wasm maintainers, and researchers, we reorganized our findings to suggestions and best practices for them accordingly. We believe this work can shed light on the future direction of the web and Wasm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21240v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ningyu He, Shangtong Cao, Haoyu Wang, Yao Guo, Xiapu Luo</dc:creator>
    </item>
    <item>
      <title>Scaling Automated Database System Testing</title>
      <link>https://arxiv.org/abs/2503.21424</link>
      <description>arXiv:2503.21424v1 Announce Type: new 
Abstract: Recently, various automated testing approaches have been proposed that use specialized test oracles to find hundreds of logic bugs in mature, widely-used Database Management Systems (DBMSs). These test oracles require database and query generators, which must account for the often significant differences between the SQL dialects of these systems. Since it can take weeks to implement such generators, many DBMS developers are unlikely to invest the time to adopt such automated testing approaches. In short, existing approaches fail to scale to the plethora of DBMSs. In this work, we present both a vision and a platform, SQLancer++, to apply test oracles to any SQL-based DBMS that supports a subset of common SQL features. Our technical core contribution is a novel architecture for an adaptive SQL statement generator. This adaptive SQL generator generates SQL statements with various features, some of which might not be supported by the given DBMS, and then learns through interaction with the DBMS, which of these are understood by the DBMS. Thus, over time, the generator will generate mostly valid SQL statements. We evaluated SQLancer++ across 17 DBMSs and discovered a total of 195 unique, previously unknown bugs, of which 180 were fixed after we reported them. While SQLancer++ is the first major step towards scaling automated DBMS testing, various follow-up challenges remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21424v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suyang Zhong, Manuel Rigger</dc:creator>
    </item>
    <item>
      <title>Automated Analysis of Pricings in SaaS-based Information Systems</title>
      <link>https://arxiv.org/abs/2503.21444</link>
      <description>arXiv:2503.21444v1 Announce Type: new 
Abstract: Software as a Service (SaaS) pricing models, encompassing features, usage limits, plans, and add-ons, have grown exponentially in complexity, evolving from offering tens to thousands of configuration options. This rapid expansion poses significant challenges for the development and operation of SaaS-based Information Systems (IS), as manual management of such configurations becomes time-consuming, error-prone, and ultimately unsustainable. The emerging paradigm of Pricing-driven DevOps aims to address these issues by automating pricing management tasks, such as transforming human-oriented pricings into machine-oriented (iPricing) or finding the optimal subscription that matches the requirements of a certain user, ultimately reducing human intervention. This paper advances the field by proposing seven analysis operations that partially or fully support these pricing management tasks, thus serving as a foundation for defining new, more specialized operations. To achieve this, we mapped iPricings into Constraint Satisfaction Optimization Problems (CSOP), an approach successfully used in similar domains, enabling us to implement and apply these operations to uncover latent, yet non-trivial insights from complex pricing models. The proposed approach has been implemented in a reference framework using MiniZinc, and tested with over 150 pricing models, identifying errors in 35 pricings of the benchmark. Results demonstrate its effectiveness in identifying errors and its potential to streamline Pricing-driven DevOps.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21444v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Garc\'ia-Fern\'andez, Jos\'e Antonio Parejo, Pablo Trinidad, Antonio Ruiz-Cort\'es</dc:creator>
    </item>
    <item>
      <title>HORIZON: a Classification and Comparison Framework for Pricing-driven Feature Toggling</title>
      <link>https://arxiv.org/abs/2503.21448</link>
      <description>arXiv:2503.21448v1 Announce Type: new 
Abstract: Software as a Service (SaaS) has seen rapid growth in recent years, thanks to its ability to adapt to diverse user needs through subscription-based models. However, as pricing models enhance the customization of subscriptions, managing the associated constraints within a system's codebase becomes increasingly challenging. In response, Pricing-driven Development and Operation has emerged to integrate pricing considerations across the software lifecycle. Among its most challenging objectives is regulating feature access according to users' subscriptions -- a process that requires managing a multitude of conditions throughout the system's codebase. Feature toggles have traditionally been employed to manage dynamic system behavior, but their application to pricing-driven constraints presents unique challenges. When used to enforce subscription-based restrictions, toggles must adapt -- among other factors -- to individual user's use of features, ensuring that subscription limits are not exceeded. Despite the increasing significance of this problem, current industrial solutions lack explicit support for pricing-driven feature toggling, and existing academic contributions remain constrained to specific architectures. This paper contributes to fill this gap by introducing HORIZON, a classification and comparison framework for feature toggling tools tailored to pricing-driven environments. Its utility is demonstrated by categorizing the solutions identified in the literature as promising for such environments, revealing both their strengths and limitations, and thereby pinpointing critical avenues for improvement. In doing so, HORIZON not only provides a comprehensive view of the current landscape but also lays the groundwork for a focused research agenda, guiding the development of more robust and adaptable solutions for streamlining SaaS development and operations driven by pricings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21448v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alejandro Garc\'ia-Fern\'andez, Jose Antonio Parejo, Antonio Ruiz-Cort\'es</dc:creator>
    </item>
    <item>
      <title>Code Review Comprehension: Reviewing Strategies Seen Through Code Comprehension Theories</title>
      <link>https://arxiv.org/abs/2503.21455</link>
      <description>arXiv:2503.21455v1 Announce Type: new 
Abstract: Despite the popularity and importance of modern code review, the understanding of the cognitive processes that enable reviewers to analyze code and provide meaningful feedback is lacking. To address this gap, we observed and interviewed ten experienced reviewers while they performed 25 code reviews from their review queue. Since comprehending code changes is essential to perform code review and the primary challenge for reviewers, we focused our analysis on this cognitive process. Using Letovsky's model of code comprehension, we performed a theory-driven thematic analysis to investigate how reviewers apply code comprehension to navigate changes and provide feedback. Our findings confirm that code comprehension is fundamental to code review. We extend Letovsky's model to propose the Code Review Comprehension Model and demonstrate that code review, like code comprehension, relies on opportunistic strategies. These strategies typically begin with a context-building phase, followed by code inspection involving code reading, testing, and discussion management. To interpret and evaluate the proposed change, reviewers construct a mental model of the change as an extension of their understanding of the overall software system and contrast mental representations of expected and ideal solutions against the actual implementation. Based on our findings, we discuss how review tools and practices can better support reviewers in employing their strategies and in forming understanding. Data and material: https://doi.org/10.5281/zenodo.14748996</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21455v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pavl\'ina Wurzel Gon\c{c}alves, Pooja Rani, Margaret-Anne Storey, Diomidis Spinellis, Alberto Bacchelli</dc:creator>
    </item>
    <item>
      <title>MONO2REST: Identifying and Exposing Microservices: a Reusable RESTification Approach</title>
      <link>https://arxiv.org/abs/2503.21522</link>
      <description>arXiv:2503.21522v1 Announce Type: new 
Abstract: The microservices architectural style has become the de facto standard for large-scale cloud applications, offering numerous benefits in scalability, maintainability, and deployment flexibility. Many organizations are pursuing the migration of legacy monolithic systems to a microservices architecture. However, this process is challenging, risky, time-intensive, and prone-to-failure while several organizations lack necessary financial resources, time, or expertise to set up this migration process. So, rather than trying to migrate a legacy system where migration is risky or not feasible, we suggest exposing it as a microservice application without without having to migrate it. In this paper, we present a reusable, automated, two-phase approach that combines evolutionary algorithms with machine learning techniques. In the first phase, we identify microservices at the method level using a multi-objective genetic algorithm that considers both structural and semantic dependencies between methods. In the second phase, we generate REST APIs for each identified microservice using a classification algorithm to assign HTTP methods and endpoints. We evaluated our approach with a case study on the Spring PetClinic application, which has both monolithic and microservices implementations that serve as ground truth for comparison. Results demonstrate that our approach successfully aligns identified microservices with those in the reference microservices implementation, highlighting its effectiveness in service identification and API generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21522v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matth\'eo Lecrivain, Hanifa Barry, Dalila Tamzalit, Houari Sahraoui</dc:creator>
    </item>
    <item>
      <title>KRAFT -- A Knowledge-Graph-Based Resource Allocation Framework</title>
      <link>https://arxiv.org/abs/2503.21636</link>
      <description>arXiv:2503.21636v1 Announce Type: new 
Abstract: Resource allocation in business process management involves assigning resources to open tasks while considering factors such as individual roles, aptitudes, case-specific characteristics, and regulatory constraints. Current information systems for resource allocation often require extensive manual effort to specify and maintain allocation rules, making them rigid and challenging to adapt. In contrast, fully automated approaches provide limited explainability, making it difficult to understand and justify allocation decisions. Knowledge graphs, which represent real-world entities and their relationships, offer a promising solution by capturing complex dependencies and enabling dynamic, context-aware resource allocation. This paper introduces KRAFT, a novel approach that leverages knowledge graphs and reasoning techniques to support resource allocation decisions. We demonstrate that integrating knowledge graphs into resource allocation software allows for adaptable and transparent decision-making based on an evolving knowledge base.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21636v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Bein, Niels Martin, Luise Pufahl</dc:creator>
    </item>
    <item>
      <title>SoK: Towards Reproducibility for Software Packages in Scripting Language Ecosystems</title>
      <link>https://arxiv.org/abs/2503.21705</link>
      <description>arXiv:2503.21705v1 Announce Type: new 
Abstract: The disconnect between distributed software artifacts and their supposed source code enables attackers to leverage the build process for inserting malicious functionality. Past research in this field focuses on compiled language ecosystems, mostly analysing Linux distribution packages. However, the popular scripting language ecosystems potentially face unique issues given the systematic difference in distributed artifacts. This SoK provides an overview of existing research, aiming to highlight future directions, as well as chances to transfer existing knowledge from compiled language ecosystems. To that end, we work out key aspects in current research, systematize identified challenges for software reproducibility, and map them between the ecosystems. We find that the literature is sparse, focusing on few individual problems and ecosystems. This allows us to effectively identify next steps to improve reproducibility in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21705v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Pohl, Pavel Nov\'ak, Marc Ohm, Michael Meier</dc:creator>
    </item>
    <item>
      <title>Enhancing Repository-Level Software Repair via Repository-Aware Knowledge Graphs</title>
      <link>https://arxiv.org/abs/2503.21710</link>
      <description>arXiv:2503.21710v1 Announce Type: new 
Abstract: Repository-level software repair faces challenges in bridging semantic gaps between issue descriptions and code patches. Existing approaches, which mostly depend on large language models (LLMs), suffer from semantic ambiguities, limited structural context understanding, and insufficient reasoning capability. To address these limitations, we propose KGCompass with two innovations: (1) a novel repository-aware knowledge graph (KG) that accurately links repository artifacts (issues and pull requests) and codebase entities (files, classes, and functions), allowing us to effectively narrow down the vast search space to only 20 most relevant functions with accurate candidate bug locations and contextual information, and (2) a path-guided repair mechanism that leverages KG-mined entity path, tracing through which allows us to augment LLMs with relevant contextual information to generate precise patches along with their explanations. Experimental results in the SWE-Bench-Lite demonstrate that KGCompass achieves state-of-the-art repair performance (45.67%) and function-level localization accuracy (51.33%) across open-source approaches, costing only $0.20 per repair. Our analysis reveals that among successfully localized bugs, 69.7% require multi-hop traversals through the knowledge graph, without which LLM-based approaches struggle to accurately locate bugs. The knowledge graph built in KGCompass is language agnostic and can be incrementally updated, making it a practical solution for real-world development environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21710v1</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boyang Yang, Haoye Tian, Jiadong Ren, Shunfu Jin, Yang Liu, Feng Liu, Bach Le</dc:creator>
    </item>
    <item>
      <title>GateLens: A Reasoning-Enhanced LLM Agent for Automotive Software Release Analytics</title>
      <link>https://arxiv.org/abs/2503.21735</link>
      <description>arXiv:2503.21735v1 Announce Type: new 
Abstract: Ensuring the reliability and effectiveness of software release decisions is critical, particularly in safety-critical domains like automotive systems. Precise analysis of release validation data, often presented in tabular form, plays a pivotal role in this process. However, traditional methods that rely on manual analysis of extensive test datasets and validation metrics are prone to delays and high costs. Large Language Models (LLMs) offer a promising alternative but face challenges in analytical reasoning, contextual understanding, handling out-of-scope queries, and processing structured test data consistently; limitations that hinder their direct application in safety-critical scenarios. This paper introduces GateLens, an LLM-based tool for analyzing tabular data in the automotive domain. GateLens translates natural language queries into Relational Algebra (RA) expressions and then generates optimized Python code. It outperforms the baseline system on benchmarking datasets, achieving higher F1 scores and handling complex and ambiguous queries with greater robustness. Ablation studies confirm the critical role of the RA module, with performance dropping sharply when omitted. Industrial evaluations reveal that GateLens reduces analysis time by over 80% while maintaining high accuracy and reliability. As demonstrated by presented results, GateLens achieved high performance without relying on few-shot examples, showcasing strong generalization across various query types from diverse company roles. Insights from deploying GateLens with a partner automotive company offer practical guidance for integrating AI into critical workflows such as release validation. Results show that by automating test result analysis, GateLens enables faster, more informed, and dependable release decisions, and can thus advance software scalability and reliability in automotive systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21735v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.MA</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arsham Gholamzadeh Khoee, Shuai Wang, Yinan Yu, Robert Feldt, Dhasarathy Parthasarathy</dc:creator>
    </item>
    <item>
      <title>How to Secure Existing C and C++ Software without Memory Safety</title>
      <link>https://arxiv.org/abs/2503.21145</link>
      <description>arXiv:2503.21145v1 Announce Type: cross 
Abstract: The most important security benefit of software memory safety is easy to state: for C and C++ software, attackers can exploit most bugs and vulnerabilities to gain full, unfettered control of software behavior, whereas this is not true for most bugs in memory-safe software.
  Fortunately, this security benefit -- most bugs don't give attackers full control -- can be had for unmodified C/C++ software, without per-application effort.
  This doesn't require trying to establish memory safety; instead, it is sufficient to eliminate most of the combinatorial ways in which software with corrupted memory can execute. To eliminate these interleavings, there already exist practical compiler and runtime mechanisms that incur little overhead and need no special hardware or platform support.
  Each of the mechanisms described here is already in production use, at scale, on one or more platforms. By supporting their combined use in development toolchains, the security of all C and C++ software against remote code execution attacks can be rapidly, and dramatically, improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21145v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Ulfar Erlingsson</dc:creator>
    </item>
    <item>
      <title>A Data-Driven Method for INS/DVL Alignment</title>
      <link>https://arxiv.org/abs/2503.21350</link>
      <description>arXiv:2503.21350v1 Announce Type: cross 
Abstract: Autonomous underwater vehicles (AUVs) are sophisticated robotic platforms crucial for a wide range of applications. The accuracy of AUV navigation systems is critical to their success. Inertial sensors and Doppler velocity logs (DVL) fusion is a promising solution for long-range underwater navigation. However, the effectiveness of this fusion depends heavily on an accurate alignment between the inertial sensors and the DVL. While current alignment methods show promise, there remains significant room for improvement in terms of accuracy, convergence time, and alignment trajectory efficiency. In this research we propose an end-to-end deep learning framework for the alignment process. By leveraging deep-learning capabilities, such as noise reduction and capture of nonlinearities in the data, we show using simulative data, that our proposed approach enhances both alignment accuracy and reduces convergence time beyond current model-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21350v1</guid>
      <category>cs.RO</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guy Damari, Itzik Klein</dc:creator>
    </item>
    <item>
      <title>debug-gym: A Text-Based Environment for Interactive Debugging</title>
      <link>https://arxiv.org/abs/2503.21557</link>
      <description>arXiv:2503.21557v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly relied upon for coding tasks, yet in most scenarios it is assumed that all relevant information can be either accessed in context or matches their training data. We posit that LLMs can benefit from the ability to interactively explore a codebase to gather the information relevant to their task. To achieve this, we present a textual environment, namely debug-gym, for developing LLM-based agents in an interactive coding setting. Our environment is lightweight and provides a preset of useful tools, such as a Python debugger (pdb), designed to facilitate an LLM-based agent's interactive debugging. Beyond coding and debugging tasks, this approach can be generalized to other tasks that would benefit from information-seeking behavior by an LLM agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21557v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xingdi Yuan, Morgane M Moss, Charbel El Feghali, Chinmay Singh, Darya Moldavskaya, Drew MacPhee, Lucas Caccia, Matheus Pereira, Minseon Kim, Alessandro Sordoni, Marc-Alexandre C\^ot\'e</dc:creator>
    </item>
    <item>
      <title>A Measure Based Generalizable Approach to Understandability</title>
      <link>https://arxiv.org/abs/2503.21615</link>
      <description>arXiv:2503.21615v1 Announce Type: cross 
Abstract: Successful agent-human partnerships require that any agent generated information is understandable to the human, and that the human can easily steer the agent towards a goal. Such effective communication requires the agent to develop a finer-level notion of what is understandable to the human. State-of-the-art agents, including LLMs, lack this detailed notion of understandability because they only capture average human sensibilities from the training data, and therefore afford limited steerability (e.g., requiring non-trivial prompt engineering).
  In this paper, instead of only relying on data, we argue for developing generalizable, domain-agnostic measures of understandability that can be used as directives for these agents. Existing research on understandability measures is fragmented, we survey various such efforts across domains, and lay a cognitive-science-rooted groundwork for more coherent and domain-agnostic research investigations in future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21615v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vikas Kushwaha, Sruti Srinivasa Ragavan, Subhajit Roy</dc:creator>
    </item>
    <item>
      <title>Evaluation-Driven Development of LLM Agents: A Process Model and Reference Architecture</title>
      <link>https://arxiv.org/abs/2411.13768</link>
      <description>arXiv:2411.13768v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have enabled the emergence of LLM agents: autonomous systems capable of achieving under-specified goals and adapting post-deployment, often without explicit code or model changes. Evaluating these agents is critical to ensuring their performance and safety, especially given their dynamic, probabilistic, and evolving nature. However, traditional approaches such as predefined test cases and standard redevelopment pipelines struggle to address the unique challenges of LLM agent evaluation. These challenges include capturing open-ended behaviors, handling emergent outcomes, and enabling continuous adaptation over the agent's lifecycle. To address these issues, we propose an evaluation-driven development approach, inspired by test-driven and behavior-driven development but reimagined for the unique characteristics of LLM agents. Through a multivocal literature review (MLR), we synthesize the limitations of existing LLM evaluation methods and introduce a novel process model and reference architecture tailored for evaluation-driven development of LLM agents. Our approach integrates online (runtime) and offline (redevelopment) evaluations, enabling adaptive runtime adjustments and systematic iterative refinement of pipelines, artifacts, system architecture, and LLMs themselves. By continuously incorporating evaluation results, including fine-grained feedback from human and AI evaluators, into each stage of development and operation, this framework ensures that LLM agents remain aligned with evolving goals, user needs, and governance standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13768v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boming Xia, Qinghua Lu, Liming Zhu, Zhenchang Xing, Dehai Zhao, Hao Zhang</dc:creator>
    </item>
    <item>
      <title>Repository-level Code Translation Benchmark Targeting Rust</title>
      <link>https://arxiv.org/abs/2411.13990</link>
      <description>arXiv:2411.13990v5 Announce Type: replace 
Abstract: Recent advancements in large language models (LLMs) have demonstrated impressive capabilities in code translation, typically evaluated using benchmarks like CodeTransOcean. However, these benchmarks fail to capture real-world complexities by focusing primarily on simple function-level translations and overlooking repository-level context (e.g., dependencies). Moreover, LLMs' effectiveness in translating to newer, low-resource languages like Rust remains largely underexplored. To address this gap, we introduce RustRepoTrans, the first repository-level code translation benchmark, comprising 375 tasks translating into Rust from C++, Java, and Python. Using this benchmark, we evaluate four state-of-the-art LLMs, analyzing their errors to assess limitations in complex translation scenarios. Among them, Claude-3.5 performs best with 43.5% Pass@1, excelling in both basic functionality and additional translation abilities, such as noise robustness and syntactical difference identification. However, even Claude-3.5 experiences a 30.8% performance drop (Pass@1 from 74.3% to 43.5%) when handling repository-level context compared to previous benchmarks without such context. We also find that LLMs struggle with language differences in complex tasks, and dependencies further increase translation difficulty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13990v5</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangsheng Ou, Mingwei Liu, Yuxuan Chen, Xin Peng, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>A Catalog of Micro Frontends Anti-patterns</title>
      <link>https://arxiv.org/abs/2411.19472</link>
      <description>arXiv:2411.19472v4 Announce Type: replace 
Abstract: Micro frontend (MFE) architectures have gained significant popularity for promoting independence and modularity in development. Despite their widespread adoption, the field remains relatively unexplored, especially concerning identifying problems and documenting best practices. Drawing on both established microservice (MS) anti-patterns and the analysis of real problems faced by software development teams that adopt MFE, this paper presents a catalog of 12 MFE anti-patterns. We composed an initial version of the catalog by recognizing parallels between MS anti-patterns and recurring issues in MFE projects to map and adapt MS anti-patterns to the context of MFE. To validate the identified problems and proposed solutions, we conducted a survey with industry practitioners, collecting valuable feedback to refine the anti-patterns. Additionally, we asked participants if they had encountered these problems in practice and to rate their harmfulness on a 10-point Likert scale. The survey results revealed that participants had encountered all the proposed anti-patterns in real-world MFE architectures, with only one reported by less than 50\% of participants. They stated that the catalog can serve as a valuable guide for both new and experienced developers, with the potential to enhance MFE development quality. The collected feedback led to the development of an improved version of the anti-patterns catalog. Furthermore, we developed a web application designed to not only showcase the anti-patterns but also to actively foster collaboration and engagement within the MFE community. The proposed catalog is a valuable resource for identifying and mitigating potential pitfalls in MFE development. It empowers developers of all experience levels to create more robust, maintainable, and well-designed MFE applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19472v4</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nabson Silva, Eriky Rodrigues, Tayana Conte</dc:creator>
    </item>
    <item>
      <title>MANTRA: Enhancing Automated Method-Level Refactoring with Contextual RAG and Multi-Agent LLM Collaboration</title>
      <link>https://arxiv.org/abs/2503.14340</link>
      <description>arXiv:2503.14340v2 Announce Type: replace 
Abstract: Maintaining and scaling software systems relies heavily on effective code refactoring, yet this process remains labor-intensive, requiring developers to carefully analyze existing codebases and prevent the introduction of new defects. Although recent advancements have leveraged Large Language Models (LLMs) to automate refactoring tasks, current solutions are constrained in scope and lack mechanisms to guarantee code compilability and successful test execution. In this work, we introduce MANTRA, a comprehensive LLM agent-based framework that automates method-level refactoring. MANTRA integrates Context-Aware Retrieval-Augmented Generation, coordinated Multi-Agent Collaboration, and Verbal Reinforcement Learning to emulate human decision-making during refactoring while preserving code correctness and readability. Our empirical study, conducted on 703 instances of "pure refactorings" (i.e., code changes exclusively involving structural improvements), drawn from 10 representative Java projects, covers the six most prevalent refactoring operations. Experimental results demonstrate that MANTRA substantially surpasses a baseline LLM model (RawGPT ), achieving an 82.8% success rate (582/703) in producing code that compiles and passes all tests, compared to just 8.7% (61/703) with RawGPT. Moreover, in comparison to IntelliJ's LLM-powered refactoring tool (EM-Assist), MANTRA exhibits a 50% improvement in generating Extract Method transformations. A usability study involving 37 professional developers further shows that refactorings performed by MANTRA are perceived to be as readable and reusable as human-written code, and in certain cases, even more favorable. These results highlight the practical advantages of MANTRA and emphasize the growing potential of LLM-based systems in advancing the automation of software refactoring tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14340v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yisen Xu (Peter), Feng Lin (Peter), Jinqiu Yang (Peter),  Tse-Hsun (Peter),  Chen, Nikolaos Tsantalis</dc:creator>
    </item>
    <item>
      <title>Enhancing LLM-based Code Translation in Repository Context via Triple Knowledge-Augmented</title>
      <link>https://arxiv.org/abs/2503.18305</link>
      <description>arXiv:2503.18305v2 Announce Type: replace 
Abstract: Large language models (LLMs) have behaved well in function-level code translation without repository-level context. However, the performance of LLMs in repository-level context code translation remains suboptimal due to complex dependencies and context, hindering their adoption in industrial settings. In this work, we propose a novel LLM-based code translation technique K-Trans, which leverages triple knowledge augmentation to enhance LLM's translation quality under repository context in real-world software development. First, K-Trans constructs a translation knowledge base by extracting relevant information from target-language codebases, the repository being translated, and prior translation results. Second, for each function to be translated, K-Trans retrieves relevant triple knowledge, including target-language code samples, dependency usage examples, and successful translation function pairs, serving as references to enhance LLM for translation. Third, K-Trans constructs a knowledge-augmented translation prompt using the retrieved triple knowledge and employs LLMs to generate the translated code while preserving repository context. It further leverages LLMs for self-debugging, enhancing translation correctness.
  The experiments show that K-Trans substantially outperforms the baseline adapted from previous work by 19.4%/40.2% relative improvement in pass@1 and 0.138 in CodeBLEU. It is important to note that the results also demonstrate that each knowledge significantly contributes to K-Trans's effectiveness in handling repository-level context code translation, with dependency usage examples making the most notable contribution. Moreover, as the self-evolution process progresses, the knowledge base continuously enhances the LLM's performance across various aspects of the repository-level code translation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18305v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangsheng Ou, Mingwei Liu, Yuxuan Chen, Xueying Du, Shengbo Wang, Zekai Zhang, Xin Peng, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>LLPut: Investigating Large Language Models for Bug Report-Based Input Generation</title>
      <link>https://arxiv.org/abs/2503.20578</link>
      <description>arXiv:2503.20578v2 Announce Type: replace 
Abstract: Failure-inducing inputs play a crucial role in diagnosing and analyzing software bugs. Bug reports typically contain these inputs, which developers extract to facilitate debugging. Since bug reports are written in natural language, prior research has leveraged various Natural Language Processing (NLP) techniques for automated input extraction. With the advent of Large Language Models (LLMs), an important research question arises: how effectively can generative LLMs extract failure-inducing inputs from bug reports? In this paper, we propose LLPut, a technique to empirically evaluate the performance of three open-source generative LLMs -- LLaMA, Qwen, and Qwen-Coder -- in extracting relevant inputs from bug reports. We conduct an experimental evaluation on a dataset of 206 bug reports to assess the accuracy and effectiveness of these models. Our findings provide insights into the capabilities and limitations of generative LLMs in automated bug diagnosis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20578v2</guid>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alif Al Hasan, Subarna Saha, Mia Mohammad Imran, Tarannum Shaila Zaman</dc:creator>
    </item>
    <item>
      <title>How to organize an in-person, online or hybrid hackathon -- A revised planning kit</title>
      <link>https://arxiv.org/abs/2008.08025</link>
      <description>arXiv:2008.08025v3 Announce Type: replace-cross 
Abstract: Hackathons and similar time-bounded events are a global phenomenon. Their proliferation in various domains and their usefulness for a variety of goals has led to the emergence of different formats. While there are a multitude of guidelines available on how to prepare and run a hackathon, most of them focus on a particular format that was created for a specific purpose within a domain for a certain type of participant. This makes it difficult, in particular, for novice organizers to decide how to run an event that fits their needs. To address this gap we developed the original version of this planning kit in 2020 which focused on in-person events that were the dominant form of hackathons then. That planning kit was organized around 12 key decisions that organizers need to take when preparing for, running, and following up on a hackathon. Fast forward to 2025, after going through a global pandemic that forced all events to move online, we now see different forms of events - in-person, online, and hybrid - taking place across the globe, and while they can be all valuable, they have different affordances and require different considerations when planning. To account for these differences, we decided to update the original planning kit by adding a section that discusses the affordances and requirements of in-person, online, and hybrid events to each of the 12 decisions. In addition, we modified the original example timelines to include different forms and types of events. We also updated the planning kit in general based on insights we gained through continuing to organize and study hackathons. The main planning kit is available online while this report is meant to be a downloadable and citable resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.08025v3</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abasi-amefon Obot Affia-Jomants, Kiev Gama, James D. Herbsleb, Alexander Nolte</dc:creator>
    </item>
    <item>
      <title>Process Channels: A New Layer for Process Enactment Based on Blockchain State Channels</title>
      <link>https://arxiv.org/abs/2304.01107</link>
      <description>arXiv:2304.01107v3 Announce Type: replace-cross 
Abstract: For the enactment of inter-organizational business processes, blockchain can guarantee the enforcement of process models and the integrity of execution traces. However, existing solutions come with downsides regarding throughput scalability, latency, and suboptimal tradeoffs between confidentiality and transparency. To address these issues, we propose to change the foundation of blockchain-based business process execution: from on-chain smart contracts to state channels, an overlay network on top of a blockchain. State channels allow conducting most transactions off-chain while mostly retaining the core security properties offered by blockchain. Our proposal, process channels, is a model-driven approach to enacting processes on state channels, with the aim to retain the desired blockchain properties while reducing the on-chain footprint as much as possible. We here focus on the principled approach of state channels as a platform, to enable manifold future optimizations in various directions, like latency and confidentiality. We implement our approach prototypical and evaluate it both qualitatively (w.r.t. assumptions and guarantees) and quantitatively (w.r.t. correctness and gas cost). In short, while the initial deployment effort is higher with state channels, it typically pays off after a few process instances; and as long as the new assumptions hold, so do the guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01107v3</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-41620-0_12</arxiv:DOI>
      <arxiv:journal_reference>In: Di Francescomarino, C., Burattin, A., Janiesch, C., Sadiq, S. (eds) Business Process Management. BPM 2023</arxiv:journal_reference>
      <dc:creator>Fabian Stiehle, Ingo Weber</dc:creator>
    </item>
    <item>
      <title>Empowering WebAssembly with Thin Kernel Interfaces</title>
      <link>https://arxiv.org/abs/2312.03858</link>
      <description>arXiv:2312.03858v4 Announce Type: replace-cross 
Abstract: Wasm is gaining popularity outside the Web as a well-specified low-level binary format with ISA portability, low memory footprint and polyglot targetability, enabling efficient in-process sandboxing of untrusted code. Despite these advantages, Wasm adoption for new domains is often hindered by the lack of many standard system interfaces which precludes reusability of existing software and slows ecosystem growth.
  This paper proposes thin kernel interfaces for Wasm, which directly expose OS userspace syscalls without breaking intra-process sandboxing, enabling a new class of virtualization with Wasm as a universal binary format. By virtualizing the bottom layer of userspace, kernel interfaces enable effortless application ISA portability, compiler backend reusability, and armor programs with Wasm's built-in control flow integrity and arbitrary code execution protection. Furthermore, existing capability-based APIs for Wasm, such as WASI, can be implemented as a Wasm module over kernel interfaces, improving reuse, robustness, and portability through better layering. We present an implementation of this concept for two kernels -- Linux and Zephyr -- by extending a modern Wasm engine and evaluate our system's performance on a number of sophisticated applications which can run for the first time on Wasm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03858v4</guid>
      <category>cs.OS</category>
      <category>cs.SE</category>
      <pubDate>Fri, 28 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3689031.3717470</arxiv:DOI>
      <arxiv:journal_reference>Twentieth European Conference on Computer Systems (EuroSys 2025)</arxiv:journal_reference>
      <dc:creator>Arjun Ramesh, Tianshu Huang, Ben L. Titzer, Anthony Rowe</dc:creator>
    </item>
  </channel>
</rss>
