<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 12 Nov 2024 05:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Inherent Adversarial Robustness of Analog In-Memory Computing</title>
      <link>https://arxiv.org/abs/2411.07023</link>
      <description>arXiv:2411.07023v1 Announce Type: new 
Abstract: A key challenge for Deep Neural Network (DNN) algorithms is their vulnerability to adversarial attacks. Inherently non-deterministic compute substrates, such as those based on Analog In-Memory Computing (AIMC), have been speculated to provide significant adversarial robustness when performing DNN inference. In this paper, we experimentally validate this conjecture for the first time on an AIMC chip based on Phase Change Memory (PCM) devices. We demonstrate higher adversarial robustness against different types of adversarial attacks when implementing an image classification network. Additional robustness is also observed when performing hardware-in-the-loop attacks, for which the attacker is assumed to have full access to the hardware. A careful study of the various noise sources indicate that a combination of stochastic noise sources (both recurrent and non-recurrent) are responsible for the adversarial robustness and that their type and magnitude disproportionately effects this property. Finally, it is demonstrated, via simulations, that when a much larger transformer network is used to implement a Natural Language Processing (NLP) task, additional robustness is still observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07023v1</guid>
      <category>cs.ET</category>
      <category>cs.CR</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corey Lammie, Julian B\"uchel, Athanasios Vasilopoulos, Manuel Le Gallo, Abu Sebastian</dc:creator>
    </item>
    <item>
      <title>Exploring the Feasibility of Affordable Sonar Technology: Object Detection in Underwater Environments Using the Ping 360</title>
      <link>https://arxiv.org/abs/2411.05863</link>
      <description>arXiv:2411.05863v1 Announce Type: cross 
Abstract: This study explores the potential of the Ping 360 sonar device, primarily used for navigation, in detecting complex underwater obstacles. The key motivation behind this research is the device's affordability and open-source nature, offering a cost-effective alternative to more expensive imaging sonar systems. The investigation focuses on understanding the behaviour of the Ping 360 in controlled environments and assessing its suitability for object detection, particularly in scenarios where human operators are unavailable for inspecting offshore structures in shallow waters. Through a series of carefully designed experiments, we examined the effects of surface reflections and object shadows in shallow underwater environments. Additionally, we developed a manually annotated sonar image dataset to train a U-Net segmentation model. Our findings indicate that while the Ping 360 sonar demonstrates potential in simpler settings, its performance is limited in more cluttered or reflective environments unless extensive data pre-processing and annotation are applied. To our knowledge, this is the first study to evaluate the Ping 360's capabilities for complex object detection. By investigating the feasibility of low-cost sonar devices, this research provides valuable insights into their limitations and potential for future AI-based interpretation, marking a unique contribution to the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05863v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Md Junayed Hasan, Somasundar Kannan, Ali Rohan, Mohd Asif Shah</dc:creator>
    </item>
    <item>
      <title>ANCoEF: Asynchronous Neuromorphic Algorithm/Hardware Co-Exploration Framework with a Fully Asynchronous Simulator</title>
      <link>https://arxiv.org/abs/2411.06059</link>
      <description>arXiv:2411.06059v1 Announce Type: cross 
Abstract: Developing asynchronous neuromorphic hardware to meet the demands of diverse real-life edge scenarios remains significant challenges. These challenges include constraints on hardware resources and power budgets while satisfying the requirements for real-time responsiveness, reliable inference accuracy, and so on. Besides, the existing system-level simulators for asynchronous neuromorphic hardware suffer from runtime limitations. To address these challenges, we propose an Asynchronous Neuromorphic algorithm/hardware Co-Exploration Framework (ANCoEF) including multi-objective reinforcement learning (RL)-based hardware architecture optimization method, and a fully asynchronous simulator (TrueAsync) which achieves over 2 times runtime speedups than the state-of-the-art (SOTA) simulator. Our experimental results show that, the RL-based hardware architecture optimization approach of ANCoEF outperforms the SOTA method by reducing 1.81 times hardware energy-delay product (EDP) with 2.73 times less search time on N-MNIST dataset, and the co-exploration framework of ANCoEF improves SNN accuracy by 9.72% and reduces hardware EDP by 28.85 times compared to the SOTA work on DVS128Gesture dataset. Furthermore, ANCoEF framework is evaluated on external neuromorphic dataset CIFAR10-DVS, and static datasets including CIFAR10, CIFAR100, SVHN, and Tiny-ImageNet. For instance, after 26.23 ThreadHour of co-exploration process, the result on CIFAR10-DVS dataset achieves an SNN accuracy of 98.48% while consuming hardware EDP of 0.54 s nJ per sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06059v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Zhang, Xiang Zhang, Jingchen Huang, Jilin Zhang, Hong Chen</dc:creator>
    </item>
    <item>
      <title>Exploring Structural Nonlinearity in Binary Polariton-Based Neuromorphic Architectures</title>
      <link>https://arxiv.org/abs/2411.06124</link>
      <description>arXiv:2411.06124v1 Announce Type: cross 
Abstract: This study investigates the performance of a binarized neuromorphic network leveraging polariton dyads, optically excited pairs of interfering polariton condensates within a microcavity to function as binary logic gate neurons. Employing numerical simulations, we explore various neuron configurations, both linear (NAND, NOR) and nonlinear (XNOR), to assess their effectiveness in image classification tasks. We demonstrate that structural nonlinearity, derived from the network's layout, plays a crucial role in facilitating complex computational tasks, effectively reducing the reliance on the inherent nonlinearity of individual neurons. Our findings suggest that the network's configuration and the interaction among its elements can emulate the benefits of nonlinearity, thus potentially simplifying the design and manufacturing of neuromorphic systems and enhancing their scalability. This shift in focus from individual neuron properties to network architecture could lead to significant advancements in the efficiency and applicability of neuromorphic computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06124v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>physics.app-ph</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evgeny Sedov, Alexey Kavokin</dc:creator>
    </item>
    <item>
      <title>A Critical Analysis of Foundations, Challenges and Directions for Zero Trust Security in Cloud Environments</title>
      <link>https://arxiv.org/abs/2411.06139</link>
      <description>arXiv:2411.06139v1 Announce Type: cross 
Abstract: This review discusses the theoretical frameworks and application prospects of Zero Trust Security (ZTS) in cloud computing context. This is because, as organisations move more of their applications and data to the cloud, the old borders-based security model that many implemented are inadequate, therefore a model that has a trust no one, verify everything approach is required. This paper analyzes the core principles of ZTS, including micro-segmentation, least privileged access, and continuous monitoring, while critically examining four major controversies: scalability issues, Economics, Integration issues with existing systems, and Compliance to legal requirements. In this paper, having reviewed the existing literature in the field and various implementation cases, the main barriers to implementing zero trust security were outlined, including the dimensions of decreased performance in large-scale production and the need for major upfront investments that can be difficult for small companies to meet effectively. This research shows that there is no clear correlation between security effectiveness and operational efficiency: while organisations experience up to 40% decrease of security incidents after implementation, they note first negative impacts on performance. This study also shows that to support ZTS there is a need to address the context as the economics and operations of ZTS differ in strengths depending on the size of the organizations and the infrastructures. Some of these are: performance enhancement and optimizations, economic optimization, architectural blend, and privacy-preserving technologies. This review enriches the existing literature on cloud security by presenting both the theoretical framework of ZTS and the observed issues, and provides suggestions useful for future research and practice in the construction of the cloud security architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06139v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ganiyu Oladimeji</dc:creator>
    </item>
    <item>
      <title>Will Central Bank Digital Currencies (CBDC) and Blockchain Cryptocurrencies Coexist in the Post Quantum Era?</title>
      <link>https://arxiv.org/abs/2411.06362</link>
      <description>arXiv:2411.06362v1 Announce Type: cross 
Abstract: This paper explores the coexistence possibilities of Central Bank Digital Currencies (CBDCs) and blockchain-based cryptocurrencies within a post-quantum computing landscape. It examines the implications of emerging quantum algorithms and cryptographic techniques such as Multi-Party Computation (MPC) and Oblivious Transfer (OT). While exploring how CBDCs and cryptocurrencies might integrate defenses like post-quantum cryptography, it highlights the substantial hurdles in transitioning legacy systems and fostering widespread adoption of new standards. The paper includes comprehensive evaluations of CBDCs in a quantum context. It also features comparisons to alternative cryptocurrency models. Additionally, the paper provides insightful analyses of pertinent quantum methodologies. Examinations of interfaces between these methods and blockchain architectures are also included. The paper carries out considered appraisals of quantum threats and their relevance for cryptocurrency schemes. Furthermore, it features discussions of the influence of anticipated advances in quantum computing on algorithms and their applications. The paper renders the judicious conclusion that long-term coexistence is viable provided challenges are constructively addressed through ongoing collaborative efforts to validate solutions and guide evolving policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06362v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Itzhak Weinberg, Pythagoras Petratos, Alessio Faccia</dc:creator>
    </item>
    <item>
      <title>EO-GRAPE and EO-DRLPE: Open and Closed Loop Approaches for Energy Efficient Quantum Optimal Control</title>
      <link>https://arxiv.org/abs/2411.06556</link>
      <description>arXiv:2411.06556v1 Announce Type: cross 
Abstract: This research investigates the possibility of using quantum optimal control techniques to co-optimize the energetic cost and the process fidelity of a quantum unitary gate. The energetic cost is theoretically defined, and thereby, the gradient of the energetic cost for pulse engineering is derived. We empirically demonstrate the Pareto optimality in the trade-off between process fidelity and energetic cost. Thereafter, two novel numerical quantum optimal control approaches are proposed: (i) energy-optimized gradient ascent pulse engineering (EO-GRAPE) as an open-loop gradient-based method, and (ii) energy-optimized deep reinforcement learning for pulse engineering (EO-DRLPE) as a closed-loop method. The performance of both methods is probed in the presence of increasing noise. We find that the EO-GRAPE method performs better than the EO-DRLPE methods with and without a warm start for most experimental settings. Additionally, for one qubit unitary gate, we illustrate the correlation between the Bloch sphere path length and the energetic cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06556v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastiaan Fauquenot, Aritra Sarkar, Sebastian Feld</dc:creator>
    </item>
    <item>
      <title>Quantum-Powered Optimization for Electric Vehicle Charging Infrastructure Deployment</title>
      <link>https://arxiv.org/abs/2411.06684</link>
      <description>arXiv:2411.06684v1 Announce Type: cross 
Abstract: The infrastructure development of electric vehicle charging stations (EVCS) is critical to the integration of electrical vehicles (EVs) into transportation systems, which requires significant investment and has long-term impact on the adoption of EVs. In this paper, a mathematical model is developed to identify the optimal placement of EVCS by utilizing a novel quantum annealing (QA) algorithm and quantum computation (QC). The objective of the optimization model is to determine the locations of EVCS that maximize their service quality for EV users. The model is validated using a real-world case study and solved using commercially available quantum computers from D-Wave. The case study shows that the QA algorithm can find the optimal placement of EVCS within seconds. The quality of the solutions obtained using QC is not sensitive to the shape or size of the area where EVCS are to be deployed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06684v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nazmush Sakib, Xin Chen</dc:creator>
    </item>
    <item>
      <title>Computable Model-Independent Bounds for Adversarial Quantum Machine Learning</title>
      <link>https://arxiv.org/abs/2411.06863</link>
      <description>arXiv:2411.06863v1 Announce Type: cross 
Abstract: By leveraging the principles of quantum mechanics, QML opens doors to novel approaches in machine learning and offers potential speedup. However, machine learning models are well-documented to be vulnerable to malicious manipulations, and this susceptibility extends to the models of QML. This situation necessitates a thorough understanding of QML's resilience against adversarial attacks, particularly in an era where quantum computing capabilities are expanding. In this regard, this paper examines model-independent bounds on adversarial performance for QML. To the best of our knowledge, we introduce the first computation of an approximate lower bound for adversarial error when evaluating model resilience against sophisticated quantum-based adversarial attacks. Experimental results are compared to the computed bound, demonstrating the potential of QML models to achieve high robustness. In the best case, the experimental error is only 10% above the estimated bound, offering evidence of the inherent robustness of quantum models. This work not only advances our theoretical understanding of quantum model resilience but also provides a precise reference bound for the future development of robust QML algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06863v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bacui Li, Tansu Alpcan, Chandra Thapa, Udaya Parampalli</dc:creator>
    </item>
    <item>
      <title>Toward Standardized Performance Evaluation of Flow-guided Nanoscale Localization</title>
      <link>https://arxiv.org/abs/2303.07804</link>
      <description>arXiv:2303.07804v3 Announce Type: replace-cross 
Abstract: Nanoscale devices with Terahertz (THz) communication capabilities are envisioned to be deployed within human bloodstreams. Such devices will enable fine-grained sensing-based applications for detecting early indications (i.e., biomarkers) of various health conditions, as well as actuation-based ones such as targeted drug delivery. Associating the locations of such events with the events themselves would provide an additional utility for precision diagnostics and treatment. This vision yielded a new class of in-body localization coined under the term "flow-guided nanoscale localization". Such localization can be piggybacked on THz communication for detecting body regions in which biological events were observed based on the duration of one circulation of a nanodevice in the bloodstream. From a decades-long research on objective benchmarking of "traditional" indoor localization, as well as its eventual standardization (e.g., ISO/IEC 18305:2016), we know that in early stages the reported performance results were often incomplete (e.g., targeting a subset of relevant performance metrics), carrying out benchmarking experiments in different evaluation environments and scenarios, and utilizing inconsistent performance indicators. To avoid such a "lock-in" in flow-guided localization, in this paper we propose a workflow for standardized performance evaluation of such localization. The workflow is implemented in the form of an open-source simulation framework that is able to jointly account for the mobility of the nanodevices, in-body THz communication between with on-body anchors, and energy-related and other technological constraints (e.g., pulse-based modulation) at the nanodevice level. Accounting for these constraints, the framework is able to generate the raw data that can be streamlined into different flow-guided localization solutions for generating standardized performance benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07804v3</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnau Brosa L\'opez, Filip Lemic, Jakob Struye, Jorge Torres G\'omez, Esteban Municio, Carmen Delgado, Gerard Calvo Bartra, Falko Dressler, Eduard Alarc\'on, Jeroen Famaey, Sergi Abadal, Xavier Costa P\'erez</dc:creator>
    </item>
    <item>
      <title>Sustainable business decision modelling with blockchain and digital twins: A survey</title>
      <link>https://arxiv.org/abs/2405.12101</link>
      <description>arXiv:2405.12101v2 Announce Type: replace-cross 
Abstract: Industry 4.0 and beyond will rely heavily on sustainable Business Decision Modelling (BDM) that can be accelerated by blockchain and Digital Twin (DT) solutions. BDM is built on models and frameworks refined by key identification factors, data analysis, and mathematical or computational aspects applicable to complex business scenarios. Gaining actionable intelligence from collected data for BDM requires a carefully considered infrastructure to ensure data transparency, security, accessibility and sustainability. Organisations should consider social, economic and environmental factors (based on the triple bottom line approach) to ensure sustainability when integrating such an infrastructure. These sustainability features directly impact BDM concerning resource optimisation, stakeholder engagement, regulatory compliance and environmental impacts. To further understand these segments, taxonomies are defined to evaluate blockchain and DT sustainability features based on an in-depth review of the current state-of-the-art research. Detailed comparative evaluations provide insight into the reachability of the sustainable solution in terms of ideologies, access control and performance overheads. Several research questions are put forward to motivate further research that significantly impacts BDM. Finally, a case study based on an exemplary supply chain management system is presented to show the interoperability of blockchain and DT with BDM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12101v2</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gyan Wickremasinghe, Siofra Frost, Karen Rafferty, Vishal Sharma</dc:creator>
    </item>
    <item>
      <title>Five Key Enablers for Communication during and after Disasters</title>
      <link>https://arxiv.org/abs/2409.06822</link>
      <description>arXiv:2409.06822v2 Announce Type: replace-cross 
Abstract: Civilian communication during disasters such as earthquakes, floods, and military conflicts is crucial for saving lives. Nevertheless, several challenges exist during these circumstances such as the destruction of cellular communication and electricity infrastructure, lack of line of sight (LoS), and difficulty of localization under the rubble. In this article, we discuss key enablers that can boost communication during disasters, namely, satellite and aerial platforms, redundancy, silencing, and sustainable networks aided with wireless energy transfer (WET). The article also highlights how these solutions can be implemented in order to solve the failure of communication during disasters. Finally, it sheds light on unresolved challenges, as well as future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06822v2</guid>
      <category>eess.SP</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Shehab, Mustafa Kishk, Maurilio Matracia, Mehdi Bennis, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis</title>
      <link>https://arxiv.org/abs/2411.03205</link>
      <description>arXiv:2411.03205v3 Announce Type: replace-cross 
Abstract: Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a "GIS Copilot" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated with over 100 spatial analysis tasks with three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03205v3</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Temitope Akinboyewa, Zhenlong Li, Huan Ning, M. Naser Lessani</dc:creator>
    </item>
  </channel>
</rss>
