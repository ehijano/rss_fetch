<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 07 Apr 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A model-free feature extraction procedure for interval-valued time series prediction</title>
      <link>https://arxiv.org/abs/2504.03310</link>
      <description>arXiv:2504.03310v1 Announce Type: new 
Abstract: In this paper, we present a novel feature extraction procedure to predict interval-valued time series by combing transfer learning and imaging approaches. Initially, we represent interval-valued time series using a bivariate point-valued time series, which serves as a representative form. We first transform each time series into images by employing various imaging approaches such as recurrence plot, gramian angular summation/difference field, and Markov transition field, and construct an image dataset by treating each imaging method's output as a separate class. Based on this dataset, we train several candidates for a feature extraction network (FEN), specifically ResNet with varying layers. Then we choose the penultimate layer of the FEN to extract the most relevant features from the transformed images. We integrate the extracted features into conventional predictive models to formulate the corresponding prediction models. To formulate prediction, we integrate the extracted features into a regular prediction model. The proposed methods are evaluated based on the S\&amp;P 500 index and three data-generating processes (DGPs), and the experimental results demonstrate a notable improvement in prediction performance compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03310v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wan Tian, Zhongfeng Qin, Tao Hu</dc:creator>
    </item>
    <item>
      <title>Bayesian LSTM for indoor temperature modeling</title>
      <link>https://arxiv.org/abs/2504.03350</link>
      <description>arXiv:2504.03350v1 Announce Type: new 
Abstract: Improving energy efficiency of building heating systems is essential for reducing global energy consumption and greenhouse gas emissions. Traditional control methods in buildings rely on static heating curves based solely on outdoor temperature measurements, neglecting system state and free heat sources like solar gain. Model predictive control (MPC) not only addresses these limitations but further optimizes heating control by incorporating weather forecasts and system state predictions. However, current industrial MPC solutions often use simplified physics-inspired models, which compromise accuracy for interpretability. While purely data-driven models offer better predictive performance, they face challenges like overfitting and lack of transparency.
  To bridge this gap, we propose a Bayesian Long Short-Term Memory (LSTM) architecture for indoor temperature modeling. Our experiments across 100 real-world buildings demonstrate that the Bayesian LSTM outperforms an industrial physics-based model in predictive accuracy, enabling potential for improved energy efficiency and thermal comfort if deployed in heating MPC solutions. Over deterministic black-box approaches, the Bayesian framework provides additional advantages by improving generalization ability and allowing interpretation of predictions via uncertainty quantification. This work advances data-driven heating control by balancing predictive performance with the transparency and reliability required for real-world heating MPC applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03350v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Hannula, Arttu H\"akkinen, Antti Solonen, Felibe Uribe, Jana de Wiljes, Lassi Roininen</dc:creator>
    </item>
    <item>
      <title>Target Prediction Under Deceptive Switching Strategies via Outlier-Robust Filtering of Partially Observed Incomplete Trajectories</title>
      <link>https://arxiv.org/abs/2504.03502</link>
      <description>arXiv:2504.03502v1 Announce Type: new 
Abstract: Motivated by a study on deception and counter-deception, this paper addresses the problem of identifying an agent's target as it seeks to reach one of two targets in a given environment. In practice, an agent may initially follow a strategy to aim at one target but decide to switch to another midway. Such a strategy can be deceptive when the counterpart only has access to imperfect observations, which include heavily corrupted sensor noise and possible outliers, making it difficult to visually identify the agent's true intent. To counter deception and identify the true target, we utilize prior knowledge of the agent's dynamics and the imprecisely observed partial trajectory of the agent's states to dynamically update the estimation of the posterior probability of whether a deceptive switch has taken place. However, existing methods in the literature have not achieved effective deception identification within a reasonable computation time. We propose a set of outlier-robust change detection methods to track relevant change-related statistics efficiently, enabling the detection of deceptive strategies in hidden nonlinear dynamics with reasonable computational effort. The performance of the proposed framework is examined for Weapon-Target Assignment (WTA) detection under deceptive strategies using random simulations in the kinematics model with external forcing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03502v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Meng, Dongchang Li, Melkior Ornik</dc:creator>
    </item>
    <item>
      <title>evalprob4cast: An R-package for evaluation of ensembles as probabilistic forecasts or event forecasts</title>
      <link>https://arxiv.org/abs/2504.03544</link>
      <description>arXiv:2504.03544v1 Announce Type: new 
Abstract: For any forecasting application, evaluation of forecasts is an important task. For example, in the field of renewable energy sources there is high variability and uncertainty of power production, which makes forecasting and the evaluation hereof crucial both for power trading and power grid balancing. In particular, probabilistic forecasts represented by ensembles are popular due to their ability to cover the full range of scenarios that can occur, thus enabling forecast users to make more informed decisions than what would be possible with simple deterministic forecasts. The selection of open source software that supports evaluation of ensemble forecasts, and especially event detection, is currently limited. As a solution, evalprob4cast is a new R-package for probabilistic forecast evaluation that aims to provide its users with all the tools needed for the assessment of ensemble forecasts, in the form of metrics and visualization methods. Both univariate and multivariate probabilistic forecasts as well as event detection are covered. Furthermore, it offers a user-friendly design where all of the evaluation methods can be applied in a fast and easy way, as long as the input data is organized in accordance with the format defined by the package. While its development is motivated by forecasting of renewables, the package can be used for any application with ensemble forecasts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03544v1</guid>
      <category>stat.AP</category>
      <category>math.PR</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mathias Blicher Bjerreg{\aa}rd, Jethro Browell, John Zack, Jan Kloppenborg M{\o}ller, Henrik Madsen, Gregor Giebel, Corinna M\"ohrlen</dc:creator>
    </item>
    <item>
      <title>Going green across boundaries: Spatial effects of environmental policies on tourism flows</title>
      <link>https://arxiv.org/abs/2504.03608</link>
      <description>arXiv:2504.03608v1 Announce Type: new 
Abstract: This study investigates the relationship between environmental sustainability policies and tourism flows across Italian provinces using a Spatial Durbin Error Model (SDEM) within a gravity framework. By incorporating both public and corporate environmental initiatives, the analysis highlights the direct and spatial spillover effects of sustainability measures on tourism demand. The findings indicate that corporate-led initiatives, such as ecocertifications and green investments, exert a stronger direct influence on tourism flows compared to public measures, underscoring the visibility and immediate impact of private sector actions. However, both types of initiatives generate significant positive spatial spillovers, suggesting that sustainability efforts extend beyond local boundaries. These results demonstrate the interconnected nature of regional tourism systems and emphasize the critical role of coordinated sustainability policies in fostering tourism growth while promoting environmental protection. By addressing the spatial interdependencies of tourism flows and sustainability practices, this research provides valuable insights for policymakers and stakeholders seeking to improve sustainable tourism development at regional and national levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03608v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo Gianluigi Serio, Diego Giuliani, Maria Michela Dickson, Giuseppe Espa</dc:creator>
    </item>
    <item>
      <title>Enhanced ECG Arrhythmia Detection Accuracy by Optimizing Divergence-Based Data Fusion</title>
      <link>https://arxiv.org/abs/2504.02842</link>
      <description>arXiv:2504.02842v1 Announce Type: cross 
Abstract: AI computation in healthcare faces significant challenges when clinical datasets are limited and heterogeneous. Integrating datasets from multiple sources and different equipments is critical for effective AI computation but is complicated by their diversity, complexity, and lack of representativeness, so we often need to join multiple datasets for analysis. The currently used method is fusion after normalization. But when using this method, it can introduce redundant information, decreasing the signal-to-noise ratio and reducing classification accuracy. To tackle this issue, we propose a feature-based fusion algorithm utilizing Kernel Density Estimation (KDE) and Kullback-Leibler (KL) divergence. Our approach involves initially preprocessing and continuous estimation on the extracted features, followed by employing the gradient descent method to identify the optimal linear parameters that minimize the KL divergence between the feature distributions. Using our in-house datasets consisting of ECG signals collected from 2000 healthy and 2000 diseased individuals by different equipments and verifying our method by using the publicly available PTB-XL dataset which contains 21,837 ECG recordings from 18,885 patients. We employ a Light Gradient Boosting Machine (LGBM) model to do the binary classification. The results demonstrate that the proposed fusion method significantly enhances feature-based classification accuracy for abnormal ECG cases in the merged datasets, compared to the normalization method. This data fusion strategy provides a new approach to process heterogeneous datasets for the optimal AI computation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02842v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baozhuo Su, Qingli Dou, Kang Liu, Zhengxian Qu, Jerry Deng, Ting Tan, Yanan Gu</dc:creator>
    </item>
    <item>
      <title>Bayesian sequential analysis of adverse events with binary data</title>
      <link>https://arxiv.org/abs/2504.02959</link>
      <description>arXiv:2504.02959v1 Announce Type: cross 
Abstract: We propose a Bayesian Sequential procedure to test hypotheses concerning the Relative Risk between two specific treatments based on the binary data obtained from the two-arm clinical trial. Our development is based on the optimal sequential test of \citet{wang2024early}, which is cast within the Bayesian framework. This approach enables us to provide, in a straightforward manner based on the Stopping Rule Principle (SRP), an assessment of the various error probabilities via posterior probabilities and conditional error probabilities. Additionally, we present the connection to the notion of the Uniformly Most Powerful Bayesian Test (UMPBT). To illustrate our procedure, we utilized the data from \citet{silva2020optimal} to analyze the results obtained from the standard Bayesian and the modified Bayesian test of \citet{berger1997unified} under several different prior distributions of the parameters involved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02959v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Wang, Ben Boukai</dc:creator>
    </item>
    <item>
      <title>Scalable Fitting Methods for Multivariate Gaussian Additive Models with Covariate-dependent Covariance Matrices</title>
      <link>https://arxiv.org/abs/2504.03368</link>
      <description>arXiv:2504.03368v1 Announce Type: cross 
Abstract: We propose efficient computational methods to fit multivariate Gaussian additive models, where the mean vector and the covariance matrix are allowed to vary with covariates, in an empirical Bayes framework. To guarantee the positive-definiteness of the covariance matrix, we model the elements of an unconstrained parametrisation matrix, focussing particularly on the modified Cholesky decomposition and the matrix logarithm. A key computational challenge arises from the fact that, for the model class considered here, the number of parameters increases quadratically with the dimension of the response vector. Hence, here we discuss how to achieve fast computation and low memory footprint in moderately high dimensions, by exploiting parsimonious model structures, sparse derivative systems and by employing block-oriented computational methods. Methods for building and fitting multivariate Gaussian additive models are provided by the SCM R package, available at https://github.com/VinGioia90/SCM, while the code for reproducing the results in this paper is available at https://github.com/VinGioia90/SACM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03368v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincenzo Gioia, Matteo Fasiolo, Ruggero Bellio, Simon N. Wood</dc:creator>
    </item>
    <item>
      <title>Spatiotemporal causal inference with arbitrary spillover and carryover effects</title>
      <link>https://arxiv.org/abs/2504.03464</link>
      <description>arXiv:2504.03464v1 Announce Type: cross 
Abstract: Micro-level data with granular spatial and temporal information are becoming increasingly available to social scientists. Most researchers aggregate such data into a convenient panel data format and apply standard causal inference methods. This approach, however, has two limitations. First, data aggregation results in the loss of detailed geo-location and temporal information, leading to potential biases. Second, most panel data methods either ignore spatial spillover and temporal carryover effects or impose restrictive assumptions on their structure. We introduce a general methodological framework for spatiotemporal causal inference with arbitrary spillover and carryover effects. Under this general framework, we demonstrate how to define and estimate causal quantities of interest, explore heterogeneous treatment effects, investigate causal mechanisms, and visualize the results to facilitate their interpretation. We illustrate the proposed methodology through an analysis of airstrikes and insurgent attacks in Iraq. The open-source software package geocausal implements all of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03464v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mitsuru Mukaigawara, Kosuke Imai, Jason Lyall, Georgia Papadogeorgou</dc:creator>
    </item>
    <item>
      <title>A New Statistical Approach to Calibration-Free Localization Using Unlabeled Crowdsourced Data</title>
      <link>https://arxiv.org/abs/2504.03619</link>
      <description>arXiv:2504.03619v1 Announce Type: cross 
Abstract: Fingerprinting-based indoor localization methods typically require labor-intensive site surveys to collect signal measurements at known reference locations and frequent recalibration, which limits their scalability. This paper addresses these challenges by presenting a novel approach for indoor localization that utilizes crowdsourced data {\em without location labels}. We leverage the statistical information of crowdsourced data and propose a cumulative distribution function (CDF) based distance estimation method that maps received signal strength (RSS) to distances from access points. This approach overcomes the limitations of conventional distance estimation based on the empirical path loss model by efficiently capturing the impacts of shadow fading and multipath. Compared to fingerprinting, our {\em unsupervised} statistical approach eliminates the need for signal measurements at known reference locations. The estimated distances are then integrated into a three-step framework to determine the target location. The localization performance of our proposed method is evaluated using RSS data generated from ray-tracing simulations. Our results demonstrate significant improvements in localization accuracy compared to methods based on the empirical path loss model. Furthermore, our statistical approach, which relies on unlabeled data, achieves localization accuracy comparable to that of the {\em supervised} approach, the $k$-Nearest Neighbor ($k$NN) algorithm, which requires fingerprints with location labels. For reproducibility and future research, we make the ray-tracing dataset publicly available at [2].</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03619v1</guid>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haozhou Hu, Harpreet S. Dhillon, R. Michael Buehrer</dc:creator>
    </item>
    <item>
      <title>Estimating excess mortality during the Covid-19 pandemic in Aotearoa New Zealand</title>
      <link>https://arxiv.org/abs/2412.08927</link>
      <description>arXiv:2412.08927v2 Announce Type: replace 
Abstract: Background. The excess mortality rate in Aotearoa New Zealand during the Covid-19 pandemic is frequently estimated to be among the lowest in the world. However, to facilitate international comparisons, many of the methods that have been used to estimate excess mortality do not use age-stratified data on deaths and population size, which may compromise their accuracy.
  Methods. We used a quasi-Poisson regression model for monthly all-cause deaths among New Zealand residents, controlling for age, sex and seasonality. We fitted the model to deaths data for 2014-19. We estimated monthly excess mortality for 2020-23 as the difference between actual deaths and projected deaths according to the model. We conducted sensitivity analysis on the length of the pre-pandemic period used to fit the model. We benchmarked our results against a simple linear regression on the standardised annual mortality rate.
  Results. We estimated cumulative excess mortality in New Zealand in 2020-23 was 1040 (95% confidence interval [-1134, 2927]), equivalent to 0.7% [-0.8%, 2.0%] of expected mortality. Excess mortality was negative in 2020-21. The magnitude, timing and age-distribution of the positive excess mortality in 2022-23 were closely matched with confirmed Covid-19 deaths.
  Conclusions. Negative excess mortality in 2020-21 reflects very low levels of Covid-19 and major reductions in seasonal respiratory diseases during this period. In 2022-23, Covid-19 deaths were the main contributor to excess mortality and there was little or no net non-Covid-19 excess. Overall, New Zealand experienced one of the lowest rates of pandemic excess mortality in the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08927v2</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael John Plank, Pubudu Senanayake, Richard Lyon</dc:creator>
    </item>
    <item>
      <title>A co-segmentation algorithm to predict emotional stress from passively sensed mHealth data</title>
      <link>https://arxiv.org/abs/2502.10558</link>
      <description>arXiv:2502.10558v2 Announce Type: replace 
Abstract: We develop a data-driven co-segmentation algorithm of passively sensed and self-reported active variables collected through smartphones to identify emotionally stressful states in middle-aged and older patients with mood disorders undergoing therapy, some of whom also have chronic pain. Our method leverages the association between the different types of time series. These data are typically non-stationary, with meaningful associations often occurring only over short time windows. Traditional machine learning (ML) methods, when applied globally on the entire time series, often fail to capture these time-varying local patterns. Our approach first segments the passive sensing variables by detecting their change points, then examines segment-specific associations with the active variable to identify co-segmented periods that exhibit distinct relationships between stress and passively sensed measures. We then use these periods to predict future emotional stress states using standard ML methods. By shifting the unit of analysis from individual time points to data-driven segments of time and allowing for different associations in different segments, our algorithm helps detect patterns that only exist within short-time windows. We apply our method to detect periods of stress in patient data collected during ALACRITY Phase I study. Our findings indicate that the data-driven segmentation algorithm identifies stress periods more accurately than traditional ML methods that do not incorporate segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10558v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Sumanta Basu, Samprit Banerjee</dc:creator>
    </item>
  </channel>
</rss>
