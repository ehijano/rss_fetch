<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 04:00:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Distribution of Deep Gaussian process Gradients and Sequential Design for Simulators with Sharp Variations</title>
      <link>https://arxiv.org/abs/2503.16027</link>
      <description>arXiv:2503.16027v1 Announce Type: new 
Abstract: Deep Gaussian Processes (DGPs), multi-layered extensions of GPs, better emulate simulators with regime transitions or sharp changes than standard GPs. Gradient information is crucial for tasks like sensitivity analysis and dimension reduction. Although gradient posteriors are well-defined in GPs, extending them to DGPs is challenging due to their hierarchical structure. We propose a novel method to approximate the DGP emulator's gradient distribution, enabling efficient gradient computation with uncertainty quantification (UQ). Our approach derives an analytical gradient mean and the covariance. The numerical results show that our method outperforms GP and DGP with finite difference methods in gradient accuracy, offering the extra unique benefit of UQ. Based on the gradient information, we further propose a sequential design criterion to identify the sharp variation regions efficiently, with the gradient norm as a key indicator whose distribution can be readily evaluated in our framework. We evaluated the proposed sequential design using synthetic examples and empirical applications, demonstrating its superior performance in emulating functions with sharp changes compared to existing design methods. The DGP gradient computation is seamlessly integrated into the advanced Python package dgpsi for DGP emulation, along with the proposed sequential design available at https://github.com/yyimingucl/DGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16027v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Yang, Deyu Ming, Serge Guillas</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems</title>
      <link>https://arxiv.org/abs/2503.16222</link>
      <description>arXiv:2503.16222v1 Announce Type: new 
Abstract: This paper introduces a novel plug-and-play (PnP) Langevin sampling methodology for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling algorithms offer a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. However, existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we propose two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is demonstrated through extensive numerical experiments and comparisons with state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16222v1</guid>
      <category>stat.CO</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Teresa Klatzer, Savvas Melidonis, Marcelo Pereyra, Konstantinos C. Zygalakis</dc:creator>
    </item>
    <item>
      <title>Tuning Sequential Monte Carlo Samplers via Greedy Incremental Divergence Minimization</title>
      <link>https://arxiv.org/abs/2503.15704</link>
      <description>arXiv:2503.15704v1 Announce Type: cross 
Abstract: The performance of sequential Monte Carlo (SMC) samplers heavily depends on the tuning of the Markov kernels used in the path proposal. For SMC samplers with unadjusted Markov kernels, standard tuning objectives, such as the Metropolis-Hastings acceptance rate or the expected-squared jump distance, are no longer applicable. While stochastic gradient-based end-to-end optimization has been explored for tuning SMC samplers, they often incur excessive training costs, even for tuning just the kernel step sizes. In this work, we propose a general adaptation framework for tuning the Markov kernels in SMC samplers by minimizing the incremental Kullback-Leibler (KL) divergence between the proposal and target paths. For step size tuning, we provide a gradient- and tuning-free algorithm that is generally applicable for kernels such as Langevin Monte Carlo (LMC). We further demonstrate the utility of our approach by providing a tailored scheme for tuning \textit{kinetic} LMC used in SMC samplers. Our implementations are able to obtain a full \textit{schedule} of tuned parameters at the cost of a few vanilla SMC runs, which is a fraction of gradient-based approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15704v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyurae Kim, Zuheng Xu, Jacob R. Gardner, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Alignment of Continuous Brain Connectivity</title>
      <link>https://arxiv.org/abs/2503.15830</link>
      <description>arXiv:2503.15830v1 Announce Type: cross 
Abstract: Brain networks are typically represented by adjacency matrices, where each node corresponds to a brain region. In traditional brain network analysis, nodes are assumed to be matched across individuals, but the methods used for node matching often overlook the underlying connectivity information. This oversight can result in inaccurate node alignment, leading to inflated edge variability and reduced statistical power in downstream connectivity analyses. To overcome this challenge, we propose a novel framework for registering high resolution continuous connectivity (ConCon), defined as a continuous function on a product manifold space specifically, the cortical surface capturing structural connectivity between all pairs of cortical points. Leveraging ConCon, we formulate an optimal diffeomorphism problem to align both connectivity profiles and cortical surfaces simultaneously. We introduce an efficient algorithm to solve this problem and validate our approach using data from the Human Connectome Project (HCP). Results demonstrate that our method substantially improves the accuracy and robustness of connectome-based analyses compared to existing techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15830v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Cole, Yang Xiang, Will Consagra, Anuj Srivastava, Xing Qiu, Zhengwu Zhang</dc:creator>
    </item>
    <item>
      <title>Expected Information Gain Estimation via Density Approximations: Sample Allocation and Dimension Reduction</title>
      <link>https://arxiv.org/abs/2411.08390</link>
      <description>arXiv:2411.08390v2 Announce Type: replace-cross 
Abstract: Computing expected information gain (EIG) from prior to posterior (equivalently, mutual information between candidate observations and model parameters or other quantities of interest) is a fundamental challenge in Bayesian optimal experimental design. We formulate flexible transport-based schemes for EIG estimation in general nonlinear/non-Gaussian settings, compatible with both standard and implicit Bayesian models. These schemes are representative of two-stage methods for estimating or bounding EIG using marginal and conditional density estimates. In this setting, we analyze the optimal allocation of samples between training (density estimation) and approximation of the outer prior expectation. We show that with this optimal sample allocation, the MSE of the resulting EIG estimator converges more quickly than that of a standard nested Monte Carlo scheme. We then address the estimation of EIG in high dimensions, by deriving gradient-based upper bounds on the mutual information lost by projecting the parameters and/or observations to lower-dimensional subspaces. Minimizing these upper bounds yields projectors and hence low-dimensional EIG approximations that outperform approximations obtained via other linear dimension reduction schemes. Numerical experiments on a PDE-constrained Bayesian inverse problem also illustrate a favorable trade-off between dimension truncation and the modeling of non-Gaussianity, when estimating EIG from finite samples in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08390v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengyi Li, Ricardo Baptista, Youssef Marzouk</dc:creator>
    </item>
    <item>
      <title>Counting the number of group orbits by marrying the Burnside process with importance sampling</title>
      <link>https://arxiv.org/abs/2501.11731</link>
      <description>arXiv:2501.11731v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel and general algorithm for approximately counting the number of orbits under group actions. The method is based on combining the Burnside process and importance sampling. Specializing to unitriangular groups yields an efficient algorithm for estimating the number of conjugacy classes of such groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11731v2</guid>
      <category>math.PR</category>
      <category>math.CO</category>
      <category>math.GR</category>
      <category>stat.CO</category>
      <pubDate>Fri, 21 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Persi Diaconis, Chenyang Zhong</dc:creator>
    </item>
  </channel>
</rss>
