<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Robust Survival Estimation under Interval Censoring: Expectation-Maximization and Bayesian Accelerated Failure Time Assessment via Simulation and Application</title>
      <link>https://arxiv.org/abs/2509.02634</link>
      <description>arXiv:2509.02634v1 Announce Type: new 
Abstract: Interval censoring occurs when event times are only known to fall between scheduled assessments, a common design in clinical trials, epidemiology, and reliability studies. Standard right-censoring methods, such as Kaplan-Meier and Cox regression, are not directly applicable and can produce biased results. This study compares three complementary approaches for interval-censored survival data. First, the Turnbull nonparametric maximum likelihood estimator (NPMLE) via the EM algorithm recovers the survival distribution without strong assumptions. Second, Weibull and log-normal accelerated failure time (AFT) models with interval likelihoods provide smooth, covariate-adjusted survival curves and interpretable time-ratio effects. Third, Bayesian AFT models extend these tools by quantifying posterior uncertainty, incorporating prior information, and enabling interval-aware model comparisons via PSIS-LOO cross-validation. Simulations across generating distributions, censoring intensities, sample sizes, and covariate structures evaluated the integrated squared error (ISE) for curve recovery, integrated Brier score (IBS) for prediction, and coverage for uncertainty calibration. Results show that the EM achieves the lowest ISE for distribution recovery, AFT models improve predictive performance when families are correctly specified, and Bayesian AFT offers calibrated uncertainty and principled model selection. An application to the ovarian cancer dataset, restructured into interval-censored form, demonstrates the workflow in practice: the EM algorithm reveals the baseline shape, parametric AFT provides covariate-adjusted predictions, and Bayesian AFT validates model adequacy through posterior predictive checks. Together, these methods form a tiered strategy: EM for shape discovery, AFT for covariate-driven prediction, and Bayesian AFT for complete uncertainty quantification and model comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02634v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>J. T. Korley</dc:creator>
    </item>
    <item>
      <title>Analyzing health care data using count models: A novel approach to Length of Stay analysis</title>
      <link>https://arxiv.org/abs/2509.02703</link>
      <description>arXiv:2509.02703v1 Announce Type: new 
Abstract: Count data modeling has been extensively applied in medical sciences to analyze various healthcare datasets. Numerous probability models have been developed to address diverse aspects of healthcare data. In this study, we propose a novel count data model for analyzing healthcare datasets. Key structural properties of the model are established, and an associated regression framework is introduced to examine the effects of various covariates. Additionally, a three-inflated distribution, based on the proposed model, is presented to analyze length of stay of patients in hospitals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02703v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peer Bilal Ahmad, Na Elah</dc:creator>
    </item>
    <item>
      <title>Calibration Prediction Interval for Non-parametric Regression and Neural Networks</title>
      <link>https://arxiv.org/abs/2509.02735</link>
      <description>arXiv:2509.02735v1 Announce Type: new 
Abstract: Accurate conditional prediction in the regression setting plays an important role in many real-world problems. Typically, a point prediction often falls short since no attempt is made to quantify the prediction accuracy. Classically, under the normality and linearity assumptions, the Prediction Interval (PI) for the response variable can be determined routinely based on the $t$ distribution. Unfortunately, these two assumptions are rarely met in practice. To fully avoid these two conditions, we develop a so-called calibration PI (cPI) which leverages estimations by Deep Neural Networks (DNN) or kernel methods. Moreover, the cPI can be easily adjusted to capture the estimation variability within the prediction procedure, which is a crucial error source often ignored in practice. Under regular assumptions, we verify that our cPI has an asymptotically valid coverage rate. We also demonstrate that cPI based on the kernel method ensures a coverage rate with a high probability when the sample size is large. Besides, with several conditions, the cPI based on DNN works even with finite samples. A comprehensive simulation study supports the usefulness of cPI, and the convincing performance of cPI with a short sample is confirmed with two empirical datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02735v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kejin Wu, Dimitris N. Politis</dc:creator>
    </item>
    <item>
      <title>The Nearest-Neighbor Derivative Process: Modeling Spatial Rates of Change in Massive Datasets</title>
      <link>https://arxiv.org/abs/2509.02752</link>
      <description>arXiv:2509.02752v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are instrumental in modeling spatial processes, offering precise interpolation and prediction capabilities across fields such as environmental science and biology. Recently, there has been growing interest in extending GPs to infer spatial derivatives, which are vital for analyzing spatial dynamics and detecting subtle changes in data patterns. Despite their utility, traditional GPs suffer from computational inefficiencies, due to the cubic scaling with the number of spatial locations. Fortunately, the computational challenge has spurred extensive research on scalable GP methods. However, these scalable approaches do not directly accommodate the inference of derivative processes. A straightforward approach is to use scalable GP models followed by finite-difference methods, known as the plug-in estimator. This approach, while intuitive, suffers from sensitivity to parameter choices, and the approximate gradient may not be a valid GP, leading to compromised inference. To bridge this gap, we introduce the Nearest-Neighbor Derivative Process (NNDP), an innovative framework that models the spatial processes and their derivatives within a single scalable GP model. NNDP significantly reduces the computational time complexity from $O(n^3)$ to $O(n)$, making it feasible for large datasets. We provide various theoretical supports for NNDP and demonstrate its effectiveness through extensive simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02752v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawen Chen, Aritra Halder, Yun Li, Sudipto Banerjee, Didong Li</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bounds in Causal Mediation Analysis in the Presence of Unmeasured Confounding and Imperfect Compliance</title>
      <link>https://arxiv.org/abs/2509.02756</link>
      <description>arXiv:2509.02756v1 Announce Type: new 
Abstract: The average causal mediation effect (ACME) and the natural direct effect (NDE) are two parameters of primary interest in causal mediation analysis. However, the two causal parameters are not identifiable from randomized experimental data in the presence of outcome-mediator confounding and treatment-assignment noncompliance. Sj\"{o}lander (2009) addressed the partial identification issue and derived nonparametric bounds of the NDE in randomized controlled trials under a set of monotonicity assumptions based on the Balke-Pearl algorithm. These bounds provide partial information on the parameters and can be used for sensitivity analysis. In this paper, we extend Sj\"{o}lander's bounds on the NDE as well as the ACME to randomized controlled trials in the presence of noncompliance when the treatment assignment serves as an instrumental variable. Nonparametric sharp bounds for the local causal parameters defined on the subpopulation of treatment-assignment compliers are also established. We demonstrate the practical usefulness of the proposed upper and lower bounds through an application to the randomized experimental dataset on Job Search Intervention Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02756v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Liang, Changbao Wu</dc:creator>
    </item>
    <item>
      <title>Inference on covariance structure in high-dimensional multi-view data</title>
      <link>https://arxiv.org/abs/2509.02772</link>
      <description>arXiv:2509.02772v1 Announce Type: new 
Abstract: This article focuses on covariance estimation for multi-view data. Popular approaches rely on factor-analytic decompositions that have shared and view-specific latent factors. Posterior computation is conducted via expensive and brittle Markov chain Monte Carlo (MCMC) sampling or variational approximations that underestimate uncertainty and lack theoretical guarantees. Our proposed methodology employs spectral decompositions to estimate and align latent factors that are active in at least one view. Conditionally on these factors, we choose jointly conjugate prior distributions for factor loadings and residual variances. The resulting posterior is a simple product of normal-inverse gamma distributions for each variable, bypassing MCMC and facilitating posterior computation. We prove favorable increasing-dimension asymptotic properties, including posterior contraction and central limit theorems for point estimators. We show excellent performance in simulations, including accurate uncertainty quantification, and apply the methodology to integrate four high-dimensional views from a multi-omics dataset of cancer cell samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02772v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Mauri, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Covariate Adjustment Cannot Hurt: Treatment Effect Estimation under Interference with Low-Order Outcome Interactions</title>
      <link>https://arxiv.org/abs/2509.03050</link>
      <description>arXiv:2509.03050v1 Announce Type: new 
Abstract: In randomized experiments, covariates are often used to reduce variance and improve the precision of treatment effect estimates. However, in many real world settings, interference between units, where one unit's treatment affects another's outcome, complicates causal inference. This raises a key question: how can covariates be effectively used in the presence of interference? Addressing this challenge is nontrivial, as direct covariate adjustment, such as through regression, can sometimes increase variance due to dependencies across units. In this paper, we study how to use covariate information to reduce the variance of treatment effect estimators under interference. We focus on the total treatment effect (TTE), defined as the difference in average outcomes when all units are treated versus when all are controlled. Our analysis is conducted under the neighborhood interference model and a low order interaction outcome model. Building on the SNIPE estimator from Cortez-Rodriguez et al. (2023), we propose a covariate adjusted SNIPE estimator and show that, under sparsity conditions on the interference network, the proposed estimator is asymptotically unbiased and has asymptotic variance no greater than that of the original SNIPE estimator. This parallels the classical result of Lin (2013) under the no interference assumption, where covariate adjustment does not worsen estimation precision. Importantly, our variance improvement result does not rely on strong assumptions about the covariates: the covariates may be arbitrarily dependent, affect outcomes across units, and depend on the interference network itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03050v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyi Wang, Shuangning Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Network Propensity Score to Evaluate Treatment Effects in Observational Studies</title>
      <link>https://arxiv.org/abs/2509.03194</link>
      <description>arXiv:2509.03194v1 Announce Type: new 
Abstract: This paper focuses on the Bayesian Network Propensity Score (BNPS), a novel approach for estimating treatment effects in observational studies characterized by unknown (and likely unbalanced) designs and complex dependency structures among covariates. Traditional methods, such as logistic regression, often impose rigid parametric assumptions that may lead to misspecification errors, compromising causal inference. Recent classical and machine learning alternatives, such as boosted CART, random forests, and Stable Balancing Weights, seem to be attractive in a predictive perspective, but they typically lack asymptotic properties, such as consistency, efficiency, and valid variance estimation. In contrast, the recently proposed BNPS to estimate propensity scores uses Bayesian Networks to flexibly model conditional dependencies while preserving essential statistical properties such as consistency, asymptotic normality and asymptotic efficiency. Combined with the H\'ajek estimator, BNPS enables robust estimation of the Average Treatment Effect (ATE) in scenarios with strong covariate interactions and unknown data-generating mechanisms. Through extensive simulations across fifteen realistic scenarios and varying sample sizes, BNPS consistently outperforms benchmark methods in both empirical rejection rates and coverage accuracy. Finally, an application to a real-world dataset of 7,162 prostate cancer patients from San Raffaele Hospital (Milan, Italy) demonstrates BNPS's practical value in assessing the impact of pelvic lymph node dissection on hospitalization duration and biochemical recurrence. The findings support BNPS as a statistically robust, interpretable and transparent alternative for causal inference in complex observational settings, enhancing the reliability of evidence from real-world biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03194v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clelia Di Serio, Federica Cugnata, Pier Luigi Conti, Alberto Briganti, Fulvia Mecatti, Paola Vicard, Paola Maria Vittoria Rancoita</dc:creator>
    </item>
    <item>
      <title>Feedback-Enhanced Online Multiple Testing with Applications to Conformal Selection</title>
      <link>https://arxiv.org/abs/2509.03297</link>
      <description>arXiv:2509.03297v1 Announce Type: new 
Abstract: We study online multiple testing with feedback, where decisions are made sequentially and the true state of the hypothesis is revealed after the decision has been made, either instantly or with a delay. We propose GAIF, a feedback-enhanced generalized alpha-investing framework that dynamically adjusts thresholds using revealed outcomes, ensuring finite-sample false discovery rate (FDR)/marginal FDR control. Extending GAIF to online conformal testing, we construct independent conformal $p$-values and introduce a feedback-driven model selection criterion to identify the best model/score, thereby improving statistical power. We demonstrate the effectiveness of our methods through numerical simulations and real-data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03297v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Lu, Yuyang Huo, Haojie Ren, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Exponentially weighted moving average chart using zero-inflated negative binomial distribution</title>
      <link>https://arxiv.org/abs/2509.03304</link>
      <description>arXiv:2509.03304v1 Announce Type: new 
Abstract: Zero-inflated models are frequently used to deal with data having many zeros. A commonly used model for over-dispersed data containing zeros is known as the zero-inflated Poisson model. However, to account for the heterogeneity of counts that leads to excess variance besides inflation of zeros in the data using a more flexible model than the zero-inflated Poisson model, a zero-inflated negative binomial (ZINB) is suggested. In the present study, Shewhart and exponentially weighted moving average (EWMA) control charts are suggested to monitor the ZINB data. The charts are compared using the average run length and standard deviation of run length by using extensive Monte Carlo simulations. Besides a comprehensive simulation study assuming different settings of parameters of ZINB, a real data set is used to show the practicality of the proposed charts. The results indicate that the EWMA chart is better than the Shewhart chart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03304v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00949655.2024.2385014</arxiv:DOI>
      <dc:creator>Ali Abbas, Sajid Ali, Ismail Shah</dc:creator>
    </item>
    <item>
      <title>A Measure of Predictive Sharpness for Probabilistic Models</title>
      <link>https://arxiv.org/abs/2509.03309</link>
      <description>arXiv:2509.03309v1 Announce Type: new 
Abstract: This paper introduces a measure of predictive sharpness for probabilistic models, capturing the extent to which predictive mass departs from uniformity and concentrates over narrower subsets of the domain. For discrete settings, the measure is defined by cumulatively quantifying deviation from uniformity, taking into account both exclusion of outcomes and concentration of probability mass. For continuous settings, we develop an integral representation based on a monotone rearrangement of the predictive density. In supplementary material, we establish mathematical properties of the measure, derive formulas for forward and inverse domain transformations to support cross-domain comparisons, and provide an analysis of the relationship of the measure with entropy and variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03309v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pekka Syrj\"anen</dc:creator>
    </item>
    <item>
      <title>The super learner for time-to-event outcomes: A tutorial</title>
      <link>https://arxiv.org/abs/2509.03315</link>
      <description>arXiv:2509.03315v1 Announce Type: new 
Abstract: Estimating risks or survival probabilities conditional on individual characteristics based on censored time-to-event data is a commonly faced task. This may be for the purpose of developing a prediction model or may be part of a wider estimation procedure, such as in causal inference. A challenge is that it is impossible to know at the outset which of a set of candidate models will provide the best predictions. The super learner is a powerful approach for finding the best model or combination of models ('ensemble') among a pre-specified set of candidate models or 'learners', which can include parametric and machine learning models. Super learners for time-to-event outcomes have been developed, but the literature is technical and a reader may find it challenging to gather together the full details of how these methods work and can be implemented. In this paper we provide a practical tutorial on super learner methods for time-to-event outcomes. An overview of the general steps involved in the super learner is given, followed by details of three specific implementations for time-to-event outcomes. We cover discrete-time and continuous-time versions of the super learner, as described by Polley and van der Laan (2011), Westling et al. (2023) and Munch and Gerds (2024). We compare the properties of the methods and provide information on how they can be implemented in R. The methods are illustrated using an open access data set and R code is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03315v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruth H. Keogh, Karla Diaz-Ordaz, Nan van Geloven, Jon Michael Gran, Kamaryn T. Tanner</dc:creator>
    </item>
    <item>
      <title>Markov Missing Graph: A Graphical Approach for Missing Data Imputation</title>
      <link>https://arxiv.org/abs/2509.03410</link>
      <description>arXiv:2509.03410v1 Announce Type: new 
Abstract: We introduce the Markov missing graph (MMG), a novel framework that imputes missing data based on undirected graphs. MMG leverages conditional independence relationships to locally decompose the imputation model. To establish the identification, we introduce the Principle of Available Information (PAI), which guides the use of all relevant observed data. We then propose a flexible statistical learning paradigm, MMG Imputation Risk Minimization under PAI, that frames the imputation task as an empirical risk minimization problem. This framework is adaptable to various modeling choices. We develop theories of MMG, including the connection between MMG and Little's complete-case missing value assumption, recovery under missing completely at random, efficiency theory, and graph-related properties. We show the validity of our method with simulation studies and illustrate its application with a real-world Alzheimer's data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03410v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanjiao Yang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Temporal Exposure Dependence Bias in Vaccine Efficacy Trials</title>
      <link>https://arxiv.org/abs/2509.03476</link>
      <description>arXiv:2509.03476v1 Announce Type: new 
Abstract: Using time-to-event methods such as Cox proportional hazards models, it is well established that unmeasured heterogeneity in exposure or infection risk can lead to downward bias in point estimates of the per-contact vaccine efficacy (VE) in infectious disease trials. In this study, we explore an unreported source of bias-arising from temporally correlated exposure status-that is typically unmeasured and overlooked in standard analyses. Although this form of bias can plausibly affect a wide range of VE trials, it has received limited empirical attention. We develop a mathematical framework to characterize the mechanism of this bias and derive a closed-form approximation to quantify its magnitude without requiring direct measurement of exposure. Our findings show that, under realistic parameter settings, the resulting bias can be substantial. These results suggest that temporally correlated exposure should be recognized as a potentially important factor in the design and analysis of infectious disease vaccine trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03476v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroyasu Ando, A. James O'Malley, Akihiro Nishi</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariate Sparse Functional PCA</title>
      <link>https://arxiv.org/abs/2509.03512</link>
      <description>arXiv:2509.03512v1 Announce Type: new 
Abstract: Functional Principal Components Analysis (FPCA) provides a parsimonious, semi-parametric model for multivariate, sparsely-observed functional data. Frequentist FPCA approaches estimate principal components (PCs) from the data, then condition on these estimates in subsequent analyses. As an alternative, we propose a fully Bayesian inferential framework for multivariate, sparse functional data (MSFAST) which explicitly models the PCs and incorporates their uncertainty. MSFAST builds upon the FAST approach to FPCA for univariate, densely-observed functional data. Like FAST, MSFAST represents PCs using orthonormal splines, samples the orthonormal spline coefficients using parameter expansion, and enforces eigenvalue ordering during model fit. MSFAST extends FAST to multivariate, sparsely-observed data by (1) standardizing each functional covariate to mitigate poor posterior conditioning due to disparate scales; (2) using a better-suited orthogonal spline basis; (3) parallelizing likelihood calculations over covariates; (4) updating parameterizations and priors for computational stability; (5) using a Procrustes-based posterior alignment procedure; and (6) providing efficient prediction routines. We evaluated MSFAST alongside existing implementations using simulations. MSFAST produces uniquely valid inferences and accurate estimates, particularly for smaller signals. MSFAST is motivated by and applied to a study of child growth, with an accompanying vignette illustrating the implementation step-by-step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03512v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph Sartini, Scott Zeger, Ciprian Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Fast kernel methods: Sobolev, physics-informed, and additive models</title>
      <link>https://arxiv.org/abs/2509.02649</link>
      <description>arXiv:2509.02649v1 Announce Type: cross 
Abstract: Kernel methods are powerful tools in statistical learning, but their cubic complexity in the sample size n limits their use on large-scale datasets. In this work, we introduce a scalable framework for kernel regression with O(n log n) complexity, fully leveraging GPU acceleration. The approach is based on a Fourier representation of kernels combined with non-uniform fast Fourier transforms (NUFFT), enabling exact, fast, and memory-efficient computations. We instantiate our framework in three settings: Sobolev kernel regression, physics-informed regression, and additive models. When known, the proposed estimators are shown to achieve minimax convergence rates, consistent with classical kernel theory. Empirical results demonstrate that our methods can process up to tens of billions of samples within minutes, providing both statistical accuracy and computational scalability. These contributions establish a flexible approach, paving the way for the routine application of kernel methods in large-scale learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02649v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nathan Doum\`eche (LPSM, EDF R&amp;D OSIRIS), Francis Bach (ENS-PSL), G\'erard Biau (LPSM, IUF), Claire Boyer (LMO)</dc:creator>
    </item>
    <item>
      <title>Improving Generative Methods for Causal Evaluation via Simulation-Based Inference</title>
      <link>https://arxiv.org/abs/2509.02892</link>
      <description>arXiv:2509.02892v1 Announce Type: cross 
Abstract: Generating synthetic datasets that accurately reflect real-world observational data is critical for evaluating causal estimators, but remains a challenging task. Existing generative methods offer a solution by producing synthetic datasets anchored in the observed data (source data) while allowing variation in key parameters such as the treatment effect and amount of confounding bias. However, existing methods typically require users to provide point estimates of such parameters (rather than distributions) and fixed estimates (rather than estimates that can be improved with reference to the source data). This denies users the ability to express uncertainty over parameter values and removes the potential for posterior inference, potentially leading to unreliable estimator comparisons. We introduce simulation-based inference for causal evaluation (SBICE), a framework that models generative parameters as uncertain and infers their posterior distribution given a source dataset. Leveraging techniques in simulation-based inference, SBICE identifies parameter configurations that produce synthetic datasets closely aligned with the source data distribution. Empirical results demonstrate that SBICE improves the reliability of estimator evaluations by generating more realistic datasets, which supports a robust and data-consistent approach to causal benchmarking under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02892v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Pracheta Amaranath, Vinitra Muralikrishnan, Amit Sharma, David D. Jensen</dc:creator>
    </item>
    <item>
      <title>Evaluation of Stress Detection as Time Series Events -- A Novel Window-Based F1-Metric</title>
      <link>https://arxiv.org/abs/2509.03240</link>
      <description>arXiv:2509.03240v1 Announce Type: cross 
Abstract: Accurate evaluation of event detection in time series is essential for applications such as stress monitoring with wearable devices, where ground truth is typically annotated as single-point events, even though the underlying phenomena are gradual and temporally diffused. Standard metrics like F1 and point-adjusted F1 (F1$_{pa}$) often misrepresent model performance in such real-world, imbalanced datasets. We introduce a window-based F1 metric (F1$_w$) that incorporates temporal tolerance, enabling a more robust assessment of event detection when exact alignment is unrealistic. Empirical analysis in three physiological datasets, two in-the-wild (ADARP, Wrist Angel) and one experimental (ROAD), indicates that F1$_w$ reveals meaningful model performance patterns invisible to conventional metrics, while its window size can be adapted to domain knowledge to avoid overestimation. We show that the choice of evaluation metric strongly influences the interpretation of model performance: using predictions from TimesFM, only our temporally tolerant metrics reveal statistically significant improvements over random and null baselines in the two in-the-wild use cases. This work addresses key gaps in time series evaluation and provides practical guidance for healthcare applications where requirements for temporal precision vary by context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03240v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harald Vilhelm Skat-R{\o}rdam, Sneha Das, Kathrine Sofie Rasmussen, Nicole Nadine L{\o}nfeldt, Line Clemmensen</dc:creator>
    </item>
    <item>
      <title>Cluster and then Embed: A Modular Approach for Visualization</title>
      <link>https://arxiv.org/abs/2509.03373</link>
      <description>arXiv:2509.03373v1 Announce Type: cross 
Abstract: Dimensionality reduction methods such as t-SNE and UMAP are popular methods for visualizing data with a potential (latent) clustered structure. They are known to group data points at the same time as they embed them, resulting in visualizations with well-separated clusters that preserve local information well. However, t-SNE and UMAP also tend to distort the global geometry of the underlying data. We propose a more transparent, modular approach consisting of first clustering the data, then embedding each cluster, and finally aligning the clusters to obtain a global embedding. We demonstrate this approach on several synthetic and real-world datasets and show that it is competitive with existing methods, while being much more transparent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03373v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elizabeth Coda, Ery Arias-Castro, Gal Mishne</dc:creator>
    </item>
    <item>
      <title>Adaptive greedy forward variable selection for linear regression models with incomplete data using multiple imputation</title>
      <link>https://arxiv.org/abs/2210.10967</link>
      <description>arXiv:2210.10967v3 Announce Type: replace 
Abstract: Variable selection is crucial for sparse modeling in this age of big data. Missing values are common in data, and make variable selection more complicated. The approach of multiple imputation (MI) results in multiply imputed datasets for missing values, and has been widely applied in various variable selection procedures. However, directly performing variable selection on the whole MI data or bootstrapped MI data may not be worthy in terms of computation cost. To fast identify the active variables in the linear regression model, we propose the adaptive grafting procedure with three pooling rules on MI data. The proposed methods proceed iteratively, which starts from finding the active variables based on the complete case subset and then expand the working data matrix with both the number of active variables and available observations. A comprehensive simulation study shows the selection accuracy in different aspects and computational efficiency of the proposed methods. Two real-life examples illustrate the strength of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10967v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yong-Shiuan Lee</dc:creator>
    </item>
    <item>
      <title>Geodesic slice sampling on the sphere</title>
      <link>https://arxiv.org/abs/2301.08056</link>
      <description>arXiv:2301.08056v4 Announce Type: replace 
Abstract: Probability measures on the sphere form an important class of statistical models and are used, for example, in modeling directional data or shapes. Due to their widespread use, but also as an algorithmic building block, efficient sampling of distributions on the sphere is highly desirable. We propose a shrinkage based and an idealized geodesic slice sampling Markov chain, designed to generate approximate samples from distributions on the sphere. In particular, the shrinkage-based version of the algorithm can be implemented such that it runs efficiently in any dimension and has no tuning parameters. We verify reversibility and prove that under weak regularity conditions geodesic slice sampling is uniformly ergodic. Numerical experiments show that the proposed slice samplers achieve excellent mixing on challenging targets including the Bingham distribution and mixtures of von Mises-Fisher distributions. In these settings our approach outperforms standard samplers such as random-walk Metropolis-Hastings and Hamiltonian Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08056v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Habeck, Mareike Hasenpflug, Shantanu Kodgirwar, Daniel Rudolf</dc:creator>
    </item>
    <item>
      <title>Harnessing The Collective Wisdom: Fusion Learning Using Decision Sequences From Diverse Sources</title>
      <link>https://arxiv.org/abs/2308.11026</link>
      <description>arXiv:2308.11026v3 Announce Type: replace 
Abstract: We introduce an Integrative Ranking and Thresholding (IRT) framework for fusing evidence from multiple testing procedures. The key innovation is a method that transforms binary testing decisions into compound $e-$values, enabling the combination of findings across diverse data sources or studies. We demonstrate that IRT ensures overall false discovery rate (FDR) control, provided the individual studies maintain their respective FDR levels. This approach is highly flexible and is a powerful alternative for fusing inferences in meta-analysis where some studies report summary statistics while the rest reveal only the rejections under a pre-specified FDR level. Extensions to alternative Type I error control measures are explored.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11026v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Trambak Banerjee, Bowen Gang, Jianliang He</dc:creator>
    </item>
    <item>
      <title>Confounder selection via iterative graph expansion</title>
      <link>https://arxiv.org/abs/2309.06053</link>
      <description>arXiv:2309.06053v3 Announce Type: replace 
Abstract: Confounder selection, namely choosing a set of covariates to control for confounding between a treatment and an outcome, is arguably the most important step in the design of an observational study. Previous methods, such as Pearl's back-door criterion, typically require pre-specifying a causal graph, which can often be difficult in practice. We propose an interactive procedure for confounder selection that does not require pre-specifying the graph or the set of observed variables. This procedure iteratively expands the causal graph by finding what we call "primary adjustment sets" for a pair of possibly confounded variables. This can be viewed as inverting a sequence of marginalizations of the underlying causal graph. Structural information in the form of primary adjustment sets is elicited from the user, bit by bit, until either a set of covariates is found to control for confounding or it can be determined that no such set exists. Other information, such as the causal relations between confounders, is not required by the procedure. We show that if the user correctly specifies the primary adjustment sets in every step, our procedure is both sound and complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.06053v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. Richard Guo, Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Design of Bayesian A/B Tests Controlling False Discovery Rates and Power</title>
      <link>https://arxiv.org/abs/2312.10814</link>
      <description>arXiv:2312.10814v5 Announce Type: replace 
Abstract: Businesses frequently run online controlled experiments (i.e., A/B tests) to learn about the effect of an intervention on multiple business metrics. To account for multiple hypothesis testing, multiple metrics are commonly aggregated into a single composite measure, losing valuable information, or strict family-wise error rate adjustments are imposed, leading to reduced power. In this paper, we propose an economical framework to design Bayesian A/B tests while controlling both power and the false discovery rate (FDR). Selecting optimal decision thresholds to control power and the FDR typically relies on intensive simulation at each sample size considered. Our framework efficiently recommends optimal sample sizes and decision thresholds for Bayesian A/B tests that satisfy criteria for the FDR and average power. Our approach is efficient because we leverage new theoretical results to obtain these recommendations using simulations conducted at only two sample sizes. Our methodology is illustrated using an example based on a real A/B test involving several metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10814v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>Calibrated sensitivity models</title>
      <link>https://arxiv.org/abs/2405.08738</link>
      <description>arXiv:2405.08738v4 Announce Type: replace 
Abstract: In causal inference, sensitivity models assess how unmeasured confounders could alter causal analyses, but the sensitivity parameter -- which quantifies the degree of unmeasured confounding -- is often difficult to interpret. For this reason, researchers sometimes compare the sensitivity parameter to an estimate of measured confounding. This is known as calibration, or benchmarking. However, calibrated estimates are not always interpreted correctly, and uncertainty in the estimate of measured confounding is rarely accounted for. To address these limitations, we propose calibrated sensitivity models, which directly bound the degree of unmeasured confounding by a multiple of measured confounding. We develop a clear framework for interpreting calibrated sensitivity models and derive statistical methods for accounting for uncertainty due to estimating measured confounding. Incorporating this uncertainty shows causal analyses may be either less or more robust to unmeasured confounding than suggested by standard approaches. We develop efficient estimators and inferential methods for bounds on the average treatment effect with three calibrated sensitivity models, establishing parametric efficiency and asymptotic normality under doubly robust style nonparametric conditions. We illustrate our methods with an analysis of the effect of mothers' smoking on infant birthweight.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08738v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alec McClean, Zach Branson, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>An online generalization of the (e-)Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2407.20683</link>
      <description>arXiv:2407.20683v4 Announce Type: replace 
Abstract: In online multiple testing, the hypotheses arrive one by one, and at each time we must immediately reject or accept the current hypothesis solely based on the data and hypotheses observed so far. Many online procedures have been proposed, but none of them are generalizations of the Benjamini-Hochberg (BH) procedure based on p-values, or of the e-BH procedure that uses e-values. In this paper, we consider a relaxed problem setup that allows the current hypothesis to be rejected at any later step. We show that this relaxation allows us to define -- what we justify extensively to be -- the natural and appropriate online extension of the BH and e-BH procedures. We show that the FDR guarantees for BH (resp. e-BH) and online BH (resp. online e-BH) are identical under positive, negative or arbitrary dependence, at fixed and stopping times. Further, the online BH (resp. online e-BH) rule recovers the BH (resp. e-BH) rule as a special case when the number of hypotheses is known to be fixed. Of independent interest, our proof techniques also allow us to prove that numerous existing online procedures, which were known to control the FDR at fixed times, also control the FDR at stopping times. Finally, we extend the recently proposed Closure Principle for FDR control to the online case, which can potentially be used to improve the methods even further.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20683v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Ziyu Xu, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Sharp Bounds on the Variance of General Regression Adjustment in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2411.00191</link>
      <description>arXiv:2411.00191v2 Announce Type: replace 
Abstract: A growing statistical literature focuses on causal inference in the context of experiments where the target of inference is the average treatment effect in a finite population and random assignment determines which subjects are allocated to one of the experimental conditions. In this framework, variances of average treatment effect estimators remain unidentified because they depend on the covariance between treated and untreated potential outcomes, which are never jointly observed. Conventional variance estimators are upwardly biased. Aronow, Green and Lee [Ann. Statist. 42(3): 850-871 (June 2014)] provide an estimator for the variance of the difference-in-means estimator that is asymptotically sharp. In practice, researchers often use some form of covariate adjustment, such as linear regression, when estimating the average treatment effect. Adapting propositions from empirical process theory, we extend the result in (Aronow et al., 2014), providing asymptotically sharp variance bounds for general regression adjustment. We apply these results to linear regression adjustment and show benefits both in a simulation and in three empirical applications drawn from different disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.00191v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas M. Mikhaeil, Donald P. Green</dc:creator>
    </item>
    <item>
      <title>Distal Causal Excursion Effects: Modeling Long-Term Effects of Time-Varying Treatments in Micro-Randomized Trials</title>
      <link>https://arxiv.org/abs/2502.13500</link>
      <description>arXiv:2502.13500v2 Announce Type: replace 
Abstract: Micro-randomized trials (MRTs) play a crucial role in optimizing digital interventions. In an MRT, each participant is sequentially randomized among treatment options hundreds of times. While the interventions tested in MRTs target short-term behavioral responses (proximal outcomes), their ultimate goal is to drive long-term behavior change (distal outcomes). However, existing causal inference methods, such as the causal excursion effect, are limited to proximal outcomes, making it challenging to quantify the long-term impact of interventions. To address this gap, we introduce the distal causal excursion effect (DCEE), a novel estimand that quantifies the long-term effect of time-varying treatments. The DCEE contrasts distal outcomes under two excursion policies while marginalizing over most treatment assignments, enabling a parsimonious and interpretable causal model even with a large number of decision points. We propose two estimators for the DCEE -- one with cross-fitting and one without -- both robust to misspecification of the outcome model. We establish their asymptotic properties and validate their performance through simulations. We apply our method to the HeartSteps MRT to assess the impact of activity prompts on long-term habit formation. Our findings suggest that prompts delivered earlier in the study have a stronger long-term effect than those delivered later, underscoring the importance of intervention timing in behavior change. This work provides the critically needed toolkit for scientists working on digital interventions to assess long-term causal effects using MRT data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13500v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianchen Qian</dc:creator>
    </item>
    <item>
      <title>Multilevel Primary Aim Analyses of Clustered SMARTs: With Applications in Health Policy</title>
      <link>https://arxiv.org/abs/2503.08987</link>
      <description>arXiv:2503.08987v2 Announce Type: replace 
Abstract: In many health policy settings, adaptive interventions target a population of clusters (e.g., schools), with the ultimate intent of impacting outcomes at the level of individuals within the clusters. Health policy researchers can use clustered, sequential, multiple assignment, randomized trials (SMARTs) to answer important scientific questions concerning clustered adaptive interventions. A common primary aim is to compare the mean of a nested, end-of-study outcome between two clustered adaptive interventions. However, existing methods are not suitable when the primary outcome in a clustered SMART is nested and longitudinal (e.g., repeated outcome measures nested within mental healthcare providers, and mental healthcare providers nested within schools). This manuscript proposes a three-level marginal mean modeling and estimation approach for comparing adaptive interventions in a clustered SMART. The proposed method enables policy analysts to answer a wider array of scientific questions in the marginal comparison of clustered adaptive interventions. Further, relative to using an existing two-level method with a nested end-of-study outcome, the proposed method benefits from improved statistical efficiency. With this approach, we examine longitudinal comparisons of adaptive interventions for improving school-based mental healthcare and contrast its performance with existing approaches for studying static end-of-study outcomes. Methods were motivated by the Adaptive School-Based Implementation of CBT (ASIC) study, a clustered SMART designed to construct an adaptive health policy to improve the adoption of evidence-based CBT by mental healthcare professionals in high schools across Michigan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08987v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Durham, Anil Battalahalli, Amy Kilbourne, Andrew Quanbeck, Wenchu Pan, Tim Lycurgus, Daniel Almirall</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Heterogeneous Treatment Effect with Right-censored Data from Synthesizing Randomized Clinical Trials and Real-world Data</title>
      <link>https://arxiv.org/abs/2503.15745</link>
      <description>arXiv:2503.15745v2 Announce Type: replace 
Abstract: The heterogeneous treatment effect plays a crucial role in precision medicine.There is evidence that real-world data, even subject to biases, can be employed as supplementary evidence for randomized clinical trials to improve the statistical efficiency of the heterogeneous treatment effect estimation. In this paper, for survival data with right censoring, we consider estimating the heterogeneous treatment effect, defined as the difference of the treatment-specific conditional restricted mean survival times given covariates, by synthesizing evidence from randomized clinical trials and the real-world data with possible biases. We define an omnibus bias function to characterize the effect of biases caused by unmeasured confounders, censoring, and outcome heterogeneity, and further, identify it by combining the trial and real-world data. We propose a penalized sieve method to estimate the heterogeneous treatment effect and the bias function. We further study the theoretical properties of the proposed integrative estimators based on the theory of reproducing kernel Hilbert space and empirical process. The proposed methodology outperforms the approach solely based on the trial data through simulation studies and an integrative analysis of the data from a randomized trial and a real-world registry on early-stage non-small-cell lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.15745v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangcai Mao, Shu Yang, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>An integrated method for clustering and association network inference</title>
      <link>https://arxiv.org/abs/2503.22467</link>
      <description>arXiv:2503.22467v2 Announce Type: replace 
Abstract: High dimensional Gaussian graphical models provide a rigorous framework to describe a network of statistical dependencies between entities, such as genes in genomic regulation studies or species in ecology. Penalized methods, including the standard Graphical-Lasso, are well-known approaches to infer the parameters of these models. As the number of variables in the model (of entities in the network) grow, the network inference and interpretation become more complex. The Normal-Block model is introduced, a new model that clusters variables and consider a network at the cluster level. Normal-Block both adds structure to the network and reduces its size. The approach builds on Graphical-Lasso to add a penalty on the network's edges and limit the detection of spurious dependencies. A zero-inflated version of the model is also proposed to account for real-world data properties. For the inference procedure, two approaches are introduced, a straightforward method based on state-of-the-art approaches and an original, more rigorous method that simultaneously infers the clustering of variables and the association network between clusters, using a penalized variational Expectation-Maximization approach. An implementation of the model in R, in a package called \textbf{normalblockr}, is available on github\footnote{https://github.com/jeannetous/normalblockr}. The results of the models in terms of clustering and network inference are presented, using both simulated data and various types of real-world data (proteomics and words occurrences on webpages).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22467v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeanne Tous, Julien Chiquet</dc:creator>
    </item>
    <item>
      <title>Bringing closure to FDR control: beating the e-Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2504.11759</link>
      <description>arXiv:2504.11759v3 Announce Type: replace 
Abstract: False discovery rate (FDR) has been a key metric for error control in multiple hypothesis testing, and many methods have developed for FDR control across a diverse cross-section of settings and applications. We develop a closure principle for all FDR controlling procedures, i.e., we provide a characterization based on e-values for all admissible FDR controlling procedures. A general version of this closure principle can recover any multiple testing error metric and allows one to choose the error metric post-hoc. We leverage this idea to formulate the closed eBH procedure, a (usually strict) improvement over the eBH procedure for FDR control when provided with e-values. This also yields a closed BY procedure that dominates the Benjamini-Yekutieli (BY) procedure for FDR control with arbitrarily dependent p-values, thus proving that the latter is inadmissibile. We demonstrate the practical performance of our new procedures in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11759v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Lasse Fischer, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Modelling Skewed and Heavy-Tailed Errors in Bayesian Mediation Analysis</title>
      <link>https://arxiv.org/abs/2508.09311</link>
      <description>arXiv:2508.09311v3 Announce Type: replace 
Abstract: Traditional mediation models in both the frequentist and Bayesian frameworks typically assume normality of the error terms. Violations of this assumption can impair the estimation and hypothesis testing of the mediation effect in conventional approaches. This study addresses the non-normality issue by explicitly modelling skewed and heavy-tailed error terms within the Bayesian mediation framework. Building on the work of Fern\'andez and Steel (1998), this study introduces a novel family of distributions, termed the Centred Two-Piece Student $t$ Distribution (CTPT). The new distribution incorporates a skewness parameter into the Student t distribution and centres it to have a mean of zero, enabling flexible modelling of error terms in Bayesian regression and mediation analysis. A class of standard improper priors is employed, and conditions for the existence of the posterior distribution and posterior moments are established, while enabling inference on both skewness and tail parameters. Simulation studies are conducted to examine parameter recovery accuracy and statistical power in testing mediation effects. Compared to traditional Bayesian and frequentist methods, particularly bootstrap-based approaches, our method gives greater statistical power when correctly specified, while maintaining robustness against model misspecification. The application of the proposed approach is illustrated through real data analysis. Additionally, we have developed an R package FlexBayesMed to implement our methods in linear regression and mediation analysis, available at https://github.com/Zongyu-Li/FlexBayesMed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.09311v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongyu Li, Mark Steel, Zhiyong Zhang</dc:creator>
    </item>
    <item>
      <title>Deconfounding via Profiled Transfer Learning</title>
      <link>https://arxiv.org/abs/2508.11622</link>
      <description>arXiv:2508.11622v2 Announce Type: replace 
Abstract: Unmeasured confounders are a major source of bias in regression-based effect estimation and causal inference. In this paper, we advocate a new profiled transfer learning framework, ProTrans, to address confounding effects in the target dataset, when additional source datasets that possess similar confounding structures are available. We introduce the concept of profiled residuals to characterize the shared confounding patterns between source and target datasets. By incorporating these profiled residuals into the target debiasing step, we effectively mitigates the latent confounding effects. We also propose a source selection strategy to enhance robustness of ProTrans against noninformative sources. As a byproduct, ProTrans can also be utilized to estimate treatment effects when potential confounders exist, without the use of auxiliary features such as instrumental or proxy variables, which are often challenging to select in practice. Theoretically, we prove that the resulting estimated model shift from sources to target is confounding-free without any assumptions imposed on the true confounding structure, and that the target parameter estimation achieves the minimax optimal rate under mild conditions. Simulated and real-world experiments validate the effectiveness of ProTrans and support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11622v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyuan Chen, Yifan Jiang, Jingyuan Liu, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Testing Mechanisms</title>
      <link>https://arxiv.org/abs/2404.11739</link>
      <description>arXiv:2404.11739v2 Announce Type: replace-cross 
Abstract: Economists are often interested in the mechanisms by which a treatment affects an outcome. We develop tests for the "sharp null of full mediation" that a treatment $D$ affects an outcome $Y$ only through a particular mechanism (or set of mechanisms) $M$. Our approach exploits connections between mediation analysis and the econometric literature on testing instrument validity. We also provide tools for quantifying the magnitude of alternative mechanisms when the sharp null is rejected: we derive sharp lower bounds on the fraction of individuals whose outcome is affected by the treatment despite having the same value of $M$ under both treatments (``always-takers''), as well as sharp bounds on the average effect of the treatment for such always-takers. An advantage of our approach relative to existing tools for mediation analysis is that it does not require stringent assumptions about how $M$ is assigned. We illustrate our methodology in two empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11739v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soonwoo Kwon, Jonathan Roth</dc:creator>
    </item>
    <item>
      <title>Likelihood distortion and Bayesian local robustness</title>
      <link>https://arxiv.org/abs/2405.15141</link>
      <description>arXiv:2405.15141v4 Announce Type: replace-cross 
Abstract: Robust Bayesian analysis has been mainly devoted to detecting and measuring robustness w.r.t. the prior distribution. Many contributions in the literature aim to define suitable classes of priors which allow the computation of variations of quantities of interest while the prior changes within those classes. The literature has devoted much less attention to the robustness of Bayesian methods w.r.t. the likelihood function due to mathematical and computational complexity, and because it is often arguably considered a more objective choice compared to the prior. In this contribution, we propose a new approach to Bayesian local robustness, mainly focusing on robustness w.r.t. the likelihood function. Successively, we extend it to account for robustness w.r.t. the prior, as well as the prior and the likelihood jointly. This approach is based on the notion of distortion function introduced in the literature on risk theory. The novel robustness measure is a local sensitivity measure that turns out to be very tractable and easy to compute for several classes of distortion functions. Asymptotic properties are derived, and numerical experiments illustrate the theory and its applicability for modelling purposes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15141v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/25-BA1526</arxiv:DOI>
      <dc:creator>Antonio Di Noia, Fabrizio Ruggeri, Antonietta Mira</dc:creator>
    </item>
    <item>
      <title>k-Sample inference via Multimarginal Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.05645</link>
      <description>arXiv:2501.05645v2 Announce Type: replace-cross 
Abstract: This paper proposes a Multimarginal Optimal Transport ($MOT$) approach for simultaneously comparing $k\geq 2$ measures supported on finite subsets of $\mathbb{R}^d$, $d \geq 1$. We derive asymptotic distributions of the optimal value of the empirical $MOT$ program under the null hypothesis that all $k$ measures are same, and the alternative hypothesis that at least two measures are different. We use these results to construct the test of the null hypothesis and provide consistency and power guarantees of this $k$-sample test. We consistently estimate asymptotic distributions using bootstrap, and propose a low complexity linear program to approximate the test cut-off. We demonstrate the advantages of our approach on synthetic and real datasets, including the real data on cancers in the United States in 2004 - 2020.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05645v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Natalia Kravtsova</dc:creator>
    </item>
  </channel>
</rss>
