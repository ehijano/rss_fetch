<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 03:46:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Central schemes for systems of non-local balance laws</title>
      <link>https://arxiv.org/abs/2501.14425</link>
      <description>arXiv:2501.14425v1 Announce Type: new 
Abstract: We present numerical approaches to approximate the solutions of systems of non-local balance laws. In particular, we derive a non-staggered central scheme based on the well-known Nessyahu-Tadmor scheme and show that it preserves the positivity of solutions. To reduce the numerical diffusion, we then consider a non-local version of the Kurganov-Tadmor scheme. For both schemes, an appropriate approximation of the non-local term is crucial to maintain a second-order accuracy. Numerical examples validate our theory and demonstrate its applicability to various systems of non-local problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14425v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjibanee Sudha, Jan Friedrich, Samala Rathan</dc:creator>
    </item>
    <item>
      <title>Point Cloud Neural Operator for Parametric PDEs on Complex and Variable Geometries</title>
      <link>https://arxiv.org/abs/2501.14475</link>
      <description>arXiv:2501.14475v1 Announce Type: new 
Abstract: Surrogate models are critical for accelerating computationally expensive simulations in science and engineering, particularly for solving parametric partial differential equations (PDEs). Key challenges in developing practical surrogate models include managing high-dimensional inputs and outputs and handling geometrically complex and variable domains, which are often represented as point clouds. In this work, we systematically investigate the formulation of neural operators on point clouds and introduce the Point Cloud Neural Operator (PCNO), a neural network-based surrogate model designed to efficiently approximate solution maps of parametric PDEs on complex and variable geometries. We evaluate the performance of PCNO on a range of pedagogical PDE problems, focusing on aspects such as boundary layers, adaptively meshed point clouds, and variable domains with topological variations. Its practicality is further demonstrated through three-dimensional applications, such as predicting pressure loads on various types of vehicles and simulating the inflation process of intricate parachute structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14475v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenyu Zeng, Yanshu Zhang, Jiayi Zhou, Yuhan Wang, Zilin Wang, Yuhao Liu, Lei Wu, Daniel Zhengyu Huang</dc:creator>
    </item>
    <item>
      <title>A Space Mapping approach for the calibration of financial models with the application to the Heston model</title>
      <link>https://arxiv.org/abs/2501.14521</link>
      <description>arXiv:2501.14521v1 Announce Type: new 
Abstract: We present a novel approach for parameter calibration of the Heston model for pricing an Asian put option, namely space mapping. Since few parameters of the Heston model can be directly extracted from real market data, calibration to real market data is implicit and therefore a challenging task. In addition, some of the parameters in the model are non-linear, which makes it difficult to find the global minimum of the optimization problem within the calibration. Our approach is based on the idea of space mapping, exploiting the residuum of a coarse surrogate model that allows optimization and a fine model that needs to be calibrated. In our case, the pricing of an Asian option using the Heston model SDE is the fine model, and the surrogate is chosen to be the Heston model PDE pricing a European option. We formally derive a gradient descent algorithm for the PDE constrained calibration model using well-known techniques from optimization with PDEs. Our main goal is to provide evidence that the space mapping approach can be useful in financial calibration tasks. Numerical results underline the feasibility of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14521v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>q-fin.CP</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Clevenhaus, Claudia Totzeck, Matthias Ehrhardt</dc:creator>
    </item>
    <item>
      <title>Integrating Moving Least Squares with non-linear WENO method: A novel Partition of Unity approach in 1D</title>
      <link>https://arxiv.org/abs/2501.14536</link>
      <description>arXiv:2501.14536v1 Announce Type: new 
Abstract: The approximation of data is a fundamental challenge encountered in various fields, including computer-aided geometric design, the numerical solution of partial differential equations, or the design of curves and surfaces. Numerous methods have been developed to address this issue, providing good results when the data is continuous. Among these, the Moving Least Squares (MLS) method has proven to be an effective strategy for fitting data, finding applications in both statistics and applied mathematics. However, the presence of isolated discontinuities in the data can lead to undesirable artifacts, such as the Gibbs phenomenon, which adversely affects the quality of the approximation.
  In this paper, we propose a novel approach that integrates the Moving Least Squares method with the well-established non-linear Weighted Essentially Non-Oscillatory (WENO) method. This combination aims to construct a non-linear operator that enhances the accuracy of approximations near discontinuities while maintaining the order of accuracy in smooth regions. We investigate the properties of this operator in one dimension, demonstrating its effectiveness through a series of numerical experiments that validate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14536v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Inmaculada Garc\'es, Jos\'e M. Ram\'on, Juan Ruiz-\'Alvarez, Dionisio F. Y\'a\~nez</dc:creator>
    </item>
    <item>
      <title>Approximation of Set-Valued Functions with images sets in $\mathbb{R}^d$</title>
      <link>https://arxiv.org/abs/2501.14591</link>
      <description>arXiv:2501.14591v1 Announce Type: new 
Abstract: Given a finite number of samples of a continuous set-valued function F, mapping an interval to non-empty compact subsets of $\mathbb{R}^d$, $F: [a,b] \to K(\mathbb{R}^d)$, we discuss the problem of computing good approximations of F. We also discuss algorithms for a direct high-order evaluation of the graph of $F$, namely, the set $Graph(F)=\{(t,y)\ | \ y\in F(t),\ t\in [a,b]\}\in K(\mathbb{R}^{d+1})$. A set-valued function can be continuous and yet have points where the topology of the image sets changes. The main challenge in set-valued function approximation is to derive high-order approximations near these points. In a previous paper, we presented with Q. Muzaffar, an algorithm for approximating set-valued functions with 1D sets ($d=1$) as images, achieving high approximation order near points of topology change. Here we build upon the results and algorithms in the $d=1$ case, first in more detail for the important case $d=2$, and later for approximating set-valued functions and their graphs in higher dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14591v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nira Dyn, David Levin</dc:creator>
    </item>
    <item>
      <title>FIAT: enabling classical and modern macroelements</title>
      <link>https://arxiv.org/abs/2501.14599</link>
      <description>arXiv:2501.14599v1 Announce Type: new 
Abstract: Many classical and modern finite element spaces are derived by dividing each computational cell into finer pieces. Such \emph{macroelements} frequently enable the enforcement of mathematically desirable properties such as divergence-free conditions or $C^1$ continuity in a simpler or more efficient manner than elements without the subdivision. Although a few modern software projects provide one-off support for particular macroelements, a general approach facilitating broad-based support has, until now, been lacking. In this work, we describe a major addition to the FIAT project to support a wide range of different macroelements. These enhancements have been integrated into the Firedrake code stack. Numerical evaluation of the new macroelement facility is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14599v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo D. Brubeck, Robert C. Kirby</dc:creator>
    </item>
    <item>
      <title>A general correction for numerical integration rules over piece-wise continuous functions</title>
      <link>https://arxiv.org/abs/2501.14608</link>
      <description>arXiv:2501.14608v1 Announce Type: new 
Abstract: This article presents a novel approach to enhance the accuracy of classical quadrature rules by incorporating correction terms. The proposed method is particularly effective when the position of an isolated discontinuity in the function and the jump in the function and its derivatives at that position are known. Traditional numerical integration rules are exact for polynomials of certain degree. However, they may not provide accurate results for piece-wise polynomials or functions with discontinuities without modifying the location and number of data points in the formula. Our proposed correction terms address this limitation, enabling the integration rule to conserve its accuracy even in the presence of a jump discontinuity. The numerical experiments that we present support the theoretical results obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14608v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shipra Mahata, Samala Rathan, Juan Ruiz-\'Alvarez, Dionisio F. Y\'a\~nez</dc:creator>
    </item>
    <item>
      <title>Equal order stabilized finite elements with Nitsche for stationary Navier-Stokes problem with slip boundary conditions : a priori and a posteriori error analysis</title>
      <link>https://arxiv.org/abs/2501.14715</link>
      <description>arXiv:2501.14715v1 Announce Type: new 
Abstract: In this work, we extend the equal-order stabilized scheme discussed in [Franca et al., Comput. Methods Appl. Mech. Engrg. 99 (1992) 209-233] to accommodate slip (i.e., Navier) boundary conditions for the stationary Navier-Stokes equations. Our analysis presents a robust formulation for implementing slip boundary conditions using Nitsche's method on arbitrarily complex boundaries. The well-posedness of the discrete problem is established under mild assumptions together with optimal convergence rates for the approximation error. Furthermore, we establish the efficiency and reliability of residual-based a posteriori error estimators for the stationary discrete problem. Several well-known numerical tests validate our theoretical findings. The proposed method fits naturally within the framework of finite element implementation, offering both accuracy and enhanced flexibility in the selection of finite element pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14715v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Bansal, Nicol\'as Barnafi, Rodolfo Araya, Dwijendra Narain Pandey</dc:creator>
    </item>
    <item>
      <title>Well-Posedness of the R13 Equations Using Tensor-Valued Korn Inequalities</title>
      <link>https://arxiv.org/abs/2501.14108</link>
      <description>arXiv:2501.14108v1 Announce Type: cross 
Abstract: In this paper, we finally catch up with proving the well-posedness of the linearized R13 moment model, which describes, e.g., rarefied gas flows. As an extension of the classical fluid equations, moment models are robust and have been frequently used, yet they are challenging to analyze due to their additional equations. By effectively grouping variables, we identify a 2-by-2 block structure, allowing the analysis of the well-posedness within the abstract LBB framework of saddle point problems. Due to the unique tensorial structure of the equations, in addition to an interesting combination of tools from Stokes' and linear elasticity theory, we also need new coercivity estimates for tensor fields. These Korn-type inequalities are established by analyzing the symbol map of the symmetric and trace-free part of tensor derivative fields. Together with the corresponding right inverse of the tensorial divergence, we obtain the existence and uniqueness of weak solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14108v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Lewintan, Lambert Theisen, Manuel Torrilhon</dc:creator>
    </item>
    <item>
      <title>Convergence of gradient based training for linear Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2501.14440</link>
      <description>arXiv:2501.14440v1 Announce Type: cross 
Abstract: Graph Neural Networks (GNNs) are powerful tools for addressing learning problems on graph structures, with a wide range of applications in molecular biology and social networks. However, the theoretical foundations underlying their empirical performance are not well understood. In this article, we examine the convergence of gradient dynamics in the training of linear GNNs. Specifically, we prove that the gradient flow training of a linear GNN with mean squared loss converges to the global minimum at an exponential rate. The convergence rate depends explicitly on the initial weights and the graph shift operator, which we validate on synthetic datasets from well-known graph models and real-world datasets. Furthermore, we discuss the gradient flow that minimizes the total weights at the global minimum. In addition to the gradient flow, we study the convergence of linear GNNs under gradient descent training, an iterative scheme viewed as a discretization of gradient flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14440v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.NA</category>
      <category>cs.SI</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhiraj Patel, Anton Savostianov, Michael T. Schaub</dc:creator>
    </item>
    <item>
      <title>Diffusive transport on the real line: semi-contractive gradient flows and their discretization</title>
      <link>https://arxiv.org/abs/2501.14527</link>
      <description>arXiv:2501.14527v1 Announce Type: cross 
Abstract: The diffusive transport distance, a novel pseudo-metric between probability measures on the real line, is introduced. It generalizes Martingale optimal transport, and forms a hierarchy with the Hellinger and the Wasserstein metrics. We observe that certain classes of parabolic PDEs, among them the porous medium equation of exponent two, are formally semi-contractive metric gradient flows in the new distance. This observation is made rigorous for a suitable spatial discretization of the considered PDEs: these are semi-contractive gradient flows with respect to an adapted diffusive transport distance for measures on the point lattice. The main result is that the modulus of convexity is uniform with respect to the lattice spacing. Particularly for the quadratic porous medium equation, this is in contrast to what has been observed for discretizations of the Wasserstein gradient flow structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14527v1</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Matthes, Eva-Maria Rott, Andr\'e Schlichting</dc:creator>
    </item>
    <item>
      <title>Bean: A Language for Backward Error Analysis</title>
      <link>https://arxiv.org/abs/2501.14550</link>
      <description>arXiv:2501.14550v1 Announce Type: cross 
Abstract: Backward error analysis offers a method for assessing the quality of numerical programs in the presence of floating-point rounding errors. However, techniques from the numerical analysis literature for quantifying backward error require substantial human effort, and there are currently no tools or automated methods for statically deriving sound backward error bounds. To address this gap, we propose Bean, a typed first-order programming language designed to express quantitative bounds on backward error. Bean's type system combines a graded coeffect system with strict linearity to soundly track the flow of backward error through programs. We prove the soundness of our system using a novel categorical semantics, where every Bean program denotes a triple of related transformations that together satisfy a backward error guarantee.
  To illustrate Bean's potential as a practical tool for automated backward error analysis, we implement a variety of standard algorithms from numerical linear algebra in Bean, establishing fine-grained backward error bounds via typing in a compositional style. We also develop a prototype implementation of Bean that infers backward error bounds automatically. Our evaluation shows that these inferred bounds match worst-case theoretical relative backward error bounds from the literature, underscoring Bean's utility in validating a key property of numerical programs: numerical stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14550v1</guid>
      <category>cs.PL</category>
      <category>cs.LO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariel E. Kellison, Laura Zielinski, David Bindel, Justin Hsu</dc:creator>
    </item>
    <item>
      <title>Type-Based Approaches to Rounding Error Analysis</title>
      <link>https://arxiv.org/abs/2501.14598</link>
      <description>arXiv:2501.14598v1 Announce Type: cross 
Abstract: This dissertation explores the design and implementation of programming languages that represent rounding error analysis through typing.
  The first part of this dissertation demonstrates that it is possible to design languages for forward error analysis, as illustrated with NumFuzz, a functional programming language whose type system expresses quantitative bounds on rounding error. This type system combines a sensitivity analysis, enforced through a linear typing discipline, with a novel graded monad to track the accumulation of rounding errors. We establish the soundness of the type system by relating the denotational semantics of the language to both an exact and floating-point operational semantics. To demonstrate the practical utility of NumFuzz as a tool for automated error analysis, we have developed a prototype implementation capable of automatically inferring error bounds. Our implementation produces bounds competitive with existing tools, while often achieving significantly faster analysis times.
  The second part of this dissertation explores a type-based approach to backward error analysis with Bean, a first-order programming language with a linear type system that can express quantitative bounds on backward error. Bean's type system combines a graded coeffect system with strict linearity to soundly track the flow of backward error through programs. To illustrate Bean's potential as a practical tool for automated backward error analysis, we implement a variety of standard algorithms from numerical linear algebra in Bean, establishing fine-grained backward error bounds via typing in a compositional style. We also develop a prototype implementation of Bean that infers backward error bounds automatically. Our evaluation shows that these inferred bounds match worst-case theoretical relative backward error bounds from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14598v1</guid>
      <category>cs.PL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ariel Eileen Kellison</dc:creator>
    </item>
    <item>
      <title>A Paired Autoencoder Framework for Inverse Problems via Bayes Risk Minimization</title>
      <link>https://arxiv.org/abs/2501.14636</link>
      <description>arXiv:2501.14636v1 Announce Type: cross 
Abstract: In this work, we describe a new data-driven approach for inverse problems that exploits technologies from machine learning, in particular autoencoder network structures. We consider a paired autoencoder framework, where two autoencoders are used to efficiently represent the input and target spaces separately and optimal mappings are learned between latent spaces, thus enabling forward and inverse surrogate mappings. We focus on interpretations using Bayes risk and empirical Bayes risk minimization, and we provide various theoretical results and connections to existing works on low-rank matrix approximations. Similar to end-to-end approaches, our paired approach creates a surrogate model for forward propagation and regularized inversion. However, our approach outperforms existing approaches in scenarios where training data for unsupervised learning are readily available but training pairs for supervised learning are scarce. Furthermore, we show that cheaply computable evaluation metrics are available through this framework and can be used to predict whether the solution for a new sample should be predicted well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14636v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Hart, Julianne Chung, Matthias Chung</dc:creator>
    </item>
    <item>
      <title>Acceleration of RANS Solver Convergence via Initialization with Wake Extension Models</title>
      <link>https://arxiv.org/abs/2501.14699</link>
      <description>arXiv:2501.14699v1 Announce Type: cross 
Abstract: Use of appropriate initialization to warm-start Reynolds-averaged Navier-Stokes (RANS) simulations of turbulent flow can facilitate convergence and lead to efficient use of computational resources. In this work, a method to model downstream wake development in external turbulent flow is proposed and used for RANS solver convergence acceleration. To balance the model accuracy and cost, the proposed method divides the analysis domain into three regions: near-body, wake and off-body. An approach based on a convolutional neural network is introduced as an efficient method to predict the downstream wake development. The model training only requires data from a single simulation, and its use is demonstrated to be effective in accelerating the RANS simulation when combined with an accurate flow prediction in the near-body region. The simulation using the proposed method took 26.3x fewer iterations, achieving 16.4x speedup in wall-clock time, compared to a baseline run using freestream initialization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14699v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kazuko W. Fuchi, Eric M. Wolf, Christopher R. Schrock, Philip S. Beran</dc:creator>
    </item>
    <item>
      <title>Stroke classification using Virtual Hybrid Edge Detection from in silico electrical impedance tomography data</title>
      <link>https://arxiv.org/abs/2501.14704</link>
      <description>arXiv:2501.14704v1 Announce Type: cross 
Abstract: Electrical impedance tomography (EIT) is a non-invasive imaging method for recovering the internal conductivity of a physical body from electric boundary measurements. EIT combined with machine learning has shown promise for the classification of strokes. However, most previous works have used raw EIT voltage data as network inputs. We build upon a recent development which suggested the use of special noise-robust Virtual Hybrid Edge Detection (VHED) functions as network inputs, although that work used only highly simplified and mathematically ideal models. In this work we strengthen the case for the use of EIT, and VHED functions especially, for stroke classification. We design models with high detail and mathematical realism to test the use of VHED functions as inputs. Virtual patients are created using a physically detailed 2D head model which includes features known to create challenges in real-world imaging scenarios. Conductivity values are drawn from statistically realistic distributions, and phantoms are afflicted with either hemorrhagic or ischemic strokes of various shapes and sizes. Simulated noisy EIT electrode data, generated using the realistic Complete Electrode Model (CEM) as opposed to the mathematically ideal continuum model, is processed to obtain VHED functions. We compare the use of VHED functions as inputs against the alternative paradigm of using raw EIT voltages. Our results show that (i) stroke classification can be performed with high accuracy using 2D EIT data from physically detailed and mathematically realistic models, and (ii) in the presence of noise, VHED functions outperform raw data as network inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14704v1</guid>
      <category>math.AP</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Pablo Agnelli, Fernando S. Moura, Siiri Rautio, Melody Alsaker, Rashmi Murthy, Matti Lassas, Samuli Siltanen</dc:creator>
    </item>
    <item>
      <title>Numerical Ergodicity and Uniform Estimate of Monotone SPDEs Driven by Multiplicative Noise</title>
      <link>https://arxiv.org/abs/2305.06070</link>
      <description>arXiv:2305.06070v3 Announce Type: replace 
Abstract: We analyze the long-time behavior of numerical schemes for a class of monotone stochastic partial differential equations (SPDEs) driven by multiplicative noise. By deriving several time-independent a priori estimates for the numerical solutions, combined with the ergodic theory of Markov processes, we establish the exponential ergodicity of these schemes with a unique invariant measure, respectively. Applying these results to the stochastic Allen--Cahn equation indicates that these schemes always have at least one invariant measure, respectively, and converge strongly to the exact solution with sharp time-independent rates. We also show that these numerical invariant measures are exponentially ergodic and thus give an affirmative answer to a question proposed in (J. Cui, J. Hong, and L. Sun, Stochastic Process. Appl. (2021): 55--93), provided that the interface thickness is not too small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.06070v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihui Liu</dc:creator>
    </item>
    <item>
      <title>Convergent finite difference schemes for stochastic transport equations</title>
      <link>https://arxiv.org/abs/2309.02208</link>
      <description>arXiv:2309.02208v4 Announce Type: replace 
Abstract: We present difference schemes for stochastic transport equations with low-regularity velocity fields. We establish $L^2$ stability and convergence of the difference approximations under conditions that are less strict than those required for deterministic transport equations. The $L^2$ estimate, crucial for the analysis, is obtained through a discrete duality argument and a comprehensive examination of a class of backward parabolic difference schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02208v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/23M159946X</arxiv:DOI>
      <dc:creator>Ulrik S. Fjordholm, Kenneth H. Karlsen, Peter H. C. Pang</dc:creator>
    </item>
    <item>
      <title>Local space-preserving decompositions for the bubble transform</title>
      <link>https://arxiv.org/abs/2312.13161</link>
      <description>arXiv:2312.13161v2 Announce Type: replace 
Abstract: The bubble transform is a procedure to decompose differential forms, which are piecewise smooth with respect to a given triangulation of the domain, into a sum of local bubbles. In this paper, an improved version of a construction in the setting of the de Rham complex previously proposed by the authors is presented. The major improvement in the decomposition is that unlike the previous results, in which the individual bubbles were rational functions with the property that groups of local bubbles summed up to preserve piecewise smoothness, the new decomposition is strictly space-preserving in the sense that each local bubble preserves piecewise smoothness. An important property of the transform is that the construction only depends on the given triangulation of the domain and is independent of any finite element space. On the other hand, all the standard piecewise polynomial spaces are invariant under the transform. Other key properties of the transform are that it commutes with the exterior derivative, is bounded in L^2, and satisfies the stable decomposition property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13161v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard S. Falk, Ragnar Winther</dc:creator>
    </item>
    <item>
      <title>Robust Implicit Adaptive Low Rank Time-Stepping Methods for Matrix Differential Equations</title>
      <link>https://arxiv.org/abs/2402.05347</link>
      <description>arXiv:2402.05347v4 Announce Type: replace 
Abstract: In this work, we develop implicit rank-adaptive schemes for time-dependent matrix differential equations. The dynamic low rank approximation (DLRA) is a well-known technique to capture the dynamic low rank structure based on Dirac-Frenkel time-dependent variational principle. In recent years, it has attracted a lot of attention due to its wide applicability. Our schemes are inspired by the three-step procedure used in the rank adaptive version of the unconventional robust integrator (the so called BUG integrator) for DLRA. First, a prediction (basis update) step is made computing the approximate column and row spaces at the next time level. Second, a Galerkin evolution step is invoked using a base implicit solve for the small core matrix. Finally, a truncation is made according to a prescribed error threshold. Since the DLRA is evolving the differential equation projected on to the tangent space of the low rank manifold, the error estimate of the BUG integrator contains the tangent projection (modeling) error which cannot be easily controlled by mesh refinement. This can cause convergence issue for equations with cross terms.
  To address this issue, we propose a simple modification, consisting of merging the row and column spaces from the explicit step truncation method together with the BUG spaces in the prediction step. In addition, we propose an adaptive strategy where the BUG spaces are only computed if the residual for the solution obtained from the prediction space by explicit step truncation method, is too large. We prove stability and estimate the local truncation error of the schemes under assumptions. We benchmark the schemes in several tests, such as anisotropic diffusion, solid body rotation and the combination of the two, to show robust convergence properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05347v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Appel\"o, Yingda Cheng</dc:creator>
    </item>
    <item>
      <title>Vortex-capturing multiscale spaces for the Ginzburg-Landau equation</title>
      <link>https://arxiv.org/abs/2405.14772</link>
      <description>arXiv:2405.14772v4 Announce Type: replace 
Abstract: This paper considers minimizers of the Ginzburg-Landau energy functional in special multiscale spaces that are based on finite elements. The spaces are constructed by localized orthogonal decomposition techniques and their usage for solving the Ginzburg-Landau equation was first suggested in [D\"orich, Henning, SINUM 2024]. In this work we further explore their approximation properties and give an analytical explanation for why vortex structures of energy minimizers can be captured more accurately in these spaces. We quantify the necessary mesh resolution in terms of the Ginzburg-Landau parameter $\kappa$ and a stabilization parameter $\beta \ge 0$ that is used in the construction of the multiscale spaces. Furthermore, we analyze how $\kappa$ affects the necessary locality of the multiscale basis functions and we prove that the choice $\beta=0$ yields typically the highest accuracy. Our findings are supported by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14772v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Blum, Christian D\"oding, Patrick Henning</dc:creator>
    </item>
    <item>
      <title>High-order conservative and accurately dissipative numerical integrators via auxiliary variables</title>
      <link>https://arxiv.org/abs/2407.11904</link>
      <description>arXiv:2407.11904v2 Announce Type: replace 
Abstract: Numerical methods for the simulation of transient systems with structure-preserving properties are known to exhibit greater accuracy and physical reliability, in particular over long durations. However, there remain difficulties in devising geometric numerical integrators that preserve dissipation laws and conserve non-quadratic invariants. In this work, we propose a framework for the construction of timestepping schemes that preserve dissipation laws and conserve multiple general invariants. The framework employs finite elements in time and systematically introduces auxiliary variables; it extends to arbitrary order in time. We demonstrate the ideas by devising novel integrators that conserve (to machine precision) all known invariants of general conservative ODEs, energy-conserving finite-element discretisations of general Hamiltonian PDEs, and finite-element schemes for the compressible Navier-Stokes equations that conserve mass, momentum, and energy, and provably possess non-decreasing entropy. The approach generalises and unifies several existing ideas in the literature, including Gauss methods, the framework of Cohen &amp; Hairer, and the energy- and helicity-conserving scheme of Rebholz.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11904v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Boris D. Andrews, Patrick E. Farrell</dc:creator>
    </item>
    <item>
      <title>Numerical Ergodicity of Stochastic Allen--Cahn Equation driven by Multiplicative White Noise</title>
      <link>https://arxiv.org/abs/2408.02935</link>
      <description>arXiv:2408.02935v2 Announce Type: replace 
Abstract: We establish the unique ergodicity of a fully discrete scheme for monotone SPDEs with polynomial growth drift and bounded diffusion coefficients driven by multiplicative white noise. The main ingredient of our method depends on the satisfaction of a Lyapunov condition followed by a uniform moments' estimate, combined with the regularity property for the full discretization. We transform the original stochastic equation into an equivalent random equation where the discrete stochastic convolutions are uniformly controlled to derive the desired uniform moments' estimate. Applying the main result to the stochastic Allen--Cahn equation driven by multiplicative white noise indicates that this full discretization is uniquely ergodic for any interface thickness. Numerical experiments validate our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02935v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihui Liu</dc:creator>
    </item>
    <item>
      <title>A three-stage method for reconstructing multiple coefficients in coupled photoacoustic and diffuse optical imaging</title>
      <link>https://arxiv.org/abs/2408.03496</link>
      <description>arXiv:2408.03496v3 Announce Type: replace 
Abstract: This paper studies inverse problems in quantitative photoacoustic tomography with additional optical current data supplemented from diffuse optical tomography. We propose a three-stage image reconstruction method for the simultaneous recovery of the absorption, diffusion, and Gr\"uneisen coefficients. We demonstrate, through numerical simulations, that: (i) when the Gr\"uneisen coefficient is known, the addition of the optical measurements allows a more accurate reconstruction of the scattering and absorption coefficients; and (ii) when the Gr\"uneisen coefficient is not known, the addition of optical current measurements allows us to reconstruct uniquely the Gr\"uneisen, the scattering and absorption coefficients. Numerical simulations based on synthetic data are presented to demonstrate the effectiveness of the proposed idea.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03496v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <category>physics.comp-ph</category>
      <category>physics.med-ph</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxi Pan, Kui Ren, Shanyin Tong</dc:creator>
    </item>
    <item>
      <title>FIAT: improving performance and accuracy for high-order finite elements</title>
      <link>https://arxiv.org/abs/2408.03565</link>
      <description>arXiv:2408.03565v3 Announce Type: replace 
Abstract: FIAT (the FInite element Automatic Tabulator) provides a powerful Python library for the generation and evaluation of finite element basis functions on a reference element. This release paper describes recent improvements to FIAT aimed at improving its run time and the accuracy and efficiency of code generated using FIAT-provided information. In the first category, we have greatly streamlined the implementation of orthogonal polynomials out of which finite element bases are built. The second category comprises several more advances. For one, we have built an interface to the $\texttt{recursivenodes}$ package to enable more accurate Lagrange bases at high order. We have also implemented integral-type degrees of freedom for $H(\mathrm{div})$ and $H(\mathrm{curl})$ elements, which match the mathematical definitions of the elements more closely and also avoid loss of accuracy in interpolation. More fundamentally, we have included families of simplicial quadrature rules that require many fewer quadrature points than the Stroud rules previously used in FIAT. Finally, FIAT now provides support for fast diagonalization methods, which enable fast solution algorithms at very high order. In each case, we describe the new features in FIAT and illustrate some of the gains obtained through simple numerical tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03565v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pablo D. Brubeck, Robert C. Kirby, Fabian Laakmann, Lawrence Mitchell</dc:creator>
    </item>
    <item>
      <title>Numerical Analysis of the Parallel Orbital-Updating Approach for Eigenvalue Problems</title>
      <link>https://arxiv.org/abs/2409.00767</link>
      <description>arXiv:2409.00767v2 Announce Type: replace 
Abstract: The parallel orbital-updating approach is an orbital/eigenfunction iteration based approach for solving eigenvalue problems when many eigenpairs are required. It has been proven to be efficient, for instance, in electronic structure calculations. In this paper, based on the investigation of a quasi-orthogonality, we present the numerical analysis of the parallel orbital-updating approach for linear eigenvalue problems, including convergence and error estimates of the numerical approximations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00767v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoying Dai, Yan Li, Bin Yang, Aihui Zhou</dc:creator>
    </item>
    <item>
      <title>A fast numerical scheme for fractional viscoelastic models of wave propagation</title>
      <link>https://arxiv.org/abs/2410.01467</link>
      <description>arXiv:2410.01467v3 Announce Type: replace 
Abstract: Due to the nonlocal feature of fractional differential operators, the numerical solution to fractional partial differential equations usually requires expensive memory and computation costs. This paper develops a fast scheme for fractional viscoelastic models of wave propagation.
  We first apply the Laplace transform to convert the time-fractional constitutive equation into an integro-differential form that involves the Mittag-Leffler function as a convolution kernel. Then we construct an efficient sum-of-exponentials (SOE) approximation for the Mittag-Leffler function. We use mixed finite elements for the spatial discretization and the Newmark scheme for the temporal discretization of the second time-derivative of the displacement variable in the kinematical equation and finally obtain the fast algorithm. Compared with the traditional L1 scheme for time fractional derivative, our fast scheme reduces the memory complexity from $\mathcal O(N_sN) $ to $\mathcal O(N_sN_{exp})$ and the computation complexity from $\mathcal O(N_sN^2)$ to $\mathcal O(N_sN_{exp}N)$, where $N$ denotes the total number of temporal grid points, $N_{exp}$ is the number of exponentials in SOE, and $N_s$ represents the complexity of memory and computation related to the spatial discretization. Numerical experiments confirm the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01467v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hao Yuan, Xiaoping Xie</dc:creator>
    </item>
    <item>
      <title>Artificial Barriers for stochastic differential equations and for construction of boundary-preserving schemes</title>
      <link>https://arxiv.org/abs/2410.04850</link>
      <description>arXiv:2410.04850v2 Announce Type: replace 
Abstract: We develop the novel method of artificial barriers for scalar stochastic differential equations (SDEs) and use it to construct boundary-preserving numerical schemes for strong approximations of scalar SDEs, possibly with non-globally Lipschitz drift and diffusion coefficients, whose state-space is either bounded or half-bounded. The idea of artificial barriers is to augment the SDE with artificial barriers outside the state-space to not change the solution process, and then apply a boundary-preserving numerical scheme to the resulting reflected SDE (RSDE). This enables us to construct boundary-preserving numerical schemes with the same strong convergence rate as the strong convergence rate of the numerical scheme for the corresponding RSDE. Based on the method of artificial barriers, we construct two boundary-preserving schemes that we call the Artificial Barriers Euler--Maruyama (ABEM) scheme and the Artificial Barriers Euler--Peano (ABEP) scheme, respectively. We provide numerical experiments for the ABEM scheme and the numerical results agree with the obtained theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04850v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johan Ulander</dc:creator>
    </item>
    <item>
      <title>Non-isothermal non-Newtonian fluids: the stationary case</title>
      <link>https://arxiv.org/abs/2202.13075</link>
      <description>arXiv:2202.13075v2 Announce Type: replace-cross 
Abstract: The stationary Navier-Stokes equations for a non-Newtonian incompressible fluid are coupled with the stationary heat equation and subject to Dirichlet type boundary conditions. The viscosity is supposed to depend on the temperature and the stress depends on the strain through a suit-able power law depending on $p \in (1,2)$ (shear thinning case). For this problem we establish the existence of a weak solution as well as we prove some regularity results both for the Navier-Stokes and the Stokes cases. Then, the latter case with the Carreau power law is approximated through a FEM scheme and some error estimates are obtained. Such estimates are then validated through some two-dimensional numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.13075v2</guid>
      <category>math.AP</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1142/S0218202523500410</arxiv:DOI>
      <arxiv:journal_reference>Mathematical Models and Methods in Applied Sciences, 33(9) pp. 1747-1801, 2023</arxiv:journal_reference>
      <dc:creator>Maurizio Grasselli, Nicola Parolini, Andrea Poiatti, Marco Verani</dc:creator>
    </item>
    <item>
      <title>Polynomial Selection in Spectral Graph Neural Networks: An Error-Sum of Function Slices Approach</title>
      <link>https://arxiv.org/abs/2404.15354</link>
      <description>arXiv:2404.15354v2 Announce Type: replace-cross 
Abstract: Spectral graph neural networks are proposed to harness spectral information inherent in graph-structured data through the application of polynomial-defined graph filters, recently achieving notable success in graph-based web applications. Existing studies reveal that various polynomial choices greatly impact spectral GNN performance, underscoring the importance of polynomial selection. However, this selection process remains a critical and unresolved challenge. Although prior work suggests a connection between the approximation capabilities of polynomials and the efficacy of spectral GNNs, there is a lack of theoretical insights into this relationship, rendering polynomial selection a largely heuristic process.
  To address the issue, this paper examines polynomial selection from an error-sum of function slices perspective. Inspired by the conventional signal decomposition, we represent graph filters as a sum of disjoint function slices. Building on this, we then bridge the polynomial capability and spectral GNN efficacy by proving that the construction error of graph convolution layer is bounded by the sum of polynomial approximation errors on function slices. This result leads us to develop an advanced filter based on trigonometric polynomials, a widely adopted option for approximating narrow signal slices. The proposed filter remains provable parameter efficiency, with a novel Taylor-based parameter decomposition that achieves streamlined, effective implementation. With this foundation, we propose TFGNN, a scalable spectral GNN operating in a decoupled paradigm. We validate the efficacy of TFGNN via benchmark node classification tasks, along with an example graph anomaly detection application to show its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15354v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3696410.3714760</arxiv:DOI>
      <dc:creator>Guoming Li, Jian Yang, Shangsong Liang, Dongsheng Luo</dc:creator>
    </item>
    <item>
      <title>A New Cross-Space Total Variation Regularization Model for Color Image Restoration with Quaternion Blur Operator</title>
      <link>https://arxiv.org/abs/2405.12114</link>
      <description>arXiv:2405.12114v3 Announce Type: replace-cross 
Abstract: The cross-channel deblurring problem in color image processing is difficult to solve due to the complex coupling and structural blurring of color pixels. Until now, there are few efficient algorithms that can reduce color artifacts in deblurring process. To solve this challenging problem, we present a novel cross-space total variation (CSTV) regularization model for color image deblurring by introducing a quaternion blur operator and a cross-color space regularization functional. The existence and uniqueness of the solution are proved and a new L-curve method is proposed to find a balance of regularization terms on different color spaces. The Euler-Lagrange equation is derived to show that CSTV has taken into account the coupling of all color channels and the local smoothing within each color channel. A quaternion operator splitting method is firstly proposed to enhance the ability of color artifacts reduction of the CSTV regularization model. This strategy also applies to the well-known color deblurring models. Numerical experiments on color image databases illustrate the efficiency and effectiveness of the new model and algorithms. The color images restored by them successfully maintain the color and spatial information and are of higher quality in terms of PSNR, SSIM, MSE and CIEde2000 than the restorations of the-state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12114v3</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 27 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhigang Jia, Yuelian Xiang, Meixiang Zhao, Tingting Wu, Michael K. Ng</dc:creator>
    </item>
  </channel>
</rss>
