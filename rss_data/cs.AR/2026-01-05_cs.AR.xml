<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 05 Jan 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhancing Reliability of STT-MRAM Caches by Eliminating Read Disturbance Accumulation</title>
      <link>https://arxiv.org/abs/2601.00450</link>
      <description>arXiv:2601.00450v1 Announce Type: new 
Abstract: Spin-Transfer Torque Magnetic RAM (STT-MRAM) as one of the most promising replacements for SRAMs in on-chip cache memories benefits from higher density and scalability, near-zero leakage power, and non-volatility, but its reliability is threatened by high read disturbance error rate. Error-Correcting Codes (ECCs) are conventionally suggested to overcome the read disturbance errors in STT-MRAM caches. By employing aggressive ECCs and checking out a cache block on every read access, a high level of cache reliability is achieved. However, to minimize the cache access time in modern processors, all blocks in the target cache set are simultaneously read in parallel for tags comparison operation and only the requested block is sent out, if any, after checking its ECC. These extra cache block reads without checking their ECCs until requesting the blocks by the processor cause the accumulation of read disturbance error, which significantly degrade the cache reliability. In this paper, we first introduce and formulate the read disturbance accumulation phenomenon and reveal that this accumulation due to conventional parallel accesses of cache blocks significantly increases the cache error rate. Then, we propose a simple yet effective scheme, so-called Read Error Accumulation Preventer cache (REAP-cache), to completely eliminate the accumulation of read disturbances without compromising the cache performance. Our evaluations show that the proposed REAP-cache extends the cache Mean Time To Failure (MTTF) by 171x, while increases the cache area by less than 1% and energy consumption by only 2.7%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00450v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elham Cheshmikhani, Hamed Farbeh, Hossein Asadi</dc:creator>
    </item>
    <item>
      <title>ROBIN: Incremental Oblique Interleaved ECC for Reliability Improvement in STT-MRAM Caches</title>
      <link>https://arxiv.org/abs/2601.00456</link>
      <description>arXiv:2601.00456v1 Announce Type: new 
Abstract: Spin-Transfer Torque Magnetic RAM} (STT-MRAM) is a promising alternative for SRAMs in on-chip cache memories. Besides all its advantages, high error rate in STT-MRAM is a major limiting factor for on-chip cache memories. In this paper, we first present a comprehensive analysis that reveals that the conventional Error-Correcting Codes (ECCs) lose their efficiency due to data-dependent error patterns, and then propose an efficient ECC configuration, so-called ROBIN, to improve the correction capability. The evaluations show that the inefficiency of conventional ECC increases the cache error rate by an average of 151.7% while ROBIN reduces this value by more than 28.6x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00456v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Elham Cheshmikhani, Hamed Farbeh, Hossein Asadi</dc:creator>
    </item>
    <item>
      <title>Toward Large-Scale Photonics-Empowered AI Systems: From Physical Design Automation to System-Algorithm Co-Exploration</title>
      <link>https://arxiv.org/abs/2601.00129</link>
      <description>arXiv:2601.00129v1 Announce Type: cross 
Abstract: In this work, we identify three considerations that are essential for realizing practical photonic AI systems at scale: (1) dynamic tensor operation support for modern models rather than only weight-static kernels, especially for attention/Transformer-style workloads; (2) systematic management of conversion, control, and data-movement overheads, where multiplexing and dataflow must amortize electronic costs instead of letting ADC/DAC and I/O dominate; and (3) robustness under hardware non-idealities that become more severe as integration density grows. To study these coupled tradeoffs quantitatively, and to ensure they remain meaningful under real implementation constraints, we build a cross-layer toolchain that supports photonic AI design from early exploration to physical realization. SimPhony provides implementation-aware modeling and rapid cross-layer evaluation, translating physical costs into system-level metrics so architectural decisions are grounded in realistic assumptions. ADEPT and ADEPT-Z enable end-to-end circuit and topology exploration, connecting system objectives to feasible photonic fabrics under practical device and circuit constraints. Finally, Apollo and LiDAR provide scalable photonic physical design automation, turning candidate circuits into manufacturable layouts while accounting for routing, thermal, and crosstalk constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00129v1</guid>
      <category>physics.optics</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziang Yin, Hongjian Zhou, Nicholas Gangi, Meng Zhang, Jeff Zhang, Zhaoran Rena Huang, Jiaqi Gu</dc:creator>
    </item>
    <item>
      <title>Democratizing Electronic-Photonic AI Systems: An Open-Source AI-Infused Cross-Layer Co-Design and Design Automation Toolflow</title>
      <link>https://arxiv.org/abs/2601.00130</link>
      <description>arXiv:2601.00130v1 Announce Type: cross 
Abstract: Photonics is becoming a cornerstone technology for high-performance AI systems and scientific computing, offering unparalleled speed, parallelism, and energy efficiency. Despite this promise, the design and deployment of electronic-photonic AI systems remain highly challenging due to a steep learning curve across multiple layers, spanning device physics, circuit design, system architecture, and AI algorithms. The absence of a mature electronic-photonic design automation (EPDA) toolchain leads to long, inefficient design cycles and limits cross-disciplinary innovation and co-evolution. In this work, we present a cross-layer co-design and automation framework aimed at democratizing photonic AI system development. We begin by introducing our architecture designs for scalable photonic edge AI and Transformer inference, followed by SimPhony, an open-source modeling tool for rapid EPIC AI system evaluation and design-space exploration. We then highlight advances in AI-enabled photonic design automation, including physical AI-based Maxwell solvers, a fabrication-aware inverse design framework, and a scalable inverse training algorithm for meta-optical neural networks, enabling a scalable EPDA stack for next-generation electronic-photonic AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00130v1</guid>
      <category>physics.optics</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongjian Zhou, Ziang Yin, Jiaqi Gu</dc:creator>
    </item>
    <item>
      <title>Splitting Precoding with Subspace Selection and Quantized Refinement for Massive MIMO</title>
      <link>https://arxiv.org/abs/2601.00616</link>
      <description>arXiv:2601.00616v1 Announce Type: cross 
Abstract: Limited fronthaul capacity is a practical bottleneck in massive multiple-input multiple-output (MIMO) 5G architectures, where a base station (BS) consists of an advanced antenna system (AAS) connected to a baseband unit (BBU). Conventional downlink designs place the entire precoding computation at the BBU and transmit a high-dimensional precoding matrix over the fronthaul, resulting in substantial quantization losses and signaling overhead. This letter proposes a splitting precoding architecture that separates the design between the AAS and BBU. The AAS performs a local subspace selection to reduce the channel dimensionality, while the BBU computes an optimized quantized refinement precoding based on the resulting effective channel. The numerical results show that the proposed splitting precoding strategy achieves higher sum spectral efficiency than conventional one-stage precoding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.00616v1</guid>
      <category>eess.SP</category>
      <category>cs.AR</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasaman Khorsandmanesh, Emil Bjornson, Joakim Jalden</dc:creator>
    </item>
    <item>
      <title>Enthuse: Efficient Adaptable High-throughput Streaming Aggregation Engines</title>
      <link>https://arxiv.org/abs/2405.18168</link>
      <description>arXiv:2405.18168v4 Announce Type: replace 
Abstract: Aggregation queries are a series of computationally-demanding analytics operations on counted, grouped or time series data. They include tasks such as summation or finding the median among the items of the same group, and within a specified number of the last observed tuples for sliding window aggregation (SWAG). They have a wide range of applications including database analytics, operating systems, bank security and medical sensors. Existing challenges include the hardware complexity that comes with efficiently handling per-group states using hash-based approaches. This paper presents Enthuse, an adaptable pipeline for calculating a wide range of aggregation queries with high throughput. It is then adapted for SWAG and achieves up to 476x speedup over the CPU core of the same platform. It achieves unparalleled levels of performance and functionality such as a throughput of 1 GT/s on our setup for SWAG without groups, and more advanced operators with up to 4x the window sizes than the state-of-the-art with groups as an approximation for SWAG featuring per-group windows using a fraction of the resources and no DRAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18168v4</guid>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <pubDate>Mon, 05 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippos Papaphilippou, Wayne Luk</dc:creator>
    </item>
  </channel>
</rss>
