<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On the Power of Spatial Locality on Online Routing Problems</title>
      <link>https://arxiv.org/abs/2506.17517</link>
      <description>arXiv:2506.17517v1 Announce Type: new 
Abstract: We consider the online versions of two fundamental routing problems, traveling salesman (TSP) and dial-a-ride (DARP), which have a variety of relevant applications in logistics and robotics. The online versions of these problems concern with efficiently serving a sequence of requests presented in a real-time on-line fashion located at points of a metric space by servers (salesmen/vehicles/robots). In this paper, motivated from real-world applications, such as Uber/Lyft rides, where some limited knowledge is available on the future requests, we propose the {\em spatial locality} model that provides in advance the distance within which new request(s) will be released from the current position of server(s). We study the usefulness of this advanced information on achieving the improved competitive ratios for both the problems with $k\geq 1$ servers, compared to the competitive results established in the literature without such spatial locality consideration. We show that small locality is indeed useful in obtaining improved competitive ratios irrespective of the metric space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17517v1</guid>
      <category>cs.DS</category>
      <category>cs.MA</category>
      <category>cs.RO</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swapnil Guragain, Gokarna Sharma</dc:creator>
    </item>
    <item>
      <title>Structural Optimal Jacobian Accumulation and Minimum Edge Count are NP-Complete Under Vertex Elimination</title>
      <link>https://arxiv.org/abs/2506.17521</link>
      <description>arXiv:2506.17521v1 Announce Type: new 
Abstract: We study graph-theoretic formulations of two fundamental problems in algorithmic differentiation. The first (Structural Optimal Jacobian Accumulation) is that of computing a Jacobian while minimizing multiplications. The second (Minimum Edge Count) is to find a minimum-size computational graph. For both problems, we consider the vertex elimination operation. Our main contribution is to show that both problems are NP-complete, thus resolving longstanding open questions. In contrast to prior work, our reduction for Structural Optimal Jacobian Accumulation does not rely on any assumptions about the algebraic relationships between local partial derivatives; we allow these values to be mutually independent. We also provide $O^*(2^n)$-time exact algorithms for both problems, and show that under the exponential time hypothesis these running times are essentially tight. Finally, we provide a data reduction rule for Structural Optimal Jacobian Accumulation by showing that false twins may always be eliminated consecutively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17521v1</guid>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Bentert, Alex Crane, P{\aa}l Gr{\o}n{\aa}s Drange, Yosuke Mizutani, Blair D. Sullivan</dc:creator>
    </item>
    <item>
      <title>Faster Low-Rank Approximation and Kernel Ridge Regression via the Block-Nystr\"om Method</title>
      <link>https://arxiv.org/abs/2506.17556</link>
      <description>arXiv:2506.17556v1 Announce Type: new 
Abstract: The Nystr\"om method is a popular low-rank approximation technique for large matrices that arise in kernel methods and convex optimization. Yet, when the data exhibits heavy-tailed spectral decay, the effective dimension of the problem often becomes so large that even the Nystr\"om method may be outside of our computational budget. To address this, we propose Block-Nystr\"om, an algorithm that injects a block-diagonal structure into the Nystr\"om method, thereby significantly reducing its computational cost while recovering strong approximation guarantees. We show that Block-Nystr\"om can be used to construct improved preconditioners for second-order optimization, as well as to efficiently solve kernel ridge regression for statistical learning over Hilbert spaces. Our key technical insight is that, within the same computational budget, combining several smaller Nystr\"om approximations leads to stronger tail estimates of the input spectrum than using one larger approximation. Along the way, we provide a novel recursive preconditioning scheme for efficiently inverting the Block-Nystr\"om matrix, and provide new statistical learning bounds for a broad class of approximate kernel ridge regression solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17556v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sachin Garg, Micha{\l} Derezi\'nski</dc:creator>
    </item>
    <item>
      <title>Contextual Pattern Mining and Counting</title>
      <link>https://arxiv.org/abs/2506.17613</link>
      <description>arXiv:2506.17613v1 Announce Type: new 
Abstract: Given a string $P$ of length $m$, a longer string $T$ of length $n&gt;m$, and two integers $l\geq 0$ and $r\geq 0$, the context of $P$ in $T$ is the set of all string pairs $(L,R)$, with $|L|=l$ and $|R|=r$, such that the string $LPR$ occurs in $T$. We introduce two problems related to the notion of context: (1) the Contextual Pattern Mining (CPM) problem, which given $T$, $(m,l,r)$, and an integer $\tau&gt;0$, asks for outputting the context of each substring $P$ of length $m$ of $T$, provided that the size of the context of $P$ is at least $\tau$; and (2) the Contextual Pattern Counting (CPC) problem, which asks for preprocessing $T$ so that the size of the context of a given query string $P$ of length $m$ can be found efficiently.
  For CPM, we propose a linear-work algorithm that either uses only internal memory, or a bounded amount of internal memory and external memory, which allows much larger datasets to be handled. For CPC, we propose an $\widetilde{\mathcal{O}}(n)$-space index that can be constructed in $\widetilde{\mathcal{O}}n)$ time and answers queries in $\mathcal{O}(m)+\widetilde{\mathcal{O}}(1)$ time. We further improve the practical performance of the CPC index by optimizations that exploit the LZ77 factorization of $T$ and an upper bound on the query length. Using billion-letter datasets from different domains, we show that the external memory version of our CPM algorithm can deal with very large datasets using a small amount of internal memory while its runtime is comparable to that of the internal memory version. Interestingly, we also show that our optimized index for CPC outperforms an approach based on the state of the art for the reporting version of CPC [Navarro, SPIRE 2020] in terms of query time, index size, construction time, and construction space, often by more than an order of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17613v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ling Li, Daniel Gibney, Sharma V. Thankachan, Solon P. Pissis, Grigorios Loukides</dc:creator>
    </item>
    <item>
      <title>Optimizing Periodic Operations for Efficient Inland Waterway Lock Management</title>
      <link>https://arxiv.org/abs/2506.17743</link>
      <description>arXiv:2506.17743v1 Announce Type: new 
Abstract: In inland waterways, the efficient management of water lock operations impacts the level of congestion and the resulting uncertainty in inland waterway transportation. To achieve reliable and efficient traffic, schedules should be easy to understand and implement, reducing the likelihood of errors. The simplest schedules follow periodic patterns, reducing complexity and facilitating predictable management. Since vessels do not arrive in perfectly regular intervals, periodic schedules may lead to more wait time. The aim of this research is to estimate this cost by evaluating how effective these periodic schedules manage vessel traffic at water locks. The first objective is to estimate a periodic arrival pattern that closely matches a dataset of irregular vessel arrivals at a specific lock. We develop an algorithm that, given a fixed number of vessel streams, solves the problem in polynomial time. The solution then serves as input for the subsequent part, where we consider algorithms that compute operational schedules by formulating an optimisation problem with periodic arrival patterns as input, and the goal is to determine a periodic schedule that minimises the long-run average waiting time of vessels. We present a polynomial-time algorithm for the two-stream case and a pseudo-polynomial-time algorithm for the general case, along with incremental polynomial-time approximation schemes. In our numerical experiments, use AIS data to construct a periodic arrival pattern closely matching the observed data. Our experiments demonstrate that when evaluated against actual data, intuitive and straightforward policies often outperform optimal policies specifically trained on the periodic arrival pattern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17743v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Golak, Alexander Grigoriev, Freija van Lent, Tom van der Zanden</dc:creator>
    </item>
    <item>
      <title>Semirandom Planted Clique via 1-norm Isometry Property</title>
      <link>https://arxiv.org/abs/2506.17916</link>
      <description>arXiv:2506.17916v1 Announce Type: new 
Abstract: We give a polynomial-time algorithm that finds a planted clique of size $k \ge \sqrt{n \log n}$ in the semirandom model, improving the state-of-the-art $\sqrt{n} (\log n)^2$ bound. This $\textit{semirandom planted clique problem}$ concerns finding the planted subset $S$ of $k$ vertices of a graph $G$ on $V$, where the induced subgraph $G[S]$ is complete, the cut edges in $G[S; V \setminus S]$ are random, and the remaining edges in $G[V \setminus S]$ are adversarial.
  An elegant greedy algorithm by Blasiok, Buhai, Kothari, and Steurer [BBK24] finds $S$ by sampling inner products of the columns of the adjacency matrix of $G$, and checking if they deviate significantly from typical inner products of random vectors. Their analysis uses a suitably random matrix that, with high probability, satisfies a certain restricted isometry property. Inspired by Wootters's work on list decoding, we put forth and implement the $1$-norm analog of this argument, and quantitatively improve their analysis to work all the way up to the conjectured optimal $\sqrt{n \log n}$ bound on clique size, answering one of the main open questions posed in [BBK24].</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17916v1</guid>
      <category>cs.DS</category>
      <category>math.PR</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Venkatesan Guruswami, Hsin-Po Wang</dc:creator>
    </item>
    <item>
      <title>Fully-Dynamic Parallel Algorithms for Single-Linkage Clustering</title>
      <link>https://arxiv.org/abs/2506.18384</link>
      <description>arXiv:2506.18384v1 Announce Type: new 
Abstract: Single-linkage clustering is a popular form of hierarchical agglomerative clustering (HAC) where the distance between two clusters is defined as the minimum distance between any pair of points across the two clusters. In single-linkage HAC, the output is typically the single-linkage dendrogram (SLD), which is the binary tree representing the hierarchy of clusters formed by iteratively contracting the two closest clusters. In the dynamic setting, prior work has only studied maintaining a minimum spanning forest over the data since single-linkage HAC reduces to computing the SLD on the minimum spanning forest of the data.
  In this paper, we study the problem of maintaining the SLD in the fully-dynamic setting. We assume the input is a dynamic forest $F$ (representing the minimum spanning forest of the data) which receives a sequence of edge insertions and edge deletions. To our knowledge, no prior work has provided algorithms to update an SLD asymptotically faster than recomputing it from scratch. All of our update algorithms are asymptotically faster than the best known static SLD computation algorithm, which takes $O(n \log h)$ time where $h$ is the height of the dendrogram ($h \leq n-1$). Furthermore, our algorithms are much faster in many cases, such as when $h$ is low. Our first set of results are an insertion algorithm in $O(h)$ time and a deletion algorithm in $O(h \log (1+n/h))$ time. Next, we describe parallel and batch-parallel versions of these algorithms which are work-efficient or nearly work-efficient and have poly-logarithmic depth. Finally, we show how to perform insertions near-optimally in $O(c \log(1+n/c))$ time, where $c$ is the number of structural changes in the dendrogram caused by the update, and give a work-efficient parallel version of this algorithm that has polylogarithmic depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18384v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Quinten De Man, Laxman Dhulipala, Kishen N Gowda</dc:creator>
    </item>
    <item>
      <title>Tight simulation of a distribution using conditional samples</title>
      <link>https://arxiv.org/abs/2506.18444</link>
      <description>arXiv:2506.18444v1 Announce Type: new 
Abstract: We present an algorithm for simulating a distribution using prefix conditional samples (Adar, Fischer and Levi, 2024), as well as ``prefix-compatible'' conditional models such as the interval model (Cannone, Ron and Servedio, 2015) and the subcube model (CRS15, Bhattacharyya and Chakraborty, 2018). The conditional sample complexity is $O(\log^2 N / \varepsilon^2)$ prefix conditional samples per query, which improves on the previously known $\tilde{O}(\log^3 N / \varepsilon^2)$ (Kumar, Meel and Pote, 2025). Moreover, our simulating distribution is $O(\varepsilon^2)$-close to the input distribution with respect to the Kullback-Leibler divergence, which is stricter than the usual guarantee of being $O(\varepsilon)$-close with respect to the total-variation distance.
  We show that our algorithm is tight with respect to the highly-related task of estimation: every algorithm that is able to estimate the mass of individual elements within $(1 \pm \varepsilon)$-multiplicative error must make $\Omega(\log^2 N / \varepsilon^2)$ prefix conditional samples per element.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18444v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomer Adar</dc:creator>
    </item>
    <item>
      <title>Near-Optimal Dynamic Policies for Joint Replenishment in Continuous/Discrete Time</title>
      <link>https://arxiv.org/abs/2506.18491</link>
      <description>arXiv:2506.18491v1 Announce Type: new 
Abstract: While dynamic policies have historically formed the foundation of most influential papers dedicated to the joint replenishment problem, we are still facing profound gaps in our structural understanding of optimal such policies as well as in their surrounding computational questions. To date, the seminal work of Roundy (1985, 1986) and Jackson et al. (1985) remains unsurpassed in efficiently developing provably-good dynamic policies in this context.
  The principal contribution of this paper consists in developing a wide range of algorithmic ideas and analytical insights around the continuous-time joint replenishment problem, culminating in a deterministic framework for efficiently approximating optimal dynamic policies to any desired level of accuracy. These advances enable us to derive a compactly-encoded replenishment policy whose long-run average cost is within factor $1 + \epsilon$ of the dynamic optimum, arriving at an efficient polynomial-time approximation scheme (EPTAS). Technically speaking, our approach hinges on affirmative resolutions to two fundamental open questions:
  -- We devise the first efficient discretization-based framework for approximating the joint replenishment problem. Specifically, we prove that every continuous-time infinite-horizon instance can be reduced to a corresponding discrete-time $O( \frac{ n^3 }{ \epsilon^6 } )$-period instance, while incurring a multiplicative optimality loss of at most $1 + \epsilon$.
  -- Motivated by this relation, we substantially improve on the $O( 2^{2^{O(1/\epsilon)}} \cdot (nT)^{ O(1) } )$-time approximation scheme of Nonner and Sviridenko (2013) for the discrete-time joint replenishment problem. Beyond an exponential improvement in running time, we demonstrate that randomization and hierarchical decompositions can be entirely avoided, while concurrently offering a relatively simple analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18491v1</guid>
      <category>cs.DS</category>
      <category>math.OC</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Danny Segev</dc:creator>
    </item>
    <item>
      <title>AlgoSelect: Universal Algorithm Selection via the Comb Operator</title>
      <link>https://arxiv.org/abs/2506.17304</link>
      <description>arXiv:2506.17304v1 Announce Type: cross 
Abstract: We introduce AlgoSelect, a principled framework for learning optimal algorithm selection from data, centered around the novel Comb Operator. Given a set of algorithms and a feature representation of problems, AlgoSelect learns to interpolate between diverse computational approaches. For pairs of algorithms, a simple sigmoid-gated selector, an instance of the Comb Operator, facilitates this interpolation. We extend this to an N-Path Comb for multiple algorithms. We prove that this framework is universal (can approximate any algorithm selector), information-theoretically optimal in its learnability (thresholds for selection converge almost surely, demonstrated via Borel-Cantelli arguments), computationally efficient, and robust. Key theoretical contributions include: (1) a universal approximation theorem demonstrating that Comb-based selectors can achieve arbitrary accuracy; (2) information-theoretic learnability for selection thresholds; (3) formalization of the Comb Operator within linear operator theory, detailing its boundedness and spectral properties; (4) an N-Path Comb generalization for multi-algorithm selection; and (5) a practical learning framework for the adaptive seeding functions that guide the Comb Operator. Empirical validation on a comprehensive 20$\times$20 problem-algorithm study demonstrates near-perfect selection (99.9\%+ accuracy) with remarkably few samples and rapid convergence, revealing that $H(\text{Algorithm}|\text{Problem}) \approx 0$ in structured domains. AlgoSelect provides a theoretically grounded, practically deployable solution to automated algorithm selection with provable optimality and learnability guarantees, with significant implications for AI and adaptive systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17304v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasper Yao</dc:creator>
    </item>
    <item>
      <title>Dual-Hierarchy Labelling: Scaling Up Distance Queries on Dynamic Road Networks</title>
      <link>https://arxiv.org/abs/2506.18013</link>
      <description>arXiv:2506.18013v1 Announce Type: cross 
Abstract: Computing the shortest-path distance between any two given vertices in road networks is an important problem. A tremendous amount of research has been conducted to address this problem, most of which are limited to static road networks. Since road networks undergo various real-time traffic conditions, there is a pressing need to address this problem for dynamic road networks. Existing state-of-the-art methods incrementally maintain an indexing structure to reflect dynamic changes on road networks. However, these methods suffer from either slow query response time or poor maintenance performance, particularly when road networks are large. In this work, we propose an efficient solution \emph{Dual-Hierarchy Labelling (DHL)} for distance querying on dynamic road networks from a novel perspective, which incorporates two hierarchies with different but complementary data structures to support efficient query and update processing. Specifically, our proposed solution is comprised of three main components: \emph{query hierarchy}, \emph{update hierarchy}, and \emph{hierarchical labelling}, where \emph{query hierarchy} enables efficient query answering by exploring only a small subset of vertices in the labels of two query vertices and \emph{update hierarchy} supports efficient maintenance of distance labelling under edge weight increase or decrease. We further develop dynamic algorithms to reflect dynamic changes by efficiently maintaining the update hierarchy and hierarchical labelling. We also propose a parallel variant of our dynamic algorithms by exploiting labelling structure. We evaluate our methods on 10 large road networks and it shows that our methods significantly outperform the state-of-the-art methods, i.e., achieving considerably faster construction and update time, while being consistently 2-4 times faster in terms of query processing and consuming only 10\%-20\% labelling space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18013v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Farhan, Henning Koehler, Qing Wang</dc:creator>
    </item>
    <item>
      <title>Continuous Map Matching to Paths under Travel Time Constraints</title>
      <link>https://arxiv.org/abs/2506.18354</link>
      <description>arXiv:2506.18354v1 Announce Type: cross 
Abstract: In this paper, we study the problem of map matching with travel time constraints. Given a sequence of $k$ spatio-temporal measurements and an embedded path graph with travel time costs, the goal is to snap each measurement to a close-by location in the graph, such that consecutive locations can be reached from one another along the path within the timestamp difference of the respective measurements. This problem arises in public transit data processing as well as in map matching of movement trajectories to general graphs. We show that the classical approach for this problem, which relies on selecting a finite set of candidate locations in the graph for each measurement, cannot guarantee to find a consistent solution. We propose a new algorithm that can deal with an infinite set of candidate locations per measurement. We prove that our algorithm always detects a consistent map matching path (if one exists). Despite the enlarged candidate set, we also demonstrate that our algorithm has superior running time in theory and practice. For a path graph with $n$ nodes, we show that our algorithm runs in $\mathcal{O}(k^2 n \log {nk})$ and under mild assumptions in $\mathcal{O}(k n ^\lambda + n \log^3 n)$ for $\lambda \approx 0.695$. This is a significant improvement over the baseline, which runs in $\mathcal{O}(k n^2)$ and which might not even identify a correct solution. The performance of our algorithm hinges on an efficient segment-circle intersection data structure. We describe how to design and implement such a data structure for our application. In the experimental evaluation, we demonstrate the usefulness of our novel algorithm on a diverse set of generated measurements as well as GTFS data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18354v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannick Bosch, Sabine Storandt</dc:creator>
    </item>
    <item>
      <title>Perfect phylogenies via the Minimum Uncovering Branching problem: efficiently solvable cases</title>
      <link>https://arxiv.org/abs/2506.18578</link>
      <description>arXiv:2506.18578v1 Announce Type: cross 
Abstract: In this paper, we present new efficiently solvable cases of the Minimum Uncovering Branching problem, an optimization problem with applications in cancer genomics introduced by Hujdurovi\'c, Husi\'c, Milani\v{c}, Rizzi, and Tomescu in 2018. The problem involves a family of finite sets, and the goal is to map each non-maximal set to exactly one set that contains it, minimizing the sum of uncovered elements across all sets in the family. Hujdurovi\'c et al. formulated the problem in terms of branchings of the digraph formed by the proper set inclusion relation on the input sets and studied the problem complexity based on properties of the corresponding partially ordered set, in particular, with respect to its height and width, defined respectively as the maximum cardinality of a chain and an antichain. They showed that the problem is APX-complete for instances of bounded height and that a constant-factor approximation algorithm exists for instances of bounded width, but left the exact complexity for bounded-width instances open. In this paper, we answer this question by proving that the problem is solvable in polynomial time. We derive this result by examining the structural properties of optimal solutions and reducing the problem to computing maximum matchings in bipartite graphs and maximum weight antichains in partially ordered sets. We also introduce a new polynomially computable lower bound and identify another condition for polynomial-time solvability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18578v1</guid>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TCBBIO.2025.3580476</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Computational Biology and Bioinformatics 2025</arxiv:journal_reference>
      <dc:creator>Narmina Baghirova, Esther Galby, Martin Milani\v{c}</dc:creator>
    </item>
    <item>
      <title>Semidefinite Programming for the Asymmetric Stochastic Block Model</title>
      <link>https://arxiv.org/abs/2506.18754</link>
      <description>arXiv:2506.18754v1 Announce Type: cross 
Abstract: We consider semidefinite programming (SDP) for the binary stochastic block model with equal-sized communities. Prior work of Hajek, Wu, and Xu proposed an SDP (sym-SDP) for the symmetric case where the intra-community edge probabilities are equal, and showed that the SDP achieves the information-theoretic threshold for exact recovery under the symmetry assumption. A key open question is whether SDPs can be used to achieve exact recovery for non-symmetric block models. In order to inform the design of a new SDP for the non-symmetric setting, we investigate the failure of sym-SDP when it is applied to non-symmetric settings. We formally show that sym-SDP fails to return the correct labeling of the vertices in some information-theoretically feasible, asymmetric cases. In addition, we give an intuitive geometric interpretation of the failure of sym-SDP in asymmetric settings, which in turn suggests an SDP formulation to handle the asymmetric setting. Still, this new SDP cannot be readily analyzed by existing techniques, suggesting a fundamental limitation in the design of SDPs for community detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18754v1</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>math.IT</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Julia Gaudio, Phawin Prongpaophan</dc:creator>
    </item>
    <item>
      <title>Universal Solvability for Robot Motion Planning on Graphs</title>
      <link>https://arxiv.org/abs/2506.18755</link>
      <description>arXiv:2506.18755v1 Announce Type: cross 
Abstract: We study the Universal Solvability of Robot Motion Planning on Graphs (USolR) problem: given an undirected graph G = (V, E) and p robots, determine whether any arbitrary configuration of the robots can be transformed into any other arbitrary configuration via a sequence of valid, collision-free moves. We design a canonical accumulation procedure that maps arbitrary configurations to configurations that occupy a fixed subset of vertices, enabling us to analyze configuration reachability in terms of equivalence classes. We prove that in instances that are not universally solvable, at least half of all configurations are unreachable from a given one, and leverage this to design an efficient randomized algorithm with one-sided error, which can be derandomized with a blow-up in the running time by a factor of p. Further, we optimize our deterministic algorithm by using the structure of the input graph G = (V, E), achieving a running time of O(p * (|V| + |E|)) in sparse graphs and O(|V| + |E|) in dense graphs. Finally, we consider the Graph Edge Augmentation for Universal Solvability (EAUS) problem, where given a connected graph G that is not universally solvable for p robots, the question is to check if for a given budget b, at most b edges can be added to G to make it universally solvable for p robots. We provide an upper bound of p - 2 on b for general graphs. On the other hand, we also provide examples of graphs that require Theta(p) edges to be added. We further study the Graph Vertex and Edge Augmentation for Universal Solvability (VEAUS) problem, where a vertices and b edges can be added, and we provide lower bounds on a and b.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.18755v1</guid>
      <category>cs.CC</category>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anubhav Dhar, Ashlesha Hota, Sudeshna Kolay, Pranav Nyati, Tanishq Prasad</dc:creator>
    </item>
    <item>
      <title>Efficient stream-based Max-Min diversification with minimal failure rate</title>
      <link>https://arxiv.org/abs/2011.10659</link>
      <description>arXiv:2011.10659v2 Announce Type: replace 
Abstract: The streaming max-min diversification problem concerns the selection of a limited and diverse sample of items out of a data stream of known finite length. The objective to be maximized is the minimum distance among any pair of selected items. We consider the irrevocable-choice sampling, where decisions need to be immediate and irrevocable while processing the items of the stream, which is a setting little studied in the literature. Standard algorithmic approaches for sequential selection disregard selection failures, which is when the last items of the stream are picked by default, to prevent delivering an incomplete selection set. This defect can be catastrophic for the max-min diversification objective. The proposed Failure Rate Minimization (FRM) is a rank-based algorithm that selects a set of diverse items and, in addition, reduces significantly the probability of having failures. We demonstrate with simulations FRM's performance comparing with existing selection strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.10659v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Argyris Kalogeratos, Yutai Nazir Zhao, Mathilde Fekom</dc:creator>
    </item>
    <item>
      <title>Sharper Bounds for Chebyshev Moment Matching, with Applications</title>
      <link>https://arxiv.org/abs/2408.12385</link>
      <description>arXiv:2408.12385v2 Announce Type: replace 
Abstract: We study the problem of approximately recovering a probability distribution given noisy measurements of its Chebyshev polynomial moments. This problem arises broadly across algorithms, statistics, and machine learning. By leveraging a global decay bound on the coefficients in the Chebyshev expansion of any Lipschitz function, we sharpen prior work, proving that accurate recovery in the Wasserstein distance is possible with more noise than previously known. Our result immediately yields a number of applications:
  1) We give a simple "linear query" algorithm for constructing a differentially private synthetic data distribution with Wasserstein-$1$ error $\tilde{O}(1/n)$ based on a dataset of $n$ points in $[-1,1]$. This bound is optimal up to log factors, and matches a recent result of Boedihardjo, Strohmer, and Vershynin [Probab. Theory. Rel., 2024], which uses a more complex "superregular random walk" method.
  2) We give an $\tilde{O}(n^2/\epsilon)$ time algorithm for the linear algebraic problem of estimating the spectral density of an $n\times n$ symmetric matrix up to $\epsilon$ error in the Wasserstein distance. Our result accelerates prior methods from Chen et al. [ICML 2021] and Braverman et al. [STOC 2022].
  3) We tighten an analysis of Vinayak, Kong, Valiant, and Kakade [ICML 2019] on the maximum likelihood estimator for the statistical problem of "Learning Populations of Parameters'', extending the parameter regime in which sample optimal results can be obtained.
  Beyond these main results, we provide an extension of our bound to estimating distributions in $d &gt; 1$ dimensions. We hope that these bounds will find applications more broadly to problems involving distribution recovery from noisy moment information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12385v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cameron Musco, Christopher Musco, Lucas Rosenblatt, Apoorv Vikram Singh</dc:creator>
    </item>
    <item>
      <title>Approximating the Held-Karp Bound for Metric TSP in Nearly Linear Work and Polylogarithmic Depth</title>
      <link>https://arxiv.org/abs/2411.14745</link>
      <description>arXiv:2411.14745v2 Announce Type: replace 
Abstract: We present a nearly linear work parallel algorithm for approximating the Held-Karp bound for the Metric TSP problem. Given an edge-weighted undirected graph $G=(V,E)$ on $m$ edges and $\epsilon&gt;0$, it returns a $(1+\epsilon)$-approximation to the Held-Karp bound with high probability, in $\tilde{O}(m/\epsilon^4)$ work and $\tilde{O}(1/\epsilon^4)$ depth. While a nearly linear time sequential algorithm was known for almost a decade (Chekuri and Quanrud'17), it was not known how to simultaneously achieve nearly linear work alongside polylogarithmic depth. Using a reduction by Chalermsook et al.'22, we also give a parallel algorithm for computing a $(1+\epsilon)$-approximate fractional solution to the $k$-edge-connected spanning subgraph (kECSS) problem, with similar complexity.
  To obtain these results, we introduce a notion of core-sequences for the parallel Multiplicative Weights Update (MWU) framework (Luby-Nisan'93, Young'01). For the Metric TSP and kECSS problems, core-sequences enable us to exploit the structure of approximate minimum cuts to reduce the cost per iteration and/or the number of iterations. The acceleration technique via core-sequences is generic and of independent interest. In particular, it improves the best-known iteration complexity of MWU algorithms for packing/covering LPs from $poly(\log nnz(A))$ to polylogarithmic in the product of cardinalities of the core-sequence sets, where $A$ is the constraint matrix of the LP. For certain implicitly defined LPs such as the kECSS LP, this yields an exponential improvement in depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14745v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuan Khye Koh, Omri Weinstein, Sorrachai Yingchareonthawornchai</dc:creator>
    </item>
    <item>
      <title>Treewidth Parameterized by Feedback Vertex Number</title>
      <link>https://arxiv.org/abs/2504.18302</link>
      <description>arXiv:2504.18302v2 Announce Type: replace 
Abstract: We provide the first algorithm for computing an optimal tree decomposition for a given graph $G$ that runs in single exponential time in the feedback vertex number of $G$, that is, in time $2^{O(\text{fvn}(G))}\cdot n^{O(1)}$, where $\text{fvn}(G)$ is the feedback vertex number of $G$ and $n$ is the number of vertices of $G$. On a classification level, this improves the previously known results by Chapelle et al. [Discrete Applied Mathematics '17] and Fomin et al. [Algorithmica '18], who independently showed that an optimal tree decomposition can be computed in single exponential time in the vertex cover number of $G$.
  One of the biggest open problems in the area of parameterized complexity is whether we can compute an optimal tree decomposition in single exponential time in the treewidth of the input graph. The currently best known algorithm by Korhonen and Lokshtanov [STOC '23] runs in $2^{O(\text{tw}(G)^2)}\cdot n^4$ time, where $\text{tw}(G)$ is the treewidth of $G$. Our algorithm improves upon this result on graphs $G$ where $\text{fvn}(G)\in o(\text{tw}(G)^2)$. On a different note, since $\text{fvn}(G)$ is an upper bound on $\text{tw}(G)$, our algorithm can also be seen either as an important step towards a positive resolution of the above-mentioned open problem, or, if its answer is negative, then a mark of the tractability border of single exponential time algorithms for the computation of treewidth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18302v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hendrik Molter, Meirav Zehavi, Amit Zivan</dc:creator>
    </item>
    <item>
      <title>On the fast convergence of minibatch heavy ball momentum</title>
      <link>https://arxiv.org/abs/2206.07553</link>
      <description>arXiv:2206.07553v5 Announce Type: replace-cross 
Abstract: Simple stochastic momentum methods are widely used in machine learning optimization, but their good practical performance is at odds with an absence of theoretical guarantees of acceleration in the literature. In this work, we aim to close the gap between theory and practice by showing that stochastic heavy ball momentum retains the fast linear rate of (deterministic) heavy ball momentum on quadratic optimization problems, at least when minibatching with a sufficiently large batch size. The algorithm we study can be interpreted as an accelerated randomized Kaczmarz algorithm with minibatching and heavy ball momentum. The analysis relies on carefully decomposing the momentum transition matrix, and using new spectral norm concentration bounds for products of independent random matrices. We provide numerical illustrations demonstrating that our bounds are reasonably sharp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.07553v5</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/imanum/drae033</arxiv:DOI>
      <dc:creator>Raghu Bollapragada, Tyler Chen, Rachel Ward</dc:creator>
    </item>
    <item>
      <title>Dynamic Pricing for Reusable Resources: The Power of Two Prices</title>
      <link>https://arxiv.org/abs/2308.13822</link>
      <description>arXiv:2308.13822v3 Announce Type: replace-cross 
Abstract: Motivated by real-world applications such as rental and cloud computing services, we investigate pricing for reusable resources. We consider a system where a single resource with a fixed number of identical copies serves customers with heterogeneous willingness-to-pay (WTP), and the usage duration distribution is general. Optimal dynamic policies are computationally intractable when usage durations are not memoryless, so existing literature has focused on static pricing, which incurs a steady-state performance loss of ${O}(\sqrt{c})$ compared to optimality when supply and demand scale with $c$. We propose a class of dynamic "stock-dependent" policies that 1) are computationally tractable and 2) can attain a steady-state performance loss of $o(\sqrt{c})$. We give parametric bounds based on the local shape of the reward function at the optimal fluid admission probability and show that the performance loss of stock-dependent policies can be as low as ${O}((\log{c})^2)$. We characterize the tight performance loss for stock-dependent policies and show that they can in fact be achieved by a simple two-price policy that sets a higher price when the stock is below some threshold and a lower price otherwise. We extend our results to settings with multiple resources and multiple customer classes. Finally, we demonstrate this "minimally dynamic" class of two-price policies performs well numerically, even in non-asymptotic settings, suggesting that a little dynamicity can go a long way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.13822v3</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago R. Balseiro, Will Ma, Wenxin Zhang</dc:creator>
    </item>
    <item>
      <title>Conformality of Minimal Transversals of Maximal Cliques</title>
      <link>https://arxiv.org/abs/2405.10789</link>
      <description>arXiv:2405.10789v2 Announce Type: replace-cross 
Abstract: Given a hypergraph $H$, the dual hypergraph of $H$ is the hypergraph of all minimal transversals of $H$. A hypergraph is conformal if it is the family of maximal cliques of a graph. In a recent work, Boros, Gurvich, Milani\v{c}, and Uno (Journal of Graph Theory, 2025) studied conformality of dual hypergraphs and proved several results related to this property, leading in particular to a polynomial-time algorithm for recognizing graphs in which all minimal transversals of maximal cliques have size at most $k$, for any fixed $k$. In this follow-up work, we provide a novel aspect to the study of graph clique transversals, by considering the dual conformality property from the perspective of graphs. More precisely, we study graphs for which the family of minimal transversals of maximal cliques is conformal. Such graphs are called clique dually conformal (CDC for short). It turns out that the class of CDC graphs is a rich generalization of the class of $P_4$-free graphs. As our main results, we completely characterize CDC graphs within the families of triangle-free graphs and split graphs. Both characterizations lead to polynomial-time recognition algorithms. Generalizing the fact that every $P_4$-free graph is CDC, we also show that the class of CDC graphs is closed under substitution, in the strong sense that substituting a graph $H$ for a vertex of a graph $G$ results in a CDC graph if and only if both $G$ and $H$ are CDC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10789v2</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Endre Boros, Vladimir Gurvich, Martin Milani\v{c}, Dmitry Tikhanovsky, Yushi Uno</dc:creator>
    </item>
    <item>
      <title>The Empirical Spectral Distribution of i.i.d. Random Matrices with Random Perturbations</title>
      <link>https://arxiv.org/abs/2410.21919</link>
      <description>arXiv:2410.21919v3 Announce Type: replace-cross 
Abstract: A large i.i.d. random matrix with deterministic low-rank perturbation has been extensively studied, particularly in the aspects of the ESD (Empirical Spectral Distribution) and the outliers of eigenvalues. In this work, we investigate the analogous scenario where the perturbation is random and extend the previous results from the deterministic perturbation to the random case. Specifically, we consider an i.i.d. matrix with random perturbation, $\mathbf{M}$. Our results show that: (i) the eigenvalue outliers of $\mathbf{M}$ converge to the eigenvalues of its perturbation; (ii) the ESD of $\mathbf{M}$ converges to the circular law; (iii) the eigenvector alignment holds for specific perturbations.
  As an application of the above random matrices, we present the first optimal query complexity lower bound for approximating the top eigenvector of asymmetric matrices. In the inverse polynomial accuracy regime, the complexity matches the upper bounds that can be obtained via the power method. As far as we know, it is the first lower bound for approximating the eigenvector of an asymmetric matrix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21919v3</guid>
      <category>cs.IT</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kun Chen, Zhihua Zhang</dc:creator>
    </item>
    <item>
      <title>Accelerating Proximal Gradient Descent via Silver Stepsizes</title>
      <link>https://arxiv.org/abs/2412.05497</link>
      <description>arXiv:2412.05497v2 Announce Type: replace-cross 
Abstract: Surprisingly, recent work has shown that gradient descent can be accelerated without using momentum -- just by judiciously choosing stepsizes. An open question raised by several papers is whether this phenomenon of stepsize-based acceleration holds more generally for constrained and/or composite convex optimization via projected and/or proximal versions of gradient descent. We answer this in the affirmative by proving that the silver stepsize schedule yields analogously accelerated rates in these settings. These rates are conjectured to be asymptotically optimal among all stepsize schedules, and match the silver convergence rate of vanilla gradient descent (Altschuler and Parrilo, 2024, 2025), namely $O(\varepsilon^{- \log_{\rho} 2})$ for smooth convex optimization and $O(\kappa^{\log_\rho 2} \log \frac{1}{\varepsilon})$ under strong convexity, where $\varepsilon$ is the precision, $\kappa$ is the condition number, and $\rho = 1 + \sqrt{2}$ is the silver ratio. The key technical insight is the combination of recursive gluing -- the technique underlying all analyses of gradient descent accelerated with time-varying stepsizes -- with a certain Laplacian-structured sum-of-squares certificate for the analysis of proximal point updates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05497v2</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 24 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinho Bok, Jason M. Altschuler</dc:creator>
    </item>
  </channel>
</rss>
