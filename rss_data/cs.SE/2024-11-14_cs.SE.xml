<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 15 Nov 2024 02:35:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fault Localization in Deep Learning-based Software: A System-level Approach</title>
      <link>https://arxiv.org/abs/2411.08172</link>
      <description>arXiv:2411.08172v1 Announce Type: new 
Abstract: Over the past decade, Deep Learning (DL) has become an integral part of our daily lives. This surge in DL usage has heightened the need for developing reliable DL software systems. Given that fault localization is a critical task in reliability assessment, researchers have proposed several fault localization techniques for DL-based software, primarily focusing on faults within the DL model. While the DL model is central to DL components, there are other elements that significantly impact the performance of DL components. As a result, fault localization methods that concentrate solely on the DL model overlook a large portion of the system. To address this, we introduce FL4Deep, a system-level fault localization approach considering the entire DL development pipeline to effectively localize faults across the DL-based systems. In an evaluation using 100 faulty DL scripts, FL4Deep outperformed four previous approaches in terms of accuracy for three out of six DL-related faults, including issues related to data (84%), mismatched libraries between training and deployment (100%), and loss function (69%). Additionally, FL4Deep demonstrated superior precision and recall in fault localization for five categories of faults including three mentioned fault types in terms of accuracy, plus insufficient training iteration and activation function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08172v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Mehdi Morovati, Amin Nikanjam, Foutse Khomh</dc:creator>
    </item>
    <item>
      <title>VALTEST: Automated Validation of Language Model Generated Test Cases</title>
      <link>https://arxiv.org/abs/2411.08254</link>
      <description>arXiv:2411.08254v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have demonstrated significant potential in automating software testing, specifically in generating unit test cases. However, the validation of LLM-generated test cases remains a challenge, particularly when the ground truth is unavailable. This paper introduces VALTEST, a novel framework designed to automatically validate test cases generated by LLMs by leveraging token probabilities. We evaluate VALTEST using nine test suites generated from three datasets (HumanEval, MBPP, and LeetCode) across three LLMs (GPT-4o, GPT-3.5-turbo, and LLama3.1 8b). By extracting statistical features from token probabilities, we train a machine learning model to predict test case validity. VALTEST increases the validity rate of test cases by 6.2% to 24%, depending on the dataset and LLM. Our results suggest that token probabilities are reliable indicators for distinguishing between valid and invalid test cases, which provides a robust solution for improving the correctness of LLM-generated test cases in software testing. In addition, we found that replacing the identified invalid test cases by VALTEST, using a Chain-of-Thought prompting results in a more effective test suite while keeping the high validity rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08254v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Hamed Taherkhani, Hadi Hemmati</dc:creator>
    </item>
    <item>
      <title>TimeLess: A Vision for the Next Generation of Software Development</title>
      <link>https://arxiv.org/abs/2411.08507</link>
      <description>arXiv:2411.08507v1 Announce Type: new 
Abstract: Present-day software development faces three major challenges: complexity, time consumption, and high costs. Developing large software systems often requires battalions of teams and considerable time for meetings, which end without any action, resulting in unproductive cycles, delayed progress, and increased cost. What if, instead of large meetings with no immediate results, the software product is completed by the end of the meeting? In response, we present a vision for a system called TimeLess, designed to reshape the software development process by enabling immediate action during meetings. The goal is to shift meetings from planning discussions to productive, action-oriented sessions. This approach minimizes the time and effort required for development, allowing teams to focus on critical decision-making while AI agents execute development tasks based on the meeting discussions. We will employ multiple AI agents that work collaboratively to capture human discussions and execute development tasks in real time. This represents a step toward next-generation software development environments, where human expertise drives strategy and AI accelerates task execution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08507v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeeshan Rasheed, Malik Abdul Sami, Jussi Rasku, Kai-Kristian Kemell, Zheying Zhang, Janne Harjamaki, Shahbaz Siddeeq, Sami Lahti, Tomas Herda, Mikko Nurminen, Niklas Lavesson, Jose Siqueira de Cerqueira, Toufique Hasan, Ayman Khan, Mahade Hasan, Mika Saari, Petri Rantanen, Jari Soini, Pekka Abrahamsson</dc:creator>
    </item>
    <item>
      <title>CorrectBench: Automatic Testbench Generation with Functional Self-Correction using LLMs for HDL Design</title>
      <link>https://arxiv.org/abs/2411.08510</link>
      <description>arXiv:2411.08510v1 Announce Type: new 
Abstract: Functional simulation is an essential step in digital hardware design. Recently, there has been a growing interest in leveraging Large Language Models (LLMs) for hardware testbench generation tasks. However, the inherent instability associated with LLMs often leads to functional errors in the generated testbenches. Previous methods do not incorporate automatic functional correction mechanisms without human intervention and still suffer from low success rates, especially for sequential tasks. To address this issue, we propose CorrectBench, an automatic testbench generation framework with functional self-validation and self-correction. Utilizing only the RTL specification in natural language, the proposed approach can validate the correctness of the generated testbenches with a success rate of 88.85%. Furthermore, the proposed LLM-based corrector employs bug information obtained during the self-validation process to perform functional self-correction on the generated testbenches. The comparative analysis demonstrates that our method achieves a pass ratio of 70.13% across all evaluated tasks, compared with the previous LLM-based testbench generation framework's 52.18% and a direct LLM-based generation method's 33.33%. Specifically in sequential circuits, our work's performance is 62.18% higher than previous work in sequential tasks and almost 5 times the pass ratio of the direct method. The codes and experimental results are open-sourced at the link: https://github.com/AutoBench/CorrectBench</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08510v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruidi Qiu, Grace Li Zhang, Rolf Drechsler, Ulf Schlichtmann, Bing Li</dc:creator>
    </item>
    <item>
      <title>The EU AI Act is a good start but falls short</title>
      <link>https://arxiv.org/abs/2411.08535</link>
      <description>arXiv:2411.08535v1 Announce Type: new 
Abstract: The EU AI Act was created to ensure ethical and safe Artificial Intelligence (AI) development and deployment across the EU. This study aims to identify key challenges and strategies for helping enterprises focus on resources effectively. To achieve this aim, we conducted a Multivocal Literature Review (MLR) to explore the sentiments of both the industry and the academia. From 130 articles, 56 met the criteria. Our key findings are three-fold. First, liability. Second, discrimination. Third, tool adequacy. Additionally, some negative sentiments were expressed by industry and academia regarding regulatory interpretations, specific requirements, and transparency issues. Next, our findings are three essential themes for enterprises. First, risk-based regulatory compliance. Second, ethical frameworks and principles in technology development. Third, policies and systems for regulatory risk management. These results identify the key challenges and strategies and provide less commonly discussed themes, enabling enterprises to align with the requirements and minimize their distance from the EU market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08535v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chalisa Veesommai Sillberg, Jose Siqueira De Cerqueira, Pekka Sillberg, Kai-Kristian Kemell, Pekka Abrahamsson</dc:creator>
    </item>
    <item>
      <title>LogLLM: Log-based Anomaly Detection Using Large Language Models</title>
      <link>https://arxiv.org/abs/2411.08561</link>
      <description>arXiv:2411.08561v1 Announce Type: new 
Abstract: Software systems often record important runtime information in logs to help with troubleshooting. Log-based anomaly detection has become a key research area that aims to identify system issues through log data, ultimately enhancing the reliability of software systems. Traditional deep learning methods often struggle to capture the semantic information embedded in log data, which is typically organized in natural language. In this paper, we propose LogLLM, a log-based anomaly detection framework that leverages large language models (LLMs). LogLLM employs BERT for extracting semantic vectors from log messages, while utilizing Llama, a transformer decoder-based model, for classifying log sequences. Additionally, we introduce a projector to align the vector representation spaces of BERT and Llama, ensuring a cohesive understanding of log semantics. Unlike conventional methods that require log parsers to extract templates, LogLLM preprocesses log messages with regular expressions, streamlining the entire process. Our framework is trained through a novel three-stage procedure designed to enhance performance and adaptability. Experimental results across four public datasets demonstrate that LogLLM outperforms state-of-the-art methods. Even when handling unstable logs, it effectively captures the semantic meaning of log messages and detects anomalies accurately.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08561v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Guan, Jian Cao, Shiyou Qian, Jianqi Gao</dc:creator>
    </item>
    <item>
      <title>Practitioners' Discussions on Building LLM-based Applications for Production</title>
      <link>https://arxiv.org/abs/2411.08574</link>
      <description>arXiv:2411.08574v1 Announce Type: new 
Abstract: \textit{Background}: Large language models (LLMs) have become a paramount interest of researchers and practitioners alike, yet a comprehensive overview of key considerations for those developing LLM-based systems is lacking. This study addresses this gap by collecting and mapping the topics practitioners discuss online, offering practical insights into where priorities lie in developing LLM-based applications. \textit{Method}: We collected 189 videos from 2022 to 2024 from practitioners actively developing such systems and discussing various aspects they encounter during development and deployment of LLMs in production. We analyzed the transcripts using BERTopic, then manually sorted and merged the generated topics into themes, leading to a total of 20 topics in 8 themes. \textit{Results}: The most prevalent topics fall within the theme Design \&amp; Architecture, with a strong focus on retrieval-augmented generation (RAG) systems. Other frequently discussed topics include model capabilities and enhancement techniques (e.g., fine-tuning, prompt engineering), infrastructure and tooling, and risks and ethical challenges. \textit{Implications}: Our results highlight current discussions and challenges in deploying LLMs in production. This way, we provide a systematic overview of key aspects practitioners should be aware of when developing LLM-based applications. We further pale off topics of interest for academics where further research is needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08574v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alina Mailach, Sebastian Simon, Johannes Dorn, Norbert Siegmund</dc:creator>
    </item>
    <item>
      <title>Learning-Guided Fuzzing for Testing Stateful SDN Controllers</title>
      <link>https://arxiv.org/abs/2411.08626</link>
      <description>arXiv:2411.08626v1 Announce Type: new 
Abstract: Controllers for software-defined networks (SDNs) are centralised software components that enable advanced network functionalities, such as dynamic traffic engineering and network virtualisation. However, these functionalities increase the complexity of SDN controllers, making thorough testing crucial. SDN controllers are stateful, interacting with multiple network devices through sequences of control messages. Identifying stateful failures in an SDN controller is challenging due to the infinite possible sequences of control messages, which result in an unbounded number of stateful interactions between the controller and network devices. In this article, we propose SeqFuzzSDN, a learning-guided fuzzing method for testing stateful SDN controllers. SeqFuzzSDN aims to (1) efficiently explore the state space of the SDN controller under test, (2) generate effective and diverse tests (i.e., control message sequences) to uncover failures, and (3) infer accurate failure-inducing models that characterise the message sequences leading to failures. In addition, we compare SeqFuzzSDN with three extensions of state-of-the-art (SOTA) methods for fuzzing SDNs. Our findings show that, compared to the extended SOTA methods, SeqFuzzSDN (1) generates more diverse message sequences that lead to failures within the same time budget, and (2) produces more accurate failure-inducing models, significantly outperforming the other extended SOTA methods in terms of sensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08626v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rapha\"el Ollando, Seung Yeob Shin, Lionel C. Briand</dc:creator>
    </item>
    <item>
      <title>Diagnosing Refactoring Dangers</title>
      <link>https://arxiv.org/abs/2411.08648</link>
      <description>arXiv:2411.08648v1 Announce Type: new 
Abstract: This report investigates the relationship between software refactoring and behavior preservation. Existing behavior preservation analyses often lack comprehensive insights into refactoring rejections and do not provide actionable solutions. To address these issues, we developed a conceptual model to detect refactoring dangers, and created an Eclipse plugin based upon this model, called ReFD.
  Every refactoring can be partitioned in microsteps, each of which carries potential risks. ReFD evaluates a given code context to identify if these potential risks are present, making them actual risks, and employs a verdict mechanism to reduce false positives. To facilitate the risk detection, several components called detectors and subdetectors are defined, which can be reused for multiple refactorings. The tool was validated by implementing the detection for multiple refactorings, which produce the expected information about the risks detected. This information leads a developer to actively think about solutions to the problems a refactoring might cause within an actual codebase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08648v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Wouter Brinksma, William Wernsen, Evert Verduin, Herman Hilberink, Patrick de Beer, Lex Bijlsma, Harrie Passier</dc:creator>
    </item>
    <item>
      <title>Human-Centered AI Transformation: Exploring Behavioral Dynamics in Software Engineering</title>
      <link>https://arxiv.org/abs/2411.08693</link>
      <description>arXiv:2411.08693v1 Announce Type: new 
Abstract: As Artificial Intelligence (AI) becomes integral to software development, understanding the social and cooperative dynamics that affect AI-driven organizational change is important. Yet, despite AI's rapid progress and influence, the human and cooperative facets of these shifts in software organizations remain relatively less explored. This study uses Behavioral Software Engineering (BSE) as a lens to examine these often-overlooked dimensions of AI transformation. Through a qualitative approach involving ten semi-structured interviews across four organizations that are undergoing AI transformations, we performed a thematic analysis that revealed numerous sub-themes linked to twelve BSE concepts across individual, group, and organizational levels. Since the organizations are at an early stage of transformation we found more emphasis on the individual level.
  Our findings further reveal six key challenges tied to these BSE aspects that the organizations face during their AI transformation. Aligned with change management literature, we emphasize that effective communication, proactive leadership, and resistance management are essential for successful AI integration. However, we also identify ethical considerations as critical in the AI context-an area largely overlooked in previous research. Furthermore, a narrative analysis illustrates how different roles within an organization experience the AI transition in unique ways. These insights underscore that AI transformation extends beyond technical solutions; it requires a thoughtful approach that balances technological and human factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08693v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theocharis Tavantzis, Robert Feldt</dc:creator>
    </item>
    <item>
      <title>Towards Reliable Evaluation of Neural Program Repair with Natural Robustness Testing</title>
      <link>https://arxiv.org/abs/2402.11892</link>
      <description>arXiv:2402.11892v2 Announce Type: replace 
Abstract: In this paper, we propose shifting the focus of robustness evaluation for Neural Program Repair (NPR) techniques toward naturally-occurring data transformations. To accomplish this, we first examine the naturalness of semantic-preserving transformations through a two-stage human study. This study includes (1) interviews with senior software developers to establish concrete criteria for evaluating the naturalness of these transformations, and (2) a survey involving 10 developers to assess the naturalness of 1,178 transformations, i.e., pairs of original and transformed programs, applied to 225 real-world bugs. Our findings show that only 60% of these transformations are deemed natural, while 20% are considered unnatural, with strong agreement among annotators. Moreover, the unnaturalness of these transformations significantly impacts both their applicability to benchmarks and the conclusions drawn from robustness testing. Next, we conduct natural robustness testing on NPR techniques to assess their true effectiveness against real-world data variations. Our experimental results reveal a substantial number of prediction changes in NPR techniques, leading to significant reductions in both plausible and correct patch rates when comparing performance on the original and transformed datasets. Additionally, we observe notable differences in performance improvements between NPR techniques, suggesting potential biases on NPR evaluation introduced by limited datasets. Finally, we propose an LLM-based metric to automate the assessment of transformation naturalness, ensuring the scalability of natural robustness testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11892v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thanh Le-Cong, Dat Nguyen, Bach Le, Toby Murray</dc:creator>
    </item>
    <item>
      <title>SoliDiffy: AST Differencing for Solidity Smart Contracts</title>
      <link>https://arxiv.org/abs/2411.07718</link>
      <description>arXiv:2411.07718v2 Announce Type: replace 
Abstract: Smart contracts, primarily written in Solidity, are integral to blockchain software applications, yet precise analysis and maintenance are hindered by the limitations of existing differencing tools. We introduce SoliDiffy, a novel Abstract Syntax Tree (AST) differencing tool specifically designed for Solidity. SoliDiffy enables fine-grained analysis by generating accurate and concise edit scripts of smart contracts, making it ideal for downstream tasks such as vulnerability detection, automated code repair, and code reviews. Our comprehensive evaluation on a large dataset of real-world Solidity contracts demonstrates that SoliDiffy delivers shorter and more precise edit scripts compared to state-of-the-art tools, while performing consistently in complex contract modifications. SoliDiffy is made publicly available at https://github.com/mojtaba-eshghie/SoliDiffy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07718v2</guid>
      <category>cs.SE</category>
      <category>cs.PL</category>
      <pubDate>Thu, 14 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mojtaba Eshghie, Viktor {\AA}ryd, Martin Monperrus, Cyrille Artho</dc:creator>
    </item>
  </channel>
</rss>
