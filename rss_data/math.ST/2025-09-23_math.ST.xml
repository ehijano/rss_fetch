<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Sep 2025 01:52:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Monte Carlo on a single sample</title>
      <link>https://arxiv.org/abs/2509.17025</link>
      <description>arXiv:2509.17025v1 Announce Type: new 
Abstract: In this paper, we consider a Monte Carlo simulation method (MinMC) that approximates prices and risk measures for a range $\Gamma$ of model parameters at once. The simulation method that we study has recently gained popularity [HS20, FPP22, BDG24], and we provide a theoretical framework and convergence rates for it. In particular, we show that sample-based approximations to $\mathbb{E}_{\theta}[X]$, where $\theta$ denotes the model and $\mathbb{E}_{\theta}$ the expectation with respect to the distribution $P_\theta$ of the model $\theta$, can be obtained across all $\theta \in \Gamma$ by minimizing a map $V:H\rightarrow \mathbb{R}$ with $H$ a suitable function space. The minimization can be achieved easily by fitting a standard feedforward neural network with stochastic gradient descent. We show that MinMC, which uses only one sample for each model, significantly outperforms a traditional Monte Carlo method performed for multiple values of $\theta$, which are subsequently interpolated. Our case study suggests that MinMC might serve as a new benchmark for parameter-dependent Monte Carlo simulations, which appear not only in quantitative finance but also in many other areas of scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17025v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Detering, Paul Eisenberg, Nicole Hufnagel</dc:creator>
    </item>
    <item>
      <title>A new perspective on dominating the James-Stein estimator</title>
      <link>https://arxiv.org/abs/2509.17504</link>
      <description>arXiv:2509.17504v1 Announce Type: new 
Abstract: This paper presents a novel approach to constructing estimators that dominate the classical James-Stein estimator under the quadratic loss for multivariate normal means. Building on Stein's risk representation, we introduce a new sufficient condition involving a monotonicity property of a transformed shrinkage function. We derive a general class of shrinkage estimators that satisfy minimaxity and dominance over the James-Stein estimator, including cases with polynomial or logarithmic convergence to the optimal shrinkage factor. We also provide conditions for uniform dominance across dimensions and for improved asymptotic risk performance. We present several examples and numerical validations to illustrate the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17504v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yuzo Maruyama, Akimichi Takemura</dc:creator>
    </item>
    <item>
      <title>Improving Cram\'er-Rao Bound And Its Variants: An Extrinsic Geometry Perspective</title>
      <link>https://arxiv.org/abs/2509.17886</link>
      <description>arXiv:2509.17886v1 Announce Type: new 
Abstract: This work presents a geometric refinement of the classical Cram\'er-Rao bound (CRB) by incorporating curvature-aware corrections based on the second fundamental form associated with the statistical model manifold. That is, our formulation shows that relying on the extrinsic geometry of the square root embedding of the manifold in the ambient Hilbert space comprising square integrable functions with respect to a fixed base measure offers a rigorous (and intuitive) way to improve upon the CRB and some of its variants, such as the Bhattacharyya-type bounds, that use higher-order derivatives of the log-likelihood. Precisely, the improved bounds in the latter case make explicit use of the elegant framework offered by employing the Fa\`a di Bruno formula and exponential Bell polynomials in expressing the jets associated with the square root embedding in terms of the raw scores. The interplay between the geometry of the statistical embedding and the behavior of the estimator variance is quantitatively analyzed in concrete examples, showing that our corrections can meaningfully tighten the lower bound, suggesting further exploration into connections with estimator efficiency in more general situations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17886v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sunder Ram Krishnan</dc:creator>
    </item>
    <item>
      <title>Monitoring Time Series for Relevant Changes</title>
      <link>https://arxiv.org/abs/2509.01756</link>
      <description>arXiv:2509.01756v1 Announce Type: cross 
Abstract: We consider the problem of sequentially testing for changes in the mean parameter of a time series, compared to a benchmark period. Most tests in the literature focus on the null hypothesis of a constant mean versus the alternative of a single change at an unknown time. Yet in many applications it is unrealistic that no change occurs at all, or that after one change the time series remains stationary forever. We introduce a new setup, modeling the sequence of means as a piecewise constant function with arbitrarily many changes. Instead of testing for a change, we ask whether the evolving sequence of means, say $(\mu_n)_{n \geq 1}$, stays within a narrow corridor around its initial value, that is, $\mu_n \in [\mu_1-\Delta, \mu_1+\Delta]$ for all $n \ge 1$. Combining elements from multiple change point detection with a H\"older-type monitoring procedure, we develop a new online monitoring tool. A key challenge in both construction and proof of validity is that the risk of committing a type-I error after any time $n$ fundamentally depends on the unknown future of the time series. Simulations support our theoretical results and we present two real-world applications: (1) healthcare monitoring, with a focus on blood glucose tracking, and (2) political consensus analysis via citizen opinion polls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01756v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Tim Kutta, Rupsa Basu, Holger Dette</dc:creator>
    </item>
    <item>
      <title>Learning Centre Partitions from Summaries</title>
      <link>https://arxiv.org/abs/2509.16337</link>
      <description>arXiv:2509.16337v1 Announce Type: cross 
Abstract: Multi-centre studies increasingly rely on distributed inference, where sites share only centre-level summaries. Homogeneity of parameters across centres is often violated, motivating methods that both \emph{test} for equality and \emph{learn} centre groupings before estimation. We develop multivariate Cochran-type tests that operate on summary statistics and embed them in a sequential, test-driven \emph{Clusters-of-Centres (CoC)} algorithm that merges centres (or blocks) only when equality is not rejected. We derive the asymptotic $\chi^2$-mixture distributions of the test statistics and provide plug-in estimators for implementation. To improve finite-sample integration, we introduce a multi-round bootstrap CoC that re-evaluates merges across independently resampled summary sets; under mild regularity and a separation condition, we prove a \emph{golden-partition recovery} result: as the number of rounds grows with $n$, the true partition is recovered with probability tending to one. We also give simple numerical guidelines, including a plateau-based stopping rule, to make the multi-round procedure reproducible. Simulations and a real-data analysis of U.S.\ airline on-time performance (2007) show accurate heterogeneity detection and partitions that change little with the choice of resampling scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16337v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zinsou Max Debaly, Jean-Francois Ethier, Michael H. Neumann, F\'elix Camirand Lemyre</dc:creator>
    </item>
    <item>
      <title>On Quantification of Borrowing of Information in Hierarchical Bayesian Models</title>
      <link>https://arxiv.org/abs/2509.17301</link>
      <description>arXiv:2509.17301v1 Announce Type: cross 
Abstract: In this work, we offer a thorough analytical investigation into the role of shared hyperparameters in a hierarchical Bayesian model, examining their impact on information borrowing and posterior inference. Our approach is rooted in a non-asymptotic framework, where observations are drawn from a mixed-effects model, and a Gaussian distribution is assumed for the true effect generator. We consider a nested hierarchical prior distribution model to capture these effects and use the posterior means for Bayesian estimation. To quantify the effect of information borrowing, we propose an integrated risk measure relative to the true data-generating distribution. Our analysis reveals that the Bayes estimator for the model with a deeper hierarchy performs better, provided that the unknown random effects are correlated through a compound symmetric structure. Our work also identifies necessary and sufficient conditions for this model to outperform the one nested within it. We further obtain sufficient conditions when the correlation is perturbed. Our study suggests that the model with a deeper hierarchy tends to outperform the nested model unless the true data-generating distribution favors sufficiently independent groups. These findings have significant implications for Bayesian modeling, and we believe they will be of interest to researchers across a wide range of fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17301v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prasenjit Ghosh, Anirban Bhattacharya, Debdeep Pati</dc:creator>
    </item>
    <item>
      <title>Bias-variance Tradeoff in Tensor Estimation</title>
      <link>https://arxiv.org/abs/2509.17382</link>
      <description>arXiv:2509.17382v1 Announce Type: cross 
Abstract: We study denoising of a third-order tensor when the ground-truth tensor is not necessarily Tucker low-rank. Specifically, we observe $$ Y=X^\ast+Z\in \mathbb{R}^{p_{1} \times p_{2} \times p_{3}}, $$ where $X^\ast$ is the ground-truth tensor, and $Z$ is the noise tensor. We propose a simple variant of the higher-order tensor SVD estimator $\widetilde{X}$. We show that uniformly over all user-specified Tucker ranks $(r_{1},r_{2},r_{3})$, $$ \| \widetilde{X} - X^* \|_{ \mathrm{F}}^2 = O \Big( \kappa^2 \Big\{ r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k} \Big\} \; + \; \xi_{(r_{1},r_{2},r_{3})}^2\Big) \quad \text{ with high probability.} $$ Here, the bias term $\xi_{(r_1,r_2,r_3)}$ corresponds to the best achievable approximation error of $X^\ast$ over the class of tensors with Tucker ranks $(r_1,r_2,r_3)$; $\kappa^2$ quantifies the noise level; and the variance term $\kappa^2 \{r_{1}r_{2}r_{3}+\sum_{k=1}^{3} p_{k} r_{k}\}$ scales with the effective number of free parameters in the estimator $\widetilde{X}$. Our analysis achieves a clean rank-adaptive bias--variance tradeoff: as we increase the ranks of estimator $\widetilde{X}$, the bias $\xi(r_{1},r_{2},r_{3})$ decreases and the variance increases. As a byproduct we also obtain a convenient bias-variance decomposition for the vanilla low-rank SVD matrix estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17382v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shivam Kumar, Haotian Xu, Carlos Misael Madrid Padilla, Yuehaw Khoo, Oscar Hernan Madrid Padilla, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Semi-supervised Inference via a Debiased Modeling Approach</title>
      <link>https://arxiv.org/abs/2509.17385</link>
      <description>arXiv:2509.17385v1 Announce Type: cross 
Abstract: Inference in semi-supervised (SS) settings has gained substantial attention in recent years due to increased relevance in modern big-data problems. In a typical SS setting, there is a much larger-sized unlabeled data, containing only observations of predictors, and a moderately sized labeled data containing observations for both an outcome and the set of predictors. Such data naturally arises when the outcome, unlike the predictors, is costly or difficult to obtain. One of the primary statistical objectives in SS settings is to explore whether parameter estimation can be improved by exploiting the unlabeled data. We propose a novel Bayesian method for estimating the population mean in SS settings. The approach yields estimators that are both efficient and optimal for estimation and inference. The method itself has several interesting artifacts. The central idea behind the method is to model certain summary statistics of the data in a targeted manner, rather than the entire raw data itself, along with a novel Bayesian notion of debiasing. Specifying appropriate summary statistics crucially relies on a debiased representation of the population mean that incorporates unlabeled data through a flexible nuisance function while also learning its estimation bias. Combined with careful usage of sample splitting, this debiasing approach mitigates the effect of bias due to slow rates or misspecification of the nuisance parameter from the posterior of the final parameter of interest, ensuring its robustness and efficiency. Concrete theoretical results, via Bernstein--von Mises theorems, are established, validating all claims, and are further supported through extensive numerical studies. To our knowledge, this is possibly the first work on Bayesian inference in SS settings, and its central ideas also apply more broadly to other Bayesian semi-parametric inference problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17385v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ecosta.2025.05.001</arxiv:DOI>
      <arxiv:journal_reference>Econometrics and Statistics (2025)</arxiv:journal_reference>
      <dc:creator>G\"ozde Sert, Abhishek Chakrabortty, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>A Generative Conditional Distribution Equality Testing Framework and Its Minimax Analysis</title>
      <link>https://arxiv.org/abs/2509.17729</link>
      <description>arXiv:2509.17729v1 Announce Type: cross 
Abstract: In this paper, we propose a general framework for testing the equality of the conditional distributions in a two-sample problem. This problem is most relevant to transfer learning under covariate shift. Our framework is built on neural network-based generative methods and sample splitting techniques by transforming the conditional distribution testing problem into an unconditional one. We introduce two special tests: the generative permutation-based conditional distribution equality test and the generative classification accuracy-based conditional distribution equality test. Theoretically, we establish a minimax lower bound for statistical inference in testing the equality of two conditional distributions under certain smoothness conditions. We demonstrate that the generative permutation-based conditional distribution equality test and its modified version can attain this lower bound precisely or up to some iterated logarithmic factor. Moreover, we prove the testing consistency of the generative classification accuracy-based conditional distribution equality test. We also establish the convergence rate for the learned conditional generator by deriving new results related to the recently-developed offset Rademacher complexity and approximation properties using neural networks. Empirically, we conduct numerical studies including synthetic datasets and two real-world datasets, demonstrating the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17729v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Siming Zheng, Meifang Lan, Tong Wang, Yuanyuan Lin</dc:creator>
    </item>
    <item>
      <title>The Equivariance Criterion in a Linear Model for Fixed-X Cases</title>
      <link>https://arxiv.org/abs/2204.10488</link>
      <description>arXiv:2204.10488v3 Announce Type: replace 
Abstract: The field of machine have seen rising applications of equivariance criterion. However, there is no systematic way to justify its usage, including why it works, whether there is an optimal solution and if so, what form it carries. In this article, we explored the usage of equivariance criterion in a normal linear model with fixed-$X$ and extended the model to allow multiple populations, which, in turn, leads to a multivariate invariant location-scale transformation group, compared than the commonly used univariate one. The minimum risk equivariant estimators of the coefficient vector and the diagonal covariance matrix were derived, which were consistent with literature works. This work serves as an early exploration of the usage of equivariance criterion in machine learning, where we confirmed that the least square approach widely used in machine learning indeed carries optimality in some sense at least in the framework of estimation.
  Meanwhile, the problems can be shown to be equivalent to a mixture from $p$ independent normal samples and via the principle of functional equivariance, an alternative proof can be derived. However, such an approach carries its own limitation with a strong tie to equivariance criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10488v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daowei Wang, Mian Wu, Haojin Zhou</dc:creator>
    </item>
    <item>
      <title>Posterior contraction rates in a sparse non-linear mixed-effects model</title>
      <link>https://arxiv.org/abs/2405.01206</link>
      <description>arXiv:2405.01206v2 Announce Type: replace 
Abstract: Recent works have shown an interest in investigating the frequentist asymptotic properties of Bayesian procedures for high-dimensional linear models under sparsity constraints. However, there exists a gap in the literature regarding analogous theoretical findings for non-linear models within the high-dimensional setting. The current study provides a novel contribution, focusing specifically on a non-linear mixed-effects model. In this model, the residual variance is assumed to be known, while the regression vector and the covariance matrix of the random effects are unknown and must be estimated. The prior distribution for the sparse regression coefficients consists of a mixture of a point mass at zero and a Laplace distribution, while an Inverse-Wishart prior is employed for the covariance parameter of the random effects. First, the effective dimension of this model is bounded with high posterior probabilities. Subsequently, we derive posterior contraction rates for both the covariance parameter and the prediction term of the response vector. Finally, under additional assumptions, the posterior distribution is shown to contract for recovery of the unknown sparse regression vector at a rate similar to that established in the linear case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01206v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marion Naveau (MIA Paris-Saclay), Maud Delattre (MaIAGE), Laure Sansonnet (MIA Paris-Saclay)</dc:creator>
    </item>
    <item>
      <title>Semiparametrics via parametrics and contiguity</title>
      <link>https://arxiv.org/abs/2501.09483</link>
      <description>arXiv:2501.09483v2 Announce Type: replace 
Abstract: Inference on the parametric part of a semiparametric model is no trivial task. If one approximates the infinite dimensional part of the semiparametric model by a parametric function, one obtains a parametric model that is in some sense close to the semiparametric model and inference may proceed by the method of maximum likelihood. Under regularity conditions, the ensuing maximum likelihood estimator is asymptotically normal and efficient in the approximating parametric model. Thus one obtains a sequence of asymptotically normal and efficient estimators in a sequence of growing parametric models that approximate the semiparametric model and, intuitively, the limiting 'semiparametric' estimator should be asymptotically normal and efficient as well. In this paper we make this intuition rigorous: we move much of the semiparametric analysis back into classical parametric terrain, and then translate our parametric results back to the semiparametric world by way of contiguity. Our approach departs from the conventional sieve literature by being more specific about the approximating parametric models, by working not only with but also under these when treating the parametric models, and by taking full advantage of the mutual contiguity that we require between the parametric and semiparametric models. We illustrate our theory with two canonical examples of semiparametric models, namely the partially linear regression model and the Cox regression model. An upshot of our theory is a new, relatively simple, and rather parametric proof of the efficiency of the Cox partial likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09483v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Lee, Emil A. Stoltenberg, Per A. Mykland</dc:creator>
    </item>
    <item>
      <title>Locally minimax optimal confidence sets for the best model</title>
      <link>https://arxiv.org/abs/2503.21639</link>
      <description>arXiv:2503.21639v4 Announce Type: replace 
Abstract: This paper tackles a fundamental inference problem: given $n$ observations from a distribution $P$ over $\mathbb{R}^d$ with unknown mean $\boldsymbol{\mu}$, we must form a confidence set for the index (or indices) corresponding to the smallest component of $\boldsymbol{\mu}$. By duality, we reduce this to testing, for each $r$ in $1,\ldots,d$, whether $\mu_r$ is the smallest. Based on the sample splitting and self-normalization approach of Kim and Ramdas (2024), we propose "dimension-agnostic" tests that maintain validity regardless of how $d$ scales with $n$, and regardless of arbitrary ties in $\boldsymbol{\mu}$. Notably, our validity holds under mild moment conditions, requiring little more than finiteness of a second moment, and permitting possibly strong dependence between coordinates. In addition, we establish the \emph{local} minimax separation rate for this problem, which adapts to the cardinality of a confusion set, and show that the proposed tests attain this rate. Furthermore, we develop robust variants that continue to achieve the same minimax rate under heavy-tailed distributions with only finite second moments. While these results highlight the theoretical strength of our method, a practical concern is that sample splitting can reduce finite-sample power. We show that this drawback can be substantially alleviated by the multi-split aggregation method of Guo and Shah (2025). Finally, empirical results on simulated and real data illustrate the strong performance of our approach in terms of type I error control and power compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21639v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilmun Kim, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Global Activity Scores</title>
      <link>https://arxiv.org/abs/2505.00711</link>
      <description>arXiv:2505.00711v3 Announce Type: replace 
Abstract: We introduce a new global sensitivity measure, the global activity scores. We establish its theoretical connection with Sobol' sensitivity indices and demonstrate its performance through numerical examples. In these examples, we compare global activity scores with Sobol' sensitivity indices, derivative-based sensitivity measures, and activity scores. The results show that in the presence of noise or high variability, global activity scores outperform derivative-based measures and activity scores, while in noiseless settings the three approaches yield similar results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00711v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruilong Yue, Giray \"Okten</dc:creator>
    </item>
    <item>
      <title>Minimax Adaptive Online Nonparametric Regression over Besov Spaces</title>
      <link>https://arxiv.org/abs/2505.19741</link>
      <description>arXiv:2505.19741v2 Announce Type: replace 
Abstract: We study online adversarial regression with convex losses against a rich class of continuous yet highly irregular prediction rules, modeled by Besov spaces $B\_{pq}^s$ with general parameters $1 \leq p,q \leq \infty$ and smoothness $s &gt; \tfrac{d}{p}$. We introduce an adaptive wavelet-based algorithm that performs sequential prediction without prior knowledge of $(s,p,q)$, and establish minimax-optimal regret bounds against any comparator in $B\_{pq}^s$. We further design a locally adaptive extension capable of dynamically tracking spatially inhomogeneous smoothness. This adaptive mechanism adjusts the resolution of the predictions over both time and space, yielding refined regret bounds in terms of local regularity. Consequently, in heterogeneous environments, our adaptive guarantees can significantly surpass those obtained by standard global methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19741v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Liautaud (LPSM), Pierre Gaillard (Thoth, UGA), Olivier Wintenberger (LPSM)</dc:creator>
    </item>
    <item>
      <title>Kernel Ridge Regression with Predicted Feature Inputs and Applications to Factor-Based Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2505.20022</link>
      <description>arXiv:2505.20022v2 Announce Type: replace 
Abstract: Kernel methods, particularly kernel ridge regression (KRR), are time-proven, powerful nonparametric regression techniques known for their rich capacity, analytical simplicity, and computational tractability. The analysis of their predictive performance has received continuous attention for more than two decades. However, in many modern regression problems where the feature inputs used in KRR cannot be directly observed and must instead be inferred from other measurements, the theoretical foundations of KRR remain largely unexplored. In this paper, we introduce a novel approach for analyzing KRR with predicted feature inputs. Our framework is not only essential for handling predicted feature inputs -- enabling us to derive risk bounds without imposing any assumptions on the error of the predicted feature -- but also strengthens existing analyses in the classical setting by allowing arbitrary model misspecification, requiring weaker conditions under the squared loss, particularly allowing both an unbounded response and an unbounded function class, and being flexible enough to accommodate other convex loss functions. We apply our general theory to factor-based nonparametric regression models and establish the minimax optimality of KRR when the feature inputs are predicted using principal component analysis. Our theoretical findings are further corroborated by simulation studies and real-data analyses using pretrained LLM embeddings for the downstream prediction task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20022v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Bing, Xin He, Chao Wang</dc:creator>
    </item>
    <item>
      <title>Almost Unbiased Liu Type Estimator in Bell Regression Model: Theory, Simulation and Application</title>
      <link>https://arxiv.org/abs/2505.20946</link>
      <description>arXiv:2505.20946v2 Announce Type: replace 
Abstract: In this paper, we gain the new almost unbiased Liu-type estimators to literature for the Bell regression model. We provide the superiority of the proposed estimator to its competitors such as the maximum likelihood estimator and Liu-type estimators via some theorems. We also design an extensive Monte Carlo simulation study to show that the proposed estimators outperforms the competitors in terms of mean squared error theoretically. Finally, we present a real data study to assess the performance of the introduced estimators in modeling real-life data. The findings of both the simulation and the empirical study demonstrate that the proposed regression estimators surpasses its competitors based on the mean square error criterion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20946v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Caner Tan{\i}\c{s}, Yasin Asar</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Misspecified Contextual Bandits</title>
      <link>https://arxiv.org/abs/2509.06287</link>
      <description>arXiv:2509.06287v2 Announce Type: replace 
Abstract: Contextual bandit algorithms have transformed modern experimentation by enabling real-time adaptation for personalized treatment and efficient use of data. Yet these advantages create challenges for statistical inference due to adaptivity. A fundamental property that supports valid inference is policy convergence, meaning that action-selection probabilities converge in probability given the context. Convergence ensures replicability of adaptive experiments and stability of online algorithms. In this paper, we highlight a previously overlooked issue: widely used algorithms such as LinUCB may fail to converge when the reward model is misspecified, and such non-convergence creates fundamental obstacles for statistical inference. This issue is practically important, as misspecified models -- such as linear approximations of complex dynamic system -- are often employed in real-world adaptive experiments to balance bias and variance.
  Motivated by this insight, we propose and analyze a broad class of algorithms that are guaranteed to converge even under model misspecification. Building on this guarantee, we develop a general inference framework based on an inverse-probability-weighted Z-estimator (IPW-Z) and establish its asymptotic normality with a consistent variance estimator. Simulation studies confirm that the proposed method provides robust and data-efficient confidence intervals, and can outperform existing approaches that exist only in the special case of offline policy evaluation. Taken together, our results underscore the importance of designing adaptive algorithms with built-in convergence guarantees to enable stable experimentation and valid statistical inference in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06287v2</guid>
      <category>math.ST</category>
      <category>cs.AI</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongyi Guo, Ziping Xu</dc:creator>
    </item>
    <item>
      <title>Theoretical Insights into CycleGAN: Analyzing Approximation and Estimation Errors in Unpaired Data Generation</title>
      <link>https://arxiv.org/abs/2407.11678</link>
      <description>arXiv:2407.11678v3 Announce Type: replace-cross 
Abstract: In this paper, we focus on analyzing the excess risk of the unpaired data generation model, called CycleGAN. Unlike classical GANs, CycleGAN not only transforms data between two unpaired distributions but also ensures the mappings are consistent, which is encouraged by the cycle-consistency term unique to CycleGAN. The increasing complexity of model structure and the addition of the cycle-consistency term in CycleGAN present new challenges for error analysis. By considering the impact of both the model architecture and training procedure, the risk is decomposed into two terms: approximation error and estimation error. These two error terms are analyzed separately and ultimately combined by considering the trade-off between them. Each component is rigorously analyzed; the approximation error through constructing approximations of the optimal transport maps, and the estimation error through establishing an upper bound using Rademacher complexity. Our analysis not only isolates these errors but also explores the trade-offs between them, which provides a theoretical insights of how CycleGAN's architecture and training procedures influence its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11678v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luwei Sun, Dongrui Shen, Han Feng</dc:creator>
    </item>
    <item>
      <title>Learning Massive-scale Partial Correlation Networks in Clinical Multi-omics Studies with HP-ACCORD</title>
      <link>https://arxiv.org/abs/2412.11554</link>
      <description>arXiv:2412.11554v4 Announce Type: replace-cross 
Abstract: Graphical model estimation from multi-omics data requires a balance between statistical estimation performance and computational scalability. We introduce a novel pseudolikelihood-based graphical model framework that reparameterizes the target precision matrix while preserving the sparsity pattern and estimates it by minimizing an $\ell_1$-penalized empirical risk based on a new loss function. The proposed estimator maintains estimation and selection consistency in various metrics under high-dimensional assumptions. The associated optimization problem allows for a provably fast computation algorithm using a novel operator-splitting approach and communication-avoiding distributed matrix multiplication. A high-performance computing implementation of our framework was tested using simulated data with up to one million variables, demonstrating complex dependency structures similar to those found in biological networks. Leveraging this scalability, we estimated a partial correlation network from a dual-omic liver cancer data set. The co-expression network estimated from the ultrahigh-dimensional data demonstrated superior specificity in prioritizing key transcription factors and co-activators by excluding the impact of epigenetic regulation, thereby highlighting the value of computational scalability in multi-omic data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11554v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sungdong Lee, Joshua Bang, Youngrae Kim, Hyungwon Choi, Sang-Yun Oh, Joong-Ho Won</dc:creator>
    </item>
    <item>
      <title>On the Maximum and Minimum of a Multivariate Poisson Distribution</title>
      <link>https://arxiv.org/abs/2412.13535</link>
      <description>arXiv:2412.13535v2 Announce Type: replace-cross 
Abstract: In this paper, we investigate the cumulative distribution functions (CDFs) of the maximum and minimum of multivariate Poisson distributions with three dependence structures, namely, the common shock, comonotonic shock and thinning-dependence models. In particular, we formulate the definition of a thinning-dependent multivariate Poisson distribution based on Wang and Yuen (2005). We derive explicit CDFs of the maximum and minimum of the multivariate Poisson random vectors and conduct asymptotic analyses on them. Our results reveal the substantial difference between the three dependence structures for multivariate Poisson distribution and may suggest an alternative method for studying the dependence for other multivariate distributions. We further provide numerical examples demonstrating obtained results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13535v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Liu, Feifan Shi, Jing Yao, Yang Yang</dc:creator>
    </item>
    <item>
      <title>Bayesian Algorithms for Adversarial Online Learning: from Finite to Infinite Action Spaces</title>
      <link>https://arxiv.org/abs/2502.14790</link>
      <description>arXiv:2502.14790v5 Announce Type: replace-cross 
Abstract: We develop a form Thompson sampling for online learning under full feedback - also known as prediction with expert advice - where the learner's prior is defined over the space of an adversary's future actions, rather than the space of experts. We show regret decomposes into regret the learner expected a priori, plus a prior-robustness-type term we call excess regret. In the classical finite-expert setting, this recovers optimal rates. As an initial step towards practical online learning in settings with a potentially-uncountably-infinite number of experts, we show that Thompson sampling over the $d$-dimensional unit cube, using a certain Gaussian process prior widely-used in the Bayesian optimization literature, has a $\mathcal{O}\Big(\beta\sqrt{Td\log(1+\sqrt{d}\frac{\lambda}{\beta})}\Big)$ rate against a $\beta$-bounded $\lambda$-Lipschitz adversary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14790v5</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Terenin, Jeffrey Negrea</dc:creator>
    </item>
    <item>
      <title>Absorb and Converge: Provable Convergence Guarantee for Absorbing Discrete Diffusion Models</title>
      <link>https://arxiv.org/abs/2506.02318</link>
      <description>arXiv:2506.02318v2 Announce Type: replace-cross 
Abstract: Discrete state space diffusion models have shown significant advantages in applications involving discrete data, such as text and image generation. It has also been observed that their performance is highly sensitive to the choice of rate matrices, particularly between uniform and absorbing rate matrices. While empirical results suggest that absorbing rate matrices often yield better generation quality compared to uniform rate matrices, existing theoretical works have largely focused on the uniform rate matrices case. Notably, convergence guarantees and error analyses for absorbing diffusion models are still missing. In this work, we provide the first finite-time error bounds and convergence rate analysis for discrete diffusion models using absorbing rate matrices. We begin by deriving an upper bound on the KL divergence of the forward process, introducing a surrogate initialization distribution to address the challenge posed by the absorbing stationary distribution, which is a singleton and causes the KL divergence to be ill-defined. We then establish the first convergence guarantees for both the $\tau$-leaping and uniformization samplers under absorbing rate matrices, demonstrating improved rates over their counterparts using uniform rate matrices. Furthermore, under suitable assumptions, we provide convergence guarantees without early stopping. Our analysis introduces several new technical tools to address challenges unique to absorbing rate matrices. These include a Jensen-type argument for bounding forward process convergence, novel techniques for bounding absorbing score functions, and a non-divergent upper bound on the score near initialization that removes the need of early-stopping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02318v2</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuchen Liang, Renxiang Huang, Lifeng Lai, Ness Shroff, Yingbin Liang</dc:creator>
    </item>
    <item>
      <title>Canonical Representations of Markovian Structural Causal Models: A Framework for Counterfactual Reasoning</title>
      <link>https://arxiv.org/abs/2507.16370</link>
      <description>arXiv:2507.16370v2 Announce Type: replace-cross 
Abstract: Counterfactual reasoning aims at answering contrary-to-fact questions like ``Would have Alice recovered had she taken aspirin?'' and corresponds to the most fine-grained layer of causation. Critically, while many counterfactual statements cannot be falsified-even by randomized experiments-they underpin fundamental concepts like individual-wise fairness. Therefore, providing models to formalize and implement counterfactual beliefs remains a fundamental scientific problem. In the Markovian setting of Pearl's causal framework, we propose an alternative approach to structural causal models to represent counterfactuals compatible with a given causal graphical model. More precisely, we introduce counterfactual models, also called canonical representations of structural causal models. They enable analysts to choose a counterfactual assumption via random-process probability distributions with preassigned marginals and characterize the counterfactual equivalence class of structural causal models. Using these representations, we present a normalization procedure to disentangle the (arbitrary and unfalsifiable) counterfactual choice from the (typically testable) interventional constraints. In contrast to structural causal models, this allows to implement many counterfactual assumptions while preserving interventional knowledge, and does not require any estimation step at the individual-counterfactual layer: only to make a choice. Finally, we illustrate the specific role of counterfactuals in causality and the benefits of our approach on theoretical and numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16370v2</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas de Lara (IECL)</dc:creator>
    </item>
    <item>
      <title>A principled approach for comparing Variable Importance</title>
      <link>https://arxiv.org/abs/2507.17306</link>
      <description>arXiv:2507.17306v2 Announce Type: replace-cross 
Abstract: Variable importance measures (VIMs) aim to quantify the contribution of each input covariate to the predictability of a given output. With the growing interest in explainable AI, numerous VIMs have been proposed, many of which are heuristic in nature. This is often justified by the inherent subjectivity of the notion of importance. This raises important questions regarding usage: What makes a good VIM? How can we compare different VIMs?
  In this paper, we address these questions by: (1) proposing an axiomatic framework that bridges the gap between variable importance and variable selection. This framework formalizes the intuitive principle that features providing no additional information should not be assigned importance. It helps avoid false positives due to spurious correlations, which can arise with popular methods such as Shapley values; and (2) introducing a general pipeline for constructing VIMs, which clarifies the objective of various VIMs and thus facilitates meaningful comparisons. This approach is natural in statistics, but the literature has diverged from it.
  Finally, we provide an extensive set of examples to guide practitioners in selecting and estimating appropriate indices aligned with their specific goals and data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17306v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angel Reyero-Lobo, Pierre Neuvial, Bertrand Thirion</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Bounds and Task-Centric Learning Complexity for Real-World Dynamic Nonlinear Systems</title>
      <link>https://arxiv.org/abs/2509.06599</link>
      <description>arXiv:2509.06599v2 Announce Type: replace-cross 
Abstract: Dynamic nonlinear systems exhibit distortions arising from coupled static and dynamic effects. Their intertwined nature poses major challenges for data-driven modeling. This paper presents a theoretical framework grounded in structured decomposition, variance analysis, and task-centric complexity bounds.
  The framework employs a directional lower bound on interactions between measurable system components, extending orthogonality in inner product spaces to structurally asymmetric settings. This bound supports variance inequalities for decomposed systems. Key behavioral indicators are introduced along with a memory finiteness index. A rigorous power-based condition establishes a measurable link between finite memory in realizable systems and the First Law of Thermodynamics. This offers a more foundational perspective than classical bounds based on the Second Law.
  Building on this foundation, we formulate a `Behavioral Uncertainty Principle,' demonstrating that static and dynamic distortions cannot be minimized simultaneously. We identify that real-world systems seem to resist complete deterministic decomposition due to entangled static and dynamic effects. We also present two general-purpose theorems linking function variance to mean-squared Lipschitz continuity and learning complexity. This yields a model-agnostic, task-aware complexity metric, showing that lower-variance components are inherently easier to learn.
  These insights explain the empirical benefits of structured residual learning, including improved generalization, reduced parameter count, and lower training cost, as previously observed in power amplifier linearization experiments. The framework is broadly applicable and offers a scalable, theoretically grounded approach to modeling complex dynamic nonlinear systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06599v2</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanp\"a\"a</dc:creator>
    </item>
    <item>
      <title>Power-Dominance in Estimation Theory: A Third Pathological Axis</title>
      <link>https://arxiv.org/abs/2509.12691</link>
      <description>arXiv:2509.12691v2 Announce Type: replace-cross 
Abstract: This paper introduces a novel framework for estimation theory by introducing a second-order diagnostic for estimator design. While classical analysis focuses on the bias-variance trade-off, we present a more foundational constraint. This result is model-agnostic, domain-agnostic, and is valid for both parametric and non-parametric problems, Bayesian and frequentist frameworks. We propose to classify the estimators into three primary power regimes. We theoretically establish that any estimator operating in the `power-dominant regime' incurs an unavoidable mean-squared error penalty, making it structurally prone to sub-optimal performance. We propose a `safe-zone law' and make this diagnostic intuitive through two safe-zone maps. One map is a geometric visualization analogous to a receiver operating characteristic curve for estimators, and the other map shows that the safe-zone corresponds to a bounded optimization problem, while the forbidden `power-dominant zone' represents an unbounded optimization landscape. This framework reframes estimator design as a path optimization problem, providing new theoretical underpinnings for regularization and inspiring novel design philosophies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12691v2</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sri Satish Krishna Chaitanya Bulusu, Mikko Sillanp\"a\"a</dc:creator>
    </item>
    <item>
      <title>Rate doubly robust estimation for weighted average treatment effects</title>
      <link>https://arxiv.org/abs/2509.14502</link>
      <description>arXiv:2509.14502v2 Announce Type: replace-cross 
Abstract: The weighted average treatment effect (WATE) defines a versatile class of causal estimands for populations characterized by propensity score weights, including the average treatment effect (ATE), treatment effect on the treated (ATT), on controls (ATC), and for the overlap population (ATO). WATE has broad applicability in social and medical research, as many datasets from these fields align with its framework. However, the literature lacks a systematic investigation into the robustness and efficiency conditions for WATE estimation. Although doubly robust (DR) estimators are well-studied for ATE, their applicability to other WATEs remains uncertain. This paper investigates whether widely used WATEs admit DR or rate doubly robust (RDR) estimators and assesses the role of nuisance function accuracy, particularly with machine learning. Using semiparametric efficient influence function (EIF) theory and double/debiased machine learning (DML), we propose three RDR estimators under specific rate and regularity conditions and evaluate their performance via Monte Carlo simulations. Applications to NHANES data on smoking and blood lead levels, and SIPP data on 401(k) eligibility, demonstrate the methods' practical relevance in medical and social sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14502v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 23 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiming Wang, Yi Liu, Shu Yang</dc:creator>
    </item>
  </channel>
</rss>
