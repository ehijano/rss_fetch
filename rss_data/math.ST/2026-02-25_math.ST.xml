<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 26 Feb 2026 02:54:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Semi-parametric Bayesian inference under Neyman orthogonality</title>
      <link>https://arxiv.org/abs/2602.20371</link>
      <description>arXiv:2602.20371v1 Announce Type: new 
Abstract: The validity of two-step or plug-in inference methods is questioned in the Bayesian framework. We study semi-parametric models where the plug-in of a non-parametrically modelled nuisance component is used. We show that when the nuisance and targeted parameters satisfy a Neyman orthogonal score property, the approach of cutting feedback through a two-step procedure is a valid way of conducting Bayesian inference. Our method relies on a non-parametric Bayesian formulation based on the Dirichlet process and the Bayesian bootstrap. We show that the marginal posterior of the targeted parameter exhibits good frequentist properties despite not accounting for the inferential uncertainty of the nuisance parameter. We adopt this approach in Bayesian causal inference problems where the nuisance propensity score model is estimated to obtain marginal inference for the treatment effect parameter, and demonstrate that a plug-in of the propensity score has a negligible effect on marginal posterior inference for the causal contrast. We investigate the absence of Neyman orthogonality and exploit our findings to show that in conventional two-step procedures, the posterior distribution converges under weaker restrictions than those needed in the frequentist sequel. For a simple family of useful scores, we demonstrate that even in the absence of Neyman orthogonality, the posterior distribution is asymptotically unchanged by the estimation of the nuisance parameter, merely provided the latter estimator is consistent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20371v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Magid Sabbagh, David A. Stephens</dc:creator>
    </item>
    <item>
      <title>Scalable multitask Gaussian processes for complex mechanical systems with functional covariates</title>
      <link>https://arxiv.org/abs/2602.20640</link>
      <description>arXiv:2602.20640v1 Announce Type: new 
Abstract: Functional covariates arise in many scientific and engineering applications when model inputs take the form of time-dependent or spatially distributed profiles, such as varying boundary conditions or changing material behaviours. In addition, new practices in digital simulation require predictions accompanied by confidence intervals. Models based on Gaussian processes (GPs) provide principled uncertainty quantification. However, GPs capable of jointly handling functional covariates and multiple correlated functional tasks remain largely under-explored. In this work, we extend the framework of GPs with functional covariates to multitask problems by introducing a fully separable kernel structure that captures dependencies across tasks and functional inputs. By taking advantage of the Kronecker structure of the covariance matrix, the model is made scalable. The proposed model is validated on a synthetic benchmark and applied to a realistic structure, a riveted assembly with functional descriptions of the material behaviour and response forces. The proposed functional multitask GP significantly improves over single task GPs. For the riveted assembly, it requires less than 100 samples to produce an accurate mean and confidence interval prediction. Despite its larger number of parameters, the multitask GP is computationally easier to learn than its single task pendant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20640v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Razak Christophe Sabi Gninkou (UPHF, INSA Hauts-De-France, CERAMATHS), Andr\'es F. L\'opez-Lopera (IMAG, LEMON, UM), Franck Massa (LAMIH, INSA Hauts-De-France, UPHF), Rodolphe Le Riche (LIMOS, UCA, ENSM ST-ETIENNE, CNRS)</dc:creator>
    </item>
    <item>
      <title>Statistical Inference in Causal Partial Identification with Smooth Densities</title>
      <link>https://arxiv.org/abs/2602.20681</link>
      <description>arXiv:2602.20681v1 Announce Type: new 
Abstract: Many causal quantities are only partially identifiable due to the inherent missingness of potential outcomes, and the associated partial identification (PI) sets can be obtained by solving an optimal transport (OT) problem. Covariates often provide additional information about the potential outcomes and thus yield tighter PI sets, which can be obtained via conditional optimal transport (COT). However, COT-based PI set estimators are susceptible to the curse of dimensionality in the covariates and outcomes, which precludes the asymptotic normality and hinders statistical inference. In this paper, we exploit smoothness in the marginal densities of covariates and potential outcomes and develop a wavelet-based primal method for COT with multivariate outcomes and covariates. Moreover, for quadratic cost functions, we establish a stability result for COT and prove asymptotic normality of the proposed estimator. This characterization of the asymptotic distribution enables valid statistical inference for the partial identification set. Empirically, we validate the estimation and inference performance of our approach through numerical experiments in comparison with existing benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20681v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sirui Lin, Zijun Gao, Jose Blanchet, Peter Glynn</dc:creator>
    </item>
    <item>
      <title>Upper Bounds for the I-MSE and max-MSE of Kernel Density Estimators</title>
      <link>https://arxiv.org/abs/2602.20815</link>
      <description>arXiv:2602.20815v1 Announce Type: new 
Abstract: The performance of kernel density estimators is usually studied via Taylor expansions and asymptotic approximation arguments, in which the bandwidth parameter tends to zero with increasing sample size. In contrast, this paper focusses directly on the finite-sample situation. Informative upper bounds are derived both for the integrated and the maximal mean squared error function. Results are reached for the traditional case, where the kernel is a probability density function, under various sets of assumptions on the underlying density to be estimated. Results are also derived for the important non-conventional case of the sinc kernel, which is not integrable and also takes negative values. We pin-point ways in which the sinc-based estimator performs better than the conventional kernel estimators. When proving our results we rely on methods related to characteristic and empirical characteristic functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20815v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Lid Hjort, Nikolai G. Ushakov</dc:creator>
    </item>
    <item>
      <title>Maximum entropy based testing in network models: ERGMs and constrained optimization</title>
      <link>https://arxiv.org/abs/2602.20844</link>
      <description>arXiv:2602.20844v1 Announce Type: new 
Abstract: Stochastic network models play a central role across a wide range of scientific disciplines, and questions of statistical inference arise naturally in this context. In this paper we investigate goodness-of-fit and two-sample testing procedures for statistical networks based on the principle of maximum entropy (MaxEnt). Our approach formulates a constrained entropy-maximization problem on the space of networks, subject to prescribed structural constraints. The resulting test statistics are defined through the Lagrange multipliers associated with the constrained optimization problem, which, to our knowledge, is novel in the statistical networks literature.
  We establish consistency in the classical regime where the number of vertices is fixed. We then consider asymptotic regimes in which the graph size grows with the sample size, developing tests for both dense and sparse settings. In the dense case, we analyze exponential random graph models (ERGM) (including the Erd\"os-R\`enyi models), while in the sparse regime our theory applies to Erd{\"o}s-R{\`e}nyi graphs.
  Our analysis leverages recent advances in nonlinear large deviation theory for random graphs. We further show that the proposed Lagrange-multiplier framework connects naturally to classical score tests for constrained maximum likelihood estimation. The results provide a unified entropy-based framework for network model assessment across diverse growth regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20844v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhrosekhar Ghosh, Rathindra Nath Karmakar, Samriddha Lahiry</dc:creator>
    </item>
    <item>
      <title>Efficient Online Learning in Interacting Particle Systems</title>
      <link>https://arxiv.org/abs/2602.20875</link>
      <description>arXiv:2602.20875v1 Announce Type: new 
Abstract: We introduce a new method for online parameter estimation in stochastic interacting particle systems, based on continuous observation of a small number of particles from the system. Our method recursively updates the model parameters using a stochastic approximation of the gradient of the asymptotic log likelihood, which is computed using the continuous stream of observations. Under suitable assumptions, we rigorously establish convergence of our method to the stationary points of the asymptotic log-likelihood of the interacting particle system. We consider asymptotics both in the limit as the time horizon $t\rightarrow\infty$, for a fixed and finite number of particles, and in the joint limit as the number of particles $N\rightarrow\infty$ and the time horizon $t\rightarrow\infty$. Under additional assumptions on the asymptotic log-likelihood, we also establish an $\mathrm{L}^2$ convergence rate and a central limit theorem. Finally, we present several numerical examples of practical interest, including a model for systemic risk, a model of interacting FitzHugh--Nagumo neurons, and a Cucker--Smale flocking model. Our numerical results corroborate our theoretical results, and also suggest that our estimator is effective even in cases where the assumptions required for our theoretical analysis do not hold.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20875v1</guid>
      <category>math.ST</category>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louis Sharrock, Nikolas Kantas, Grigorios A. Pavliotis</dc:creator>
    </item>
    <item>
      <title>On Stein's test of uniformity on the hypersphere</title>
      <link>https://arxiv.org/abs/2602.20896</link>
      <description>arXiv:2602.20896v1 Announce Type: new 
Abstract: We propose a new test of uniformity on the hypersphere based on a Stein characterization associated with the Laplace--Beltrami operator. We identify a sufficient class of test functions for this characterization, linked to the moment generating function. Exploiting the operator's eigenfunctions to obtain a harmonic decomposition in terms of Gegenbauer polynomials, we show that the proposed procedure belongs to the class of Sobolev tests. We derive closed-form expressions for the distribution of the test statistic under the null hypothesis and under fixed alternatives. To enhance power against a range of alternatives, we introduce a tuning parameter into the characterization and study its impact on rejection probabilities. We discuss data-driven strategies for selecting this parameter to maximize rejection rates for a given alternative and compare the resulting performance with that of related parametric tests. Additional numerical experiments compare the proposed test with competing Sobolev-class procedures, highlighting settings in which it offers clear advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20896v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul Axmann, Bruno Ebner, Eduardo Garc\'ia-Portugu\'es</dc:creator>
    </item>
    <item>
      <title>Adjacency Spectral Embeddings of Correlation Networks</title>
      <link>https://arxiv.org/abs/2602.21055</link>
      <description>arXiv:2602.21055v1 Announce Type: new 
Abstract: In many applications, weighted networks are constructed based on time series data: each time series is associated to a vertex and edge weights are given by pairwise correlations. The result is a network whose edge dependency structure violates the assumptions of most common network models. Nonetheless, it is common to analyze these "correlation networks" using embedding methods derived from edge-independent network models, based on a belief that the edges are approximately independent. In this work, we put this modeling choice on firm theoretical ground. We show that when the time series are expressible in terms of a small number of Fourier basis elements (or in some other suitably-chosen basis), correlation networks correspond to latent space networks with dependent edge noise in which the vertex-level latent variables encode the basis coefficients. Further, we show that when time series are observed subject to noise, spectral embedding of the resulting noisy correlation network still recovers these true vertex-level latent representations under suitable assumptions. This characterization of embeddings as learning Fourier coefficients appears to be folklore in the signal processing community in the context of principal component analysis, but is, to the best of our knowledge, new to the statistical network analysis literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21055v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Keith Levin</dc:creator>
    </item>
    <item>
      <title>Multiple Poisson-Dirichlet diffusions on generalized Kingman simplices</title>
      <link>https://arxiv.org/abs/2602.20266</link>
      <description>arXiv:2602.20266v1 Announce Type: cross 
Abstract: We construct a new class of infinite-dimensional diffusions taking values in a generalized Kingman simplex. Our model describes the temporal evolution of the relative frequencies of infinitely-many types which are "labeled" by an arbitrary finite number of marks or colors, but "unlabeled" within each mark. We start with a finite-dimensional construction which extends to Wright-Fisher diffusions a self-similarity property known for Dirichlet distributions, and corresponds to a multiple skew-product representation of the Wright-Fisher diffusion relative to the marks in the population. After ranking decreasingly the frequencies within each mark, we identify the limit in distribution of the resulting diffusion when the number of types for each mark goes to infinity, and describe its infinitesimal operator. The limiting process reduces to a diffusion in the Thoma simplex in the special case of only two marks, whereas the infinitely-many-neutral-alleles model is recovered when all frequencies have the same mark. The stationary measure of the limit diffusion is shown to be the recently introduced multiple Poisson-Dirichlet distribution, which extends Kingman's Poisson-Dirichlet distribution and is the de Finetti representing measure for a family of random partitions whose elements are marked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20266v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>q-bio.PE</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cristina Costantini, Matteo Ruggiero</dc:creator>
    </item>
    <item>
      <title>Local Fr\'echet regression with toroidal predictors</title>
      <link>https://arxiv.org/abs/2602.20572</link>
      <description>arXiv:2602.20572v1 Announce Type: cross 
Abstract: We provide the first regression framework that simultaneously accommodates responses taking values in a general metric space and predictors lying on a general torus. We propose intrinsic local constant and local linear estimators that respect the underlying geometries of both the response and predictor spaces. Our local linear estimator is novel even in the case of scalar responses. We further establish their asymptotic properties, including consistency and convergence rates. Simulation studies, together with an application to real data, illustrate the superior performance of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20572v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Jun Im, Jeong Min Jeon</dc:creator>
    </item>
    <item>
      <title>Extreme eigenvalues and eigenvectors for finite rank additive deformations of non-hermitian sparse random matrices</title>
      <link>https://arxiv.org/abs/2602.20956</link>
      <description>arXiv:2602.20956v1 Announce Type: cross 
Abstract: Consider a $n\times n$ sparse non-Hermitian random matrix $X_n$ defined as the Hadamard product between a random matrix with centered independent and identically distributed entries and a sparse Bernoulli matrix with success probability $K_n/n$ where $K_n\le n$ (and possibly $K_n\ll n$) and $K_n\to \infty$ as $n\to \infty$. Let $E_n$ be a deterministic $n\times n$ finite-rank matrix. We prove that the outlier eigenvalues of $Y_n= X_n +E_n$ asymptotically match those of $E_n$. In the special case of a rank-one deformation, assuming further that the sparsity parameter satisfies $K_n \gg \log^9(n)$ and that the entries of the random matrix are sub-Gaussian, we describe the limiting behavior of the projection of the right eigenvector associated with the leading eigenvalue onto the right eigenvector of the rank-one deformation. In particular, we prove that the projection behaves as in the Hermitian case. To that end, we rely on the recent universality results of Brailovskaya and van Handel (2024) relating the singular value spectra of deformations of $X_n$ to Gaussian analogues of these matrices. Our analysis builds upon a recent framework introduced by Bordenave et.al. (2022), and amounts to showing the asymptotic equivalence between the reverse characteristic polynomial of the random matrix and a random analytic function on the unit disc with explicit dependence on the finite-rank deformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20956v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Walid Hachem, Michail Louvaris, Jamal Najim</dc:creator>
    </item>
    <item>
      <title>Estimating the Partially Linear Zero-Inflated Poisson Regression Model: a Robust Approach Using a EM-like Algorithm</title>
      <link>https://arxiv.org/abs/2602.20965</link>
      <description>arXiv:2602.20965v1 Announce Type: cross 
Abstract: Count data with an excessive number of zeros frequently arise in fields such as economics, medicine, and public health. Traditional count models often fail to adequately handle such data, especially when the relationship between the response and some predictors is nonlinear. To overcome these limitations, the partially linear zero-inflated Poisson (PLZIP) model has been proposed as a flexible alternative. However, all existing estimation approaches for this model are based on likelihood, which is known to be highly sensitive to outliers and slight deviations from the model assumptions. This article presents the first robust estimation method specifically developed for the PLZIP model. An Expectation-Maximization-like algorithm is used to take advantage of the mixture nature of the model and to address extreme observations in both the response and the covariates. Results of the algorithm convergence and the consistency of the estimators are proved. A simulation study under various contamination schemes showed the robustness and efficiency of the proposed estimators in finite samples, compared to classical estimators. Finally, the application of the methodology is illustrated through an example using real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20965v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mar\'ia Jos\'e Llop, Andrea Bergesio, Anne-Fran\c{c}oise Yao</dc:creator>
    </item>
    <item>
      <title>Detecting Where Effects Occur by Testing Hypotheses in Order</title>
      <link>https://arxiv.org/abs/2602.21068</link>
      <description>arXiv:2602.21068v1 Announce Type: cross 
Abstract: Experimental evaluations of public policies often randomize a new intervention within many sites or blocks. After a report of an overall result -- statistically significant or not -- the natural question from a policy maker is: \emph{where} did any effects occur? Standard adjustments for multiple testing provide little power to answer this question. In simulations modeled after a 44-block education trial, the Hommel adjustment -- among the most powerful procedures controlling the family-wise error rate (FWER) -- detects effects in only 11\% of truly non-null blocks. We develop a procedure that tests hypotheses top-down through a tree: test the overall null at the root, then groups of blocks, then individual blocks, stopping any branch where the null is not rejected. In the same 44-block design, this approach detects effects in 44\% of non-null blocks -- roughly four times the detection rate. A stopping rule and valid tests at each node suffice for weak FWER control. We show that the strong-sense FWER depends on how rejection probabilities accumulate along paths through the tree. This yields a diagnostic: when power decays fast enough relative to branching, no adjustment is needed; otherwise, an adaptive $\alpha$-adjustment restores control. We apply the method to 25 MDRC education trials and provide an R package, \texttt{manytestsr}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.21068v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jake Bowers, David Kim, Nuole Chen</dc:creator>
    </item>
    <item>
      <title>Local Fr\'echet regression with circular predictors</title>
      <link>https://arxiv.org/abs/2408.10118</link>
      <description>arXiv:2408.10118v4 Announce Type: replace 
Abstract: Fr\'echet regression extends the principles of linear regression to accommodate responses valued in generic metric spaces. While this approach has primarily focused on exploring relationships between Euclidean predictors and non-Euclidean responses, our work introduces a novel statistical method for handling random objects with circular predictors. We concentrate on local constant and local linear Fr\'echet regression, providing rigorous proofs for the upper bounds of both bias and stochastic deviation of the estimators under mild conditions. This research lays the groundwork for broadening the application of Fr\'echet regression to scenarios involving non-Euclidean covariates, thereby expanding its utility in complex data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10118v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Jun Im, Jeong Min Jeon</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation with functional outcomes missing at random</title>
      <link>https://arxiv.org/abs/2411.17224</link>
      <description>arXiv:2411.17224v3 Announce Type: replace 
Abstract: We present and study semi-parametric estimators for the mean of functional outcomes in situations where some of these outcomes are missing and covariate information is available on all units. Assuming that the missingness mechanism depends only on the covariates (missing at random assumption), we present two estimators for the functional mean parameter, using working models for the functional outcome given the covariates, and the probability of missingness given the covariates. We contribute by establishing that both these estimators have Gaussian processes as limiting distributions and explicitly give their covariance functions. One of the estimators is double robust in the sense that the limiting distribution holds whenever at least one of the nuisance models is correctly specified. These results allow us to present simultaneous confidence bands for the mean function with asymptotically guaranteed coverage. A Monte Carlo study shows the finite sample properties of the proposed functional estimators and their associated simultaneous inference. The use of the method is illustrated in an application where the mean of counterfactual outcomes is targeted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17224v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xijia Liu, Kreske Felix Ecker, Lina Schelin, Xavier de Luna</dc:creator>
    </item>
    <item>
      <title>Locally Differentially Private Two-Sample Testing</title>
      <link>https://arxiv.org/abs/2505.24811</link>
      <description>arXiv:2505.24811v2 Announce Type: replace 
Abstract: We consider the problem of two-sample testing under a local differential privacy constraint where a permutation procedure is used to calibrate the tests. We develop testing procedures which are optimal up to logarithmic factors, for general discrete distributions and continuous distributions subject to a smoothness constraint. Both non-interactive and interactive tests are considered, and we show allowing interactivity results in an improvement in the minimax separation rates. Our results show that permutation procedures remain feasible in practice under local privacy constraints, despite the inability to permute the non-private data directly and only the private views. Further, through a refined theoretical analysis of the permutation procedure, we are able to avoid an equal sample size assumption which has been made in the permutation testing literature regardless of the presence of the privacy constraint. Lastly, we conduct numerical experiments which demonstrate the performance of our proposed test and verify the theoretical findings, especially the improved performance enabled by allowing interactivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24811v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Kent, Thomas B. Berrett, Yi Yu</dc:creator>
    </item>
    <item>
      <title>Parametric Mean-Field empirical Bayes in high-dimensional linear regression</title>
      <link>https://arxiv.org/abs/2601.16842</link>
      <description>arXiv:2601.16842v2 Announce Type: replace 
Abstract: In this paper, we consider the problem of parametric empirical Bayes estimation of an i.i.d. prior in high-dimensional Bayesian linear regression, with random design. We obtain the asymptotic distribution of the variational Empirical Bayes (vEB) estimator, which approximately maximizes a variational lower bound of the intractable marginal likelihood. We characterize a sharp phase transition behavior for the vEB estimator -- namely that it is information theoretically optimal (in terms of limiting variance) up to $p=o(n^{2/3})$ while it suffers from a sub-optimal convergence rate in higher dimensions. In the first regime, i.e., when $p=o(n^{2/3})$, we show how the estimated prior can be calibrated to enable valid coordinate-wise and delocalized inference, both under the \emph{empirical Bayes posterior} and the oracle posterior. In the second regime, we propose a debiasing technique as a way to improve the performance of the vEB estimator beyond $p=o(n^{2/3})$. Extensive numerical experiments corroborate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.16842v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seunghyun Lee, Nabarun Deb</dc:creator>
    </item>
    <item>
      <title>Universality of Benign Overfitting in Binary Linear Classification</title>
      <link>https://arxiv.org/abs/2501.10538</link>
      <description>arXiv:2501.10538v2 Announce Type: replace-cross 
Abstract: The practical success of deep learning has led to the discovery of several surprising phenomena. One of these phenomena, that has spurred intense theoretical research, is ``benign overfitting'': deep neural networks seem to generalize well in the over-parametrized regime even though the networks show a perfect fit to noisy training data. It is now known that benign overfitting also occurs in various classical statistical models. For linear maximum margin classifiers, benign overfitting has been established theoretically in a class of mixture models with very strong assumptions on the covariate distribution. However, even in this simple setting, many questions remain open. For instance, most of the existing literature focuses on the noiseless case where all true class labels are observed without errors, whereas the more interesting noisy case remains poorly understood. We provide a comprehensive study of benign overfitting for linear maximum margin classifiers. We discover a phase transition in test error bounds for the noisy model which was previously unknown and provide some geometric intuition behind it. We further considerably relax the required covariate assumptions in both, the noisy and noiseless case. Our results demonstrate that benign overfitting of maximum margin classifiers holds in a much wider range of scenarios than was previously known and provide new insights into the underlying mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10538v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ichiro Hashimoto, Stanislav Volgushev, Piotr Zwiernik</dc:creator>
    </item>
    <item>
      <title>Sharp Gaussian approximations for Decentralized Federated Learning</title>
      <link>https://arxiv.org/abs/2505.08125</link>
      <description>arXiv:2505.08125v3 Announce Type: replace-cross 
Abstract: Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08125v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Bonnerjee, Sayar Karmakar, Wei Biao Wu</dc:creator>
    </item>
    <item>
      <title>Enjoying Non-linearity in Multinomial Logistic Bandits: A Minimax-Optimal Algorithm</title>
      <link>https://arxiv.org/abs/2507.05306</link>
      <description>arXiv:2507.05306v3 Announce Type: replace-cross 
Abstract: We consider the multinomial logistic bandit problem in which a learner interacts with an environment by selecting actions to maximize expected rewards based on probabilistic feedback from multiple possible outcomes. In the binary setting, recent work has focused on understanding the impact of the non-linearity of the logistic model (Faury et al., 2020; Abeille et al., 2021). They introduced a problem-dependent constant $\kappa_* \geq 1$ that may be exponentially large in some problem parameters and which is captured by the derivative of the sigmoid function. It encapsulates the non-linearity and improves existing regret guarantees over $T$ rounds from $\smash{O(d\sqrt{T})}$ to $\smash{O(d\sqrt{T/\kappa_*})}$, where $d$ is the dimension of the parameter space. We extend their analysis to the multinomial logistic bandit framework with a finite action space, making it suitable for complex applications with more than two choices, such as reinforcement learning or recommender systems. To achieve this, we extend the definition of $ \kappa_* $ to the multinomial setting and propose an efficient algorithm that leverages the problem's non-linearity. Our method yields a problem-dependent regret bound of order $ \smash{\widetilde{\mathcal{O}}( R d \sqrt{ {KT}/{\kappa_*}} ) } $, where $R$ denotes the norm of the vector of rewards and $K$ is the number of outcomes. This improves upon the best existing guarantees of order $ \smash{\widetilde{\mathcal{O}}( RdK \sqrt{T} )}$. Moreover, we provide a matching $\smash{ \Omega(dR\sqrt{KT/\kappa_*})}$ lower-bound, showing that our algorithm is minimax-optimal and that our definition of $\kappa_*$ is optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05306v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pierre Boudart (SIERRA), Pierre Gaillard (Thoth), Alessandro Rudi (PSL, DI-ENS, Inria)</dc:creator>
    </item>
    <item>
      <title>Optimal and Structure-Adaptive CATE Estimation with Kernel Ridge Regression</title>
      <link>https://arxiv.org/abs/2602.18958</link>
      <description>arXiv:2602.18958v2 Announce Type: replace-cross 
Abstract: We propose an optimal algorithm for estimating conditional average treatment effects (CATEs) when response functions lie in a reproducing kernel Hilbert space (RKHS). We study settings in which the contrast function is structurally simpler than the nuisance functions: (i) it lies in a lower-complexity RKHS with faster eigenvalue decay, (ii) it satisfies a source condition relative to the nuisance kernel, or (iii) it depends on a known low-dimensional covariate representation. We develop a unified two-stage kernel ridge regression (KRR) method that attains minimax rates governed by the complexity of the contrast function rather than the nuisance class, in terms of both sample size and overlap. We also show that a simple model-selection step over candidate contrast spaces and regularization levels yields an oracle inequality, enabling adaptation to unknown CATE regularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18958v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 25 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seok-Jin Kim</dc:creator>
    </item>
  </channel>
</rss>
