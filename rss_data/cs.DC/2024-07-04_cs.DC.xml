<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Jul 2024 04:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DRLQ: A Deep Reinforcement Learning-based Task Placement for Quantum Cloud Computing</title>
      <link>https://arxiv.org/abs/2407.02748</link>
      <description>arXiv:2407.02748v1 Announce Type: new 
Abstract: The quantum cloud computing paradigm presents unique challenges in task placement due to the dynamic and heterogeneous nature of quantum computation resources. Traditional heuristic approaches fall short in adapting to the rapidly evolving landscape of quantum computing. This paper proposes DRLQ, a novel Deep Reinforcement Learning (DRL)-based technique for task placement in quantum cloud computing environments, addressing the optimization of task completion time and quantum task scheduling efficiency. It leverages the Deep Q Network (DQN) architecture, enhanced with the Rainbow DQN approach, to create a dynamic task placement strategy. This approach is one of the first in the field of quantum cloud resource management, enabling adaptive learning and decision-making for quantum cloud environments and effectively optimizing task placement based on changing conditions and resource availability. We conduct extensive experiments using the QSimPy simulation toolkit to evaluate the performance of our method, demonstrating substantial improvements in task execution efficiency and a reduction in the need to reschedule quantum tasks. Our results show that utilizing the DRLQ approach for task placement can significantly reduce total quantum task completion time by 37.81% to 72.93% and prevent task rescheduling attempts compared to other heuristic approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02748v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hoa T. Nguyen, Muhammad Usman, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Computational Performance and Energy Efficiency of ARM based HPC servers</title>
      <link>https://arxiv.org/abs/2407.03148</link>
      <description>arXiv:2407.03148v1 Announce Type: new 
Abstract: HPC world is dominated by x86 ISA CPUs. This monoculture is not necessarily justified by best performance evaluation, but may inherit from e.g. SW related restrictions on the choice of HW platforms. To avoid running (further) into path dependency, alternate HW platforms need to be evaluated for performance compared to existing HPC setup. As a result, it may turn out alternate HW platforms are more efficient for HPC. In any case, even if performance differences are low, avoiding path dependencies that stem from HW choice restrictions simplifies switching to different HW platforms in future, should suitable systems evolve. Moreover, broadening the perspective to generic HW platforms may trigger cooperation and wield influence on HW platform development, resulting in HW/SW co-design advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03148v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oskar Schirmer</dc:creator>
    </item>
    <item>
      <title>Bridging Model Heterogeneity in Federated Learning via Uncertainty-based Asymmetrical Reciprocity Learning</title>
      <link>https://arxiv.org/abs/2407.03247</link>
      <description>arXiv:2407.03247v1 Announce Type: new 
Abstract: This paper presents FedType, a simple yet pioneering framework designed to fill research gaps in heterogeneous model aggregation within federated learning (FL). FedType introduces small identical proxy models for clients, serving as agents for information exchange, ensuring model security, and achieving efficient communication simultaneously. To transfer knowledge between large private and small proxy models on clients, we propose a novel uncertainty-based asymmetrical reciprocity learning method, eliminating the need for any public data. Comprehensive experiments conducted on benchmark datasets demonstrate the efficacy and generalization ability of FedType across diverse settings. Our approach redefines federated learning paradigms by bridging model heterogeneity, eliminating reliance on public data, prioritizing client privacy, and reducing communication costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03247v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaqi Wang, Chenxu Zhao, Lingjuan Lyu, Quanzeng You, Mengdi Huai, Fenglong Ma</dc:creator>
    </item>
    <item>
      <title>RollupTheCrowd: Leveraging ZkRollups for a Scalable and Privacy-Preserving Reputation-based Crowdsourcing Platform</title>
      <link>https://arxiv.org/abs/2407.02226</link>
      <description>arXiv:2407.02226v1 Announce Type: cross 
Abstract: Current blockchain-based reputation solutions for crowdsourcing fail to tackle the challenge of ensuring both efficiency and privacy without compromising the scalability of the blockchain. Developing an effective, transparent, and privacy-preserving reputation model necessitates on-chain implementation using smart contracts. However, managing task evaluation and reputation updates alongside crowdsourcing transactions on-chain substantially strains system scalability and performance. This paper introduces RollupTheCrowd, a novel blockchain-powered crowdsourcing framework that leverages zkRollups to enhance system scalability while protecting user privacy. Our framework includes an effective and privacy-preserving reputation model that gauges workers' trustworthiness by assessing their crowdsourcing interactions. To alleviate the load on our blockchain, we employ an off-chain storage scheme, optimizing RollupTheCrowd's performance. Utilizing smart contracts and zero-knowledge proofs, our Rollup layer achieves a significant 20x reduction in gas consumption. To prove the feasibility of the proposed framework, we developed a proof-of-concept implementation using cutting-edge tools. The experimental results presented in this paper demonstrate the effectiveness and scalability of RollupTheCrowd, validating its potential for real-world application scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02226v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmed Mounsf Rafik Bendada, Mouhamed Amine Bouchiha, Mourad Rabah, Yacine Ghamri-Doudane</dc:creator>
    </item>
    <item>
      <title>The dimensionality of a coiled helical coil</title>
      <link>https://arxiv.org/abs/2407.02493</link>
      <description>arXiv:2407.02493v1 Announce Type: cross 
Abstract: The helical coil is ubiquitous in biological and natural systems and often it is the basic form that leads to complex structures. This paper considers the question of its dimensionality in biological information as the helical coil goes through recursive coiling as happens to DNA and RNA molecules in chromatin. It has been shown that the dimensionality of coiled coils is virtually equal to e. Of the three forms of DNA, the dimensionality of the B-form is nearest to the optimal value and this might be the reason why it is most common.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02493v1</guid>
      <category>physics.bio-ph</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhash Kak</dc:creator>
    </item>
    <item>
      <title>A New Classification of Clustering-based for Different Problems in Different Wireless Ad-hoc Networks</title>
      <link>https://arxiv.org/abs/2407.02498</link>
      <description>arXiv:2407.02498v1 Announce Type: cross 
Abstract: Ad-hoc networks are specifically designed to facilitate communication in environments where establishing a dedicated network infrastructure is exceedingly complex or impractical. The integration of clustering concepts into various ad-hoc network scenarios, including Wireless Sensor Networks (WSN), Mobile Ad-hoc Networks (MANET), Vehicular Ad-hoc Networks (VANET), Delay-Tolerant Networks (DTN), Wireless Ad-hoc Networks (WANET), Underwater Wireless Sensor Networks (UWSN), Unmanned Aerial Vehicle Networks (UAV), commonly known as "drones," Space Networks (SN), and Satellite Networks (TN), presents abundant opportunities for refining strategies in event tracking and area monitoring across both deterministic and uncertain environments. This paper conducts a comparative analysis of diverse proposed strategies leveraging clustering concepts to address coverage challenges within deterministic and uncertain environments. As a result, it addresses current and future challenges inherent in clustering-based WANET, elucidating the merits, shortcomings, and weaknesses of clustering models. Lastly, it identifies potential avenues for addressing coverage issues in existing and emerging technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02498v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5121/csit.2024.141010</arxiv:DOI>
      <arxiv:journal_reference>Computer Science &amp; Information Technology (CS &amp; IT), 2024</arxiv:journal_reference>
      <dc:creator>Adda Boualem, Marwane Ayaida, Hichem Sedjelmaci, Chaimaa Khalfi, Kamilia Brahimi, Bochra Khelil, Sanaa Bouchama</dc:creator>
    </item>
    <item>
      <title>Towards Federated Learning with On-device Training and Communication in 8-bit Floating Point</title>
      <link>https://arxiv.org/abs/2407.02610</link>
      <description>arXiv:2407.02610v1 Announce Type: cross 
Abstract: Recent work has shown that 8-bit floating point (FP8) can be used for efficiently training neural networks with reduced computational overhead compared to training in FP32/FP16. In this work, we investigate the use of FP8 training in a federated learning context. This brings not only the usual benefits of FP8 which are desirable for on-device training at the edge, but also reduces client-server communication costs due to significant weight compression. We present a novel method for combining FP8 client training while maintaining a global FP32 server model and provide convergence analysis. Experiments with various machine learning models and datasets show that our method consistently yields communication reductions of at least 2.9x across a variety of tasks and models compared to an FP32 baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02610v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bokun Wang, Axel Berg, Durmus Alp Emre Acar, Chuteng Zhou</dc:creator>
    </item>
    <item>
      <title>Accelerating Distributed Optimization: A Primal-Dual Perspective on Local Steps</title>
      <link>https://arxiv.org/abs/2407.02689</link>
      <description>arXiv:2407.02689v1 Announce Type: cross 
Abstract: In distributed machine learning, efficient training across multiple agents with different data distributions poses significant challenges. Even with a centralized coordinator, current algorithms that achieve optimal communication complexity typically require either large minibatches or compromise on gradient complexity. In this work, we tackle both centralized and decentralized settings across strongly convex, convex, and nonconvex objectives. We first demonstrate that a basic primal-dual method, (Accelerated) Gradient Ascent Multiple Stochastic Gradient Descent (GA-MSGD), applied to the Lagrangian of distributed optimization inherently incorporates local updates, because the inner loops of running Stochastic Gradient Descent on the primal variable require no inter-agent communication. Notably, for strongly convex objectives, we show (Accelerated) GA-MSGD achieves linear convergence in communication rounds despite the Lagrangian being only linear in the dual variables. This is due to a unique structural property where the dual variable is confined to the span of the coupling matrix, rendering the dual problem strongly concave. When integrated with the Catalyst framework, our approach achieves nearly optimal communication complexity across various settings without the need for minibatches. Moreover, in stochastic decentralized problems, it attains communication complexities comparable to those in deterministic settings, improving over existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02689v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Junchi Yang, Murat Yildirim, Qiu Feng</dc:creator>
    </item>
    <item>
      <title>On the Client Preference of LLM Fine-tuning in Federated Learning</title>
      <link>https://arxiv.org/abs/2407.03038</link>
      <description>arXiv:2407.03038v1 Announce Type: cross 
Abstract: Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using preference datasets, enabling the LLM to generate outputs that align with human preferences. Given the sensitive nature of these preference datasets held by various clients, there is a need to implement RLHF within a federated learning (FL) framework, where clients are reluctant to share their data due to privacy concerns. To address this, we introduce a feasible framework in which clients collaboratively train a binary selector with their preference datasets using our proposed FedBis. With a well-trained selector, we can further enhance the LLM that generates human-preferred completions. Meanwhile, we propose a novel algorithm, FedBiscuit, that trains multiple selectors by organizing clients into balanced and disjoint clusters based on their preferences. Compared to the FedBis, FedBiscuit demonstrates superior performance in simulating human preferences for pairwise completions. Our extensive experiments on federated human preference datasets -- marking the first benchmark to address heterogeneous data partitioning among clients -- demonstrate that FedBiscuit outperforms FedBis and even surpasses traditional centralized training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03038v1</guid>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feijie Wu, Xiaoze Liu, Haoyu Wang, Xingchen Wang, Jing Gao</dc:creator>
    </item>
    <item>
      <title>Effective Heterogeneous Federated Learning via Efficient Hypernetwork-based Weight Generation</title>
      <link>https://arxiv.org/abs/2407.03086</link>
      <description>arXiv:2407.03086v1 Announce Type: cross 
Abstract: While federated learning leverages distributed client resources, it faces challenges due to heterogeneous client capabilities. This necessitates allocating models suited to clients' resources and careful parameter aggregation to accommodate this heterogeneity. We propose HypeMeFed, a novel federated learning framework for supporting client heterogeneity by combining a multi-exit network architecture with hypernetwork-based model weight generation. This approach aligns the feature spaces of heterogeneous model layers and resolves per-layer information disparity during weight aggregation. To practically realize HypeMeFed, we also propose a low-rank factorization approach to minimize computation and memory overhead associated with hypernetworks. Our evaluations on a real-world heterogeneous device testbed indicate that HypeMeFed enhances accuracy by 5.12% over FedAvg, reduces the hypernetwork memory requirements by 98.22%, and accelerates its operations by 1.86 times compared to a naive hypernetwork approach. These results demonstrate HypeMeFed's effectiveness in leveraging and engaging heterogeneous clients for federated learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03086v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujin Shin, Kichang Lee, Sungmin Lee, You Rim Choi, Hyung-Sin Kim, JeongGil Ko</dc:creator>
    </item>
    <item>
      <title>Streaming Large-Scale Electron Microscopy Data to a Supercomputing Facility</title>
      <link>https://arxiv.org/abs/2407.03215</link>
      <description>arXiv:2407.03215v1 Announce Type: cross 
Abstract: Data management is a critical component of modern experimental workflows. As data generation rates increase, transferring data from acquisition servers to processing servers via conventional file-based methods is becoming increasingly impractical. The 4D Camera at the National Center for Electron Microscopy (NCEM) generates data at a nominal rate of 480 Gbit/s (87,000 frames/s) producing a 700 GB dataset in fifteen seconds. To address the challenges associated with storing and processing such quantities of data, we developed a streaming workflow that utilizes a high-speed network to connect the 4D Camera's data acquisition (DAQ) system to supercomputing nodes at the National Energy Research Scientific Computing Center (NERSC), bypassing intermediate file storage entirely. In this work, we demonstrate the effectiveness of our streaming pipeline in a production setting through an hour-long experiment that generated over 10 TB of raw data, yielding high-quality datasets suitable for advanced analyses. Additionally, we compare the efficacy of this streaming workflow against the conventional file-transfer workflow by conducting a post-mortem analysis on historical data from experiments performed by real users. Our findings show that the streaming workflow significantly improves data turnaround time, enables real-time decision-making, and minimizes the potential for human error by eliminating manual user interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03215v1</guid>
      <category>physics.ins-det</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel S. Welborn, Chris Harris, Stephanie M. Ribet, Georgios Varnavides, Colin Ophus, Bjoern Enders, Peter Ercius</dc:creator>
    </item>
    <item>
      <title>Synergy: Towards On-Body AI via Tiny AI Accelerator Collaboration on Wearables</title>
      <link>https://arxiv.org/abs/2401.08637</link>
      <description>arXiv:2401.08637v2 Announce Type: replace 
Abstract: The advent of tiny artificial intelligence (AI) accelerators enables AI to run at the extreme edge, offering reduced latency, lower power cost, and improved privacy. When integrated into wearable devices, these accelerators open exciting opportunities, allowing various AI apps to run directly on the body. We present Synergy that provides AI apps with best-effort performance via system-driven holistic collaboration over AI accelerator-equipped wearables. To achieve this, Synergy provides device-agnostic programming interfaces to AI apps, giving the system visibility and controllability over the app's resource use. Then, Synergy maximizes the inference throughput of concurrent AI models by creating various execution plans for each app considering AI accelerator availability and intelligently selecting the best set of execution plans. Synergy further improves throughput by leveraging parallelization opportunities over multiple computation units. Our evaluations with 7 baselines and 8 models demonstrate that, on average, Synergy achieves a 23.0 times improvement in throughput, while reducing latency by 73.9% and power consumption by 15.8%, compared to the baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.08637v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Taesik Gong, Si Young Jang, Utku G\"unay Acer, Fahim Kawsar, Chulhong Min</dc:creator>
    </item>
    <item>
      <title>Parm: Efficient Training of Large Sparsely-Activated Models with Dedicated Schedules</title>
      <link>https://arxiv.org/abs/2407.00599</link>
      <description>arXiv:2407.00599v2 Announce Type: replace 
Abstract: Sparsely-activated Mixture-of-Expert (MoE) layers have found practical applications in enlarging the model size of large-scale foundation models, with only a sub-linear increase in computation demands. Despite the wide adoption of hybrid parallel paradigms like model parallelism, expert parallelism, and expert-sharding parallelism (i.e., MP+EP+ESP) to support MoE model training on GPU clusters, the training efficiency is hindered by communication costs introduced by these parallel paradigms. To address this limitation, we propose Parm, a system that accelerates MP+EP+ESP training by designing two dedicated schedules for placing communication tasks. The proposed schedules eliminate redundant computations and communications and enable overlaps between intra-node and inter-node communications, ultimately reducing the overall training time. As the two schedules are not mutually exclusive, we provide comprehensive theoretical analyses and derive an automatic and accurate solution to determine which schedule should be applied in different scenarios. Experimental results on an 8-GPU server and a 32-GPU cluster demonstrate that Parm outperforms the state-of-the-art MoE training system, DeepSpeed-MoE, achieving 1.13$\times$ to 5.77$\times$ speedup on 1296 manually configured MoE layers and approximately 3$\times$ improvement on two real-world MoE models based on BERT and GPT-2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00599v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinglin Pan, Wenxiang Lin, Shaohuai Shi, Xiaowen Chu, Weinong Sun, Bo Li</dc:creator>
    </item>
    <item>
      <title>The Gossiping Insert-Eliminate Algorithm for Multi-Agent Bandits</title>
      <link>https://arxiv.org/abs/2001.05452</link>
      <description>arXiv:2001.05452v4 Announce Type: replace-cross 
Abstract: We consider a decentralized multi-agent Multi Armed Bandit (MAB) setup consisting of $N$ agents, solving the same MAB instance to minimize individual cumulative regret. In our model, agents collaborate by exchanging messages through pairwise gossip style communications on an arbitrary connected graph. We develop two novel algorithms, where each agent only plays from a subset of all the arms. Agents use the communication medium to recommend only arm-IDs (not samples), and thus update the set of arms from which they play. We establish that, if agents communicate $\Omega(\log(T))$ times through any connected pairwise gossip mechanism, then every agent's regret is a factor of order $N$ smaller compared to the case of no collaborations. Furthermore, we show that the communication constraints only have a second order effect on the regret of our algorithm. We then analyze this second order term of the regret to derive bounds on the regret-communication tradeoffs. Finally, we empirically evaluate our algorithm and conclude that the insights are fundamental and not artifacts of our bounds. We also show a lower bound which gives that the regret scaling obtained by our algorithm cannot be improved even in the absence of any communication constraints. Our results thus demonstrate that even a minimal level of collaboration among agents greatly reduces regret for all agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.05452v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronshee Chawla, Abishek Sankararaman, Ayalvadi Ganesh, Sanjay Shakkottai</dc:creator>
    </item>
    <item>
      <title>Collaborative Multi-Agent Heterogeneous Multi-Armed Bandits</title>
      <link>https://arxiv.org/abs/2305.18784</link>
      <description>arXiv:2305.18784v2 Announce Type: replace-cross 
Abstract: The study of collaborative multi-agent bandits has attracted significant attention recently. In light of this, we initiate the study of a new collaborative setting, consisting of $N$ agents such that each agent is learning one of $M$ stochastic multi-armed bandits to minimize their group cumulative regret. We develop decentralized algorithms which facilitate collaboration between the agents under two scenarios. We characterize the performance of these algorithms by deriving the per agent cumulative regret and group regret upper bounds. We also prove lower bounds for the group regret in this setting, which demonstrates the near-optimal behavior of the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18784v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronshee Chawla, Daniel Vial, Sanjay Shakkottai, R. Srikant</dc:creator>
    </item>
    <item>
      <title>TriMe++: Multi-threaded triangular meshing in two dimensions</title>
      <link>https://arxiv.org/abs/2309.13824</link>
      <description>arXiv:2309.13824v2 Announce Type: replace-cross 
Abstract: We present TriMe++, a multi-threaded software library designed for generating two-dimensional meshes for intricate geometric shapes using the Delaunay triangulation. Multi-threaded parallel computing is implemented throughout the meshing procedure, making it suitable for fast generation of large-scale meshes. Three iterative meshing algorithms are implemented: the DistMesh algorithm, the centroidal Voronoi diagram meshing, and a hybrid of the two. We compare the performance of the three meshing methods in TriMe++, and show that the hybrid method retains the advantages of the other two. The software library achieves significant parallel speedup when generating a large mesh with $10^6$ points. TriMe++ can handle complicated geometries and generates adaptive meshes of high quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13824v2</guid>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayin Lu, Chris H. Rycroft</dc:creator>
    </item>
    <item>
      <title>JumpBackHash: Say Goodbye to the Modulo Operation to Distribute Keys Uniformly to Buckets</title>
      <link>https://arxiv.org/abs/2403.18682</link>
      <description>arXiv:2403.18682v2 Announce Type: replace-cross 
Abstract: The distribution of keys to a given number of buckets is a fundamental task in distributed data processing and storage. A simple, fast, and therefore popular approach is to map the hash values of keys to buckets based on the remainder after dividing by the number of buckets. Unfortunately, these mappings are not stable when the number of buckets changes, which can lead to severe spikes in system resource utilization, such as network or database requests. Consistent hash algorithms can minimize remappings, but are either significantly slower than the modulo-based approach, require floating-point arithmetic, or are based on a family of hash functions rarely available in standard libraries. This paper introduces JumpBackHash, which uses only integer arithmetic and a standard pseudorandom generator. Due to its speed and simple implementation, it can safely replace the modulo-based approach to improve assignment and system stability. A production-ready Java implementation of JumpBackHash has been released as part of the Hash4j open source library.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18682v2</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Otmar Ertl</dc:creator>
    </item>
    <item>
      <title>GraphSnapShot: Graph Machine Learning Acceleration with Fast Storage and Retrieval</title>
      <link>https://arxiv.org/abs/2406.17918</link>
      <description>arXiv:2406.17918v2 Announce Type: replace-cross 
Abstract: In our recent research, we have developed a framework called GraphSnapShot, which has been proven an useful tool for graph learning acceleration. GraphSnapShot is a framework for fast cache, storage, retrieval and computation for graph learning. It can quickly store and update the local topology of graph structure and allows us to track patterns in the structure of graph networks, just like take snapshots of the graphs. In experiments, GraphSnapShot shows efficiency, it can achieve up to 30% training acceleration and 73% memory reduction for lossless graph ML training compared to current baselines such as dgl.This technique is particular useful for large dynamic graph learning tasks such as social media analysis and recommendation systems to process complex relationships between entities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17918v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dong Liu, Roger Waleffe, Meng Jiang, Shivaram Venkataraman</dc:creator>
    </item>
    <item>
      <title>Maximizing Blockchain Performance: Mitigating Conflicting Transactions through Parallelism and Dependency Management</title>
      <link>https://arxiv.org/abs/2407.01426</link>
      <description>arXiv:2407.01426v2 Announce Type: replace-cross 
Abstract: While blockchains initially gained popularity in the realm of cryptocurrencies, their widespread adoption is expanding beyond conventional applications, driven by the imperative need for enhanced data security. Despite providing a secure network, blockchains come with certain tradeoffs, including high latency, lower throughput, and an increased number of transaction failures. A pivotal issue contributing to these challenges is the improper management of "conflicting transactions", commonly referred to as "contention". When a number of pending transactions within a blockchain collide with each other, this results in a state of contention. This situation worsens network latency, leads to the wastage of system resources, and ultimately contributes to reduced throughput and higher transaction failures. In response to this issue, in this work, we present a novel blockchain scheme that integrates transaction parallelism and an intelligent dependency manager aiming to reduce the occurrence of conflicting transactions within blockchain networks. In terms of effectiveness and efficiency, experimental results show that our scheme not only mitigates the challenges posed by conflicting transactions, but also outperforms both existing parallel and non-parallel Hyperledger Fabric blockchain networks achieving higher transaction success rate, throughput, and latency. The integration of our scheme with Hyperledger Fabric appears to be a promising solution for improving the overall performance and stability of blockchain networks in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01426v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faisal Haque Bappy, Tarannum Shaila Zaman, Md Sajidul Islam Sajid, Mir Mehedi Ahsan Pritom, Tariqul Islam</dc:creator>
    </item>
  </channel>
</rss>
