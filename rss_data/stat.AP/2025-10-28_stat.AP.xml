<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 29 Oct 2025 04:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A modified particle filter that reduces weight collapse</title>
      <link>https://arxiv.org/abs/2510.23740</link>
      <description>arXiv:2510.23740v1 Announce Type: new 
Abstract: Particle filters are a widely used Monte Carlo based data assimilation technique that estimates the probability distribution of a system's state conditioned on observations through a collection of weights and particles. A known problem for particle filters is weight collapse, or degeneracy, where a single weight attains a value of one while all others are close to zero, thereby collapsing the estimated distribution. We address this issue by introducing a novel modification to the particle filter that is simple to implement and inspired by energy-based diversity measures. Our approach adjusts particle weights to minimize a two-body energy potential, promoting balanced weight distributions and mitigating collapse. We demonstrate the performance of this modified particle filter in a series of numerical experiments with linear and nonlinear dynamical models, where we compare with the classical particle filter and ensemble Kalman filters in the nonlinear case. We find that our new approach improves weight distributions compared to the classical particle filter and thereby improve state estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23740v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shay Gilpin, Michael Herty</dc:creator>
    </item>
    <item>
      <title>A web-based user interface for Fam3PRO, a multi-gene, multi-cancer risk prediction model for families with cancer history</title>
      <link>https://arxiv.org/abs/2510.23805</link>
      <description>arXiv:2510.23805v1 Announce Type: new 
Abstract: Purpose: Hereditary cancer risk is key to guiding screening and prevention strategies. Cancer risks can vary by individual due to the presence or absence of high- and moderate-risk pathogenic variants (PV) in cancer-associated genes, in addition to sex, age, and other risk factors. We previously developed Fam3PRO, a flexible multi-gene, multi-cancer Mendelian risk prediction model that estimates a patient's risk of carrying a PV in hereditary cancer genes and their future risk of developing several types of cancer. The Fam3PRO R package includes 22 genes with 18 associated cancers, allowing users to build customized sub-models from any gene-cancer set. However, the current R package lacks a user interface (UI), limiting its practical use in clinical settings. Therefore, we aim to develop a web-based UI for broader use of the Fam3PRO functionalities.
  Methods: The Fam3PRO UI (F3PI), built with R Shiny, collects and formats inputs including family health history, genetic test results, and other risk factors. Pedigree data are interactively visualized and modified via pedigreejs, while the backend Fam3PRO model takes all the inputs to generate carrier probabilities and future cancer risks, presented through an interactive UI.
  Results: F3PI streamlines the collection of patient and family history data, which is analyzed by the Fam3PRO models to provide personalized cancer risks for each proband across 18 cancers, as well as probabilities that a proband has a PV in up to 22 hereditary cancer genes. These results are returned to the user, within one minute on average and are available in both interactive and downloadable formats.
  Conclusion: We have developed F3PI, an easy-to-use, interactive web application that makes cancer and genetic risk information more accessible to providers and their patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23805v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xueying Chen, Jianfeng Ke, Lauren Flynn, Giovanni Parmigiani, Danielle Braun</dc:creator>
    </item>
    <item>
      <title>Universal Inference for Testing Calibration of Mean Estimates within the Exponential Dispersion Family</title>
      <link>https://arxiv.org/abs/2510.23821</link>
      <description>arXiv:2510.23821v1 Announce Type: new 
Abstract: Calibration of mean estimates for predictions is a crucial property in many applications, particularly in the fields of financial and actuarial decision-making. In this paper, we first review classical approaches for validating mean-calibration, and we discuss the Likelihood Ratio Test (LRT) within the Exponential Dispersion Family (EDF). Then, we investigate the framework of universal inference to test for mean-calibration. We develop a sub-sampled split LRT within the EDF that provides finite sample guarantees with universally valid critical values. We investigate type I error, power and e-power of this sub-sampled split LRT, we compare it to the classical LRT, and we propose a novel test statistics based on the sub-sampled split LRT to enhance the performance of the calibration test. A numerical analysis verifies that our proposal is an attractive alternative to the classical LRT achieving a high power in detecting miscalibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23821v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>{\L}ukasz Delong, Mario W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Forecasting Melting Points in Svalbard, Norway Using Quantile Gradient Boosting and Adaptive Conformal Prediction Region</title>
      <link>https://arxiv.org/abs/2510.23976</link>
      <description>arXiv:2510.23976v1 Announce Type: new 
Abstract: Using data from the Longyearbyen weather station, quantile gradient boosting (``small AI'') is applied to forecast daily 2023 temperatures in Svalbard, Norway. The 0.60 quantile loss weights underestimates about 1.5 times more than overestimates. Predictors include five routinely collected indicators of weather conditions, each lagged by 14~days, yielding temperature forecasts with a two-week lead time. Conformal prediction regions quantify forecasting uncertainty with provably valid coverage. Forecast accuracy is evaluated with attention to local stakeholder concerns, and implications for Arctic adaptation policy are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23976v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Richard Berk</dc:creator>
    </item>
    <item>
      <title>Machine Learning for the Production of Official Statistics: Density Ratio Estimation using Biased Transaction Data for Japanese labor statistics</title>
      <link>https://arxiv.org/abs/2510.24153</link>
      <description>arXiv:2510.24153v1 Announce Type: new 
Abstract: National statistical institutes are beginning to use non-traditional data sources to produce official statistics. These sources, originally collected for non-statistical purposes, include point-of-sales(POS) data and mobile phone global positioning system(GPS) data. Such data have the potential to significantly enhance the usefulness of official statistics. In the era of big data, many private companies are accumulating vast amounts of transaction data. Exploring how to leverage these data for official statistics is increasingly important. However, progress has been slower than expected, mainly because such data are not collected through sample-based survey methods and therefore exhibit substantial selection bias. If this bias can be properly addressed, these data could become a valuable resource for official statistics, substantially expanding their scope and improving the quality of decision-making, including economic policy. This paper demonstrates that even biased transaction data can be useful for producing official statistics for prompt release, by drawing on the concepts of density ratio estimation and supervised learning under covariate shift, both developed in the field of machine learning. As a case study, we show that preliminary statistics can be produced in a timely manner using biased data from a Japanese private employment agency. This approach enables the early release of a key labor market indicator that would otherwise be delayed by up to a year, thereby making it unavailable for timely decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24153v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Takada, Kiyoshi Izumi</dc:creator>
    </item>
    <item>
      <title>Streamlining business functions in official statistical production with Machine Learning</title>
      <link>https://arxiv.org/abs/2510.24394</link>
      <description>arXiv:2510.24394v1 Announce Type: new 
Abstract: We provide a description of pilot and production experiences to streamline some business functions in the official statistical production process using statistical learning models. Our approach is quality-oriented searching for an improvement on accuracy, cost-efficiency, timeliness, granularity, response burden reduction, and frequency. Pilot experiences have been conducted with data from real surveys in Statistics Spain (INE).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24394v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sandra Barrag\'an, Adri\'an P\'erez-Bote, Carlos S\'aez, David Salgado, Luis Sanguiao-Sande</dc:creator>
    </item>
    <item>
      <title>GNAR-HARX Models for Realised Volatility: Incorporating Exogenous Predictors and Network Effects</title>
      <link>https://arxiv.org/abs/2510.24443</link>
      <description>arXiv:2510.24443v1 Announce Type: new 
Abstract: This project introduces the GNAR-HARX model, which combines Generalised Network Autoregressive (GNAR) structure with Heterogeneous Autoregressive (HAR) dynamics and exogenous predictors such as implied volatility. The model is designed for forecasting realised volatility by capturing both temporal persistence and cross-sectional spillovers in financial markets. We apply it to daily realised variance data for ten international stock indices, generating one-step-ahead forecasts in a rolling window over an out-of-sample period of approximately 16 years (2005-2020).
  Forecast accuracy is evaluated using the Quasi-Likelihood (QLIKE) loss and mean squared error (MSE), and we compare global, standard, and local variants across different network structures and exogenous specifications. The best model found by QLIKE is a local GNAR-HAR without exogenous variables, while the lowest MSE is achieved by a standard GNAR-HARX with implied volatility. Fully connected networks consistently outperform dynamically estimated graphical lasso networks.
  Overall, local and standard GNAR-HAR(X) models deliver the strongest forecasts, though at the cost of more parameters than the parsimonious global variant, which nevertheless remains competitive. Across all cases, GNAR-HAR(X) models outperform univariate HAR(X) benchmarks, which often require more parameters than the GNAR-based specifications. While the top model found by QLIKE does not use exogenous variables, implied volatility and overnight returns emerge as the most useful predictors when included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24443v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom \'O Nuall\'ain</dc:creator>
    </item>
    <item>
      <title>Assessing the influence of social media feedback on traveler's future trip-planning behavior: A multi-model machine learning approach</title>
      <link>https://arxiv.org/abs/2510.24077</link>
      <description>arXiv:2510.24077v1 Announce Type: cross 
Abstract: With the surge of domestic tourism in India and the influence of social media on young tourists, this paper aims to address the research question on how "social return" - responses received on social media sharing - of recent trip details can influence decision-making for short-term future travels. The paper develops a multi-model framework to build a predictive machine learning model that establishes a relationship between a traveler's social return, various social media usage, trip-related factors, and her future trip-planning behavior. The primary data was collected via a survey from Indian tourists. After data cleaning, the imbalance in the data was addressed using a robust oversampling method, and the reliability of the predictive model was ensured by applying a Monte Carlo cross-validation technique. The results suggest at least 75% overall accuracy in predicting the influence of social return on changing the future trip plan. Moreover, the model fit results provide crucial practical implications for the domestic tourism sector in India with future research directions concerning social media, destination marketing, smart tourism, heritage tourism, etc.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24077v1</guid>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/10941665.2025.2574034</arxiv:DOI>
      <arxiv:journal_reference>Asia Pacific Journal of Tourism Research, 2025,</arxiv:journal_reference>
      <dc:creator>Sayantan Mukherjee, Pritam Ranjan, Joysankar Bhattacharya</dc:creator>
    </item>
    <item>
      <title>An N-of-1 Artificial Intelligence Ecosystem for Precision Medicine</title>
      <link>https://arxiv.org/abs/2510.24359</link>
      <description>arXiv:2510.24359v1 Announce Type: cross 
Abstract: Artificial intelligence in medicine is built to serve the average patient. By minimizing error across large datasets, most systems deliver strong aggregate accuracy yet falter at the margins: patients with rare variants, multimorbidity, or underrepresented demographics. This average patient fallacy erodes both equity and trust. We propose a different design: a multi-agent ecosystem for N-of-1 decision support. In this environment, agents clustered by organ systems, patient populations, and analytic modalities draw on a shared library of models and evidence synthesis tools. Their results converge in a coordination layer that weighs reliability, uncertainty, and data density before presenting the clinician with a decision-support packet: risk estimates bounded by confidence ranges, outlier flags, and linked evidence. Validation shifts from population averages to individual reliability, measured by error in low-density regions, calibration in the small, and risk--coverage trade-offs. Anticipated challenges include computational demands, automation bias, and regulatory fit, addressed through caching strategies, consensus checks, and adaptive trial frameworks. By moving from monolithic models to orchestrated intelligence, this approach seeks to align medical AI with the first principle of medicine: care that is transparent, equitable, and centered on the individual.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24359v1</guid>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedram Fard, Alaleh Azhir, Neguine Rezaii, Jiazi Tian, Hossein Estiri</dc:creator>
    </item>
    <item>
      <title>Quality Coefficients for Interferometric Phase Linking</title>
      <link>https://arxiv.org/abs/2510.24512</link>
      <description>arXiv:2510.24512v1 Announce Type: cross 
Abstract: In multi-temporal InSAR, phase linking refers to the estimation of a single-reference interferometric phase history from the information contained in the coherence matrix of a distributed scatterer. Since the phase information in the coherence matrix is typically inconsistent, the extent to which the estimated phase history captures it must be assessed to exclude unreliable pixels from further processing. We introduce three quality criteria in the form of coefficients, for threshold-based pixel selection: a coefficient based on closure phase that quantifies the internal consistency of the phase information in the coherence matrix; a goodness-of-fit coefficient that quantifies how well a resulting phase history estimate approximates the phase information according to the characteristic optimization model of a given phase linking method; and an ambiguity coefficient that compares the goodness of fit of the original estimate with that of an orthogonal alternative. We formulate the phase linking methods and these criteria within a unified mathematical framework and discuss computational and algorithmic aspects. Unlike existing goodness-of-fit indicators, the proposed coefficients are normalized to the unit interval with explicit noise-floor correction, improving interpretability across stacks of different size. Experiments on TerraSAR-X data over Visp, Switzerland, indicate that the closure phase coefficient effectively pre-screens stable areas, the goodness-of-fit coefficient aligns with and systematically generalizes established quality indicators, and the ambiguity coefficient flags solutions that fit well but are unstable. Together, the coefficients enable systematic pixel selection and quality control in the interferometric processing of distributed scatterers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24512v1</guid>
      <category>eess.SP</category>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Magnus Heimpel, Irena Hajnsek, Othmar Frey</dc:creator>
    </item>
    <item>
      <title>Imaging Genetics Analysis of Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2510.22723</link>
      <description>arXiv:2510.22723v2 Announce Type: replace 
Abstract: Alzheimer's disease (AD) is a progressive neurodegenerative disorder characterized by cognitive decline, structural brain changes, and genetic predispositions. This study leverages machine-learning and statistical techniques to investigate the mechanistic relationships between cognitive function, genetic markers, and neuroimaging biomarkers in AD progression. Using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI), we perform both low-dimensional and high-dimensional analyses to identify key predictors of disease states, including cognitively normal (CN), mild cognitive impairment (MCI), and AD. Our low-dimensional approach utilizes multiple linear and ordinal logistic regression to examine the influence of cognitive scores, cerebrospinal fluid (CSF) biomarkers, and demographic factors on disease classification. The results highlight significant associations between Mini-Mental State Examination (MMSE), Clinical Dementia Rating Sum of Boxes (CDRSB), and phosphorylated tau levels in predicting cognitive decline. The high-dimensional analysis employs Sure Independence Screening (SIS) and LASSO regression to reduce dimensionality and identify genetic markers correlated with cognitive impairment and white matter integrity. Genes such as CLIC1, NAB2, and TGFBR1 emerge as significant predictors across multiple analyses, linking genetic expression to neurodegeneration. Additionally, imaging genetic analysis reveals shared genetic influences across brain hemispheres and the corpus callosum, suggesting distinct genetic contributions to white matter degradation. These findings enhance our understanding of AD pathology by integrating cognitive, genetic, and imaging data. Future research should explore longitudinal analyses and potential gene-environment interactions to further elucidate the biological mechanisms underlying AD progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.22723v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riddhik Basu, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Program Evaluation with Remotely Sensed Outcomes</title>
      <link>https://arxiv.org/abs/2411.10959</link>
      <description>arXiv:2411.10959v3 Announce Type: replace-cross 
Abstract: Economists often estimate treatment effects in experiments using remotely sensed variables (RSVs), e.g., satellite images or mobile phone activity, in place of directly measured economic outcomes. A common practice is to use an observational sample to train a predictor of the economic outcome from the RSV, and then use these predictions as the outcomes in the experiment. We show that this method is biased whenever the RSV is a post-outcome variable, meaning that variation in the economic outcome causes variation in the RSV. For example, changes in poverty or environmental quality cause changes in satellite images, but not vice versa. As our main result, we nonparametrically identify the treatment effect by formalizing the intuition underlying common practice: the conditional distribution of the RSV given the outcome and treatment is stable across samples. Our identifying formula reveals that efficient inference requires predictions of three quantities from the RSV -- the outcome, treatment, and sample indicator -- whereas common practice only predicts the outcome. Valid inference does not require any rate conditions on RSV predictions, justifying the use of complex deep learning algorithms with unknown statistical properties. We reanalyze the effect of an anti-poverty program in India using satellite images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10959v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashesh Rambachan, Rahul Singh, Davide Viviano</dc:creator>
    </item>
    <item>
      <title>Selecting Critical Scenarios of DER Adoption in Distribution Grids Using Bayesian Optimization</title>
      <link>https://arxiv.org/abs/2501.14118</link>
      <description>arXiv:2501.14118v2 Announce Type: replace-cross 
Abstract: We develop a new methodology to select scenarios of DER adoption most critical for distribution grids. Anticipating risks of future voltage and line flow violations due to additional PV adopters is central for utility investment planning but continues to rely on deterministic or ad hoc scenario selection. We propose a highly efficient search framework based on multi-objective Bayesian Optimization. We treat underlying grid stress metrics as computationally expensive black-box functions, approximated via Gaussian Process surrogates and design an acquisition function based on probability of scenarios being Pareto-critical across a collection of line- and bus-based violation objectives. Our approach provides a statistical guarantee and offers an order of magnitude speed-up relative to a conservative exhaustive search. Case studies on realistic feeders with 200-400 buses demonstrate the effectiveness and accuracy of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14118v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Olivier Mulkin, Miguel Heleno, Mike Ludkovski</dc:creator>
    </item>
    <item>
      <title>Nonparametric Bayesian Multi-Treatment Mixture Cure Survival Model with Application in Pediatric Oncology</title>
      <link>https://arxiv.org/abs/2508.08975</link>
      <description>arXiv:2508.08975v5 Announce Type: replace-cross 
Abstract: Heterogeneous treatment effect estimation is critical in oncology, particularly in multi-arm trials with overlapping therapeutic components and long-term survivors. These shared mechanisms pose a central challenge to identifying causal effects in precision medicine. We propose a novel covariate-dependent nonparametric Bayesian multi-treatment cure survival model that jointly accounts for common structures among treatments and cure fractions. Through latent link functions, our model leverages sharing among treatments through a flexible modeling approach, enabling individualized survival inference. We adopt a Bayesian route for inference and implement an efficient MCMC algorithm for approximating the posterior. Simulation studies demonstrate the method's robustness and superiority in various specification scenarios. Finally, application to the AALL0434 trial reveals clinically meaningful differences in survival across methotrexate-based regimens and their associations with different covariates, underscoring its practical utility for learning treatment effects in real-world pediatric oncology data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08975v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Chang, John Kairalla, Arkaprava Roy</dc:creator>
    </item>
    <item>
      <title>Robustness is Important: Limitations of LLMs for Data Fitting</title>
      <link>https://arxiv.org/abs/2508.19563</link>
      <description>arXiv:2508.19563v3 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are being applied in a wide array of settings, well beyond the typical language-oriented use cases. In particular, LLMs are increasingly used as a plug-and-play method for fitting data and generating predictions. Prior work has shown that LLMs, via in-context learning or supervised fine-tuning, can perform competitively with many tabular supervised learning techniques in terms of predictive performance. However, we identify a critical vulnerability of using LLMs for data fitting -- making changes to data representation that are completely irrelevant to the underlying learning task can drastically alter LLMs' predictions on the same data. For example, simply changing variable names can sway the size of prediction error by as much as 82% in certain settings. Such prediction sensitivity with respect to task-irrelevant variations manifests under both in-context learning and supervised fine-tuning, for both close-weight and open-weight general-purpose LLMs. Moreover, by examining the attention scores of an open-weight LLM, we discover a non-uniform attention pattern: training examples and variable names/values which happen to occupy certain positions in the prompt receive more attention when output tokens are generated, even though different positions are expected to receive roughly the same attention. This partially explains the sensitivity in the presence of task-irrelevant variations. We also consider a state-of-the-art tabular foundation model (TabPFN) trained specifically for data fitting. Despite being explicitly designed to achieve prediction robustness, TabPFN is still not immune to task-irrelevant variations. Overall, despite LLMs' impressive predictive capabilities, currently they lack even the basic level of robustness to be used as a principled data-fitting tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.19563v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hejia Liu, Mochen Yang, Gediminas Adomavicius</dc:creator>
    </item>
    <item>
      <title>Filtering amplitude dependence of correlation dynamics in complex systems: application to the cryptocurrency market</title>
      <link>https://arxiv.org/abs/2509.18820</link>
      <description>arXiv:2509.18820v2 Announce Type: replace-cross 
Abstract: Based on the cryptocurrency market dynamics, this study presents a general methodology for analyzing evolving correlation structures in complex systems using the $q$-dependent detrended cross-correlation coefficient \rho(q,s). By extending traditional metrics, this approach captures correlations at varying fluctuation amplitudes and time scales. The method employs $q$-dependent minimum spanning trees ($q$MSTs) to visualize evolving network structures. Using minute-by-minute exchange rate data for 140 cryptocurrencies on Binance (Jan 2021-Oct 2024), a rolling window analysis reveals significant shifts in $q$MSTs, notably around April 2022 during the Terra/Luna crash. Initially centralized around Bitcoin (BTC), the network later decentralized, with Ethereum (ETH) and others gaining prominence. Spectral analysis confirms BTC's declining dominance and increased diversification among assets. A key finding is that medium-scale fluctuations exhibit stronger correlations than large-scale ones, with $q$MSTs based on the latter being more decentralized. Properly exploiting such facts may offer the possibility of a more flexible optimal portfolio construction. Distance metrics highlight that major disruptions amplify correlation differences, leading to fully decentralized structures during crashes. These results demonstrate $q$MSTs' effectiveness in uncovering fluctuation-dependent correlations, with potential applications beyond finance, including biology, social and other complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18820v2</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/v7cl-h7xr</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. E 112, 044309 (2025)</arxiv:journal_reference>
      <dc:creator>Marcin W\k{a}torek, Marija Bezbradica, Martin Crane, Jaros{\l}aw Kwapie\'n, Stanis{\l}aw Dro\.zd\.z</dc:creator>
    </item>
    <item>
      <title>Evaluating the Use of Large Language Models as Synthetic Social Agents in Social Science Research</title>
      <link>https://arxiv.org/abs/2509.26080</link>
      <description>arXiv:2509.26080v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) are being increasingly used as synthetic agents in social science, in applications ranging from augmenting survey responses to powering multi-agent simulations. This paper outlines cautions that should be taken when interpreting LLM outputs and proposes a pragmatic reframing for the social sciences in which LLMs are used as high-capacity pattern matchers for quasi-predictive interpolation under explicit scope conditions and not as substitutes for probabilistic inference. Practical guardrails such as independent draws, preregistered human baselines, reliability-aware validation, and subgroup calibration, are introduced so that researchers may engage in useful prototyping and forecasting while avoiding category errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26080v2</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Rose Madden</dc:creator>
    </item>
    <item>
      <title>Beyond PCA: Manifold Dimension Estimation via Local Graph Structure</title>
      <link>https://arxiv.org/abs/2510.15141</link>
      <description>arXiv:2510.15141v2 Announce Type: replace-cross 
Abstract: Local principal component analysis (Local PCA) has proven to be an effective tool for estimating the intrinsic dimension of a manifold. More recently, curvature-adjusted PCA (CA-PCA) has improved upon this approach by explicitly accounting for the curvature of the underlying manifold, rather than assuming local flatness. Building on these insights, we propose a general framework for manifold dimension estimation that captures the manifold's local graph structure by integrating PCA with regression-based techniques. Within this framework, we introduce two representative estimators: quadratic embedding (QE) and total least squares (TLS). Experiments on both synthetic and real-world datasets demonstrate that these methods perform competitively with, and often outperform, state-of-the-art alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15141v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 29 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zelong Bi, Pierre Lafaye de Micheaux</dc:creator>
    </item>
  </channel>
</rss>
