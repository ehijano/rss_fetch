<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 02:36:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Cortex AISQL: A Production SQL Engine for Unstructured Data</title>
      <link>https://arxiv.org/abs/2511.07663</link>
      <description>arXiv:2511.07663v1 Announce Type: new 
Abstract: Snowflake's Cortex AISQL is a production SQL engine that integrates native semantic operations directly into SQL. This integration allows users to write declarative queries that combine relational operations with semantic reasoning, enabling them to query both structured and unstructured data effortlessly. However, making semantic operations efficient at production scale poses fundamental challenges. Semantic operations are more expensive than traditional SQL operations, possess distinct latency and throughput characteristics, and their cost and selectivity are unknown during query compilation. Furthermore, existing query engines are not designed to optimize semantic operations. The AISQL query execution engine addresses these challenges through three novel techniques informed by production deployment data from Snowflake customers. First, AI-aware query optimization treats AI inference cost as a first-class optimization objective, reasoning about large language model (LLM) cost directly during query planning to achieve 2-8$\times$ speedups. Second, adaptive model cascades reduce inference costs by routing most rows through a fast proxy model while escalating uncertain cases to a powerful oracle model, achieving 2-6$\times$ speedups while maintaining 90-95% of oracle model quality. Third, semantic join query rewriting lowers the quadratic time complexity of join operations to linear through reformulation as multi-label classification tasks, achieving 15-70$\times$ speedups with often improved prediction quality. AISQL is deployed in production at Snowflake, where it powers diverse customer workloads across analytics, search, and content understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07663v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paritosh Aggarwal, Bowei Chen, Anupam Datta, Benjamin Han, Boxin Jiang, Nitish Jindal, Zihan Li, Aaron Lin, Pawel Liskowski, Jay Tayade, Dimitris Tsirogiannis, Nathan Wiegand, Weicheng Zhao</dc:creator>
    </item>
    <item>
      <title>ACGraph: An Efficient Asynchronous Out-of-Core Graph Processing Framework</title>
      <link>https://arxiv.org/abs/2511.07886</link>
      <description>arXiv:2511.07886v1 Announce Type: new 
Abstract: Graphs are a ubiquitous data structure in diverse domains such as machine learning, social networks, and data mining. As real-world graphs continue to grow beyond the memory capacity of single machines, out-of-core graph processing systems have emerged as a viable solution. Yet, existing systems that rely on strictly synchronous, iteration-by-iteration execution incur significant overheads. In particular, their scheduling mechanisms lead to I/O inefficiencies, stemming from read and work amplification, and induce costly synchronization stalls hindering sustained disk utilization. To overcome these limitations, we present {\em ACGraph}, a novel asynchronous graph processing system optimized for SSD-based environments with constrained memory resources. ACGraph employs a dynamic, block-centric priority scheduler that adjusts in real time based on workload, along with an online asynchronous worklist that minimizes redundant disk accesses by efficiently reusing active blocks in memory. Moreover, ACGraph unifies asynchronous I/O with computation in a pipelined execution model that maintains sustained I/O activation, and leverages a highly optimized hybrid storage format to expedite access to low-degree vertices. We implement popular graph algorithms, such as Breadth-First Search (BFS), Weakly Connected Components (WCC), personalized PageRank (PPR), PageRank (PR), and $k$-core on ACGraph and demonstrate that ACGraph substantially outperforms state-of-the-art out-of-core graph processing systems in both runtime and I/O efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07886v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dechuang Chen, Sibo Wang, Qintian Guo</dc:creator>
    </item>
    <item>
      <title>A Service Suite for Specifying Digital Twins for Industry 5.0</title>
      <link>https://arxiv.org/abs/2511.07506</link>
      <description>arXiv:2511.07506v1 Announce Type: cross 
Abstract: One of the challenges of predictive maintenance is making decisions based on data in an agile and assertive way. Connected sensors and operational data favor intelligent processing techniques to enrich information and enable decision-making. Digital Twins (DTs) can be used to process information and support decision-making. DTs are a real-time representation of physical machines and generate data that predictive maintenance can use to make assertive and quick decisions. The main contribution of this work is the specification of a suite of services for specifying DTs, called DT-Create, focused on decision support in predictive maintenance. DT-Create suite is based on intelligent techniques, semantic data processing, and self-adaptation. This suite was developed using the Design Science Research (DSR) methodology through two development cycles and evaluated through case studies. The results demonstrate the feasibility of using DT-Create in specifying DTs considering the following aspects: (i) collection, storage, and intelligent processing of data generated by sensors, (ii) enrichment of information through machine learning and ontologies, (iii) use of intelligent techniques to select predictive models that adhere to the available data set, and (iv) decision support and self-adaptation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07506v1</guid>
      <category>cs.SE</category>
      <category>cs.DB</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Izaque Esteves, Regina Braga, Jos\'e Maria David, Victor Stroele</dc:creator>
    </item>
    <item>
      <title>Towards a Multimodal Stream Processing System</title>
      <link>https://arxiv.org/abs/2510.14631</link>
      <description>arXiv:2510.14631v2 Announce Type: replace 
Abstract: In this paper, we present a vision for a new generation of multimodal streaming systems that embed MLLMs as first-class operators, enabling real-time query processing across multiple modalities. Achieving this is non-trivial: while recent work has integrated MLLMs into databases for multimodal queries, streaming systems require fundamentally different approaches due to their strict latency and throughput requirements. Our approach proposes novel optimizations at all levels, including logical, physical, and semantic query transformations that reduce model load to improve throughput while preserving accuracy. We demonstrate this with Samsara, a prototype leveraging such optimizations to improve performance by more than an order of magnitude. Moreover, we discuss a research roadmap that outlines open research challenges for building a scalable and efficient multimodal stream processing systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14631v2</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>U\'elison Jean Lopes dos Santos, Alessandro Ferri, Szilard Nistor, Riccardo Tommasini, Carsten Binnig, Manisha Luthra</dc:creator>
    </item>
    <item>
      <title>Compass: General Filtered Search across Vector and Structured Data</title>
      <link>https://arxiv.org/abs/2510.27141</link>
      <description>arXiv:2510.27141v3 Announce Type: replace 
Abstract: The increasing prevalence of hybrid vector and relational data necessitates efficient, general support for queries that combine high-dimensional vector search with complex relational filtering. However, existing filtered search solutions are fundamentally limited by specialized indices, which restrict arbitrary filtering and hinder integration with general-purpose DBMSs. This work introduces \textsc{Compass}, a unified framework that enables general filtered search across vector and structured data without relying on new index designs. Compass leverages established index structures -- such as HNSW and IVF for vector attributes, and B+-trees for relational attributes -- implementing a principled cooperative query execution strategy that coordinates candidate generation and predicate evaluation across modalities. Uniquely, Compass maintains generality by allowing arbitrary conjunctions, disjunctions, and range predicates, while ensuring robustness even with highly-selective or multi-attribute filters. Comprehensive empirical evaluations demonstrate that Compass consistently outperforms NaviX, the only existing performant general framework, across diverse hybrid query workloads. It also matches the query throughput of specialized single-attribute indices in their favorite settings with only a single attribute involved, all while maintaining full generality and DBMS compatibility. Overall, Compass offers a practical and robust solution for achieving truly general filtered search in vector database systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27141v3</guid>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunxiao Ye, Xiao Yan, Eric Lo</dc:creator>
    </item>
    <item>
      <title>A High-Throughput GPU Framework for Adaptive Lossless Compression of Floating-Point Data</title>
      <link>https://arxiv.org/abs/2511.04140</link>
      <description>arXiv:2511.04140v2 Announce Type: replace 
Abstract: The torrential influx of floating-point data from domains like IoT and HPC necessitates high-performance lossless compression to mitigate storage costs while preserving absolute data fidelity. Leveraging GPU parallelism for this task presents significant challenges, including bottlenecks in heterogeneous data movement, complexities in executing precision-preserving conversions, and performance degradation due to anomaly-induced sparsity. To address these challenges, this paper introduces a novel GPU-based framework for floating-point adaptive lossless compression. The proposed solution employs three key innovations: a lightweight asynchronous pipeline that effectively hides I/O latency during CPU-GPU data transfer; a fast and theoretically guaranteed float-to-integer conversion method that eliminates errors inherent in floating-point arithmetic; and an adaptive sparse bit-plane encoding strategy that mitigates the sparsity caused by outliers. Extensive experiments on 12 diverse datasets demonstrate that the proposed framework significantly outperforms state-of-the-art competitors, achieving an average compression ratio of 0.299 (a 9.1% relative improvement over the best competitor), an average compression throughput of 10.82 GB/s (2.4x higher), and an average decompression throughput of 12.32 GB/s (2.4x higher).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04140v2</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Li (Chongqing University), Weiyan Wang (Chongqing University), Ruiyuan Li (Chongqing University), Chao Chen (Chongqing University), Xianlei Long (Chongqing University), Linjiang Zheng (Chongqing University), Quanqing Xu (OceanBase, Ant Group), Chuanhui Yang (OceanBase, Ant Group)</dc:creator>
    </item>
    <item>
      <title>Towards Synthesizing High-Dimensional Tabular Data with Limited Samples</title>
      <link>https://arxiv.org/abs/2503.06444</link>
      <description>arXiv:2503.06444v2 Announce Type: replace-cross 
Abstract: Diffusion-based tabular data synthesis models have yielded promising results. However, when the data dimensionality increases, existing models tend to degenerate and may perform even worse than simpler, non-diffusion-based models. This is because limited training samples in high-dimensional space often hinder generative models from capturing the distribution accurately. To mitigate the insufficient learning signals and to stabilize training under such conditions, we propose CtrTab, a condition-controlled diffusion model that injects perturbed ground-truth samples as auxiliary inputs during training. This design introduces an implicit L2 regularization on the model's sensitivity to the control signal, improving robustness and stability in high-dimensional, low-data scenarios. Experimental results across multiple datasets show that CtrTab outperforms state-of-the-art models, with a performance gap in accuracy over 90% on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06444v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zuqing Li, Junhao Gan, Jianzhong Qi</dc:creator>
    </item>
    <item>
      <title>REIS: A High-Performance and Energy-Efficient Retrieval System with In-Storage Processing</title>
      <link>https://arxiv.org/abs/2506.16444</link>
      <description>arXiv:2506.16444v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) face an inherent challenge: their knowledge is confined to the data that they have been trained on. To overcome this issue, Retrieval-Augmented Generation (RAG) complements the static training-derived knowledge of LLMs with an external knowledge repository. RAG consists of three stages: indexing, retrieval, and generation. The retrieval stage of RAG becomes a significant bottleneck in inference pipelines. In this stage, a user query is mapped to an embedding vector and an Approximate Nearest Neighbor Search (ANNS) algorithm searches for similar vectors in the database to identify relevant items. Due to the large database sizes, ANNS incurs significant data movement overheads between the host and the storage system. To alleviate these overheads, prior works propose In-Storage Processing (ISP) techniques that accelerate ANNS by performing computations inside storage. However, existing works that leverage ISP for ANNS (i) employ algorithms that are not tailored to ISP systems, (ii) do not accelerate data retrieval operations for data selected by ANNS, and (iii) introduce significant hardware modifications, limiting performance and hindering their adoption. We propose REIS, the first ISP system tailored for RAG that addresses these limitations with three key mechanisms. First, REIS employs a database layout that links database embedding vectors to their associated documents, enabling efficient retrieval. Second, it enables efficient ANNS by introducing an ISP-tailored data placement technique that distributes embeddings across the planes of the storage system and employs a lightweight Flash Translation Layer. Third, REIS leverages an ANNS engine that uses the existing computational resources inside the storage system. Compared to a server-grade system, REIS improves the performance (energy efficiency) of retrieval by an average of 13x (55x).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16444v2</guid>
      <category>cs.CL</category>
      <category>cs.AR</category>
      <category>cs.DB</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kangqi Chen, Andreas Kosmas Kakolyris, Rakesh Nadig, Manos Frouzakis, Nika Mansouri Ghiasi, Yu Liang, Haiyu Mao, Jisung Park, Mohammad Sadrosadati, Onur Mutlu</dc:creator>
    </item>
  </channel>
</rss>
