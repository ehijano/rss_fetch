<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 10 Sep 2025 01:30:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predicting Brain Morphogenesis via Physics-Transfer Learning</title>
      <link>https://arxiv.org/abs/2509.05305</link>
      <description>arXiv:2509.05305v1 Announce Type: new 
Abstract: Brain morphology is shaped by genetic and mechanical factors and is linked to biological development and diseases. Its fractal-like features, regional anisotropy, and complex curvature distributions hinder quantitative insights in medical inspections. Recognizing that the underlying elastic instability and bifurcation share the same physics as simple geometries such as spheres and ellipses, we developed a physics-transfer learning framework to address the geometrical complexity. To overcome the challenge of data scarcity, we constructed a digital library of high-fidelity continuum mechanics modeling that both describes and predicts the developmental processes of brain growth and disease. The physics of nonlinear elasticity from simple geometries is embedded into a neural network and applied to brain models. This physics-transfer approach demonstrates remarkable performance in feature characterization and morphogenesis prediction, highlighting the pivotal role of localized deformation in dominating over the background geometry. The data-driven framework also provides a library of reduced-dimensional evolutionary representations that capture the essential physics of the highly folded cerebral cortex. Validation through medical images and domain expertise underscores the deployment of digital-twin technology in comprehending the morphological complexity of the brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05305v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>nlin.PS</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingjie Zhao, Yicheng Song, Fan Xu, Zhiping Xu</dc:creator>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>https://arxiv.org/abs/2509.06426</link>
      <description>arXiv:2509.06426v1 Announce Type: new 
Abstract: Computational models are critical to advance our understanding of how neural, biomechanical, and physical systems interact to orchestrate animal behaviors. Despite the availability of near-complete reconstructions of the Drosophila melanogaster central nervous system, musculature, and exoskeleton, anatomically and physically grounded models of fly leg muscles are still missing. These models provide an indispensable bridge between motor neuron activity and joint movements. Here, we introduce the first 3D, data-driven musculoskeletal model of Drosophila legs, implemented in both OpenSim and MuJoCo simulation environments. Our model incorporates a Hill-type muscle representation based on high-resolution X-ray scans from multiple fixed specimens. We present a pipeline for constructing muscle models using morphological imaging data and for optimizing unknown muscle parameters specific to the fly. We then combine our musculoskeletal models with detailed 3D pose estimation data from behaving flies to achieve muscle-actuated behavioral replay in OpenSim. Simulations of muscle activity across diverse walking and grooming behaviors predict coordinated muscle synergies that can be tested experimentally. Furthermore, by training imitation learning policies in MuJoCo, we test the effect of different passive joint properties on learning speed and find that damping and stiffness facilitate learning. Overall, our model enables the investigation of motor control in an experimentally tractable model organism, providing insights into how biomechanics contribute to generation of complex limb movements. Moreover, our model can be used to control embodied artificial agents to generate naturalistic and compliant locomotion in simulated environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06426v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pembe Gizem \"Ozdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya</dc:creator>
    </item>
    <item>
      <title>Reward function compression facilitates goal-dependent reinforcement learning</title>
      <link>https://arxiv.org/abs/2509.06810</link>
      <description>arXiv:2509.06810v1 Announce Type: new 
Abstract: Reinforcement learning agents learn from rewards, but humans can uniquely assign value to novel, abstract outcomes in a goal-dependent manner. However, this flexibility is cognitively costly, making learning less efficient. Here, we propose that goal-dependent learning is initially supported by a capacity-limited working memory system. With consistent experience, learners create a "compressed" reward function (a simplified rule defining the goal) which is then transferred to long-term memory and applied automatically upon receiving feedback. This process frees up working memory resources, boosting learning efficiency. We test this theory across six experiments. Consistent with our predictions, our findings demonstrate that learning is parametrically impaired by the size of the goal space, but improves when the goal space structure allows for compression. We also find faster reward processing to correlate with better learning performance, supporting the idea that as goal valuation becomes more automatic, more resources are available for learning. We leverage computational modeling to support this interpretation. Our work suggests that efficient goal-directed learning relies on compressing complex goal information into a stable reward function, shedding light on the cognitive mechanisms of human motivation. These findings generate new insights into the neuroscience of intrinsic motivation and could help improve behavioral techniques that support people in achieving their goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06810v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaia Molinaro, Anne G. E. Collins</dc:creator>
    </item>
    <item>
      <title>Modeling Visual Hallucination: A Generative Adversarial Network Framework</title>
      <link>https://arxiv.org/abs/2102.08209</link>
      <description>arXiv:2102.08209v2 Announce Type: replace 
Abstract: Visual hallucination refers to the perception of recognizable things that are not present. These phenomena are commonly linked to a range of neurological/psychiatric disorders. Despite ongoing research, the mechanisms through which the visual system generates hallucinations from real-world environments are still not well understood. Abnormal interactions between different regions of the brain responsible for perception are known to contribute to the occurrence of visual hallucinations. In this study, we propose and extend a generative neural network-based framework to address challenges within the visual system, aiming to create goal-driven models inspired by neurobiological mechanisms of visual hallucinations. We focus on the adversarial interactions between the visual system and the frontal lobe regions, proposing the Hallu-GAN model to suggest how these interactions can give rise to visual hallucinations. The architecture of the Hallu-GAN model is based on generative adversarial networks. Our simulation results indicate that disturbances in the ventral stream can lead to visual hallucinations. To further analyze the impact of other brain regions on the visual system, we extend the Hallu-GAN model by adding EEG data from individuals. This extended model, referred to as Hallu-GAN+, enables the examination of both hallucinating and non-hallucinating states. By training the Hallu-GAN+ model with EEG data from an individual with Charles Bonnet syndrome, we demonstrated its utility in analyzing the behavior of those experiencing hallucinations. Our simulation results confirmed the capability of the proposed model in resembling the visual system in both healthy and hallucinating states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.08209v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masoumeh Zareh, Mohammad Hossein Manshaei, Sayed Jalal Zahabi, Marwan Krunz</dc:creator>
    </item>
    <item>
      <title>A Framework for Standardizing Similarity Measures in a Rapidly Evolving Field</title>
      <link>https://arxiv.org/abs/2409.18333</link>
      <description>arXiv:2409.18333v2 Announce Type: replace 
Abstract: Similarity measures are fundamental tools for quantifying the alignment between artificial and biological systems. However, the diversity of similarity measures and their varied naming and implementation conventions makes it challenging to compare across studies. To facilitate comparisons and make explicit the implementation choices underlying a given code package, we have created and are continuing to develop a Python repository that benchmarks and standardizes similarity measures. The goal of creating a consistent naming convention that uniquely and efficiently specifies a similarity measure is not trivial as, for example, even commonly used methods like Centered Kernel Alignment (CKA) have at least 12 different variations, and this number will likely continue to grow as the field evolves. For this reason, we do not advocate for a fixed, definitive naming convention. The landscape of similarity measures and best practices will continue to change and so we see our current repository, which incorporates approximately 100 different similarity measures from 14 packages, as providing a useful tool at this snapshot in time. To accommodate the evolution of the field we present a framework for developing, validating, and refining naming conventions with the goal of uniquely and efficiently specifying similarity measures, ultimately making it easier for the community to make comparisons across studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18333v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Cloos, Guangyu Robert Yang, Christopher J. Cueva</dc:creator>
    </item>
    <item>
      <title>Theta and/or alpha? Neural oscillational substrates for dynamic inter-brain synchrony during mother-child cooperation</title>
      <link>https://arxiv.org/abs/2410.13669</link>
      <description>arXiv:2410.13669v3 Announce Type: replace 
Abstract: Mother-child interaction is a highly dynamic process neurally characterized by inter-brain synchrony (IBS) at {\theta} and/or {\alpha} rhythms. However, their establishment, dynamic changes, and roles in mother-child interactions remain unknown. Through dynamic analysis of dual-EEG from 40 mother-child dyads during turn-taking cooperation, we uncover that {\theta}-IBS and {\alpha}-IBS alternated with interactive behaviors, with EEG frequency-shift as a prerequisite for IBS transitions. When mothers attempt to track their children's attention and/or predict their intentions, they will adjust their EEG frequencies to align with their children's {\theta} oscillations, leading to a higher occurrence of the {\theta}-IBS state. Conversely, the {\alpha}-IBS state, accompanied by the EEG frequency-shift to the {\alpha} range, is more prominent during mother-led interactions. Further exploratory analysis reveals greater presence and stability of the {\theta}-IBS state during cooperative than non-cooperative conditions, particularly in dyads with stronger emotional attachments and more frequent interactions in their daily lives. Our findings shed light on the neural oscillational substrates underlying the IBS dynamics during mother-child interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13669v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/JBHI.2025.3603544</arxiv:DOI>
      <dc:creator>Jiayang Xu, Yamin Li, Ruxin Su, Saishuang Wu, Chengcheng Wu, Haiwa Wang, Qi Zhu, Yue Fang, Fan Jiang, Shanbao Tong, Yunting Zhang, Xiaoli Guo</dc:creator>
    </item>
    <item>
      <title>Diagrammatic expansion for the mutual-information rate in the realm of limited statistics</title>
      <link>https://arxiv.org/abs/2504.06255</link>
      <description>arXiv:2504.06255v3 Announce Type: replace 
Abstract: Neurons in sensory systems encode stimulus information into their stochastic spiking response. The mutual information has been extensively applied to these systems to quantify the neurons' capacity of transmitting such information. Yet, while for discrete stimuli, like flashed images or single tones, its computation is straightforward, for dynamical stimuli it is necessary to compute a (mutual) information rate (MIR), therefore integrating over the multiple temporal correlations which characterize sensory systems. Previous methods are based on extensive sampling of the neuronal response, require large amounts of data and are therefore prone to biases and inaccuracy. Here, we develop Moba-MIRA (moment-based mutual-information-rate approximation), a computational method to estimate the mutual information rate. To derive Moba-MIRA, we use Feynman diagrams to expand the mutual information to arbitrary order in the correlations around the corresponding value for the empirical spike count distributions of single bins. As a result, only the empirical estimation of the pairwise correlations between time bins and the single-bin entropies are required, without the need for the whole joint probability distributions. We tested Moba-MIRA on synthetic data generated with generalized linear models, and showed that it requires only a few tens of stimulus repetitions to provide an accurate estimate of the information rate. Finally, we applied it to ex-vivo electrophysiological recordings of rats retina, obtaining rates ranging between 5 to 20 bits per second, consistent with earlier estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06255v3</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tobias K\"uhn, Gabriel Mahuas, Ulisse Ferrari</dc:creator>
    </item>
    <item>
      <title>Patterns of imbalance states between sub-brain regimes during development in the resting state</title>
      <link>https://arxiv.org/abs/2508.01307</link>
      <description>arXiv:2508.01307v2 Announce Type: replace 
Abstract: The functional brain network emerges from the complex, coordinated activity of distinct yet connected regions, which underlie the diverse repertoire of human cognitive functions. Structural Balance Theory (SBT) has been successfully applied to model such nontrivial connections through the analysis of balance and unbalance triadic configurations. In this study, using SBT, we examine the network of imbalanced triads in the resting-state brain subnetworks, which undergo dynamic changes during development. We demonstrate that anticorrelation patterns evolve across the lifespan, reflecting a developmental trajectory from a locally modular organization in childhood to a flexible and reconfigurable architecture during adolescence and finally to a highly segregated and functionally specialized network system in adulthood. This developmental trajectory indicates that the spread of anticorrelations is not an inherent feature of brain organization. This mature organization facilitates a balance between self-referential, internally generated cognitive processes and externally oriented, goal-directed cognition, enabling efficient and adaptive cognitive control. This balance is underpinned by prominent anticorrelations between the Default Mode Network (DMN) and the Frontoparietal Network (FPN) in adulthood. This is while during adolescence these anticorrelations are substantially weaker, suggesting that the maturation of these network connections from adolescence to adulthood establishes a functional architecture that supports the segregation of internal and external cognitive processes. These findings elucidate how the dynamic evolution of anticorrelation patterns in brain networks supports cognitive development across the lifespan, offering new insights into the neural basis of adaptive cognitive control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01307v2</guid>
      <category>q-bio.NC</category>
      <category>physics.bio-ph</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fahimeh Ahmadi, Zahra Moradimanesh, Reza Khosrowabadi, G. Reza Jafari</dc:creator>
    </item>
    <item>
      <title>A Differentiable Model for Optimizing the Genetic Drivers of Synaptogenesis</title>
      <link>https://arxiv.org/abs/2402.07242</link>
      <description>arXiv:2402.07242v3 Announce Type: replace-cross 
Abstract: There is growing consensus among neuroscientists that neural circuits critical for survival are the result of genomic decompression processes. We introduce SynaptoGen, a novel computational framework--member of the Connectome Models family--bringing synthetic biological intelligence closer, facilitating neural biological agent development through precise genetic control of synaptogenesis. SynaptoGen is the first model of its kind offering mechanistic explanation of synaptic multiplicity based on genetic expression and protein interaction probabilities. The framework connects genetic factors through a differentiable function, working as a neural network where synaptic weights equal average numbers of synapses between neurons, multiplied by conductance, derived from genetic profiles. Differentiability enables gradient-based optimization, allowing generation of genetic expression patterns producing pre-wired biological agents for specific tasks. Validation in simulated synaptogenesis scenarios shows agents successfully solving four reinforcement learning benchmarks, consistently surpassing control baselines. Despite gaps in biological realism requiring mitigation, this framework has potential to accelerate synthetic biological intelligence research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07242v3</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tommaso Boccato, Matteo Ferrante, Nicola Toschi</dc:creator>
    </item>
    <item>
      <title>Chaotic Dynamics and Fractal Geometry in Ring Lattice Systems of Nonchaotic Rulkov Neurons</title>
      <link>https://arxiv.org/abs/2412.12134</link>
      <description>arXiv:2412.12134v3 Announce Type: replace-cross 
Abstract: This paper investigates the complex dynamics and fractal attractors that arise in a 60-dimensional ring lattice system of electrically coupled nonchaotic Rulkov neurons. While networks of chaotic Rulkov neurons have been widely studied, systems of nonchaotic Rulkov neurons have not been extensively explored due to the piecewise complexity of the nonchaotic Rulkov map. Here, we find that rich dynamics emerge from the electrical coupling of regular-spiking Rulkov neurons, including chaotic spiking, synchronized chaotic bursting, and synchronized hyperchaos. By systematically varying the electrical coupling strength between neurons, we also uncover general trends in the maximal Lyapunov exponent across the system's dynamical regimes. By means of the Kaplan-Yorke conjecture, we examine the fractal geometry of the ring system's high-dimensional chaotic attractors and find that these attractors can occupy as many as 45 of the 60 dimensions of state space. We further explore how variations in chaotic behavior - quantified by the full Lyapunov spectra - correspond to changes in the attractors' fractal dimensions. This analysis advances our understanding of how complex collective behavior can emerge from the interaction of multiple simple neuron models and highlights the deep interplay between dynamics and geometry in high-dimensional systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12134v3</guid>
      <category>nlin.CD</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/fractalfract9090584</arxiv:DOI>
      <arxiv:journal_reference>Fractal Fract. 2025, 9(9), 584</arxiv:journal_reference>
      <dc:creator>Brandon B. Le</dc:creator>
    </item>
    <item>
      <title>Emergence of the Primacy Effect in Structured State-Space Models</title>
      <link>https://arxiv.org/abs/2502.13729</link>
      <description>arXiv:2502.13729v5 Announce Type: replace-cross 
Abstract: Structured state-space models (SSMs) have been developed to offer more persistent memory retention than traditional recurrent neural networks, while maintaining real-time inference capabilities and addressing the time-complexity limitations of Transformers. Despite this intended persistence, the memory mechanism of canonical SSMs is theoretically designed to decay monotonically over time, meaning that more recent inputs are expected to be retained more accurately than earlier ones. Contrary to this theoretical expectation, however, the present study reveals a counterintuitive finding: when trained and evaluated on a synthetic, statistically balanced memorization task, SSMs predominantly preserve the *initially* presented data in memory. This pattern of memory bias, known as the *primacy effect* in psychology, presents a non-trivial challenge to the current theoretical understanding of SSMs and opens new avenues for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13729v5</guid>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 09 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takashi Morita</dc:creator>
    </item>
  </channel>
</rss>
