<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Sep 2024 02:49:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Representation Learning of Complex Assemblies, An Effort to Improve Corporate Scope 3 Emissions Calculation</title>
      <link>https://arxiv.org/abs/2409.03769</link>
      <description>arXiv:2409.03769v1 Announce Type: new 
Abstract: Climate change is a pressing global concern for governments, corporations, and citizens alike. This concern underscores the necessity for these entities to accurately assess the climate impact of manufacturing goods and providing services. Tools like process life cycle analysis (pLCA) are used to evaluate the climate impact of production, use, and disposal, from raw material mining through end-of-life. pLCA further enables practitioners to look deeply into material choices or manufacturing processes for individual parts, sub-assemblies, assemblies, and the final product. Reliable and detailed data on the life cycle stages and processes of the product or service under study are not always available or accessible, resulting in inaccurate assessment of climate impact. To overcome the data limitation and enhance the effectiveness of pLCA to generate an improved environmental impact profile, we are adopting an innovative strategy to identify alternative parts, products, and components that share similarities in terms of their form, function, and performance to serve as qualified substitutes. Focusing on enterprise electronics hardware, we propose a semi-supervised learning-based framework to identify substitute parts that leverages product bill of material (BOM) data and a small amount of component-level qualified substitute data (positive samples) to generate machine knowledge graph (MKG) and learn effective embeddings of the components that constitute electronic hardware. Our methodology is grounded in attributed graph embeddings and introduces a strategy to generate biased negative samples to significantly enhance the training process. We demonstrate improved performance and generalization over existing published models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03769v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ajay Chatterjee, Srikanth Ranganathan</dc:creator>
    </item>
    <item>
      <title>Federated Learning Approach to Mitigate Water Wastage</title>
      <link>https://arxiv.org/abs/2409.03776</link>
      <description>arXiv:2409.03776v1 Announce Type: new 
Abstract: Residential outdoor water use in North America accounts for nearly 9 billion gallons daily, with approximately 50\% of this water wasted due to over-watering, particularly in lawns and gardens. This inefficiency highlights the need for smart, data-driven irrigation systems. Traditional approaches to reducing water wastage have focused on centralized data collection and processing, but such methods can raise privacy concerns and may not account for the diverse environmental conditions across different regions. In this paper, we propose a federated learning-based approach to optimize water usage in residential and agricultural settings. By integrating moisture sensors and actuators with a distributed network of edge devices, our system allows each user to locally train a model on their specific environmental data while sharing only model updates with a central server. This preserves user privacy and enables the creation of a global model that can adapt to varying conditions. Our implementation leverages low-cost hardware, including an Arduino Uno microcontroller and soil moisture sensors, to demonstrate how federated learning can be applied to reduce water wastage while maintaining efficient crop production. The proposed system not only addresses the need for water conservation but also provides a scalable, privacy-preserving solution adaptable to diverse environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03776v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sina Hajer Ahmadi, Amruta Pranadika Mahashabde</dc:creator>
    </item>
    <item>
      <title>ChatGPT and Its Educational Impact: Insights from a Software Development Competition</title>
      <link>https://arxiv.org/abs/2409.03779</link>
      <description>arXiv:2409.03779v1 Announce Type: new 
Abstract: This study explores the integration and impact of ChatGPT, a generative AI that utilizes natural language processing, in an educational environment. The main goal is to evaluate how ChatGPT affects project performance. To this end, we organize a software development competition utilizing ChatGPT, lasting for four weeks and involving 36 students. The competition is structured in two rounds: in the first round, all 36 students participate and are evaluated based on specific performance metrics such as code quality, innovation, and adherence to project requirements. The top 15 performers from the first round are then selected to advance to the second round, where they compete for the final rankings and the overall winner is determined. The competition shows that students who use ChatGPT extensively in various stages of development, including ideation, documentation, software development, and quality assurance, have higher project completion rates and better scores. A detailed comparative analysis between first-round and second-round winners reveals significant differences in their experience with generative AI for software development, experience learning large-scale language models, and interest in their respective fields of study. These findings suggest that ChatGPT enhances individual learning and project performance. A post-survey of participants also reveals high levels of satisfaction, further emphasizing the benefits of integrating generative AI like ChatGPT in academic settings. This study highlights the transformative potential of ChatGPT in project-based learning environments and supports further research into its long-term impact and broader application in a variety of educational contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03779v1</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sunhee Hwang, Yudoo Kim, Heejin Lee</dc:creator>
    </item>
    <item>
      <title>Flat-earth communities on Brazilian Telegram: when faith is used to question the existence of gravity as a physics phenomenon</title>
      <link>https://arxiv.org/abs/2409.03800</link>
      <description>arXiv:2409.03800v1 Announce Type: new 
Abstract: Conspiracy theories related to flat-earthism have gained traction on Brazilian Telegram, especially in times of global crisis, such as the COVID-19 pandemic, when distrust in scientific and governmental institutions has intensified. Therefore, this study aims to address the research question: how are Brazilian conspiracy theory communities on flat earth topics characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies whose main objective is to understand and characterize Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on arXiv at Cornell University, applying a mirrored method across the seven studies, changing only the thematic object of analysis and providing investigation replicability, including with proprietary and authored codes, adding to the culture of free and open-source software. Regarding the main findings of this study, the following were observed: During the Pandemic, flat-earthist discussions increased by 400%, driven by distrust in scientific institutions; Flat-Earther communities act as portals for other conspiracy theories, such as the New World Order; Although smaller, the flat-Earther network has influential groups that disseminate content and perpetuate narratives; Religious themes such as God and the Bible are central, combining religious elements with distrust in science; Flat-Earther communities use themes such as gravity to challenge established scientific concepts, reinforcing an alternative view of the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03800v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>Understanding Online Discussion Across Difference: Insights from Gun Discourse on Reddit</title>
      <link>https://arxiv.org/abs/2409.03989</link>
      <description>arXiv:2409.03989v1 Announce Type: cross 
Abstract: When discussing difficult topics online, is it common to meaningfully engage with people from diverse perspectives? Why or why not? Could features of the online environment be redesigned to encourage civil conversation across difference? In this paper, we study discussions of gun policy on Reddit, with the overarching goal of developing insights into the potential of the internet to support understanding across difference. We use two methods: a clustering analysis of Reddit posts to contribute insights about what people discuss, and an interview study of twenty Reddit users to help us understand why certain kinds of conversation take place and others don't. We find that the discussion of gun politics falls into three groups: conservative pro-gun, liberal pro-gun, and liberal anti-gun. Each type of group has its own characteristic topics. While our subjects state that they would be willing to engage with others across the ideological divide, in practice they rarely do. Subjects are siloed into like-minded subreddits through a two-pronged effect, where they are simultaneously pushed away from opposing-view communities while actively seeking belonging in like-minded ones. Another contributing factor is Reddit's "karma" mechanism: fear of being downvoted and losing karma points and social approval of peers causes our subjects to hesitate to say anything in conflict with group norms. The pseudonymous nature of discussion on Reddit plays a complex role, with some subjects finding it freeing and others fearing reprisal from others not bound by face-to-face norms of politeness. Our subjects believe that content moderation can help ameliorate these issues; however, our findings suggest that moderators need different tools to do so effectively. We conclude by suggesting platform design changes that might increase discussion across difference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03989v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rijul Magu, Nivedhitha Mathan Kumar, Yihe Liu, Xander Koo, Diyi Yang, Amy Bruckman</dc:creator>
    </item>
    <item>
      <title>Structure and dynamics of growing networks of Reddit threads</title>
      <link>https://arxiv.org/abs/2409.04085</link>
      <description>arXiv:2409.04085v1 Announce Type: cross 
Abstract: Millions of people use online social networks to reinforce their sense of belonging, for example by giving and asking for feedback as a form of social validation and self-recognition. It is common to observe disagreement among people beliefs and points of view when expressing this feedback. Modeling and analyzing such interactions is crucial to understand social phenomena that happen when people face different opinions while expressing and discussing their values. In this work, we study a Reddit community in which people participate to judge or be judged with respect to some behavior, as it represents a valuable source to study how users express judgments online. We model threads of this community as complex networks of user interactions growing in time, and we analyze the evolution of their structural properties. We show that the evolution of Reddit networks differ from other real social networks, despite falling in the same category. This happens because their global clustering coefficient is extremely small and the average shortest path length increases over time. Such properties reveal how users discuss in threads, i.e. with mostly one other user and often by a single message. We strengthen such result by analyzing the role that disagreement and reciprocity play in such conversations. We also show that Reddit thread's evolution over time is governed by two subgraphs growing at different speeds. We discover that, in the studied community, the difference of such speed is higher than in other communities because of the user guidelines enforcing specific user interactions. Finally, we interpret the obtained results on user behavior drawing back to Social Judgment Theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04085v1</guid>
      <category>cs.SI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s41109-024-00654-y</arxiv:DOI>
      <arxiv:journal_reference>Applied Network Science volume 9, Article number: 48 (2024)</arxiv:journal_reference>
      <dc:creator>Diletta Goglia, Davide Vega</dc:creator>
    </item>
    <item>
      <title>Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers</title>
      <link>https://arxiv.org/abs/2409.04109</link>
      <description>arXiv:2409.04109v1 Announce Type: cross 
Abstract: Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p &lt; 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04109v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenglei Si, Diyi Yang, Tatsunori Hashimoto</dc:creator>
    </item>
    <item>
      <title>Secure Traffic Sign Recognition: An Attention-Enabled Universal Image Inpainting Mechanism against Light Patch Attacks</title>
      <link>https://arxiv.org/abs/2409.04133</link>
      <description>arXiv:2409.04133v1 Announce Type: cross 
Abstract: Traffic sign recognition systems play a crucial role in assisting drivers to make informed decisions while driving. However, due to the heavy reliance on deep learning technologies, particularly for future connected and autonomous driving, these systems are susceptible to adversarial attacks that pose significant safety risks to both personal and public transportation. Notably, researchers recently identified a new attack vector to deceive sign recognition systems: projecting well-designed adversarial light patches onto traffic signs. In comparison with traditional adversarial stickers or graffiti, these emerging light patches exhibit heightened aggression due to their ease of implementation and outstanding stealthiness. To effectively counter this security threat, we propose a universal image inpainting mechanism, namely, SafeSign. It relies on attention-enabled multi-view image fusion to repair traffic signs contaminated by adversarial light patches, thereby ensuring the accurate sign recognition. Here, we initially explore the fundamental impact of malicious light patches on the local and global feature spaces of authentic traffic signs. Then, we design a binary mask-based U-Net image generation pipeline outputting diverse contaminated sign patterns, to provide our image inpainting model with needed training data. Following this, we develop an attention mechanism-enabled neural network to jointly utilize the complementary information from multi-view images to repair contaminated signs. Finally, extensive experiments are conducted to evaluate SafeSign's effectiveness in resisting potential light patch-based attacks, bringing an average accuracy improvement of 54.8% in three widely-used sign recognition models</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04133v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hangcheng Cao, Longzhi Yuan, Guowen Xu, Ziyang He, Zhengru Fang, Yuguang Fang</dc:creator>
    </item>
    <item>
      <title>Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects</title>
      <link>https://arxiv.org/abs/2304.08275</link>
      <description>arXiv:2304.08275v4 Announce Type: replace 
Abstract: Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and developers of AI/ML systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08275v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/IJCNN54540.2023.10191274</arxiv:DOI>
      <arxiv:journal_reference>International Joint Conference on Neural Networks (IJCNN), 2023</arxiv:journal_reference>
      <dc:creator>Conrad Sanderson, David Douglas, Qinghua Lu</dc:creator>
    </item>
    <item>
      <title>Between Copyright and Computer Science: The Law and Ethics of Generative AI</title>
      <link>https://arxiv.org/abs/2403.14653</link>
      <description>arXiv:2403.14653v2 Announce Type: replace 
Abstract: Copyright and computer science continue to intersect and clash, but they can coexist. The advent of new technologies such as digitization of visual and aural creations, sharing technologies, search engines, social media offerings, and more challenge copyright-based industries and reopen questions about the reach of copyright law. Breakthroughs in artificial intelligence research, especially Large Language Models that leverage copyrighted material as part of training models, are the latest examples of the ongoing tension between copyright and computer science. The exuberance, rush-to-market, and edge problem cases created by a few misguided companies now raises challenges to core legal doctrines and may shift Open Internet practices for the worse. That result does not have to be, and should not be, the outcome.
  This Article shows that, contrary to some scholars' views, fair use law does not bless all ways that someone can gain access to copyrighted material even when the purpose is fair use. Nonetheless, the scientific need for more data to advance AI research means access to large book corpora and the Open Internet is vital for the future of that research. The copyright industry claims, however, that almost all uses of copyrighted material must be compensated, even for non-expressive uses. The Article's solution accepts that both sides need to change. It is one that forces the computer science world to discipline its behaviors and, in some cases, pay for copyrighted material. It also requires the copyright industry to abandon its belief that all uses must be compensated or restricted to uses sanctioned by the copyright industry. As part of this re-balancing, the Article addresses a problem that has grown out of this clash and under theorized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14653v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Deven R. Desai, Mark Riedl</dc:creator>
    </item>
    <item>
      <title>Using GPT-4 to Augment Unbalanced Data for Automatic Scoring</title>
      <link>https://arxiv.org/abs/2310.18365</link>
      <description>arXiv:2310.18365v3 Announce Type: replace-cross 
Abstract: Machine learning-based automatic scoring faces challenges with unbalanced student responses across scoring categories. To address this, we introduce a novel text data augmentation framework leveraging GPT-4, a generative large language model, specifically tailored for unbalanced datasets in automatic scoring. Our experimental dataset comprised student written responses to four science items. We crafted prompts for GPT-4 to generate responses, especially for minority scoring classes, enhancing the data set. We then finetuned DistillBERT for automatic scoring based on the augmented and original datasets. Model performance was assessed using accuracy, precision, recall, and F1 metrics. Our findings revealed that incorporating GPT-4-augmented data remarkedly improved model performance, particularly for precision and F1 scores. Interestingly, the extent of improvement varied depending on the specific dataset and the proportion of augmented data used. Notably, we found that a varying amount of augmented data (20%-40%) was needed to obtain stable improvement for automatic scoring. Comparisons with models trained on additional student-written responses suggest that GPT-4 augmented models match those trained with student data. This research underscores the potential and effectiveness of data augmentation techniques utilizing generative large language models like GPT-4 in addressing unbalanced datasets within automated assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18365v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luyang Fang, Gyeong-Geon Lee, Xiaoming Zhai</dc:creator>
    </item>
    <item>
      <title>Foundational Challenges in Assuring Alignment and Safety of Large Language Models</title>
      <link>https://arxiv.org/abs/2404.09932</link>
      <description>arXiv:2404.09932v2 Announce Type: replace-cross 
Abstract: This work identifies 18 foundational challenges in assuring the alignment and safety of large language models (LLMs). These challenges are organized into three different categories: scientific understanding of LLMs, development and deployment methods, and sociotechnical challenges. Based on the identified challenges, we pose $200+$ concrete research questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09932v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Usman Anwar, Abulhair Saparov, Javier Rando, Daniel Paleka, Miles Turpin, Peter Hase, Ekdeep Singh Lubana, Erik Jenner, Stephen Casper, Oliver Sourbut, Benjamin L. Edelman, Zhaowei Zhang, Mario G\"unther, Anton Korinek, Jose Hernandez-Orallo, Lewis Hammond, Eric Bigelow, Alexander Pan, Lauro Langosco, Tomasz Korbak, Heidi Zhang, Ruiqi Zhong, Se\'an \'O h\'Eigeartaigh, Gabriel Recchia, Giulio Corsi, Alan Chan, Markus Anderljung, Lilian Edwards, Aleksandar Petrov, Christian Schroeder de Witt, Sumeet Ramesh Motwan, Yoshua Bengio, Danqi Chen, Philip H. S. Torr, Samuel Albanie, Tegan Maharaj, Jakob Foerster, Florian Tramer, He He, Atoosa Kasirzadeh, Yejin Choi, David Krueger</dc:creator>
    </item>
    <item>
      <title>Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing</title>
      <link>https://arxiv.org/abs/2408.02558</link>
      <description>arXiv:2408.02558v4 Announce Type: replace-cross 
Abstract: With the European Union's Artificial Intelligence Act taking effect on 1 August 2024, high-risk AI applications must adhere to stringent transparency and fairness standards. This paper addresses a crucial question: how can we scientifically audit algorithmic fairness? Current methods typically remain at the basic detection stage of auditing, without accounting for more complex scenarios. We propose a novel framework, ``peer-induced fairness'', which combines the strengths of counterfactual fairness and peer comparison strategy, creating a reliable and robust tool for auditing algorithmic fairness. Our framework is universal, adaptable to various domains, and capable of handling different levels of data quality, including skewed distributions. Moreover, it can distinguish whether adverse decisions result from algorithmic discrimination or inherent limitations of the subjects, thereby enhancing transparency. This framework can serve as both a self-assessment tool for AI developers and an external assessment tool for auditors to ensure compliance with the EU AI Act. We demonstrate its utility in small and medium-sized enterprises access to finance, uncovering significant unfairness-41.51% of micro-firms face discrimination compared to non-micro firms. These findings highlight the framework's potential for broader applications in ensuring equitable AI-driven decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02558v4</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>q-fin.CP</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiqi Fang, Zexun Chen, Jake Ansell</dc:creator>
    </item>
    <item>
      <title>Large-scale Urban Facility Location Selection with Knowledge-informed Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2409.01588</link>
      <description>arXiv:2409.01588v2 Announce Type: replace-cross 
Abstract: The facility location problem (FLP) is a classical combinatorial optimization challenge aimed at strategically laying out facilities to maximize their accessibility. In this paper, we propose a reinforcement learning method tailored to solve large-scale urban FLP, capable of producing near-optimal solutions at superfast inference speed. We distill the essential swap operation from local search, and simulate it by intelligently selecting edges on a graph of urban regions, guided by a knowledge-informed graph neural network, thus sidestepping the need for heavy computation of local search. Extensive experiments on four US cities with different geospatial conditions demonstrate that our approach can achieve comparable performance to commercial solvers with less than 5\% accessibility loss, while displaying up to 1000 times speedup. We deploy our model as an online geospatial application at https://huggingface.co/spaces/randommmm/MFLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01588v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678717.3691254</arxiv:DOI>
      <dc:creator>Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li</dc:creator>
    </item>
  </channel>
</rss>
