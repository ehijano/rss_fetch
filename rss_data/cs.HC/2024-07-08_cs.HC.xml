<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Jul 2024 04:00:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Diversity, Representation, and Accessibility Concerns in Game Development</title>
      <link>https://arxiv.org/abs/2407.04892</link>
      <description>arXiv:2407.04892v1 Announce Type: new 
Abstract: This study delves into the key issues of representation and accessibility in game development. Despite their societal significance, video games face ongoing criticism for lacking diversity in both the workforce and content, excluding marginalized gamers. This study explores game-based learning (GBL) while emphasizing the importance of accurate representation, particularly in educational settings to enhance engagement and learning outcomes. Our research findings revolve around the perspectives of a professional in the gaming industry and the challenges associated with creating accessible games. By providing actionable insights, it aims to influence regulatory reforms, industry practices, and game creation itself, to foster diversity, representation, and accessibility in the video game industry. In doing so, we seek to promote a more inclusive and equitable future in the educational gaming world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04892v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nooshin Darvishinia (Kansas State University, USA), Todd Goodson (Kansas State University, USA)</dc:creator>
    </item>
    <item>
      <title>Safe Generative Chats in a WhatsApp Intelligent Tutoring System</title>
      <link>https://arxiv.org/abs/2407.04915</link>
      <description>arXiv:2407.04915v1 Announce Type: new 
Abstract: Large language models (LLMs) are flexible, personalizable, and available, which makes their use within Intelligent Tutoring Systems (ITSs) appealing. However, that flexibility creates risks: inaccuracies, harmful content, and non-curricular material. Ethically deploying LLM-backed ITS systems requires designing safeguards that ensure positive experiences for students. We describe the design of a conversational system integrated into an ITS, and our experience evaluating its safety with red-teaming, an in-classroom usability test, and field deployment. We present empirical data from more than 8,000 student conversations with this system, finding that GPT-3.5 rarely generates inappropriate messages. Comparatively more common is inappropriate messages from students, which prompts us to reason about safeguarding as a content moderation and classroom management problem. The student interaction behaviors we observe provide implications for designers - to focus on student inputs as a content moderation problem - and implications for researchers - to focus on subtle forms of bad content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04915v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zachary Levonian, Owen Henkel</dc:creator>
    </item>
    <item>
      <title>DCitizens Roles Unveiled: SIG Navigating Identities in Digital Civics and the Spectrum of Societal Impact</title>
      <link>https://arxiv.org/abs/2407.05003</link>
      <description>arXiv:2407.05003v1 Announce Type: new 
Abstract: The DCitizens SIG aims to navigate ethical dimensions in forthcoming Digital Civics projects, ensuring enduring benefits and community resilience. Additionally, it seeks to shape the future landscape of digital civics for ethical and sustainable interventions. As we dive into these interactive processes, a challenge arises of discerning authentic intentions and validating perspectives. This exploration extends to evaluating the sustainability of future interactions and scrutinising biases impacting engaged communities. The commitment is to ensure future outcomes align with genuine community needs and address the ethical imperative of a considerate departure strategy. This dialogue encourages future researchers and practitioners to integrate ethical considerations and community-centric principles, fostering a more sustainable and responsible approach to technology-driven interventions in future urban regeneration and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05003v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna R. L. Carter, Kyle Montague, Reem Talhouk, Shaun Lawson, Hugo Nicolau, Ana Cristina Pires, Markus Rohde, Alessio Del Bue, Tiffany Knearem</dc:creator>
    </item>
    <item>
      <title>Envisioning Collaborative Futures: Advancing the Frontiers of Embedded Research</title>
      <link>https://arxiv.org/abs/2407.05016</link>
      <description>arXiv:2407.05016v1 Announce Type: new 
Abstract: Participatory design initiatives, especially within the realm of digital civics, are often integrated and co-developed with the very citizens and communities they intend to assist. Digital civics research aims to create positive social change using a variety of digital technologies. These research projects commonly adopt various embedded processes, such as commissioning models \cite{dcitizensproj22}. Despite the adoption of this process within a range of domains, there isn't currently a framework for best practices and accountability procedures to ensure we engage with citizens ethically and ensure the sustainability of our projects. This workshop aims to provide a space to start collaboratively constructing a dynamic framework of best practices, laying the groundwork for the future of sustainable embedded research processes. The overarching goal is to foster discussions and share insights that contribute to developing effective practices, ensuring the longevity and impact of participatory digital civics projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05016v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna R. L. Carter, Kyle Montague, Reem Talhouk, Ana O. Henriques, Hugo Nicolau, Tiffany Knearem, Ceylan Besevli, Firaz Peer, Clara Crivellaro, Sarah R\"uller</dc:creator>
    </item>
    <item>
      <title>And this is where we fu***d up! Lessons learned from Participatory Design in Digital Civic Initiatives</title>
      <link>https://arxiv.org/abs/2407.05032</link>
      <description>arXiv:2407.05032v1 Announce Type: new 
Abstract: Participatory design in digital civics aims to foster mutual learning and co-creation between public services and citizens. However, rarely do we collectively explore the challenges and failures we experience within PD and digital civics, to enable us to grow as a community. This workshop will explore real-world experiences that had to adapt to unforeseen circumstances. Through case presentations and thematic group discussions, participants will reflect on the challenges faced, the causes that led to these challenges, and collaboratively problem-solve effective solutions. Furthermore, we aim to discuss well-being impact on researchers and communities when faced with these obstacles, the strategies participants use to overcome them and how this can be fed back into the digital civics community. By that, the workshop seeks to foster dialogue, reflection, and collective learning, empowering participants with insights to navigate complexities effectively and promote resilient design practices in digital civics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05032v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Clara Rosa Cardoso, Sarah R\"uller, Ana O Henriques, Anna R L Carter, Markus Rohde</dc:creator>
    </item>
    <item>
      <title>Form Forge: Latent Space Exploration of Architectural Forms via Explicit Latent Variable Manipulation</title>
      <link>https://arxiv.org/abs/2407.05079</link>
      <description>arXiv:2407.05079v1 Announce Type: new 
Abstract: This paper presents 'Form Forge,' a prototype of a creative system for interactively exploring the latent space of architectural forms, inspired by Franois Blanciak's SITELESS: 1001 Building Forms via direct manipulation of latent variables. Utilizing a fine-tuned StyleGAN2-ADA model, the system allows users to navigate an array of possible building forms derived from Blanciak's sketches. Distinct from common latent space exploration tools that often rely on projected navigation landmarks, Form Forge provides direct access to manipulate each latent variable, aiming to offer a more granular exploration of the model's capabilities. Form Forge's design is intended to simplify the interaction with a complex, high-dimensional space and to serve as a preliminary investigation into how such tools might support creative processes in architectural design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05079v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kevin Dunnell, Andy Lippman</dc:creator>
    </item>
    <item>
      <title>Toward a Unified Metadata Schema for Ecological Momentary Assessment with Voice-First Virtual Assistants</title>
      <link>https://arxiv.org/abs/2407.05203</link>
      <description>arXiv:2407.05203v1 Announce Type: new 
Abstract: Ecological momentary assessment (EMA) is used to evaluate subjects' behaviors and moods in their natural environments, yet collecting real-time and self-report data with EMA is challenging due to user burden. Integrating voice into EMA data collection platforms through today's intelligent virtual assistants (IVAs) is promising due to hands-free and eye-free nature. However, efficiently managing conversations and EMAs is non-trivial and time consuming due to the ambiguity of the voice input. We approach this problem by rethinking the data modeling of EMA questions and what is needed to deploy them on voice-first user interfaces. We propose a unified metadata schema that models EMA questions and the necessary attributes to effectively and efficiently integrate voice as a new EMA modality. Our schema allows user experience researchers to write simple rules that can be rendered at run-time, instead of having to edit the source code. We showcase an example EMA survey implemented with our schema, which can run on multiple voice-only and voice-first devices. We believe that our work will accelerate the iterative prototyping and design process of real-world voice-based EMA data collection platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05203v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3469595.3469626</arxiv:DOI>
      <dc:creator>Chen Chen, Khalil Mrini, Kemeberly Charles, Ella T. Lifset, Michael Hogarth, Alison A. Moore, Nadir Weibel, Emilia Farcas</dc:creator>
    </item>
    <item>
      <title>MelodyVis: Visual Analytics for Melodic Patterns in Sheet Music</title>
      <link>https://arxiv.org/abs/2407.05427</link>
      <description>arXiv:2407.05427v1 Announce Type: new 
Abstract: Manual melody detection is a tedious task requiring high expertise level, while automatic detection is often not expressive or powerful enough. Thus, we present MelodyVis, a visual application designed in collaboration with musicology experts to explore melodic patterns in digital sheet music. MelodyVis features five connected views, including a Melody Operator Graph and a Voicing Timeline. The system utilizes eight atomic operators, such as transposition and mirroring, to capture melody repetitions and variations. Users can start their analysis by manually selecting patterns in the sheet view, and then identifying other patterns based on the selected samples through an interactive exploration process. We conducted a user study to investigate the effectiveness and usefulness of our approach and its integrated melodic operators, including usability and mental load questions. We compared the analysis executed by 25 participants with and without the operators. The study results indicate that the participants could identify at least twice as many patterns with activated operators. MelodyVis allows analysts to steer the analysis process and interpret results. Our study also confirms the usefulness of MelodyVis in supporting common analytical tasks in melodic analysis, with participants reporting improved pattern identification and interpretation. Thus, MelodyVis addresses the limitations of fully-automated approaches, enabling music analysts to step into the analysis process and uncover and understand intricate melodic patterns and transformations in sheet music.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05427v1</guid>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Miller, Daniel F\"urst, Maximilian T. Fischer, Hanna Hauptmann, Daniel Keim, Mennatallah El-Assady</dc:creator>
    </item>
    <item>
      <title>Towards Perceived Security, Perceived Privacy, and the Universal Design of E-Payment Applications</title>
      <link>https://arxiv.org/abs/2407.05446</link>
      <description>arXiv:2407.05446v1 Announce Type: new 
Abstract: With the growth of digital monetary transactions and cashless payments, encouraged by the COVID-19 pandemic, use of e-payment applications is on the rise. It is thus imperative to understand and evaluate the current posture of e-payment applications from three major user-facing angles: security, privacy, and usability. To this, we created a high-fidelity prototype of an e-payment application that encompassed features that we wanted to test with users. We then conducted a pilot study where we recruited 12 participants who tested our prototype. We find that both security and privacy are important for users of e-payment applications. Additionally, some participants perceive the strength of security and privacy based on the usability of the application. We provide recommendations such as universal design of e-payment applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05446v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Urvashi Kishnani, Isabella Cardenas, Jailene Castillo, Rosalyn Conry, Lukas Rodwin, Rika Ruiz, Matthew Walther, Sanchari Das</dc:creator>
    </item>
    <item>
      <title>Understanding Professional Needs to Create Privacy-Preserving and Secure Emergent Digital Artworks</title>
      <link>https://arxiv.org/abs/2407.05450</link>
      <description>arXiv:2407.05450v1 Announce Type: new 
Abstract: In recent years, immersive art installations featuring interactive artworks have been on the rise. These installations are an integral part of museums and art centers like selfie museums, teamLab Borderless, ARTECHOUSE, and Meow Wolf. Moreover, immersive art have also been increasingly incorporated into traditional museums as well. However, immersive art requires active user participation and often captures information from viewers and participants through cameras, sensors, microphones, embodied interaction devices, surveillance, and kinetic mirrors. Therefore, we propose a new line of research to examine the security and privacy postures of immersive artworks. In our pilot study, we conducted a semi-structured interview with five experienced practitioners from either the art (2) or cybersecurity (3) fields. Our aim was to understand their current security and privacy practices, along with their needs when it comes to immersive art. From their responses, we created a list of security and privacy parameters, such as, providing opt-in mechanics for data collection, knowledge of data collection tools such as proximity sensors, and creating security awareness amongst participants by communicating security protocols and threat models. These parameters allow us to build privacy-preserving, secure, and accessible software for individuals working in media arts, who often have no background on security and privacy. In the future, we plan to utilize these parameters to develop software in response to those needs and then host an art exhibition of immersive artworks utilizing the platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05450v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathryn Lichlyter, Urvashi Kishnani, Kate Hollenbach, Sanchari Das</dc:creator>
    </item>
    <item>
      <title>Multimedia and Immersive Training Materials Influence Impressions of Learning But Not Learning Outcomes</title>
      <link>https://arxiv.org/abs/2407.05504</link>
      <description>arXiv:2407.05504v1 Announce Type: new 
Abstract: Although the use of technologies like multimedia and virtual reality (VR) in training offer the promise of improved learning, these richer and potentially more engaging materials do not consistently produce superior learning outcomes. Default approaches to such training may inadvertently mimic concepts like naive realism in display design, and desirable difficulties in the science of learning - fostering an impression of greater learning dissociated from actual gains in memory. This research examined the influence of format of instructions in learning to assemble items from components. Participants in two experiments were trained on the steps to assemble a series of bars, that resembled Meccano pieces, into eight different shapes. After training on pairs of shapes, participants rated the likelihood they would remember the shapes and then were administered a recognition test. Relative to viewing a static diagram, viewing videos of shapes being constructed in a VR environment (Experiment 1) or viewing within an immersive VR system (Experiment 2) elevated participants' assessments of their learning but without enhancing learning outcomes. Overall, these findings illustrate how future workers might mistakenly come to believe that technologically advanced support improves learning and prefer instructional designs that integrate similarly complex cues into training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05504v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benjamin A. Clegg, Alex Karduna, Ethan Holen, Jason Garcia, Matthew G. Rhodes, Francisco R. Ortega</dc:creator>
    </item>
    <item>
      <title>MEEG and AT-DGNN: Advancing EEG Emotion Recognition with Music and Graph Learning</title>
      <link>https://arxiv.org/abs/2407.05550</link>
      <description>arXiv:2407.05550v1 Announce Type: new 
Abstract: Recent advances in neuroscience have elucidated the crucial role of coordinated brain region activities during cognitive tasks. To explore the complexity, we introduce the MEEG dataset, a comprehensive multi-modal music-induced electroencephalogram (EEG) dataset and the Attention-based Temporal Learner with Dynamic Graph Neural Network (AT-DGNN), a novel framework for EEG-based emotion recognition. The MEEG dataset captures a wide range of emotional responses to music, enabling an in-depth analysis of brainwave patterns in musical contexts. The AT-DGNN combines an attention-based temporal learner with a dynamic graph neural network (DGNN) to accurately model the local and global graph dynamics of EEG data across varying brain network topology. Our evaluations show that AT-DGNN achieves superior performance, with an accuracy (ACC) of 83.06\% in arousal and 85.31\% in valence, outperforming state-of-the-art (SOTA) methods on the MEEG dataset. Comparative analyses with traditional datasets like DEAP highlight the effectiveness of our approach and underscore the potential of music as a powerful medium for emotion induction. This study not only advances our understanding of the brain emotional processing, but also enhances the accuracy of emotion recognition technologies in brain-computer interfaces (BCI), leveraging both graph-based learning and the emotional impact of music. The source code and dataset are available at \textit{https://github.com/xmh1011/AT-DGNN}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05550v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minghao Xiao, Zhengxi Zhu, Wenyu Wang, Meixia Qu</dc:creator>
    </item>
    <item>
      <title>Exploring Real-Time Music-to-Image Systems for Creative Inspiration in Music Creation</title>
      <link>https://arxiv.org/abs/2407.05584</link>
      <description>arXiv:2407.05584v1 Announce Type: new 
Abstract: This paper presents a study on the use of a real-time music-to-image system as a mechanism to support and inspire musicians during their creative process. The system takes MIDI messages from a keyboard as input which are then interpreted and analysed using state-of-the-art generative AI models. Based on the perceived emotion and music structure, the system's interpretation is converted into visual imagery that is presented in real-time to musicians. We conducted a user study in which musicians improvised and composed using the system. Our findings show that most musicians found the generated images were a novel mechanism when playing, evidencing the potential of music-to-image systems to inspire and enhance their creative process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05584v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Yang, Maria Teresa Llano, Jon McCormack</dc:creator>
    </item>
    <item>
      <title>Constrained Online Recursive Source Separation Framework for Real-time Electrophysiological Signal Processing</title>
      <link>https://arxiv.org/abs/2407.05655</link>
      <description>arXiv:2407.05655v1 Announce Type: new 
Abstract: Electrophysiological signal processing often requires blind source separation (BSS) techniques due to the nature of mixing source signals. However, its complex computational demands make real-time applicability challenging. In this study, we propose a Constrained Online Recursive Source Separation (CORSS) framework for real time electrophysiological signals processing. With a stepwise recursive unmixing matrix learning rule, the algorithm achieves real-time updates with minimal computational overhead. By incorporating prior information of target signals to optimize the cost function, the algorithm converges more readily to ideal sources, yielding more accurate results. Two downstream tasks, real-time surface electromyogram (sEMG) decomposition and real-time respiratory intent monitoring based on diaphragmatic electromyogram (sEMGdi) extraction, were employed to assess the efficacy of our method. The results demonstrated superior performance compared to alternative methods, achieving a matching rate of 96.00 for the sEMG decomposition task and 98.12 for the sEMGdi extraction task. Moreover, compared to Online PFP, our method exhibits minimal time delay during computation, reflecting its streamlined processing and excellent real-time capabilities. Our approach demonstrates strong performance across various real-time processing tasks of electrophysiological signals, highlighting its significance for applications in real-time human-computer interaction and clinical monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05655v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Yao, Zhao Haowen, Liu Yunfei, Zhang Xu</dc:creator>
    </item>
    <item>
      <title>Multimodal Machine Learning for Automated Assessment of Attention-Related Processes during Learning</title>
      <link>https://arxiv.org/abs/2407.05803</link>
      <description>arXiv:2407.05803v1 Announce Type: new 
Abstract: Attention is a key factor for successful learning, with research indicating strong associations between (in)attention and learning outcomes. This dissertation advanced the field by focusing on the automated detection of attention-related processes using eye tracking, computer vision, and machine learning, offering a more objective, continuous, and scalable assessment than traditional methods such as self-reports or observations. It introduced novel computational approaches for assessing various dimensions of (in)attention in online and classroom learning settings and addressing the challenges of precise fine-granular assessment, generalizability, and in-the-wild data quality. First, this dissertation explored the automated detection of mind-wandering, a shift in attention away from the learning task. Aware and unaware mind wandering were distinguished employing a novel multimodal approach that integrated eye tracking, video, and physiological data. Further, the generalizability of scalable webcam-based detection across diverse tasks, settings, and target groups was examined. Second, this thesis investigated attention indicators during online learning. Eye-tracking analyses revealed significantly greater gaze synchronization among attentive learners. Third, it addressed attention-related processes in classroom learning by detecting hand-raising as an indicator of behavioral engagement using a novel view-invariant and occlusion-robust skeleton-based approach. This thesis advanced the automated assessment of attention-related processes within educational settings by developing and refining methods for detecting mind wandering, on-task behavior, and behavioral engagement. It bridges educational theory with advanced methods from computer science, enhancing our understanding of attention-related processes that significantly impact learning outcomes and educational practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05803v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Babette B\"uhler</dc:creator>
    </item>
    <item>
      <title>See-Through Face Display: Mutual Gaze Communication in Remote Conversations with Transparent Display</title>
      <link>https://arxiv.org/abs/2407.05833</link>
      <description>arXiv:2407.05833v1 Announce Type: new 
Abstract: We propose See-Through Face Display, a display that facilitates mutual gaze communication with remote users through the display. This display comprises a transparent display that repeatedly turns on and off and a camera installed behind the display so that the user's gaze and the optical path of the camera are closely aligned. This display is designed to show the user's face, and eye contact occurs when the user looks at the face on the display. Therefore, by using this display for all participants in a conversation, users can communicate with each other via the face on the display via facial and gaze cues. This interaction provides an experience as if the remote user's face were present in the local environment. See-Through Face Display proposes the importance of mutual gaze communication in telepresence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05833v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kazuya Izumi, Ryosuke Hyakuta, Ippei Suzuki, Yoichi Ochiai</dc:creator>
    </item>
    <item>
      <title>Investigating Trading Mechanisms as a Driver for User Experience in Racing Games</title>
      <link>https://arxiv.org/abs/2407.05874</link>
      <description>arXiv:2407.05874v1 Announce Type: new 
Abstract: The exchange of digital goods has become a significant aspect of the global economy, with digital products offering inexpensive reproduction and distribution. In-game objects, a type of digital currency, have emerged as tradable commodities within gaming ecosystems. Despite extensive research on various aspects of digital goods, little attention has been given to the impact of in-game trading mechanisms on user experience. This paper presents a study aimed at evaluating the influence of trading systems on user experience in a racing game context. We developed a simple racing game featuring an in-game market for buying and selling car variants and conducted an A/B study comparing user experiences between groups utilizing the trading system and those unlocking cars through race completion. Our findings suggest that while the trading system did not significantly alter the overall user experience, further exploration of diverse trading approaches may offer insights into their impact on user engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05874v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georg Arbesser-Rastburg, Thomas Olip, Johanna Pirker</dc:creator>
    </item>
    <item>
      <title>Exploring Human-LLM Conversations: Mental Models and the Originator of Toxicity</title>
      <link>https://arxiv.org/abs/2407.05977</link>
      <description>arXiv:2407.05977v1 Announce Type: new 
Abstract: This study explores real-world human interactions with large language models (LLMs) in diverse, unconstrained settings in contrast to most prior research focusing on ethically trimmed models like ChatGPT for specific tasks. We aim to understand the originator of toxicity. Our findings show that although LLMs are rightfully accused of providing toxic content, it is mostly demanded or at least provoked by humans who actively seek such content. Our manual analysis of hundreds of conversations judged as toxic by APIs commercial vendors, also raises questions with respect to current practices of what user requests are refused to answer. Furthermore, we conjecture based on multiple empirical indicators that humans exhibit a change of their mental model, switching from the mindset of interacting with a machine more towards interacting with a human.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05977v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Johannes Schneider, Arianna Casanova Flores, Anne-Catherine Kranz</dc:creator>
    </item>
    <item>
      <title>Investigating User Perceptions of Collaborative Agenda Setting in Virtual Health Counseling Session</title>
      <link>https://arxiv.org/abs/2407.06123</link>
      <description>arXiv:2407.06123v1 Announce Type: new 
Abstract: Virtual health counselors offer the potential to provide users with information and counseling in complex areas such as disease management and health education. However, ensuring user engagement is challenging, particularly when the volume of information and length of counseling sessions increase. Agenda setting a clinical counseling technique where a patient and clinician collaboratively decide on session topics is an effective approach to tailoring discussions for individual patient needs and sustaining engagement. We explore the effectiveness of agenda setting in a virtual counselor system designed to counsel women for breast cancer genetic testing. In a between subjects study, we assessed three versions of the system with varying levels of user control in the system's agenda setting approach. We found that participants' knowledge improved across all conditions. Although our results showed that any type of agenda setting was perceived as useful, regardless of user control, interviews revealed a preference for more collaboration and user involvement in the agenda setting process. Our study highlights the importance of using patient-centered approaches, such as tailored discussions, when using virtual counselors in healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06123v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mina Fallah, Farnaz Nouraei, Hye Sun Yun, Timothy Bickmore</dc:creator>
    </item>
    <item>
      <title>Depression Detection and Analysis using Large Language Models on Textual and Audio-Visual Modalities</title>
      <link>https://arxiv.org/abs/2407.06125</link>
      <description>arXiv:2407.06125v1 Announce Type: new 
Abstract: Depression has proven to be a significant public health issue, profoundly affecting the psychological well-being of individuals. If it remains undiagnosed, depression can lead to severe health issues, which can manifest physically and even lead to suicide. Generally, Diagnosing depression or any other mental disorder involves conducting semi-structured interviews alongside supplementary questionnaires, including variants of the Patient Health Questionnaire (PHQ) by Clinicians and mental health professionals. This approach places significant reliance on the experience and judgment of trained physicians, making the diagnosis susceptible to personal biases. Given that the underlying mechanisms causing depression are still being actively researched, physicians often face challenges in diagnosing and treating the condition, particularly in its early stages of clinical presentation. Recently, significant strides have been made in Artificial neural computing to solve problems involving text, image, and speech in various domains. Our analysis has aimed to leverage these state-of-the-art (SOTA) models in our experiments to achieve optimal outcomes leveraging multiple modalities. The experiments were performed on the Extended Distress Analysis Interview Corpus Wizard of Oz dataset (E-DAIC) corpus presented in the Audio/Visual Emotion Challenge (AVEC) 2019 Challenge. The proposed solutions demonstrate better results achieved by Proprietary and Open-source Large Language Models (LLMs), which achieved a Root Mean Square Error (RMSE) score of 3.98 on Textual Modality, beating the AVEC 2019 challenge baseline results and current SOTA regression analysis architectures. Additionally, the proposed solution achieved an accuracy of 71.43% in the classification task. The paper also includes a novel audio-visual multi-modal network that predicts PHQ-8 scores with an RMSE of 6.51.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06125v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avinash Anand, Chayan Tank, Sarthak Pol, Vinayak Katoch, Shaina Mehta, Rajiv Ratn Shah</dc:creator>
    </item>
    <item>
      <title>Visual Evaluative AI: A Hypothesis-Driven Tool with Concept-Based Explanations and Weight of Evidence</title>
      <link>https://arxiv.org/abs/2407.04710</link>
      <description>arXiv:2407.04710v1 Announce Type: cross 
Abstract: This paper presents Visual Evaluative AI, a decision aid that provides positive and negative evidence from image data for a given hypothesis. This tool finds high-level human concepts in an image and generates the Weight of Evidence (WoE) for each hypothesis in the decision-making process. We apply and evaluate this tool in the skin cancer domain by building a web-based application that allows users to upload a dermatoscopic image, select a hypothesis and analyse their decisions by evaluating the provided evidence. Further, we demonstrate the effectiveness of Visual Evaluative AI on different concept-based explanation approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04710v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thao Le, Tim Miller, Ruihan Zhang, Liz Sonenberg, Ronal Singh</dc:creator>
    </item>
    <item>
      <title>RAMO: Retrieval-Augmented Generation for Enhancing MOOCs Recommendations</title>
      <link>https://arxiv.org/abs/2407.04925</link>
      <description>arXiv:2407.04925v1 Announce Type: cross 
Abstract: Massive Open Online Courses (MOOCs) have significantly enhanced educational accessibility by offering a wide variety of courses and breaking down traditional barriers related to geography, finance, and time. However, students often face difficulties navigating the vast selection of courses, especially when exploring new fields of study. Driven by this challenge, researchers have been exploring course recommender systems to offer tailored guidance that aligns with individual learning preferences and career aspirations. These systems face particular challenges in effectively addressing the ``cold start'' problem for new users. Recent advancements in recommender systems suggest integrating large language models (LLMs) into the recommendation process to enhance personalized recommendations and address the ``cold start'' problem. Motivated by these advancements, our study introduces RAMO (Retrieval-Augmented Generation for MOOCs), a system specifically designed to overcome the ``cold start'' challenges of traditional course recommender systems. The RAMO system leverages the capabilities of LLMs, along with Retrieval-Augmented Generation (RAG)-facilitated contextual understanding, to provide course recommendations through a conversational interface, aiming to enhance the e-learning experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04925v1</guid>
      <category>cs.IR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jiarui Rao, Jionghao Lin</dc:creator>
    </item>
    <item>
      <title>Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning</title>
      <link>https://arxiv.org/abs/2407.04997</link>
      <description>arXiv:2407.04997v1 Announce Type: cross 
Abstract: Currently, the vast majority of locally deployed open-source large language models (LLMs) and some commercial model interfaces do not support stable tool calling functionality. The existing solution involves fine-tuning LLMs, which results in significant time and computational resource consumption. This paper proposes a method that enables LLMs to achieve stable tool calling capabilities using only prompt engineering and some ingenious code design. We conducted experiments on multiple LLMs that lack tool calling capabilities across various tool calling tasks, achieving a success rate of 100%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04997v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengtao He</dc:creator>
    </item>
    <item>
      <title>ProACT: An Augmented Reality Testbed for Intelligent Prosthetic Arms</title>
      <link>https://arxiv.org/abs/2407.05025</link>
      <description>arXiv:2407.05025v1 Announce Type: cross 
Abstract: Upper-limb amputees face tremendous difficulty in operating dexterous powered prostheses. Previous work has shown that aspects of prosthetic hand, wrist, or elbow control can be improved through "intelligent" control, by combining movement-based or gaze-based intent estimation with low-level robotic autonomy. However, no such solutions exist for whole-arm control. Moreover, hardware platforms for advanced prosthetic control are expensive, and existing simulation platforms are not well-designed for integration with robotics software frameworks. We present the Prosthetic Arm Control Testbed (ProACT), a platform for evaluating intelligent control methods for prosthetic arms in an immersive (Augmented Reality) simulation setting. Using ProACT with non-amputee participants, we compare performance in a Box-and-Blocks Task using a virtual myoelectric prosthetic arm, with and without intent estimation. Our results show that methods using intent estimation improve both user satisfaction and the degree of success in the task. To the best of our knowledge, this constitutes the first study of semi-autonomous control for complex whole-arm prostheses, the first study including sequential task modeling in the context of wearable prosthetic arms, and the first testbed of its kind. Towards the goal of supporting future research in intelligent prosthetics, the system is built upon on existing open-source frameworks for robotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05025v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shivani Guptasarma, Monroe D. Kennedy III</dc:creator>
    </item>
    <item>
      <title>Helios: An extremely low power event-based gesture recognition for always-on smart eyewear</title>
      <link>https://arxiv.org/abs/2407.05206</link>
      <description>arXiv:2407.05206v1 Announce Type: cross 
Abstract: This paper introduces Helios, the first extremely low-power, real-time, event-based hand gesture recognition system designed for all-day on smart eyewear. As augmented reality (AR) evolves, current smart glasses like the Meta Ray-Bans prioritize visual and wearable comfort at the expense of functionality. Existing human-machine interfaces (HMIs) in these devices, such as capacitive touch and voice controls, present limitations in ergonomics, privacy and power consumption. Helios addresses these challenges by leveraging natural hand interactions for a more intuitive and comfortable user experience. Our system utilizes a extremely low-power and compact 3mmx4mm/20mW event camera to perform natural hand-based gesture recognition for always-on smart eyewear. The camera's output is processed by a convolutional neural network (CNN) running on a NXP Nano UltraLite compute platform, consuming less than 350mW. Helios can recognize seven classes of gestures, including subtle microgestures like swipes and pinches, with 91% accuracy. We also demonstrate real-time performance across 20 users at a remarkably low latency of 60ms. Our user testing results align with the positive feedback we received during our recent successful demo at AWE-USA-2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05206v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Ben Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Dave Trickett, Chris Mair, Taru Muhonen, Rory Clark, Louis Berridge, Richard Vigars, Iain Wallace</dc:creator>
    </item>
    <item>
      <title>LDGCN: An Edge-End Lightweight Dual GCN Based on Single-Channel EEG for Driver Drowsiness Monitoring</title>
      <link>https://arxiv.org/abs/2407.05749</link>
      <description>arXiv:2407.05749v1 Announce Type: cross 
Abstract: Driver drowsiness electroencephalography (EEG) signal monitoring can timely alert drivers of their drowsiness status, thereby reducing the probability of traffic accidents. Graph convolutional networks (GCNs) have shown significant advancements in processing the non-stationary, time-varying, and non-Euclidean nature of EEG signals. However, the existing single-channel EEG adjacency graph construction process lacks interpretability, which hinders the ability of GCNs to effectively extract adjacency graph features, thus affecting the performance of drowsiness monitoring. To address this issue, we propose an edge-end lightweight dual graph convolutional network (LDGCN). Specifically, we are the first to incorporate neurophysiological knowledge to design a Baseline Drowsiness Status Adjacency Graph (BDSAG), which characterizes driver drowsiness status. Additionally, to express more features within limited EEG data, we introduce the Augmented Graph-level Module (AGM). This module captures global and local information at the graph level, ensuring that BDSAG features remain intact while enhancing effective feature expression capability. Furthermore, to deploy our method on the fourth-generation Raspberry Pi, we utilize Adaptive Pruning Optimization (APO) on both channels and neurons, reducing inference latency by almost half. Experiments on benchmark datasets demonstrate that LDGCN offers the best trade-off between monitoring performance and hardware resource utilization compared to existing state-of-the-art algorithms. All our source code can be found at https://github.com/BryantDom/Driver-Drowsiness-Monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05749v1</guid>
      <category>eess.SP</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingwei Huang, Chuansheng Wang, Jiayan Huang, Haoyi Fan, Antoni Grau, Fuquan Zhang</dc:creator>
    </item>
    <item>
      <title>Integrating AI in College Education: Positive yet Mixed Experiences with ChatGPT</title>
      <link>https://arxiv.org/abs/2407.05810</link>
      <description>arXiv:2407.05810v1 Announce Type: cross 
Abstract: The integration of artificial intelligence (AI) chatbots into higher education marks a shift towards a new generation of pedagogical tools, mirroring the arrival of milestones like the internet. With the launch of ChatGPT-4 Turbo in November 2023, we developed a ChatGPT-based teaching application (https://chat.openai.com/g/g-1imx1py4K-chatge-medical-imaging) and integrated it into our undergraduate medical imaging course in the Spring 2024 semester. This study investigates the use of ChatGPT throughout a semester-long trial, providing insights into students' engagement, perception, and the overall educational effectiveness of the technology. We systematically collected and analyzed data concerning students' interaction with ChatGPT, focusing on their attitudes, concerns, and usage patterns. The findings indicate that ChatGPT offers significant advantages such as improved information access and increased interactivity, but its adoption is accompanied by concerns about the accuracy of the information provided and the necessity for well-defined guidelines to optimize its use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05810v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinrui Song, Jiajin Zhang, Pingkun Yan, Juergen Hahn, Uwe Kruger, Hisham Mohamed, Ge Wang</dc:creator>
    </item>
    <item>
      <title>Cervical Auscultation Machine Learning for Dysphagia Assessment</title>
      <link>https://arxiv.org/abs/2407.05870</link>
      <description>arXiv:2407.05870v1 Announce Type: cross 
Abstract: This study evaluates the use of machine learning, specifically the Random Forest Classifier, to differentiate normal and pathological swallowing sounds. Employing a commercially available wearable stethoscope, we recorded swallows from both healthy adults and patients with dysphagia. The analysis revealed statistically significant differences in acoustic features, such as spectral crest, and zero-crossing rate between normal and pathological swallows, while no discriminating differences were demonstrated between different fluidand diet consistencies. The system demonstrated fair sensitivity (mean plus or minus SD: 74% plus or minus 8%) and specificity (89% plus or minus 6%) for dysphagic swallows. The model attained an overall accuracy of 83% plus or minus 3%, and F1 score of 78% plus or minus 5%. These results demonstrate that machine learning can be a valuable tool in non-invasive dysphagia assessment, although challenges such as sampling rate limitations and variability in sensitivity and specificity in discriminating between normal and pathological sounds are noted. The study underscores the need for further research to optimize these techniques for clinical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05870v1</guid>
      <category>cs.SD</category>
      <category>cs.HC</category>
      <category>eess.AS</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>An An Chia, Stacy Lum, Michelle Boo, Rex Tan, Balamurali B T, Jer-Ming Chen</dc:creator>
    </item>
    <item>
      <title>Evaluating the Semantic Profiling Abilities of LLMs for Natural Language Utterances in Data Visualization</title>
      <link>https://arxiv.org/abs/2407.06129</link>
      <description>arXiv:2407.06129v1 Announce Type: cross 
Abstract: Automatically generating data visualizations in response to human utterances on datasets necessitates a deep semantic understanding of the data utterance, including implicit and explicit references to data attributes, visualization tasks, and necessary data preparation steps. Natural Language Interfaces (NLIs) for data visualization have explored ways to infer such information, yet challenges persist due to inherent uncertainty in human speech. Recent advances in Large Language Models (LLMs) provide an avenue to address these challenges, but their ability to extract the relevant semantic information remains unexplored. In this study, we evaluate four publicly available LLMs (GPT-4, Gemini-Pro, Llama3, and Mixtral), investigating their ability to comprehend utterances even in the presence of uncertainty and identify the relevant data context and visual tasks. Our findings reveal that LLMs are sensitive to uncertainties in utterances. Despite this sensitivity, they are able to extract the relevant data context. However, LLMs struggle with inferring visualization tasks. Based on these results, we highlight future research directions on using LLMs for visualization generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06129v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hannah K. Bako, Arshnoor Buthani, Xinyi Liu, Kwesi A. Cobbina, Zhicheng Liu</dc:creator>
    </item>
    <item>
      <title>Community College Articulation Agreement Websites: Students' Suggestions for New Academic Advising Software Features</title>
      <link>https://arxiv.org/abs/2308.14411</link>
      <description>arXiv:2308.14411v4 Announce Type: replace 
Abstract: Articulation agreements provide more transparency about how community college courses will transfer and fulfill university requirements. However, the literature displays conflicting results on whether articulation agreements improve transfer-related outcomes; perhaps one contributor to these conflicting research results is the subpar user experience of articulation agreement reports and the websites that host them. Accordingly, we surveyed and interviewed California community college transfer students to gather their suggestions for new academic-advising-related software features for the ASSIST website. ASSIST is California's official centralized repository of articulation agreement reports between public California community colleges and universities. We analyzed the open-ended survey and interview data using structural coding and thematic analysis. We identified four themes around students' software feature suggestions for ASSIST: (a) features that automate laborious academic advising tasks, (b) features to reduce ambiguity with articulation agreements, (c) features to mitigate mistakes in term-by-term course planning, and (d) features to facilitate online advising from advisors and student peers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14411v4</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10668926.2024.2356330</arxiv:DOI>
      <arxiv:journal_reference>Community College Journal of Research and Practice (2024)</arxiv:journal_reference>
      <dc:creator>David V. Nguyen, Shayan Doroudi, Daniel A. Epstein</dc:creator>
    </item>
    <item>
      <title>Evaluating a VR System for Collecting Safety-Critical Vehicle-Pedestrian Interactions</title>
      <link>https://arxiv.org/abs/2310.05882</link>
      <description>arXiv:2310.05882v2 Announce Type: replace 
Abstract: Autonomous vehicles (AVs) require comprehensive and reliable pedestrian trajectory data to ensure safe operation. However, obtaining data of safety-critical scenarios such as jaywalking and near-collisions, or uncommon agents such as children, disabled pedestrians, and vulnerable road users poses logistical and ethical challenges. This paper evaluates a Virtual Reality (VR) system designed to collect pedestrian trajectory and body pose data in a controlled, low-risk environment. We substantiate the usefulness of such a system through semi-structured interviews with professionals in the AV field, and validate the effectiveness of the system through two empirical studies: a first-person user evaluation involving 62 participants, and a third-person evaluative survey involving 290 respondents. Our findings demonstrate that the VR-based data collection system elicits realistic responses for capturing pedestrian data in safety-critical or uncommon vehicle-pedestrian interaction scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05882v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erica Weng, Kenta Mukoya, Deva Ramanan, Kris Kitani</dc:creator>
    </item>
    <item>
      <title>Keeping Users Engaged During Repeated Administration of the Same Questionnaire: Using Large Language Models to Reliably Diversify Questions</title>
      <link>https://arxiv.org/abs/2311.12707</link>
      <description>arXiv:2311.12707v2 Announce Type: replace 
Abstract: Standardized, validated questionnaires are vital tools in research and healthcare, offering dependable self-report data. Prior work has revealed that virtual agent-administered questionnaires are almost equivalent to self-administered ones in an electronic form. Despite being an engaging method, repeated use of virtual agent-administered questionnaires in longitudinal or pre-post studies can induce respondent fatigue, impacting data quality via response biases and decreased response rates. We propose using large language models (LLMs) to generate diverse questionnaire versions while retaining good psychometric properties. In a longitudinal study, participants interacted with our agent system and responded daily for two weeks to one of the following questionnaires: a standardized depression questionnaire, question variants generated by LLMs, or question variants accompanied by LLM-generated small talk. The responses were compared to a validated depression questionnaire. Psychometric testing revealed consistent covariation between the external criterion and focal measure administered across the three conditions, demonstrating the reliability and validity of the LLM-generated variants. Participants found that the variants were significantly less repetitive than repeated administrations of the same standardized questionnaire. Our findings highlight the potential of LLM-generated variants to invigorate agent-administered questionnaires and foster engagement and interest, without compromising their validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12707v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3652988.3673929</arxiv:DOI>
      <dc:creator>Hye Sun Yun, Mehdi Arjmand, Phillip Sherlock, Michael K. Paasche-Orlow, James W. Griffith, Timothy Bickmore</dc:creator>
    </item>
    <item>
      <title>Exploring Gender and Racial/Ethnic Bias Against Video Game Streamers: Comparing Perceived Gameplay Skill and Viewer Engagement</title>
      <link>https://arxiv.org/abs/2312.00610</link>
      <description>arXiv:2312.00610v3 Announce Type: replace 
Abstract: Research suggests there is a perception that females and underrepresented racial/ethnic minorities have worse gameplay skills and produce less engaging video game streaming content. This bias might impact streamers' audience size, viewers' financial patronage of a streamer, streamers' sponsorship offers, etc. However, few studies on this topic use experimental methods. To fill this gap, we conducted a between-subjects survey experiment to examine if viewers are biased against video game streamers based on the streamer's gender or race/ethnicity. 200 survey participants rated the gameplay skill and viewer engagement of an identical gameplay recording. The only change between experimental conditions was the streamer's name who purportedly created the recording. The Dunnett's test found no statistically significant differences in viewer engagement ratings when comparing White male streamers to either White female (p = 0.37), Latino male (p = 0.66), or Asian male (p = 0.09) streamers. Similarly, there were no statistically significant differences in gameplay skill ratings when comparing White male streamers to either White female (p = 0.10), Latino male (p = 1.00), or Asian male (p = 0.59) streamers. Potential contributors to statistically non-significant results and counter-intuitive results (i.e., White females received non-significantly higher ratings than White males) are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.00610v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3649921.3650009</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the Foundations of Digital Games Conference (2024)</arxiv:journal_reference>
      <dc:creator>David V. Nguyen, Edward F. Melcer, Deanne Adams</dc:creator>
    </item>
    <item>
      <title>How Beginning Programmers and Code LLMs (Mis)read Each Other</title>
      <link>https://arxiv.org/abs/2401.15232</link>
      <description>arXiv:2401.15232v2 Announce Type: replace 
Abstract: Generative AI models, specifically large language models (LLMs), have made strides towards the long-standing goal of text-to-code generation. This progress has invited numerous studies of user interaction. However, less is known about the struggles and strategies of non-experts, for whom each step of the text-to-code problem presents challenges: describing their intent in natural language, evaluating the correctness of generated code, and editing prompts when the generated code is incorrect. This paper presents a large-scale controlled study of how 120 beginning coders across three academic institutions approach writing and editing prompts. A novel experimental design allows us to target specific steps in the text-to-code process and reveals that beginners struggle with writing and editing prompts, even for problems at their skill level and when correctness is automatically determined. Our mixed-methods evaluation provides insight into student processes and perceptions with key implications for non-expert Code LLM use within and outside of education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15232v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3613904.3642706</arxiv:DOI>
      <dc:creator>Sydney Nguyen, Hannah McLean Babe, Yangtian Zi, Arjun Guha, Carolyn Jane Anderson, Molly Q Feldman</dc:creator>
    </item>
    <item>
      <title>Electromyography Based Cross-Subject Limb Angle Estimation via Hierarchical Spiking Attentional Feature Decomposition Network</title>
      <link>https://arxiv.org/abs/2404.07517</link>
      <description>arXiv:2404.07517v2 Announce Type: replace 
Abstract: As human-machine interaction systems are developing towards lightweight and pervasive direction, the role of simultaneous and proportional control (SPC) in human-machine interaction becomes increasingly prominent. However, existing continuous joint angle prediction algorithms based on surface electromyography (sEMG) typically incur high inference costs or are only applicable to specific subjects rather than cross-subject scenarios. Therefore, we proposed a hierarchical Spiking Attentional FEature decomposition Network (SAFE-Net) in order to reduce inference costs and improve recognition accuracy in cross-subject scenarios. This network first encodes the sEMG signals into neural spiking forms through a Spiking Sparse Attention Encoder (SSAE). The compressed features are then decomposed into kinematic features and biological features by a Spiking Attentional Feature Decomposition (SAFD) module. Finally, the kinematic features and biological features are decoded into joint angle values and subject identity, respectively. We validated the effectiveness of SAFE-Net on two datasets (SIAT-DB1 and SIAT-DB2) and compared it with two state-of-the-art methods, Informer and Spikformer. Experimental results demonstrate that, on the one hand, SSAE saves 39.1% and 37.5% power consumption respectively over them in terms of inference costs. On the other hand, SAFE-Net outperforms Informer and Spikformer in recognition accuracy on both datasets. This study showcased that the proposed SAFE-Net can provide accurate predictions in cross-subject scenarios, offering a promising vision for precise continuous control of lower limb rehabilitation exoskeleton robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07517v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhou, Chuang Lin, Can Wang, Xiaojiang Peng</dc:creator>
    </item>
    <item>
      <title>Project Beyond: An Escape Room Game in Virtual Reality to Teach Building Energy Simulations</title>
      <link>https://arxiv.org/abs/2407.02981</link>
      <description>arXiv:2407.02981v2 Announce Type: replace 
Abstract: In recent years, Virtual Reality (VR) has found its way into different fields besides pure entertainment. One of the topics that can benefit from the immersive experience of VR is education. Furthermore, using game-based approaches in education can increase user motivation and engagement. Accordingly, in this paper, we designed and developed an immersive escape room game in VR to teach building energy simulation topics. In the game, players must solve puzzles like, for instance, assembling walls using different materials. We use a player guidance system that combines educational content, puzzles, and different types of hints to educate the players about parameters that influence energy efficiency, structural resistance, and costs. To improve user onboarding, we implemented a tutorial level to teach players general interactions and locomotion. To assess the user experience, we evaluate both the tutorial and the game with an expert study with gaming and VR experts (n=11). The participants were asked to play both the tutorial level and the escape room level and complete two sets of post-questionnaires, one after the tutorial and one after the puzzle level. The one after the tutorial level consisted of NASA-TLX and SUS questionnaires, while after the escape room level we asked users to complete the NASA-TLX, UESSF, and PXI questionnaires. The results indicate that the onboarding level successfully provided good usability while maintaining a low task load. On the other hand, the escape room level can provide an engaging, visually appealing, and usable learning environment by arousing players' curiosity through the gameplay. This environment can be extended in future development stages with different educational contents from various fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02981v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georg Arbesser-Rastburg, Saeed Safikhani, Matej Gustin, Christina Hopfe, Gerald Schweiger, Johanna Pirker</dc:creator>
    </item>
    <item>
      <title>ScreenTK: Seamless Detection of Time-Killing Moments Using Continuous Mobile Screen Text and on-device LLM</title>
      <link>https://arxiv.org/abs/2407.03063</link>
      <description>arXiv:2407.03063v2 Announce Type: replace 
Abstract: Smartphones have become essential to people's digital lives, providing a continuous stream of information and connectivity. However, this constant flow can lead to moments where users are simply passing time rather than engaging meaningfully. This underscores the importance of developing methods to identify these "time-killing" moments, enabling the delivery of important notifications in a way that minimizes interruptions and enhances user engagement. Recent work has utilized screenshots taken every 5 seconds to detect time-killing activities on smartphones. However, this method often misses to capture phone usage between intervals. We demonstrate that up to 50% of time-killing instances go undetected using screenshots, leading to substantial gaps in understanding user behavior. To address this limitation, we propose a method called ScreenTK that detects time-killing moments by leveraging continuous screen text monitoring and on-device large language models (LLMs). Screen text contains more comprehensive information than screenshots and allows LLMs to summarize detailed phone usage. To verify our framework, we conducted experiments with six participants, capturing 1,034 records of different time-killing moments. Initial results show that our framework outperforms state-of-the-art solutions by 38% in our case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03063v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Le Fang, Shiquan Zhang, Hong Jia, Jorge Goncalves, Vassilis Kostakos</dc:creator>
    </item>
    <item>
      <title>A Joint Cross-Attention Model for Audio-Visual Fusion in Dimensional Emotion Recognition</title>
      <link>https://arxiv.org/abs/2203.14779</link>
      <description>arXiv:2203.14779v4 Announce Type: replace-cross 
Abstract: Multimodal emotion recognition has recently gained much attention since it can leverage diverse and complementary relationships over multiple modalities (e.g., audio, visual, biosignals, etc.), and can provide some robustness to noisy modalities. Most state-of-the-art methods for audio-visual (A-V) fusion rely on recurrent networks or conventional attention mechanisms that do not effectively leverage the complementary nature of A-V modalities. In this paper, we focus on dimensional emotion recognition based on the fusion of facial and vocal modalities extracted from videos. Specifically, we propose a joint cross-attention model that relies on the complementary relationships to extract the salient features across A-V modalities, allowing for accurate prediction of continuous values of valence and arousal. The proposed fusion model efficiently leverages the inter-modal relationships, while reducing the heterogeneity between the features. In particular, it computes the cross-attention weights based on correlation between the combined feature representation and individual modalities. By deploying the combined A-V feature representation into the cross-attention module, the performance of our fusion module improves significantly over the vanilla cross-attention module. Experimental results on validation-set videos from the AffWild2 dataset indicate that our proposed A-V fusion model provides a cost-effective solution that can outperform state-of-the-art approaches. The code is available on GitHub: https://github.com/praveena2j/JointCrossAttentional-AV-Fusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14779v4</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. Gnana Praveen, Wheidima Carneiro de Melo, Nasib Ullah, Haseeb Aslam, Osama Zeeshan, Th\'eo Denorme, Marco Pedersoli, Alessandro Koerich, Simon Bacon, Patrick Cardinal, Eric Granger</dc:creator>
    </item>
    <item>
      <title>MenuCraft: Interactive Menu System Design with Large Language Models</title>
      <link>https://arxiv.org/abs/2303.04496</link>
      <description>arXiv:2303.04496v3 Announce Type: replace-cross 
Abstract: Menu system design for user interfaces is a challenging task involving many design options and various human factors. For example, one crucial factor that designers need to consider is the semantic and systematic relation of menu commands. However, capturing these relations can be challenging due to limited available resources. Large language models can be helpful in this regard, using their pre-training knowledge to design and refine menu systems. In this paper, we propose MenuCraft, an AI-assisted designer for menu design that enables collaboration between the designer and a dialogue system to design menus. MenuCraft offers an interactive language-based menu design tool that simplifies the menu design process and enables easy customization of design options. MenuCraft supports a variety of interactions through dialog that allows performing in-context learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.04496v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amir Hossein Kargaran, Nafiseh Nikeghbal, Abbas Heydarnoori, Hinrich Sch\"utze</dc:creator>
    </item>
    <item>
      <title>Deep Neural Networks and Brain Alignment: Brain Encoding and Decoding (Survey)</title>
      <link>https://arxiv.org/abs/2307.10246</link>
      <description>arXiv:2307.10246v2 Announce Type: replace-cross 
Abstract: Can we obtain insights about the brain using AI models? How is the information in deep learning models related to brain recordings? Can we improve AI models with the help of brain recordings? Such questions can be tackled by studying brain recordings like functional magnetic resonance imaging (fMRI). As a first step, the neuroscience community has contributed several large cognitive neuroscience datasets related to passive reading/listening/viewing of concept words, narratives, pictures, and movies. Encoding and decoding models using these datasets have also been proposed in the past two decades. These models serve as additional tools for basic cognitive science and neuroscience research. Encoding models aim at generating fMRI brain representations given a stimulus automatically. They have several practical applications in evaluating and diagnosing neurological conditions and thus may also help design therapies for brain damage. Decoding models solve the inverse problem of reconstructing the stimuli given the fMRI. They are useful for designing brain-machine or brain-computer interfaces. Inspired by the effectiveness of deep learning models for natural language processing, computer vision, and speech, several neural encoding and decoding models have been recently proposed. In this survey, we will first discuss popular representations of language, vision and speech stimuli, and present a summary of neuroscience datasets. Further, we will review popular deep learning based encoding and decoding architectures and note their benefits and limitations. Finally, we will conclude with a summary and discussion about future trends. Given the large amount of recently published work in the computational cognitive neuroscience (CCN) community, we believe that this survey enables an entry point for DNN researchers to diversify into CCN research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10246v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subba Reddy Oota, Zijiao Chen, Manish Gupta, Raju S. Bapi, Gael Jobard, Frederic Alexandre, Xavier Hinaut</dc:creator>
    </item>
    <item>
      <title>AI-Enhanced Virtual Reality in Medicine: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2402.03093</link>
      <description>arXiv:2402.03093v2 Announce Type: replace-cross 
Abstract: With the rapid advance of computer graphics and artificial intelligence technologies, the ways we interact with the world have undergone a transformative shift. Virtual Reality (VR) technology, aided by artificial intelligence (AI), has emerged as a dominant interaction media in multiple application areas, thanks to its advantage of providing users with immersive experiences. Among those applications, medicine is considered one of the most promising areas. In this paper, we present a comprehensive examination of the burgeoning field of AI-enhanced VR applications in medical care and services. By introducing a systematic taxonomy, we meticulously classify the pertinent techniques and applications into three well-defined categories based on different phases of medical diagnosis and treatment: Visualization Enhancement, VR-related Medical Data Processing, and VR-assisted Intervention. This categorization enables a structured exploration of the diverse roles that AI-powered VR plays in the medical domain, providing a framework for a more comprehensive understanding and evaluation of these technologies. To our best knowledge, this is the first systematic survey of AI-powered VR systems in medical settings, laying a foundation for future research in this interdisciplinary domain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03093v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yixuan Wu, Kaiyuan Hu, Danny Z. Chen, Jian Wu</dc:creator>
    </item>
    <item>
      <title>On-Demand Mobility Services for Infrastructure and Community Resilience: A Review toward Synergistic Disaster Response Systems</title>
      <link>https://arxiv.org/abs/2403.03107</link>
      <description>arXiv:2403.03107v2 Announce Type: replace-cross 
Abstract: Mobility-on-demand (MOD) services have the potential to significantly improve the adaptiveness and recovery of urban systems, in the wake of disruptive events. But there lacks a comprehensive review on using MOD services for such purposes in addition to serving regular travel demand. This paper presents a review that suggests a noticeable increase within recent years on this topic across four main areas: resilient MOD services, novel usage of MOD services for improving infrastructure and community resilience, empirical impact evaluation, and enabling and augmenting technologies. The review shows that MOD services have been utilized to support anomaly detection, essential supply delivery, evacuation and rescue, on-site medical care, power grid stabilization, transit service substitution during downtime, and infrastructure and equipment repair. Such a versatility suggests a comprehensive assessment framework and modeling methodologies for evaluating system design alternatives that simultaneously serve different purposes. The review also reveals that integrating suitable technologies, business models, and long-term planning efforts offers significant synergistic benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03107v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangbo Yu</dc:creator>
    </item>
    <item>
      <title>Synthetic Participatory Planning of Shard Automated Electric Mobility Systems</title>
      <link>https://arxiv.org/abs/2404.12317</link>
      <description>arXiv:2404.12317v4 Announce Type: replace-cross 
Abstract: Unleashing the synergies among rapidly evolving mobility technologies in a multi-stakeholder setting presents unique challenges and opportunities for addressing urban transportation problems. This paper introduces a novel synthetic participatory method that critically leverages large language models (LLMs) to create digital avatars representing diverse stakeholders to plan shared automated electric mobility systems (SAEMS). These calibratable agents collaboratively identify objectives, envision and evaluate SAEMS alternatives, and strategize implementation under risks and constraints. The results of a Montreal case study indicate that a structured and parameterized workflow provides outputs with higher controllability and comprehensiveness on an SAEMS plan than that generated using a single LLM-enabled expert agent. Consequently, this approach provides a promising avenue for cost-efficiently improving the inclusivity and interpretability of multi-objective transportation planning, suggesting a paradigm shift in how we envision and strategize for sustainable transportation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12317v4</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/su16135618</arxiv:DOI>
      <dc:creator>Jiangbo Yu, Graeme McKinley</dc:creator>
    </item>
    <item>
      <title>Beyond static AI evaluations: advancing human interaction evaluations for LLM harms and risks</title>
      <link>https://arxiv.org/abs/2405.10632</link>
      <description>arXiv:2405.10632v4 Announce Type: replace-cross 
Abstract: Model evaluations are central to understanding the safety, risks, and societal impacts of AI systems. While most real-world AI applications involve human-AI interaction, most current evaluations (e.g., common benchmarks) of AI models do not. Instead, they incorporate human factors in limited ways, assessing the safety of models in isolation, thereby falling short of capturing the complexity of human-model interactions. In this paper, we discuss and operationalize a definition of an emerging category of evaluations -- "human interaction evaluations" (HIEs) -- which focus on the assessment of human-model interactions or the process and the outcomes of humans using models. First, we argue that HIEs can be used to increase the validity of safety evaluations, assess direct human impact and interaction-specific harms, and guide future assessments of models' societal impact. Second, we propose a safety-focused HIE design framework -- containing a human-LLM interaction taxonomy -- with three stages: (1) identifying the risk or harm area, (2) characterizing the use context, and (3) choosing the evaluation parameters. Third, we apply our framework to two potential evaluations for overreliance and persuasion risks. Finally, we conclude with tangible recommendations for addressing concerns over costs, replicability, and unrepresentativeness of HIEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10632v4</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujain Ibrahim, Saffron Huang, Lama Ahmad, Markus Anderljung</dc:creator>
    </item>
    <item>
      <title>Open-TeleVision: Teleoperation with Immersive Active Visual Feedback</title>
      <link>https://arxiv.org/abs/2407.01512</link>
      <description>arXiv:2407.01512v2 Announce Type: replace-cross 
Abstract: Teleoperation serves as a powerful method for collecting on-robot data essential for robot learning from demonstrations. The intuitiveness and ease of use of the teleoperation system are crucial for ensuring high-quality, diverse, and scalable data. To achieve this, we propose an immersive teleoperation system Open-TeleVision that allows operators to actively perceive the robot's surroundings in a stereoscopic manner. Additionally, the system mirrors the operator's arm and hand movements on the robot, creating an immersive experience as if the operator's mind is transmitted to a robot embodiment. We validate the effectiveness of our system by collecting data and training imitation learning policies on four long-horizon, precise tasks (Can Sorting, Can Insertion, Folding, and Unloading) for 2 different humanoid robots and deploy them in the real world. The system is open-sourced at: https://robot-tv.github.io/</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01512v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuxin Cheng, Jialong Li, Shiqi Yang, Ge Yang, Xiaolong Wang</dc:creator>
    </item>
    <item>
      <title>Adaptive Modality Balanced Online Knowledge Distillation for Brain-Eye-Computer based Dim Object Detection</title>
      <link>https://arxiv.org/abs/2407.01894</link>
      <description>arXiv:2407.01894v2 Announce Type: replace-cross 
Abstract: Advanced cognition can be extracted from the human brain using brain-computer interfaces. Integrating these interfaces with computer vision techniques, which possess efficient feature extraction capabilities, can achieve more robust and accurate detection of dim targets in aerial images. However, existing target detection methods primarily concentrate on homogeneous data, lacking efficient and versatile processing capabilities for heterogeneous multimodal data. In this paper, we first build a brain-eye-computer based object detection system for aerial images under few-shot conditions. This system detects suspicious targets using region proposal networks, evokes the event-related potential (ERP) signal in electroencephalogram (EEG) through the eye-tracking-based slow serial visual presentation (ESSVP) paradigm, and constructs the EEG-image data pairs with eye movement data. Then, an adaptive modality balanced online knowledge distillation (AMBOKD) method is proposed to recognize dim objects with the EEG-image data. AMBOKD fuses EEG and image features using a multi-head attention module, establishing a new modality with comprehensive features. To enhance the performance and robust capability of the fusion modality, simultaneous training and mutual learning between modalities are enabled by end-to-end online knowledge distillation. During the learning process, an adaptive modality balancing module is proposed to ensure multimodal equilibrium by dynamically adjusting the weights of the importance and the training gradients across various modalities. The effectiveness and superiority of our method are demonstrated by comparing it with existing state-of-the-art methods. Additionally, experiments conducted on public datasets and system validations in real-world scenarios demonstrate the reliability and practicality of the proposed system and the designed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01894v2</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixing Li, Chao Yan, Zhen Lan, Xiaojia Xiang, Han Zhou, Jun Lai, Dengqing Tang</dc:creator>
    </item>
  </channel>
</rss>
