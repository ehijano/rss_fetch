<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>eess.IV updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/eess.IV</link>
    <description>eess.IV updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/eess.IV" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 18:39:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fundus2Globe: Generative AI-Driven 3D Digital Twins for Personalized Myopia Management</title>
      <link>https://arxiv.org/abs/2502.13182</link>
      <description>arXiv:2502.13182v1 Announce Type: new 
Abstract: Myopia, projected to affect 50% population globally by 2050, is a leading cause of vision loss. Eyes with pathological myopia exhibit distinctive shape distributions, which are closely linked to the progression of vision-threatening complications. Recent understanding of eye-shape-based biomarkers requires magnetic resonance imaging (MRI), however, it is costly and unrealistic in routine ophthalmology clinics. We present Fundus2Globe, the first AI framework that synthesizes patient-specific 3D eye globes from ubiquitous 2D color fundus photographs (CFPs) and routine metadata (axial length, spherical equivalent), bypassing MRI dependency. By integrating a 3D morphable eye model (encoding biomechanical shape priors) with a latent diffusion model, our approach achieves submillimeter accuracy in reconstructing posterior ocular anatomy efficiently. Fundus2Globe uniquely quantifies how vision-threatening lesions (e.g., staphylomas) in CFPs correlate with MRI-validated 3D shape abnormalities, enabling clinicians to simulate posterior segment changes in response to refractive shifts. External validation demonstrates its robust generation performance, ensuring fairness across underrepresented groups. By transforming 2D fundus imaging into 3D digital replicas of ocular structures, Fundus2Globe is a gateway for precision ophthalmology, laying the foundation for AI-driven, personalized myopia management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13182v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Danli Shi, Bowen Liu, Zhen Tian, Yue Wu, Jiancheng Yang, Ruoyu Chen, Bo Yang, Ou Xiao, Mingguang He</dc:creator>
    </item>
    <item>
      <title>Synthetic generation of 2D data records based on Autoencoders</title>
      <link>https://arxiv.org/abs/2502.13183</link>
      <description>arXiv:2502.13183v1 Announce Type: new 
Abstract: Gas Chromatography coupled with Ion Mobility Spectrometry (GC-IMS) is a dual-separation analytical technique widely used for identifying components in gaseous samples by separating and analysing the arrival times of their constituent species. Data generated by GC-IMS is typically represented as two-dimensional spectra, providing rich information but posing challenges for data-driven analysis due to limited labelled datasets. This study introduces a novel method for generating synthetic 2D spectra using a deep learning framework based on Autoencoders. Although applied here to GC-IMS data, the approach is broadly applicable to any two-dimensional spectral measurements where labelled data are scarce. While performing component classification over a labelled dataset of GC-IMS records, the addition of synthesized records significantly has improved the classification performance, demonstrating the method's potential for overcoming dataset limitations in machine learning frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13183v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Darius Couchard, Oscar Olarte, Rob Haelterman</dc:creator>
    </item>
    <item>
      <title>SpeHeatal: A Cluster-Enhanced Segmentation Method for Sperm Morphology Analysis</title>
      <link>https://arxiv.org/abs/2502.13192</link>
      <description>arXiv:2502.13192v1 Announce Type: new 
Abstract: The accurate assessment of sperm morphology is crucial in andrological diagnostics, where the segmentation of sperm images presents significant challenges. Existing approaches frequently rely on large annotated datasets and often struggle with the segmentation of overlapping sperm and the presence of dye impurities. To address these challenges, this paper first analyzes the issue of overlapping sperm tails from a geometric perspective and introduces a novel clustering algorithm, Con2Dis, which effectively segments overlapping tails by considering three essential factors: CONnectivity, CONformity, and DIStance. Building on this foundation, we propose an unsupervised method, SpeHeatal, designed for the comprehensive segmentation of the SPErm HEAd and TAiL. SpeHeatal employs the Segment Anything Model(SAM) to generate masks for sperm heads while filtering out dye impurities, utilizes Con2Dis to segment tails, and then applies a tailored mask splicing technique to produce complete sperm masks. Experimental results underscore the superior performance of SpeHeatal, particularly in handling images with overlapping sperm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13192v1</guid>
      <category>eess.IV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Shi, Yunkai Wang, Xupeng Tian, Tieyi Zhang, Bing Yao, Hui Wang, Yong Shao, Cencen Wang, Rong Zeng</dc:creator>
    </item>
    <item>
      <title>Denoising Designs-inherited Search Framework for Image Denoising</title>
      <link>https://arxiv.org/abs/2502.13359</link>
      <description>arXiv:2502.13359v1 Announce Type: new 
Abstract: How to benefit from plenty of existing denoising designs? Few methods via Neural Architecture Search (NAS) intend to answer this question. However, these NAS-based denoising methods explore limited search space and are hard to extend in terms of search space due to high computational burden. To tackle these limitations, we propose the first search framework to explore mainstream denoising designs. In our framework, the search space consists of the network-level, the cell-level and the kernel-level search space, which aims to inherit as many denoising designs as possible. Coordinating search strategies are proposed to facilitate the extension of various denoising designs. In such a giant search space, it is laborious to search for an optimal architecture. To solve this dilemma, we introduce the first regularization, i.e., denoising prior-based regularization, which reduces the search difficulty. To get an efficient architecture, we introduce the other regularization, i.e., inference time-based regularization, optimizes the search process on model complexity. Based on our framework, our searched architecture achieves state-of-the-art results for image denoising on multiple real-world and synthetic datasets. The parameters of our searched architecture are $1/3$ of Restormer's, and our method surpasses existing NAS-based denoising methods by $1.50$ dB in the real-world dataset. Moreover, we discuss the preferences of $\textbf{200}$ searched architectures, and provide directions for further work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13359v1</guid>
      <category>eess.IV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheyu Zhang, Yueyi Zhang, Xiaoyan sun</dc:creator>
    </item>
    <item>
      <title>RestoreGrad: Signal Restoration Using Conditional Denoising Diffusion Models with Jointly Learned Prior</title>
      <link>https://arxiv.org/abs/2502.13574</link>
      <description>arXiv:2502.13574v1 Announce Type: new 
Abstract: Denoising diffusion probabilistic models (DDPMs) can be utilized for recovering a clean signal from its degraded observation(s) by conditioning the model on the degraded signal. The degraded signals are themselves contaminated versions of the clean signals; due to this correlation, they may encompass certain useful information about the target clean data distribution. However, existing adoption of the standard Gaussian as the prior distribution in turn discards such information, resulting in sub-optimal performance. In this paper, we propose to improve conditional DDPMs for signal restoration by leveraging a more informative prior that is jointly learned with the diffusion model. The proposed framework, called RestoreGrad, seamlessly integrates DDPMs into the variational autoencoder framework and exploits the correlation between the degraded and clean signals to encode a better diffusion prior. On speech and image restoration tasks, we show that RestoreGrad demonstrates faster convergence (5-10 times fewer training steps) to achieve better quality of restored signals over existing DDPM baselines, and improved robustness to using fewer sampling steps in inference time (2-2.5 times fewer), advocating the advantages of leveraging jointly learned prior for efficiency improvements in the diffusion process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13574v1</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>eess.AS</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ching-Hua Lee, Chouchang Yang, Jaejin Cho, Yashas Malur Saidutta, Rakshith Sharma Srinivasa, Yilin Shen, Hongxia Jin</dc:creator>
    </item>
    <item>
      <title>Pericoronary adipose tissue attenuation as a predictor of functional severity of coronary stenosis</title>
      <link>https://arxiv.org/abs/2502.13649</link>
      <description>arXiv:2502.13649v1 Announce Type: new 
Abstract: Objective: This study aims to evaluate the functional significance of coronary stenosis by analyzing low-level radiomic features of the pericoronary adipose tissue (PCAT) surrounding the lesions, which are indicative of its inflammation status. Methods: A dataset of 72 patients who underwent coronary computed tomography angiography (CCTA) was analyzed, with 3D segmentation and computational fluid dynamics (CFD) simulations from a prior study. Centerlines of the main epicardial branches were automatically extracted, and lesions identified using Gaussian kernel regression to estimate healthy branch caliber. PCAT features were computed per vessel following guideline recommendations and per lesion within a region extending radially for two vessel radii. Features like fat volume and mean attenuation (FAI) were analyzed for their relationship with CFD-derived hemodynamic biomarkers, such as fractional flow reserve (FFR) and wall shear stress (WSS). These features also informed a machine learning (ML) model for classifying potentially ischemic lesions. Results: PCAT exhibited, on average, higher attenuation in the presence of hemodynamically significant lesions (i.e., FFR &lt; 0.80), although this difference was of limited statistical significance. The ML classifier, trained on PCAT features, successfully distinguished potentially ischemic lesions, yielding average accuracy of 0.84. Conclusion: PCAT attenuation is correlated with the functional status of coronary stenosis and can be used to inform ML models for predicting potential ischemia. Significance: PCAT features, readily available from CCTA, can be used to predict the hemodynamic characteristics of a lesion without the need for an invasive FFR examination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13649v1</guid>
      <category>eess.IV</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marta Pillitteri, Guido Nannini, Simone Saitta, Luca Mariani, Riccardo Maragna, Andrea Baggiano, Gianluca Pontone, Alberto Redaelli</dc:creator>
    </item>
    <item>
      <title>MGFI-Net: A Multi-Grained Feature Integration Network for Enhanced Medical Image Segmentation</title>
      <link>https://arxiv.org/abs/2502.13808</link>
      <description>arXiv:2502.13808v1 Announce Type: new 
Abstract: Medical image segmentation plays a crucial role in various clinical applications. A major challenge in medical image segmentation is achieving accurate delineation of regions of interest in the presence of noise, low contrast, or complex anatomical structures. Existing segmentation models often neglect the integration of multi-grained information and fail to preserve edge details, which are critical for precise segmentation. To address these challenges, we propose a novel image semantic segmentation model called the Multi-Grained Feature Integration Network (MGFI-Net). Our MGFI-Net is designed with two dedicated modules to tackle these issues. First, to enhance segmentation accuracy, we introduce a Multi-Grained Feature Extraction Module, which leverages hierarchical relationships between different feature scales to selectively focus on the most relevant information. Second, to preserve edge details, we incorporate an Edge Enhancement Module that effectively retains and integrates boundary information to refine segmentation results. Extensive experiments demonstrate that MGFI-Net not only outperforms state-of-the-art methods in terms of segmentation accuracy but also achieves superior time efficiency, establishing it as a leading solution for real-time medical image segmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13808v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yucheng Zeng</dc:creator>
    </item>
    <item>
      <title>Impact of Optic Nerve Tortuosity, Globe Proptosis, and Size on Retinal Ganglion Cell Thickness Across General, Glaucoma, and Myopic Populations: Insights from the UK Biobank</title>
      <link>https://arxiv.org/abs/2502.13147</link>
      <description>arXiv:2502.13147v1 Announce Type: cross 
Abstract: Purpose: To investigate the impact of optic nerve tortuosity (ONT), and the interaction of globe proptosis and globe size on retinal ganglion cell (RGC) thickness, using Retinal Nerve Fiber Layer (RNFL) thickness, across general, glaucoma, and myopic populations. Methods: We analyzed 17,970 eyes from the UKBiobank cohort (ID 76442), including 371 glaucoma and 2481 myopic eyes. AI models segmented structures from 3D optical coherence tomography (OCT) scans and magnetic resonance images (MRI). RNFL thickness was derived from OCT scans and corrected for ocular magnification, was derived from OCT. From MRIs, we extracted: ONT, globe proptosis, axial length, and a novel interzygomatic line-to-posterior pole (ILPP) distance, a composite marker of globe proptosis and size. GEE models assessed associations between orbital and retinal features across all populations. Results: Segmentation models achieved Dice coefficients over 0.94 for both MRI and OCT. RNFL thickness was positively correlated with both ONT and ILPP distance (r = 0.065, p &lt; 0.001, and r = 0.206, p &lt; 0.001 respectively). The same was true for glaucoma (r = 0.140, p &lt; 0.01, and r = 0.256, p &lt; 0.01), and for myopia (r = 0.071, p &lt; 0.001, and r = 0.100, p &lt; 0.0001). GEE models revealed straighter optic nerves and shorter ILPP distance as predictive of thinner RNFL in all populations. Conclusions: This study emphasizes the impact of ONT, globe size, and proptosis on retinal health, suggesting RNFL thinning may arise from biomechanical stress due to straighter optic nerves or reduced ILPP distance, particularly in glaucoma or myopia. The novel ILPP metric, integrating globe size and position, shows potential as a biomarker for axonal health. These findings highlight the role of orbit structures in RGC axonal health and warrant further exploration of the biomechanical relationship between the orbit and optic nerve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13147v1</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Charis Y. N. Chiang, Xiaofei Wang, Stuart K. Gardiner, Martin Buist, Michael J. A. Girard</dc:creator>
    </item>
    <item>
      <title>Generative Video Semantic Communication via Multimodal Semantic Fusion with Large Model</title>
      <link>https://arxiv.org/abs/2502.13838</link>
      <description>arXiv:2502.13838v1 Announce Type: cross 
Abstract: Despite significant advancements in traditional syntactic communications based on Shannon's theory, these methods struggle to meet the requirements of 6G immersive communications, especially under challenging transmission conditions. With the development of generative artificial intelligence (GenAI), progress has been made in reconstructing videos using high-level semantic information. In this paper, we propose a scalable generative video semantic communication framework that extracts and transmits semantic information to achieve high-quality video reconstruction. Specifically, at the transmitter, description and other condition signals (e.g., first frame, sketches, etc.) are extracted from the source video, functioning as text and structural semantics, respectively. At the receiver, the diffusion-based GenAI large models are utilized to fuse the semantics of the multiple modalities for reconstructing the video. Simulation results demonstrate that, at an ultra-low channel bandwidth ratio (CBR), our scheme effectively captures semantic information to reconstruct videos aligned with human perception under different signal-to-noise ratios. Notably, the proposed ``First Frame+Desc." scheme consistently achieves CLIP score exceeding 0.92 at CBR = 0.0057 for SNR &gt; 0 dB. This demonstrates its robust performance even under low SNR conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13838v1</guid>
      <category>eess.SP</category>
      <category>cs.CV</category>
      <category>cs.IT</category>
      <category>eess.IV</category>
      <category>math.IT</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hang Yin, Li Qiao, Yu Ma, Shuo Sun, Kan Li, Zhen Gao, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>V2C-Long: Longitudinal Cortex Reconstruction with Spatiotemporal Correspondence</title>
      <link>https://arxiv.org/abs/2402.17438</link>
      <description>arXiv:2402.17438v2 Announce Type: replace 
Abstract: Reconstructing the cortex from longitudinal magnetic resonance imaging (MRI) is indispensable for analyzing morphological alterations in the human brain. Despite the recent advancement of cortical surface reconstruction with deep learning, challenges arising from longitudinal data are still persistent. Especially the lack of strong spatiotemporal point correspondence between highly convoluted brain surfaces hinders downstream analyses, as local morphology is not directly comparable if the anatomical location is not matched precisely. To address this issue, we present V2C-Long, the first dedicated deep learning-based cortex reconstruction method for longitudinal MRI. V2C-Long exhibits strong inherent spatiotemporal correspondence across subjects and visits, thereby reducing the need for surface-based post-processing. We establish this correspondence directly during the reconstruction via the composition of two deep template-deformation networks and innovative aggregation of within-subject templates in mesh space. We validate V2C-Long on two large neuroimaging studies, focusing on surface accuracy, consistency, generalization, test-retest reliability, and sensitivity. The results reveal a substantial improvement in longitudinal consistency and accuracy compared to existing methods. In addition, we demonstrate stronger evidence for longitudinal cortical atrophy in Alzheimer's disease than longitudinal FreeSurfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.17438v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fabian Bongratz, Jan Fecht, Anne-Marie Rickmann, Christian Wachinger</dc:creator>
    </item>
    <item>
      <title>FreqPrior: Improving Video Diffusion Models with Frequency Filtering Gaussian Noise</title>
      <link>https://arxiv.org/abs/2502.03496</link>
      <description>arXiv:2502.03496v2 Announce Type: replace 
Abstract: Text-driven video generation has advanced significantly due to developments in diffusion models. Beyond the training and sampling phases, recent studies have investigated noise priors of diffusion models, as improved noise priors yield better generation results. One recent approach employs the Fourier transform to manipulate noise, marking the initial exploration of frequency operations in this context. However, it often generates videos that lack motion dynamics and imaging details. In this work, we provide a comprehensive theoretical analysis of the variance decay issue present in existing methods, contributing to the loss of details and motion dynamics. Recognizing the critical impact of noise distribution on generation quality, we introduce FreqPrior, a novel noise initialization strategy that refines noise in the frequency domain. Our method features a novel filtering technique designed to address different frequency signals while maintaining the noise prior distribution that closely approximates a standard Gaussian distribution. Additionally, we propose a partial sampling process by perturbing the latent at an intermediate timestep during finding the noise prior, significantly reducing inference time without compromising quality. Extensive experiments on VBench demonstrate that our method achieves the highest scores in both quality and semantic assessments, resulting in the best overall total score. These results highlight the superiority of our proposed noise prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03496v2</guid>
      <category>eess.IV</category>
      <category>cs.GR</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunlong Yuan, Yuanfan Guo, Chunwei Wang, Wei Zhang, Hang Xu, Li Zhang</dc:creator>
    </item>
    <item>
      <title>FakET: Simulating Cryo-Electron Tomograms with Neural Style Transfer</title>
      <link>https://arxiv.org/abs/2304.02011</link>
      <description>arXiv:2304.02011v4 Announce Type: replace-cross 
Abstract: In cryo-electron microscopy, accurate particle localization and classification are imperative. Recent deep learning solutions, though successful, require extensive training data sets. The protracted generation time of physics-based models, often employed to produce these data sets, limits their broad applicability. We introduce FakET, a method based on Neural Style Transfer, capable of simulating the forward operator of any cryo transmission electron microscope. It can be used to adapt a synthetic training data set according to reference data producing high-quality simulated micrographs or tilt-series. To assess the quality of our generated data, we used it to train a state-of-the-art localization and classification architecture and compared its performance with a counterpart trained on benchmark data. Remarkably, our technique matches the performance, boosts data generation speed 750 times, uses 33 times less memory, and scales well to typical transmission electron microscope detector sizes. It leverages GPU acceleration and parallel processing. The source code is available at https://github.com/paloha/faket.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02011v4</guid>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.str.2025.01.020</arxiv:DOI>
      <arxiv:journal_reference>Structure, 2025</arxiv:journal_reference>
      <dc:creator>Pavol Harar, Lukas Herrmann, Philipp Grohs, David Haselbach</dc:creator>
    </item>
    <item>
      <title>Spatially Regularized Super-Resolved Constrained Spherical Deconvolution (SR$^2$-CSD) of Diffusion MRI Data</title>
      <link>https://arxiv.org/abs/2408.12921</link>
      <description>arXiv:2408.12921v2 Announce Type: replace-cross 
Abstract: Constrained Spherical Deconvolution (CSD) is crucial for estimating white matter fiber orientations using diffusion MRI data. A relevant parameter in CSD is the maximum order $l_{max}$ used in the spherical harmonics series, influencing the angular resolution of the Fiber Orientation Distributions (FODs). Lower $l_{max}$ values produce smoother and more stable estimates, but result in reduced angular resolution. Conversely, higher $l_{max}$ values, as employed in the Super-Resolved CSD variant, are essential for resolving narrow inter-fiber angles but lead to spurious lobes due to increased noise sensitivity. To address this issue, we propose a novel Spatially Regularized Super-Resolved CSD (SR$^2$-CSD) approach, incorporating spatial priors into the CSD framework. This method leverages spatial information among adjacent voxels, enhancing the stability and noise robustness of FOD estimations. SR$^2$-CSD facilitates the practical use of Super-Resolved CSD by including a J-invariant auto-calibrated total variation FOD denoiser. We evaluated the performance of SR$^2$-CSD against standard CSD and Super-Resolved CSD using phantom numerical data and various real brain datasets, including a test-retest sample of six subjects scanned twice. In phantom data, SR$^2$-CSD outperformed both CSD and Super-Resolved CSD, reducing the angular error (AE) by approximately half and the peak number error (PNE) by a factor of three across all noise levels considered. In real data, SR$^2$-CSD produced more continuous FOD estimates with higher spatial-angular coherency. In the test-retest sample, SR$^2$-CSD consistently yielded more reproducible estimates, with reduced AE, PNE, mean squared error, and increased angular correlation coefficient between the FODs estimated from the two scans for each subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12921v2</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ekin Taskin (Signal Processing Laboratory 5), Juan Luis Villarreal Haro (Signal Processing Laboratory 5), Gabriel Girard (Signal Processing Laboratory 5, Department of Computer Science, Universit\'e de Sherbrooke, Sherbrooke, Canada), Jonathan Rafael-Pati\~no (Signal Processing Laboratory 5, Radiology Department, Centre Hospitalier Universitaire Vaudois and University of Lausanne, Switzerland), Eleftherios Garyfallidis (Intelligent Systems Engineering, Indiana University, Bloomington, United States), Jean-Philippe Thiran (Signal Processing Laboratory 5, Radiology Department, Centre Hospitalier Universitaire Vaudois and University of Lausanne, Switzerland), Erick Jorge Canales-Rodr\'iguez (Signal Processing Laboratory 5)</dc:creator>
    </item>
    <item>
      <title>Geometry of the cumulant series in neuroimaging</title>
      <link>https://arxiv.org/abs/2409.03010</link>
      <description>arXiv:2409.03010v2 Announce Type: replace-cross 
Abstract: Water diffusion gives rise to micrometer-scale sensitivity of diffusion MRI (dMRI) to cellular-level tissue structure. The advent of precision medicine and quantitative imaging hinges on revealing the information content of dMRI, and providing its parsimonious basis- and hardware-independent "fingerprint". Here we reveal the geometry of a multi-dimensional dMRI signal, classify all 21 invariants of diffusion and covariance tensors in terms of irreducible representations of the group of rotations, and relate them to tissue properties. Previously studied dMRI contrasts are expressed via 7 invariants, while the remaining 14 provide novel complementary information. We design acquisitions based on icosahedral vertices guaranteeing minimal number of measurements to determine 3-4 most used invariants in only 1-2 minutes for the whole brain. Representing dMRI signals via scalar invariant maps with definite symmetries will underpin machine learning classifiers of brain pathology, development, and aging, while fast protocols will enable translation of advanced dMRI into clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03010v2</guid>
      <category>physics.med-ph</category>
      <category>eess.IV</category>
      <category>physics.bio-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago Coelho, Filip Szczepankiewicz, Els Fieremans, Dmitry S. Novikov</dc:creator>
    </item>
    <item>
      <title>RSNet: A Light Framework for The Detection of Multi-scale Remote Sensing Targets</title>
      <link>https://arxiv.org/abs/2410.23073</link>
      <description>arXiv:2410.23073v5 Announce Type: replace-cross 
Abstract: Recent advancements in synthetic aperture radar (SAR) ship detection using deep learning have significantly improved accuracy and speed, yet effectively detecting small objects in complex backgrounds with fewer parameters remains a challenge. This letter introduces RSNet, a lightweight framework constructed to enhance ship detection in SAR imagery. To ensure accuracy with fewer parameters, we proposed Waveletpool-ContextGuided (WCG) as its backbone, guiding global context understanding through multi-scale wavelet features for effective detection in complex scenes. Additionally, Waveletpool-StarFusion (WSF) is introduced as the neck, employing a residual wavelet element-wise multiplication structure to achieve higher dimensional nonlinear features without increasing network width. The Lightweight-Shared (LS) module is designed as detect components to achieve efficient detection through lightweight shared convolutional structure and multi-format compatibility. Experiments on the SAR Ship Detection Dataset (SSDD) and High-Resolution SAR Image Dataset (HRSID) demonstrate that RSNet achieves a strong balance between lightweight design and detection performance, surpassing many state-of-the-art detectors, reaching 72.5\% and 67.6\% in \textbf{\(\mathbf{mAP_{.50:.95}}\) }respectively with 1.49M parameters. Our code will be released soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23073v5</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hongyu Chen, Chengcheng Chen, Fei Wang, Yuhu Shi, Weiming Zeng</dc:creator>
    </item>
    <item>
      <title>A New Logic For Pediatric Brain Tumor Segmentation</title>
      <link>https://arxiv.org/abs/2411.01390</link>
      <description>arXiv:2411.01390v3 Announce Type: replace-cross 
Abstract: In this paper, we present a novel approach for segmenting pediatric brain tumors using a deep learning architecture, inspired by expert radiologists' segmentation strategies. Our model delineates four distinct tumor labels and is benchmarked on a held-out PED BraTS 2024 test set (i.e., pediatric brain tumor datasets introduced by BraTS). Furthermore, we evaluate our model's performance against the state-of-the-art (SOTA) model using a new external dataset of 30 patients from CBTN (Children's Brain Tumor Network), labeled in accordance with the PED BraTS 2024 guidelines and 2023 BraTS Adult Glioma dataset. We compare segmentation outcomes with the winning algorithm from the PED BraTS 2023 challenge as the SOTA model. Our proposed algorithm achieved an average Dice score of 0.642 and an HD95 of 73.0 mm on the CBTN test data, outperforming the SOTA model, which achieved a Dice score of 0.626 and an HD95 of 84.0 mm. Moreover, our model exhibits strong generalizability, attaining a 0.877 Dice score in whole tumor segmentation on the BraTS 2023 Adult Glioma dataset, surpassing existing SOTA. Our results indicate that the proposed model is a step towards providing more accurate segmentation for pediatric brain tumors, which is essential for evaluating therapy response and monitoring patient progress. Our source code is available at https://github.com/NUBagciLab/Pediatric-Brain-Tumor-Segmentation-Model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01390v3</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Bengtsson, Elif Keles, Gorkem Durak, Syed Anwar, Yuri S. Velichko, Marius G. Linguraru, Angela J. Waanders, Ulas Bagci</dc:creator>
    </item>
  </channel>
</rss>
