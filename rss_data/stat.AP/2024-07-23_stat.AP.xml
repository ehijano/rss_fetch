<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Jul 2024 04:01:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multiple merger coalescent inference of effective population size</title>
      <link>https://arxiv.org/abs/2407.14976</link>
      <description>arXiv:2407.14976v1 Announce Type: new 
Abstract: Variation in a sample of molecular sequence data informs about the past evolutionary history of the sample's population. Traditionally, Bayesian modeling coupled with the standard coalescent, is used to infer the sample's bifurcating genealogy and demographic and evolutionary parameters such as effective population size, and mutation rates. However, there are many situations where binary coalescent models do not accurately reflect the true underlying ancestral processes. Here, we propose a Bayesian nonparametric method for inferring effective population size trajectories from a multifurcating genealogy under the $\Lambda-$coalescent. In particular, we jointly estimate the effective population size and model parameters for the Beta-coalescent model, a special type of $\Lambda-$coalescent. Finally, we test our methods on simulations and apply them to study various viral dynamics as well as Japanese sardine population size changes over time. The code and vignettes can be found in the phylodyn package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14976v1</guid>
      <category>stat.AP</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julie Zhang, Julia A. Palacios</dc:creator>
    </item>
    <item>
      <title>A new paradigm of mortality modeling via individual vitality dynamics</title>
      <link>https://arxiv.org/abs/2407.15388</link>
      <description>arXiv:2407.15388v1 Announce Type: new 
Abstract: The significance of mortality modeling extends across multiple research areas, including life insurance valuation, longevity risk management, life-cycle hypothesis, and retirement income planning. Despite the variety of existing approaches, such as mortality laws and factor-based models, they often lack compatibility or fail to meet specific research needs. To address these shortcomings, this study introduces a novel approach centered on modeling the dynamics of individual vitality and defining mortality as the depletion of vitality level to zero. More specifically, we develop a four-component framework to analyze the initial value, trend, diffusion, and sudden changes in vitality level over an individual's lifetime. We demonstrate the framework's estimation and analytical capabilities in various settings and discuss its practical implications in actuarial problems and other research areas. The broad applicability and interpretability of our vitality-based modeling approach offer an enhanced paradigm for mortality modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15388v1</guid>
      <category>stat.AP</category>
      <category>q-fin.RM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaobai Zhu, Kenneth Q. Zhou, Zijia Wang</dc:creator>
    </item>
    <item>
      <title>Data Space Inversion for Efficient Predictions and Uncertainty Quantification for Geothermal Models</title>
      <link>https://arxiv.org/abs/2407.15401</link>
      <description>arXiv:2407.15401v1 Announce Type: new 
Abstract: The ability to make accurate predictions with quantified uncertainty provides a crucial foundation for the successful management of a geothermal reservoir. Conventional approaches for making predictions using geothermal reservoir models involve estimating unknown model parameters using field data, then propagating the uncertainty in these estimates through to the predictive quantities of interest. However, the unknown parameters are not always of direct interest; instead, the predictions are of primary importance. Data space inversion (DSI) is an alternative methodology that allows for the efficient estimation of predictive quantities of interest, with quantified uncertainty, that avoids the need to estimate model parameters entirely. In this paper, we evaluate the applicability of DSI to geothermal reservoir modelling. We first review the processes of model calibration, prediction and uncertainty quantification from a Bayesian perspective, and introduce data space inversion as a simple, efficient technique for approximating the posterior predictive distribution. We then apply the DSI framework to two model problems in geothermal reservoir modelling. We evaluate the accuracy and efficiency of DSI relative to other common methods for uncertainty quantification, study how the number of reservoir model simulations affects the resulting approximation to the posterior predictive distribution, and demonstrate how the framework can be enhanced through the use of suitable reparametrisations. Our results support the idea that data space inversion is a simple, robust and efficient technique for making predictions with quantified uncertainty using geothermal reservoir models, providing a useful alternative to more conventional approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15401v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex de Beer, Andrew Power, Daniel Wong, Ken Dekkers, Michael Gravatt, John P. O'Sullivan, Michael J. O'Sullivan, Oliver J. Maclaren, Ruanui Nicholson</dc:creator>
    </item>
    <item>
      <title>A two-step model to study the inclusivity's distribution of Italian early childhood education and care services</title>
      <link>https://arxiv.org/abs/2407.15610</link>
      <description>arXiv:2407.15610v1 Announce Type: new 
Abstract: This study investigates how to define and measure inclusivity in Italy's early childhood education and care (ECEC) services, bringing to light the gap between legislative principles and local/regional applications. The Italian legislative decree n. 65/2017 prescribes inclusivity in ECEC, defined as being open to all children and indicating it as a top priority. To delve into this concept, we propose a two-step model. First, a latent trait model estimates an inclusivity index as a latent variable. Then, a mixed quantile model examines the distribution of this novel latent inclusivity index across Italian regions. Our findings reveal a substantial variation in inclusivity across Italy. In addition, a proper indicator based on the latent inclusivity index defined in the first step is provided at the NUTS-3 level using the empirical best predictor approach. From our analysis, public facilities demonstrate a higher level of inclusivity compared to their private counterparts. Despite these challenges, we are compelled to identify positive scenarios that can serve as models for regions facing more critical situations. Besides its methodological advancement, this paper provides policymakers and stakeholders with an evident call to action, offering valuable insights into the inclusivity landscape of Italian ECEC services. It underscores the urgent need to standardize the accessibility characteristics of ECEC services throughout Italy to ensure equitable access for all children.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15610v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angela Andreella, Gaia Bertarelli, Federico Caldura, Stefano Campostrini</dc:creator>
    </item>
    <item>
      <title>Small but not least changes: The Art of Creating Disruptive Innovations</title>
      <link>https://arxiv.org/abs/2407.14537</link>
      <description>arXiv:2407.14537v1 Announce Type: cross 
Abstract: In the ever-evolving landscape of technology, product innovation thrives on replacing outdated technologies with groundbreaking ones or through the ingenious recombination of existing technologies. Our study embarks on a revolutionary journey by genetically representing products, extracting their chromosomal data, and constructing a comprehensive phylogenetic network of automobiles. We delve deep into the technological features that shape innovation, pinpointing the ancestral roots of products and mapping out intricate product-family triangles. By leveraging the similarities within these triangles, we introduce a pioneering "Product Disruption Index"-inspired by the CD index (Funk and Owen-Smith, 2017)-to quantify a product's disruptiveness. Our approach is rigorously validated against the scientifically recognized trend of decreasing disruptiveness over time (Park et al., 2023) and through compelling case studies. Our statistical analysis reveals a fascinating insight: disruptive product innovations often stem from minor, yet crucial, modifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14537v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youwei He, Jeong-Dong Lee</dc:creator>
    </item>
    <item>
      <title>Identification of changes in gene expression</title>
      <link>https://arxiv.org/abs/2407.14630</link>
      <description>arXiv:2407.14630v1 Announce Type: cross 
Abstract: Evaluating the change in gene expression is a common goal in many research areas, such as in toxicological studies as well as in clinical trials. In practice, the analysis is often based on multiple t-tests evaluated at the observed time points. This severely limits the accuracy of determining the time points at which the gene changes in expression. Even if a parametric approach is chosen, the analysis is often restricted to identifying the onset of an effect. In this paper, we propose a parametric method to identify the time frame where the gene expression significantly changes. This is achieved by fitting a parametric model to the time-response data and constructing a confidence band for its first derivative. The confidence band is derived by a flexible two step bootstrap approach, which can be applied to a wide variety of possible curves. Our method focuses on the first derivative, since it provides an easy to compute and reliable measure for the change in response. It is summarised in terms of a hypothesis test, such that rejecting the null hypothesis means detecting a significant change in gene expression. Furthermore, a method for calculating confidence intervals for time points of interest (e.g. the beginning and end of significant change) is developed. We demonstrate the validity of our approach through a simulation study and present a variety of different applications to mouse gene expression data from a study investigating the effect of a Western diet on the progression of non-alcoholic fatty liver disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14630v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucia Ameis, Kathrin M\"ollenhoff</dc:creator>
    </item>
    <item>
      <title>A Bayesian workflow for securitizing casualty insurance risk</title>
      <link>https://arxiv.org/abs/2407.14666</link>
      <description>arXiv:2407.14666v1 Announce Type: cross 
Abstract: Casualty insurance-linked securities (ILS) are appealing to investors because the underlying insurance claims, which are directly related to resulting security performance, are uncorrelated with most other asset classes. Conversely, casualty ILS are appealing to insurers as an efficient capital managment tool. However, securitizing casualty insurance risk is non-trivial, as it requires forecasting loss ratios for pools of insurance policies that have not yet been written, in addition to estimating how the underlying losses will develop over time within future accident years. In this paper, we lay out a Bayesian workflow that tackles these complexities by using: (1) theoretically informed time-series and state-space models to capture how loss ratios develop and change over time; (2) historic industry data to inform prior distributions of models fit to individual programs; (3) stacking to combine loss ratio predictions from candidate models, and (4) both prior predictive simulations and simulation-based calibration to aid model specification. Using historic Schedule P filings, we then show how our proposed Bayesian workflow can be used to assess and compare models across a variety of key model performance metrics evaluated on future accident year losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14666v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Haines, Conor Goold, J. Mark Shoun</dc:creator>
    </item>
    <item>
      <title>Addressing Data Heterogeneity in Federated Learning of Cox Proportional Hazards Models</title>
      <link>https://arxiv.org/abs/2407.14960</link>
      <description>arXiv:2407.14960v1 Announce Type: cross 
Abstract: The diversity in disease profiles and therapeutic approaches between hospitals and health professionals underscores the need for patient-centric personalized strategies in healthcare. Alongside this, similarities in disease progression across patients can be utilized to improve prediction models in survival analysis. The need for patient privacy and the utility of prediction models can be simultaneously addressed in the framework of Federated Learning (FL). This paper outlines an approach in the domain of federated survival analysis, specifically the Cox Proportional Hazards (CoxPH) model, with a specific focus on mitigating data heterogeneity and elevating model performance. We present an FL approach that employs feature-based clustering to enhance model accuracy across synthetic datasets and real-world applications, including the Surveillance, Epidemiology, and End Results (SEER) database. Furthermore, we consider an event-based reporting strategy that provides a dynamic approach to model adaptation by responding to local data changes. Our experiments show the efficacy of our approach and discuss future directions for a practical application of FL in healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14960v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Navid Seidi, Satyaki Roy, Sajal K. Das, Ardhendu Tripathy</dc:creator>
    </item>
    <item>
      <title>Statistical Models for Outbreak Detection of Measles in North Cotabato, Philippines</title>
      <link>https://arxiv.org/abs/2407.15028</link>
      <description>arXiv:2407.15028v1 Announce Type: cross 
Abstract: A measles outbreak occurs when the number of cases of measles in the population exceeds the typical level. Outbreaks that are not detected and managed early can increase mortality and morbidity and incur costs from activities responding to these events. The number of measles cases in the Province of North Cotabato, Philippines, was used in this study. Weekly reported cases of measles from January 2016 to December 2021 were provided by the Epidemiology and Surveillance Unit of the North Cotabato Provincial Health Office. Several integer-valued autoregressive (INAR) time series models were used to explore the possibility of detecting and identifying measles outbreaks in the province along with the classical ARIMA model. These models were evaluated based on goodness of fit, measles outbreak detection accuracy, and timeliness. The results of this study confirmed that INAR models have the conceptual advantage over ARIMA since the latter produces non-integer forecasts, which are not realistic for count data such as measles cases. Among the INAR models, the ZINGINAR (1) model was recommended for having a good model fit and timely and accurate detection of outbreaks. Furthermore, policymakers and decision-makers from relevant government agencies can use the ZINGINAR (1) model to improve disease surveillance and implement preventive measures against contagious diseases beforehand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15028v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Mindanao Journal of Science and Technology, 22(1) (2024)</arxiv:journal_reference>
      <dc:creator>Julienne Kate N. Kintanar, Roel F. Ceballos</dc:creator>
    </item>
    <item>
      <title>High-dimensional log contrast models with measurement errors</title>
      <link>https://arxiv.org/abs/2407.15084</link>
      <description>arXiv:2407.15084v1 Announce Type: cross 
Abstract: High-dimensional compositional data are frequently encountered in many fields of modern scientific research. In regression analysis of compositional data, the presence of covariate measurement errors poses grand challenges for existing statistical error-in-variable regression analysis methods since measurement error in one component of the composition has an impact on others. To simultaneously address the compositional nature and measurement errors in the high-dimensional design matrix of compositional covariates, we propose a new method named Error-in-composition (Eric) Lasso for regression analysis of corrupted compositional predictors. Estimation error bounds of Eric Lasso and its asymptotic sign-consistent selection properties are established. We then illustrate the finite sample performance of Eric Lasso using simulation studies and demonstrate its potential usefulness in a real data application example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15084v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenxi Tan, Lingzhou Xue, Songshan Yang, Xiang Zhan</dc:creator>
    </item>
    <item>
      <title>Replicable Bandits for Digital Health Interventions</title>
      <link>https://arxiv.org/abs/2407.15377</link>
      <description>arXiv:2407.15377v1 Announce Type: cross 
Abstract: Adaptive treatment assignment algorithms, such as bandit and reinforcement learning algorithms, are increasingly used in digital health intervention clinical trials. Causal inference and related data analyses are critical for evaluating digital health interventions, deciding how to refine the intervention, and deciding whether to roll-out the intervention more broadly. However the replicability of these analyses has received relatively little attention. This work investigates the replicability of statistical analyses from trials deploying adaptive treatment assignment algorithms. We demonstrate that many standard statistical estimators can be inconsistent and fail to be replicable across repetitions of the clinical trial, even as the sample size grows large. We show that this non-replicability is intimately related to properties of the adaptive algorithm itself. We introduce a formal definition of a "replicable bandit algorithm" and prove that under such algorithms, a wide variety of common statistical analyses are guaranteed to be consistent. We present both theoretical results and simulation studies based on a mobile health oral health self-care intervention. Our findings underscore the importance of designing adaptive algorithms with replicability in mind, especially for settings like digital health where deployment decisions rely heavily on replicated evidence. We conclude by discussing open questions on the connections between algorithm design, statistical inference, and experimental replicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15377v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kelly W. Zhang, Nowell Closser, Anna L. Trella, Susan A. Murphy</dc:creator>
    </item>
    <item>
      <title>Degree distributions in networks: beyond the power law</title>
      <link>https://arxiv.org/abs/2008.03073</link>
      <description>arXiv:2008.03073v5 Announce Type: replace 
Abstract: The power law is useful in describing count phenomena such as network degrees and word frequencies. With a single parameter, it captures the main feature that the frequencies are linear on the log-log scale. Nevertheless, there have been criticisms of the power law, for example that a threshold needs to be pre-selected without its uncertainty quantified, that the power law is simply inadequate, and that subsequent hypothesis tests are required to determine whether the data could have come from the power law. We propose a modelling framework that combines two different generalisations of the power law, namely the generalised Pareto distribution and the Zipf-polylog distribution, to resolve these issues. The proposed mixture distributions are shown to fit the data well and quantify the threshold uncertainty in a natural way. A model selection step embedded in the Bayesian inference algorithm further answers the question whether the power law is adequate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2008.03073v5</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Clement Lee, Emma Eastoe, Aiden Farrell</dc:creator>
    </item>
    <item>
      <title>Joint Modeling of Biomarker Cascades Along An Unobserved Disease Progression with Differentiate Covariate Effects: An Application in Alzheimer's Disease</title>
      <link>https://arxiv.org/abs/2304.09754</link>
      <description>arXiv:2304.09754v2 Announce Type: replace 
Abstract: Alzheimer's Disease (AD) research has shifted to focus on biomarker trajectories and their potential use in understanding the underlying AD-related pathological process. A conceptual framework was proposed in modern AD research that hypothesized biomarker cascades as a result of underlying AD pathology. In this paper, we leveraged this idea to jointly model AD biomarker trajectories as a function of the latent AD disease progression with individual and covariate effects in the latent disease progression model and the biomarker cascade. We tailored our methods to address a number of real-data challenges that are often present in AD studies. Simulation studies were performed to investigate the proposed approach under various realistic but less-than-ideal situations. Finally, we illustrated the methods using real data from the BIOCARD and the ADNI studies. The analyses investigated cascading patterns of AD biomarkers in these datasets and presented prediction results for individual-level profiles over time. These findings highlight the potential of the conceptual biomarker cascade framework to be leveraged for diagnosis and monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09754v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuojun Tang, Yuxin Zhu, Kexin Zhang, Zheyu Wang</dc:creator>
    </item>
    <item>
      <title>Credibility Theory Based on Winsorizing</title>
      <link>https://arxiv.org/abs/2306.09507</link>
      <description>arXiv:2306.09507v4 Announce Type: replace 
Abstract: The classical B\"{u}hlmann credibility model has been widely applied to premium estimation for group insurance contracts and other insurance types. In this paper, we develop a robust B\"{u}hlmann credibility model using the winsorized version of loss data, also known as the winsorized mean (a robust alternative to the traditional individual mean). This approach assumes that the observed sample data come from a contaminated underlying model with a small percentage of contaminated sample data. This framework provides explicit formulas for the structural parameters in credibility estimation for scale-shape distribution families, location-scale distribution families, and their variants, commonly used in insurance risk modeling. Using the theory of \(L\)-estimators (different from the influence function approach), we derive the asymptotic properties of the proposed method and validate them through a comprehensive simulation study, comparing their performance to credibility based on the trimmed mean. By varying the winsorizing/trimming thresholds in several parametric models, we find that all structural parameters derived from the winsorized approach are less volatile than those from the trimmed approach. Using the winsorized mean as a robust risk measure can reduce the influence of parametric loss assumptions on credibility estimation. Additionally, we discuss non-parametric estimations in credibility. Finally, a numerical illustration from the Wisconsin Local Government Property Insurance Fund indicates that the proposed robust credibility approach mitigates the impact of model mis-specification and captures the risk behavior of loss data from a broader perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.09507v4</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s13385-024-00391-7</arxiv:DOI>
      <arxiv:journal_reference>European Actuarial Journal, 2024</arxiv:journal_reference>
      <dc:creator>Qian Zhao, Chudamani Poudyal</dc:creator>
    </item>
    <item>
      <title>Online Optimization and Ambiguity-based Learning of Distributionally Uncertain Dynamic Systems</title>
      <link>https://arxiv.org/abs/2102.09111</link>
      <description>arXiv:2102.09111v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel approach to construct data-driven online solutions to optimization problems (P) subject to a class of distributionally uncertain dynamical systems. The introduced framework allows for the simultaneous learning of distributional system uncertainty via a parameterized, control-dependent ambiguity set using a finite historical data set, and its use to make online decisions with probabilistic regret function bounds. Leveraging the merits of Machine Learning, the main technical approach relies on the theory of Distributional Robust Optimization (DRO), to hedge against uncertainty and provide less conservative results than standard Robust Optimization approaches. Starting from recent results that describe ambiguity sets via parameterized, and control-dependent empirical distributions as well as ambiguity radii, we first present a tractable reformulation of the corresponding optimization problem while maintaining the probabilistic guarantees. We then specialize these problems to the cases of 1) optimal one-stage control of distributionally uncertain nonlinear systems, and 2) resource allocation under distributional uncertainty. A novelty of this work is that it extends DRO to online optimization problems subject to a distributionally uncertain dynamical system constraint, handled via a control-dependent ambiguity set that leads to online-tractable optimization with probabilistic guarantees on regret bounds. Further, we introduce an online version of Nesterov's accelerated-gradient algorithm, and analyze its performance to solve this class of problems via dissipativity theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.09111v2</guid>
      <category>eess.SY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TAC.2024.3396378</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Automatic Control 2024</arxiv:journal_reference>
      <dc:creator>Dan Li, Dariush Fooladivanda, Sonia Martinez</dc:creator>
    </item>
    <item>
      <title>The SPDE approach for spatio-temporal datasets with advection and diffusion</title>
      <link>https://arxiv.org/abs/2208.14015</link>
      <description>arXiv:2208.14015v4 Announce Type: replace-cross 
Abstract: In the task of predicting spatio-temporal fields in environmental science using statistical methods, introducing statistical models inspired by the physics of the underlying phenomena that are numerically efficient is of growing interest. Large space-time datasets call for new numerical methods to efficiently process them. The Stochastic Partial Differential Equation (SPDE) approach has proven to be effective for the estimation and the prediction in a spatial context. We present here the advection-diffusion SPDE with first order derivative in time which defines a large class of nonseparable spatio-temporal models. A Gaussian Markov random field approximation of the solution to the SPDE is built by discretizing the temporal derivative with a finite difference method (implicit Euler) and by solving the spatial SPDE with a finite element method (continuous Galerkin) at each time step. The ''Streamline Diffusion'' stabilization technique is introduced when the advection term dominates the diffusion. Computationally efficient methods are proposed to estimate the parameters of the SPDE and to predict the spatio-temporal field by kriging, as well as to perform conditional simulations. The approach is applied to a solar radiation dataset. Its advantages and limitations are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.14015v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucia Clarotto (MIA Paris-Saclay), Denis Allard (BioSP), Thomas Romary (GEOSCIENCES), Nicolas Desassis (GEOSCIENCES)</dc:creator>
    </item>
    <item>
      <title>Priming bias versus post-treatment bias in experimental designs</title>
      <link>https://arxiv.org/abs/2306.01211</link>
      <description>arXiv:2306.01211v4 Announce Type: replace-cross 
Abstract: Conditioning on variables affected by treatment can induce post-treatment bias when estimating causal effects. Although this suggests that researchers should measure potential moderators before administering the treatment in an experiment, doing so may also bias causal effect estimation if the covariate measurement primes respondents to react differently to the treatment. This paper formally analyzes this trade-off between post-treatment and priming biases in three experimental designs that vary when moderators are measured: pre-treatment, post-treatment, or a randomized choice between the two. We derive nonparametric bounds for interactions between the treatment and the moderator under each design and show how to use substantive assumptions to narrow these bounds. These bounds allow researchers to assess the sensitivity of their empirical findings to either source of bias. We then apply the proposed methodology to a survey experiment on electoral messaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01211v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Blackwell, Jacob R. Brown, Sophie Hill, Kosuke Imai, Teppei Yamamoto</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian functional principal component analysis of irregularly-observed multivariate curves</title>
      <link>https://arxiv.org/abs/2311.05200</link>
      <description>arXiv:2311.05200v2 Announce Type: replace-cross 
Abstract: The analysis of multivariate functional curves has the potential to yield important scientific discoveries in domains such as healthcare, medicine, economics and social sciences. However, it is common for real-world settings to present longitudinal data that are both irregularly and sparsely observed, which introduces important challenges for the current functional data methodology. A Bayesian hierarchical framework for multivariate functional principal component analysis is proposed, which accommodates the intricacies of such irregular observation settings by flexibly pooling information across subjects and correlated curves. The model represents common latent dynamics via shared functional principal component scores, thereby effectively borrowing strength across curves while circumventing the computationally challenging task of estimating covariance matrices. These scores also provide a parsimonious representation of the major modes of joint variation of the curves and constitute interpretable scalar summaries that can be employed in follow-up analyses. Estimation is carried out using variational inference, which combines efficiency, modularity and approximate posterior density estimation, enabling the joint analysis of large datasets with parameter uncertainty quantification. Detailed simulations assess the effectiveness of the approach in sharing information from sparse and irregularly sampled multivariate curves. The methodology is also exploited to estimate the molecular disease courses of individual patients with SARS-CoV-2 infection and characterise patient heterogeneity in recovery outcomes; this study reveals key coordinated dynamics across the immune, inflammatory and metabolic systems, which are associated with survival and long-COVID symptoms up to one year post disease onset. The approach is implemented in the R package bayesFPCA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05200v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tui Nolan, Sylvia Richardson, H\'el\`ene Ruffieux</dc:creator>
    </item>
    <item>
      <title>ChatGPT and post-test probability</title>
      <link>https://arxiv.org/abs/2311.12188</link>
      <description>arXiv:2311.12188v5 Announce Type: replace-cross 
Abstract: Reinforcement learning-based large language models, such as ChatGPT, are believed to have potential to aid human experts in many domains, including healthcare. There is, however, little work on ChatGPT's ability to perform a key task in healthcare: formal, probabilistic medical diagnostic reasoning. This type of reasoning is used, for example, to update a pre-test probability to a post-test probability. In this work, we probe ChatGPT's ability to perform this task. In particular, we ask ChatGPT to give examples of how to use Bayes rule for medical diagnosis. Our prompts range from queries that use terminology from pure probability (e.g., requests for a posterior of A given B and C) to queries that use terminology from medical diagnosis (e.g., requests for a posterior probability of Covid given a test result and cough). We show how the introduction of medical variable names leads to an increase in the number of errors that ChatGPT makes. Given our results, we also show how one can use prompt engineering to facilitate ChatGPT's partial avoidance of these errors. We discuss our results in light of recent commentaries on sensitivity and specificity. We also discuss how our results might inform new research directions for large language models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12188v5</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel J. Weisenthal</dc:creator>
    </item>
    <item>
      <title>On the Asymptotic Normality of Trimmed and Winsorized L-statistics</title>
      <link>https://arxiv.org/abs/2402.07406</link>
      <description>arXiv:2402.07406v3 Announce Type: replace-cross 
Abstract: There are several ways to establish the asymptotic normality of $L$-statistics, which depend on the choice of the weights-generating function and the cumulative distribution selection of the underlying model. In this study, we focus on stablishing computational formulas for the asymptotic variance of two robust $L$-estimators: the method of trimmed moments (MTM) and the method of winsorized moments (MWM). We demonstrate that two asymptotic approaches for MTM are equivalent for a specific choice of the weights-generating function. These findings enhance the applicability of these estimators across various underlying distributions, making them effective tools in diverse statistical scenarios. Such scenarios include actuarial contexts, such as payment-per-payment and payment-per-loss data scenarios, as well as in evaluating the asymptotic distributional properties of distortion risk measures. The effectiveness of our methodologies depends on the availability of the cumulative distribution function, ensuring broad usability in various statistical environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07406v3</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal</dc:creator>
    </item>
    <item>
      <title>Resolution of Simpson's paradox via the common cause principle</title>
      <link>https://arxiv.org/abs/2403.00957</link>
      <description>arXiv:2403.00957v2 Announce Type: replace-cross 
Abstract: Simpson's paradox is an obstacle to establishing a probabilistic association between two events $a_1$ and $a_2$, given the third (lurking) random variable $B$. We focus on scenarios when the random variables $A$ (which combines $a_1$, $a_2$, and their complements) and $B$ have a common cause $C$ that need not be observed. Alternatively, we can assume that $C$ screens out $A$ from $B$. For such cases, the correct association between $a_1$ and $a_2$ is to be defined via conditioning over $C$. This setup generalizes the original Simpson's paradox: now its two contradicting options refer to two particular and different causes $C$. We show that if $B$ and $C$ are binary and $A$ is quaternary (the minimal and the most widespread situation for the Simpson's paradox), the conditioning over any binary common cause $C$ establishes the same direction of association between $a_1$ and $a_2$ as the conditioning over $B$ in the original formulation of the paradox. Thus, for the minimal common cause, one should choose the option of Simpson's paradox that assumes conditioning over $B$ and not its marginalization. The same conclusion is reached when Simpson's paradox is formulated via 3 continuous Gaussian variables: within the minimal formulation of the paradox (3 scalar continuous variables $A_1$, $A_2$, and $B$), one should choose the option with the conditioning over $B$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00957v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>math.PR</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Hovhannisyan, A. E. Allahverdyan</dc:creator>
    </item>
    <item>
      <title>Emotion Detection with Transformers: A Comparative Study</title>
      <link>https://arxiv.org/abs/2403.15454</link>
      <description>arXiv:2403.15454v3 Announce Type: replace-cross 
Abstract: In this study, we explore the application of transformer-based models for emotion classification on text data. We train and evaluate several pre-trained transformer models, on the Emotion dataset using different variants of transformers. The paper also analyzes some factors that in-fluence the performance of the model, such as the fine-tuning of the transformer layer, the trainability of the layer, and the preprocessing of the text data. Our analysis reveals that commonly applied techniques like removing punctuation and stop words can hinder model performance. This might be because transformers strength lies in understanding contextual relationships within text. Elements like punctuation and stop words can still convey sentiment or emphasis and removing them might disrupt this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15454v3</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mahdi Rezapour</dc:creator>
    </item>
    <item>
      <title>LLM4ED: Large Language Models for Automatic Equation Discovery</title>
      <link>https://arxiv.org/abs/2405.07761</link>
      <description>arXiv:2405.07761v2 Announce Type: replace-cross 
Abstract: Equation discovery is aimed at directly extracting physical laws from data and has emerged as a pivotal research domain. Previous methods based on symbolic mathematics have achieved substantial advancements, but often require the design of implementation of complex algorithms. In this paper, we introduce a new framework that utilizes natural language-based prompts to guide large language models (LLMs) in automatically mining governing equations from data. Specifically, we first utilize the generation capability of LLMs to generate diverse equations in string form, and then evaluate the generated equations based on observations. In the optimization phase, we propose two alternately iterated strategies to optimize generated equations collaboratively. The first strategy is to take LLMs as a black-box optimizer and achieve equation self-improvement based on historical samples and their performance. The second strategy is to instruct LLMs to perform evolutionary operators for global search. Experiments are extensively conducted on both partial differential equations and ordinary differential equations. Results demonstrate that our framework can discover effective equations to reveal the underlying physical laws under various nonlinear dynamic systems. Further comparisons are made with state-of-the-art models, demonstrating good stability and usability. Our framework substantially lowers the barriers to learning and applying equation discovery techniques, demonstrating the application potential of LLMs in the field of knowledge discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07761v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengge Du, Yuntian Chen, Zhongzheng Wang, Longfeng Nie, Dongxiao Zhang</dc:creator>
    </item>
    <item>
      <title>Tackling GenAI Copyright Issues: Originality Estimation and Genericization</title>
      <link>https://arxiv.org/abs/2406.03341</link>
      <description>arXiv:2406.03341v2 Announce Type: replace-cross 
Abstract: The rapid progress of generative AI technology has sparked significant copyright concerns, leading to numerous lawsuits filed against AI developers. While some studies explore methods to mitigate copyright risks by steering the outputs of generative models away from those resembling copyrighted data, little attention has been paid to the question of how much of a resemblance is undesirable; more original or unique data are afforded stronger protection, and the threshold level of resemblance for constituting infringement correspondingly lower. Here, leveraging this principle, we propose a genericization method that modifies the outputs of a generative model to make them more generic and less likely to infringe copyright. To achieve this, we introduce a metric for quantifying the level of originality of data in a manner that is consistent with the legal framework. This metric can be practically estimated by drawing samples from a generative model, which is then used for the genericization process. Experiments demonstrate that our genericization method successfully modifies the output of a text-to-image generative model so that it produces more generic, copyright-compliant images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03341v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroaki Chiba-Okabe, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Quantifying distribution system resilience from utility data: large event risk and benefits of investments</title>
      <link>https://arxiv.org/abs/2407.10773</link>
      <description>arXiv:2407.10773v2 Announce Type: replace-cross 
Abstract: We focus on large blackouts in electric distribution systems caused by extreme winds. Such events have a large cost and impact on customers. To quantify resilience to these events, we formulate large event risk and show how to calculate it from the historical outage data routinely collected by utilities' outage management systems. Risk is defined using an event cost exceedance curve. The tail of this curve and the large event risk is described by the probability of a large cost event and the slope magnitude of the tail on a log-log plot. Resilience can be improved by planned investments to upgrade system components or speed up restoration. The benefits that these investments would have had if they had been made in the past can be quantified by "rerunning history" with the effects of the investment included, and then recalculating the large event risk to find the improvement in resilience. An example using utility data shows a 12% and 22% reduction in the probability of a large cost event due to 10% wind hardening and 10% faster restoration respectively. This new data-driven approach to quantify resilience and resilience investments is realistic and much easier to apply than complicated approaches based on modeling all the phases of resilience. Moreover, an appeal to improvements to past lived experience may well be persuasive to customers and regulators in making the case for resilience investments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10773v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arslan Ahmad, Ian Dobson</dc:creator>
    </item>
  </channel>
</rss>
