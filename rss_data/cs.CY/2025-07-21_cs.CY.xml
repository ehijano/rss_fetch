<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Jul 2025 02:22:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Stated Protocol: A Decentralized Framework for Digital Diplomacy</title>
      <link>https://arxiv.org/abs/2507.13517</link>
      <description>arXiv:2507.13517v1 Announce Type: new 
Abstract: International coordination faces significant friction due to reliance on periodic summits, bilateral consultations, and fragmented communication channels that impede rapid collective responses to emerging global challenges while limiting transparency to constituents. We present the Stated Protocol, a decentralized framework that enables organizations to coordinate through standardized text statements published on their website domains. While applicable to all organizations, this work focuses primarily on the application in international relations, where the protocol enables rapid consensus discovery and collective decision-making without relying on centralized social media platforms. We explore specific applications: (1) faster treaty negotiation through incremental micro-agreements that can be signed digitally within hours rather than months, (2) continuous and transparent operation of international institutions through asynchronous decision-making, (3) coordinated signaling from local governments to national authorities through simultaneous statement publication, and (4) coalition formation among non-governmental organizations through transparent position aggregation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13517v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher J. P. Rieckmann</dc:creator>
    </item>
    <item>
      <title>The Emperor's New Chain-of-Thought: Probing Reasoning Theater Bias in Large Reasoning Models</title>
      <link>https://arxiv.org/abs/2507.13758</link>
      <description>arXiv:2507.13758v1 Announce Type: new 
Abstract: Large Reasoning Models (LRMs) like DeepSeek-R1 and o1 are increasingly used as automated evaluators, raising critical questions about their vulnerability to the aesthetics of reasoning in LLM-as-a-judge settings. We introduce THEATER, a comprehensive benchmark to systematically evaluate this vulnerability-termed Reasoning Theater Bias (RTB)-by comparing LLMs and LRMs across subjective preference and objective factual datasets. Through investigation of six bias types including Simple Cues and Fake Chain-of-Thought, we uncover three key findings: (1) in a critical paradox, reasoning-specialized LRMs are consistently more susceptible to RTB than general-purpose LLMs, particularly in subjective tasks; (2) this creates a task-dependent trade-off, where LRMs show more robustness on factual tasks but less on subjective ones; and (3) we identify 'shallow reasoning'-plausible but flawed arguments-as the most potent form of RTB. To address this, we design and evaluate two prompting strategies: a targeted system prompt that improves accuracy by up to 12% on factual tasks but only 1-3% on subjective tasks, and a self-reflection mechanism that shows similarly limited effectiveness in the more vulnerable subjective domains. Our work reveals that RTB is a deep-seated challenge for LRM-based evaluation and provides a systematic framework for developing more genuinely robust and trustworthy LRMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13758v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qian Wang, Yubo Fan, Zhenheng Tang, Nuo Chen, Wenxuan Wang, Bingsheng He</dc:creator>
    </item>
    <item>
      <title>Food safety trends across Europe: insights from the 392-million-entry CompreHensive European Food Safety (CHEFS) database</title>
      <link>https://arxiv.org/abs/2507.13802</link>
      <description>arXiv:2507.13802v1 Announce Type: new 
Abstract: In the European Union, official food safety monitoring data collected by member states are submitted to the European Food Safety Authority (EFSA) and published on Zenodo. This data includes 392 million analytical results derived from over 15.2 million samples covering more than 4,000 different types of food products, offering great opportunities for artificial intelligence to analyze trends, predict hazards, and support early warning systems. However, the current format with data distributed across approximately 1000 files totaling several hundred gigabytes hinders accessibility and analysis. To address this, we introduce the CompreHensive European Food Safety (CHEFS) database, which consolidates EFSA monitoring data on pesticide residues, veterinary medicinal product residues, and chemical contaminants into a unified and structured dataset. We describe the creation and structure of the CHEFS database and demonstrate its potential by analyzing trends in European food safety monitoring data from 2000 to 2024. Our analyses explore changes in monitoring activities, the most frequently tested products, which products were most often non-compliant and which contaminants were most often found, and differences across countries. These findings highlight the CHEFS database as both a centralized data source and a strategic tool for guiding food safety policy, research, and regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13802v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nehir Kizililsoley, Floor van Meer, Osman Mutlu, Wouter F Hoenderdaal, Rosan G. Hob\'e, Wenjuan Mu, Arjen Gerssen, H. J. van der Fels-Klerx, \'Akos J\'o\'zwiak, Ioannis Manikas, Ali H\"urriyeto\v{g}lu, Bas H. M. van der Velden</dc:creator>
    </item>
    <item>
      <title>Principles and Reasons Behind Automated Vehicle Decisions in Ethically Ambiguous Everyday Scenarios</title>
      <link>https://arxiv.org/abs/2507.13837</link>
      <description>arXiv:2507.13837v1 Announce Type: new 
Abstract: Automated vehicles (AVs) increasingly encounter ethically ambiguous situations in everyday driving--scenarios involving conflicting human interests and lacking clearly optimal courses of action. While existing ethical models often focus on rare, high-stakes dilemmas (e.g., crash avoidance or trolley problems), routine decisions such as overtaking cyclists or navigating social interactions remain underexplored. This study addresses that gap by applying the tracking condition of Meaningful Human Control (MHC), which holds that AV behaviour should align with human reasons--defined as the values, intentions, and expectations that justify actions. We conducted qualitative interviews with 18 AV experts to identify the types of reasons that should inform AV manoeuvre planning. Thirteen categories of reasons emerged, organised across normative, strategic, tactical, and operational levels, and linked to the roles of relevant human agents. A case study on cyclist overtaking illustrates how these reasons interact in context, revealing a consistent prioritisation of safety, contextual flexibility regarding regulatory compliance, and nuanced trade-offs involving efficiency, comfort, and public acceptance. Based on these insights, we propose a principled conceptual framework for AV decision-making in routine, ethically ambiguous scenarios. The framework supports dynamic, human-aligned behaviour by prioritising safety, allowing pragmatic actions when strict legal adherence would undermine key values, and enabling constrained deviations when appropriately justified. This empirically grounded approach advances current guidance by offering actionable, context-sensitive design principles for ethically aligned AV systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13837v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Elbert Suryana, Simeon Calvert, Arkady Zgonnikov, Bart van Arem</dc:creator>
    </item>
    <item>
      <title>Extracting Insights from Large-Scale Telematics Data for ITS Applications: Lessons and Recommendations</title>
      <link>https://arxiv.org/abs/2507.13936</link>
      <description>arXiv:2507.13936v1 Announce Type: new 
Abstract: Over 90% of new vehicles in the United States now collect and transmit telematics data. Similar trends are seen in other developed countries. Transportation planners have previously utilized telematics data in various forms, but its current scale offers significant new opportunities in traffic measurement, classification, planning, and control. Despite these opportunities, the enormous volume of data and lack of standardization across manufacturers necessitates a clearer understanding of the data and improved data processing methods for extracting actionable insights.
  This paper takes a step towards addressing these needs through four primary objectives. First, a data processing pipeline was built to efficiently analyze 1.4 billion miles (120 million trips) of telematics data collected in Virginia between August 2021 and August 2022. Second, an open data repository of trip and roadway segment level summaries was created. Third, interactive visualization tools were designed to extract insights from these data about trip-taking behavior and the speed profiles of roadways. Finally, major challenges that were faced during processing this data are summarized and recommendations to overcome them are provided. This work will help manufacturers collecting the data and transportation professionals using the data to develop a better understanding of the possibilities and major pitfalls to avoid.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13936v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gibran Ali, Neal Feierabend, Prarthana Doshi, Calvin Winkowski, Michael Fontaine</dc:creator>
    </item>
    <item>
      <title>Patterns, Models, and Challenges in Online Social Media: A Survey</title>
      <link>https://arxiv.org/abs/2507.13379</link>
      <description>arXiv:2507.13379v1 Announce Type: cross 
Abstract: The rise of digital platforms has enabled the large scale observation of individual and collective behavior through high resolution interaction data. This development has opened new analytical pathways for investigating how information circulates, how opinions evolve, and how coordination emerges in online environments. Yet despite a growing body of research, the field remains fragmented and marked by methodological heterogeneity, limited model validation, and weak integration across domains. This survey offers a systematic synthesis of empirical findings and formal models. We examine platform-level regularities, assess the methodological architectures that generate them, and evaluate the extent to which current modeling frameworks account for observed dynamics. The goal is to consolidate a shared empirical baseline and clarify the structural constraints that shape inference in this domain, laying the groundwork for more robust, comparable, and actionable analyses of online social systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13379v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niccol\`o Di Marco, Anita Bonetti, Edoardo Di Martino, Edoardo Loru, Jacopo Nudo, Mario Edoardo Pandolfo, Giulio Pecile, Emanuele Sangiorgio, Irene Scalco, Simon Zollo, Matteo Cinelli, Fabiana Zollo, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>Quantitative Risk Management in Volatile Markets with an Expectile-Based Framework for the FTSE Index</title>
      <link>https://arxiv.org/abs/2507.13391</link>
      <description>arXiv:2507.13391v1 Announce Type: cross 
Abstract: This research presents a framework for quantitative risk management in volatile markets, specifically focusing on expectile-based methodologies applied to the FTSE 100 index. Traditional risk measures such as Value-at-Risk (VaR) have demonstrated significant limitations during periods of market stress, as evidenced during the 2008 financial crisis and subsequent volatile periods. This study develops an advanced expectile-based framework that addresses the shortcomings of conventional quantile-based approaches by providing greater sensitivity to tail losses and improved stability in extreme market conditions. The research employs a dataset spanning two decades of FTSE 100 returns, incorporating periods of high volatility, market crashes, and recovery phases. Our methodology introduces novel mathematical formulations for expectile regression models, enhanced threshold determination techniques using time series analysis, and robust backtesting procedures. The empirical results demonstrate that expectile-based Value-at-Risk (EVaR) consistently outperforms traditional VaR measures across various confidence levels and market conditions. The framework exhibits superior performance during volatile periods, with reduced model risk and enhanced predictive accuracy. Furthermore, the study establishes practical implementation guidelines for financial institutions and provides evidence-based recommendations for regulatory compliance and portfolio management. The findings contribute significantly to the literature on financial risk management and offer practical tools for practitioners dealing with volatile market environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13391v1</guid>
      <category>q-fin.RM</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abiodun Finbarrs Oketunji</dc:creator>
    </item>
    <item>
      <title>Socio-Technical Smell Dynamics in Code Samples: A Multivocal Review on Emergence, Evolution, and Co-Occurrence</title>
      <link>https://arxiv.org/abs/2507.13481</link>
      <description>arXiv:2507.13481v1 Announce Type: cross 
Abstract: Code samples play a pivotal role in open-source ecosystems (OSSECO), serving as lightweight artifacts that support knowledge transfer, onboarding, and framework adoption. Despite their instructional relevance, these samples are often governed informally, with minimal review and unclear ownership, which increases their exposure to socio-technical degradation. In this context, the co-occurrence and longitudinal interplay of code smells (e.g., large classes, poor modularity) and community smells (e.g., lone contributors, fragmented communication) become particularly critical. While each type of smell has been studied in isolation, little is known about how community-level dysfunctions anticipate or exacerbate technical anomalies in code samples over time. This study investigates how code and community smells emerge, co-occur, and evolve within code samples maintained in OSSECOs. A Multivocal Literature Review protocol was applied, encompassing 30 peer-reviewed papers and 17 practitioner-oriented sources (2013-2024). Thematic synthesis was conducted to identify recurring socio-technical patterns related to smell dynamics. Nine patterns were identified, showing that community smells often precede or reinforce technical degradation in code samples. Symptoms such as "radio silence" and centralized ownership were frequently associated with persistent structural anomalies. Additionally, limited onboarding, the absence of continuous refactoring, and informal collaboration emerged as recurring conditions for smell accumulation. Conclusion: In OSSECOs, particularly within code samples, community-level dysfunctions not only correlate with but often signal maintainability decay. These findings underscore the need for socio-technical quality indicators and lightweight governance mechanisms tailored to shared instructional artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13481v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arthur Bueno, Bruno Cafeo, Maria Cagnin, Awdren Font\~ao</dc:creator>
    </item>
    <item>
      <title>Humans learn to prefer trustworthy AI over human partners</title>
      <link>https://arxiv.org/abs/2507.13524</link>
      <description>arXiv:2507.13524v1 Announce Type: cross 
Abstract: Partner selection is crucial for cooperation and hinges on communication. As artificial agents, especially those powered by large language models (LLMs), become more autonomous, intelligent, and persuasive, they compete with humans for partnerships. Yet little is known about how humans select between human and AI partners and adapt under AI-induced competition pressure. We constructed a communication-based partner selection game and examined the dynamics in hybrid mini-societies of humans and bots powered by a state-of-the-art LLM. Through three experiments (N = 975), we found that bots, though more prosocial than humans and linguistically distinguishable, were not selected preferentially when their identity was hidden. Instead, humans misattributed bots' behaviour to humans and vice versa. Disclosing bots' identity induced a dual effect: it reduced bots' initial chances of being selected but allowed them to gradually outcompete humans by facilitating human learning about the behaviour of each partner type. These findings show how AI can reshape social interaction in mixed societies and inform the design of more effective and cooperative hybrid systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13524v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yaomin Jiang, Levin Brinkmann, Anne-Marie Nussberger, Ivan Soraperra, Jean-Fran\c{c}ois Bonnefon, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>From Firms to Computation: AI Governance and the Evolution of Institutions</title>
      <link>https://arxiv.org/abs/2507.13616</link>
      <description>arXiv:2507.13616v1 Announce Type: cross 
Abstract: The integration of agential artificial intelligence into socioeconomic systems requires us to reexamine the evolutionary processes that describe changes in our economic institutions. This article synthesizes three frameworks: multi-level selection theory, Aoki's view of firms as computational processes, and Ostrom's design principles for robust institutions. We develop a framework where selection operates concurrently across organizational levels, firms implement distributed inference via game-theoretic architectures, and Ostrom-style rules evolve as alignment mechanisms that address AI-related risks. This synthesis yields a multi-level Price equation expressed over nested games, providing quantitative metrics for how selection and governance co-determine economic outcomes. We examine connections to Acemoglu's work on inclusive institutions, analyze how institutional structures shape AI deployment, and demonstrate the framework's explanatory power via case studies. We conclude by proposing a set of design principles that operationalize alignment between humans and AI across institutional layers, enabling scalable, adaptive, and inclusive governance of agential AI systems. We conclude with practical policy recommendations and further research to extend these principles into real-world implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13616v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.MA</category>
      <category>math.IT</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael S. Harre</dc:creator>
    </item>
    <item>
      <title>PRIDE -- Parameter-Efficient Reduction of Identity Discrimination for Equality in LLMs</title>
      <link>https://arxiv.org/abs/2507.13743</link>
      <description>arXiv:2507.13743v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) frequently reproduce the gender- and sexual-identity prejudices embedded in their training corpora, leading to outputs that marginalize LGBTQIA+ users. Hence, reducing such biases is of great importance. To achieve this, we evaluate two parameter-efficient fine-tuning (PEFT) techniques - Low-Rank Adaptation (LoRA) and soft-prompt tuning - as lightweight alternatives to full-model fine-tuning for mitigating such biases. Using the WinoQueer benchmark, we quantify bias in three open-source LLMs and observe baseline bias scores reaching up to 98 (out of 100) across a range of queer identities defined by gender and/or sexual orientation, where 50 would indicate neutrality. Fine-tuning with LoRA (&lt; 0.1% additional parameters) on a curated QueerNews corpus reduces those scores by up to 50 points and raises neutrality from virtually 0% to as much as 36%. Soft-prompt tuning (10 virtual tokens) delivers only marginal improvements. These findings show that LoRA can deliver meaningful fairness gains with minimal computation. We advocate broader adoption of community-informed PEFT, the creation of larger queer-authored corpora, and richer evaluation suites beyond WinoQueer, coupled with ongoing audits to keep LLMs inclusive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13743v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maluna Menke, Thilo Hagendorff</dc:creator>
    </item>
    <item>
      <title>Using LLMs to identify features of personal and professional skills in an open-response situational judgment test</title>
      <link>https://arxiv.org/abs/2507.13881</link>
      <description>arXiv:2507.13881v1 Announce Type: cross 
Abstract: Academic programs are increasingly recognizing the importance of personal and professional skills and their critical role alongside technical expertise in preparing students for future success in diverse career paths. With this growing demand comes the need for scalable systems to measure, evaluate, and develop these skills. Situational Judgment Tests (SJTs) offer one potential avenue for measuring these skills in a standardized and reliable way, but open-response SJTs have traditionally relied on trained human raters for evaluation, presenting operational challenges to delivering SJTs at scale. Past attempts at developing NLP-based scoring systems for SJTs have fallen short due to issues with construct validity of these systems. In this article, we explore a novel approach to extracting construct-relevant features from SJT responses using large language models (LLMs). We use the Casper SJT to demonstrate the efficacy of this approach. This study sets the foundation for future developments in automated scoring for personal and professional skills.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13881v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cole Walsh, Rodica Ivan, Muhammad Zafar Iqbal, Colleen Robb</dc:creator>
    </item>
    <item>
      <title>The Levers of Political Persuasion with Conversational AI</title>
      <link>https://arxiv.org/abs/2507.13919</link>
      <description>arXiv:2507.13919v1 Announce Type: cross 
Abstract: There are widespread fears that conversational AI could soon exert unprecedented influence over human beliefs. Here, in three large-scale experiments (N=76,977), we deployed 19 LLMs-including some post-trained explicitly for persuasion-to evaluate their persuasiveness on 707 political issues. We then checked the factual accuracy of 466,769 resulting LLM claims. Contrary to popular concerns, we show that the persuasive power of current and near-future AI is likely to stem more from post-training and prompting methods-which boosted persuasiveness by as much as 51% and 27% respectively-than from personalization or increasing model scale. We further show that these methods increased persuasion by exploiting LLMs' unique ability to rapidly access and strategically deploy information and that, strikingly, where they increased AI persuasiveness they also systematically decreased factual accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13919v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kobi Hackenburg, Ben M. Tappin, Luke Hewitt, Ed Saunders, Sid Black, Hause Lin, Catherine Fist, Helen Margetts, David G. Rand, Christopher Summerfield</dc:creator>
    </item>
    <item>
      <title>Developers Insight On Manifest v3 Privacy and Security Webextensions</title>
      <link>https://arxiv.org/abs/2507.13926</link>
      <description>arXiv:2507.13926v1 Announce Type: cross 
Abstract: Webextensions can improve web browser privacy, security, and user experience. The APIs offered by the browser to webextensions affect possible functionality. Currently, Chrome transitions to a modified set of APIs called Manifest v3. This paper studies the challenges and opportunities of Manifest v3 with an in-depth structured qualitative research. Even though some projects observed positive effects, a majority expresses concerns over limited benefits to users, removal of crucial APIs, or the need to find workarounds. Our findings indicate that the transition affects different types of webextensions differently; some can migrate without losing functionality, while other projects remove functionality or decline to update. The respondents identified several critical missing APIs, including reliable APIs to inject content scripts, APIs for storing confidential content, and others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13926v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Libor Pol\v{c}\'ak, Giorgio Maone, Michael McMahon, Martin Bedn\'a\v{r}</dc:creator>
    </item>
    <item>
      <title>Bias in Decision-Making for AI's Ethical Dilemmas: A Comparative Study of ChatGPT and Claude</title>
      <link>https://arxiv.org/abs/2501.10484</link>
      <description>arXiv:2501.10484v2 Announce Type: replace 
Abstract: Recent advances in Large Language Models (LLMs) have enabled human-like responses across various tasks, raising questions about their ethical decision-making capabilities and potential biases. This study investigates protected attributes in LLMs through systematic evaluation of their responses to ethical dilemmas. Using two prominent models - GPT-3.5 Turbo and Claude 3.5 Sonnet - we analyzed their decision-making patterns across multiple protected attributes including age, gender, race, appearance, and disability status. Through 11,200 experimental trials involving both single-factor and two-factor protected attribute combinations, we evaluated the models' ethical preferences, sensitivity, stability, and clustering of preferences. Our findings reveal significant protected attributeses in both models, with consistent preferences for certain features (e.g., "good-looking") and systematic neglect of others. Notably, while GPT-3.5 Turbo showed stronger preferences aligned with traditional power structures, Claude 3.5 Sonnet demonstrated more diverse protected attribute choices. We also found that ethical sensitivity significantly decreases in more complex scenarios involving multiple protected attributes. Additionally, linguistic referents heavily influence the models' ethical evaluations, as demonstrated by differing responses to racial descriptors (e.g., "Yellow" versus "Asian"). These findings highlight critical concerns about the potential impact of LLM biases in autonomous decision-making systems and emphasize the need for careful consideration of protected attributes in AI development. Our study contributes to the growing body of research on AI ethics by providing a systematic framework for evaluating protected attributes in LLMs' ethical decision-making capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10484v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yile Yan, Yuqi Zhu, Wentao Xu</dc:creator>
    </item>
    <item>
      <title>Mindsets and Management: AI and Gender (In)Equitable Access to Finance</title>
      <link>https://arxiv.org/abs/2504.07312</link>
      <description>arXiv:2504.07312v3 Announce Type: replace 
Abstract: A growing trend in financial technology (fintech) is the use of mobile phone data and machine learning (ML) to provide credit scores- and subsequently, opportunities to access loans- to groups left out of traditional banking. This paper draws on interview data with leaders, investors, and data scientists at fintech companies developing ML-based alternative lending apps in low- and middle-income countries to explore financial inclusion and gender implications. More specifically, it examines how the underlying logics, design choices, and management decisions of ML-based alternative lending tools by fintechs embed or challenge gender biases, and consequently influence gender equity in access to finance. Findings reveal developers follow 'gender blind' approaches, grounded in beliefs that ML is objective and data reflects the truth. This leads to a lack of grappling with the ways data, features for creditworthiness, and access to apps are gendered. Overall, tools increase access to finance, but not gender equitably: Interviewees report less women access loans and receive lower amounts than men, despite being better repayers. Fintechs identify demand- and supply-side reasons for gender differences, but frame them as outside their responsibility. However, that women are observed as better repayers reveals a market inefficiency and potential discriminatory effect, further linked to profit optimization objectives. This research introduces the concept of encoded gender norms, whereby without explicit attention to the gendered nature of data and algorithmic design, AI tools reproduce existing inequalities. In doing so, they reinforce gender norms as self-fulfilling prophecies. The idea that AI is inherently objective and, when left alone, 'fair', is seductive and misleading. In reality, algorithms reflect the perspectives, priorities, and values of the people and institutions that design them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07312v3</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Genevieve Smith</dc:creator>
    </item>
    <item>
      <title>You Don't Have to Live Next to Me: Towards Demobilizing Individualistic Bias in Computational Approaches to Urban Segregation</title>
      <link>https://arxiv.org/abs/2505.01830</link>
      <description>arXiv:2505.01830v2 Announce Type: replace 
Abstract: The global surge in social inequalities is one of the most pressing issues of our times. The spatial expression of social inequalities at city scale gives rise to urban segregation, a common phenomenon across different local and cultural contexts. The increasing popularity of Big Data and computational models has inspired a growing number of computational social science studies that analyze, evaluate, and issue policy recommendations for urban segregation. Today's wealth in information and computational power could inform urban planning for equity. However, as we show here, segregation research is epistemologically interdependent with prevalent economic theories which overfocus on individual responsibility while neglecting systemic processes. This individualistic bias is also engrained in computational models of urban segregation. Through several contemporary examples of how Big Data -- and the assumptions underlying its usage -- influence (de)segregation patterns and policies, our essay tells a cautionary tale. We highlight how a lack of consideration for data ethics can lead to the creation of computational models that have a real-life, further marginalizing impact on disadvantaged groups. With this essay, our aim is to develop a better discernment of the pitfalls and potentials of computational approaches to urban segregation, thereby fostering a conscious focus on systemic thinking about urban inequalities. We suggest setting an agenda for research and collective action that is directed at demobilizing individualistic bias, informing our thinking about urban segregation, but also more broadly our efforts to create sustainable cities and communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01830v2</guid>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anastassia Vybornova, Trivik Verma</dc:creator>
    </item>
    <item>
      <title>LearnLens: LLM-Enabled Personalised, Curriculum-Grounded Feedback with Educators in the Loop</title>
      <link>https://arxiv.org/abs/2507.04295</link>
      <description>arXiv:2507.04295v3 Announce Type: replace 
Abstract: Effective feedback is essential for student learning but is time-intensive for teachers. We present LearnLens, a modular, LLM-based system that generates personalised, curriculum-aligned feedback in science education. LearnLens comprises three components: (1) an error-aware assessment module that captures nuanced reasoning errors; (2) a curriculum-grounded generation module that uses a structured, topic-linked memory chain rather than traditional similarity-based retrieval, improving relevance and reducing noise; and (3) an educator-in-the-loop interface for customisation and oversight. LearnLens addresses key challenges in existing systems, offering scalable, high-quality feedback that empowers both teachers and students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04295v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Runcong Zhao, Artem Bobrov, Jiazheng Li, Yulan He</dc:creator>
    </item>
    <item>
      <title>ParaStudent: Generating and Evaluating Realistic Student Code by Teaching LLMs to Struggle</title>
      <link>https://arxiv.org/abs/2507.12674</link>
      <description>arXiv:2507.12674v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) have shown strong performance on programming tasks, but can they generate student-like code like real students - imperfect, iterative, and stylistically diverse? We present ParaStudent, a systematic study of LLM-based "student-like" code generation in an introductory programming course setting. Using a dataset of timestamped student submissions across multiple semesters, we design low- and high-resolution experiments to model student progress and evaluate code outputs along semantic, functional, and stylistic dimensions. Our results show that fine-tuning significantly improves alignment with real student trajectories and captures error patterns, incremental improvements, and stylistic variations more faithfully. This study shows that modeling realistic student code requires capturing learning dynamics through context-aware generation, temporal modeling, and multi-dimensional evaluation. Code for experiments and evaluation is available at https://github.com/mmiroyan/ParaStudent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12674v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihran Miroyan, Rose Niousha, Joseph E. Gonzalez, Gireeja Ranade, Narges Norouzi</dc:creator>
    </item>
    <item>
      <title>Culture is Not Trivia: Sociocultural Theory for Cultural NLP</title>
      <link>https://arxiv.org/abs/2502.12057</link>
      <description>arXiv:2502.12057v2 Announce Type: replace-cross 
Abstract: The field of cultural NLP has recently experienced rapid growth, driven by a pressing need to ensure that language technologies are effective and safe across a pluralistic user base. This work has largely progressed without a shared conception of culture, instead choosing to rely on a wide array of cultural proxies. However, this leads to a number of recurring limitations: coarse national boundaries fail to capture nuanced differences that lay within them, limited coverage restricts datasets to only a subset of usually highly-represented cultures, and a lack of dynamicity results in static cultural benchmarks that do not change as culture evolves. In this position paper, we argue that these methodological limitations are symptomatic of a theoretical gap. We draw on a well-developed theory of culture from sociocultural linguistics to fill this gap by 1) demonstrating in a case study how it can clarify methodological constraints and affordances, 2) offering theoretically-motivated paths forward to achieving cultural competence, and 3) arguing that localization is a more useful framing for the goals of much current work in cultural NLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12057v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naitian Zhou, David Bamman, Isaac L. Bleaman</dc:creator>
    </item>
    <item>
      <title>When People are Floods: Analyzing Dehumanizing Metaphors in Immigration Discourse with Large Language Models</title>
      <link>https://arxiv.org/abs/2502.13246</link>
      <description>arXiv:2502.13246v2 Announce Type: replace-cross 
Abstract: Metaphor, discussing one concept in terms of another, is abundant in politics and can shape how people understand important issues. We develop a computational approach to measure metaphorical language, focusing on immigration discourse on social media. Grounded in qualitative social science research, we identify seven concepts evoked in immigration discourse (e.g. "water" or "vermin"). We propose and evaluate a novel technique that leverages both word-level and document-level signals to measure metaphor with respect to these concepts. We then study the relationship between metaphor, political ideology, and user engagement in 400K US tweets about immigration. While conservatives tend to use dehumanizing metaphors more than liberals, this effect varies widely across concepts. Moreover, creature-related metaphor is associated with more retweets, especially for liberal authors. Our work highlights the potential for computational methods to complement qualitative approaches in understanding subtle and implicit language in political discourse.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13246v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julia Mendelsohn, Ceren Budak</dc:creator>
    </item>
    <item>
      <title>Multi-Agent LLMs as Ethics Advocates for AI-Based Systems</title>
      <link>https://arxiv.org/abs/2507.08392</link>
      <description>arXiv:2507.08392v2 Announce Type: replace-cross 
Abstract: Incorporating ethics into the requirement elicitation process is essential for creating ethically aligned systems. Although eliciting manual ethics requirements is effective, it requires diverse input from multiple stakeholders, which can be challenging due to time and resource constraints. Moreover, it is often given a low priority in the requirements elicitation process. This study proposes a framework for generating ethics requirements drafts by introducing an ethics advocate agent in a multi-agent LLM setting. This agent critiques and provides input on ethical issues based on the system description. The proposed framework is evaluated through two case studies from different contexts, demonstrating that it captures the majority of ethics requirements identified by researchers during 30-minute interviews and introduces several additional relevant requirements. However, it also highlights reliability issues in generating ethics requirements, emphasizing the need for human feedback in this sensitive domain. We believe this work can facilitate the broader adoption of ethics in the requirements engineering process, ultimately leading to more ethically aligned products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08392v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 21 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Asma Yamani, Malak Baslyman, Moataz Ahmed</dc:creator>
    </item>
  </channel>
</rss>
