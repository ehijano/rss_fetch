<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 04:00:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Universality of Estimator for High-Dimensional Linear Models with Block Dependency</title>
      <link>https://arxiv.org/abs/2410.19244</link>
      <description>arXiv:2410.19244v1 Announce Type: new 
Abstract: We study the universality property of estimators for high-dimensional linear models, which exhibits a distribution of estimators is independent of whether covariates follows a Gaussian distribution. Recent high-dimensional statistics require covariates to strictly follow a Gaussian distribution to reveal precise properties of estimators. To relax the Gaussianity requirement, the existing literature has studied the conditions that allow estimators to achieve universality. In particular, independence of each element of the high-dimensional covariates plays an important role. In this study, we focus on high-dimensional linear models and covariates with block dependencies, where elements of covariates only within a block can be dependent, then show that estimators for the model maintain the universality. Specifically, we prove that a distribution of estimators with Gaussian covariates is approximated by an estimator with non-Gaussian covariates with same moments, with the setup of block dependence. To establish the result, we develop a generalized Lindeberg principle to handle block dependencies and derive new error bounds for correlated elements of covariates. We also apply our result of the universality to a distribution of robust estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19244v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toshiki Tsuda, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of regularised Nystr\"om method for Functional Linear Regression</title>
      <link>https://arxiv.org/abs/2410.19312</link>
      <description>arXiv:2410.19312v1 Announce Type: new 
Abstract: The functional linear regression model has been widely studied and utilized for dealing with functional predictors. In this paper, we study the Nystr\"om subsampling method, a strategy used to tackle the computational complexities inherent in big data analytics, especially within the domain of functional linear regression model in the framework of reproducing kernel Hilbert space.
  By adopting a Nystr\"om subsampling strategy, our aim is to mitigate the computational overhead associated with kernel methods, which often struggle to scale gracefully with dataset size. Specifically, we investigate a regularization-based approach combined with Nystr\"om subsampling for functional linear regression model, effectively reducing the computational complexity from $O(n^3)$ to $O(m^2 n)$, where $n$ represents the size of the observed empirical dataset and $m$ is the size of subsampled dataset. Notably, we establish that these methodologies will achieve optimal convergence rates, provided that the subsampling level is appropriately selected. We have also demonstrated a numerical example of Nystr\"om subsampling in the RKHS framework for the functional linear regression model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19312v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naveen Gupta, Sivananthan Sampath</dc:creator>
    </item>
    <item>
      <title>Extreme values of the mass distribution associated with $d$-quasi-copulas via linear programming</title>
      <link>https://arxiv.org/abs/2410.19339</link>
      <description>arXiv:2410.19339v1 Announce Type: new 
Abstract: The recent survey published in Fuzzy Sets and Systems nicknamed ``Hitchhiker's Guide'' has raised the rating of quasi-copula problems in the dependence modeling community in spite of the lack of statistical interpretation of quasi-copulas. Some of the open problems listed there were solved, and some conjectured one way or the other. This paper concentrates on the Open Problem 5 of this list concerning bounds on the volume of a $d$--variate quasi-copula. We disprove a recent conjecture published in the same journal on the lower bound of this volume. We also give evidence that the problem is much more difficult than suspected and provide hints about its final solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19339v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matej Bel\v{s}ak, Matja\v{z} Omladi\v{c}, Martin Vuk, Alja\v{z} Zalar</dc:creator>
    </item>
    <item>
      <title>On low frequency inference for diffusions without the hot spots conjecture</title>
      <link>https://arxiv.org/abs/2410.19393</link>
      <description>arXiv:2410.19393v1 Announce Type: new 
Abstract: We remove the dependence on the `hot-spots' conjecture in two of the main theorems of the recent paper of Nickl (2024, Annals of Statistics). Specifically, we characterise the minimax convergence rates for estimation of the transition operator $P_{f}$ arising from the Neumann Laplacian with diffusion coefficient $f$ on arbitrary convex domains with smooth boundary, and further show that a general Lipschitz stability estimate holds for the inverse map $P_f\mapsto f$ from $H^2\to H^2$ to $L^1$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19393v1</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni S. Alberti, Douglas Barnes, Aditya Jambhale, Richard Nickl</dc:creator>
    </item>
    <item>
      <title>Distance and Kernel-Based Measures for Global and Local Two-Sample Conditional Distribution Testing</title>
      <link>https://arxiv.org/abs/2210.08149</link>
      <description>arXiv:2210.08149v2 Announce Type: cross 
Abstract: Testing the equality of two conditional distributions is crucial in various modern applications, including transfer learning and causal inference. Despite its importance, this fundamental problem has received surprisingly little attention in the literature. This work aims to present a unified framework based on distance and kernel methods for both global and local two-sample conditional distribution testing. To this end, we introduce distance and kernel-based measures that characterize the homogeneity of two conditional distributions. Drawing from the concept of conditional U-statistics, we propose consistent estimators for these measures. Theoretically, we derive the convergence rates and the asymptotic distributions of the estimators under both the null and alternative hypotheses. Utilizing these measures, along with a local bootstrap approach, we develop global and local tests that can detect discrepancies between two conditional distributions at global and local levels, respectively. Our tests demonstrate reliable performance through simulations and real data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08149v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jian Yan, Zhuoxi Li, Xianyang Zhang</dc:creator>
    </item>
    <item>
      <title>Deterministic Fokker-Planck Transport -- With Applications to Sampling, Variational Inference, Kernel Mean Embeddings &amp; Sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2410.18993</link>
      <description>arXiv:2410.18993v1 Announce Type: cross 
Abstract: The Fokker-Planck equation can be reformulated as a continuity equation, which naturally suggests using the associated velocity field in particle flow methods. While the resulting probability flow ODE offers appealing properties - such as defining a gradient flow of the Kullback-Leibler divergence between the current and target densities with respect to the 2-Wasserstein distance - it relies on evaluating the current probability density, which is intractable in most practical applications. By closely examining the drawbacks of approximating this density via kernel density estimation, we uncover opportunities to turn these limitations into advantages in contexts such as variational inference, kernel mean embeddings, and sequential Monte Carlo.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18993v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilja Klebanov</dc:creator>
    </item>
    <item>
      <title>A spectral method for multi-view subspace learning using the product of projections</title>
      <link>https://arxiv.org/abs/2410.19125</link>
      <description>arXiv:2410.19125v1 Announce Type: cross 
Abstract: Multi-view data provides complementary information on the same set of observations, with multi-omics and multimodal sensor data being common examples. Analyzing such data typically requires distinguishing between shared (joint) and unique (individual) signal subspaces from noisy, high-dimensional measurements. Despite many proposed methods, the conditions for reliably identifying joint and individual subspaces remain unclear. We rigorously quantify these conditions, which depend on the ratio of the signal rank to the ambient dimension, principal angles between true subspaces, and noise levels. Our approach characterizes how spectrum perturbations of the product of projection matrices, derived from each view's estimated subspaces, affect subspace separation. Using these insights, we provide an easy-to-use and scalable estimation algorithm. In particular, we employ rotational bootstrap and random matrix theory to partition the observed spectrum into joint, individual, and noise subspaces. Diagnostic plots visualize this partitioning, providing practical and interpretable insights into the estimation performance. In simulations, our method estimates joint and individual subspaces more accurately than existing approaches. Applications to multi-omics data from colorectal cancer patients and nutrigenomic study of mice demonstrate improved performance in downstream predictive tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19125v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renat Sergazinov, Armeen Taeb, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Probability Proofs for Stirling (and More): the Ubiquitous Role of $\mathbf{\sqrt{2\pi}}$</title>
      <link>https://arxiv.org/abs/2410.19555</link>
      <description>arXiv:2410.19555v1 Announce Type: cross 
Abstract: The Stirling approximation formula for $n!$ dates from 1730. Here we give new and instructive proofs of this and related approximation formulae via tools of probability and statistics. There are connections to the Central Limit Theorem and also to approximations of marginal distributions in Bayesian setups. Certain formulae emerge by working through particular instances, some independently verifiable but others perhaps not. A particular case yielding new formulae is that of summing independent uniforms, related to the Irwin--Hall distribution. Yet further proofs of the Stirling flow from examining aspects of limiting normality of the sample median of uniforms, and from these again we find a proof for the Wallis product formula for $\pi$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19555v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nils Lid Hjort, Emil Aas Stoltenberg</dc:creator>
    </item>
    <item>
      <title>On the robustness of semi-discrete optimal transport</title>
      <link>https://arxiv.org/abs/2410.19596</link>
      <description>arXiv:2410.19596v1 Announce Type: cross 
Abstract: We derive the breakdown point for solutions of semi-discrete optimal transport problems, which characterizes the robustness of the multivariate quantiles based on optimal transport proposed in Ghosal and Sen (2022). We do so under very mild assumptions: the absolutely continuous reference measure is only assumed to have a support that is compact and convex, whereas the target measure is a general discrete measure on a finite number, $n$ say, of atoms. The breakdown point depends on the target measure only through its probability weights (hence not on the location of the atoms) and involves the geometry of the reference measure through the Tukey (1975) concept of halfspace depth. Remarkably, depending on this geometry, the breakdown point of the optimal transport median can be strictly smaller than the breakdown point of the univariate median or the breakdown point of the spatial median, namely~$\lceil n/2\rceil /2$. In the context of robust location estimation, our results provide a subtle insight on how to perform multivariate trimming when constructing trimmed means based on optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19596v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Davy Paindaveine, Riccardo Passeggeri</dc:creator>
    </item>
    <item>
      <title>Rate-optimal estimation of mixed semimartingales</title>
      <link>https://arxiv.org/abs/2207.10464</link>
      <description>arXiv:2207.10464v2 Announce Type: replace 
Abstract: Consider the sum $Y=B+B(H)$ of a Brownian motion $B$ and an independent fractional Brownian motion $B(H)$ with Hurst parameter $H\in(0,1)$. Even though $B(H)$ is not a semimartingale, it was shown in [\textit{Bernoulli} \textbf{7} (2001) 913--934] that $Y$ is a semimartingale if $H&gt;3/4$. Moreover, $Y$ is locally equivalent to $B$ in this case, so $H$ cannot be consistently estimated from local observations of $Y$. This paper pivots on another unexpected feature in this model: if $B$ and $B(H)$ become correlated, then $Y$ will never be a semimartingale, and $H$ can be identified, regardless of its value. This and other results will follow from a detailed statistical analysis of a more general class of processes called \emph{mixed semimartingales}, which are semiparametric extensions of $Y$ with stochastic volatility in both the martingale and the fractional component. In particular, we derive consistent estimators and feasible central limit theorems for all parameters and processes that can be identified from high-frequency observations. We further show that our estimators achieve optimal rates in a minimax sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.10464v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carsten H. Chong, Thomas Delerue, Fabian Mies</dc:creator>
    </item>
    <item>
      <title>The Power of Two Matrices in Spectral Algorithms for Community Recovery</title>
      <link>https://arxiv.org/abs/2210.05893</link>
      <description>arXiv:2210.05893v3 Announce Type: replace 
Abstract: Spectral algorithms are some of the main tools in optimization and inference problems on graphs. Typically, the graph is encoded as a matrix and eigenvectors and eigenvalues of the matrix are then used to solve the given graph problem. Spectral algorithms have been successfully used for graph partitioning, hidden clique recovery and graph coloring. In this paper, we study the power of spectral algorithms using two matrices in a graph partitioning problem. We use two different matrices resulting from two different encodings of the same graph and then combine the spectral information coming from these two matrices.
  We analyze a two-matrix spectral algorithm for the problem of identifying latent community structure in large random graphs. In particular, we consider the problem of recovering community assignments exactly in the censored stochastic block model, where each edge status is revealed independently with some probability. We show that spectral algorithms based on two matrices are optimal and succeed in recovering communities up to the information theoretic threshold. Further, we show that for most choices of the parameters, any spectral algorithm based on one matrix is suboptimal. The latter observation is in contrast to our prior works (2022a, 2022b) which showed that for the symmetric Stochastic Block Model and the Planted Dense Subgraph problem, a spectral algorithm based on one matrix achieves the information theoretic threshold. We additionally provide more general geometric conditions for the (sub)-optimality of spectral algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.05893v3</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Souvik Dhara, Julia Gaudio, Elchanan Mossel, Colin Sandon</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Spacings Goodness-of-Fit Statistics for Univariate Shape-constrained Densities</title>
      <link>https://arxiv.org/abs/2211.13272</link>
      <description>arXiv:2211.13272v2 Announce Type: replace 
Abstract: A variety of statistics based on sample spacings has been studied in the literature for testing goodness-of-fit to parametric distributions. To test the goodness-of-fit to a nonparametric class of univariate shape-constrained densities, including widely studied classes such as k-monotone and log-concave densities, a likelihood ratio test with a working alternative density estimate based on the spacings of the observations is considered, and is shown to be asymptotically normal and distribution-free under the null, consistent under fixed alternatives, and admits bootstrap calibration. The distribution-freeness under the null comes from the fact that the asymptotic dominant term depends only on a function of the spacings of transformed outcomes that are uniformly distributed. Applications and extensions of theoretical results in the literature of shape-constrained estimation are required to show that the average log-density ratio converges to zero at a faster rate than the sample spacing term under the null, and diverges under the alternatives. Numerical studies are conducted to demonstrate that the test is applicable to various classes of shape-constrained densities and has a good balance between type-I error control under the null and power under alternative distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13272v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kwun Chuen Gary Chan, Hok Kan Ling, Chuan-Fa Tang, Sheung Chi Phillip Yam</dc:creator>
    </item>
    <item>
      <title>Aliasing and Label-Independent Decomposition of Risk: Beyond the bias-variance trade-off</title>
      <link>https://arxiv.org/abs/2408.08294</link>
      <description>arXiv:2408.08294v2 Announce Type: replace 
Abstract: A central problem in data science is to use potentially noisy samples of an unknown function to predict function values for unseen inputs. In classical statistics, the predictive error is understood as a trade-off between the bias and the variance that balances model simplicity with its ability to fit complex functions. However, over-parameterized models exhibit counter-intuitive behaviors, such as "double descent" in which models of increasing complexity exhibit decreasing generalization error. In contrast to the bias-variance trade-off, we introduce an alternative paradigm called the generalized aliasing decomposition (GAD). We explain the asymptotically small error of complex models as a systematic "de-aliasing" that occurs in the over-parameterized regime. In the limit of large models, the error contribution due to aliasing vanishes, leaving an expression for the asymptotic total error we call the data insufficiency failure of very large models on few training points. Because the generalized aliasing decomposition can be explicitly calculated from the relationship between model class and samples without seeing any data labels, it can answer questions related to experimental design and model selection before collecting data or performing experiments. We demonstrate this approach using several examples, including classical regression problems and a cluster expansion model used in materials science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08294v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark K. Transtrum, Gus L. W. Hart, Tyler J. Jarvis, Jared P. Whitehead</dc:creator>
    </item>
    <item>
      <title>Bivariate dynamic conditional failure extropy</title>
      <link>https://arxiv.org/abs/2410.09882</link>
      <description>arXiv:2410.09882v4 Announce Type: replace 
Abstract: Nair and Sathar (2020) introduced a new metric for uncertainty known as dynamic failure extropy, focusing on the analysis of past lifetimes. In this study, we extend this concept to a bivariate context, exploring various properties associated with the proposed bivariate measure. We show that bivariate conditional failure extropy can uniquely determine the joint distribution function. Additionally, we derive characterizations for certain bivariate lifetime models using this measure. A new stochastic ordering, based on bivariate conditional failure extropy, is also proposed, along with some established bounds. We further develop an estimator for the bivariate conditional failure extropy using a smoothed kernel and empirical approach. The performance of the proposed estimator is evaluated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09882v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aman Pandey, Chanchal Kundu</dc:creator>
    </item>
    <item>
      <title>Vecchia Gaussian Processes: Probabilistic Properties, Minimax Rates and Methodological Developments</title>
      <link>https://arxiv.org/abs/2410.10649</link>
      <description>arXiv:2410.10649v2 Announce Type: replace 
Abstract: Gaussian Processes (GPs) are widely used to model dependency in spatial statistics and machine learning, yet the exact computation suffers an intractable time complexity of $O(n^3)$. Vecchia approximation allows scalable Bayesian inference of GPs in $O(n)$ time by introducing sparsity in the spatial dependency structure that is characterized by a directed acyclic graph (DAG). Despite the popularity in practice, it is still unclear how to choose the DAG structure and there are still no theoretical guarantees in nonparametric settings. In this paper, we systematically study the Vecchia GPs as standalone stochastic processes and uncover important probabilistic properties and statistical results in methodology and theory. For probabilistic properties, we prove that the conditional distributions of the Mat\'{e}rn GPs, as well as the Vecchia approximations of the Mat\'{e}rn GPs, can be characterized by polynomials. This allows us to prove a series of results regarding the small ball probabilities and RKHSs of Vecchia GPs. For statistical methodology, we provide a principled guideline to choose parent sets as norming sets with fixed cardinality and provide detailed algorithms following such guidelines. For statistical theory, we prove posterior contraction rates for applying Vecchia GPs to regression problems, where minimax optimality is achieved by optimally tuned GPs via either oracle rescaling or hierarchical Bayesian methods. Our theory and methodology are demonstrated with numerical studies, where we also provide efficient implementation of our methods in C++ with R interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10649v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Botond Szabo, Yichen Zhu</dc:creator>
    </item>
    <item>
      <title>A Short Note on the Efficiency of Markov Chains for Bayesian Linear Regression Models with Heavy-Tailed Errors</title>
      <link>https://arxiv.org/abs/2410.17070</link>
      <description>arXiv:2410.17070v2 Announce Type: replace 
Abstract: In this short note, we consider posterior simulation for a linear regression model when the error distribution is given by a scale mixture of multivariate normals. We show that the sampler of Backlund and Hobert (2020) for the case of the conditionally conjugate normal-inverse Wishart prior is geometrically ergodic even when the error density is heavier-tailed. Moreover, we prove that their sampler is uniformly ergodic if, in addition, the columns of the design matrix are linearly independent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17070v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura</dc:creator>
    </item>
    <item>
      <title>Least Squares Regression Can Exhibit Under-Parameterized Double Descent</title>
      <link>https://arxiv.org/abs/2305.14689</link>
      <description>arXiv:2305.14689v3 Announce Type: replace-cross 
Abstract: The relationship between the number of training data points, the number of parameters, and the generalization capabilities of models has been widely studied. Previous work has shown that double descent can occur in the over-parameterized regime and that the standard bias-variance trade-off holds in the under-parameterized regime. These works provide multiple reasons for the existence of the peak. We postulate that the location of the peak depends on the technical properties of both the spectrum as well as the eigenvectors of the sample covariance. We present two simple examples that provably exhibit double descent in the under-parameterized regime and do not seem to occur for reasons provided in prior work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14689v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinyue Li, Rishi Sonthalia</dc:creator>
    </item>
  </channel>
</rss>
