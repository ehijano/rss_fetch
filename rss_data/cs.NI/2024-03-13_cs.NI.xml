<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Mar 2024 04:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Reconfigurable Intelligent Surfaces for THz: Hardware Design and Signal Processing Challenges</title>
      <link>https://arxiv.org/abs/2403.07889</link>
      <description>arXiv:2403.07889v1 Announce Type: new 
Abstract: Wireless communications in the THz frequency band is an envisioned revolutionary technology for sixth Generation (6G) networks. However, such frequencies impose certain coverage and device design challenges that need to be efficiently overcome. To this end, the development of cost- and energy-efficient approaches for scaling these networks to realistic scenarios constitute a necessity. Among the recent research trends contributing to these objectives belongs the technology of Reconfigurable Intelligent Surfaces (RISs). In fact, several high-level descriptions of THz systems based on RISs have been populating the literature. Nevertheless, hardware implementations of those systems are still very scarce, and not at the scale intended for most envisioned THz scenarios. In this paper, we overview some of the most significant hardware design and signal processing challenges with THz RISs, and present a preliminary analysis of their impact on the overall link budget and system performance, conducted in the framework of the ongoing TERRAMETA project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07889v1</guid>
      <category>cs.NI</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>George C. Alexandropoulos, Antonio Clemente, S\'ergio Matos, Ryan Husbands, Sean Ahearne, Qi Luo, Ver\'onica Lain-Rubio, Thomas K\"urner, Lu\'is M. Pessoa</dc:creator>
    </item>
    <item>
      <title>Multiple Access in the Era of Distributed Computing and Edge Intelligence</title>
      <link>https://arxiv.org/abs/2403.07903</link>
      <description>arXiv:2403.07903v1 Announce Type: new 
Abstract: This paper focuses on the latest research and innovations in fundamental next-generation multiple access (NGMA) techniques and the coexistence with other key technologies for the sixth generation (6G) of wireless networks. In more detail, we first examine multi-access edge computing (MEC), which is critical to meeting the growing demand for data processing and computational capacity at the edge of the network, as well as network slicing. We then explore over-the-air (OTA) computing, which is considered to be an approach that provides fast and efficient computation of various functions. We also explore semantic communications, identified as an effective way to improve communication systems by focusing on the exchange of meaningful information, thus minimizing unnecessary data and increasing efficiency. The interrelationship between machine learning (ML) and multiple access technologies is also reviewed, with an emphasis on federated learning, federated distillation, split learning, reinforcement learning, and the development of ML-based multiple access protocols. Finally, the concept of digital twinning and its role in network management is discussed, highlighting how virtual replication of physical networks can lead to improvements in network efficiency and reliability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07903v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikos G. Evgenidis, Nikos A. Mitsiou, Vasiliki I. Koutsioumpa, Sotiris A. Tegos, Panagiotis D. Diamantoulakis, George K. Karagiannidis</dc:creator>
    </item>
    <item>
      <title>CycloWatt: An Affordable, TinyML-enhanced IoT Device Revolutionizing Cycling Power Metrics</title>
      <link>https://arxiv.org/abs/2403.07915</link>
      <description>arXiv:2403.07915v1 Announce Type: new 
Abstract: Cycling power measurement is an indispensable metric with profound implications for cyclists' performance and fitness levels. It empowers riders with real-time feedback, supports precise training regimen planning, mitigates injury risks, and enhances muscular development. Despite these advantages, the widespread adoption of cycling power meters has been hampered by their prohibitive cost and deployment complexity. This paper pioneers a groundbreaking approach to power measurement in cycling, prioritizing affordability and user-friendliness. To achieve this goal, we introduce a cutting-edge Internet of Things (IoT) device that seamlessly integrates force signals with inertial sensor data while leveraging the power of edge machine learning techniques. In-field experimental evaluations demonstrate that our prototype can estimate power with remarkable accuracy, boasting a Mean Absolute Error (MAE) of only 12.29 Watts (4.1\%). Notably, our design emphasizes energy efficiency, operating in a low-power mode that consumes a mere 50 milliwatts and offers an exceptional battery life of up to 25.8 hours in always-on active mode. With an ultra-low latency of 4.33 milliseconds for data processing and inference, our system ensures real-time power estimation during cycling activities. Incorporating IoT concepts and devices, this paper marks a significant milestone in developing cost-effective and accurate cycling power meters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07915v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Luder, Sizhen Bian, Michele Magno</dc:creator>
    </item>
    <item>
      <title>The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments</title>
      <link>https://arxiv.org/abs/2403.07923</link>
      <description>arXiv:2403.07923v1 Announce Type: new 
Abstract: In response to the demand for real-time performance and control quality in industrial Internet of Things (IoT) environments, this paper proposes an optimization control system based on deep reinforcement learning and edge computing. The system leverages cloud-edge collaboration, deploys lightweight policy networks at the edge, predicts system states, and outputs controls at a high frequency, enabling monitoring and optimization of industrial objectives. Additionally, a dynamic resource allocation mechanism is designed to ensure rational scheduling of edge computing resources, achieving global optimization. Results demonstrate that this approach reduces cloud-edge communication latency, accelerates response to abnormal situations, reduces system failure rates, extends average equipment operating time, and saves costs for manual maintenance and replacement. This ensures real-time and stable control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07923v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingyu Xu, Weixiang Wan, Linying Pan, Wenjian Sun, Yuxiang Liu</dc:creator>
    </item>
    <item>
      <title>Intelligent Monitoring Framework for Cloud Services: A Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2403.07927</link>
      <description>arXiv:2403.07927v1 Announce Type: new 
Abstract: Cloud service owners need to continuously monitor their services to ensure high availability and reliability. Gaps in monitoring can lead to delay in incident detection and significant negative customer impact. Current process of monitor creation is ad-hoc and reactive in nature. Developers create monitors using their tribal knowledge and, primarily, a trial and error based process. As a result, monitors often have incomplete coverage which leads to production issues, or, redundancy which results in noise and wasted effort.
  In this work, we address this issue by proposing an intelligent monitoring framework that recommends monitors for cloud services based on their service properties. We start by mining the attributes of 30,000+ monitors from 791 production services at Microsoft and derive a structured ontology for monitors. We focus on two crucial dimensions: what to monitor (resources) and which metrics to monitor. We conduct an extensive empirical study and derive key insights on the major classes of monitors employed by cloud services at Microsoft, their associated dimensions, and the interrelationship between service properties and this ontology. Using these insights, we propose a deep learning based framework that recommends monitors based on the service properties. Finally, we conduct a user study with engineers from Microsoft which demonstrates the usefulness of the proposed framework. The proposed framework along with the ontology driven projections, succeeded in creating production quality recommendations for majority of resource classes. This was also validated by the users from the study who rated the framework's usefulness as 4.27 out of 5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07927v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pooja Srinivas, Fiza Husain, Anjaly Parayil, Ayush Choure, Chetan Bansal, Saravan Rajmohan</dc:creator>
    </item>
    <item>
      <title>Zero-Rating, One Big Mess: Analyzing Differential Pricing Practices of European MNOs</title>
      <link>https://arxiv.org/abs/2403.08066</link>
      <description>arXiv:2403.08066v1 Announce Type: new 
Abstract: Zero-rating, the practice of not billing data traffic that belongs to certain applications, has become popular within the mobile ecosystem around the globe. There is an ongoing debate whether mobile operators should be allowed to differentiate traffic or whether net neutrality regulations should prevent this. Despite the importance of this issue, we know little about the technical aspects of zero-rating offers since the implementation is kept secret by mobile operators and therefore is opaque to end-users and regulatory agencies.
  This work aims to independently audit classification practices used for zero-rating of four popular applications at seven different mobile operators in the EU. We execute and evaluate more than 300 controlled experiments within domestic and internationally roamed environments and identify potentially problematic behavior at almost all investigated operators. With this study, we hope to increase transparency around the current practices and inform future decisions and policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08066v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>IEEE Global Communications Conference (GLOBECOM) 2022</arxiv:journal_reference>
      <dc:creator>Gabriel Karl Gegenhuber, Wilfried Mayer, Edgar Weippl</dc:creator>
    </item>
    <item>
      <title>From Channel Measurement to Training Data for PHY Layer AI Applications</title>
      <link>https://arxiv.org/abs/2403.08317</link>
      <description>arXiv:2403.08317v1 Announce Type: new 
Abstract: Learning-based techniques such as artificial intelligence (AI) and machine learning (ML) play an increasingly important role in the development of future communication networks. The success of a learning algorithm depends on the quality and quantity of the available training data. In the physical layer (PHY), channel information data can be obtained either through measurement campaigns or through simulations based on predefined channel models. Performing measurements can be time consuming while only gaining information about one specific position or scenario. Simulated data, on the other hand, are more generalized and reflect in most cases not a real environment but instead, a statistical approximation based on a mathematical model. This paper presents a procedure for acquiring channel data by means of fast and flexible software defined radio (SDR) based channel measurements along with a method for a parameter extraction that provides configuration input to the simulator. The procedure from the measurement to the simulated channel data is demonstrated in two exemplary propagation scenarios. It is shown, that in both cases the simulated data is in good accordance to the measurements</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08317v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Zentarra, Julian Ahrens, Lia Ahrens</dc:creator>
    </item>
    <item>
      <title>MobileAtlas: Geographically Decoupled Measurements in Cellular Networks for Security and Privacy Research</title>
      <link>https://arxiv.org/abs/2403.08507</link>
      <description>arXiv:2403.08507v1 Announce Type: new 
Abstract: Cellular networks are not merely data access networks to the Internet. Their distinct services and ability to form large complex compounds for roaming purposes make them an attractive research target in their own right. Their promise of providing a consistent service with comparable privacy and security across roaming partners falls apart at close inspection.
  Thus, there is a need for controlled testbeds and measurement tools for cellular access networks doing justice to the technology's unique structure and global scope. Particularly, such measurements suffer from a combinatorial explosion of operators, mobile plans, and services. To cope with these challenges, we built a framework that geographically decouples the SIM from the cellular modem by selectively connecting both remotely. This allows testing any subscriber with any operator at any modem location within minutes without moving parts. The resulting GSM/UMTS/LTE measurement and testbed platform offers a controlled experimentation environment, which is scalable and cost-effective. The platform is extensible and fully open-sourced, allowing other researchers to contribute locations, SIM cards, and measurement scripts.
  Using the above framework, our international experiments in commercial networks revealed exploitable inconsistencies in traffic metering, leading to multiple phreaking opportunities, i.e., fare-dodging. We also expose problematic IPv6 firewall configurations, hidden SIM card communication to the home network, and fingerprint dial progress tones to track victims across different roaming networks and countries with voice calls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08507v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>USENIX Security 2023</arxiv:journal_reference>
      <dc:creator>Gabriel Karl Gegenhuber, Wilfried Mayer, Edgar Weippl, Adrian Dabrowski</dc:creator>
    </item>
    <item>
      <title>Plotinus: A Satellite Internet Digital Twin System</title>
      <link>https://arxiv.org/abs/2403.08515</link>
      <description>arXiv:2403.08515v1 Announce Type: new 
Abstract: The development of integrated space-air-ground network (SAGIN) requires sophisticated satellite Internet emulation tools that can handle complex, dynamic topologies and offer in-depth analysis. Existing emulation platforms struggle with challenges like the need for detailed implementation across all network layers, real-time response times, and the ability to scale. Plotinus, a new digital twin system based on microservices for satellite Internet emulation, aims to solve these problems. It features a modular design, allowing for easy replacement of the physical layer to emulate different aerial vehicles and analyze channel interference. It also enables the replacement of path computation methods to simplify testing and deploying algorithms. In particular, Plotinus allows for real-time emulation with live network traffic, enhancing the realism of network models. Evaluation result shows that Plotinus's effective emulation of dynamic satellite networks with real-world devices. Its adaptability for various communication models and algorithm testing highlights Plotinus's role as a vital tool for developing and analyzing SAGIN systems, offering a scalable, real-time response, and flexible digital twin system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08515v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Gao, Kun Qiu, Zhe Chen, Wenjun Zhu, Qi Zhang, Handong Luo, Quanwei Lin, Ziheng Yang, Wenhao Liu</dc:creator>
    </item>
    <item>
      <title>An Extended View on Measuring Tor AS-level Adversaries</title>
      <link>https://arxiv.org/abs/2403.08517</link>
      <description>arXiv:2403.08517v1 Announce Type: new 
Abstract: Tor provides anonymity to millions of users around the globe which has made it a valuable target for malicious actors. As a low-latency anonymity system, it is vulnerable to traffic correlation attacks from strong passive adversaries such as large autonomous systems (ASes). In preliminary work, we have developed a measurement approach utilizing the RIPE Atlas framework -- a network of more than 11,000 probes worldwide -- to infer the risk of deanonymization for IPv4 clients in Germany and the US.
  In this paper, we apply our methodology to additional scenarios providing a broader picture of the potential for deanonymization in the Tor network. In particular, we (a) repeat our earlier (2020) measurements in 2022 to observe changes over time, (b) adopt our approach for IPv6 to analyze the risk of deanonymization when using this next-generation Internet protocol, and (c) investigate the current situation in Russia, where censorship has been intensified after the beginning of Russia's full-scale invasion of Ukraine. According to our results, Tor provides user anonymity at consistent quality: While individual numbers vary in dependence of client and destination, we were able to identify ASes with the potential to conduct deanonymization attacks. For clients in Germany and the US, the overall picture, however, has not changed since 2020. In addition, the protocols (IPv4 vs. IPv6) do not significantly impact the risk of deanonymization. Russian users are able to securely evade censorship using Tor. Their general risk of deanonymization is, in fact, lower than in the other investigated countries. Beyond, the few ASes with the potential to successfully perform deanonymization are operated by Western companies, further reducing the risk for Russian users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08517v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Computers &amp; Security 2023/5/25</arxiv:journal_reference>
      <dc:creator>Gabriel Karl Gegenhuber, Markus Maier, Florian Holzbauer, Wilfried Mayer, Georg Merzdovnik, Edgar Weippl, Johanna Ullrich</dc:creator>
    </item>
    <item>
      <title>Evaluation of Control/User-Plane Denial-of-Service (DoS) Attack on O-RAN Fronthaul Interface</title>
      <link>https://arxiv.org/abs/2403.08600</link>
      <description>arXiv:2403.08600v1 Announce Type: new 
Abstract: The open fronthaul interface defined by O-RAN ALLIANCE aims to support the interoperability between multi-vendor open radio access network (O-RAN) radio units (O-RU) and O-RAN distributed units (O-DU). This paper introduces a new tool that could be used to evaluate Denial-of-Service (DoS) attacks against the open fronthaul interface. We launched an array of control/user planes (C/U-Planes) attacks with the tool under different traffic types and data rates, and we evaluated their impacts on the throughput and block error rate (BLER) of real-world O-RAN systems with commercial hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08600v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferlinda Feliana, Ting-Wei Hung, Binbin Chen, Ray-Guang Cheng</dc:creator>
    </item>
    <item>
      <title>Digital Twin-assisted Reinforcement Learning for Resource-aware Microservice Offloading in Edge Computing</title>
      <link>https://arxiv.org/abs/2403.08687</link>
      <description>arXiv:2403.08687v1 Announce Type: new 
Abstract: Collaborative edge computing (CEC) has emerged as a promising paradigm, enabling edge nodes to collaborate and execute microservices from end devices. Microservice offloading, a fundamentally important problem, decides when and where microservices are executed upon the arrival of services. However, the dynamic nature of the real-world CEC environment often leads to inefficient microservice offloading strategies, resulting in underutilized resources and network congestion. To address this challenge, we formulate an online joint microservice offloading and bandwidth allocation problem, JMOBA, to minimize the average completion time of services. In this paper, we introduce a novel microservice offloading algorithm, DTDRLMO, which leverages deep reinforcement learning (DRL) and digital twin technology. Specifically, we employ digital twin techniques to predict and adapt to changing edge node loads and network conditions of CEC in real-time. Furthermore, this approach enables the generation of an efficient offloading plan, selecting the most suitable edge node for each microservice. Simulation results on real-world and synthetic datasets demonstrate that DTDRLMO outperforms heuristic and learning-based methods in average service completion time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08687v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MASS58611.2023.00012</arxiv:DOI>
      <arxiv:journal_reference>2023 IEEE 20th International Conference on Mobile Ad Hoc and Smart Systems (MASS), Toronto, ON, Canada, 2023, pp. 28-36</arxiv:journal_reference>
      <dc:creator>Xiangchun Chen, Jiannong Cao, Zhixuan Liang, Yuvraj Sahni, Mingjin Zhang</dc:creator>
    </item>
    <item>
      <title>SNOW-SCA: ML-assisted Side-Channel Attack on SNOW-V</title>
      <link>https://arxiv.org/abs/2403.08267</link>
      <description>arXiv:2403.08267v1 Announce Type: cross 
Abstract: This paper presents SNOW-SCA, the first power side-channel analysis (SCA) attack of a 5G mobile communication security standard candidate, SNOW-V, running on a 32-bit ARM Cortex-M4 microcontroller. First, we perform a generic known-key correlation (KKC) analysis to identify the leakage points. Next, a correlation power analysis (CPA) attack is performed, which reduces the attack complexity to two key guesses for each key byte. The correct secret key is then uniquely identified utilizing linear discriminant analysis (LDA). The profiled SCA attack with LDA achieves 100% accuracy after training with $&lt;200$ traces, which means the attack succeeds with just a single trace. Overall, using the \textit{combined CPA and LDA attack} model, the correct secret key byte is recovered with &lt;50 traces collected using the ChipWhisperer platform. The entire 256-bit secret key of SNOW-V can be recovered incrementally using the proposed SCA attack. Finally, we suggest low-overhead countermeasures that can be used to prevent these SCA attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08267v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harshit Saurabh, Anupam Golder, Samarth Shivakumar Titti, Suparna Kundu, Chaoyun Li, Angshuman Karmakar, Debayan Das</dc:creator>
    </item>
    <item>
      <title>OSMOSIS: Enabling Multi-Tenancy in Datacenter SmartNICs</title>
      <link>https://arxiv.org/abs/2309.03628</link>
      <description>arXiv:2309.03628v3 Announce Type: replace 
Abstract: Multi-tenancy is essential for unleashing SmartNIC's potential in datacenters. Our systematic analysis in this work shows that existing on-path SmartNICs have resource multiplexing limitations. For example, existing solutions lack multi-tenancy capabilities such as performance isolation and QoS provisioning for compute and IO resources. Compared to standard NIC data paths with a well-defined set of offloaded functions, unpredictable execution times of SmartNIC kernels make conventional approaches for multi-tenancy and QoS insufficient. We fill this gap with OSMOSIS, a SmartNICs resource manager co-design. OSMOSIS extends existing OS mechanisms to enable dynamic hardware resource multiplexing of the on-path packet processing data plane. We integrate OSMOSIS within an open-source RISC-V-based 400Gbit/s SmartNIC. Our performance results demonstrate that OSMOSIS fully supports multi-tenancy and enables broader adoption of SmartNICs in datacenters with low overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03628v3</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.OS</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mikhail Khalilov, Marcin Chrapek, Siyuan Shen, Alessandro Vezzu, Thomas Benz, Salvatore Di Girolamo, Timo Schneider, Daniele De Sensi, Luca Benini, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>CacheGen: Fast Context Loading for Language Model Applications via KV Cache Streaming</title>
      <link>https://arxiv.org/abs/2310.07240</link>
      <description>arXiv:2310.07240v2 Announce Type: replace 
Abstract: As large language models (LLMs) take on complex tasks, their inputs are supplemented with longer contexts that incorporate domain knowledge or user-specific information. Yet using long contexts poses a challenge for responsive LLM systems, as nothing can be generated until the whole context is processed by the LLM. While the context-processing delay can be reduced by reusing the KV cache of a context across different inputs, fetching the KV cache, which contains large tensors, over the network can cause extra network delays.
  CacheGen is a fast context-loading module for LLM systems. First, CacheGen uses a custom tensor encoder, which embraces KV cache's distributional properties, to encode a KV cache into more compact bitstream representations with negligible encoding/decoding overhead. This reduces the bandwidth demand to fetch the KV cache. Second, to maintain low context-loading delay and high generation quality, CacheGen adapts the streaming strategies to cope with changes in available bandwidth. When available bandwidth drops, CacheGen may raise the compression level for a part of the context or choose to recompute its KV cache on the fly. We test CacheGen on four popular LLMs of various sizes and four datasets (662 contexts in total). Compared to the recent systems that reuse the KV cache, CacheGen reduces the KV cache size by 3.7-4.3x and the total delay in fetching and processing contexts by 2.7-3.2x while having negligible impact on the LLM response quality in accuracy or perplexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07240v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhan Liu, Hanchen Li, Yihua Cheng, Siddhant Ray, Yuyang Huang, Qizheng Zhang, Kuntai Du, Jiayi Yao, Shan Lu, Ganesh Ananthanarayanan, Michael Maire, Henry Hoffmann, Ari Holtzman, Junchen Jiang</dc:creator>
    </item>
    <item>
      <title>WetLinks: a Large-Scale Longitudinal Starlink Dataset with Contiguous Weather Data</title>
      <link>https://arxiv.org/abs/2402.16448</link>
      <description>arXiv:2402.16448v2 Announce Type: replace 
Abstract: Low Orbit Satellite (LEO) networks such as Starlink promise Internet access everywhere around the world. In this paper, we present WetLinks - a large and publicly available trace-based dataset of Starlink measurements. The measurements were concurrently collected from two European vantage points over a span of six months. Consisting of approximately 140,000 measurements, the dataset comprises all relevant network parameters such as the upload and download throughputs, the RTT, packet loss, and traceroutes. We further augment the dataset with concurrent data from professional weather stations placed next to both Starlink terminals. Based on our dataset, we analyse Starlink performance, including its susceptibility to weather conditions. We use this to validate our dataset by replicating the results of earlier smaller-scale studies. We release our datasets and all accompanying tooling as open data. To the best of our knowledge, ours is the largest Starlink dataset to date.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16448v2</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Laniewski, Eric Lanfer, Bernd Meijerink, Roland van Rijswijk-Deij, Nils Aschenbruck</dc:creator>
    </item>
    <item>
      <title>GRACE: Loss-Resilient Real-Time Video through Neural Codecs</title>
      <link>https://arxiv.org/abs/2305.12333</link>
      <description>arXiv:2305.12333v4 Announce Type: replace-cross 
Abstract: In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder. We present a loss-resilient real-time video system called GRACE, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE's enhanced loss resilience is its joint training of the neural encoder and decoder under a spectrum of simulated packet losses. In lossless scenarios, GRACE achieves video quality on par with conventional codecs (e.g., H.265). As the loss rate escalates, GRACE exhibits a more graceful, less pronounced decline in quality, consistently outperforming other loss-resilient schemes. Through extensive evaluation on various videos and real network traces, we demonstrate that GRACE reduces undecodable frames by 95% and stall duration by 90% compared with FEC, while markedly boosting video quality over error concealment methods. In a user study with 240 crowdsourced participants and 960 subjective ratings, GRACE registers a 38% higher mean opinion score (MOS) than other baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12333v4</guid>
      <category>cs.MM</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihua Cheng, Ziyi Zhang, Hanchen Li, Anton Arapin, Yue Zhang, Qizheng Zhang, Yuhan Liu, Xu Zhang, Francis Y. Yan, Amrita Mazumdar, Nick Feamster, Junchen Jiang</dc:creator>
    </item>
  </channel>
</rss>
