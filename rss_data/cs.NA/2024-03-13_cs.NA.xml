<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Mar 2024 04:00:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 14 Mar 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Parallelization in time by diagonalization</title>
      <link>https://arxiv.org/abs/2403.07875</link>
      <description>arXiv:2403.07875v1 Announce Type: new 
Abstract: This is a review of preconditioning techniques based on fast-diagonalization methods for space-time isogeometric discretization of the heat equation. Three formulation are considered: the Galerkin approach, a discrete least-square and a continuous least square. For each formulation the heat differential operator is written as a sum of terms that are kronecker products of uni-variate operators. These are used to speed-up the application of the operator in iterative solvers and to construct a suitable preconditioner. Contrary to the fast-diagonalization technique for the Laplace equation where all uni-variate operators acting on the same direction can be simultaneously diagonalized in the case of the heat equation this is not possible. Luckily this can be done up to an additional term that has low rank allowing for the utilization of arrow-head like factorization or inversion by Sherman-Morrison formula. The proposed preconditioners work extremely well on the parametric domain and, when the domain is parametrized or when the equation coefficients are not constant, they can be adapted and retain good performance characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07875v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bressan, Alen Kushova, Gabriele Loli, Monica Montardini, Giancarlo Sangalli, Mattia Tani</dc:creator>
    </item>
    <item>
      <title>Note on a weakly over-penalised symmetric interior penalty method on anisotropic meshes for the Poisson equation, Ver. 1</title>
      <link>https://arxiv.org/abs/2403.07899</link>
      <description>arXiv:2403.07899v1 Announce Type: new 
Abstract: The purpose is to make an easy-to-understand note of "Special Topics in Finite Element Methods." There might be typos and mistakes. Therefore, I do not take any responsibility for unauthorised use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07899v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiroki Ishizaka</dc:creator>
    </item>
    <item>
      <title>Stability of the Ritz projection in weighted $W^{1,1}$</title>
      <link>https://arxiv.org/abs/2403.07934</link>
      <description>arXiv:2403.07934v1 Announce Type: new 
Abstract: We prove the stability in weighted $W^{1,1}$ spaces for standard finite element approximations of the Poisson equation in convex polygonal or polyhedral domains, when the weight belongs to Muckenhoupt's class $A_1$ and the family of meshes is quasi-uniform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07934v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Irene Drelichman, Ricardo G. Duran</dc:creator>
    </item>
    <item>
      <title>Accelerating multigrid solver with generative super-resolution</title>
      <link>https://arxiv.org/abs/2403.07936</link>
      <description>arXiv:2403.07936v1 Announce Type: new 
Abstract: The geometric multigrid algorithm is an efficient numerical method for solving a variety of elliptic partial differential equations (PDEs). The method damps errors at progressively finer grid scales, resulting in faster convergence compared to iterative methods such as Gauss-Seidel. The prolongation or coarse-to-fine interpolation operator within the multigrid algorithm, lends itself to a data-driven treatment with deep learning super-resolution, commonly used to increase the resolution of images. We (i) propose the integration of a super-resolution generative adversarial network (GAN) model with the multigrid algorithm as the prolongation operator and (ii) show that the GAN-interpolation can improve the convergence properties of multigrid in comparison to cubic spline interpolation on a class of multiscale PDEs typically solved in fluid mechanics and engineering simulations. We also highlight the importance of characterizing hybrid (machine learning/traditional) algorithm parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07936v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Francisco Holguin, GS Sidharth, Gavin Portwood</dc:creator>
    </item>
    <item>
      <title>The $L_p$-discrepancy for finite $p&gt;1$ suffers from the curse of dimensionality</title>
      <link>https://arxiv.org/abs/2403.07961</link>
      <description>arXiv:2403.07961v1 Announce Type: new 
Abstract: The $L_p$-discrepancy is a classical quantitative measure for the irregularity of distribution of an $N$-element point set in the $d$-dimensional unit cube. Its inverse for dimension $d$ and error threshold $\varepsilon \in (0,1)$ is the number of points in $[0,1)^d$ that is required such that the minimal normalized $L_p$-discrepancy is less or equal $\varepsilon$. It is well known, that the inverse of $L_2$-discrepancy grows exponentially fast with the dimension $d$, i.e., we have the curse of dimensionality, whereas the inverse of $L_{\infty}$-discrepancy depends exactly linearly on $d$. The behavior of inverse of $L_p$-discrepancy for general $p \not\in \{2,\infty\}$ was an open problem since many years. Recently, the curse of dimensionality for the $L_p$-discrepancy was shown for an infinite sequence of values $p$ in $(1,2]$, but the general result seemed to be out of reach.
  In the present paper we show that the $L_p$-discrepancy suffers from the curse of dimensionality for all $p$ in $(1,\infty)$ and only the case $p=1$ is still open.
  This result follows from a more general result that we show for the worst-case error of positive quadrature formulas for an anchored Sobolev space of once differentiable functions in each variable whose first mixed derivative has finite $L_q$-norm, where $q$ is the H\"older conjugate of $p$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07961v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erich Novak, Friedrich Pillichshammer</dc:creator>
    </item>
    <item>
      <title>Extending Irksome: improvements in automated Runge--Kutta time stepping for finite element methods</title>
      <link>https://arxiv.org/abs/2403.08084</link>
      <description>arXiv:2403.08084v1 Announce Type: new 
Abstract: Irksome is a library based on the Unified Form Language (UFL) that enables automated generation of Runge--Kutta methods for time-stepping finite element spatial discretizations of partial differential equations (PDE). Allowing users to express semidiscrete forms of PDE, it generates UFL representations for the stage-coupled variational problems to be solved at each time step. The Firedrake package then generates efficient code for evaluating these variational problems and allows users a wide range of options to deploy efficient algebraic solvers in PETSc.
  In this paper, we describe several recent advances in Irksome. These include alternate formulations of the Runge--Kutta time-stepping methods and optimized support for diagonally implicit (DIRK) methods. Additionally, we present new and improved tools for building preconditioners for the resulting linear and linearized systems, demonstrating that these can lead to efficient approaches for solving fully implicit Runge-Kutta discretizations.
  The new features are demonstrated through a sequence of computational examples demonstrating the high-level interface and obtained solver performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08084v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert C. Kirby, Scott P. MacLachlan</dc:creator>
    </item>
    <item>
      <title>Efficient geometric Markov chain Monte Carlo for nonlinear Bayesian inversion enabled by derivative-informed neural operators</title>
      <link>https://arxiv.org/abs/2403.08220</link>
      <description>arXiv:2403.08220v1 Announce Type: new 
Abstract: We propose an operator learning approach to accelerate geometric Markov chain Monte Carlo (MCMC) for solving infinite-dimensional nonlinear Bayesian inverse problems. While geometric MCMC employs high-quality proposals that adapt to posterior local geometry, it requires computing local gradient and Hessian information of the log-likelihood, incurring a high cost when the parameter-to-observable (PtO) map is defined through expensive model simulations. We consider a delayed-acceptance geometric MCMC method driven by a neural operator surrogate of the PtO map, where the proposal is designed to exploit fast surrogate approximations of the log-likelihood and, simultaneously, its gradient and Hessian. To achieve a substantial speedup, the surrogate needs to be accurate in predicting both the observable and its parametric derivative (the derivative of the observable with respect to the parameter). Training such a surrogate via conventional operator learning using input--output samples often demands a prohibitively large number of model simulations. In this work, we present an extension of derivative-informed operator learning [O'Leary-Roseberry et al., J. Comput. Phys., 496 (2024)] using input--output--derivative training samples. Such a learning method leads to derivative-informed neural operator (DINO) surrogates that accurately predict the observable and its parametric derivative at a significantly lower training cost than the conventional method. Cost and error analysis for reduced basis DINO surrogates are provided. Numerical studies on PDE-constrained Bayesian inversion demonstrate that DINO-driven MCMC generates effective posterior samples 3--9 times faster than geometric MCMC and 60--97 times faster than prior geometry-based MCMC. Furthermore, the training cost of DINO surrogates breaks even after collecting merely 10--25 effective posterior samples compared to geometric MCMC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08220v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lianghao Cao, Thomas O'Leary-Roseberry, Omar Ghattas</dc:creator>
    </item>
    <item>
      <title>Explicit radial basis function Runge-Kutta methods</title>
      <link>https://arxiv.org/abs/2403.08253</link>
      <description>arXiv:2403.08253v1 Announce Type: new 
Abstract: The aim of this paper is to design the explicit radial basis function (RBF) Runge-Kutta methods for the initial value problem. We construct the two-, three- and four-stage RBF Runge-Kutta methods based on the Gaussian RBF Euler method with the shape parameter, where the analysis of the local truncation error shows that the s-stage RBF Runge-Kutta method could formally achieve order s+1. The proof for the convergence of those RBF Runge-Kutta methods follows. We then plot the stability region of each RBF Runge-Kutta method proposed and compare with the one of the correspondent Runge-Kutta method. Numerical experiments are provided to exhibit the improved behavior of the RBF Runge-Kutta methods over the standard ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08253v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaxi Gu, Xinjuan Chen, Jae-Hun Jung</dc:creator>
    </item>
    <item>
      <title>A posteriori error estimates for the Generalized Burgers-Huxley equation with weakly singular kernels</title>
      <link>https://arxiv.org/abs/2403.08269</link>
      <description>arXiv:2403.08269v1 Announce Type: new 
Abstract: This paper explores the residual based a posteriori error estimations for the generalized Burgers-Huxley equation (GBHE) featuring weakly singular kernels. Initially, we present a reliable and efficient error estimator for both the stationary GBHE and the semi-discrete GBHE with memory, utilizing the discontinuous Galerkin finite element method (DGFEM) in spatial dimensions. Additionally, employing backward Euler and Crank Nicolson discretization in the temporal domain and DGFEM in spatial dimensions, we introduce an estimator for the fully discrete GBHE, taking into account the influence of past history. The paper also establishes optimal $L^2$ error estimates for both the stationary GBHE and GBHE. Ultimately, we validate the effectiveness of the proposed error estimator through numerical results, demonstrating its efficacy in an adaptive refinement strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08269v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sumit Mahajan, Arbaz Khan</dc:creator>
    </item>
    <item>
      <title>Fully discrete finite difference schemes for the Fractional Korteweg-de Vries equation</title>
      <link>https://arxiv.org/abs/2403.08275</link>
      <description>arXiv:2403.08275v1 Announce Type: new 
Abstract: In this paper, we present and analyze fully discrete finite difference schemes designed for solving the initial value problem associated with the fractional Korteweg-de Vries (KdV) equation involving the fractional Laplacian. We design the scheme by introducing the discrete fractional Laplacian operator which is consistent with the continuous operator, and posses certain properties which are instrumental for the convergence analysis. Assuming the initial data (u_0 \in H^{1+\alpha}(\mathbb{R})), where (\alpha \in [1,2)), our study establishes the convergence of the approximate solutions obtained by the fully discrete finite difference schemes to a classical solution of the fractional KdV equation. Theoretical results are validated through several numerical illustrations for various values of fractional exponent $\alpha$. Furthermore, we demonstrate that the Crank-Nicolson finite difference scheme preserves the inherent conserved quantities along with the improved convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08275v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mukul Dwivedi, Tanmay Sarkar</dc:creator>
    </item>
    <item>
      <title>A fast wavefield evaluation method using a modified proxy-surface accelerated interpolative decomposition for scattering problems in two dimensions</title>
      <link>https://arxiv.org/abs/2403.08290</link>
      <description>arXiv:2403.08290v1 Announce Type: new 
Abstract: This paper presents a fast wavefield evaluation method for wave scattering problems. The typical previous fast method is the fast multipole method (FMM). The FMM, however, requires the combined use of non-fast direct evaluations near the boundaries of scatterers. The proposed method is based on a modified proxy-surface method accelerated interpolative decomposition. It is, therefore, effective even if evaluation points are near the boundary and, moreover, is free from analytical expansion of kernel functions unlike FMM. The validness and effectiveness of the proposed method are shown by numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08290v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuhiro Matsumoto</dc:creator>
    </item>
    <item>
      <title>Weak Collocation Regression for Inferring Stochastic Dynamics with L\'{e}vy Noise</title>
      <link>https://arxiv.org/abs/2403.08292</link>
      <description>arXiv:2403.08292v1 Announce Type: new 
Abstract: With the rapid increase of observational, experimental and simulated data for stochastic systems, tremendous efforts have been devoted to identifying governing laws underlying the evolution of these systems. Despite the broad applications of non-Gaussian fluctuations in numerous physical phenomena, the data-driven approaches to extracting stochastic dynamics with L\'{e}vy noise are relatively few. In this work, we propose a Weak Collocation Regression (WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the Stochastic Differential Equation (SDE) with both $\alpha$-stable L\'{e}vy noise and Gaussian noise, from discrete aggregate data. This method utilizes the evolution equation of the probability distribution function, i.e., the Fokker-Planck (FP) equation. With the weak form of the FP equation, the WCR constructs a linear system of unknown parameters where all integrals are evaluated by Monte Carlo method with the observations. Then, the unknown parameters are obtained by a sparse linear regression. For a SDE with L\'{e}vy noise, the corresponding FP equation is a partial integro-differential equation (PIDE), which contains nonlocal terms, and is difficult to deal with. The weak form can avoid complicated multiple integrals. Our approach can simultaneously distinguish mixed noise types, even in multi-dimensional problems. Numerical experiments demonstrate that our method is accurate and computationally efficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08292v1</guid>
      <category>math.NA</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liya Guo, Liwei Lu, Zhijun Zeng, Pipi Hu, Yi Zhu</dc:creator>
    </item>
    <item>
      <title>Mean field error estimate of the random batch method for large interacting particle system</title>
      <link>https://arxiv.org/abs/2403.08336</link>
      <description>arXiv:2403.08336v1 Announce Type: new 
Abstract: The random batch method (RBM) proposed in [Jin et al., J. Comput. Phys., 400(2020), 108877] for large interacting particle systems is an efficient with linear complexity in particle numbers and highly scalable algorithm for $N$-particle interacting systems and their mean-field limits when $N$ is large. We consider in this work the quantitative error estimate of RBM toward its mean-field limit, the Fokker-Planck equation. Under mild assumptions, we obtain a uniform-in-time $O(\tau^2 + 1/N)$ bound on the scaled relative entropy between the joint law of the random batch particles and the tensorized law at the mean-field limit, where $\tau$ is the time step size and $N$ is the number of particles. Therefore, we improve the existing rate in discretization step size from $O(\sqrt{\tau})$ to $O(\tau)$ in terms of the Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08336v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyu Huang, Shi Jin, Lei Li</dc:creator>
    </item>
    <item>
      <title>Discretization of Total Variation in Optimization with Integrality Constraint</title>
      <link>https://arxiv.org/abs/2403.08346</link>
      <description>arXiv:2403.08346v1 Announce Type: new 
Abstract: We introduce discretizations of infinite-dimensional optimization problems with total variation regularization and integrality constraints on the optimization variables. We advance the discretization of the dual formulation of the total variation term with Raviart--Thomas functions which is known from literature for certain convex problems. Since we have an integrality constraint, the previous analysis from Caillaud and Chambolle [10] does not hold anymore. Even weaker $\Gamma$-convergence results do not hold anymore because the recovery sequences generally need to attain non-integer values to recover the total variation of the limit function. We solve this issue by introducing a discretization of the input functions on an embedded, finer mesh. A superlinear coupling of the mesh sizes implies an averaging on the coarser mesh of the Raviart--Thomas ansatz, which enables to recover the total variation of integer-valued limit functions with integer-valued discretized input functions. Moreover, we are able to estimate the discretized total variation of the recovery sequence by the total variation of its limit and an error depending on the mesh size ratio. For the discretized optimization problems, we additionally add a constraint that vanishes in the limit and enforces compactness of the sequence of minimizers, which yields their convergence to a minimizer of the original problem. This constraint contains a degree of freedom whose admissible range is determined. Its choice may have a strong impact on the solutions in practice as we demonstrate with an example from imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08346v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Annika Schiemann, Paul Manns</dc:creator>
    </item>
    <item>
      <title>Boundary and distributed optimal control for a population dynamics PDE model with discontinuous in time Galerkin FEM schemes</title>
      <link>https://arxiv.org/abs/2403.08419</link>
      <description>arXiv:2403.08419v1 Announce Type: new 
Abstract: We consider fully discrete finite element approximations for a semilinear optimal control system of partial differential equations in two cases: for distributed and Robin boundary control. The ecological predator-prey optimal control model is approximated by conforming finite element methods mimicking the spatial part, while a discontinuous Galerkin method is used for the time discretization. We investigate the sensitivity of the solution distance from the target function, in cases with smooth and rough initial data. We employ low, and higher-order polynomials in time and space whenever proper regularity is present. The approximation schemes considered are with and without control constraints, driving efficiently the system to desired states realized using non-linear gradient methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08419v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>EFthymios N. Karatzas</dc:creator>
    </item>
    <item>
      <title>Non-linear collision-induced breakage equation: finite volume and semi-analytical methods</title>
      <link>https://arxiv.org/abs/2403.08457</link>
      <description>arXiv:2403.08457v1 Announce Type: new 
Abstract: The non-linear collision-induced breakage equation has significant applications in particulate processes. Two semi-analytical techniques, namely homotopy analysis method (HAM) and accelerated homotopy perturbation method (AHPM) are investigated along with the well-known finite volume method (FVM) to comprehend the dynamical behavior of the non-linear system, i.e., the concentration function, the total number and the total mass of the particles in the system. The theoretical convergence analyses of the series solutions of HAM and AHPM are discussed. In addition, the error estimations of the truncated solutions of both methods equip the maximum absolute error bound. To justify the applicability and accuracy of these methods, numerical simulations are compared with the findings of FVM and analytical solutions considering three physical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08457v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjiv Kumar Bariwal, Saddam Hussain, Rajesh Kumar</dc:creator>
    </item>
    <item>
      <title>Hamiltonian Boundary Value Methods (HBVMs) for functional differential equations with piecewise continuous arguments</title>
      <link>https://arxiv.org/abs/2403.08597</link>
      <description>arXiv:2403.08597v1 Announce Type: new 
Abstract: In this paper, a class of high-order methods to numerically solve Functional Differential Equations with Piecewise Continuous Arguments (FDEPCAs) is discussed. The framework stems from the expansion of the vector field associated with the reference differential equation along the shifted and scaled Legendre polynomial orthonormal basis, working on a suitable extension of Hamiltonian Boundary Value Methods. Within the design of the methods, a proper generalization of the perturbation results coming from the field of ordinary differential equations is considered, with the aim of handling the case of FDEPCAs. The error analysis of the devised family of methods is performed, while a few numerical tests on Hamiltonian FDEPCAs are provided, to give evidence to the theoretical findings and show the effectiveness of the obtained resolution strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08597v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianmarco Gurioli, Weijie Wang, Xiaoqiang Yan</dc:creator>
    </item>
    <item>
      <title>Tangential Fixpoint Iterations for Gromov-Wasserstein Barycenters</title>
      <link>https://arxiv.org/abs/2403.08612</link>
      <description>arXiv:2403.08612v1 Announce Type: new 
Abstract: The Gromov-Wasserstein (GW) transport problem is a relaxation of classic optimal transport, which seeks a transport between two measures while preserving their internal geometry. Due to meeting this theoretical underpinning, it is a valuable tool for the analysis of objects that do not possess a natural embedding or should be studied independently of it. Prime applications can thus be found in e.g. shape matching, classification and interpolation tasks. To tackle the latter, one theoretically justified approach is the employment of multi-marginal GW transport and GW barycenters, which are Fr\'echet means with respect to the GW distance. However, because the computation of GW itself already poses a quadratic and non-convex optimization problem, the determination of GW barycenters is a hard task and algorithms for their computation are scarce. In this paper, we revisit a known procedure for the determination of Fr\'echet means in Riemannian manifolds via tangential approximations in the context of GW. We provide a characterization of barycenters in the GW tangent space, which ultimately gives rise to a fixpoint iteration for approximating GW barycenters using multi-marginal plans. We propose a relaxation of this fixpoint iteration and show that it monotonously decreases the barycenter loss. In certain cases our proposed method naturally provides us with barycentric embeddings. The resulting algorithm is capable of producing qualitative shape interpolations between multiple 3d shapes with support sizes of over thousands of points in reasonable time. In addition, we verify our method on shape classification and multi-graph matching tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08612v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Beier, Robert Beinert</dc:creator>
    </item>
    <item>
      <title>A semidefinite programming characterization of the Crawford number</title>
      <link>https://arxiv.org/abs/2403.08617</link>
      <description>arXiv:2403.08617v1 Announce Type: new 
Abstract: We give a semidefinite programming characterization of the Crawford number. We show that the computation of the Crawford number within $\varepsilon$ precision is computable in polynomial time in the data and $|\log \varepsilon |$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08617v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shmuel Friedland, Cynthia Vinzant</dc:creator>
    </item>
    <item>
      <title>Non-linear collision-induced breakage equation: approximate solution and error estimation</title>
      <link>https://arxiv.org/abs/2403.08672</link>
      <description>arXiv:2403.08672v1 Announce Type: new 
Abstract: This article aims to provide approximate solutions for the non-linear collision-induced breakage equation using two different semi-analytical schemes, i.e., variational iteration method (VIM) and optimized decomposition method (ODM). The study also includes the detailed convergence analysis and error estimation for ODM in the case of product collisional ($K(\epsilon,\rho)=\epsilon\rho$) and breakage ($b(\epsilon,\rho,\sigma)=\frac{2}{\rho}$) kernels with an exponential decay initial condition. By contrasting estimated concentration function and moments with exact solutions, the novelty of the suggested approaches is presented considering three numerical examples. Interestingly, in one case, VIM provides a closed-form solution, however, finite term series solutions obtained via both schemes supply a great approximation for the concentration function and moments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08672v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjiv Kumar Bariwal, Rajesh Kumar</dc:creator>
    </item>
    <item>
      <title>Dynamic computerized tomography using inexact models and motion estimation</title>
      <link>https://arxiv.org/abs/2403.08714</link>
      <description>arXiv:2403.08714v1 Announce Type: new 
Abstract: Reconstructing a dynamic object with affine motion in computerized tomography (CT) leads to motion artifacts if the motion is not taken into account. In most cases, the actual motion is neither known nor can be determined easily. As a consequence, the respective model that describes CT is incomplete. The iterative RESESOP-Kaczmarz method can - under certain conditions and by exploiting the modeling error - reconstruct dynamic objects at different time points even if the exact motion is unknown. However, the method is very time-consuming. To speed the reconstruction process up and obtain better results, we combine the following three steps: 1. RESESOP-Kacmarz with only a few iterations is implemented to reconstruct the object at different time points. 2. The motion is estimated via landmark detection, e.g. using deep learning. 3. The estimated motion is integrated into the reconstruction process, allowing the use of dynamic filtered backprojection. We give a short review of all methods involved and present numerical results as a proof of principle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08714v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gesa Sarnighausen, Anne Wald, Alexander Meaney</dc:creator>
    </item>
    <item>
      <title>Sketching the Heat Kernel: Using Gaussian Processes to Embed Data</title>
      <link>https://arxiv.org/abs/2403.07929</link>
      <description>arXiv:2403.07929v1 Announce Type: cross 
Abstract: This paper introduces a novel, non-deterministic method for embedding data in low-dimensional Euclidean space based on computing realizations of a Gaussian process depending on the geometry of the data. This type of embedding first appeared in (Adler et al, 2018) as a theoretical model for a generic manifold in high dimensions.
  In particular, we take the covariance function of the Gaussian process to be the heat kernel, and computing the embedding amounts to sketching a matrix representing the heat kernel. The Karhunen-Lo\`eve expansion reveals that the straight-line distances in the embedding approximate the diffusion distance in a probabilistic sense, avoiding the need for sharp cutoffs and maintaining some of the smaller-scale structure.
  Our method demonstrates further advantage in its robustness to outliers. We justify the approach with both theory and experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07929v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna C. Gilbert, Kevin O'Neill</dc:creator>
    </item>
    <item>
      <title>Dimension reduction, exact recovery, and error estimates for sparse reconstruction in phase space</title>
      <link>https://arxiv.org/abs/2112.09743</link>
      <description>arXiv:2112.09743v2 Announce Type: replace 
Abstract: An important theme in modern inverse problems is the reconstruction of time-dependent data from only finitely many measurements. To obtain satisfactory reconstruction results in this setting it is essential to strongly exploit temporal consistency between the different measurement times. The strongest consistency can be achieved by reconstructing data directly in phase space, the space of positions and velocities. However, this space is usually too high-dimensional for feasible computations. We introduce a novel dimension reduction technique, based on projections of phase space onto lower-dimensional subspaces, which provably circumvents this curse of dimensionality: Indeed, in the exemplary framework of superresolution we prove that known exact reconstruction results stay true after dimension reduction, and we additionally prove new error estimates of reconstructions from noisy data in optimal transport metrics which are of the same quality as one would obtain in the non-dimension-reduced case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.09743v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.acha.2024.101631</arxiv:DOI>
      <arxiv:journal_reference>M. Holler, A. Schl\"uter, B. Wirth, Dimension reduction, exact recovery, and error estimates for sparse reconstruction in phase space, Applied and Computational Harmonic Analysis, Volume 70, 2024, 101631, ISSN 1063-5203</arxiv:journal_reference>
      <dc:creator>Martin Holler, Alexander Schl\"uter, Benedikt Wirth</dc:creator>
    </item>
    <item>
      <title>An implicit DG solver for incompressible two-phase flows with an artificial compressibility formulation</title>
      <link>https://arxiv.org/abs/2307.04580</link>
      <description>arXiv:2307.04580v2 Announce Type: replace 
Abstract: We propose an implicit Discontinuous Galerkin (DG) discretization for incompressible two-phase flows using an artificial compressibility formulation. The conservative level set (CLS) method is employed in combination with a reinitialization procedure to capture the moving interface. A projection method based on the L-stable TR-BDF2 method is adopted for the time discretization of the Navier-Stokes equations and of the level set method. Adaptive Mesh Refinement (AMR) is employed to enhance the resolution in correspondence of the interface between the two fluids. The effectiveness of the proposed approach is shown in a number of classical benchmarks. A specific analysis on the influence of different choices of the mixture viscosity is also carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04580v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe Orlando</dc:creator>
    </item>
    <item>
      <title>A spatial-temporal weight analysis and novel nonlinear weights of weighted essentially non-oscillatory schemes for hyperbolic conservation laws</title>
      <link>https://arxiv.org/abs/2310.05679</link>
      <description>arXiv:2310.05679v2 Announce Type: replace 
Abstract: In this paper we analyze the weighted essentially non-oscillatory (WENO) schemes in the finite volume framework by examining the first step of the explicit third-order total variation diminishing Runge-Kutta method. The rationale for the improved performance of the finite volume WENO-M, WENO-Z and WENO-ZR schemes over WENO-JS in the first time step is that the nonlinear weights corresponding to large errors are adjusted to increase the accuracy of numerical solutions. Based on this analysis, we propose novel Z-type nonlinear weights of the finite volume WENO scheme for hyperbolic conservation laws. Instead of taking the difference of the smoothness indicators for the global smoothness indicator, we employ the logarithmic function with tuners to ensure that the numerical dissipation is reduced around discontinuities while the essentially non-oscillatory property is preserved. The proposed scheme does not necessitate substantial extra computational expenses. Numerical examples are presented to demonstrate the capability of the proposed WENO scheme in shock capturing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.05679v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xinjuan Chen, Jiaxi Gu, Jae-Hun Jung</dc:creator>
    </item>
    <item>
      <title>Sparse grid approximation of stochastic parabolic PDEs: The Landau--Lifshitz--Gilbert equation</title>
      <link>https://arxiv.org/abs/2310.11225</link>
      <description>arXiv:2310.11225v2 Announce Type: replace 
Abstract: We show convergence rates for a sparse grid approximation of the distribution of solutions of the stochastic Landau-Lifshitz-Gilbert equation. Beyond being a frequently studied equation in engineering and physics, the stochastic Landau-Lifshitz-Gilbert equation poses many interesting challenges that do not appear simultaneously in previous works on uncertainty quantification: The equation is strongly non-linear, time-dependent, and has a non-convex side constraint. Moreover, the parametrization of the stochastic noise features countably many unbounded parameters and low regularity compared to other elliptic and parabolic problems studied in uncertainty quantification. We use a novel technique to establish uniform holomorphic regularity of the parameter-to-solution map based on a Gronwall-type estimate and the implicit function theorem. This method is very general and based on a set of abstract assumptions. Thus, it can be applied beyond the Landau-Lifshitz-Gilbert equation as well. We demonstrate numerically the feasibility of approximating with sparse grid and show a clear advantage of a multi-level sparse grid scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11225v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xin An, Josef Dick, Michael Feischl, Andrea Scaglioni, Thanh Tran</dc:creator>
    </item>
    <item>
      <title>High-order bounds-satisfying approximation of partial differential equations via finite element variational inequalities</title>
      <link>https://arxiv.org/abs/2311.05880</link>
      <description>arXiv:2311.05880v2 Announce Type: replace 
Abstract: Solutions to many important partial differential equations satisfy bounds constraints, but approximations computed by finite element or finite difference methods typically fail to respect the same conditions. Chang and Nakshatrala enforce such bounds in finite element methods through the solution of variational inequalities rather than linear variational problems. Here, we provide a theoretical justification for this method, including higher-order discretizations. We prove an abstract best approximation result for the linear variational inequality and estimates showing that bounds-constrained polynomials provide comparable approximation power to standard spaces. For any unconstrained approximation to a function, there exists a constrained approximation which is comparable in the $W^{1,p}$ norm. In practice, one cannot efficiently represent and manipulate the entire family of bounds-constrained polynomials, but applying bounds constraints to the coefficients of a polynomial in the Bernstein basis guarantees those constraints on the polynomial. Although our theoretical results do not guaruntee high accuracy for this subset of bounds-constrained polynomials, numerical results indicate optimal orders of accuracy for smooth solutions and sharp resolution of features in convection-diffusion problems, all subject to bounds constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05880v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert C. Kirby, Daniel Shapero</dc:creator>
    </item>
    <item>
      <title>Randomized Kaczmarz in Adversarial Distributed Setting</title>
      <link>https://arxiv.org/abs/2302.14615</link>
      <description>arXiv:2302.14615v2 Announce Type: replace-cross 
Abstract: Developing large-scale distributed methods that are robust to the presence of adversarial or corrupted workers is an important part of making such methods practical for real-world problems. In this paper, we propose an iterative approach that is adversary-tolerant for convex optimization problems. By leveraging simple statistics, our method ensures convergence and is capable of adapting to adversarial distributions. Additionally, the efficiency of the proposed methods for solving convex problems is shown in simulations with the presence of adversaries. Through simulations, we demonstrate the efficiency of our approach in the presence of adversaries and its ability to identify adversarial workers with high accuracy and tolerate varying levels of adversary rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.14615v2</guid>
      <category>math.OC</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Longxiu Huang, Xia Li, Deanna Needell</dc:creator>
    </item>
    <item>
      <title>The Exponential Stabilization of a Heat and Piezoelectric Beam Interaction with Static or Hybrid Feedback Controllers</title>
      <link>https://arxiv.org/abs/2311.05306</link>
      <description>arXiv:2311.05306v2 Announce Type: replace-cross 
Abstract: This study investigates a strongly-coupled system of partial differential equations (PDE) governing heat transfer in a copper rod, longitudinal vibrations, and total charge accumulation at electrodes within a magnetizable piezoelectric beam. Conducted within the transmission line framework, the analysis reveals profound interactions between traveling electromagnetic and mechanical waves in magnetizable piezoelectric beams, despite disparities in their velocities. Findings suggest that in the open-loop scenario, the interaction of heat and beam dynamics lacks exponential stability solely considering thermal effects. To confront this challenge, two types of boundary-type state feedback controllers are proposed: (i) employing static feedback controllers entirely and (ii) adopting a hybrid approach wherein the electrical controller dynamically enhances system dynamics. In both cases, solutions of the PDE systems demonstrate exponential stability through meticulously formulated Lyapunov functions with diverse multipliers. The proposed proof technique establishes a robust foundation for demonstrating the exponential stability of Finite-Difference-based model reductions as the discretization parameter approaches zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05306v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmet Ozkan Ozer, Ibrahim Khalilullah, Uthman Rasaq</dc:creator>
    </item>
    <item>
      <title>On optimal error rates for strong approximation of SDEs with a drift coefficient of fractional Sobolev regularity</title>
      <link>https://arxiv.org/abs/2402.13732</link>
      <description>arXiv:2402.13732v2 Announce Type: replace-cross 
Abstract: We study strong approximation of scalar additive noise driven stochastic differential equations (SDEs) at time point $1$ in the case that the drift coefficient is bounded and has Sobolev regularity $s\in(0,1)$. Recently, it has been shown in [arXiv:2101.12185v2 (2022)] that for such SDEs the equidistant Euler approximation achieves an $L^2$-error rate of at least $(1+s)/2$, up to an arbitrary small $\varepsilon$, in terms of the number of evaluations of the driving Brownian motion $W$. In the present article we prove a matching lower error bound for $s\in(1/2,1)$. More precisely we show that, for every $s\in(1/2,1)$, the $L^2$-error rate $(1+s)/2$ can, up to a logarithmic term, not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points. Up to now, this result was known in the literature only for the cases $s=1/2-$ and $s=1-$.
  For the proof we employ the coupling of noise technique recently introduced in [arXiv:2010.00915 (2020)] to bound the $L^2$-error of an arbitrary approximation from below by the $L^2$-distance of two occupation time functionals provided by a specifically chosen drift coefficient with Sobolev regularity $s$ and two solutions of the corresponding SDE with coupled driving Brownian motions. For the analysis of the latter distance we employ a transformation of the original SDE to overcome the problem of correlated increments of the difference of the two coupled solutions, occupation time estimates to cope with the lack of regularity of the chosen drift coefficient around the point $0$ and scaling properties of the drift coefficient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13732v2</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Ellinger, Thomas M\"uller-Gronbach, Larisa Yaroslavtseva</dc:creator>
    </item>
  </channel>
</rss>
