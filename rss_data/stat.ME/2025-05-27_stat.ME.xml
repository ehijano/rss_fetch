<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 May 2025 01:54:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Backward Filtering Forward Guiding</title>
      <link>https://arxiv.org/abs/2505.18239</link>
      <description>arXiv:2505.18239v1 Announce Type: new 
Abstract: We develop a general methodological framework for probabilistic inference in discrete- and continuous-time stochastic processes evolving on directed acyclic graphs (DAGs). The process is observed only at the leaf nodes, and the challenge is to infer its full latent trajectory: a smoothing problem that arises in fields such as phylogenetics, epidemiology, and signal processing. Our approach combines a backward information filtering step, which constructs likelihood-informed potentials from observations, with a forward guiding step, where a tractable process is simulated under a change of measure constructed from these potentials. This Backward Filtering Forward Guiding (BFFG) scheme yields weighted samples from the posterior distribution over latent paths and is amenable to integration with MCMC and particle filtering methods. We demonstrate that BFFG applies to both discrete- and continuous-time models, enabling probabilistic inference in settings where standard transition densities are intractable or unavailable. Our framework opens avenues for incorporating structured stochastic dynamics into probabilistic programming. We numerically illustrate our approach for a branching diffusion process on a directed tree.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18239v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frank van der Meulen, Moritz Schauer, Stefan Sommer</dc:creator>
    </item>
    <item>
      <title>Measuring the Impact of Missingness in Traffic Stop Data</title>
      <link>https://arxiv.org/abs/2505.18281</link>
      <description>arXiv:2505.18281v1 Announce Type: new 
Abstract: In this article we explore the data available through the Stanford Open Policing Project. The data consist of information on millions of traffic stops across close to 100 different cities and highway patrols. Using a variety of metrics, we identify that the data is not missing completely at random. Furthermore, we develop ways of quantifying and visualizing missingness trends for different variables across the datasets. We follow up by performing a sensitivity analysis to extend work done on the outcome test as well as to extend work done on sharp bounds on the average treatment effect. We demonstrate that bias calculations can fundamentally shift depending on the assumptions made about the observations for which the race variable has not been recorded. We suggest ways that our missingness sensitivity analysis can be extended to myriad different contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18281v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Saatvik Kher, Amber Lee, Johanna Hardin</dc:creator>
    </item>
    <item>
      <title>The tail wags the distribution: Only sample the tails for efficient reliability analysis</title>
      <link>https://arxiv.org/abs/2505.18510</link>
      <description>arXiv:2505.18510v1 Announce Type: new 
Abstract: To ensure that real-world infrastructure is safe and durable, systems are designed to not fail for any but the most rarely occurring parameter values. By only happening deep in the tails of the parameter distribution, failure probabilities are kept small. At the same time, it is essential to understand the risk associated with the failure of a system, no matter how unlikely. However, estimating such small failure probabilities is challenging; numerous system performance evaluations are necessary to produce even a single system state corresponding to failure, and each such evaluation is usually significantly computationally expensive. To alleviate this difficulty, we propose the Tail Stratified Sampling (TSS) estimator - an intuitive stratified sampling estimator for the failure probability that successively refines the tails of the system parameter distribution, enabling direct sampling of the tails, where failure is expected to occur. The most general construction of TSS is presented, highlighting its versatility and robustness for a variety of applications. The intuitions behind the formulation are explained, followed by a discussion of the theoretical and practical benefits of the method. Various details of the implementation are presented. The performance of the algorithm is then showcased through a host of analytical examples with varying failure domain geometries and failure probabilities as well as multiple numerical case studies of moderate and high dimensionality. To conclude, a qualitative comparison of TSS against the existing foundational variance-reduction methods for reliability analysis is presented, along with suggestions for future developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18510v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Promit Chakroborty, Michael D. Shields</dc:creator>
    </item>
    <item>
      <title>Multi-Layer Backward Joint Model for Dynamic Prediction of Clinical Events with Multivariate Longitudinal Predictors of Mixed Types</title>
      <link>https://arxiv.org/abs/2505.18768</link>
      <description>arXiv:2505.18768v1 Announce Type: new 
Abstract: Dynamic prediction of time-to-event outcomes using longitudinal data is highly useful in clinical research and practice. A common strategy is the joint modeling of longitudinal and time-to-event data. The shared random effect model has been widely studied for this purpose. However, it can be computationally challenging when applied to problems with a large number of longitudinal predictor variables, particularly when mixed types of continuous and categorical variables are involved. Addressing these limitations, we introduce a novel multi-layer backward joint model (MBJM). The model structure consists of multiple data layers cohesively integrated through a series of conditional distributions that involve longitudinal and time-to-event data, where the time to the clinical event is the conditioning variable. This model can be estimated with standard statistical software with rapid and robust computation, regardless of the dimension of the longitudinal predictor variables. We provide both theoretical and empirical results to show that the MBJM outperforms the static prediction model that does not fully account for the longitudinal nature of the prediction. In an empirical comparison with the shared random effects joint model, the MBJM demonstrated competitive performance with substantially faster and more robust computation. Both the simulation and real data application from a primary biliary cirrhosis study utilized seven longitudinal biomarkers, five continuous and two categorical, larger than the typically published joint modeling problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18768v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Li, Zhe Yin, Liang Li</dc:creator>
    </item>
    <item>
      <title>Regularisation of CART trees by summation of $p$-values</title>
      <link>https://arxiv.org/abs/2505.18769</link>
      <description>arXiv:2505.18769v1 Announce Type: new 
Abstract: The standard procedure to decide on the complexity of a CART regression tree is to use cross-validation with the aim of obtaining a predictor that generalises well to unseen data. The randomness in the selection of folds implies that the selected CART tree is not a deterministic function of the data. We propose a deterministic in-sample method that can be used for stopping the growing of a CART tree based on node-wise statistical tests. This testing procedure is derived using a connection to change point detection, where the null hypothesis corresponds to that there is no signal. The suggested $p$-value based procedure allows us to consider covariate vectors of arbitrary dimension and allows us to bound the $p$-value of an entire tree from above. Further, we show that the test detects a not-too-weak signal with a high probability, given a not-too-small sample size.
  We illustrate our methodology and the asymptotic results on both simulated and real world data. Additionally, we illustrate how our $p$-value based method can be used as an automatic deterministic early stopping procedure for tree-based boosting. The boosting iterations stop when the tree to be added consists only of a root node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18769v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nils Engler, Mathias Lindholm, Filip Lindskog, Taariq Nazar</dc:creator>
    </item>
    <item>
      <title>Geometric medians on product manifolds</title>
      <link>https://arxiv.org/abs/2505.18844</link>
      <description>arXiv:2505.18844v1 Announce Type: new 
Abstract: Product manifolds arise when heterogeneous geometric variables are recorded jointly. While the Fr\'{e}chet mean on Riemannian manifolds separates cleanly across factors, the canonical geometric median couples them, and its behavior in product spaces has remained largely unexplored. In this paper, we give the first systematic treatment of this problem. After formulating the coupled objective, we establish general existence and uniqueness results: the median is unique on any Hadamard product, and remains locally unique under sharp conditions on curvature and injectivity radius even when one or more factors have positive curvature. We then prove that the estimator enjoys Lipschitz stability to perturbations and the optimal breakdown point, extending classical robustness guarantees to the product-manifold setting. Two practical solvers are proposed, including a Riemannian subgradient method with global sublinear convergence and a product-aware Weiszfeld iteration that achieves local linear convergence when safely away from data singularities. Both algorithms update the factors independently while respecting the latent coupling term, enabling implementation with standard manifold primitives. Simulations on parameter spaces of univariate and multivariate Gaussian distributions endowed with the Bures-Wasserstein geometry show that the median is more resilient to contamination than the Fr\'{e}chet mean. The results provide both theoretical foundations and computational tools for robust location inference with heterogeneous manifold-valued data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18844v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kisung You</dc:creator>
    </item>
    <item>
      <title>A concordance coefficient for lattice data: An application to poverty indices in Chile</title>
      <link>https://arxiv.org/abs/2505.18935</link>
      <description>arXiv:2505.18935v1 Announce Type: new 
Abstract: This paper introduces a novel coefficient for measuring agreement between two lattice sequences observed in the same areal units, motivated by the analysis of different methodologies for measuring poverty rates in Chile. Building on the multivariate concordance coefficient framework, our approach accounts for dependencies in the multivariate lattice process using a non-negative definite matrix of weights, assuming a Multivariate Conditionally Autoregressive (GMCAR) process. We adopt a Bayesian perspective for inference, using summaries from Bayesian estimates. The methodology is illustrated through an analysis of poverty rates in the Metropolitan and Valpara\'iso regions of Chile, with High Posterior Density (HPD) intervals provided for the poverty rates. This work addresses a methodological gap in the understanding of agreement coefficients and enhances the usability of these measures in the context of social variables typically assessed in areal units.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18935v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronny Vallejos, Clemente Ferrer, Jorge Mateu</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v1 Announce Type: new 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models (Really) Need Statistical Foundations?</title>
      <link>https://arxiv.org/abs/2505.19145</link>
      <description>arXiv:2505.19145v1 Announce Type: new 
Abstract: Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19145v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>Comment on "An implementation of neural simulation-based inference for parameter estimation in ATLAS''</title>
      <link>https://arxiv.org/abs/2505.19156</link>
      <description>arXiv:2505.19156v1 Announce Type: new 
Abstract: The paper titled "An implementation of neural simulation-based inference for parameter estimation in ATLAS" by the ATLAS collaboration (arXiv:2412.01600v1 [hep-ex]) describes the implementation of neural simulation-based inference for a measurement analysis performed by ATLAS. The uncertainties in the analysis arising from the finiteness of the simulated datasets are estimated using a novel double-bootstrapping technique described in that work. In the present comment, it is claimed and demonstrated, using a toy example, that the double-bootstrapping technique does not actually capture the aforementioned uncertainties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19156v1</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prasanth Shyamsundar</dc:creator>
    </item>
    <item>
      <title>svc: An R package for Spatially Varying Coefficient Models</title>
      <link>https://arxiv.org/abs/2505.19287</link>
      <description>arXiv:2505.19287v1 Announce Type: new 
Abstract: Traditional regression models assume stationary relationships between predictors and responses, failing to capture the spatial heterogeneity present in many environmental, epidemiological, and ecological processes. To address this limitation, we develop a scalable Bayesian framework for spatially varying coefficient (SVC) models, implemented in the \pkg{svc} R package (available at https://github.com/jdta95/svc), which allows regression coefficients to vary smoothly over space. Our approach combines three key computational innovations: (1) a subset Gaussian process approximation that reduces the computational burden from $O(n^3)$ to $O(m^3)$ with $m&lt;n$, while maintaining predictive accuracy; (2) a robust adaptive Metropolis (RAM) algorithm that automatically tunes proposal distributions for efficient MCMC sampling of spatial range parameters; and (3) optimized linear algebra operations leveraging precomputed distance matrices and Cholesky decompositions to accelerate covariance calculations. We present the model's theoretical foundation, prior specification, and Gibbs sampling algorithm, with a focus on practical implementation for large spatial datasets. Simulation studies demonstrate that our method outperforms existing approaches in computational efficiency while maintaining competitive estimation accuracy. We illustrate its application in an analysis of land surface temperature (LST) data, revealing spatially varying effects of vegetation and emissivity that would be obscured by traditional regression techniques. The \pkg{svc} package provides researchers with a flexible, efficient tool for uncovering and quantifying nonstationary spatial relationships across diverse scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19287v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justice Akuoko-Frimpong, Edward Shao, Jonathan Ta</dc:creator>
    </item>
    <item>
      <title>A likelihood-based Bayesian inference framework for the calibration of and selection between stochastic velocity-jump models</title>
      <link>https://arxiv.org/abs/2505.19292</link>
      <description>arXiv:2505.19292v1 Announce Type: new 
Abstract: Advances in experimental techniques allow the collection of high-resolution spatio-temporal data that track individual motile entities over time. These tracking data can be used to calibrate mathematical models of individual motility. However, experimental data is intrinsically discrete and noisy, and complicating the calibration of models for individual motion. We consider motion of individual agents that can be described by velocity-jump models in one spatial dimension. These agents transition according to a Poisson process between an n-state network, in which each state is associated with a fixed velocity and fixed rates of switching to every other state. Exploiting an approximate solution to the resultant stochastic process, in this work we develop a corresponding Bayesian inference framework to calibrate these models to discrete-time noisy data. We first demonstrate the ability of our framework to effectively recover the model parameters of data simulated from two-state and three-state models. Moreover, we use the framework to select between three-state models with distinct networks of states. Finally, we explore the question of model selection by calibrating three-state and four-state models to data simulated from a number of different four-state models. Overall, the framework works effectively and efficiently in calibrating and selecting between velocity-jump models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19292v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arianna Ceccarelli, Alexander P. Browning, Ruth E. Baker</dc:creator>
    </item>
    <item>
      <title>Model-robust standardization in cluster-randomized trials</title>
      <link>https://arxiv.org/abs/2505.19336</link>
      <description>arXiv:2505.19336v1 Announce Type: new 
Abstract: In cluster-randomized trials, generalized linear mixed models and generalized estimating equations have conventionally been the default analytic methods for estimating the average treatment effect as routine practice. However, recent studies have demonstrated that their treatment effect coefficient estimators may correspond to ambiguous estimands when the models are misspecified or when there exists informative cluster sizes. In this article, we present a unified approach that standardizes output from a given regression model to ensure estimand-aligned inference for the treatment effect parameters in cluster-randomized trials. We introduce estimators for both the cluster-average and the individual-average treatment effects (marginal estimands) that are always consistent regardless of whether the specified working regression models align with the unknown data generating process. We further explore the use of a deletion-based jackknife variance estimator for inference. The development of our approach also motivates a natural test for informative cluster size. Extensive simulation experiments are designed to demonstrate the advantage of the proposed estimators under a variety of scenarios. The proposed model-robust standardization methods are implemented in the MRStdCRT R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19336v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fan Li, Jiaqi Tong, Xi Fang, Chao Cheng, Brennan C. Kahan, Bingkai Wang</dc:creator>
    </item>
    <item>
      <title>Hybrid Bayesian Models for Community Detection with Application to a Colombian Conflict Network</title>
      <link>https://arxiv.org/abs/2505.19399</link>
      <description>arXiv:2505.19399v1 Announce Type: new 
Abstract: We introduce a flexible Bayesian framework for clustering nodes in undirected binary networks, motivated by the need to uncover structural patterns in complex conflict environments. Building on the stochastic block model, we develop two hybrid extensions: the Class-Distance Model, which governs interaction probabilities through Euclidean distances between cluster-level latent positions, and the Class-Bilinear Model, which captures more complex relational patterns via bilinear interactions. We apply this framework to a novel network derived from the Colombian armed conflict, where municipalities are connected through the co-presence of armed actors, violence, and illicit economies. The resulting clusters align with empirical patterns of territorial control and trafficking corridors, highlighting the models' capacity to recover and explain complex governance dynamics. Full Bayesian inference is carried out via MCMC under both finite and nonparametric clustering priors. While the main application centers on the Colombian conflict, we also assess model performance using synthetic data as well as other two benchmark datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19399v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Eleni Dilma, Brenda Betancourt</dc:creator>
    </item>
    <item>
      <title>ICS for complex data with application to outlier detection for density data</title>
      <link>https://arxiv.org/abs/2505.19403</link>
      <description>arXiv:2505.19403v1 Announce Type: new 
Abstract: Invariant coordinate selection (ICS) is a dimension reduction method, used as a preliminary step for clustering and outlier detection. It has been primarily applied to multivariate data. This work introduces a coordinate-free definition of ICS in an abstract Euclidean space and extends the method to complex data. Functional and distributional data are preprocessed into a finite-dimensional subspace. For example, in the framework of Bayes Hilbert spaces, distributional data are smoothed into compositional spline functions through the Maximum Penalised Likelihood method. We describe an outlier detection procedure for complex data and study the impact of some preprocessing parameters on the results. We compare our approach with other outlier detection methods through simulations, producing promising results in scenarios with a low proportion of outliers. ICS allows detecting abnormal climate events in a sample of daily maximum temperature distributions recorded across the provinces of Northern Vietnam between 1987 and 2016.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19403v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Camille Mondon, Huong Thi Trinh, Anne Ruiz-Gazen, Christine Thomas-Agnan</dc:creator>
    </item>
    <item>
      <title>Cellwise and Casewise Robust Covariance in High Dimensions</title>
      <link>https://arxiv.org/abs/2505.19925</link>
      <description>arXiv:2505.19925v1 Announce Type: new 
Abstract: The sample covariance matrix is a cornerstone of multivariate statistics, but it is highly sensitive to outliers. These can be casewise outliers, such as cases belonging to a different population, or cellwise outliers, which are deviating cells (entries) of the data matrix. Recently some robust covariance estimators have been developed that can handle both types of outliers, but their computation is only feasible up to at most 20 dimensions. To remedy this we propose the cellRCov method, a robust covariance estimator that simultaneously handles casewise outliers, cellwise outliers, and missing data. It relies on a decomposition of the covariance on principal and orthogonal subspaces, leveraging recent work on robust PCA. It also employs a ridge-type regularization to stabilize the estimated covariance matrix. We establish some theoretical properties of cellRCov, including its casewise and cellwise influence functions as well as consistency and asymptotic normality. A simulation study demonstrates the superior performance of cellRCov in contaminated and missing data scenarios. Furthermore, its practical utility is illustrated in a real-world application to anomaly detection. We also construct and illustrate the cellRCCA method for robust and regularized canonical correlation analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19925v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Centofanti, Mia Hubert, Peter J. Rousseeuw</dc:creator>
    </item>
    <item>
      <title>Two Sample Testing for High-dimensional Functional Data: A Multi-resolution Projection Method</title>
      <link>https://arxiv.org/abs/2505.19974</link>
      <description>arXiv:2505.19974v1 Announce Type: new 
Abstract: It is of great interest to test the equality of the means in two samples of functional data. Past research has predominantly concentrated on low-dimensional functional data, a focus that may not hold up in high-dimensional scenarios. In this article, we propose a novel two-sample test for the mean functions of high-dimensional functional data, employing a multi-resolution projection (MRP) method. We establish the asymptotic normality of the proposed MRP test statistic and investigate its power performance when the dimension of the functional variables is high. In practice, functional data are observed only at discrete and usually asynchronous points. We further explore the influence of function reconstruction on our test statistic theoretically. Finally, we assess the finite-sample performance of our test through extensive simulation studies and demonstrate its practicality via two real data applications. Specifically, our analysis of global climate data uncovers significant differences in the functional means of climate variables in the years 2020-2069 when comparing intermediate greenhouse gas emission pathways (e.g., RCP4.5) to high greenhouse gas emission pathways (e.g., RCP8.5).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19974v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shouxia Wang, Jiguo Cao, Hua Liu, Jinhong You, Jicai Liu</dc:creator>
    </item>
    <item>
      <title>Causal Meta-Analysis: Rethinking the Foundations of Evidence-Based Medicine</title>
      <link>https://arxiv.org/abs/2505.20168</link>
      <description>arXiv:2505.20168v1 Announce Type: new 
Abstract: Meta-analysis, by synthesizing effect estimates from multiple studies conducted in diverse settings, stands at the top of the evidence hierarchy in clinical research. Yet, conventional approaches based on fixed- or random-effects models lack a causal framework, which may limit their interpretability and utility for public policy. Incorporating causal inference reframes meta-analysis as the estimation of well-defined causal effects on clearly specified populations, enabling a principled approach to handling study heterogeneity. We show that classical meta-analysis estimators have a clear causal interpretation when effects are measured as risk differences. However, this breaks down for nonlinear measures like the risk ratio and odds ratio. To address this, we introduce novel causal aggregation formulas that remain compatible with standard meta-analysis practices and do not require access to individual-level data. To evaluate real-world impact, we apply both classical and causal meta-analysis methods to 500 published meta-analyses. While the conclusions often align, notable discrepancies emerge, revealing cases where conventional methods may suggest a treatment is beneficial when, under a causal lens, it is in fact harmful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20168v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cl\'ement Berenfeld, Ahmed Boughdiri, B\'en\'edicte Colnet, Wouter A. C. van Amsterdam, Aur\'elien Bellet, R\'emi Khellaf, Erwan Scornet, Julie Josse</dc:creator>
    </item>
    <item>
      <title>Model-based Estimation of Difference-in-Differences with Staggered Treatments</title>
      <link>https://arxiv.org/abs/2505.18391</link>
      <description>arXiv:2505.18391v1 Announce Type: cross 
Abstract: We propose a model-based framework for estimating treatment effects in Difference-in-Differences (DiD) designs with multiple time-periods and variation in treatment timing. We first present a simple model for potential outcomes that respects the identifying conditions for the average treatment effects on the treated (ATT's). The model-based perspective is particularly valuable in applications with small sample sizes, where existing estimators that rely on asymptotic arguments may yield poor approximations to the sampling distribution of group-time ATT's. To improve parsimony and guide prior elicitation, we reparametrize the model in a way that reduces the effective number of parameters. Prior information about treatment effects is incorporated through black-box training sample priors and, in small-sample settings, by thick-tailed t-priors that shrink ATT's of small magnitudes toward zero. We provide a straightforward and computationally efficient Bayesian estimation procedure and establish a Bernstein-von Mises-type result that justifies posterior inference for the treatment effects. Simulation studies confirm that our method performs well in both large and small samples, offering credible uncertainty quantification even in settings that challenge standard estimators. We illustrate the practical value of the method through an empirical application that examines the effect of minimum wage increases on teen employment in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18391v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees</title>
      <link>https://arxiv.org/abs/2505.18659</link>
      <description>arXiv:2505.18659v1 Announce Type: cross 
Abstract: Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (\texttt{PPI}) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose \texttt{R-AutoEval+}, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of \texttt{R-AutoEval+} is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, and for prompt design in LLMs confirm the reliability and efficiency of \texttt{R-AutoEval+}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18659v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sangwoo Park, Matteo Zecchin, Osvaldo Simeone</dc:creator>
    </item>
    <item>
      <title>Optimal Intervention for Self-triggering Spatial Networks with Application to Urban Crime Analytics</title>
      <link>https://arxiv.org/abs/2505.19612</link>
      <description>arXiv:2505.19612v1 Announce Type: cross 
Abstract: In many network systems, events at one node trigger further activity at other nodes, e.g., social media users reacting to each other's posts or the clustering of criminal activity in urban environments. These systems are typically referred to as self-exciting networks. In such systems, targeted intervention at critical nodes can be an effective strategy for mitigating undesirable consequences such as further propagation of criminal activity or the spreading of misinformation on social media. In our work, we develop an optimal network intervention model to explore how targeted interventions at critical nodes can mitigate cascading effects throughout a Spatiotemporal Hawkes network. Similar models have been studied previously in the literature in purely temporal Hawkes networks, but in our work, we extend them to a spatiotemporal setup and demonstrate the efficacy of our methods by comparing the post-intervention reduction in intensity to other heuristic strategies in simulated networks. Subsequently, we use our method on crime data from the LA police department database to find neighborhoods for strategic intervention to demonstrate an application in predictive policing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19612v1</guid>
      <category>cs.SI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pramit Das, Moulinath Banerjee, Yuekai Sun</dc:creator>
    </item>
    <item>
      <title>Weighted Leave-One-Out Cross Validation</title>
      <link>https://arxiv.org/abs/2505.19737</link>
      <description>arXiv:2505.19737v1 Announce Type: cross 
Abstract: We present a weighted version of Leave-One-Out (LOO) cross-validation for estimating the Integrated Squared Error (ISE) when approximating an unknown function by a predictor that depends linearly on evaluations of the function over a finite collection of sites. The method relies on the construction of the best linear estimator of the squared prediction error at an arbitrary unsampled site based on squared LOO residuals, assuming that the function is a realization of a Gaussian Process (GP). A theoretical analysis of performance of the ISE estimator is presented, and robustness with respect to the choice of the GP kernel is investigated first analytically, then through numerical examples. Overall, the estimation of ISE is significantly more precise than with classical, unweighted, LOO cross validation. Application to model selection is briefly considered through examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19737v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1615917</arxiv:DOI>
      <arxiv:journal_reference>SIAM/ASA Journal on Uncertainty Quantification, 2024, 12 (4), pp.1213-1239</arxiv:journal_reference>
      <dc:creator>Luc Pronzato (RT-UQ), Maria-Jo\~ao Rendas</dc:creator>
    </item>
    <item>
      <title>Existence of the solution to the graphical lasso</title>
      <link>https://arxiv.org/abs/2505.20005</link>
      <description>arXiv:2505.20005v1 Announce Type: cross 
Abstract: The graphical lasso (glasso) is an $l_1$ penalised likelihood estimator for a Gaussian precision matrix. A benefit of the glasso is that it exists even when the sample covariance matrix is not positive definite but only positive semidefinite. This note collects a number of results concerning the existence of the glasso both when the penalty is applied to all entries of the precision matrix and when the penalty is only applied to the off-diagonals. New proofs are provided for these results which give insight into how the $l_1$ penalty achieves these existence properties. These proofs extend to a much larger class of penalty functions allowing one to easily determine if new penalised likelihood estimates exist for positive semidefinite sample covariance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20005v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Storror Carter</dc:creator>
    </item>
    <item>
      <title>Estimating Shapley Effects in Big-Data Emulation and Regression Settings using Bayesian Additive Regression Trees</title>
      <link>https://arxiv.org/abs/2304.03809</link>
      <description>arXiv:2304.03809v3 Announce Type: replace 
Abstract: Shapley effects are a particularly interpretable approach to assessing how a function depends on its various inputs. The existing literature contains various estimators for this class of sensitivity indices in the context of nonparametric regression where the function is observed with noise, but there does not seem to be an estimator that is computationally tractable for input dimensions in the hundreds scale. This article provides such an estimator that is computationally tractable on this scale. The estimator uses a metamodel-based approach by first fitting a Bayesian Additive Regression Trees model which is then used to compute Shapley-effect estimates. This article also establishes a theoretical guarantee of posterior consistency on a large function class for this Shapley-effect estimator. Finally, this paper explores the performance of these Shapley-effect estimators on four different test functions for various input dimensions, including $p=500$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.03809v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akira Horiguchi, Matthew T. Pratola</dc:creator>
    </item>
    <item>
      <title>Design-Based Causal Inference with Missing Outcomes: Missingness Mechanisms, Imputation-Assisted Randomization Tests, and Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2310.18556</link>
      <description>arXiv:2310.18556v5 Announce Type: replace 
Abstract: Design-based causal inference, also known as randomization-based or finite-population causal inference, is one of the most widely used causal inference frameworks, largely due to the merit that its validity can be guaranteed by study design (e.g., randomized experiments) and does not require assuming specific outcome-generating distributions or super-population models. Despite its advantages, design-based causal inference can still suffer from other issues, among which outcome missingness is a prevalent and significant challenge. This work systematically studies the outcome missingness problem in design-based causal inference. First, we propose a general and flexible outcome missingness mechanism that can facilitate finite-population-exact randomization tests of no treatment effect. Second, under this general missingness mechanism, we propose a general framework called ``imputation and re-imputation" for conducting randomization tests in design-based causal inference with missing outcomes. We prove that our framework can still ensure finite-population-exact type-I error rate control even when the imputation model was misspecified or when unobserved covariates or interference exist in the missingness mechanism. Third, we extend our framework to conduct covariate adjustment in randomization tests and construct finite-population-valid confidence regions with missing outcomes. Our framework is evaluated via extensive simulation studies and applied to a large-scale randomized experiment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18556v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siyu Heng, Jiawei Zhang, Yang Feng</dc:creator>
    </item>
    <item>
      <title>Fast Rerandomization via the BRAIN</title>
      <link>https://arxiv.org/abs/2312.17230</link>
      <description>arXiv:2312.17230v2 Announce Type: replace 
Abstract: Randomized experiments are a crucial tool for causal inference in many different fields. Rerandomization addresses any covariate imbalance in such experiments by resampling treatment assignments until certain balance criteria are satisfied. However, rerandomization based on na\"ive acceptance-rejection sampling is computationally inefficient, especially when numerous independent assignments are required to perform randomization-based statistical inference. Existing acceleration methods are suboptimal and not applicable in structured experiments, including stratified and clustered experiments. Based on metaheuristics in integer programming, we propose BRAIN -- a novel computationally-lightweight methodology that ensures covariate balance in randomized experiments while significantly accelerating the computation. Our BRAIN method provides unbiased treatment effect estimators with reduced variance compared to complete randomization, preserving the desirable statistical properties of traditional rerandomization. Simulation studies and a real data example demonstrate the benefits of our method in fast sampling while retaining the appealing statistical guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17230v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiuyao Lu, Daogao Liu, Zhanran Lin, Xiaomeng Wang</dc:creator>
    </item>
    <item>
      <title>Gaussian distributional structural equation models: A framework for modeling latent heteroscedasticity</title>
      <link>https://arxiv.org/abs/2404.14124</link>
      <description>arXiv:2404.14124v2 Announce Type: replace 
Abstract: Accounting for the complexity of psychological theories requires methods that can predict not only changes in the means of latent variables -- such as personality factors, creativity, or intelligence -- but also changes in their variances.
  Structural equation modeling (SEM) is the framework of choice for analyzing complex relationships among latent variables, but the modeling of latent variances as a function of other latent variables is a task that current methods only support to a limited extent.
  In this paper, we develop a Bayesian framework for Gaussian distributional SEM which broadens the scope of feasible models for latent heteroscedasticity. We use statistical simulation to validate our framework across four distinct model structures, in which we demonstrate that reliable statistical inferences can be achieved and that computation can be performed with sufficient efficiency for practical everyday use. We illustrate our framework's applicability in a real-world case study that addresses a substantive hypothesis from personality psychology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14124v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/00273171.2025.2483252</arxiv:DOI>
      <dc:creator>Luna Fazio, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Design-based H\'ajek estimation for clustered and stratified experiments</title>
      <link>https://arxiv.org/abs/2406.10473</link>
      <description>arXiv:2406.10473v3 Announce Type: replace 
Abstract: Random allocation is essential for causal inference, but practical constraints often require assigning participants in clusters. They may be stratified pre-assignment, either of necessity or to reduce differences between treatment and control groups; but combining clustered assignment with blocking into pairs, triples, or other fine strata makes otherwise equivalent estimators perform quite differently. The two-way ANOVA with block effects can be inconsistent, as can another popular, seemingly innocuous estimator. In contrast, H\'ajek estimation remains broadly consistent for sample average treatment effects, but lacks a design-based standard error applicable with clusters and fine strata. To fill this gap, we offer a new variance estimator and establish its consistency. Analytic and simulation results recommend a hybrid of it and Neyman's estimator for designs with both small and large strata. We extend the H\'ajek estimator to accommodate covariates and adapt variance estimators to inherit Neyman-style conservativeness, at least for hypothesis testing. Further simulations suggest that with heterogeneous treatment effects, our combination of novelties is necessary and sufficient to maintain coverage in small-$n$ designs; the relevant $n$ being that of clusters, many large-scale studies are small-$n$. We consider two: a paired, aggregate-data nutritional study and an education study with student covariates and varying block sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10473v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinhe Wang, Ben B. Hansen</dc:creator>
    </item>
    <item>
      <title>Principal component analysis balancing prediction and approximation accuracy for spatial data</title>
      <link>https://arxiv.org/abs/2408.01662</link>
      <description>arXiv:2408.01662v3 Announce Type: replace 
Abstract: Dimension reduction is often the first step in statistical modeling or prediction of multivariate spatial data. However, most existing dimension reduction techniques do not account for the spatial correlation between observations and do not take the downstream modeling task into consideration when finding the lower-dimensional representation. We formalize the closeness of approximation to the original data and the utility of lower-dimensional scores for downstream modeling as two complementary, sometimes conflicting, metrics for dimension reduction. We illustrate how existing methodologies fall into this framework and propose a flexible dimension reduction algorithm that achieves the optimal trade-off. We derive a computationally simple form for our algorithm and illustrate its performance through simulation studies, as well as two applications in air pollution modeling and spatial transcriptomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01662v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Si Cheng, Magali N. Blanco, Timothy V. Larson, Lianne Sheppard, Adam Szpiro, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Spatial Hyperspheric Models for Compositional Data</title>
      <link>https://arxiv.org/abs/2410.03648</link>
      <description>arXiv:2410.03648v3 Announce Type: replace 
Abstract: Compositional observations are an increasingly prevalent data source in spatial statistics. Analysis of such data is typically done on log-ratio transformations or via Dirichlet regression. However, these approaches often make unnecessarily strong assumptions (e.g., strictly positive components, exclusively negative correlations). An alternative approach uses square-root transformed compositions and directional distributions. Such distributions naturally allow for zero-valued components and positive correlations, yet they may include support outside the non-negative orthant and are not generative for compositional data. To overcome this challenge, we truncate the elliptically symmetric angular Gaussian (ESAG) distribution to the non-negative orthant. Additionally, we propose a spatial hyperspheric regression model that contains fixed and random multivariate spatial effects. The proposed model also contains a term that can be used to propagate uncertainty that may arise from precursory stochastic models (i.e., machine learning classification). We used our model in a simulation study and for a spatial analysis of classified bioacoustic signals of the Dryobates pubescens (downy woodpecker).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03648v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Schwob, Mevin B. Hooten, Nicholas M. Calzada, Timothy H. Keitt</dc:creator>
    </item>
    <item>
      <title>Scalable Bayesian Semiparametric Additive Regression Models For Microbiome Studies</title>
      <link>https://arxiv.org/abs/2410.03911</link>
      <description>arXiv:2410.03911v2 Announce Type: replace 
Abstract: Statistical analysis of microbiome data is challenging. Bayesian multinomial logistic-normal (MLN) models have gained popularity due to their ability to account for the count compositional nature of these data, but existing approaches are either computationally intractable or restricted to purely parametric or non-parametric methods, which limit their flexibility and scalability. In this work, we introduce \textit{MultiAddGPs}, a novel semi-parametric framework that integrates additive Gaussian Process (GP) regression within a Bayesian MLN model to disentangle linear and non-linear covariate effects, including non-stationary dynamics. Our approach builds on the computationally efficient Collapse-Uncollapse (CU) sampler and additive GP regression, introducing a novel back-sampling algorithm and marginal likelihood approximation for efficient inference and hyperparameter estimation. Our models are over 240,000 times faster than alternatives while simultaneously producing more accurate posterior estimates. Additionally, we incorporate non-stationary kernel functions designed to model treatment interventions and disease effects. We demonstrate our approach using simulated and real data studies and produce novel biological insights from a previously published human gut microbiome study. Our methods are publicly available as part of the \textit{fido} software package on CRAN \footnotemark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03911v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tinghua Chen, Michelle Pistner Nixon, Justin D. Silverman</dc:creator>
    </item>
    <item>
      <title>Flexible space-time models for extreme data</title>
      <link>https://arxiv.org/abs/2411.19184</link>
      <description>arXiv:2411.19184v2 Announce Type: replace 
Abstract: Extreme value analysis is an essential methodology in the study of rare and extreme events, which hold significant interest in various fields, particularly in the context of environmental sciences. Models that employ the exceedances of values above suitably selected high thresholds possess the advantage of capturing the "sub-asymptotic" dependence of data. This paper presents an extension of spatial random scale mixture models to the spatio-temporal domain. A comprehensive framework for characterizing the dependence structure of extreme events across both dimensions is provided. Indeed, the model is capable of distinguishing between asymptotic dependence and independence, both in space and time, through the use of parametric inference. The high complexity of the likelihood function for the proposed model necessitates a simulation approach based on neural networks for parameter estimation, which leverages summaries of the sub-asymptotic dependence present in the data. The effectiveness of the model in assessing the limiting dependence structure of spatio-temporal processes is demonstrated through both simulation studies and an application to rainfall datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19184v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Dell'Oro, Carlo Gaetan</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for LLM-Based Survey Simulations</title>
      <link>https://arxiv.org/abs/2502.17773</link>
      <description>arXiv:2502.17773v3 Announce Type: replace 
Abstract: We investigate the use of large language models (LLMs) to simulate human responses to survey questions, and perform uncertainty quantification to gain reliable insights. Our approach converts imperfect LLM-simulated responses into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17773v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengpiao Huang, Yuhang Wu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Kernel-based estimators for functional causal effects</title>
      <link>https://arxiv.org/abs/2503.05024</link>
      <description>arXiv:2503.05024v3 Announce Type: replace 
Abstract: We propose causal effect estimators based on empirical Fr\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators.
  Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05024v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yordan P. Raykov, Hengrui Luo, Justin D. Strait, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>On Bessel's Correction: Unbiased Sample Variance, the Bariance, and a Novel Runtime-Optimized Estimator</title>
      <link>https://arxiv.org/abs/2503.22333</link>
      <description>arXiv:2503.22333v5 Announce Type: replace 
Abstract: Bessel's correction adjusts the denominator in the sample variance formula from n to n-1 to ensure an unbiased estimator of the population variance. This paper provides rigorous algebraic derivations geometric interpretations and visualizations to reinforce the necessity of this correction. It further introduces the concept of Bariance an alternative dispersion measure based on pairwise squared differences that avoids reliance on the arithmetic mean. Building on this we address practical concerns raised in Rosenthals article [Rosenthal, 2015] which advocates for n-based estimates from a mean squared error (MSE) perspective particularly in pedagogical contexts and specific applied settings. Finally, the empirical component of this work based on simulation studies demonstrates that estimating the population variance via an algebraically optimized Bariance approach can yield a computational advantage. Specifically the unbiased Bariance estimator can be computed in linear time resulting in shorter runtimes while preserving statistical validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22333v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Using Time Structure to Estimate Causal Effects</title>
      <link>https://arxiv.org/abs/2504.11076</link>
      <description>arXiv:2504.11076v2 Announce Type: replace 
Abstract: There exist several approaches for estimating causal effects in time series when latent confounding is present. Many of these approaches rely on additional auxiliary observed variables or time series such as instruments, negative controls or time series that satisfy the front- or backdoor criterion in certain graphs. In this paper, we present a novel approach for estimating direct (and via Wright's path rule total) causal effects in a time series setup which does not rely on additional auxiliary observed variables or time series. This approach assumes that the underlying time series is a Structural Vector Autoregressive (SVAR) process and estimates direct causal effects by solving certain linear equation systems made up of different covariances and model parameters. We state sufficient graphical criteria in terms of the so-called full time graph under which these linear equations systems are uniquely solvable and under which their solutions contain the to-be-identified direct causal effects as components. We also state sufficient lag-based criteria under which the previously mentioned graphical conditions are satisfied and, thus, under which direct causal effects are identifiable. Several numerical experiments underline the correctness and applicability of our results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11076v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tom Hochsprung, Jakob Runge, Andreas Gerhardus</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Fusion of Many Treatments for Policy Learning</title>
      <link>https://arxiv.org/abs/2505.08092</link>
      <description>arXiv:2505.08092v2 Announce Type: replace 
Abstract: Individualized treatment rules/recommendations (ITRs) aim to improve patient outcomes by tailoring treatments to the characteristics of each individual. However, when there are many treatment groups, existing methods face significant challenges due to data sparsity within treatment groups and highly unbalanced covariate distributions across groups. To address these challenges, we propose a novel calibration-weighted treatment fusion procedure that robustly balances covariates across treatment groups and fuses similar treatments using a penalized working model. The fusion procedure ensures the recovery of latent treatment group structures when either the calibration model or the outcome model is correctly specified. In the fused treatment space, practitioners can seamlessly apply state-of-the-art ITR learning methods with the flexibility to utilize a subset of covariates, thereby achieving robustness while addressing practical concerns such as fairness. We establish theoretical guarantees, including consistency, the oracle property of treatment fusion, and regret bounds when integrated with multi-armed ITR learning methods such as policy trees. Simulation studies show superior group recovery and policy value compared to existing approaches. We illustrate the practical utility of our method using a nationwide electronic health record-derived de-identified database containing data from patients with Chronic Lymphocytic Leukemia and Small Lymphocytic Lymphoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08092v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ke Zhu, Jianing Chu, Ilya Lipkovich, Wenyu Ye, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Adaptive Inference through Bayesian and Inverse Bayesian Inference with Symmetry-Bias in Nonstationary Environments</title>
      <link>https://arxiv.org/abs/2505.12796</link>
      <description>arXiv:2505.12796v3 Announce Type: replace 
Abstract: This study introduces a novel inference framework, designated as Bayesian and inverse Bayesian (BIB) inference, which concurrently performs both conventional and inverse Bayesian updates by integrating symmetry bias into Bayesian inference. The effectiveness of the model was evaluated through a sequential estimation task involving observations sampled from a Gaussian distribution with a stochastically time-varying mean. Conventional Bayesian inference entails a fundamental trade-off between adaptability to abrupt environmental shifts and estimation accuracy during stable intervals. The BIB framework addresses this limitation by dynamically modulating the learning rate through inverse Bayesian updates, thereby enhancing adaptive flexibility. The BIB model generated spontaneous bursts in the learning rate during sudden environmental transitions, transiently entering a high-sensitivity state to accommodate incoming data. This intermittent burst-relaxation pattern functions as a dynamic mechanism that balances adaptability and accuracy. Further analysis of burst interval distributions demonstrated that the BIB model consistently produced power-law distributions under diverse conditions. Such robust scaling behavior, absent in conventional Bayesian inference, appears to emerge from a self-regulatory mechanism driven by inverse Bayesian updates. These results present a novel computational perspective on scale-free phenomena in natural systems and offer implications for designing adaptive inference systems in nonstationary environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12796v3</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuji Shinohara, Daiki Morita, Hayato Hirai, Ryosuke Kuribayashi, Nobuhito Manome, Toru Moriyama, Yoshihiro Nakajima, Yukio-Pegio Gunji, Ung-il Chung</dc:creator>
    </item>
    <item>
      <title>A novel framework for detecting multiple change points in functional data sequences</title>
      <link>https://arxiv.org/abs/2505.15188</link>
      <description>arXiv:2505.15188v2 Announce Type: replace 
Abstract: Detecting multiple change points in functional data sequences has been increasingly popular and critical in various scientific fields. In this article, we propose a novel two-stage framework for detecting multiple change points in functional data sequences, named as detection by Group Selection and Partial F-test (GS-PF). The detection problem is firstly transformed into a high-dimensional sparse estimation problem via functional basis expansion, and the penalized group selection is applied to estimate the number and locations of candidate change points in the first stage. To further circumvent the issue of overestimating the true number of change points in practice, a partial F-test is applied in the second stage to filter redundant change points so that the false discovery rate of the F-test for multiple change points is controlled. Additionally, in order to reduce complexity of the proposed GS-PF method, a link parameter is adopted to generate candidate sets of potential change points, which greatly reduces the number of detected change points and improves the efficiency. Asymptotic results are established and validated to guarantee detection consistency of the proposed GS-PF method, and its performance is evaluated through intensive simulations and real data analysis, compared with the state-of-the-art detecting methods. Our findings indicate that the proposed GS-PF method exhibits detection consistency in different scenarios, which endows our method with the capability for efficient and robust detection of multiple change points in functional data sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.15188v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqing Fang, Xin Liu</dc:creator>
    </item>
    <item>
      <title>Inference for the proportional odds cumulative logit model with monotonicity constraints for ordinal predictors and ordinal response</title>
      <link>https://arxiv.org/abs/2107.04946</link>
      <description>arXiv:2107.04946v4 Announce Type: replace-cross 
Abstract: The proportional odds cumulative logit model (POCLM) is a standard regression model for an ordinal response. Ordinality of predictors can be incorporated by monotonicity constraints for the corresponding parameters. It is shown that estimators defined by optimization, such as maximum likelihood estimators, for an unconstrained model and for parameters in the interior set of the parameter space of a constrained model are asymptotically equivalent. This is used in order to derive asymptotic confidence regions and tests for the constrained model, involving simple modifications for finite samples. The finite sample coverage probability of the confidence regions is investigated by simulation. Tests concern the effect of individual variables, monotonicity, and a specified monotonicity direction. The methodology is applied on real data related to the assessment of school performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.04946v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Espinosa-Brito, Christian Hennig</dc:creator>
    </item>
    <item>
      <title>Optimal Decision Rules when Payoffs are Partially Identified</title>
      <link>https://arxiv.org/abs/2204.11748</link>
      <description>arXiv:2204.11748v3 Announce Type: replace-cross 
Abstract: We derive asymptotically optimal statistical decision rules for discrete choice problems when payoffs depend on a partially-identified parameter $\theta$ and the decision maker can use a point-identified parameter $\mu$ to deduce restrictions on $\theta$. Examples include treatment choice under partial identification and pricing with rich unobserved heterogeneity. Our notion of optimality combines a minimax approach to handle the ambiguity from partial identification of $\theta$ given $\mu$ with an average risk minimization approach for $\mu$. We show how to implement optimal decision rules using the bootstrap and (quasi-)Bayesian methods in both parametric and semiparametric settings. We provide detailed applications to treatment choice and optimal pricing. Our asymptotic approach is well suited for realistic empirical settings in which the derivation of finite-sample optimal rules is intractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.11748v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy Christensen, Hyungsik Roger Moon, Frank Schorfheide</dc:creator>
    </item>
    <item>
      <title>Cluster Random Fields and Random-Shift Representations</title>
      <link>https://arxiv.org/abs/2206.15064</link>
      <description>arXiv:2206.15064v3 Announce Type: replace-cross 
Abstract: Cluster random fields (CRFs) play a crucial role in the study of extremes of stationary regularly varying random fields (RFs). In particular, they appear in the Rosi\'nski representation of max-stable and $\alpha$-stable RFs. In this contribution we introduce CRFs in an abstract setting proving that they are crucial for the construction of shift-generated classes of $\alpha$-homogeneous RFs. Further, we investigate the relations between CRFs, tail RFs} and spectral tail RFs. Applications discussed in this contribution include new representations of extremal functional indices and purely dissipative max-stable RFs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.15064v3</guid>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10959-025-01420-1</arxiv:DOI>
      <dc:creator>Enkelejd Hashorva</dc:creator>
    </item>
    <item>
      <title>Prediction-Powered E-Values</title>
      <link>https://arxiv.org/abs/2502.04294</link>
      <description>arXiv:2502.04294v2 Announce Type: replace-cross 
Abstract: Quality statistical inference requires a sufficient amount of data, which can be missing or hard to obtain. To this end, prediction-powered inference has risen as a promising methodology, but existing approaches are largely limited to Z-estimation problems such as inference of means and quantiles. In this paper, we apply ideas of prediction-powered inference to e-values. By doing so, we inherit all the usual benefits of e-values -- such as anytime-validity, post-hoc validity and versatile sequential inference -- as well as greatly expand the set of inferences achievable in a prediction-powered manner. In particular, we show that every inference procedure that can be framed in terms of e-values has a prediction-powered counterpart, given by our method. We showcase the effectiveness of our framework across a wide range of inference tasks, from simple hypothesis testing and confidence intervals to more involved procedures for change-point detection and causal discovery, which were out of reach of previous techniques. Our approach is modular and easily integrable into existing algorithms, making it a compelling choice for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04294v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Csillag, Claudio Jos\'e Struchiner, Guilherme Tegoni Goedert</dc:creator>
    </item>
  </channel>
</rss>
