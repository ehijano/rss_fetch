<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.comp-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.comp-ph</link>
    <description>physics.comp-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.comp-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 05:58:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Moment-preserving Monte-Carlo Coulomb collision method for particle codes</title>
      <link>https://arxiv.org/abs/2407.19151</link>
      <description>arXiv:2407.19151v1 Announce Type: new 
Abstract: Binary-pairing Monte-Carlo methods are widely used in particle-in-cell codes to capture effects of small angle Coulomb collisions. These methods preserve momentum and energy exactly when the simulation particles have equal weights. However, when the interacting particles are of varying weight, these physical conservation laws are only preserved on average. Here, we 1) extend these methods to weighted particles such that the scattering physics is correct on average, and 2) describe a new method for adjusting the particle velocities post scatter to restore exact conservation of momentum and energy. The efficacy of the model is illustrated with various test problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19151v1</guid>
      <category>physics.comp-ph</category>
      <category>physics.plasm-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Ray Angus, Yichen Fu, Vasily Geyko, Dave Grote, David Larson</dc:creator>
    </item>
    <item>
      <title>Near-Isotropic Sub-{\AA}ngstrom 3D Resolution Phase Contrast Imaging Achieved by End-to-End Ptychographic Electron Tomography</title>
      <link>https://arxiv.org/abs/2407.19407</link>
      <description>arXiv:2407.19407v1 Announce Type: new 
Abstract: Three-dimensional atomic resolution imaging using transmission electron microscopes is a unique capability that requires challenging experiments. Linear electron tomography methods are limited by the missing wedge effect, requiring a high tilt range. Multislice ptychography can achieve deep sub-{\AA}ngstrom resolution in the transverse direction, but the depth resolution is limited to 2 to 3 nanometers. In this paper, we propose and demonstrate an end-to-end approach to reconstructing the electrostatic potential volume of the sample directly from the 4D-STEM datasets. End-to-end multi-slice ptychographic tomography recovers several slices at each tomography tilt angle and compensates for the missing wedge effect. The algorithm is initially tested in simulation with a Pt@$\mathrm{Al_2O_3}$ core-shell nanoparticle, where both heavy and light atoms are recovered in 3D from an unaligned 4D-STEM tilt series with a restricted tilt range of 90 degrees. We also demonstrate the algorithm experimentally, recovering a Te nanoparticle with sub-{\AA}ngstrom resolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19407v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengboy You, Andrey Romanov, Philipp Pelz</dc:creator>
    </item>
    <item>
      <title>Multi-GPU RI-HF Energies and Analytic Gradients -- Towards High Throughput \textit{Ab Initio} Molecular Dynamics</title>
      <link>https://arxiv.org/abs/2407.19614</link>
      <description>arXiv:2407.19614v1 Announce Type: new 
Abstract: This article presents an optimized algorithm and implementation for calculating resolution-of-the-identity Hartree-Fock (RI-HF) energies and analytic gradients using multiple Graphics Processing Units (GPUs). The algorithm is especially designed for high throughput \emph{ab initio} molecular dynamics simulations of small and medium size molecules (10-100 atoms). Key innovations of this work include the exploitation of multi-GPU parallelism and a workload balancing scheme that efficiently distributes computational tasks among GPUs. Our implementation also employs techniques for symmetry utilization, integral screening and leveraging sparsity to optimize memory usage and computational efficiency. Computational results show that the implementation achieves significant performance improvements, including over $3\times$ speedups in single GPU AIMD throughput compared to previous GPU-accelerated RI-HF and traditional HF methods. Furthermore, utilizing multiple GPUs can provide super-linear speedup when the additional aggregate GPU memory allows for the storage of decompressed three-center integrals. Additionally, we report strong scaling efficiencies for systems up to 1000 basis functions and demonstrate practical applications through extensive performance benchmarks on up to quadruple-$\zeta$ primary basis sets, achieving floating-point performance of up to 47\% of the theoretical peak on a 4$\times$A100 GPU node.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19614v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.chem-ph</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ryan Stocks, Elise Palethorpe, Giuseppe M. J. Barca</dc:creator>
    </item>
    <item>
      <title>Reinforcement learning for anisotropic p-adaptation and error estimation in high-order solvers</title>
      <link>https://arxiv.org/abs/2407.19000</link>
      <description>arXiv:2407.19000v1 Announce Type: cross 
Abstract: We present a novel approach to automate and optimize anisotropic p-adaptation in high-order h/p solvers using Reinforcement Learning (RL). The dynamic RL adaptation uses the evolving solution to adjust the high-order polynomials. We develop an offline training approach, decoupled from the main solver, which shows minimal overcost when performing simulations. In addition, we derive a RL-based error estimation approach that enables the quantification of local discretization errors. The proposed methodology is agnostic to both the computational mesh and the partial differential equation being solved.
  The application of RL to mesh adaptation offers several benefits. It enables automated, adaptive mesh refinement, reducing the need for manual intervention. It optimizes computational resources by dynamically allocating high-order polynomials where necessary and minimizing refinement in stable regions. This leads to computational cost savings while maintaining solution accuracy. Furthermore, RL allows for the exploration of unconventional mesh adaptations, potentially enhancing the accuracy and robustness of simulations. This work extends our original research, offering a more robust, reproducible, and generalizable approach applicable to complex three-dimensional problems. We provide validation for laminar and turbulent cases: circular cylinders, Taylor Green Vortex and a 10MW wind turbine to illustrate the flexibility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19000v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Huergo, Mart\'in de Frutos, Eduardo Jan\'e, Oscar A. Marino, Gonzalo Rubio, Esteban Ferrer</dc:creator>
    </item>
    <item>
      <title>Generative Flow Networks in Covariant Loop Quantum Gravity</title>
      <link>https://arxiv.org/abs/2407.19036</link>
      <description>arXiv:2407.19036v1 Announce Type: cross 
Abstract: Spin foams arose as the covariant (path integral) formulation of quantum gravity depicting transition amplitudes between different quantum geometry states. As such, they provide a scheme to study the no boundary proposal, specifically the nothing to something transition and compute relevant observables using high performance computing (HPC). Following recent advances, where stochastic algorithms (Markov Chain Monte Carlo-MCMC) were used, we employ Generative Flow Networks, a newly developed machine learning algorithm to compute the expectation value of the dihedral angle for a 4-simplex and compare the results with previous works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19036v1</guid>
      <category>gr-qc</category>
      <category>hep-th</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Bunao, Pietropaolo Frisoni, Athanasios Kogios, Jared Wogan</dc:creator>
    </item>
    <item>
      <title>Exploring the soft pinning effect in the dynamics and the structure dynamics correlation in multicomponent supercooled liquids</title>
      <link>https://arxiv.org/abs/2407.19189</link>
      <description>arXiv:2407.19189v1 Announce Type: cross 
Abstract: We study multicomponent liquids by increasing the mass of $15\%$ of the particles in a binary Kob-Andersen model. We find that the heavy particles have dual effects on the lighter particles. At higher temperatures, there is a significant decoupling of the dynamics between heavier and lighter particles, with the former resembling a pinned particle to the latter. The dynamics of the lighter particles slow down due to the excluded volume around the nearly immobile heavier particles. Conversely, at lower temperatures, there is a coupling between the dynamics of the heavier and lighter particles. The heavier particles' mass slows down the dynamics of both types of particles. This makes the soft pinning effect of the heavy particles questionable in this regime. We demonstrate that as the mass of the heavy particles increases, the coupling of the dynamics between the lighter and heavier particles weakens. Consequently, the heavier the mass of the heavy particles, the more effectively they act as soft pinning centres in both high and low-temperature regimes. A key finding is that akin to the pinned system, the self and collective dynamics of the lighter particles decouple from each other as the mass of the heavy particles has a more pronounced impact on the latter. We analyze the structure dynamics correlation by considering the system under the binary and modified quaternary framework, the latter describing the pinned system. Our findings indicate that whenever the heavy mass particles function as soft pinning centres, the modified quaternary framework predicts a higher correlation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19189v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <category>physics.flu-dyn</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ehtesham Anwar, Palak Patel, Mohit Sharma, Sarika Maitra Bhattacharyya</dc:creator>
    </item>
    <item>
      <title>How to define temperature in active systems</title>
      <link>https://arxiv.org/abs/2407.19281</link>
      <description>arXiv:2407.19281v1 Announce Type: cross 
Abstract: We are used to measure temperature with a thermometer and we know from everyday life that different types of thermometers measure the same temperature. This experience can be based on equilibrium thermodynamics, which explains the equivalence of different possibilities to define temperature. In contrast, for systems out of equilibrium such as active matter, measurements performed with different thermometers can generally lead to different temperature values. In the present work, we systematically compare different possibilities to define temperature for active systems. Based on simulations and theory for inertial active Brownian particles, we find that different temperatures generally lead to different temperature values, as expected. Remarkably, however, we find that different temperatures not only lead to the same values near equilibrium (low P\'eclet number or high particle mass), but even far from equilibrium and in cases where entropy production is large, several different temperatures approximately coincide. In particular, we find that the kinetic temperature, the configurational temperature, and temperatures based on higher moments of the velocity distribution constitute a class of temperatures that all assume very similar values over a wide parameter range. The effective temperature also leads to similar temperature values but only in a limited parameter regime. Notably, temperatures exploiting the virial theorem, the Stokes-Einstein relation, or a harmonic confinement form a second class of temperatures whose values approximately coincide with each other but which strongly differ from those of the first class. Finally, we identify advantages and disadvantages of the different possibilities to define temperature and discuss their relevance for measuring the temperature of an active system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19281v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lukas Hecht, Lorenzo Caprini, Hartmut L\"owen, Benno Liebchen</dc:creator>
    </item>
    <item>
      <title>Nearest-Neighbours Neural Network architecture for efficient sampling of statistical physics models</title>
      <link>https://arxiv.org/abs/2407.19483</link>
      <description>arXiv:2407.19483v1 Announce Type: cross 
Abstract: The task of sampling efficiently the Gibbs-Boltzmann distribution of disordered systems is important both for the theoretical understanding of these models and for the solution of practical optimization problems. Unfortunately, this task is known to be hard, especially for spin glasses at low temperatures. Recently, many attempts have been made to tackle the problem by mixing classical Monte Carlo schemes with newly devised Neural Networks that learn to propose smart moves. In this article we introduce the Nearest-Neighbours Neural Network (4N) architecture, a physically-interpretable deep architecture whose number of parameters scales linearly with the size of the system and that can be applied to a large variety of topologies. We show that the 4N architecture can accurately learn the Gibbs-Boltzmann distribution for the two-dimensional Edwards-Anderson model, and specifically for some of its most difficult instances. In particular, it captures properties such as the energy, the correlation function and the overlap probability distribution. Finally, we show that the 4N performance increases with the number of layers, in a way that clearly connects to the correlation length of the system, thus providing a simple and interpretable criterion to choose the optimal depth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19483v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luca Maria Del Bono, Federico Ricci-Tersenghi, Francesco Zamponi</dc:creator>
    </item>
    <item>
      <title>Utility of High-Order Scheme for Unsteady Flow Simulations: Comparison with Second-Order Tool</title>
      <link>https://arxiv.org/abs/2407.19764</link>
      <description>arXiv:2407.19764v1 Announce Type: cross 
Abstract: The objective of this work is to investigate the utility and effectiveness of the high-order scheme for simulating unsteady turbulent flows. To achieve it, the studies were conducted from two perspectives: (i) the ability of different numerical schemes for turbulence problems under the same set of meshes; and (ii) the accuracy and stability of higher-order schemes for solving turbulence statistics for different mesh types (hexahedral, tetrahedral, and polyhedral cells). The simulations employ the third-order scheme for spatial discretization of the governing equations, while a widely-used second-order solver, namely pisoFoam, was employed for comparison. This study considers the canonical cases of the Taylor-Green vortex (TGV) problem at Re=100, 1600 and flow past a sphere at Re=10000 to address the aforementioned two key issues. For the TGV case, the high-order model significantly improves the numerical accuracy with convergence rates and reduces the numerical dissipation of nearly 1/10 of pisoFoam. In the latter case, the high-order scheme with large-eddy simulation (LES) accurately predicts the vortex structures and the flow instability, regardless of grid type. However, pisoFoam is found to be sensitive to mesh types, which results in numerous non-physical structures in the flow field due to numerical noise rather than flow physics, particularly for tetrahedral cells. Furthermore, for the typical low- and high-order flow statistics, the numerical results predicted by the present model show better agreement with the reference data and have less dependence on the type of grids compared with the conventional scheme. In addition, the obtained energy spectrum by the high-order solver accurately captures the Kelvin-Helmholtz (K-H) instability and the vortex shedding frequency, while these important features are less pronounced by the traditional low-order model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19764v1</guid>
      <category>physics.flu-dyn</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peng Jiang, Yichen Huang, Yong Cao, Shijun Liao, Bin Xie</dc:creator>
    </item>
    <item>
      <title>A feasible dose-volume estimation of radiotherapy treatment with optimal transport using a concept for transportation of Ricci-flat time-varying dose-volume</title>
      <link>https://arxiv.org/abs/2407.19876</link>
      <description>arXiv:2407.19876v1 Announce Type: cross 
Abstract: In radiotherapy, the dose-volume histogram (DVH) curve is an important means of evaluating the clinical feasibility of tumor control and side effects in normal organs against actual treatment. Fractionation, distributing the amounts of irradiation, is used to enhance the treatment effectiveness of tumor control and mitigation of normal tissue damage. Therefore, dose and volume receive time-varying effects per fractional treatment event. However, the difficulty of DVH superimposition of different situations prevents evaluation of the total DVH despite different shapes and receiving dose distributions of organs in each fraction. However, an actual evaluation is determined traditionally by the initial treatment plan because of summation difficulty. Mathematically, this difficulty can be regarded as a kind of optimal transport of DVH. For this study, we introduced DVH transportation on the curvilinear orthogonal space with respect to arbitrary time ($T$), time-varying dose ($D$), and time-varying volume ($V$), which was designated as the TDV space embedded in the Riemannian manifold.Transportation in the TDV space should satisfy the following: (a) the metrics between dose and volume must be equivalent for any fractions and (b) the cumulative characteristic of DVH must hold irrespective of the lapse of time. With consideration of the Ricci-flat condition for the $D$-direction and $V$-direction, we obtained the probability density distribution, which is described by Poisson's equation with radial diffusion process toward $T$. This geometrical requirement and transportation equation rigorously provided the feasible total DVH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19876v1</guid>
      <category>physics.med-ph</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yusuke Anetai, Jun'ichi Kotoku</dc:creator>
    </item>
    <item>
      <title>Extreme time extrapolation capabilities and thermodynamic consistency of physics-inspired Neural Networks for the 3D microstructure evolution of materials</title>
      <link>https://arxiv.org/abs/2407.20126</link>
      <description>arXiv:2407.20126v1 Announce Type: cross 
Abstract: A Convolutional Recurrent Neural Network (CRNN) is trained to reproduce the evolution of the spinodal decomposition process in three dimensions as described by the Cahn-Hilliard equation. A specialized, physics-inspired architecture is proven to provide close accordance between the predicted evolutions and the ground truth ones obtained via conventional integration schemes. The method can closely reproduce the evolution of microstructures not represented in the training set at a fraction of the computational costs. Extremely long-time extrapolation capabilities are achieved, up to reaching the theoretically expected equilibrium state of the system, despite the training set containing only relatively-short, initial phases of the evolution. Quantitative accordance with the decay rate of the Free energy is also demonstrated up to late coarsening stages, providing an example of a data-driven, physically consistent and high-accuracy Machine Learning method for the long timescale simulation of materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20126v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Lanzoni, Andrea Fantasia, Roberto Bergamaschini, Olivier Pierre-Louis, Francesco Montalenti</dc:creator>
    </item>
    <item>
      <title>Shock Hugoniot calculations using on-the-fly machine learned force fields with ab initio accuracy</title>
      <link>https://arxiv.org/abs/2407.15290</link>
      <description>arXiv:2407.15290v2 Announce Type: replace 
Abstract: We present a framework for computing the shock Hugoniot using on-the-fly machine learned force field (MLFF) molecular dynamics simulations. In particular, we employ an MLFF model based on the kernel method and Bayesian linear regression to compute the electronic free energy, atomic forces, and pressure; in conjunction with a linear regression model between the electronic internal and free energies to compute the internal energy, with all training data generated from Kohn-Sham density functional theory (DFT). We verify the accuracy of the formalism by comparing the Hugoniot for carbon with recent Kohn-Sham DFT results in the literature. In so doing, we demonstrate that Kohn-Sham calculations for the Hugoniot can be accelerated by up to two orders of magnitude, while retaining ab initio accuracy. We apply this framework to calculate the Hugoniots of 14 materials in the FPEOS database, comprising 9 single elements and 5 compounds, between temperatures of 10 kK and 2 MK. We find good agreement with first principles results in the literature while providing tighter error bars. In addition, we confirm that the inter-element interaction in compounds decreases with temperature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15290v2</guid>
      <category>physics.comp-ph</category>
      <category>physics.plasm-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shashikant Kumar, John E. Pask, Phanish Suryanarayana</dc:creator>
    </item>
    <item>
      <title>Illuminating the Bragg intersections as roots of Dirac nodal lines and high-order van Hove singularities</title>
      <link>https://arxiv.org/abs/2306.04238</link>
      <description>arXiv:2306.04238v2 Announce Type: replace-cross 
Abstract: We theoretically reexamine nearly uniform electron models with weak crystalline potentials. In particular, we theorize the modulation of the plane-wave branches at linear regions where multiple Bragg planes intersect. Any such linear intersections involve three or more plane-wave branches diffracted by the periodic potential. Small inter-branch interactions can yield various crossing and anticrossing singularities with promised breakdown of the quadratic approximation, extending alongside the intersection lines. Most of the intersections run in low-symmetric paths in the Brillouin zone and therefore we cannot completely characterize their electronic states with standard band structure plotting methods. The present theory reveals a general mechanism in nearly uniform systems to induce the Dirac nodal lines and van-Hove singularities with broken quadratic band approximation in three dimensions, which may host a variety of anomalous low-energy electronic properties. We apply the theory to a recently discovered high temperature superconductor H$_{3}$S to interpret the enigmatic density-of-state (DOS) peaking therein. The results show how and {\it why there} the continuous saddle points--the source of the peaked DOS--emerge, as well as reveal the companion Dirac nodal lines hidden in the conduction bands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04238v2</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.supr-con</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevResearch.6.033012</arxiv:DOI>
      <dc:creator>Ryosuke Akashi</dc:creator>
    </item>
    <item>
      <title>GenML: A Python Library to Generate the Mittag-Leffler Correlated Noise</title>
      <link>https://arxiv.org/abs/2403.04273</link>
      <description>arXiv:2403.04273v2 Announce Type: replace-cross 
Abstract: Mittag-Leffler correlated noise (M-L noise) plays a crucial role in the dynamics of complex systems, yet the scientific community has lacked tools for its direct generation. Addressing this gap, our work introduces GenML, a Python library specifically designed for generating M-L noise. We detail the architecture and functionalities of GenML and its underlying algorithmic approach, which enables the precise simulation of M-L noise. The effectiveness of GenML is validated through quantitative analyses of autocorrelation functions and diffusion behaviors, showcasing its capability to accurately replicate theoretical noise properties. Our contribution with GenML enables the effective application of M-L noise data in numerical simulation and data-driven methods for describing complex systems, moving beyond mere theoretical modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04273v2</guid>
      <category>cs.MS</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Qu, Hui Zhao, Wenjie Cai, Gongyi Wang, Zihan Huang</dc:creator>
    </item>
    <item>
      <title>Quantum Computing for Phonon Scattering Effects on Thermal Conductivity</title>
      <link>https://arxiv.org/abs/2407.15808</link>
      <description>arXiv:2407.15808v2 Announce Type: replace-cross 
Abstract: Recent investigations have demonstrated that multi-phonon scattering processes substantially influence the thermal conductivity of materials, posing significant computational challenges for classical simulations as the complexity of phonon modes escalates. This study examines the potential of quantum simulations to address these challenges, utilizing Noisy Intermediate Scale Quantum era (NISQ) quantum computational capabilities and quantum error mitigation techniques to optimize thermal conductivity calculations. Employing the Variational Quantum Eigensolver (VQE) algorithm, we simulate phonon-phonon contributions based on the Boltzmann Transport Equation (BTE). Our methodology involves mapping multi-phonon scattering systems to fermionic spin operators, necessitating the creation of a customized ansatz to balance circuit accuracy and depth. We construct the system within Fock space using bosonic operators and transform the Hamiltonian into the sum of Pauli operators suitable for quantum computation. By addressing the impact of depolarization and non-unitary noise effects, we benchmark the noise influence and implement error mitigation strategies to develop a more efficient model for quantum simulations in the NISQ era.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15808v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.comp-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangjun Tan</dc:creator>
    </item>
  </channel>
</rss>
