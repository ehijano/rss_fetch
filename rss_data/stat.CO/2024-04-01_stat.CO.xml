<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Apr 2024 04:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Investigating the Combinatorial Potential and Applicability of Random Equation Systems with Mixture Models in a Bayesian Framework</title>
      <link>https://arxiv.org/abs/2403.20152</link>
      <description>arXiv:2403.20152v1 Announce Type: new 
Abstract: Investigating solutions of nonlinear equation systems is challenging in a general framework, especially if the equations contain uncertainties about parameters modeled by probability densities. Such random equations, understood as stationary (non-dynamical) equations with parameters as random variables, have a long history and a broad range of applications. In this work, we study nonlinear random equations by combining them with mixture model parameter random variables in order to investigate the combinatorial complexity of such equations and how this can be utilized practically. We derive a general likelihood function and posterior density of approximate best fit solutions while avoiding significant restrictions about the type of nonlinearity or mixture models, and demonstrate their numerically efficient application for the applied researcher. In the results section we are specifically focusing on example simulations of approximate likelihood/posterior solutions for random linear equation systems, nonlinear systems of random conic section equations, as well as applications to portfolio optimization, stochastic control and random matrix theory in order to show the wide applicability of the presented methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20152v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wolfgang Hoegele</dc:creator>
    </item>
    <item>
      <title>Best Subset Solution Path for Linear Dimension Reduction Models using Continuous Optimization</title>
      <link>https://arxiv.org/abs/2403.20007</link>
      <description>arXiv:2403.20007v1 Announce Type: cross 
Abstract: The selection of best variables is a challenging problem in supervised and unsupervised learning, especially in high dimensional contexts where the number of variables is usually much larger than the number of observations. In this paper, we focus on two multivariate statistical methods: principal components analysis and partial least squares. Both approaches are popular linear dimension-reduction methods with numerous applications in several fields including in genomics, biology, environmental science, and engineering. In particular, these approaches build principal components, new variables that are combinations of all the original variables. A main drawback of principal components is the difficulty to interpret them when the number of variables is large. To define principal components from the most relevant variables, we propose to cast the best subset solution path method into principal component analysis and partial least square frameworks. We offer a new alternative by exploiting a continuous optimization algorithm for best subset solution path. Empirical studies show the efficacy of our approach for providing the best subset solution path. The usage of our algorithm is further exposed through the analysis of two real datasets. The first dataset is analyzed using the principle component analysis while the analysis of the second dataset is based on partial least square framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20007v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.OT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benoit Liquet, Sarat Moka, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Towards a turnkey approach to unbiased Monte Carlo estimation of smooth functions of expectations</title>
      <link>https://arxiv.org/abs/2403.20313</link>
      <description>arXiv:2403.20313v1 Announce Type: cross 
Abstract: Given a smooth function $f$, we develop a general approach to turn Monte
  Carlo samples with expectation $m$ into an unbiased estimate of $f(m)$.
  Specifically, we develop estimators that are based on randomly truncating
  the Taylor series expansion of $f$ and estimating the coefficients of the
  truncated series. We derive their properties and propose a strategy to set
  their tuning parameters -- which depend on $m$ -- automatically, with a
  view to make the whole approach simple to use. We develop our methods for
  the specific functions $f(x)=\log x$ and $f(x)=1/x$, as they arise in
  several statistical applications such as maximum likelihood estimation of
  latent variable models and Bayesian inference for un-normalised models.
  Detailed numerical studies are performed for a range of applications to
  determine how competitive and reliable the proposed approach is.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20313v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Chopin, Francesca R. Crucinio, Sumeetpal S. Singh</dc:creator>
    </item>
    <item>
      <title>Fast Sampling and Inference via Preconditioned Langevin Dynamics</title>
      <link>https://arxiv.org/abs/2310.07542</link>
      <description>arXiv:2310.07542v2 Announce Type: replace 
Abstract: Sampling from distributions play a crucial role in aiding practitioners with statistical inference. However, in numerous situations, obtaining exact samples from complex distributions is infeasible. Consequently, researchers often turn to approximate sampling techniques to address this challenge. Fast approximate sampling from complicated distributions has gained much traction in the last few years with considerable progress in this field. Previous work has shown that for some problems a preconditioning can make the algorithm faster. In our research, we explore the Langevin Monte Carlo (LMC) algorithm and demonstrate its effectiveness in enabling inference from the obtained samples. Additionally, we establish a convergence rate for the LMC Markov chain in total variation. Lastly, we derive non-asymptotic bounds for approximate sampling from specific target distributions in the Wasserstein distance, particularly when the preconditioning is spatially invariant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07542v2</guid>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riddhiman Bhattacharya, Tiefeng Jiang</dc:creator>
    </item>
  </channel>
</rss>
