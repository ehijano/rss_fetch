<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Language modulates vision: Evidence from neural networks and human brain-lesion models</title>
      <link>https://arxiv.org/abs/2501.13628</link>
      <description>arXiv:2501.13628v1 Announce Type: new 
Abstract: Comparing information structures in between deep neural networks (DNNs) and the human brain has become a key method for exploring their similarities and differences. Recent research has shown better alignment of vision-language DNN models, such as CLIP, with the activity of the human ventral occipitotemporal cortex (VOTC) than earlier vision models, supporting the idea that language modulates human visual perception. However, interpreting the results from such comparisons is inherently limited due to the "black box" nature of DNNs. To address this, we combined model-brain fitness analyses with human brain lesion data to examine how disrupting the communication pathway between the visual and language systems causally affects the ability of vision-language DNNs to explain the activity of the VOTC. Across four diverse datasets, CLIP consistently outperformed both label-supervised (ResNet) and unsupervised (MoCo) models in predicting VOTC activity. This advantage was left-lateralized, aligning with the human language network. Analyses of the data of 33 stroke patients revealed that reduced white matter integrity between the VOTC and the language region in the left angular gyrus was correlated with decreased CLIP performance and increased MoCo performance, indicating a dynamic influence of language processing on the activity of the VOTC. These findings support the integration of language modulation in neurocognitive models of human vision, reinforcing concepts from vision-language DNN models. The sensitivity of model-brain similarity to specific brain lesions demonstrates that leveraging manipulation of the human brain is a promising framework for evaluating and developing brain-like computer models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13628v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Chen, Bo Liu, Shuyue Wang, Xiaosha Wang, Wenjuan Han, Yixin Zhu, Xiaochun Wang, Yanchao Bi</dc:creator>
    </item>
    <item>
      <title>Spikes can transmit neurons' subthreshold membrane potentials</title>
      <link>https://arxiv.org/abs/2501.13845</link>
      <description>arXiv:2501.13845v1 Announce Type: new 
Abstract: Neurons primarily communicate through the emission of action potentials, or spikes. To generate a spike, a neuron's membrane potential must cross a defined threshold. Does this spiking mechanism inherently prevent neurons from transmitting their subthreshold membrane potential fluctuations to other neurons? We prove that, in theory, it does not. The subthreshold membrane potential fluctuations of a presynaptic population of spiking neurons can be perfectly transmitted to a downstream population of neurons. Mathematically, this surprising result is an example of concentration phenomenon in high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13845v1</guid>
      <category>q-bio.NC</category>
      <category>math.PR</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Valentin Schmutz</dc:creator>
    </item>
    <item>
      <title>Self-organization and memory in an disordered solid subject to random loading</title>
      <link>https://arxiv.org/abs/2409.17096</link>
      <description>arXiv:2409.17096v2 Announce Type: cross 
Abstract: We consider self-organization and memory formation in a mesoscopic model of an amorphous solid subject to a random shear strain protocol confined to a strain range $\pm \varepsilon_{\rm max}$. We develop proper read-out protocols to show that the response of the driven system retains a memory of the strain range, which can be subsequently retrieved. Our findings generalize previous results obtained upon oscillatory driving and suggest that self-organization and memory formation of disordered materials can emerge under more general conditions, such as a disordered system interacting with its fluctuating environment. The self-organization results in a correlation between the dynamics of the system and its environment. We conclude by discussing our results within the context of environmental sensing, highlighting their generalizability to adaptation strategies of simple organisms under changing conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17096v2</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.mtrl-sci</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhittin Mungan, Dheeraj Kumar, Sylvain Patinet, Damien Vandembroucq</dc:creator>
    </item>
    <item>
      <title>Bridging Neuroscience and AI: Environmental Enrichment as a Model for Forward Knowledge Transfer</title>
      <link>https://arxiv.org/abs/2405.07295</link>
      <description>arXiv:2405.07295v3 Announce Type: replace 
Abstract: Continual learning (CL) refers to an agent's capability to learn from a continuous stream of data and transfer knowledge without forgetting old information. One crucial aspect of CL is forward transfer, i.e., improved and faster learning on a new task by leveraging information from prior knowledge. While this ability comes naturally to biological brains, it poses a significant challenge for artificial intelligence (AI). Here, we suggest that environmental enrichment (EE) can be used as a biological model for studying forward transfer, inspiring human-like AI development. EE refers to animal studies that enhance cognitive, social, motor, and sensory stimulation and is a model for what, in humans, is referred to as 'cognitive reserve'. Enriched animals show significant improvement in learning speed and performance on new tasks, typically exhibiting forward transfer. We explore anatomical, molecular, and neuronal changes post-EE and discuss how artificial neural networks (ANNs) can be used to predict neural computation changes after enriched experiences. Finally, we provide a synergistic way of combining neuroscience and AI research that paves the path toward developing AI capable of rapid and efficient new task learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07295v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajat Saxena, Bruce L. McNaughton</dc:creator>
    </item>
    <item>
      <title>ST-USleepNet: A Spatial-Temporal Coupling Prominence Network for Multi-Channel Sleep Staging</title>
      <link>https://arxiv.org/abs/2408.11884</link>
      <description>arXiv:2408.11884v2 Announce Type: replace 
Abstract: Sleep staging is critical to assess sleep quality and diagnose disorders. Despite advancements in artificial intelligence enabling automated sleep staging, significant challenges remain: (1) Simultaneously extracting prominent temporal and spatial sleep features from multi-channel raw signals, including characteristic sleep waveforms and salient spatial brain networks. (2) Capturing the spatial-temporal coupling patterns essential for accurate sleep staging. To address these challenges, we propose a novel framework named ST-USleepNet, comprising a spatial-temporal graph construction module (ST) and a U-shaped sleep network (USleepNet). The ST module converts raw signals into a spatial-temporal graph based on signal similarity, temporal, and spatial relationships to model spatial-temporal coupling patterns. The USleepNet employs a U-shaped structure for both the temporal and spatial streams, mirroring its original use in image segmentation to isolate significant targets. Applied to raw sleep signals and graph data from the ST module, USleepNet effectively segments these inputs, simultaneously extracting prominent temporal and spatial sleep features. Testing on three datasets demonstrates that ST-USleepNet outperforms existing baselines, and model visualizations confirm its efficacy in extracting prominent sleep features and temporal-spatial coupling patterns across various sleep stages. The code is available at: https://github.com/Majy-Yuji/ST-USleepNet.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11884v2</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 24 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingying Ma, Qika Lin, Ziyu Jia, Mengling Feng</dc:creator>
    </item>
  </channel>
</rss>
