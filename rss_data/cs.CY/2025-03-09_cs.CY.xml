<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Mar 2025 04:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Urban Metaverse: Die Smart City im Industrial Metaverse</title>
      <link>https://arxiv.org/abs/2503.04729</link>
      <description>arXiv:2503.04729v1 Announce Type: new 
Abstract: The Urban Metaverse describes an immersive 3D environment that connects the physical world of the city and its citizens with its digital data and systems. Physical and digital realities merge, opening up new possibilities for the design and use of the city. This trend study serves as a source of inspiration and guidance for city and community leaders, urban planners, IT professionals, and anyone interested in the future of urban spaces. It helps to understand the opportunities and challenges of the Urban Metaverse as an evolution of the Smart City and to set the course for sustainable and innovative urban development. To this end, the study analyzes the opportunities that the Urban Metaverse offers for urban administration and the everyday life of citizens, presents key technologies, and highlights the socio-economic challenges of implementation. The focus is on the potential of the Urban Metaverse to optimize the planning and operation of urban infrastructures, to promote inclusion and civic participation, and to enhance the innovative capacity of cities and municipalities. The study develops four recommendations for the implementation of metaverse applications in an urban context: 1. user-centered design, 2. ubiquitous accessibility, 3. proactive design of the regulatory framework, and 4. development of viable business models. Note: This document is published in English. An English version is in preparation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04729v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Dienhart, Luis Kaufhold, Frank Piller</dc:creator>
    </item>
    <item>
      <title>Impacto de Treinamento em Programa\c{c}\~ao Competitiva no Ensino M\'edio: Resultados e Desafios</title>
      <link>https://arxiv.org/abs/2503.04732</link>
      <description>arXiv:2503.04732v1 Announce Type: new 
Abstract: This article presents an ongoing research aiming to develop an effective methodology for teaching programming, focusing on participation in the Brazilian Informatics Olympiad (OBI), for elementary and high school students. The training conducted with students from the Federal Institute and state schools, demonstrates the importance of programming training programs as a way to promote interest in computing, stimulate the development of computational skills, and increase participation in competitions such as the OBI. The next steps of the research include conducting more training cycles and analyzing the results obtained in the competitions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04732v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5753/sbie.2024.244908</arxiv:DOI>
      <arxiv:journal_reference>Simp\'osio Brasileiro de Inform\'atica na Educa\c{c}\~ao (SBIE) 2024</arxiv:journal_reference>
      <dc:creator>Camila da Cruz Santos, Sarah Souto dos Santos, Crishna Irion, Giullia Rodrigues de Menezes, Rafael Dias Ara\'ujo, Jo\~ao Henrique de Souza Pereira</dc:creator>
    </item>
    <item>
      <title>Ethics of generative AI and manipulation: a design-oriented research agenda</title>
      <link>https://arxiv.org/abs/2503.04733</link>
      <description>arXiv:2503.04733v1 Announce Type: new 
Abstract: Generative AI enables automated, effective manipulation at scale. Despite the growing general ethical discussion around generative AI, the specific manipulation risks remain inadequately investigated. This article outlines essential inquiries encompassing conceptual, empirical, and design dimensions of manipulation, pivotal for comprehending and curbing manipulation risks. By highlighting these questions, the article underscores the necessity of an appropriate conceptualisation of manipulation to ensure the responsible development of Generative AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04733v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10676-024-09745-x</arxiv:DOI>
      <arxiv:journal_reference>Ethics and Information Technology, 26, 9, 2024</arxiv:journal_reference>
      <dc:creator>Michael Klenk</dc:creator>
    </item>
    <item>
      <title>What can large language models do for sustainable food?</title>
      <link>https://arxiv.org/abs/2503.04734</link>
      <description>arXiv:2503.04734v1 Announce Type: new 
Abstract: Food systems are responsible for a third of human-caused greenhouse gas emissions. We investigate what Large Language Models (LLMs) can contribute to reducing the environmental impacts of food production. We define a typology of design and prediction tasks based on the sustainable food literature and collaboration with domain experts, and evaluate six LLMs on four tasks in our typology. For example, for a sustainable protein design task, food science experts estimated that collaboration with an LLM can reduce time spent by 45% on average, compared to 22% for collaboration with another expert human food scientist. However, for a sustainable menu design task, LLMs produce suboptimal solutions when instructed to consider both human satisfaction and climate impacts. We propose a general framework for integrating LLMs with combinatorial optimization to improve reasoning capabilities. Our approach decreases emissions of food choices by 79% in a hypothetical restaurant while maintaining participants' satisfaction with their set of choices. Our results demonstrate LLMs' potential, supported by optimization techniques, to accelerate sustainable food development and adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04734v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna T. Thomas, Adam Yee, Andrew Mayne, Maya B. Mathur, Dan Jurafsky, Kristina Gligori\'c</dc:creator>
    </item>
    <item>
      <title>How Personality Traits Shape LLM Risk-Taking Behaviour</title>
      <link>https://arxiv.org/abs/2503.04735</link>
      <description>arXiv:2503.04735v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are increasingly deployed as autonomous agents, necessitating a deeper understanding of their decision-making behaviour under risk. This study investigates the relationship between LLMs' personality traits and risk propensity, employing cumulative prospect theory (CPT) and the Big Five personality framework. We focus on GPT-4o, comparing its behaviour to human baselines and earlier models. Our findings reveal that GPT-4o exhibits higher Conscientiousness and Agreeableness traits compared to human averages, while functioning as a risk-neutral rational agent in prospect selection. Interventions on GPT-4o's Big Five traits, particularly Openness, significantly influence its risk propensity, mirroring patterns observed in human studies. Notably, Openness emerges as the most influential factor in GPT-4o's risk propensity, aligning with human findings. In contrast, legacy models like GPT-4-Turbo demonstrate inconsistent generalization of the personality-risk relationship. This research advances our understanding of LLM behaviour under risk and elucidates the potential and limitations of personality-based interventions in shaping LLM decision-making. Our findings have implications for the development of more robust and predictable AI systems such as financial modelling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04735v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Hartley, Conor Hamill, Devesh Batra, Dale Seddon, Ramin Okhrati, Raad Khraishi</dc:creator>
    </item>
    <item>
      <title>Standardizing Intelligence: Aligning Generative AI for Regulatory and Operational Compliance</title>
      <link>https://arxiv.org/abs/2503.04736</link>
      <description>arXiv:2503.04736v1 Announce Type: new 
Abstract: Technical standards, or simply standards, are established documented guidelines and rules that facilitate the interoperability, quality, and accuracy of systems and processes. In recent years, we have witnessed an emerging paradigm shift where the adoption of generative AI (GenAI) models has increased tremendously, spreading implementation interests across standard-driven industries, including engineering, legal, healthcare, and education. In this paper, we assess the criticality levels of different standards across domains and sectors and complement them by grading the current compliance capabilities of state-of-the-art GenAI models. To support the discussion, we outline possible challenges and opportunities with integrating GenAI for standard compliance tasks while also providing actionable recommendations for entities involved with developing and using standards. Overall, we argue that aligning GenAI with standards through computational methods can help strengthen regulatory and operational compliance. We anticipate this area of research will play a central role in the management, oversight, and trustworthiness of larger, more powerful GenAI-based systems in the near future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04736v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Joseph Marvin Imperial, Matthew D. Jones, Harish Tayyar Madabushi</dc:creator>
    </item>
    <item>
      <title>Carelessness Detection using Performance Factor Analysis: A New Operationalization with Unexpectedly Different Relationship to Learning</title>
      <link>https://arxiv.org/abs/2503.04737</link>
      <description>arXiv:2503.04737v1 Announce Type: new 
Abstract: Detection of carelessness in digital learning platforms has relied on the contextual slip model, which leverages conditional probability and Bayesian Knowledge Tracing (BKT) to identify careless errors, where students make mistakes despite having the knowledge. However, this model cannot effectively assess carelessness in questions tagged with multiple skills due to the use of conditional probability. This limitation narrows the scope within which the model can be applied. Thus, we propose a novel model, the Beyond Knowledge Feature Carelessness (BKFC) model. The model detects careless errors using performance factor analysis (PFA) and behavioral features distilled from log data, controlling for knowledge when detecting carelessness. We applied the BKFC to detect carelessness in data from middle school students playing a learning game on decimal numbers and operations. We conducted analyses comparing the careless errors detected using contextual slip to the BKFC model. Unexpectedly, careless errors identified by these two approaches did not align. We found students' post-test performance was (corresponding to past results) positively associated with the carelessness detected using the contextual slip model, while negatively associated with the carelessness detected using the BKFC model. These results highlight the complexity of carelessness and underline a broader challenge in operationalizing carelessness and careless errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04737v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayi Zhang, Ryan S. Baker, Namrata Srivastava, Jaclyn Ocumpaugh, Caitlin Mills, Bruce M. McLaren</dc:creator>
    </item>
    <item>
      <title>Copyright in AI-generated works: Lessons from recent developments in patent law</title>
      <link>https://arxiv.org/abs/2503.04738</link>
      <description>arXiv:2503.04738v1 Announce Type: new 
Abstract: In Thaler v The Comptroller-General of Patents, Designs and Trade Marks (DABUS), Smith J. held that an AI owner can possibly claim patent ownership over an AI-generated invention based on their ownership and control of the AI system. This AI-owner approach reveals a new option to allocate property rights over AI-generated output. While this judgment was primarily about inventorship and ownership of AI-generated invention in patent law, it has important implications for copyright law. After analysing the weaknesses of applying existing judicial approaches to copyright ownership of AI-generated works, this paper examines whether the AI-owner approach is a better option for determining copyright ownership of AI-generated works. The paper argues that while contracts can be used to work around the AI-owner approach in scenarios where users want to commercially exploit the outputs, this approach still provides more certainty and less transaction costs for relevant parties than other approaches proposed so far.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04738v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.2966/scrip.190122.5</arxiv:DOI>
      <arxiv:journal_reference>SCRIPTed, Vol. 19, Iss. 1 (2022)</arxiv:journal_reference>
      <dc:creator>Rita Matulionyte, Jyh-An Lee</dc:creator>
    </item>
    <item>
      <title>Responsible Artificial Intelligence Systems: A Roadmap to Society's Trust through Trustworthy AI, Auditability, Accountability, and Governance</title>
      <link>https://arxiv.org/abs/2503.04739</link>
      <description>arXiv:2503.04739v1 Announce Type: new 
Abstract: Artificial intelligence (AI) has matured as a technology, necessitating the development of responsibility frameworks that are fair, inclusive, trustworthy, safe and secure, transparent, and accountable. By establishing such frameworks, we can harness the full potential of AI while mitigating its risks, particularly in high-risk scenarios. This requires the design of responsible AI systems based on trustworthy AI technologies and ethical principles, with the aim of ensuring auditability and accountability throughout their design, development, and deployment, adhering to domain-specific regulations and standards.
  This paper explores the concept of a responsible AI system from a holistic perspective, which encompasses four key dimensions: 1) regulatory context; 2) trustworthy AI technology along with standardization and assessments; 3) auditability and accountability; and 4) AI governance. The aim of this paper is double. First, we analyze and understand these four dimensions and their interconnections in the form of an analysis and overview. Second, the final goal of the paper is to propose a roadmap in the design of responsible AI systems, ensuring that they can gain society's trust. To achieve this trustworthiness, this paper also fosters interdisciplinary discussions on the ethical, legal, social, economic, and cultural aspects of AI from a global governance perspective. Last but not least, we also reflect on the current state and those aspects that need to be developed in the near future, as ten lessons learned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04739v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'es Herrera-Poyatos, Javier Del Ser, Marcos L\'opez de Prado, Fei-Yue Wang, Enrique Herrera-Viedma, Francisco Herrera</dc:creator>
    </item>
    <item>
      <title>PRISM: Perspective Reasoning for Integrated Synthesis and Mediation as a Multi-Perspective Framework for AI Alignment</title>
      <link>https://arxiv.org/abs/2503.04740</link>
      <description>arXiv:2503.04740v1 Announce Type: new 
Abstract: In this work, we propose Perspective Reasoning for Integrated Synthesis and Mediation (PRISM), a multiple-perspective framework for addressing persistent challenges in AI alignment such as conflicting human values and specification gaming. Grounded in cognitive science and moral psychology, PRISM organizes moral concerns into seven "basis worldviews", each hypothesized to capture a distinct dimension of human moral cognition, ranging from survival-focused reflexes through higher-order integrative perspectives. It then applies a Pareto-inspired optimization scheme to reconcile competing priorities without reducing them to a single metric. Under the assumption of reliable context validation for robust use, the framework follows a structured workflow that elicits viewpoint-specific responses, synthesizes them into a balanced outcome, and mediates remaining conflicts in a transparent and iterative manner. By referencing layered approaches to moral cognition from cognitive science, moral psychology, and neuroscience, PRISM clarifies how different moral drives interact and systematically documents and mediates ethical tradeoffs. We illustrate its efficacy through real outputs produced by a working prototype, applying PRISM to classic alignment problems in domains such as public health policy, workplace automation, and education. By anchoring AI deliberation in these human vantage points, PRISM aims to bound interpretive leaps that might otherwise drift into non-human or machine-centric territory. We briefly outline future directions, including real-world deployments and formal verifications, while maintaining the core focus on multi-perspective synthesis and conflict mediation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04740v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Diamond</dc:creator>
    </item>
    <item>
      <title>Which Information should the UK and US AISI share with an International Network of AISIs? Opportunities, Risks, and a Tentative Proposal</title>
      <link>https://arxiv.org/abs/2503.04741</link>
      <description>arXiv:2503.04741v1 Announce Type: new 
Abstract: The UK AI Safety Institute (UK AISI) and its parallel organisation in the United States (US AISI) take up a unique position in the recently established International Network of AISIs. Both are in jurisdictions with frontier AI companies and are assuming leading roles in the international conversation on AI Safety. This paper argues that it is in the interest of both institutions to share specific categories of information with the International Network of AISIs, deliberately abstain from sharing others and carefully evaluate sharing some categories on a case by case basis, according to domestic priorities. The paper further proposes a provisional framework with which policymakers and researchers can distinguish between these three cases, taking into account the potential benefits and risks of sharing specific categories of information, ranging from pre-deployment evaluation results to evaluation standards. In an effort to further improve the research on AI policy relevant information sharing decisions, the paper emphasises the importance of continuously monitoring fluctuating factors influencing sharing decisions and a more in-depth analysis of specific policy relevant information categories and additional factors to consider in future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04741v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lara Thurnherr</dc:creator>
    </item>
    <item>
      <title>A case for specialisation in non-human entities</title>
      <link>https://arxiv.org/abs/2503.04742</link>
      <description>arXiv:2503.04742v1 Announce Type: new 
Abstract: With the rise of large multi-modal AI models, fuelled by recent interest in large language models (LLMs), the notion of artificial general intelligence (AGI) went from being restricted to a fringe community, to dominate mainstream large AI development programs.
  In contrast, in this paper, we make a \emph{case for specialisation}, by reviewing the pitfalls of generality and stressing the industrial value of specialised
  systems.
  Our contribution is threefold. First, we review the most widely accepted arguments \emph{against} specialisation, and discuss how their relevance in the context of human labour is actually an argument \emph{for} specialisation in the case of non human agents, be they algorithms or human organisations. Second, we propose four arguments \emph{in favor of} specialisation, ranging from machine learning robustness, to computer security, social sciences and cultural evolution.
  Third, we finally make a case for \emph{specification}, discuss how the machine learning approach to AI has so far failed to catch up with good practices from safety-engineering and formal verification of software, and discuss how some emerging good practices in machine learning help reduce this gap.
  In particular, we justify the need for \emph{specified governance} for hard-to-specify systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04742v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>El-Mahdi El-Mhamdi, L\^e-Nguy\^en Hoang, Mariame Tighanimine</dc:creator>
    </item>
    <item>
      <title>AI Safety is Stuck in Technical Terms -- A System Safety Response to the International AI Safety Report</title>
      <link>https://arxiv.org/abs/2503.04743</link>
      <description>arXiv:2503.04743v1 Announce Type: new 
Abstract: Safety has become the central value around which dominant AI governance efforts are being shaped. Recently, this culminated in the publication of the International AI Safety Report, written by 96 experts of which 30 nominated by the Organisation for Economic Co-operation and Development (OECD), the European Union (EU), and the United Nations (UN). The report focuses on the safety risks of general-purpose AI and available technical mitigation approaches. In this response, informed by a system safety perspective, I refl ect on the key conclusions of the report, identifying fundamental issues in the currently dominant technical framing of AI safety and how this frustrates meaningful discourse and policy efforts to address safety comprehensively. The system safety discipline has dealt with the safety risks of software-based systems for many decades, and understands safety risks in AI systems as sociotechnical and requiring consideration of technical and non-technical factors and their interactions. The International AI Safety report does identify the need for system safety approaches. Lessons, concepts and methods from system safety indeed provide an important blueprint for overcoming current shortcomings in technical approaches by integrating rather than adding on non-technical factors and interventions. I conclude with why building a system safety discipline can help us overcome limitations in the European AI Act, as well as how the discipline can help shape sustainable investments into Public Interest AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04743v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roel Dobbe</dc:creator>
    </item>
    <item>
      <title>Safety Cases: A Scalable Approach to Frontier AI Safety</title>
      <link>https://arxiv.org/abs/2503.04744</link>
      <description>arXiv:2503.04744v1 Announce Type: new 
Abstract: Safety cases - clear, assessable arguments for the safety of a system in a given context - are a widely-used technique across various industries for showing a decision-maker (e.g. boards, customers, third parties) that a system is safe. In this paper, we cover how and why frontier AI developers might also want to use safety cases. We then argue that writing and reviewing safety cases would substantially assist in the fulfilment of many of the Frontier AI Safety Commitments. Finally, we outline open research questions on the methodology, implementation, and technical details of safety cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04744v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Hilton, Marie Davidsen Buhl, Tomek Korbak, Geoffrey Irving</dc:creator>
    </item>
    <item>
      <title>Sovereign Large Language Models: Advantages, Strategy and Regulations</title>
      <link>https://arxiv.org/abs/2503.04745</link>
      <description>arXiv:2503.04745v1 Announce Type: new 
Abstract: This report analyzes key trends, challenges, risks, and opportunities associated with the development of Large Language Models (LLMs) globally. It examines national experiences in developing LLMs and assesses the feasibility of investment in this sector. Additionally, the report explores strategies for implementing, regulating, and financing AI projects at the state level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04745v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mykhailo Bondarenko, Sviatoslav Lushnei, Yurii Paniv, Oleksii Molchanovsky, Mariana Romanyshyn, Yurii Filipchuk, Artur Kiulian</dc:creator>
    </item>
    <item>
      <title>Emerging Practices in Frontier AI Safety Frameworks</title>
      <link>https://arxiv.org/abs/2503.04746</link>
      <description>arXiv:2503.04746v1 Announce Type: new 
Abstract: As part of the Frontier AI Safety Commitments agreed to at the 2024 AI Seoul Summit, many AI developers agreed to publish a safety framework outlining how they will manage potential severe risks associated with their systems. This paper summarises current thinking from companies, governments, and researchers on how to write an effective safety framework. We outline three core areas of a safety framework - risk identification and assessment, risk mitigation, and governance - and identify emerging practices within each area. As safety frameworks are novel and rapidly developing, we hope that this paper can serve both as an overview of work to date and as a starting point for further discussion and innovation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04746v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marie Davidsen Buhl, Ben Bucknall, Tammy Masterson</dc:creator>
    </item>
    <item>
      <title>E-LENS: User Requirements-Oriented AI Ethics Assurance</title>
      <link>https://arxiv.org/abs/2503.04747</link>
      <description>arXiv:2503.04747v1 Announce Type: new 
Abstract: Despite the much proliferation of AI ethical principles in recent years, there is a challenge of assuring AI ethics with current AI ethics frameworks in real-world applications. While system safety has emerged as a distinct discipline for a long time, originated from safety concerns in early aircraft manufacturing. The safety assurance is now an indispensable component in safety critical domains. Motivated by the assurance approaches for safety-critical systems such as aviation, this paper introduces the concept of AI ethics assurance cases into the AI ethics assurance. Three pillars of user requirements, evidence, and validation are proposed as key components and integrated into AI ethics assurance cases for a new approach of user requirements-oriented AI ethics assurance. The user requirements-oriented AI ethics assurance case is set up based on three pillars and hazard analysis methods used in the safety assurance of safety-critical systems. This paper also proposes a platform named Ethical-Lens (E-LENS) to implement the user requirements-oriented AI ethics assurance approach. The proposed user requirements-based E-LENS platform is then applied to assure AI ethics of an AI-driven human resource shortlisting system as a case study to show the effectiveness of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04747v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianlong Zhou, Fang Chen</dc:creator>
    </item>
    <item>
      <title>Large Language Models in Healthcare</title>
      <link>https://arxiv.org/abs/2503.04748</link>
      <description>arXiv:2503.04748v1 Announce Type: new 
Abstract: Large language models (LLMs) hold promise for transforming healthcare, from streamlining administrative and clinical workflows to enriching patient engagement and advancing clinical decision-making. However, their successful integration requires rigorous development, adaptation, and evaluation strategies tailored to clinical needs. In this Review, we highlight recent advancements, explore emerging opportunities for LLM-driven innovation, and propose a framework for their responsible implementation in healthcare settings. We examine strategies for adapting LLMs to domain-specific healthcare tasks, such as fine-tuning, prompt engineering, and multimodal integration with electronic health records. We also summarize various evaluation metrics tailored to healthcare, addressing clinical accuracy, fairness, robustness, and patient outcomes. Furthermore, we discuss the challenges associated with deploying LLMs in healthcare--including data privacy, bias mitigation, regulatory compliance, and computational sustainability--and underscore the need for interdisciplinary collaboration. Finally, these challenges present promising future research directions for advancing LLM implementation in clinical settings and healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04748v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mohammed Al-Garadi, Tushar Mungle, Abdulaziz Ahmed, Abeed Sarker, Zhuqi Miao, Michael E. Matheny</dc:creator>
    </item>
    <item>
      <title>Digital Transformation in the Petrochemical Industry -- Challenges and Opportunities in the Implementation of {IoT} Technologies</title>
      <link>https://arxiv.org/abs/2503.04749</link>
      <description>arXiv:2503.04749v1 Announce Type: new 
Abstract: The petrochemical industry faces significant technological, environmental, occupational safety, and financial challenges. Since its emergence in the 1920s, technologies that were once innovative have now become obsolete. However, factors such as the protection of trade secrets in industrial processes, limited budgets for research and development, doubts about the reliability of new technologies, and resistance to change from decision-makers have hindered the adoption of new approaches, such as the use of IoT devices. This paper addresses the challenges and opportunities presented by the research, development, and implementation of these technologies in the industry. It also analyzes the investment in research and development made by companies in the sector in recent years and provides a review of current research and implementations related to Industry 4.0.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04749v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Noel Portillo</dc:creator>
    </item>
    <item>
      <title>Position: AI agents should be regulated based on autonomous action sequences</title>
      <link>https://arxiv.org/abs/2503.04750</link>
      <description>arXiv:2503.04750v1 Announce Type: new 
Abstract: This position paper argues that AI agents should be regulated based on the sequence of actions they autonomously take. AI agents with long-term planning and strategic capabilities can pose significant risks of human extinction and irreversible global catastrophes. While existing regulations often focus on computational scale as a proxy for potential harm, we contend that such measures are insufficient for assessing the risks posed by AI agents whose capabilities arise primarily from inference-time computation. To support our position, we discuss relevant regulations and recommendations from AI scientists regarding existential risks, as well as the advantages of action sequences over existing impact measures that require observing environmental states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04750v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takauki Osogami</dc:creator>
    </item>
    <item>
      <title>What is Ethical: AIHED Driving Humans or Human-Driven AIHED? A Conceptual Framework enabling the Ethos of AI-driven Higher education</title>
      <link>https://arxiv.org/abs/2503.04751</link>
      <description>arXiv:2503.04751v1 Announce Type: new 
Abstract: The rapid integration of Artificial Intelligence (AI) in Higher Education (HE) is transforming personalized learning, administrative automation, and decision-making. However, this progress presents a duality, as AI adoption also introduces ethical and institutional challenges, including algorithmic bias, data privacy risks, and governance inconsistencies. To address these concerns, this study introduces the Human-Driven AI in Higher Education (HD-AIHED) Framework, ensuring compliance with UNESCO and OECD ethical standards. This conceptual research employs a qualitative meta-synthesis approach, integrating qualitative and quantitative studies to identify patterns, contradictions, and gaps in AI adoption within HE. It reinterprets existing datasets through theoretical and ethical lenses to develop governance frameworks. The study applies a participatory integrated co-system, Phased Human Intelligence, SWOC analysis, and AI ethical review boards to assess AI readiness and governance strategies for universities and HE institutions. The HD-AIHED model bridges AI research gaps, addresses global real-time challenges, and provides tailored, scalable, and ethical strategies for diverse educational contexts. By emphasizing interdisciplinary collaboration among stakeholders, this study envisions AIHED as a transparent and equitable force for innovation. The HD-AIHED framework ensures AI acts as a collaborative and ethical enabler rather than a disruptive replacement for human intelligence while advocating for responsible AI implementation in HE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04751v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prashant Mahajan</dc:creator>
    </item>
    <item>
      <title>Transforming Student Evaluation with Adaptive Intelligence and Performance Analytics</title>
      <link>https://arxiv.org/abs/2503.04752</link>
      <description>arXiv:2503.04752v1 Announce Type: new 
Abstract: The development in Artificial Intelligence (AI) offers transformative potential for redefining student assessment methodologies. This paper aims to establish the idea of the advancement of Artificial Intelligence (AI) and its prospect in reshaping approaches to assessing students. It creates a system for the evaluation of students performance using Artificial intelligence, and particularly the Gemini API for the generation of questions, grading and report on the students performances. This is to facilitate easy use of the tools in creating, scheduling, and delivering assessments with minimal chances of cheating through options such as full screen and time limit. There are formats of questions in the system which comprises multiple choice, short answers and descriptive questions, developed by Gemini. The most conspicuous feature is the self-checking system whereby the user gets instant feedback for the correct score that each of the students would have scored instantly with explanations about wrong answers. Moreover, the platform has intelligent learning progressions where the user will be able to monitor his/her performances to be recommended a certain level of performance. It will allow students as well as educators to have real-time analytics and feedback on what they are good at and where they need to improve. Not only does it make the assessment easier, but it also improves the levels of accuracy in grading and effectively strengthens a data based learning process for students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04752v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pushpalatha K S, Abhishek Mangalur, Ketan Hegde, Chetan Badachi, Mohammad Aamir</dc:creator>
    </item>
    <item>
      <title>From Analog to Digital -- Successful Implementation of IoT Solutions in the Petrochemical Industry</title>
      <link>https://arxiv.org/abs/2503.04753</link>
      <description>arXiv:2503.04753v1 Announce Type: new 
Abstract: This document describes the development and implementation of a technological solution based on IoT devices to modernize a machine known as the Cyclone. This equipment is used by a contractor collaborating with petrochemical companies in the state of Texas, performing specialized work in mechanics, engineering, catalytic material replacement, and rescue operations in refinery complexes. The Cyclone machine, with outdated relay logic technology, poses challenges in terms of operational efficiency, critical condition monitoring, and safety. The project was carried out with the collaboration of specialists in equipment handling, focusing on demonstrating the feasibility of integrating advanced Industry 4.0 technologies into legacy industrial equipment. The methodology included the incorporation of IoT sensors for real-time monitoring, an automated control system, and the digitization of key processes. Preliminary results indicate improvements in the precision of operational control and the ability for remote supervision, highlighting the potential for modernization in critical industrial applications. This work not only validates the use of IoT devices in obsolete equipment but also sets a precedent for the transition towards more sustainable and efficient technologies in the petrochemical sector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04753v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Noel Portillo</dc:creator>
    </item>
    <item>
      <title>GIS as a Job Growth Area for IT Professionals</title>
      <link>https://arxiv.org/abs/2503.04754</link>
      <description>arXiv:2503.04754v1 Announce Type: new 
Abstract: As more companies look to capitalize on the benefits of geospatial data, Geographic Information Systems provide an area for growth in the Information Technology job sector in the United States. Careers in GIS require geography, cartography, and IT skills. As the industry grows, candidates with these types of skills that are in demand and are needed to advance the geospatial industry forward. This industry is not generally known as a growth area to many IT professionals, and due to misleading job postings, many candidates may not know their skills are in demand</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04754v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>2015 World of Computer Science and Information Technology Journal (WCSIT) ISSN: 2221-0741 Vol. 5, No. 6, 98-111</arxiv:journal_reference>
      <dc:creator>Timur Mirzoev, Anthony Moore, Brianna Pryzbysz, Melissa Taylor, John Centeno</dc:creator>
    </item>
    <item>
      <title>NutriTransform: Estimating Nutritional Information From Online Food Posts</title>
      <link>https://arxiv.org/abs/2503.04755</link>
      <description>arXiv:2503.04755v1 Announce Type: new 
Abstract: Deriving nutritional information from online food posts is challenging, particularly when users do not explicitly log the macro-nutrients of a shared meal. In this work, we present an efficient and straightforward approach to approximating macro-nutrients based solely on the titles of food posts. Our method combines a public food database from the U.S. Department of Agriculture with advanced text embedding techniques. We evaluate the approach on a labeled food dataset, demonstrating its effectiveness, and apply it to over 500,000 real-world posts from Reddit's popular /r/food subreddit to uncover trends in food-sharing behavior based on the estimated macro-nutrient content. Altogether, this work lays a foundation for researchers and practitioners aiming to estimate caloric and nutritional content using only text data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04755v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thorsten Ruprechter, Marion Garaus, Ivo Ponocny, Denis Helic</dc:creator>
    </item>
    <item>
      <title>Peeking Behind Closed Doors: Risks of LLM Evaluation by Private Data Curators</title>
      <link>https://arxiv.org/abs/2503.04756</link>
      <description>arXiv:2503.04756v1 Announce Type: new 
Abstract: The rapid advancement in building large language models (LLMs) has intensified competition among big-tech companies and AI startups. In this regard, model evaluations are critical for product and investment-related decision-making. While open evaluation sets like MMLU initially drove progress, concerns around data contamination and data bias have constantly questioned their reliability. As a result, it has led to the rise of private data curators who have begun conducting hidden evaluations with high-quality self-curated test prompts and their own expert annotators. In this paper, we argue that despite potential advantages in addressing contamination issues, private evaluations introduce inadvertent financial and evaluation risks. In particular, the key concerns include the potential conflict of interest arising from private data curators' business relationships with their clients (leading LLM firms). In addition, we highlight that the subjective preferences of private expert annotators will lead to inherent evaluation bias towards the models trained with the private curators' data. Overall, this paper lays the foundation for studying the risks of private evaluations that can lead to wide-ranging community discussions and policy changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04756v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hritik Bansal, Pratyush Maini</dc:creator>
    </item>
    <item>
      <title>Electricity Demand Forecasting in Future Grid States: A Digital Twin-Based Simulation Study</title>
      <link>https://arxiv.org/abs/2503.04757</link>
      <description>arXiv:2503.04757v1 Announce Type: new 
Abstract: Short-term forecasting of residential electricity demand is an important task for utilities. Yet, many small and medium-sized utilities still use simple forecasting approaches such as Synthesized Load Profiles, which treat residential households similarly and neither account for renewable energy installations nor novel large consumers (e.g., heat pumps, electric vehicles). The effectiveness of such "one-fits-all" approaches in future grid states--where decentral generation and sector coupling increases--are questionable. Our study challenges these forecasting practices and investigates whether Machine Learning (ML) approaches are suited to predict electricity demand in today's and in future grid states. We use real smart meter data from 3,511 households in Germany over 34 months. We extrapolate this data with future grid states (i.e., increased decentral generation and storage) based on a digital twin of a local energy system. Our results show that Long Short-Term Memory (LSTM) approaches outperform SLPs as well as simple benchmark estimators with up to 68.5% lower Root Mean Squared Error for a day-ahead forecast, especially in future grid states. Nevertheless, all prediction approaches perform worse in future grid states. Our findings therefore reinforce the need (a) for utilities and grid operators to employ ML approaches instead of traditional demand prediction methods in future grid states and (b) to prepare current ML methods for future grid states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04757v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.23919/SpliTech61897.2024.10612563</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 9th International Conference on Smart and Sustainable Technologies (SpliTech 2024)</arxiv:journal_reference>
      <dc:creator>Daniel R. Bayer, Felix Haag, Marco Pruckner, Konstantin Hopf</dc:creator>
    </item>
    <item>
      <title>Chat-GPT: An AI Based Educational Revolution</title>
      <link>https://arxiv.org/abs/2503.04758</link>
      <description>arXiv:2503.04758v1 Announce Type: new 
Abstract: The AI revolution is gathering momentum at an unprecedented rate. Over the past decade, we have witnessed a seemingly inevitable integration of AI in every facet of our lives. Much has been written about the potential revolutionary impact of AI in education. AI has the potential to completely revolutionise the educational landscape as we could see entire courses and degrees developed by programs such as ChatGPT. AI has the potential to develop courses, set assignments, grade and provide feedback to students much faster than a team of teachers. In addition, because of its dynamic nature, it has the potential to continuously improve its content. In certain fields such as computer science, where technology is continuously evolving, AI based applications can provide dynamically changing, relevant material to students. AI has the potential to replace entire degrees and may challenge the concept of higher education institutions. We could also see entire new disciplines emerge as a consequence of AI. This paper examines the practical impact of ChatGPT and why it is believed that its implementation is a critical step towards a new era of education. We investigate the impact that ChatGPT will have on learning, problem solving skills and cognitive ability of students. We examine the positives, negatives and many other aspects of AI and its applications throughout this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04758v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sasa Maric, Sonja Maric, Lana Maric</dc:creator>
    </item>
    <item>
      <title>Agentic AI and the Cyber Arms Race</title>
      <link>https://arxiv.org/abs/2503.04760</link>
      <description>arXiv:2503.04760v1 Announce Type: new 
Abstract: Agentic AI is shifting the cybersecurity landscape as attackers and defenders leverage AI agents to augment humans and automate common tasks. In this article, we examine the implications for cyber warfare and global politics as Agentic AI becomes more powerful and enables the broad proliferation of capabilities only available to the most well resourced actors today.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04760v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean Oesch, Jack Hutchins, Phillipe Austria, Amul Chaulagain</dc:creator>
    </item>
    <item>
      <title>Which Economic Tasks are Performed with AI? Evidence from Millions of Claude Conversations</title>
      <link>https://arxiv.org/abs/2503.04761</link>
      <description>arXiv:2503.04761v1 Announce Type: new 
Abstract: Despite widespread speculation about artificial intelligence's impact on the future of work, we lack systematic empirical evidence about how these systems are actually being used for different tasks. Here, we present a novel framework for measuring AI usage patterns across the economy. We leverage a recent privacy-preserving system to analyze over four million Claude.ai conversations through the lens of tasks and occupations in the U.S. Department of Labor's O*NET Database. Our analysis reveals that AI usage primarily concentrates in software development and writing tasks, which together account for nearly half of all total usage. However, usage of AI extends more broadly across the economy, with approximately 36% of occupations using AI for at least a quarter of their associated tasks. We also analyze how AI is being used for tasks, finding 57% of usage suggests augmentation of human capabilities (e.g., learning or iterating on an output) while 43% suggests automation (e.g., fulfilling a request with minimal human involvement). While our data and methods face important limitations and only paint a picture of AI usage on a single platform, they provide an automated, granular approach for tracking AI's evolving role in the economy and identifying leading indicators of future impact as these technologies continue to advance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04761v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunal Handa, Alex Tamkin, Miles McCain, Saffron Huang, Esin Durmus, Sarah Heck, Jared Mueller, Jerry Hong, Stuart Ritchie, Tim Belonax, Kevin K. Troy, Dario Amodei, Jared Kaplan, Jack Clark, Deep Ganguli</dc:creator>
    </item>
    <item>
      <title>Artificial intelligence for objective assessment of acrobatic movements: How to apply machine learning for identifying tumbling elements in cheer sports</title>
      <link>https://arxiv.org/abs/2503.04764</link>
      <description>arXiv:2503.04764v1 Announce Type: new 
Abstract: Over the past four decades, cheerleading has evolved from a sideline activity at major sporting events into a professional, competitive sport with growing global popularity. Evaluating tumbling elements in cheerleading relies on both objective measures and subjective judgments, such as difficulty and execution quality. However, the complexity of tumbling - encompassing team synchronicity, ground interactions, choreography, and artistic expression - makes objective assessment challenging. Artificial intelligence (AI) has revolutionized various scientific fields and industries through precise data-driven analyses, yet their application in acrobatic sports remains limited despite significant potential for enhancing performance evaluation and coaching. This study investigates the feasibility of using an AI-based approach with data from a single inertial measurement unit to accurately identify and objectively assess tumbling elements in standard cheerleading routines. A sample of 16 participants (13 females, 3 males) from a Division I collegiate cheerleading team wore a single inertial measurement unit at the dorsal pelvis. Over a 4-week seasonal preparation period, 1102 tumbling elements were recorded during regular practice sessions. Using triaxial accelerations and rotational speeds, various ML algorithms were employed to classify and evaluate the execution of tumbling manoeuvres. Results indicate that certain machine learning models can effectively identify different tumbling elements despite inter-individual variability and data noise, achieving high accuracy. These findings demonstrate the significant potential for integrating AI-driven assessments into cheerleading and other acrobatic sports, providing objective metrics that complement traditional judging methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04764v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophia Wesely, Ella Hofer, Robin Curth, Shyam Paryani, Nicole Mills, Olaf Uebersch\"ar, Julia Westermayr</dc:creator>
    </item>
    <item>
      <title>Generative AI in Academic Writing: A Comparison of DeepSeek, Qwen, ChatGPT, Gemini, Llama, Mistral, and Gemma</title>
      <link>https://arxiv.org/abs/2503.04765</link>
      <description>arXiv:2503.04765v1 Announce Type: new 
Abstract: Deepseek and Qwen LLMs became popular at the beginning of 2025 with their low-cost and open-access LLM solutions. A company based in Hangzhou, Zhejiang, China, announced its new LLM, DeepSeek v3, in December 2024. Then, Alibaba released its AI model, Qwen 2.5 Max, on January 29, 2025. These tools, which are free and open-source have made a significant impact on the world. Deepseek and Qwen also have the potential to be used by many researchers and individuals around the world in academic writing and content creation. Therefore, it is important to determine the capacity of these new LLMs to generate high-quality academic content. This study aims to evaluate the academic writing performance of both Qwen 2.5 Max and DeepSeek v3 by comparing these models with popular systems such as ChatGPT, Gemini, Llama, Mistral, and Gemma. In this research, 40 articles on the topics of Digital Twin and Healthcare were used. The method of this study involves using generative AI tools to generate texts based on posed questions and paraphrased abstracts of these 40 articles. Then, the generated texts were evaluated through the plagiarism tool, AI detection tools, word count comparisons, semantic similarity tools and readability assessments. It was observed that plagiarism test result rates were generally higher for the paraphrased abstract texts and lower for the answers generated to the questions, but both were above acceptable levels. In the evaluations made with the AI detection tool, it was determined with high accuracy that all the generated texts were detected as AI-generated. In terms of the generated word count comparison, it was evaluated that all chatbots generated satisfactory amount of content. Semantic similarity tests show that the generated texts have high semantic overlap with the original texts. The readability tests indicated that the generated texts were not sufficiently readable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04765v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omer Aydin, Enis Karaarslan, Fatih Safa Erenay, Nebojsa Bacanin</dc:creator>
    </item>
    <item>
      <title>Global AI Governance: Where the Challenge is the Solution- An Interdisciplinary, Multilateral, and Vertically Coordinated Approach</title>
      <link>https://arxiv.org/abs/2503.04766</link>
      <description>arXiv:2503.04766v1 Announce Type: new 
Abstract: Current global AI governance frameworks struggle with fragmented disciplinary collaboration, ineffective multilateral coordination, and disconnects between policy design and grassroots implementation. This study, guided by Integration and Implementation Science (IIS) initiated a structured interdisciplinary dialogue at the UN Science Summit, convening legal, NGO, and HCI experts to tackle those challenges. Drawing on the common ground of the experts: dynamism, experimentation, inclusivity, and paradoxical governance, this study, through thematic analysis and interdisciplinary comparison analysis, identifies four core principles of global AI governance. Furthermore, we translate these abstract principles into concrete action plans leveraging the distinct yet complementary perspectives of each discipline. These principles and action plans are then integrated into a five-phase, time-sequential framework including foundation building, experimental verification, collaborative optimization, global adaptation, and continuous evolution phases. This multilevel framework offers a novel and concrete pathway toward establishing interdisciplinary, multilateral, and vertically coordinated AI governance, transforming global AI governance challenges into opportunities for political actions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04766v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huixin Zhong, Thao Do, Ynagliu Jie, Rostam J. Neuwirth, Hong Shen</dc:creator>
    </item>
    <item>
      <title>A cross-regional review of AI safety regulations in the commercial aviation</title>
      <link>https://arxiv.org/abs/2503.04767</link>
      <description>arXiv:2503.04767v1 Announce Type: new 
Abstract: In this paper we examine the existing artificial intelligence (AI) policy documents in aviation for the following three regions: the United States, European Union, and China. The aviation industry has always been a first mover in adopting technological advancements. This early adoption offers valuable insights because of its stringent regulations and safety-critical procedures. As a result, the aviation industry provides an optimal platform to counter AI vulnerabilities through its tight regulations, standardization processes, and certification of new technologies. Keywords: AI in aviation; aviation safety; standardization; certifiable AI; regulations</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04767v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Penny A. Barr, Sohel M. Imroz</dc:creator>
    </item>
    <item>
      <title>Applications of Artificial Intelligence Tools to Enhance Legislative Engagement: Case Studies from Make.Org and MAPLE</title>
      <link>https://arxiv.org/abs/2503.04769</link>
      <description>arXiv:2503.04769v1 Announce Type: new 
Abstract: This paper is a collaboration between Make.org and the Massachusetts Platform for Legislative Engagement (MAPLE), two non-partisan civic technology organizations building novel AI deployments to improve democratic capacity. Make.org, a civic innovator in Europe, is developing massive online participative platforms that can engage hundreds of thousands or even millions of participants. MAPLE, a volunteer-led NGO in the United States, is creating an open-source platform to help constituents understand and engage more effectively with the state law-making process.
  We believe that assistive integrations of AI can meaningfully impact the equity, efficiency, and accessibility of democratic legislating. We draw generalizable lessons from our experience in designing, building, and operating civic engagement platforms with AI integrations. We discuss four dimensions of legislative engagement that benefit from AI integrations: (1) making information accessible, (2) facilitating expression, (3) supporting deliberation, and (4) synthesizing insights. We present learnings from current, in development, and contemplated AI-powered features, such as summarizing and organizing policy information, supporting users in articulating their perspectives, and synthesizing consensus and controversy in public opinion.
  We outline what challenges needed to be overcome to deploy these tools equitably and discuss how Make.org and MAPLE have implemented and iteratively improved those concepts to make citizen assemblies and policymaking more participatory and responsive. We compare and contrast the approaches of Make.org and MAPLE, as well as how jurisdictional differences alter the risks and opportunities for AI deployments seeking to improve democracy. We conclude with recommendations for governments and NGOs interested in enhancing legislative engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04769v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alicia Combaz, David Mas, Nathan Sanders, Matthew Victor</dc:creator>
    </item>
    <item>
      <title>High School Computer Science Participation: A 6-Year Enrollment Study</title>
      <link>https://arxiv.org/abs/2503.04770</link>
      <description>arXiv:2503.04770v1 Announce Type: new 
Abstract: High-quality computer science (CS) instruction is essential for preparing students to thrive in an increasingly technology-driven world. This research brief presents findings from a six-year longitudinal study of CS enrollments in seven public high schools from the 2018-2019 through the 2023-2024 academic years, drawing on the administrative data of over 15,000 students. Results show that overall enrollment in CS courses rose modestly from 10% to 15% between the 2018-2019 and 2022-2023 school years. Enrollment declined to 13% in 2023-2024, though the cause and persistence of this trend remains unknown. Additional analyses differentiate foundational and advanced CS courses as well as examine participation by sex and race, offering additional insight. As CS, artificial intelligence, and related fields become more important across our society, they also become a key component of a robust K-12 education. Analyzing and understanding these trends in CS enrollments is crucial to inform policy and instruction that encourage students to participate and succeed in CS; this research brief presents one such analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04770v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cynthia L. Blitz, David J. Amiel, Teresa G. Duncan</dc:creator>
    </item>
    <item>
      <title>What do Large Language Models Say About Animals? Investigating Risks of Animal Harm in Generated Text</title>
      <link>https://arxiv.org/abs/2503.04804</link>
      <description>arXiv:2503.04804v1 Announce Type: new 
Abstract: As machine learning systems become increasingly embedded in human society, their impact on the natural world continues to escalate. Technical evaluations have addressed a variety of potential harms from large language models (LLMs) towards humans and the environment, but there is little empirical work regarding harms towards nonhuman animals. Following the growing recognition of animal protection in regulatory and ethical AI frameworks, we present the Animal Harm Assessment (AHA), a novel evaluation of risks of animal harm in LLM-generated text. Our dataset comprises 1,850 curated questions from Reddit post titles and 2,500 synthetic questions based on 50 animal categories (e.g., cats, reptiles) and 50 ethical scenarios, with further 70-30 publi-private split. Scenarios include open-ended questions about how to treat animals, practical scenarios with potential animal harm, and willingness-to-pay measures for the prevention of animal harm. Using the LLM-as-a-judge framework, answers are evaluated for their potential to increase or decrease harm, and evaluations are debiased for the tendency to judge their own outputs more favorably. We show that AHA produces meaningful evaluation results when applied to frontier LLMs, revealing significant differences between models, animal categories, scenarios, and subreddits. We conclude with future directions for technical research and the challenges of building evaluations on complex social and moral topics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04804v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arturs Kanepajs, Aditi Basu, Sankalpa Ghose, Constance Li, Akshat Mehta, Ronak Mehta, Samuel David Tucker-Davis, Eric Zhou, Bob Fischer</dc:creator>
    </item>
    <item>
      <title>Quantifying the Relevance of Youth Research Cited in the US Policy Documents</title>
      <link>https://arxiv.org/abs/2503.04977</link>
      <description>arXiv:2503.04977v1 Announce Type: new 
Abstract: In recent years, there has been a growing concern and emphasis on conducting research beyond academic or scientific research communities, benefiting society at large. A well-known approach to measuring the impact of research on society is enumerating its policy citation(s). Despite the importance of research in informing policy, there is no concrete evidence to suggest the research's relevance in cited policy documents. This is concerning because it may increase the possibility of evidence used in policy being manipulated by individual, social, or political biases that may lead to inappropriate, fragmented, or archaic research evidence in policy. Therefore, it is crucial to identify the degree of relevance between research articles and citing policy documents. In this paper, we examined the scale of contextual relevance of youth-focused research in the referenced US policy documents using natural language processing techniques, state-of-the-art pre-trained Large Language Models (LLMs), and statistical analysis. Our experiments and analysis concluded that youth-related research articles that get US policy citations are mostly relevant to the citing policy documents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04977v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/BigData62323.2024.10825004</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE International Conference on Big Data (BigData), Washington, DC, USA, 2024, pp. 5271-5280</arxiv:journal_reference>
      <dc:creator>Miftahul Jannat Mokarrama, Hamed Alhoori</dc:creator>
    </item>
    <item>
      <title>Prevalence and Impacts of Image-Based Sexual Abuse Victimization: A Multinational Study</title>
      <link>https://arxiv.org/abs/2503.04988</link>
      <description>arXiv:2503.04988v1 Announce Type: new 
Abstract: Image-based sexual abuse (IBSA) refers to the nonconsensual creating, taking, or sharing of intimate images, including threats to share intimate images. Despite the significant harms of IBSA, there is limited data on its prevalence and how it affects different identity or demographic groups. This study examines prevalence of, impacts from, and responses to IBSA via a survey with over 16,000 adults in 10 countries. More than 1 in 5 (22.6%) respondents reported an experience of IBSA. Victimization rates were higher among LGBTQ+ and younger respondents. Although victimized at similar rates, women reported greater harms and negative impacts from IBSA than men. Nearly a third (30.9%) of victim-survivors did not report or disclose their experience to anyone. We provide large-scale, granular, baseline data on prevalence in a diverse set of countries to aid in the development of effective interventions that address the experiences and intersectional identities of victim-survivors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04988v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Umbach, Nicola Henry, Gemma Beard</dc:creator>
    </item>
    <item>
      <title>Towards democratic data agency: Attitudes and concerns about online data practices</title>
      <link>https://arxiv.org/abs/2503.05058</link>
      <description>arXiv:2503.05058v1 Announce Type: new 
Abstract: Recent studies reveal widespread concern and increasing lack of understanding about how personal data is collected, shared, and used online without consent. This issue is compounded by limited options available for digital citizens to understand, control and manage their data flows across platforms, underscoring the need to explore how this lack of trust and transparency affects citizens' data practices including their capacities to act in a modern knowledge society. Despite the promising research within this field, important demographics are often overlooked, particularly people from marginalized social groups such as elderly, socially and economically challenged communities, and younger participants. This paper addresses this gap by specifically focusing on these underrepresented groups, emphasizing the need for exploring their understandings and percepts of online data practices. Drawing on three semi-structured focus group interviews, the paper asks: to what extent can public attitudes and concerns about data sharing on the internet inform the potential strategies and frameworks necessary to enhance digital trust and democratic data agency particularly among marginalized groups in Denmark? The study explores the types of information, levels of transparency, and agency people desire in their daily online data practices. Additionally, it explores how these insights can potentially inform the future development of fair data strategies and technological approaches to enhance digital trust and democratic data agency. Key findings point out the need for transparent, accessible privacy policies and data management tools, emphasizing that transparency alone is insufficient without enhancing democratic agency to address trust issues and foster a more inclusive digital environment.
  Keywords: public understanding, personal data, digital trust, data practices, data agency</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05058v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels J. Gommesen</dc:creator>
    </item>
    <item>
      <title>Cybersafety Card Game: Empowering Digital Educators to Teach Cybersafety to Older Adults</title>
      <link>https://arxiv.org/abs/2503.05430</link>
      <description>arXiv:2503.05430v1 Announce Type: new 
Abstract: Digital inequality remains a significant barrier for many older adults, limiting their ability to navigate online spaces securely and confidently while increasing their susceptibility to cyber threats. In response, we propose a novel shedding-type card game for older adults to conceptually learn and reinforce cyber hygiene practices in educational settings. We asked digital educators to participate as players alongside older adults (n = 16), departing from their usual role as teachers, they collaborated and shared a unique learning experience. The cybersafety game addresses 4 key topics: handling scams, password management, responding to cyber attacks, and staying private. We adopted a mixed-method approach of think-aloud playtesting, semi-structured interviews, and surveys to evaluate the game's reception and impact. Participants reported highly favorable gameplay experiences and found the cybersafety advice useful. Player feedback informed game modifications, detailed in this paper, to further enhance the game's usability and educational value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05430v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719806</arxiv:DOI>
      <dc:creator>Jacob Camilleri, Ashley Sheil, Michelle O'Keeffe, Moya Cronin, Melanie Gruben, Hazel Murray</dc:creator>
    </item>
    <item>
      <title>Cognitive Bias Detection Using Advanced Prompt Engineering</title>
      <link>https://arxiv.org/abs/2503.05516</link>
      <description>arXiv:2503.05516v1 Announce Type: new 
Abstract: Cognitive biases, systematic deviations from rationality in judgment, pose significant challenges in generating objective content. This paper introduces a novel approach for real-time cognitive bias detection in user-generated text using large language models (LLMs) and advanced prompt engineering techniques. The proposed system analyzes textual data to identify common cognitive biases such as confirmation bias, circular reasoning, and hidden assumption. By designing tailored prompts, the system effectively leverages LLMs' capabilities to both recognize and mitigate these biases, improving the quality of human-generated content (e.g., news, media, reports). Experimental results demonstrate the high accuracy of our approach in identifying cognitive biases, offering a valuable tool for enhancing content objectivity and reducing the risks of biased decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05516v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Frederic Lemieux, Aisha Behr, Clara Kellermann-Bryant, Zaki Mohammed</dc:creator>
    </item>
    <item>
      <title>Evaluating open-source Large Language Models for automated fact-checking</title>
      <link>https://arxiv.org/abs/2503.05565</link>
      <description>arXiv:2503.05565v1 Announce Type: new 
Abstract: The increasing prevalence of online misinformation has heightened the demand for automated fact-checking solutions. Large Language Models (LLMs) have emerged as potential tools for assisting in this task, but their effectiveness remains uncertain. This study evaluates the fact-checking capabilities of various open-source LLMs, focusing on their ability to assess claims with different levels of contextual information. We conduct three key experiments: (1) evaluating whether LLMs can identify the semantic relationship between a claim and a fact-checking article, (2) assessing models' accuracy in verifying claims when given a related fact-checking article, and (3) testing LLMs' fact-checking abilities when leveraging data from external knowledge sources such as Google and Wikipedia. Our results indicate that LLMs perform well in identifying claim-article connections and verifying fact-checked stories but struggle with confirming factual news, where they are outperformed by traditional fine-tuned models such as RoBERTa. Additionally, the introduction of external knowledge does not significantly enhance LLMs' performance, calling for more tailored approaches. Our findings highlight both the potential and limitations of LLMs in automated fact-checking, emphasizing the need for further refinements before they can reliably replace human fact-checkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05565v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolo' Fontana, Francesco Corso, Enrico Zuccolotto, Francesco Pierri</dc:creator>
    </item>
    <item>
      <title>Compliance of AI Systems</title>
      <link>https://arxiv.org/abs/2503.05571</link>
      <description>arXiv:2503.05571v1 Announce Type: new 
Abstract: The increasing integration of artificial intelligence (AI) systems in various fields requires solid concepts to ensure compliance with upcoming legislation. This paper systematically examines the compliance of AI systems with relevant legislation, focusing on the EU's AI Act and the compliance of data sets. The analysis highlighted many challenges associated with edge devices, which are increasingly being used to deploy AI applications closer and closer to the data sources. Such devices often face unique issues due to their decentralized nature and limited computing resources for implementing sophisticated compliance mechanisms. By analyzing AI implementations, the paper identifies challenges and proposes the first best practices for legal compliance when developing, deploying, and running AI. The importance of data set compliance is highlighted as a cornerstone for ensuring the trustworthiness, transparency, and explainability of AI systems, which must be aligned with ethical standards set forth in regulatory frameworks such as the AI Act. The insights gained should contribute to the ongoing discourse on the responsible development and deployment of embedded AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05571v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julius Sch\"oning, Niklas Kruse</dc:creator>
    </item>
    <item>
      <title>Nuanced Safety for Generative AI: How Demographics Shape Responsiveness to Severity</title>
      <link>https://arxiv.org/abs/2503.05609</link>
      <description>arXiv:2503.05609v1 Announce Type: new 
Abstract: Ensuring safety of Generative AI requires a nuanced understanding of pluralistic viewpoints. In this paper, we introduce a novel data-driven approach for calibrating granular ratings in pluralistic datasets. Specifically, we address the challenge of interpreting responses of a diverse population to safety expressed via ordinal scales (e.g., Likert scale). We distill non-parametric responsiveness metrics that quantify the consistency of raters in scoring the varying levels of the severity of safety violations. Using safety evaluation of AI-generated content as a case study, we investigate how raters from different demographic groups (age, gender, ethnicity) use an ordinal scale to express their perception of the severity of violations in a pluralistic safety dataset. We apply our metrics across violation types, demonstrating their utility in extracting nuanced insights that are crucial for developing reliable AI systems in a multi-cultural contexts. We show that our approach offers improved capabilities for prioritizing safety concerns by capturing nuanced viewpoints across different demographic groups, hence improving the reliability of pluralistic data collection and in turn contributing to more robust AI evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05609v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pushkar Mishra, Charvi Rastogi, Stephen R. Pfohl, Alicia Parrish, Roma Patel, Mark Diaz, Ding Wang, Michela Paganini, Vinodkumar Prabhakaran, Lora Aroyo, Verena Rieser</dc:creator>
    </item>
    <item>
      <title>Superintelligence Strategy: Expert Version</title>
      <link>https://arxiv.org/abs/2503.05628</link>
      <description>arXiv:2503.05628v1 Announce Type: new 
Abstract: Rapid advances in AI are beginning to reshape national security. Destabilizing AI developments could rupture the balance of power and raise the odds of great-power conflict, while widespread proliferation of capable AI hackers and virologists would lower barriers for rogue actors to cause catastrophe. Superintelligence -- AI vastly better than humans at nearly all cognitive tasks -- is now anticipated by AI researchers. Just as nations once developed nuclear strategies to secure their survival, we now need a coherent superintelligence strategy to navigate a new period of transformative change. We introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence regime resembling nuclear mutual assured destruction (MAD) where any state's aggressive bid for unilateral AI dominance is met with preventive sabotage by rivals. Given the relative ease of sabotaging a destabilizing AI project -- through interventions ranging from covert cyberattacks to potential kinetic strikes on datacenters -- MAIM already describes the strategic picture AI superpowers find themselves in. Alongside this, states can increase their competitiveness by bolstering their economies and militaries through AI, and they can engage in nonproliferation to rogue actors to keep weaponizable AI capabilities out of their hands. Taken together, the three-part framework of deterrence, nonproliferation, and competitiveness outlines a robust strategy to superintelligence in the years ahead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05628v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Dan Hendrycks, Eric Schmidt, Alexandr Wang</dc:creator>
    </item>
    <item>
      <title>DiMA: An LLM-Powered Ride-Hailing Assistant at DiDi</title>
      <link>https://arxiv.org/abs/2503.04768</link>
      <description>arXiv:2503.04768v1 Announce Type: cross 
Abstract: On-demand ride-hailing services like DiDi, Uber, and Lyft have transformed urban transportation, offering unmatched convenience and flexibility. In this paper, we introduce DiMA, an LLM-powered ride-hailing assistant deployed in DiDi Chuxing. Its goal is to provide seamless ride-hailing services and beyond through a natural and efficient conversational interface under dynamic and complex spatiotemporal urban contexts. To achieve this, we propose a spatiotemporal-aware order planning module that leverages external tools for precise spatiotemporal reasoning and progressive order planning. Additionally, we develop a cost-effective dialogue system that integrates multi-type dialog repliers with cost-aware LLM configurations to handle diverse conversation goals and trade-off response quality and latency. Furthermore, we introduce a continual fine-tuning scheme that utilizes real-world interactions and simulated dialogues to align the assistant's behavior with human preferred decision-making processes. Since its deployment in the DiDi application, DiMA has demonstrated exceptional performance, achieving 93% accuracy in order planning and 92% in response generation during real-world interactions. Offline experiments further validate DiMA capabilities, showing improvements of up to 70.23% in order planning and 321.27% in response generation compared to three state-of-the-art agent frameworks, while reducing latency by $0.72\times$ to $5.47\times$. These results establish DiMA as an effective, efficient, and intelligent mobile assistant for ride-hailing services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04768v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yansong Ning, Shuowei Cai, Wei Li, Jun Fang, Naiqiang Tan, Hua Chai, Hao Liu</dc:creator>
    </item>
    <item>
      <title>Invisible Walls in Cities: Leveraging Large Language Models to Predict Urban Segregation Experience with Social Media Content</title>
      <link>https://arxiv.org/abs/2503.04773</link>
      <description>arXiv:2503.04773v1 Announce Type: cross 
Abstract: Understanding experienced segregation in urban daily life is crucial for addressing societal inequalities and fostering inclusivity. The abundance of user-generated reviews on social media encapsulates nuanced perceptions and feelings associated with different places, offering rich insights into segregation. However, leveraging this data poses significant challenges due to its vast volume, ambiguity, and confluence of diverse perspectives. To tackle these challenges, we propose using Large Language Models (LLMs) to automate online review mining for segregation prediction. We design a Reflective LLM Coder to digest social media content into insights consistent with real-world feedback, and eventually produce a codebook capturing key dimensions that signal segregation experience, such as cultural resonance and appeal, accessibility and convenience, and community engagement and local involvement. Guided by the codebook, LLMs can generate both informative review summaries and ratings for segregation prediction. Moreover, we design a REasoning-and-EMbedding (RE'EM) framework, which combines the reasoning and embedding capabilities of language models to integrate multi-channel features for segregation prediction. Experiments on real-world data demonstrate that our framework greatly improves prediction accuracy, with a 22.79% elevation in R2 and a 9.33% reduction in MSE. The derived codebook is generalizable across three different cities, consistently improving prediction accuracy.Moreover, our user study confirms that the codebook-guided summaries provide cognitive gains for human participants in perceiving POIs' social inclusiveness.Our study marks an important step toward understanding implicit social barriers and inequalities, demonstrating the great potential of promoting social inclusiveness with AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04773v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bingbing Fan, Lin Chen, Songwei Li, Jian Yuan, Fengli Xu, Pan Hui, Yong Li</dc:creator>
    </item>
    <item>
      <title>Bangla Fake News Detection Based On Multichannel Combined CNN-LSTM</title>
      <link>https://arxiv.org/abs/2503.04781</link>
      <description>arXiv:2503.04781v1 Announce Type: cross 
Abstract: There have recently been many cases of unverified or misleading information circulating quickly over bogus web networks and news portals. This false news creates big damage to society and misleads people. For Example, in 2019, there was a rumor that the Padma Bridge of Bangladesh needed 100,000 human heads for sacrifice. This rumor turns into a deadly position and this misleading information takes the lives of innocent people. There is a lot of work in English but a few works in Bangla. In this study, we are going to identify the fake news from the unconsidered news source to provide the newsreader with natural news or real news. The paper is based on the combination of convolutional neural network (CNN) and long short-term memory (LSTM), where CNN is used for deep feature extraction and LSTM is used for detection using the extracted feature. The first thing we did to deploy this piece of work was data collection. We compiled a data set from websites and attempted to deploy it using the methodology of deep learning which contains about 50k of news. With the proposed model of Multichannel combined CNN-LSTM architecture, our model gained an accuracy of 75.05%, which is a good sign for detecting fake news in Bangla.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04781v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCCNT51525.2021.9580035</arxiv:DOI>
      <dc:creator>Md. Zahin Hossain George, Naimul Hossain, Md. Rafiuzzaman Bhuiyan, Abu Kaisar Mohammad Masum, Sheikh Abujar</dc:creator>
    </item>
    <item>
      <title>Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice</title>
      <link>https://arxiv.org/abs/2503.04785</link>
      <description>arXiv:2503.04785v1 Announce Type: cross 
Abstract: The rapid proliferation of Large Language Models (LLMs) has raised pressing concerns regarding their trustworthiness, spanning issues of reliability, transparency, fairness, and ethical alignment. Despite the increasing adoption of LLMs across various domains, there remains a lack of consensus on how to operationalize trustworthiness in practice. This study bridges the gap between theoretical discussions and implementation by conducting a bibliometric mapping analysis of 2,006 publications from 2019 to 2025. Through co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, we identify key research trends, influential authors, and prevailing definitions of LLM trustworthiness. Additionally, a systematic review of 68 core papers is conducted to examine conceptualizations of trust and their practical implications. Our findings reveal that trustworthiness in LLMs is often framed through existing organizational trust frameworks, emphasizing dimensions such as ability, benevolence, and integrity. However, a significant gap exists in translating these principles into concrete development strategies. To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, and post-training audits. By synthesizing bibliometric insights with practical strategies, this study contributes towards fostering more transparent, accountable, and ethically aligned LLMs, ensuring their responsible deployment in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04785v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jos\'e Siqueira de Cerqueira, Kai-Kristian Kemell, Rebekah Rousi, Nannan Xi, Juho Hamari, Pekka Abrahamsson</dc:creator>
    </item>
    <item>
      <title>Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2503.04816</link>
      <description>arXiv:2503.04816v1 Announce Type: cross 
Abstract: Dancing in a duet often requires a heightened attunement to one's partner: their orientation in space, their momentum, and the forces they exert on you. Dance artists who work in partnered settings might have a strong embodied understanding in the moment of how their movements relate to their partner's, but typical documentation of dance fails to capture these varied and subtle relationships. Working closely with dance artists interested in deepening their understanding of partnering, we leverage Graph Neural Networks (GNNs) to highlight and interpret the intricate connections shared by two dancers. Using a video-to-3D-pose extraction pipeline, we extract 3D movements from curated videos of contemporary dance duets, apply a dedicated pre-processing to improve the reconstruction, and train a GNN to predict weighted connections between the dancers. By visualizing and interpreting the predicted relationships between the two movers, we demonstrate the potential for graph-based methods to construct alternate models of the collaborative dynamics of duets. Finally, we offer some example strategies for how to use these insights to inform a generative and co-creative studio practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04816v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Luis Vitor Zerkowski, Zixuan Wang, Ilya Vidrin, Mariel Pettee</dc:creator>
    </item>
    <item>
      <title>Preserving Cultural Identity with Context-Aware Translation Through Multi-Agent AI Systems</title>
      <link>https://arxiv.org/abs/2503.04827</link>
      <description>arXiv:2503.04827v1 Announce Type: cross 
Abstract: Language is a cornerstone of cultural identity, yet globalization and the dominance of major languages have placed nearly 3,000 languages at risk of extinction. Existing AI-driven translation models prioritize efficiency but often fail to capture cultural nuances, idiomatic expressions, and historical significance, leading to translations that marginalize linguistic diversity. To address these challenges, we propose a multi-agent AI framework designed for culturally adaptive translation in underserved language communities. Our approach leverages specialized agents for translation, interpretation, content synthesis, and bias evaluation, ensuring that linguistic accuracy and cultural relevance are preserved. Using CrewAI and LangChain, our system enhances contextual fidelity while mitigating biases through external validation. Comparative analysis shows that our framework outperforms GPT-4o, producing contextually rich and culturally embedded translations, a critical advancement for Indigenous, regional, and low-resource languages. This research underscores the potential of multi-agent AI in fostering equitable, sustainable, and culturally sensitive NLP technologies, aligning with the AI Governance, Cultural NLP, and Sustainable NLP pillars of Language Models for Underserved Communities. Our full experimental codebase is publicly available at: https://github.com/ciol-researchlab/Context-Aware_Translation_MAS</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04827v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mahfuz Ahmed Anik, Abdur Rahman, Azmine Toushik Wasi, Md Manjurul Ahsan</dc:creator>
    </item>
    <item>
      <title>Enhancing Collective Intelligence in Large Language Models Through Emotional Integration</title>
      <link>https://arxiv.org/abs/2503.04849</link>
      <description>arXiv:2503.04849v1 Announce Type: cross 
Abstract: This research investigates the integration of emotional diversity into Large Language Models (LLMs) to enhance collective intelligence. Inspired by the human wisdom of crowds phenomenon, where group decisions often outperform individual judgments, we fine-tuned the DarkIdol-Llama-3.1-8B model using Google's GoEmotions dataset and Low-Rank Adaptation (LoRA) to simulate emotionally diverse responses. Evaluating the model on a distance estimation task between Fargo, ND, and Seattle, WA, across 15,064 unique persona configurations, we analyzed how emotional states and social attributes influence decision-making. Our findings demonstrate that emotional integration shapes response patterns while maintaining acceptable prediction accuracy, revealing its potential to enhance artificial collective intelligence. This study provides valuable insights into the interplay of emotional diversity and decision-making in LLMs, suggesting pathways for creating emotionally aware AI systems that balance emotional depth with analytical precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04849v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Likith Kadiyala, Ramteja Sajja, Yusuf Sermet, Ibrahim Demir</dc:creator>
    </item>
    <item>
      <title>Codebook Reduction and Saturation: Novel observations on Inductive Thematic Saturation for Large Language Models and initial coding in Thematic Analysis</title>
      <link>https://arxiv.org/abs/2503.04859</link>
      <description>arXiv:2503.04859v1 Announce Type: cross 
Abstract: This paper reflects on the process of performing Thematic Analysis with Large Language Models (LLMs). Specifically, the paper deals with the problem of analytical saturation of initial codes, as produced by LLMs. Thematic Analysis is a well-established qualitative analysis method composed of interlinked phases. A key phase is the initial coding, where the analysts assign labels to discrete components of a dataset. Saturation is a way to measure the validity of a qualitative analysis and relates to the recurrence and repetition of initial codes. In the paper we reflect on how well LLMs achieve analytical saturation and propose also a novel technique to measure Inductive Thematic Saturation (ITS). This novel technique leverages a programming framework called DSPy. The proposed novel approach allows a precise measurement of ITS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04859v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stefano De Paoli, Walter Stan Mathis</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Grasp Concepts in Visual Content? A Case Study on YouTube Shorts about Depression</title>
      <link>https://arxiv.org/abs/2503.05109</link>
      <description>arXiv:2503.05109v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used to assist computational social science research. While prior efforts have focused on text, the potential of leveraging multimodal LLMs (MLLMs) for online video studies remains underexplored. We conduct one of the first case studies on MLLM-assisted video content analysis, comparing AI's interpretations to human understanding of abstract concepts. We leverage LLaVA-1.6 Mistral 7B to interpret four abstract concepts regarding video-mediated self-disclosure, analyzing 725 keyframes from 142 depression-related YouTube short videos. We perform a qualitative analysis of MLLM's self-generated explanations and found that the degree of operationalization can influence MLLM's interpretations. Interestingly, greater detail does not necessarily increase human-AI alignment. We also identify other factors affecting AI alignment with human understanding, such as concept complexity and versatility of video genres. Our exploratory study highlights the need to customize prompts for specific concepts and calls for researchers to incorporate more human-centered evaluations when working with AI systems in a multimodal context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05109v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706599.3719821</arxiv:DOI>
      <arxiv:journal_reference>CHI Conference on Human Factors in Computing Systems (CHI EA 2025)</arxiv:journal_reference>
      <dc:creator>Jiaying "Lizzy" Liu, Yiheng Su, Praneel Seth</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of How People With and Without ADHD Recognise and Avoid Dark Patterns on Social Media</title>
      <link>https://arxiv.org/abs/2503.05263</link>
      <description>arXiv:2503.05263v1 Announce Type: cross 
Abstract: Dark patterns are deceptive strategies that recent work in human-computer interaction (HCI) has captured throughout digital domains, including social networking sites (SNSs). While research has identified difficulties among people to recognise dark patterns effectively, few studies consider vulnerable populations and their experience in this regard, including people with attention deficit hyperactivity disorder (ADHD), who may be especially susceptible to attention-grabbing tricks. Based on an interactive web study with 135 participants, we investigate SNS users' ability to recognise and avoid dark patterns by comparing results from participants with and without ADHD. In line with prior work, we noticed overall low recognition of dark patterns with no significant differences between the two groups. Yet, ADHD individuals were able to avoid specific dark patterns more often. Our results advance previous work by understanding dark patterns in a realistic environment and offer insights into their effect on vulnerable populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05263v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3706598.3713776</arxiv:DOI>
      <arxiv:journal_reference>CHI Conference on Human Factors in Computing Systems. April 26-May 1, 2025. Yokohama, Japan</arxiv:journal_reference>
      <dc:creator>Thomas Mildner, Daniel Fidel, Evropi Stefanidi, Pawel W. Wozniak, Rainer Malaka, Jasmin Niess</dc:creator>
    </item>
    <item>
      <title>Automatic Teaching Platform on Vision Language Retrieval Augmented Generation</title>
      <link>https://arxiv.org/abs/2503.05464</link>
      <description>arXiv:2503.05464v1 Announce Type: cross 
Abstract: Automating teaching presents unique challenges, as replicating human interaction and adaptability is complex. Automated systems cannot often provide nuanced, real-time feedback that aligns with students' individual learning paces or comprehension levels, which can hinder effective support for diverse needs. This is especially challenging in fields where abstract concepts require adaptive explanations. In this paper, we propose a vision language retrieval augmented generation (named VL-RAG) system that has the potential to bridge this gap by delivering contextually relevant, visually enriched responses that can enhance comprehension. By leveraging a database of tailored answers and images, the VL-RAG system can dynamically retrieve information aligned with specific questions, creating a more interactive and engaging experience that fosters deeper understanding and active student participation. It allows students to explore concepts visually and verbally, promoting deeper understanding and reducing the need for constant human oversight while maintaining flexibility to expand across different subjects and course material.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05464v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ruslan Gokhman, Jialu Li, Youshan Zhang</dc:creator>
    </item>
    <item>
      <title>The Software Diversity Card: A Framework for Reporting Diversity in Software Projects</title>
      <link>https://arxiv.org/abs/2503.05470</link>
      <description>arXiv:2503.05470v1 Announce Type: cross 
Abstract: The interest and concerns about diversity in software development have soared in recent years. Reporting diversity-related aspects of software projects can increase user trust and help regulators evaluate potential adoption. Furthermore, recent directives around AI are beginning to require diversity information in the development of AI products, indicating the growing interest of public regulators in it. Despite this importance, current documentation assets in software development processes frequently overlook diversity in favor of technical features, partly due to a lack of tools for describing and annotating diversity.
  This work introduces the Software Diversity Card, a comprehensive framework for reporting diversity-related aspects of software projects. The card is designed to profile the different types of teams involved in developing and governing software projects (including the final user groups involved in testing), and the software adaptations for specific social groups. To encourage its adoption, we provide a diversity modeling language, a toolkit for generating the cards using such language, and a collection of real-world examples from active software projects. Our proposal can enhance diversity practices in software development e.g., through open-source projects like the CONTRIBUTING.md file), support public administrations in software assessment, and help businesses promote diversity as a key asset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05470v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joan Giner-Miguelez, Sergio Morales, Sergio Cobos, Javier Luis Canovas Izquierdo, Robert Clariso, Jordi Cabot</dc:creator>
    </item>
    <item>
      <title>A Bot-based Approach to Manage Codes of Conduct in Open-Source Projects</title>
      <link>https://arxiv.org/abs/2503.05479</link>
      <description>arXiv:2503.05479v1 Announce Type: cross 
Abstract: The development of Open-Source Software (OSS) projects relies on the collaborative work of contributors, generally scattered around the world. To enable this collaboration, OSS projects are hosted on social-coding platforms like GitHub, which provide the infrastructure to host the code as well as the support for enabling the participation of the community. The potentially rich and diverse mixture of contributors in OSS projects makes their management not only a technical challenge, where automation tools and bots are usually deployed, but also a social one. To this aim, OSS projects have been increasingly deploying a declaration of their code of conduct, which defines rules to ensure a respectful and inclusive participatory environment in the community, being the Contributor Covenant the main model to follow. However, the broad adoption and enforcement of codes of conduct in OSS projects is still limited. In particular, the definition, deployment, and enforcement of codes of conduct is a very challenging task. In this paper, we propose an approach to effectively manage codes of conduct in OSS projects based on the Contributor Covenant proposal. Our solution has been implemented as a bot-based solution where bots help in the definition of codes of conduct, the monitoring of OSS projects, and the enforcement of ethical rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05479v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sergio Cobos, Javier Luis C\'anovas Izquierdo</dc:creator>
    </item>
    <item>
      <title>Synchronization between media followers and political supporters during an election process: towards a real time study</title>
      <link>https://arxiv.org/abs/2503.05552</link>
      <description>arXiv:2503.05552v1 Announce Type: cross 
Abstract: We present an analysis of the dynamics of discussions in Twitter (before it became X) among supporters of various candidates in the 2022 French presidential election, and followers of different types of media. Our study demonstrates that we can automatically detect the synchronization of interest among different groups around specific topics at particular times.
  We introduce two complementary methods for constructing dynamic semantic networks, each with its own advantages. The growing aggregated network helps identify the reactivation of past topics, while the rolling window network is more sensitive to emerging discussions that, despite their significance, may appear suddenly and have a short lifespan. These two approaches offer distinct perspectives on the discussion landscape. Rather than choosing between them, we advocate for using both, as their comparison provides valuable insights at a relatively low computational and storage cost. Our findings confirm and quantify, on a larger scale and in an automatic, agnostic manner, observations previously made using more qualitative methods. We believed this work represents a step forward in developing methodologies to assess equity in information treatment, an obligation imposed by law on broadcasters that use broadcast spectrum frequencies in certain countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05552v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\'emi Perrier, Laura Hern\'andez, J. Ignacio Alvarez-Hamelin, Mariano G. Beir\'o Dimitris Kotzinos</dc:creator>
    </item>
    <item>
      <title>Identification and explanation of disinformation in wiki data streams</title>
      <link>https://arxiv.org/abs/2503.05605</link>
      <description>arXiv:2503.05605v1 Announce Type: cross 
Abstract: Social media platforms, increasingly used as news sources for varied data analytics, have transformed how information is generated and disseminated. However, the unverified nature of this content raises concerns about trustworthiness and accuracy, potentially negatively impacting readers' critical judgment due to disinformation. This work aims to contribute to the automatic data quality validation field, addressing the rapid growth of online content on wiki pages. Our scalable solution includes stream-based data processing with feature engineering, feature analysis and selection, stream-based classification, and real-time explanation of prediction outcomes. The explainability dashboard is designed for the general public, who may need more specialized knowledge to interpret the model's prediction. Experimental results on two datasets attain approximately 90 % values across all evaluation metrics, demonstrating robust and competitive performance compared to works in the literature. In summary, the system assists editors by reducing their effort and time in detecting disinformation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05605v1</guid>
      <category>cs.IR</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1177/10692509241306580</arxiv:DOI>
      <dc:creator>Francisco de Arriba-P\'erez, Silvia Garc\'ia-M\'endez, F\'atima Leal, Benedita Malheiro, Juan C Burguillo</dc:creator>
    </item>
    <item>
      <title>Keep the Future Human: Why and How We Should Close the Gates to AGI and Superintelligence, and What We Should Build Instead</title>
      <link>https://arxiv.org/abs/2311.09452</link>
      <description>arXiv:2311.09452v4 Announce Type: replace 
Abstract: Dramatic advances in artificial intelligence over the past decade (for narrow-purpose AI) and the last several years (for general-purpose AI) have transformed AI from a niche academic field to the core business strategy of many of the world's largest companies, with hundreds of billions of dollars in annual investment in the techniques and technologies for advancing AI's capabilities. We now come to a critical juncture. As the capabilities of new AI systems begin to match and exceed those of humans across many cognitive domains, humanity must decide: how far do we go, and in what direction? This essay argues that we should keep the future human by closing the "gates" to smarter-than-human, autonomous, general-purpose AI -- sometimes called "AGI" -- and especially to the highly-superhuman version sometimes called "superintelligence." Instead, we should focus on powerful, trustworthy AI tools that can empower individuals and transformatively improve human societies' abilities to do what they do best.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09452v4</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony Aguirre</dc:creator>
    </item>
    <item>
      <title>GreenDFL: a Framework for Assessing the Sustainability of Decentralized Federated Learning Systems</title>
      <link>https://arxiv.org/abs/2502.20242</link>
      <description>arXiv:2502.20242v2 Announce Type: replace 
Abstract: Decentralized Federated Learning (DFL) is an emerging paradigm that enables collaborative model training without centralized data and model aggregation, enhancing privacy and resilience. However, its sustainability remains underexplored, as energy consumption and carbon emissions vary across different system configurations. Understanding the environmental impact of DFL is crucial for optimizing its design and deployment. This work aims to develop a comprehensive and operational framework for assessing the sustainability of DFL systems. To address it, this work provides a systematic method for quantifying energy consumption and carbon emissions, offering insights into improving the sustainability of DFL. This work proposes GreenDFL, a fully implementable framework that has been integrated into a real-world DFL platform. GreenDFL systematically analyzes the impact of various factors, including hardware accelerators, model architecture, communication medium, data distribution, network topology, and federation size, on the sustainability of DFL systems. Besides, a sustainability-aware aggregation algorithm (GreenDFL-SA) and a node selection algorithm (GreenDFL-SN) are developed to optimize energy efficiency and reduce carbon emissions in DFL training. Empirical experiments are conducted on multiple datasets, measuring energy consumption and carbon emissions at different phases of the DFL lifecycle. The proposed GreenDFL provides a comprehensive and practical approach for assessing the sustainability of DFL systems. Furthermore, it offers best practices for improving environmental efficiency in DFL, making sustainability considerations more actionable in real-world deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20242v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Feng, Alberto Huertas Celdr\'an, Xi Cheng, G\'er\^ome Bovet, Burkhard Stiller</dc:creator>
    </item>
    <item>
      <title>The Effect of Warm-Glow on User Behavioral Intention to Adopt Technology: Extending the UTAUT2 Model</title>
      <link>https://arxiv.org/abs/2210.01242</link>
      <description>arXiv:2210.01242v2 Announce Type: replace-cross 
Abstract: In this study, we enhance the Unified Theory of Acceptance and Use of Technology (UTAUT2) by incorporating the warm-glow phenomenon to clarify its impact on user decisions regarding the adoption of technology. We introduce two additional constructs aimed at capturing both the external and internal aspects of warm-glow, thus creating what we refer to as the UTAUT2 + WG model. To evaluate the effectiveness of our model, we conducted an experimental study in which participants were presented with a scenario describing a hypothetical technology designed to evoke warm-glow sensations. Using the partial least squares method, we analyzed the collected data to assess our expanded model. Our findings indicate that warm-glow significantly influences user behavior, with the internal aspect having the strongest influence, followed by hedonic motivation, performance expectancy, and finally the external aspect of warm-glow. We conclude by discussing the implications of our research, acknowledging its limitations, and suggesting directions for future exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.01242v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonios Saravanos (New York University), Neil Stott (Cambridge Judge Business School), Dongnanzi Zheng (New York University), Stavros Zervoudakis (New York University)</dc:creator>
    </item>
    <item>
      <title>Demystifying Misconceptions in Social Bots Research</title>
      <link>https://arxiv.org/abs/2303.17251</link>
      <description>arXiv:2303.17251v3 Announce Type: replace-cross 
Abstract: Research on social bots aims at advancing knowledge and providing solutions to one of the most debated forms of online manipulation. Yet, social bot research is plagued by widespread biases, hyped results, and misconceptions that set the stage for ambiguities, unrealistic expectations, and seemingly irreconcilable findings. Overcoming such issues is instrumental towards ensuring reliable solutions and reaffirming the validity of the scientific method. In this contribution, we review some recent results in social bots research, highlighting and revising factual errors as well as methodological and conceptual biases. More importantly, we demystify common misconceptions, addressing fundamental points on how social bots research is discussed. Our analysis surfaces the need to discuss research about online disinformation and manipulation in a rigorous, unbiased, and responsible way. This article bolsters such effort by identifying and refuting common fallacious arguments used by both proponents and opponents of social bots research, as well as providing directions toward sound methodologies for future research in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.17251v3</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefano Cresci, Kai-Cheng Yang, Angelo Spognardi, Roberto Di Pietro, Filippo Menczer, Marinella Petrocchi</dc:creator>
    </item>
    <item>
      <title>Democratizing Signal Processing and Machine Learning: Math Learning Equity for Elementary and Middle School Students</title>
      <link>https://arxiv.org/abs/2409.17304</link>
      <description>arXiv:2409.17304v2 Announce Type: replace-cross 
Abstract: Signal Processing (SP) and Machine Learning (ML) rely on good math and coding knowledge, in particular, linear algebra, probability, trigonometry, and complex numbers. A good grasp of these relies on scalar algebra learned in middle school. The ability to understand and use scalar algebra well, in turn, relies on a good foundation in basic arithmetic. Because of various systemic barriers, many students are not able to build a strong foundation in arithmetic in elementary school. This leads them to struggle with algebra and everything after that. Since math learning is cumulative, the gap between those without a strong early foundation and everyone else keeps increasing over the school years and becomes difficult to fill in college. In this article we discuss how SP faculty, students, and professionals can play an important role in starting, and participating in, university-run, or other, out-of-school math support programs to supplement students' learning. Two example programs run by the authors, CyMath at Iowa State and Algebra by 7th Grade (Ab7G) at Purdue, and one run by the Actuarial Foundation, are described. We conclude with providing some simple zero-cost suggestions for public schools that, if adopted, could benefit a much larger number of students than what out-of-school programs can reach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17304v2</guid>
      <category>math.HO</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Namrata Vaswani, Mohamed Y. Selim, Renee Serrell Gibert</dc:creator>
    </item>
    <item>
      <title>BuildingView: Constructing Urban Building Exteriors Databases with Street View Imagery and Multimodal Large Language Mode</title>
      <link>https://arxiv.org/abs/2409.19527</link>
      <description>arXiv:2409.19527v2 Announce Type: replace-cross 
Abstract: Urban Building Exteriors are increasingly important in urban analytics, driven by advancements in Street View Imagery and its integration with urban research. Multimodal Large Language Models (LLMs) offer powerful tools for urban annotation, enabling deeper insights into urban environments. However, challenges remain in creating accurate and detailed urban building exterior databases, identifying critical indicators for energy efficiency, environmental sustainability, and human-centric design, and systematically organizing these indicators. To address these challenges, we propose BuildingView, a novel approach that integrates high-resolution visual data from Google Street View with spatial information from OpenStreetMap via the Overpass API. This research improves the accuracy of urban building exterior data, identifies key sustainability and design indicators, and develops a framework for their extraction and categorization. Our methodology includes a systematic literature review, building and Street View sampling, and annotation using the ChatGPT-4O API. The resulting database, validated with data from New York City, Amsterdam, and Singapore, provides a comprehensive tool for urban studies, supporting informed decision-making in urban planning, architectural design, and environmental policy. The code for BuildingView is available at https://github.com/Jasper0122/BuildingView.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19527v2</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zongrong Li, Yunlei Su, Hongrong Wang, Wufan Zhao</dc:creator>
    </item>
    <item>
      <title>OASIS Uncovers: High-Quality T2I Models, Same Old Stereotypes</title>
      <link>https://arxiv.org/abs/2501.00962</link>
      <description>arXiv:2501.00962v3 Announce Type: replace-cross 
Abstract: Images generated by text-to-image (T2I) models often exhibit visual biases and stereotypes of concepts such as culture and profession. Existing quantitative measures of stereotypes are based on statistical parity that does not align with the sociological definition of stereotypes and, therefore, incorrectly categorizes biases as stereotypes. Instead of oversimplifying stereotypes as biases, we propose a quantitative measure of stereotypes that aligns with its sociological definition. We then propose OASIS to measure the stereotypes in a generated dataset and understand their origins within the T2I model. OASIS includes two scores to measure stereotypes from a generated image dataset: (M1) Stereotype Score to measure the distributional violation of stereotypical attributes, and (M2) WALS to measure spectral variance in the images along a stereotypical attribute. OASIS also includes two methods to understand the origins of stereotypes in T2I models: (U1) StOP to discover attributes that the T2I model internally associates with a given concept, and (U2) SPI to quantify the emergence of stereotypical attributes in the latent space of the T2I model during image generation. Despite the considerable progress in image fidelity, using OASIS, we conclude that newer T2I models such as FLUX.1 and SDv3 contain strong stereotypical predispositions about concepts and still generate images with widespread stereotypical attributes. Additionally, the quantity of stereotypes worsens for nationalities with lower Internet footprints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00962v3</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sepehr Dehdashtian, Gautam Sreekumar, Vishnu Naresh Boddeti</dc:creator>
    </item>
  </channel>
</rss>
