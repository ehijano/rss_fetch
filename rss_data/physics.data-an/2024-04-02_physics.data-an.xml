<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 02 Apr 2024 19:06:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 02 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Characterization of uncertainties in electron-argon collision cross sections under statistical principles</title>
      <link>https://arxiv.org/abs/2404.00467</link>
      <description>arXiv:2404.00467v1 Announce Type: cross 
Abstract: The predictive capability of a plasma discharge model depends on accurate representations of electron-impact collision cross sections, which determine the key reaction rates and transport properties of the plasma. Although many cross sections have been identified through experiments and quantum mechanical simulations, their uncertainties are not well-investigated. We characterize the uncertainties in electron-argon collision cross sections using a Bayesian framework. Six collision processes -- elastic momentum transfer, ionization, and four excitations -- are characterized with semi-empirical models, whose parametric uncertainties effectively capture the features important to the macroscopic properties of the plasma, namely transport properties and chemical reaction rates. The method is designed to capture the effects of systematic errors that lead to large discrepancies between some data sets. Specifically, for the purposes of Bayesian inference, each of the parametric cross section models is augmented with a Gaussian process representing systematic measurement errors as well as model inadequacies in the parametric form. The results show that the method is able to capture scatter in the data between the electron-beam experiments and ab-initio quantum simulations. The calibrated cross section models are further validated against measurements from swarm-parameter experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00467v1</guid>
      <category>physics.plasm-ph</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seung Whan Chung, Todd A. Oliver, Laxminarayan L. Raja, Robert D. Moser</dc:creator>
    </item>
    <item>
      <title>Machine Learning in High Energy Physics: A review of heavy-flavor jet tagging at the LHC</title>
      <link>https://arxiv.org/abs/2404.01071</link>
      <description>arXiv:2404.01071v1 Announce Type: cross 
Abstract: The application of machine learning (ML) in high energy physics (HEP), specifically in heavy-flavor jet tagging at Large Hadron Collider (LHC) experiments, has experienced remarkable growth and innovation in the past decade. This review provides a detailed examination of current and past ML techniques in this domain. It starts by exploring various data representation methods and ML architectures, encompassing traditional ML algorithms and advanced deep learning techniques. Subsequent sections discuss specific instances of successful ML applications in jet flavor tagging in the ATLAS and CMS experiments at the LHC, ranging from basic fully-connected layers to graph neural networks employing attention mechanisms. To systematically categorize the advancements over the LHC's three runs, the paper classifies jet tagging algorithms into three generations, each characterized by specific data representation techniques and ML architectures. This classification aims to provide an overview of the chronological evolution in this field. Finally, a brief discussion about anticipated future developments and potential research directions in the field is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01071v1</guid>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spandan Mondal, Luca Mastrolorenzo</dc:creator>
    </item>
    <item>
      <title>Inferring parameters and reconstruction of two-dimensional turbulent flows with physics-informed neural networks</title>
      <link>https://arxiv.org/abs/2404.01193</link>
      <description>arXiv:2404.01193v1 Announce Type: cross 
Abstract: Solving inverse problems, which means obtaining model parameters from observed data, using conventional computational fluid dynamics solvers is prohibitively expensive. Here we employ machine learning algorithms to overcome the challenge. As an example, we consider a moderately turbulent fluid flow, excited by a stationary force and described by a two-dimensional Navier-Stokes equation with linear bottom friction. Given sparse and probably noisy data for the velocity and the general form of the model, we reconstruct the dense velocity and pressure fields in the observation domain, infer the driving force, and determine the unknown fluid viscosity and friction coefficient. Our approach involves training a physics-informed neural network by minimizing the loss function, which penalizes deviations from the provided data and violations of the Navier-Stokes equation. The suggested technique extracts additional information from experimental and numerical observations, potentially enhancing the capabilities of particle image/tracking velocimetry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.01193v1</guid>
      <category>physics.flu-dyn</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vladimir Parfenyev, Mark Blumenau, Ilya Nikitin</dc:creator>
    </item>
    <item>
      <title>Self-Organization Towards $1/f$ Noise in Deep Neural Networks</title>
      <link>https://arxiv.org/abs/2301.08530</link>
      <description>arXiv:2301.08530v2 Announce Type: replace 
Abstract: The presence of $1/f$ noise, also known as pink noise, is a well-established phenomenon in biological neural networks, and is thought to play an important role in information processing in the brain. In this study, we find that such $1/f$ noise is also found in deep neural networks trained on natural language, resembling that of their biological counterparts. Specifically, we trained Long Short-Term Memory (LSTM) networks on the `IMDb' AI benchmark dataset, then measured the neuron activations. The detrended fluctuation analysis (DFA) on the time series of the different neurons demonstrate clear $1/f$ patterns, which is absent in the time series of the inputs to the LSTM. Interestingly, when the neural network is at overcapacity, having more than enough neurons to achieve the learning task, the activation patterns deviate from $1/f$ noise and shifts towards white noise. This is because many of the neurons are not effectively used, showing little fluctuations when fed with input data. We further examine the exponent values in the $1/f$ noise in ``internal" and ``external" activations in the LSTM cell, finding some resemblance in the variations of the exponents in fMRI signals of the human brain. Our findings further supports the hypothesis that $1/f$ noise is a signature of optimal learning. With deep learning models approaching or surpassing humans in certain tasks, and being more ``experimentable'' than their biological counterparts, our study suggests that they are good candidates to understand the fundamental origins of $1/f$ noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.08530v2</guid>
      <category>physics.data-an</category>
      <category>cs.AI</category>
      <category>nlin.AO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Chong Jia Le, Ling Feng</dc:creator>
    </item>
  </channel>
</rss>
