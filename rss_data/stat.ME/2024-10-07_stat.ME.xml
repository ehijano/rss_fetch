<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 08 Oct 2024 03:20:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Multiscale Multi-Type Spatial Bayesian Analysis of Wildfires and Population Change That Avoids MCMC and Approximating the Posterior Distribution</title>
      <link>https://arxiv.org/abs/2410.02905</link>
      <description>arXiv:2410.02905v1 Announce Type: new 
Abstract: In recent years, wildfires have significantly increased in the United States (U.S.), making certain areas harder to live in. This motivates us to jointly analyze active fires and population changes in the U.S. from July 2020 to June 2021. The available data are recorded on different scales (or spatial resolutions) and by different types of distributions (referred to as multi-type data). Moreover, wildfires are known to have feedback mechanism that creates signal-to-noise dependence. We analyze point-referenced remote sensing fire data from National Aeronautics and Space Administration (NASA) and county-level population change data provided by U.S. Census Bureau's Population Estimates Program (PEP). To do this, we develop a multiscale multi-type spatial Bayesian hierarchical model that assumes the average number of fires is zero-inflated normal, the incidence of fire as Bernoulli, and the percentage population change as normally distributed. This high-dimensional dataset makes Markov chain Monte Carlo (MCMC) implementation infeasible. We bypass MCMC by extending a computationally efficient Bayesian framework to directly sample from the exact posterior distribution, referred to as Exact Posterior Regression (EPR), which includes a term to model feedback. A simulation study is included to compare our new EPR method to the traditional Bayesian model fitted via MCMC. In our analysis, we obtained predictions of wildfire probabilities, identified several useful covariates, and found that regions with many fires were directly related to population change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02905v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Zhou, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Moving sum procedure for multiple change point detection in large factor models</title>
      <link>https://arxiv.org/abs/2410.02918</link>
      <description>arXiv:2410.02918v1 Announce Type: new 
Abstract: The paper proposes a moving sum methodology for detecting multiple change points in high-dimensional time series under a factor model, where changes are attributed to those in loadings as well as emergence or disappearance of factors. We establish the asymptotic null distribution of the proposed test for family-wise error control, and show the consistency of the procedure for multiple change point estimation. Simulation studies and an application to a large dataset of volatilities demonstrate the competitive performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02918v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Barigozzi, Haeran Cho, Lorenzo Trapani</dc:creator>
    </item>
    <item>
      <title>Statistical Inference with Nonignorable Non-Probability Survey Samples</title>
      <link>https://arxiv.org/abs/2410.02920</link>
      <description>arXiv:2410.02920v1 Announce Type: new 
Abstract: Statistical inference with non-probability survey samples is an emerging topic in survey sampling and official statistics and has gained increased attention from researchers and practitioners in the field. Much of the existing literature, however, assumes that the participation mechanism for non-probability samples is ignorable. In this paper, we develop a pseudo-likelihood approach to estimate participation probabilities for nonignorable non-probability samples when auxiliary information is available from an existing reference probability sample. We further construct three estimators for the finite population mean using regression-based prediction, inverse probability weighting (IPW), and augmented IPW estimators, and study their asymptotic properties. Variance estimation for the proposed methods is considered within the same framework. The efficiency of our proposed methods is demonstrated through simulation studies and a real data analysis using the ESPACOV survey on the effects of the COVID-19 pandemic in Spain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02920v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Liu, Meng Yuan, Pengfei Li, Changbao Wu</dc:creator>
    </item>
    <item>
      <title>Identifying Hierarchical Structures in Network Data</title>
      <link>https://arxiv.org/abs/2410.02929</link>
      <description>arXiv:2410.02929v1 Announce Type: new 
Abstract: In this paper, we introduce a hierarchical extension of the stochastic blockmodel to identify multilevel community structures in networks. We also present a Markov chain Monte Carlo (MCMC) and a variational Bayes algorithm to fit the model and obtain approximate posterior inference. Through simulated and real datasets, we demonstrate that the model successfully identifies communities and supercommunities when they exist in the data. Additionally, we observe that the model returns a single supercommunity when there is no evidence of multilevel community structure. As expected in the case of the single-level stochastic blockmodel, we observe that the MCMC algorithm consistently outperforms its variational Bayes counterpart. Therefore, we recommend using MCMC whenever the network size allows for computational feasibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02929v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Regueiro, Abel Rodr\'iguez, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>Efficient collaborative learning of the average treatment effect under data sharing constraints</title>
      <link>https://arxiv.org/abs/2410.02941</link>
      <description>arXiv:2410.02941v1 Announce Type: new 
Abstract: Driven by the need to generate real-world evidence from multi-site collaborative studies, we introduce an efficient collaborative learning approach to evaluate average treatment effect in a multi-site setting under data sharing constraints. Specifically, the proposed method operates in a federated manner, using individual-level data from a user-defined target population and summary statistics from other source populations, to construct efficient estimator for the average treatment effect on the target population of interest. Our federated approach does not require iterative communications between sites, making it particularly suitable for research consortia with limited resources for developing automated data-sharing infrastructures. Compared to existing work data integration methods in causal inference, it allows distributional shifts in outcomes, treatments and baseline covariates distributions, and achieves semiparametric efficiency bound under appropriate conditions. We illustrate the magnitude of efficiency gains from incorporating extra data sources by examining the effect of insulin vs. non-insulin treatments on heart failure for patients with type II diabetes using electronic health record data collected from the All of Us program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02941v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sijia Li, Rui Duan</dc:creator>
    </item>
    <item>
      <title>BSNMani: Bayesian Scalar-on-network Regression with Manifold Learning</title>
      <link>https://arxiv.org/abs/2410.02965</link>
      <description>arXiv:2410.02965v1 Announce Type: new 
Abstract: Brain connectivity analysis is crucial for understanding brain structure and neurological function, shedding light on the mechanisms of mental illness. To study the association between individual brain connectivity networks and the clinical characteristics, we develop BSNMani: a Bayesian scalar-on-network regression model with manifold learning. BSNMani comprises two components: the network manifold learning model for brain connectivity networks, which extracts shared connectivity structures and subject-specific network features, and the joint predictive model for clinical outcomes, which studies the association between clinical phenotypes and subject-specific network features while adjusting for potential confounding covariates. For posterior computation, we develop a novel two-stage hybrid algorithm combining Metropolis-Adjusted Langevin Algorithm (MALA) and Gibbs sampling. Our method is not only able to extract meaningful subnetwork features that reveal shared connectivity patterns, but can also reveal their association with clinical phenotypes, further enabling clinical outcome prediction. We demonstrate our method through simulations and through its application to real resting-state fMRI data from a study focusing on Major Depressive Disorder (MDD). Our approach sheds light on the intricate interplay between brain connectivity and clinical features, offering insights that can contribute to our understanding of psychiatric and neurological disorders, as well as mental health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02965v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijun Li, Ki Sueng Choi, Boadie W. Dunlop, Wade Edward Craighead, Helen S. Mayberg, Lana Garmire, Ying Guo, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Imputing Missing Values with External Data</title>
      <link>https://arxiv.org/abs/2410.02982</link>
      <description>arXiv:2410.02982v1 Announce Type: new 
Abstract: Missing data is a common challenge across scientific disciplines. Current imputation methods require the availability of individual data to impute missing values. Often, however, missingness requires using external data for the imputation. In this paper, we introduce a new Stata command, mi impute from, designed to impute missing values using linear predictors and their related covariance matrix from imputation models estimated in one or multiple external studies. This allows for the imputation of any missing values without sharing individual data between studies. We describe the underlying method and present the syntax of mi impute from alongside practical examples of missing data in collaborative research projects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02982v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Thiesmeier, Matteo Bottai, Nicola Orsini</dc:creator>
    </item>
    <item>
      <title>Determining adequate consistency levels for aggregation of expert estimates</title>
      <link>https://arxiv.org/abs/2410.03012</link>
      <description>arXiv:2410.03012v1 Announce Type: new 
Abstract: To obtain reliable results of expertise, which usually use individual and group expert pairwise comparisons, it is important to summarize (aggregate) expert estimates provided that they are sufficiently consistent. There are several ways to determine the threshold level of consistency sufficient for aggregation of estimates. They can be used for different consistency indices, but none of them relates the threshold value to the requirements for the reliability of the expertise's results. Therefore, a new approach to determining this consistency threshold is required. The proposed approach is based on simulation modeling of expert pairwise comparisons and a targeted search for the most inconsistent among the modeled pairwise comparison matrices. Thus, the search for the least consistent matrix is carried out for a given perturbation of the perfectly consistent matrix. This allows for determining the consistency threshold corresponding to a given permissible relative deviation of the resulting weight of an alternative from its hypothetical reference value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03012v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vitaliy Tsyganok, Andriy Olenko, Pavlo Roik, Oksana Vlasenko</dc:creator>
    </item>
    <item>
      <title>Time-In-Range Analyses of Functional Data Subject to Missing with Applications to Inpatient Continuous Glucose Monitoring</title>
      <link>https://arxiv.org/abs/2410.03091</link>
      <description>arXiv:2410.03091v1 Announce Type: new 
Abstract: Continuous glucose monitoring (CGM) has been increasingly used in US hospitals for the care of patients with diabetes. Time in range (TIR), which measures the percent of time over a specified time window with glucose values within a target range, has served as a pivotal CGM-metric for assessing glycemic control. However, inpatient CGM is prone to a prevailing issue that a limited length of hospital stay can cause insufficient CGM sampling, leading to a scenario with functional data plagued by complex missingness. Current analyses of inpatient CGM studies, however, ignore this issue and typically compute the TIR as the proportion of available CGM glucose values in range. As shown by simulation studies, this can result in considerably biased estimation and inference, largely owing to the nonstationary nature of inpatient CGM trajectories. In this work, we develop a rigorous statistical framework that confers valid inference on TIR in realistic inpatient CGM settings. Our proposals utilize a novel probabilistic representation of TIR, which enables leveraging the technique of inverse probability weighting and semiparametric survival modeling to obtain unbiased estimators of mean TIR that properly account for incompletely observed CGM trajectories. We establish desirable asymptotic properties of the proposed estimators. Results from our numerical studies demonstrate good finite-sample performance of the proposed method as well as its advantages over existing approaches. The proposed method is generally applicable to other functional data settings similar to CGM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03091v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Yu, Guillermo E. Umpierrez, Limin Peng</dc:creator>
    </item>
    <item>
      <title>Expected value of sample information calculations for risk prediction model development</title>
      <link>https://arxiv.org/abs/2410.03096</link>
      <description>arXiv:2410.03096v1 Announce Type: new 
Abstract: Uncertainty around predictions from a model due to the finite size of the development sample has traditionally been approached using classical inferential techniques. The finite size of the sample will result in the discrepancy between the final model and the correct model that maps covariates to predicted risks. From a decision-theoretic perspective, this discrepancy might affect the subsequent treatment decisions, and thus is associated with utility loss. From this perspective, procuring more development data is associated in expected gain in the utility of using the model. In this work, we define the Expected Value of Sample Information (EVSI) as the expected gain in clinical utility, defined in net benefit (NB) terms in net true positive units, by procuring a further development sample of a given size. We propose a bootstrap-based algorithm for EVSI computations, and show its feasibility and face validity in a case study. Decision-theoretic metrics can complement classical inferential methods when designing studies that are aimed at developing risk prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03096v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abdollah Safari, Paul Gustafson, Mohsen Sadatsafavi</dc:creator>
    </item>
    <item>
      <title>General linear hypothesis testing in ill-conditioned functional response model</title>
      <link>https://arxiv.org/abs/2410.03393</link>
      <description>arXiv:2410.03393v1 Announce Type: new 
Abstract: The paper concerns inference in the ill-conditioned functional response model, which is a part of functional data analysis. In this regression model, the functional response is modeled using several independent scalar variables. To verify linear hypotheses, we develop new test statistics by aggregating pointwise statistics using either integral or supremum. The new tests are scale-invariant, in contrast to the existing ones. To construct tests, we use different bootstrap methods. The performance of the new tests is compared with the performance of known tests through a simulation study and an application to a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03393v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>{\L}ukasz Smaga, Natalia Stefa\'nska</dc:creator>
    </item>
    <item>
      <title>Robust Quickest Correlation Change Detection in High-Dimensional Random Vectors</title>
      <link>https://arxiv.org/abs/2410.03593</link>
      <description>arXiv:2410.03593v1 Announce Type: new 
Abstract: Detecting changes in high-dimensional vectors presents significant challenges, especially when the post-change distribution is unknown and time-varying. This paper introduces a novel robust algorithm for correlation change detection in high-dimensional data. The approach utilizes the summary statistic of the maximum magnitude correlation coefficient to detect the change. This summary statistic captures the level of correlation present in the data but also has an asymptotic density. The robust test is designed using the asymptotic density. The proposed approach is robust because it can help detect a change in correlation level from some known level to unknown, time-varying levels. The proposed test is also computationally efficient and valid for a broad class of data distributions. The effectiveness of the proposed algorithm is demonstrated on simulated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03593v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Assma Alghamdi, Taposh Banerjee, Jayant Rajgopal</dc:creator>
    </item>
    <item>
      <title>Functional Singular Value Decomposition</title>
      <link>https://arxiv.org/abs/2410.03619</link>
      <description>arXiv:2410.03619v1 Announce Type: new 
Abstract: Heterogeneous functional data are commonly seen in time series and longitudinal data analysis. To capture the statistical structures of such data, we propose the framework of Functional Singular Value Decomposition (FSVD), a unified framework with structure-adaptive interpretability for the analysis of heterogeneous functional data. We establish the mathematical foundation of FSVD by proving its existence and providing its fundamental properties using operator theory. We then develop an implementation approach for noisy and irregularly observed functional data based on a novel joint kernel ridge regression scheme and provide theoretical guarantees for its convergence and estimation accuracy. The framework of FSVD also introduces the concepts of intrinsic basis functions and intrinsic basis vectors, which represent two fundamental statistical structures for random functions and connect FSVD to various tasks including functional principal component analysis, factor models, functional clustering, and functional completion. We compare the performance of FSVD with existing methods in several tasks through extensive simulation studies. To demonstrate the value of FSVD in real-world datasets, we apply it to extract temporal patterns from a COVID-19 case count dataset and perform data completion on an electronic health record dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03619v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jianbin Tan, Pixu Shi, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Spatial Hyperspheric Models for Compositional Data</title>
      <link>https://arxiv.org/abs/2410.03648</link>
      <description>arXiv:2410.03648v1 Announce Type: new 
Abstract: Compositional data are an increasingly prevalent data source in spatial statistics. Analysis of such data is typically done on log-ratio transformations or via Dirichlet regression. However, these approaches often make unnecessarily strong assumptions (e.g., strictly positive components, exclusively negative correlations). An alternative approach uses square-root transformed compositions and directional distributions. Such distributions naturally allow for zero-valued components and positive correlations, yet they may include support outside the non-negative orthant and are not generative for compositional data. To overcome this challenge, we truncate the elliptically symmetric angular Gaussian (ESAG) distribution to the non-negative orthant. Additionally, we propose a spatial hyperspheric regression that contains fixed and random multivariate spatial effects. The proposed method also contains a term that can be used to propagate uncertainty that may arise from precursory stochastic models (i.e., machine learning classification). We demonstrate our method on a simulation study and on classified bioacoustic signals of the Dryobates pubescens (downy woodpecker).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03648v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Schwob, Mevin B. Hooten, Nicholas M. Calzada</dc:creator>
    </item>
    <item>
      <title>Estimating the Unobservable Components of Electricity Demand Response with Inverse Optimization</title>
      <link>https://arxiv.org/abs/2410.02774</link>
      <description>arXiv:2410.02774v1 Announce Type: cross 
Abstract: Understanding and predicting the electricity demand responses to prices are critical activities for system operators, retailers, and regulators. While conventional machine learning and time series analyses have been adequate for the routine demand patterns that have adapted only slowly over many years, the emergence of active consumers with flexible assets such as solar-plus-storage systems, and electric vehicles, introduces new challenges. These active consumers exhibit more complex consumption patterns, the drivers of which are often unobservable to the retailers and system operators. In practice, system operators and retailers can only monitor the net demand (metered at grid connection points), which reflects the overall energy consumption or production exchanged with the grid. As a result, all "behind-the-meter" activities-such as the use of flexibility-remain hidden from these entities. Such behind-the-meter behavior may be controlled by third party agents or incentivized by tariffs; in either case, the retailer's revenue and the system loads would be impacted by these activities behind the meter, but their details can only be inferred. We define the main components of net demand, as baseload, flexible, and self-generation, each having nonlinear responses to market price signals. As flexible demand response and self generation are increasing, this raises a pressing question of whether existing methods still perform well and, if not, whether there is an alternative way to understand and project the unobserved components of behavior. In response to this practical challenge, we evaluate the potential of a data-driven inverse optimization (IO) methodology. This approach characterizes decomposed consumption patterns without requiring direct observation of behind-the-meter behavior or device-level metering [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02774v1</guid>
      <category>eess.SP</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrian Esteban-Perez, Derek Bunn, Yashar Ghiassi-Farrokhfal</dc:creator>
    </item>
    <item>
      <title>A Data Envelopment Analysis Approach for Assessing Fairness in Resource Allocation: Application to Kidney Exchange Programs</title>
      <link>https://arxiv.org/abs/2410.02799</link>
      <description>arXiv:2410.02799v1 Announce Type: cross 
Abstract: Kidney exchange programs have significantly increased transplantation rates but raise pressing questions about fairness in organ allocation. We present a novel framework leveraging Data Envelopment Analysis (DEA) to evaluate multiple fairness criteria--Priority, Access, and Outcome--within a single model, capturing complexities that may be overlooked in single-metric analyses. Using data from the United Network for Organ Sharing, we analyze these criteria individually, measuring Priority fairness through waitlist durations, Access fairness through Kidney Donor Profile Index scores, and Outcome fairness through graft lifespan. We then apply our DEA model to demonstrate significant disparities in kidney allocation efficiency across ethnic groups. To quantify uncertainty, we employ conformal prediction within the DEA framework, yielding group-conditional prediction intervals with finite sample coverage guarantees. Our findings show notable differences in efficiency distributions between ethnic groups. Our study provides a rigorous framework for evaluating fairness in complex resource allocation systems, where resource scarcity and mutual compatibility constraints exist. All code for using the proposed method and reproducing results is available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02799v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Kaazempur-Mofrad, Xiaowu Dai</dc:creator>
    </item>
    <item>
      <title>The MLE is minimax optimal for LGC</title>
      <link>https://arxiv.org/abs/2410.02835</link>
      <description>arXiv:2410.02835v1 Announce Type: cross 
Abstract: We revisit the recently introduced Local Glivenko-Cantelli setting, which studies distribution-dependent uniform convegence rates of the Maximum Likelihood Estimator (MLE). In this work, we investigate generalizations of this setting where arbitrary estimators are allowed rather than just the MLE. Can a strictly larger class of measures be learned? Can better risk decay rates be obtained? We provide exhaustive answers to these questions -- which are both negative, provided the learner is barred from exploiting some infinite-dimensional pathologies. On the other hand, allowing such exploits does lead to a strictly larger class of learnable measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02835v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Doron Cohen, Aryeh Kontorovich, Roi Weiss</dc:creator>
    </item>
    <item>
      <title>Implementing Response-Adaptive Randomisation in Stratified Rare-disease Trials: Design Challenges and Practical Solutions</title>
      <link>https://arxiv.org/abs/2410.03346</link>
      <description>arXiv:2410.03346v1 Announce Type: cross 
Abstract: Although response-adaptive randomisation (RAR) has gained substantial attention in the literature, it still has limited use in clinical trials. Amongst other reasons, the implementation of RAR in the real world raises important practical questions, often neglected. Motivated by an innovative phase-II stratified RAR trial, this paper addresses two challenges: (1) How to ensure that RAR allocations are both desirable and faithful to target probabilities, even in small samples? and (2) What adaptations to trigger after interim analyses in the presence of missing data? We propose a Mapping strategy that discretises the randomisation probabilities into a vector of allocation ratios, resulting in improved frequentist errors. Under the implementation of Mapping, we analyse the impact of missing data on operating characteristics by examining selected scenarios. Finally, we discuss additional concerns including: pooling data across trial strata, analysing the level of blinding in the trial, and reporting safety results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03346v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajenki Das, Nina Deliu, Mark Toshner, Sof\'ia S Villar</dc:creator>
    </item>
    <item>
      <title>Is Gibbs sampling faster than Hamiltonian Monte Carlo on GLMs?</title>
      <link>https://arxiv.org/abs/2410.03630</link>
      <description>arXiv:2410.03630v1 Announce Type: cross 
Abstract: The Hamiltonian Monte Carlo (HMC) algorithm is often lauded for its ability to effectively sample from high-dimensional distributions. In this paper we challenge the presumed domination of HMC for the Bayesian analysis of GLMs. By utilizing the structure of the compute graph rather than the graphical model, we reduce the time per sweep of a full-scan Gibbs sampler from $O(d^2)$ to $O(d)$, where $d$ is the number of GLM parameters. Our simple changes to the implementation of the Gibbs sampler allow us to perform Bayesian inference on high-dimensional GLMs that are practically infeasible with traditional Gibbs sampler implementations. We empirically demonstrate a substantial increase in effective sample size per time when comparing our Gibbs algorithms to state-of-the-art HMC algorithms. While Gibbs is superior in terms of dimension scaling, neither Gibbs nor HMC dominate the other: we provide numerical and theoretical evidence that HMC retains an edge in certain circumstances thanks to its advantageous condition number scaling. Interestingly, for GLMs of fixed data size, we observe that increasing dimensionality can stabilize or even decrease condition number, shedding light on the empirical advantage of our efficient Gibbs sampler.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03630v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Son Luu, Zuheng Xu, Nikola Surjanovic, Miguel Biron-Lattes, Trevor Campbell, Alexandre Bouchard-C\^ot\'e</dc:creator>
    </item>
    <item>
      <title>Minimax-optimal trust-aware multi-armed bandits</title>
      <link>https://arxiv.org/abs/2410.03651</link>
      <description>arXiv:2410.03651v1 Announce Type: cross 
Abstract: Multi-armed bandit (MAB) algorithms have achieved significant success in sequential decision-making applications, under the premise that humans perfectly implement the recommended policy. However, existing methods often overlook the crucial factor of human trust in learning algorithms. When trust is lacking, humans may deviate from the recommended policy, leading to undesired learning performance. Motivated by this gap, we study the trust-aware MAB problem by integrating a dynamic trust model into the standard MAB framework. Specifically, it assumes that the recommended and actually implemented policy differs depending on human trust, which in turn evolves with the quality of the recommended policy. We establish the minimax regret in the presence of the trust issue and demonstrate the suboptimality of vanilla MAB algorithms such as the upper confidence bound (UCB) algorithm. To overcome this limitation, we introduce a novel two-stage trust-aware procedure that provably attains near-optimal statistical guarantees. A simulation study is conducted to illustrate the benefits of our proposed algorithm when dealing with the trust issue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03651v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Changxiao Cai, Jiacheng Zhang</dc:creator>
    </item>
    <item>
      <title>Functional principal component analysis with informative observation times</title>
      <link>https://arxiv.org/abs/2203.14760</link>
      <description>arXiv:2203.14760v3 Announce Type: replace 
Abstract: Functional principal component analysis has been shown to be invaluable for revealing variation modes of longitudinal outcomes, which serves as important building blocks for forecasting and model building. Decades of research have advanced methods for functional principal component analysis often assuming independence between the observation times and longitudinal outcomes. Yet such assumptions are fragile in real-world settings where observation times may be driven by outcome-related reasons. Rather than ignoring the informative observation time process, we explicitly model the observational times by a counting process dependent on time-varying prognostic factors. Identification of the mean, covariance function, and functional principal components ensues via inverse intensity weighting. We propose using weighted penalized splines for estimation and establish consistency and convergence rates for the weighted estimators. Simulation studies demonstrate that the proposed estimators are substantially more accurate than the existing ones in the presence of a correlation between the observation time process and the longitudinal outcome process. We further examine the finite-sample performance of the proposed method using the Acute Infection and Early Disease Research Program study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14760v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peijun Sang, Dehan Kong, Shu Yang</dc:creator>
    </item>
    <item>
      <title>Functional varying-coefficient model under heteroskedasticity with application to DTI data</title>
      <link>https://arxiv.org/abs/2207.08373</link>
      <description>arXiv:2207.08373v2 Announce Type: replace 
Abstract: In this paper, we develop a multi-step estimation procedure to simultaneously estimate the varying-coefficient functions using a local-linear generalized method of moments (GMM) based on continuous moment conditions. To incorporate spatial dependence, the continuous moment conditions are first projected onto eigen-functions and then combined by weighted eigen-values, thereby, solving the challenges of using an inverse covariance operator directly. We propose an optimal instrument variable that minimizes the asymptotic variance function among the class of all local-linear GMM estimators, and it outperforms the initial estimates which do not incorporate the spatial dependence. Our proposed method significantly improves the accuracy of the estimation under heteroskedasticity and its asymptotic properties have been investigated. Extensive simulation studies illustrate the finite sample performance, and the efficacy of the proposed method is confirmed by real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.08373v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Ping-Shou Zhong, Xiaohong Joe Zhou</dc:creator>
    </item>
    <item>
      <title>Product Centered Dirichlet Processes for Bayesian Multiview Clustering</title>
      <link>https://arxiv.org/abs/2312.05365</link>
      <description>arXiv:2312.05365v3 Announce Type: replace 
Abstract: While there is an immense literature on Bayesian methods for clustering, the multiview case has received little attention. This problem focuses on obtaining distinct but statistically dependent clusterings in a common set of entities for different data types. For example, clustering patients into subgroups with subgroup membership varying according to the domain of the patient variables. A challenge is how to model the across-view dependence between the partitions of patients into subgroups. The complexities of the partition space make standard methods to model dependence, such as correlation, infeasible. In this article, we propose CLustering with Independence Centering (CLIC), a clustering prior that uses a single parameter to explicitly model dependence between clusterings across views. CLIC is induced by the product centered Dirichlet process (PCDP), a novel hierarchical prior that bridges between independent and equivalent partitions. We show appealing theoretic properties, provide a finite approximation and prove its accuracy, present a marginal Gibbs sampler for posterior computation, and derive closed form expressions for the marginal and joint partition distributions for the CLIC model. On synthetic data and in an application to epidemiology, CLIC accurately characterizes view-specific partitions while providing inference on the dependence level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05365v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Dombowsky, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Quadratic inference with dense functional responses</title>
      <link>https://arxiv.org/abs/2402.13907</link>
      <description>arXiv:2402.13907v2 Announce Type: replace 
Abstract: We address the challenge of estimation in the context of constant linear effect models with dense functional responses. In this framework, the conditional expectation of the response curve is represented by a linear combination of functional covariates with constant regression parameters. In this paper, we present an alternative solution by employing the quadratic inference approach, a well-established method for analyzing correlated data, to estimate the regression coefficients. Our approach leverages non-parametrically estimated basis functions, eliminating the need for choosing working correlation structures. Furthermore, we demonstrate that our method achieves a parametric $\sqrt{n}$-convergence rate, contingent on an appropriate choice of bandwidth. This convergence is observed when the number of repeated measurements per trajectory exceeds a certain threshold, specifically, when it surpasses $n^{a_{0}}$, with $n$ representing the number of trajectories. Additionally, we establish the asymptotic normality of the resulting estimator. The performance of the proposed method is compared with that of existing methods through extensive simulation studies, where our proposed method outperforms. Real data analysis is also conducted to demonstrate the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13907v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pratim Guha Niyogi, Ping-Shou Zhong</dc:creator>
    </item>
    <item>
      <title>Demystifying and avoiding the OLS "weighting problem": Unmodeled heterogeneity and straightforward solutions</title>
      <link>https://arxiv.org/abs/2403.03299</link>
      <description>arXiv:2403.03299v2 Announce Type: replace 
Abstract: Researchers have long run regressions of an outcome variable (Y) on a treatment (D) and covariates (X) to estimate treatment effects. Even absent unobserved confounding, the regression coefficient on D in this setup reports a conditional variance weighted average of strata-wise average effects, not generally equal to the average treatment effect (ATE). Numerous proposals have been offered to cope with this "weighting problem", including interpretational tools to help characterize the weights and diagnostic aids to help researchers assess the potential severity of this problem. We make two contributions that together suggest an alternative direction for researchers and this literature. Our first contribution is conceptual, demystifying these weights. Simply put, under heterogeneous treatment effects (and varying probability of treatment), the linear regression of Y on D and X will be misspecified. The "weights" of regression offer one characterization for the coefficient from regression that helps to clarify how it will depart from the ATE. We also derive a more general expression for the weights than what is usually referenced. Our second contribution is practical: as these weights simply characterize misspecification bias, we suggest simply avoiding them through an approach that tolerate heterogeneous effects. A wide range of longstanding alternatives (regression-imputation/g-computation, interacted regression, and balancing weights) relax specification assumptions to allow heterogeneous effects. We make explicit the assumption of "separate linearity", under which each potential outcome is separately linear in X. This relaxation of conventional linearity offers a common justification for all of these methods and avoids the weighting problem, at an efficiency cost that will be small when there are few covariates relative to sample size.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03299v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chad Hazlett, Tanvi Shinkre</dc:creator>
    </item>
    <item>
      <title>Comparing statistical likelihoods with diagnostic probabilities based on directly observed proportions to help understand and perhaps overcome the replication crisis</title>
      <link>https://arxiv.org/abs/2403.16906</link>
      <description>arXiv:2403.16906v4 Announce Type: replace 
Abstract: Diagnosticians use an observed proportion as a direct estimate of the posterior probability of a diagnosis. Therefore, a diagnostician regards a continuous Gaussian distribution of possible numerical outcomes conditional on the information in the study methods and data as probabilities (not likelihoods). Similarly, they might regard the distribution of possible means based on a SEM as a posterior probability distribution too. If the converse likelihood distribution of the observed mean conditional on any hypothetical mean (e.g. the null hypothesis) is assumed to be the same as the above posterior distribution (as is customary) then by Bayes rule, the prior distribution of all possible hypothetical means is uniform. It follows that the probability Q of any theoretically true mean falling into a tail beyond a null hypothesis would be equal to that tails area as a proportion of the whole. It also follows that the P value (the probability of the observed mean or something more extreme conditional on the null hypothesis) is equal to Q. Replication involves doing two independent studies, thus doubling the variance for the combined posterior probability distribution. So, if the original effect size was 1.96, the number of observations was 100, the SEM was 1 and the original P value was 0.025, the theoretical probability of a replicating study getting a P value of up to 0.025 again is only 0.283. By applying this double variance to achieve a power of 80%, the required number of observations is doubled compared to conventional approaches. If some replicating study is to achieve a P value of up to 0.025 yet again with a probability of 0.8, then this requires 3 times as many observations in the power calculation. This might explain the replication crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16906v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huw Llewelyn</dc:creator>
    </item>
    <item>
      <title>Running in circles: practical limitations for real-life application of data fission and data thinning in post-clustering differential analysis</title>
      <link>https://arxiv.org/abs/2405.13591</link>
      <description>arXiv:2405.13591v2 Announce Type: replace 
Abstract: Post-clustering inference in scRNA-seq analysis presents significant challenges in controlling Type I error during Differential Expression Analysis. Data fission, a promising approach, aims to split the data into two new independent parts, but relies on strong parametric assumptions of non-mixture distributions, which are violated in clustered data. We show that applying data fission to these mixtures requires knowledge of the clustering structure to accurately estimate component-specific scale parameters. These estimates are critical for ensuring decomposition and independence. We theoretically quantify the direct impact of the bias in estimating this scales parameters on the inflation of the Type I error rate, caused by a deviation from the independence. Since component structures are unknown in practice, we propose a heteroscedastic model with non-parametric estimators for individual scale parameters. This model uses proximity between observations to capture the effect of the underlying mixture on data dispersion. While this approach works well when clusters are well-separated, it introduces bias when separation is weak, highlighting the difficulty of applying data fission in real-world scenarios with unknown degrees of separation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13591v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Hivert, Denis Agniel, Rodolphe Thi\'ebaut, Boris P. Hejblum</dc:creator>
    </item>
    <item>
      <title>Robust Reproducible Network Exploration</title>
      <link>https://arxiv.org/abs/2405.17117</link>
      <description>arXiv:2405.17117v2 Announce Type: replace 
Abstract: We propose a novel methodology for discovering the presence of relationships realized as binary time series between variables in high dimension. To make it visually intuitive, we regard the existence of a relationship as an edge connection, and call a collection of such edges a network. Our objective is thus rephrased as uncovering the network by selecting relevant edges, referred to as the network exploration. Our methodology is based on multiple testing for the presence or absence of each edge, designed to ensure statistical reproducibility via controlling the false discovery rate (FDR). In particular, we carefully construct $p$-variables, and apply the Benjamini-Hochberg (BH) procedure. We show that the BH with our $p$-variables controls the FDR under arbitrary dependence structure with any sample size and dimension, and has asymptotic power one under mild conditions. The validity is also confirmed by simulations and a real data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17117v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masaki Toyoda, Yoshimasa Uematsu</dc:creator>
    </item>
    <item>
      <title>Assessing Spatial Disparities: A Bayesian Linear Regression Approach</title>
      <link>https://arxiv.org/abs/2407.19171</link>
      <description>arXiv:2407.19171v2 Announce Type: replace 
Abstract: Epidemiological investigations of regionally aggregated spatial data often involve detecting spatial health disparities between neighboring regions on a map of disease mortality or incidence rates. Analyzing such data introduces spatial dependence among the health outcomes and seeks to report statistically significant spatial disparities by delineating boundaries that separate neighboring regions with widely disparate health outcomes. However, current statistical methods are often inadequate for appropriately defining what constitutes a spatial disparity and for constructing rankings of posterior probabilities that are robust under changes to such a definition. More specifically, non-parametric Bayesian approaches endow spatial effects with discrete probability distributions using Dirichlet processes, or generalizations thereof, and rely upon computationally intensive methods for inferring on weakly identified parameters. In this manuscript, we introduce a Bayesian linear regression framework to detect spatial health disparities. This enables us to exploit Bayesian conjugate posterior distributions in a more accessible manner and accelerate computation significantly over existing Bayesian non-parametric approaches. Simulation experiments conducted over a county map of the entire United States demonstrate the effectiveness of our method and we apply our method to a data set from the Institute of Health Metrics and Evaluation (IHME) on age-standardized US county-level estimates of mortality rates across tracheal, bronchus, and lung cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19171v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Lin Wu, Sudipto Banerjee</dc:creator>
    </item>
    <item>
      <title>Priors from Envisioned Posterior Judgments: A Novel Elicitation Approach With Application to Bayesian Clinical Trials</title>
      <link>https://arxiv.org/abs/2409.05271</link>
      <description>arXiv:2409.05271v2 Announce Type: replace 
Abstract: The uptake of formalized prior elicitation from experts in Bayesian clinical trials has been limited, largely due to the challenges associated with complex statistical modeling, the lack of practical tools, and the cognitive burden on experts required to quantify their uncertainty using probabilistic language. Additionally, existing methods do not address prior-posterior coherence, i.e., does the posterior distribution, obtained mathematically from combining the estimated prior with the trial data, reflect the expert's actual posterior beliefs? We propose a new elicitation approach that seeks to ensure prior-posterior coherence and reduce the expert's cognitive burden. This is achieved by eliciting responses about the expert's envisioned posterior judgments under various potential data outcomes and inferring the prior distribution by minimizing the discrepancies between these responses and the expected responses obtained from the posterior distribution. The feasibility and potential value of the new approach are illustrated through an application to a real trial currently underway.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05271v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yongdong Ouyang, Janice J Eng, Denghuang Zhan, Hubert Wong</dc:creator>
    </item>
    <item>
      <title>Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes</title>
      <link>https://arxiv.org/abs/2409.19241</link>
      <description>arXiv:2409.19241v2 Announce Type: replace 
Abstract: Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19241v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Na Bo, Ying Ding</dc:creator>
    </item>
    <item>
      <title>Spectrum-Aware Debiasing: A Modern Inference Framework with Applications to Principal Components Regression</title>
      <link>https://arxiv.org/abs/2309.07810</link>
      <description>arXiv:2309.07810v5 Announce Type: replace-cross 
Abstract: Debiasing is a fundamental concept in high-dimensional statistics. While degrees-of-freedom adjustment is the state-of-the-art technique in high-dimensional linear regression, it is limited to i.i.d. samples and sub-Gaussian covariates. These constraints hinder its broader practical use. Here, we introduce Spectrum-Aware Debiasing--a novel method for high-dimensional regression. Our approach applies to problems with structured dependencies, heavy tails, and low-rank structures. Our method achieves debiasing through a rescaled gradient descent step, deriving the rescaling factor using spectral information of the sample covariance matrix. The spectrum-based approach enables accurate debiasing in much broader contexts. We study the common modern regime where the number of features and samples scale proportionally. We establish asymptotic normality of our proposed estimator (suitably centered and scaled) under various convergence notions when the covariates are right-rotationally invariant. Such designs have garnered recent attention due to their crucial role in compressed sensing. Furthermore, we devise a consistent estimator for its asymptotic variance.
  Our work has two notable by-products: first, we use Spectrum-Aware Debiasing to correct bias in principal components regression (PCR), providing the first debiased PCR estimator in high dimensions. Second, we introduce a principled test for checking alignment between the signal and the eigenvectors of the sample covariance matrix. This test is independently valuable for statistical methods developed using approximate message passing, leave-one-out, or convex Gaussian min-max theorems. We demonstrate our method through simulated and real data experiments. Technically, we connect approximate message passing algorithms with debiasing and provide the first proof of the Cauchy property of vector approximate message passing (V-AMP).</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07810v5</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufan Li, Pragya Sur</dc:creator>
    </item>
    <item>
      <title>G-Transformer for Conditional Average Potential Outcome Estimation over Time</title>
      <link>https://arxiv.org/abs/2405.21012</link>
      <description>arXiv:2405.21012v2 Announce Type: replace-cross 
Abstract: Estimating potential outcomes for treatments over time based on observational data is important for personalized decision-making in medicine. Yet, existing neural methods for this task either (1) do not perform proper adjustments for time-varying confounders, or (2) suffer from large estimation variance. In order to address both limitations, we introduce the G-transformer (GT). Our GT is a novel, neural end-to-end model which adjusts for time-varying confounders, and provides low-variance estimation of conditional average potential outcomes (CAPOs) over time. Specifically, our GT is the first neural model to perform regression-based iterative G-computation for CAPOs in the time-varying setting. We evaluate the effectiveness of our GT across various experiments. In sum, this work represents a significant step towards personalized decision-making from electronic health records.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.21012v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 07 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Hess, Dennis Frauen, Valentyn Melnychuk, Stefan Feuerriegel</dc:creator>
    </item>
  </channel>
</rss>
