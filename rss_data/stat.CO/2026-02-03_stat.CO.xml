<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Feb 2026 03:03:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exact Gibbs sampling for stochastic differential equations with gradient drift and constant diffusion</title>
      <link>https://arxiv.org/abs/2602.00512</link>
      <description>arXiv:2602.00512v1 Announce Type: new 
Abstract: Stochastic differential equations (SDEs) are an important class of time-series models, used to describe stochastic systems evolving in continuous time. Simulating paths from these processes, particularly after conditioning on noisy observations of the latent path, remains a challenge. Existing methods often introduce bias through time-discretization, require involved rejection sampling or debiasing schemes or are restricted to a narrow family of diffusions. In this work, we propose an exact Markov chain Monte Carlo (MCMC) sampling algorithm that is applicable to a broad subset of all SDEs with unit diffusion coefficient; after suitable transformation, this includes an even larger class of multivariate SDEs and most 1-d SDEs. We develop a Gibbs sampling framework that allows exact MCMC for such diffusions, without any discretization error. We demonstrate how our MCMC methodology requires only fairly straightforward simulation steps. Our framework can be extended to include parameter simulation, and allows tools from the Gaussian process literature to be easily applied. We evaluate our method on synthetic and real datasets, demonstrating superior performance to particle MCMC approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00512v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Pei, Minhyeok Kim, Vinayak Rao</dc:creator>
    </item>
    <item>
      <title>Complexity bounds for Dirichlet process slice samplers</title>
      <link>https://arxiv.org/abs/2602.00878</link>
      <description>arXiv:2602.00878v1 Announce Type: new 
Abstract: Slice sampling is a standard Monte Carlo technique for Dirichlet process (DP)-based models, widely used in posterior simulation. However, formal assessments of the scalability of posterior slice samplers have remained largely unexplored, primarily because the computational cost of a slice-sampling iteration is random and potentially unbounded. In this work, we obtain high-probability bounds on the computational complexity of DP slice samplers. Our main results show that, uniformly across posterior cluster-growth regimes, the overhead induced by slice variables, relatively to the number of clusters supported by the posterior, is $O_{\mathbb P}(\log n)$. As a consequence, even in worst-case configurations, superlinear blow-ups in per-iteration computational cost occur with vanishing probability. Our analysis applies broadly to DP-based models without any likelihood-specific assumptions, still providing complexity guarantees for posterior sampling on arbitrary datasets. These results establish a theoretical foundation for assessing the practical scalability of slice sampling in DP-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00878v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Francesco Gaffi</dc:creator>
    </item>
    <item>
      <title>A multifidelity approximate Bayesian computation with pre-filtering</title>
      <link>https://arxiv.org/abs/2602.01770</link>
      <description>arXiv:2602.01770v1 Announce Type: new 
Abstract: Approximate Bayesian Computation (ABC) methods often require extensive simulations, resulting in high computational costs. This paper focuses on multifidelity simulation models and proposes a pre-filtering hierarchical importance sampling algorithm. Under mild assumptions, we theoretically prove that the proposed algorithm satisfies posterior concentration properties, characterize the error upper bound and the relationship between algorithmic efficiency and pre-filtering criteria. Additionally, we provide a practical strategy to assess the suitability of multifidelity models for the proposed method. Finally, we develop a multifidelity ABC sequential Monte Carlo with adaptive pre-filtering strategy. Numerical experiments are used to demonstrate the effectiveness of the proposed approach. We develop an R package that is available at https://github.com/caofff/MAPS</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01770v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefei Cao, Shijia Wang, Yongdao Zhou</dc:creator>
    </item>
    <item>
      <title>Sampling from multi-modal distributions on Riemannian manifolds with training-free stochastic interpolants</title>
      <link>https://arxiv.org/abs/2602.00641</link>
      <description>arXiv:2602.00641v1 Announce Type: cross 
Abstract: In this paper, we propose a general methodology for sampling from un-normalized densities defined on Riemannian manifolds, with a particular focus on multi-modal targets that remain challenging for existing sampling methods. Inspired by the framework of diffusion models developed for generative modeling, we introduce a sampling algorithm based on the simulation of a non-equilibrium deterministic dynamics that transports an easy-to-sample noise distribution toward the target. At the marginal level, the induced density path follows a prescribed stochastic interpolant between the noise and target distributions, specifically constructed to respect the underlying Riemannian geometry. In contrast to related generative modeling approaches that rely on machine learning, our method is entirely training-free. It instead builds on iterative posterior sampling procedures using only standard Monte Carlo techniques, thereby extending recent diffusion-based sampling methodologies beyond the Euclidean setting. We complement our approach with a rigorous theoretical analysis and demonstrate its effectiveness on a range of multi-modal sampling problems, including high-dimensional and heavy-tailed examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00641v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alain Durmus, Maxence Noble, Thibaut Pellerin</dc:creator>
    </item>
    <item>
      <title>Multimodal Scientific Learning Beyond Diffusions and Flows</title>
      <link>https://arxiv.org/abs/2602.00960</link>
      <description>arXiv:2602.00960v1 Announce Type: cross 
Abstract: Scientific machine learning (SciML) increasingly requires models that capture multimodal conditional uncertainty arising from ill-posed inverse problems, multistability, and chaotic dynamics. While recent work has favored highly expressive implicit generative models such as diffusion and flow-based methods, these approaches are often data-hungry, computationally costly, and misaligned with the structured solution spaces frequently found in scientific problems. We demonstrate that Mixture Density Networks (MDNs) provide a principled yet largely overlooked alternative for multimodal uncertainty quantification in SciML. As explicit parametric density estimators, MDNs impose an inductive bias tailored to low-dimensional, multimodal physics, enabling direct global allocation of probability mass across distinct solution branches. This structure delivers strong data efficiency, allowing reliable recovery of separated modes in regimes where scientific data is scarce. We formalize these insights through a unified probabilistic framework contrasting explicit and implicit distribution networks, and demonstrate empirically that MDNs achieve superior generalization, interpretability, and sample efficiency across a range of inverse, multistable, and chaotic scientific regression tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.00960v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Leonardo Ferreira Guilhoto, Akshat Kaushal, Paris Perdikaris</dc:creator>
    </item>
    <item>
      <title>The Effect of Mini-Batch Noise on the Implicit Bias of Adam</title>
      <link>https://arxiv.org/abs/2602.01642</link>
      <description>arXiv:2602.01642v1 Announce Type: cross 
Abstract: With limited high-quality data and growing compute, multi-epoch training is gaining back its importance across sub-areas of deep learning. Adam(W), versions of which are go-to optimizers for many tasks such as next token prediction, has two momentum hyperparameters $(\beta_1, \beta_2)$ controlling memory and one very important hyperparameter, batch size, controlling (in particular) the amount mini-batch noise. We introduce a theoretical framework to understand how mini-batch noise influences the implicit bias of memory in Adam (depending on $\beta_1$, $\beta_2$) towards sharper or flatter regions of the loss landscape, which is commonly observed to correlate with the generalization gap in multi-epoch training. We find that in the case of large batch sizes, higher $\beta_2$ increases the magnitude of anti-regularization by memory (hurting generalization), but as the batch size becomes smaller, the dependence of (anti-)regulariation on $\beta_2$ is reversed. A similar monotonicity shift (in the opposite direction) happens in $\beta_1$. In particular, the commonly "default" pair $(\beta_1, \beta_2) = (0.9, 0.999)$ is a good choice if batches are small; for larger batches, in many settings moving $\beta_1$ closer to $\beta_2$ is much better in terms of validation accuracy in multi-epoch training. Moreover, our theoretical derivations connect the scale of the batch size at which the shift happens to the scale of the critical batch size. We illustrate this effect in experiments with small-scale data in the about-to-overfit regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01642v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Boris Shigida</dc:creator>
    </item>
    <item>
      <title>Probabilistic function-on-function nonlinear autoregressive model for emulation and reliability analysis of dynamical systems</title>
      <link>https://arxiv.org/abs/2602.01929</link>
      <description>arXiv:2602.01929v1 Announce Type: cross 
Abstract: Constructing accurate and computationally efficient surrogate models (or emulators) for predicting dynamical system responses is critical in many engineering domains, yet remains challenging due to the strongly nonlinear and high-dimensional mapping from external excitations and system parameters to system responses. This work introduces a novel Function-on-Function Nonlinear AutoRegressive model with eXogenous inputs (F2NARX), which reformulates the conventional NARX model from a function-on-function regression perspective, inspired by the recently proposed $\mathcal{F}$-NARX method. The proposed framework substantially improves predictive efficiency while maintaining high accuracy. By combining principal component analysis with Gaussian process regression, F2NARX further enables probabilistic predictions of dynamical responses via the unscented transform in an autoregressive manner. The effectiveness of the method is demonstrated through case studies of varying complexity. Results show that F2NARX outperforms state-of-the-art NARX model by orders of magnitude in efficiency while achieving higher accuracy in general. Moreover, its probabilistic prediction capabilities facilitate active learning, enabling accurate estimation of first-passage failure probabilities of dynamical systems using only a small number of training time histories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01929v1</guid>
      <category>math.DS</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhouzhou Song, Marcos A. Valdebenito, Styfen Sch\"ar, Stefano Marelli, Bruno Sudret, Matthias G. R. Faes</dc:creator>
    </item>
    <item>
      <title>Posterior Uncertainty for Targeted Parameters in Bayesian Bootstrap Procedures</title>
      <link>https://arxiv.org/abs/2602.02216</link>
      <description>arXiv:2602.02216v1 Announce Type: cross 
Abstract: We propose a general method to carry out a valid Bayesian analysis of a finite-dimensional `targeted' parameter in the presence of a finite-dimensional nuisance parameter. We apply our methods to causal inference based on estimating equations. While much of the literature in Bayesian causal inference has relied on the conventional 'likelihood times prior' framework, a recently proposed method, the 'Linked Bayesian Bootstrap', deviated from this classical setting to obtain valid Bayesian inference using the Dirichlet process and the Bayesian bootstrap. These methods rely on an adjustment based on the propensity score and explain how to handle the uncertainty concerning it when studying the posterior distribution of a treatment effect. We examine theoretically the asymptotic properties of the posterior distribution obtained and show that our proposed method, a generalized version of the 'Linked Bayesian Bootstrap', enjoys desirable frequentist properties. In addition, we show that the credible intervals have asymptotically the correct coverage properties. We discuss the applications of our method to mis-specified and singly-robust models in causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.02216v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Magid Sabbagh, David A. Stephens</dc:creator>
    </item>
    <item>
      <title>Dimension-independent convergence rates of randomized nets using median-of-means</title>
      <link>https://arxiv.org/abs/2505.13815</link>
      <description>arXiv:2505.13815v4 Announce Type: replace 
Abstract: Recent advances in quasi-Monte Carlo integration demonstrate that the median of linearly scrambled digital net estimators achieves near-optimal convergence rates for high-dimensional integrals without requiring a priori knowledge of the integrand's smoothness. Building on this framework, we prove that the median estimator attains dimension-independent convergence, a property known as strong tractability in complexity theory, under tractability conditions characterized by low effective dimensionality. Using a probabilistic, integrand-specific error criterion, our analysis establishes both faster and dimension-independent convergence under weaker assumptions than previously possible in the worst-case setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13815v4</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zexin Pan</dc:creator>
    </item>
    <item>
      <title>Bayesian modelling and quantification of Raman spectroscopy</title>
      <link>https://arxiv.org/abs/1604.07299</link>
      <description>arXiv:1604.07299v3 Announce Type: replace-cross 
Abstract: Raman spectroscopy can be used to identify molecules such as DNA by the characteristic scattering of light from a laser. It is sensitive at very low concentrations and can accurately quantify the amount of a given molecule in a sample. The presence of a large, nonuniform background presents a major challenge to analysis of these spectra. To overcome this challenge, we introduce a sequential Monte Carlo (SMC) algorithm to separate the observed spectrum into a series of peaks plus a smoothly-varying baseline, corrupted by additive white noise. The peaks are modelled using Lorentzian or Gaussian broadening functions, while the baseline is estimated using a penalised cubic spline. This latent continuous representation accounts for differences in resolution between measurements. By incorporating this representation in a Bayesian model, we can quantify the relationship between molecular concentration and peak intensity, thereby providing an improved estimate of the limit of detection (LOD), which is of major importance in analytical chemistry.</description>
      <guid isPermaLink="false">oai:arXiv.org:1604.07299v3</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Moores, Kirsten Gracie, Jake Carson, Karen Faulds, Duncan Graham, Mark Girolami</dc:creator>
    </item>
    <item>
      <title>Unbiased Approximations for Stationary Distributions of McKean-Vlasov SDEs</title>
      <link>https://arxiv.org/abs/2411.11270</link>
      <description>arXiv:2411.11270v2 Announce Type: replace-cross 
Abstract: We consider the development of unbiased estimators, to approximate the stationary distribution of Mckean-Vlasov stochastic differential equations (MVSDEs). These are an important class of processes, which frequently appear in applications such as mathematical finance, biology and opinion dynamics. Typically the stationary distribution is unknown and indeed one cannot simulate such processes exactly. As a result one commonly requires a time-discretization scheme which results in a discretization bias and a bias from not being able to simulate the associated stationary distribution. To overcome this bias, we present a new unbiased estimator taking motivation from the literature on unbiased Monte Carlo. We prove the unbiasedness of our estimator, under assumptions. In order to prove this we require developing ergodicity results of various discrete time processes, through an appropriate discretization scheme, towards the invariant measure. Numerous numerical experiments are provided, on a range of MVSDEs, to demonstrate the effectiveness of our unbiased estimator. Such examples include the Currie-Weiss model, a 3D neuroscience model and a parameter estimation problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11270v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elsiddig Awadelkarim, Neil K. Chada, Ajay Jasra</dc:creator>
    </item>
    <item>
      <title>Synchronized step multilevel Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2501.16538</link>
      <description>arXiv:2501.16538v2 Announce Type: replace-cross 
Abstract: We propose SYNCE (synchronized step correlation enhancement), a new algorithm for coupling Markov chains within multilevel Markov chain Monte Carlo (ML-MCMC) estimators. We apply this algorithm to solve Bayesian inverse problems using multiple model fidelities. SYNCE is inspired by the concept of common random number coupling in Markov chain Monte Carlo sampling. Unlike state-of-the-art methods that rely on the overlap of level-wise posteriors, our approach enables effective coupling even when posteriors differ substantially. This overlap-independence generates significantly higher correlation between samples at different fidelity levels, improving variance reduction and computational efficiency in the ML-MCMC estimator. We prove that SYNCE admits a unique invariant probability measure and demonstrate that the coupled chains converge to this measure faster than existing overlap-dependent methods, particularly when models are dissimilar. Numerical experiments validate that SYNCE consistently outperforms current coupling strategies in terms of computational efficiency and scalability across varying model fidelities and problem dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16538v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjan C. Muchandimath, Alex A. Gorodetsky</dc:creator>
    </item>
    <item>
      <title>An Overview of Low-Rank Structures in the Training and Adaptation of Large Models</title>
      <link>https://arxiv.org/abs/2503.19859</link>
      <description>arXiv:2503.19859v3 Announce Type: replace-cross 
Abstract: The substantial computational demands of modern large-scale deep learning present significant challenges for efficient training and deployment. Recent research has revealed a widespread phenomenon wherein deep networks inherently learn low-rank structures in their weights and representations during training. This tutorial paper provides a comprehensive review of advances in identifying and exploiting these low-rank structures, bridging mathematical foundations with practical applications. We present two complementary theoretical perspectives on the emergence of low-rankness: viewing it through the optimization dynamics of gradient descent throughout training, and understanding it as a result of implicit regularization effects at convergence. Practically, these theoretical perspectives provide a foundation for understanding the success of techniques such as Low-Rank Adaptation (LoRA) in fine-tuning, inspire new parameter-efficient low-rank training strategies, and explain the effectiveness of masked training approaches like dropout and masked self-supervised learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19859v3</guid>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Balzano, Tianjiao Ding, Benjamin D. Haeffele, Soo Min Kwon, Qing Qu, Peng Wang, Zhangyang Wang, Can Yaras</dc:creator>
    </item>
    <item>
      <title>shapr: Explaining Machine Learning Models with Conditional Shapley Values in R and Python</title>
      <link>https://arxiv.org/abs/2504.01842</link>
      <description>arXiv:2504.01842v3 Announce Type: replace-cross 
Abstract: This paper introduces the shapr R package, a versatile tool for generating Shapley value-based prediction explanations for machine learning and statistical regression models. Moreover, the shaprpy Python library brings the core capabilities of shapr to the Python ecosystem. Shapley values originate from cooperative game theory in the 1950s, but have over the past few years become a widely used method for quantifying how a model's features/covariates contribute to specific prediction outcomes. The shapr package emphasizes conditional Shapley value estimates, providing a comprehensive range of approaches for accurately capturing feature dependencies -- a crucial aspect for correct model explanation, typically lacking in similar software. In addition to regular tabular data, the shapr R package includes specialized functionality for explaining time series forecasts. The package offers a minimal set of user functions with sensible default values for most use cases while providing extensive flexibility for advanced users to fine-tune computations. Additional features include parallelized computations, iterative estimation with convergence detection, and rich visualization tools. shapr also extends its functionality to compute causal and asymmetric Shapley values when causal information is available. Overall, the shapr and shaprpy packages aim to enhance the interpretability of predictive models within a powerful and user-friendly framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01842v3</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Jullum, Lars Henry Berge Olsen, Jon Lachmann, Annabelle Redelmeier</dc:creator>
    </item>
    <item>
      <title>Empirical Bernstein and betting confidence intervals for randomized quasi-Monte Carlo</title>
      <link>https://arxiv.org/abs/2504.18677</link>
      <description>arXiv:2504.18677v2 Announce Type: replace-cross 
Abstract: Randomized quasi-Monte Carlo (RQMC) methods estimate the mean of a random variable by sampling an integrand at $n$ equidistributed points. For scrambled digital nets, the resulting variance is typically $\tilde O(n^{-\theta})$ where $\theta\in[1,3]$ depends on the smoothness of the integrand and $\tilde O$ neglects logarithmic factors. While RQMC can be far more accurate than plain Monte Carlo (MC) it remains difficult to get confidence intervals on RQMC estimates. We investigate some empirical Bernstein confidence intervals (EBCI) and hedged betting confidence intervals (HBCI), both from Waudby-Smith and Ramdas (2024), when the random variable of interest is subject to known bounds. When there are $N$ integrand evaluations partitioned into $R$ independent replicates of $n=N/R$ RQMC points, and the RQMC variance is $\Theta(n^{-\theta})$, then an oracle minimizing the width of a Bennett confidence interval would choose $n =\Theta(N^{1/(\theta+1)})$. The resulting intervals have a width that is $\Theta(N^{-\theta/(\theta+1)})$. Our empirical investigations had optimal values of $n$ grow slowly with $N$, HBCI intervals that were usually narrower than the EBCI ones, and optimal values of $n$ for HBCI that were equal to or smaller than the ones for the oracle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.18677v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aadit Jain, Fred J. Hickernell, Art B. Owen, Aleksei G. Sorokin</dc:creator>
    </item>
    <item>
      <title>Scalable Signed Exponential Random Graph Models under Local Dependence</title>
      <link>https://arxiv.org/abs/2507.07660</link>
      <description>arXiv:2507.07660v4 Announce Type: replace-cross 
Abstract: Traditional network analysis focuses on binary edges, while real-world relationships are more nuanced, encompassing cooperation, neutrality, and conflict. The rise of negative edges in social media discussions spurred interest in analyzing signed interactions, especially in polarized debates. However, the vast data generated by digital networks presents challenges for traditional methods like Stochastic Block Models (SBM) and Exponential Family Random Graph Models (ERGM), particularly due to the homogeneity assumption and global dependence, which become increasingly unrealistic as network size grows. To address this, we propose a novel method that combines the strengths of SBM and ERGM while mitigating their weaknesses by incorporating local dependence based on nonoverlapping blocks. Our approach involves a two-step process: First, decomposing the network into sub-networks using SBM approximation, and, second, estimating parameters using ERGM methods. We validate our method on large synthetic networks and apply it to a signed Wikipedia network of thousands of editors. Through the use of local dependence, we find patterns consistent with structural balance theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07660v4</guid>
      <category>cs.SI</category>
      <category>stat.CO</category>
      <pubDate>Tue, 03 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marc Schalberger, Cornelius Fritz</dc:creator>
    </item>
  </channel>
</rss>
