<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Nov 2025 05:00:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>FlowMesh: A Service Fabric for Composable LLM Workflows</title>
      <link>https://arxiv.org/abs/2510.26913</link>
      <description>arXiv:2510.26913v1 Announce Type: new 
Abstract: AI deployment increasingly resembles a pipeline of data transformation, fine-tuning, and agent interactions rather than a monolithic LLM job; recent examples include RLHF/RLAIF training and agentic workflows. To cope with this shift, we propose FlowMesh, a multi-tenant service fabric that executes and optimizes these workloads as one shared service instead of isolated pipelines. It decomposes workflows into fine-grained operators with recorded lineage, enabling de-duplication of work across users and batching requests on the same hardware while preserving per-workflow provenance. A global control plane maintains a cluster-wide pool of ready operators and uses a single utility function to pick both the batch and the worker, balancing throughput, cost, and data locality on heterogeneous GPUs. The data plane is an elastic fleet of stateless workers backed by a content-addressable store, enabling rapid, automatic scale-out, safe retry after preemption, and portability across managed clusters such as Kubernetes and geo-distributed GPU marketplaces such as Vast.ai. Compared with baseline solutions, FlowMesh achieves up to 3.8x cost reduction and 2.0x lower energy usage, provides a similar or better latency profile, and remains efficient under dynamic and failure-prone conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26913v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junyi Shen, Noppanat Wadlom, Lingfeng Zhou, Dequan Wang, Xu Miao, Lei Fang, Yao Lu</dc:creator>
    </item>
    <item>
      <title>A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration</title>
      <link>https://arxiv.org/abs/2510.27039</link>
      <description>arXiv:2510.27039v1 Announce Type: new 
Abstract: Accurate traffic flow forecasting is essential for the development of intelligent transportation systems (ITS), supporting tasks such as traffic signal optimization, congestion management, and route planning. Traditional models often fail to effectively capture complex spatial-temporal dependencies in large-scale road networks, especially under the influence of external factors such as weather, holidays, and traffic accidents. To address this challenge, this paper proposes a cloud-based hybrid model that integrates Spatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecture for traffic flow prediction. The model leverages the strengths of GNNs in modeling spatial correlations across road networks and the Transformers' ability to capture long-term temporal dependencies. External contextual features are incorporated via feature fusion to enhance predictive accuracy. The proposed model is deployed on a cloud computing platform to achieve scalability and real-time adaptability. Experimental evaluation of the dataset shows that our model outperforms baseline methods (LSTM, TCN, GCN, pure Transformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findings suggest that the hybrid GNN-Transformer approach provides an effective and scalable solution for cloud-based ITS applications, offering methodological advancements for traffic flow forecasting and practical implications for congestion mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27039v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuo Zheng, Lingran Meng, Ziyu Lin</dc:creator>
    </item>
    <item>
      <title>Synergistic Tensor and Pipeline Parallelism</title>
      <link>https://arxiv.org/abs/2510.27257</link>
      <description>arXiv:2510.27257v1 Announce Type: new 
Abstract: In the machine learning system, the hybrid model parallelism combining tensor parallelism (TP) and pipeline parallelism (PP) has become the dominant solution for distributed training of Large Language Models~(LLMs) and Multimodal LLMs (MLLMs). However, TP introduces significant collective communication overheads, while PP suffers from synchronization inefficiencies such as pipeline bubbles. Existing works primarily address these challenges from isolated perspectives, focusing either on overlapping TP communication or on flexible PP scheduling to mitigate pipeline bubbles. In this paper, we propose a new synergistic tensor and pipeline parallelism schedule that simultaneously reduces both types of bubbles. Our proposed schedule decouples the forward and backward passes in PP into fine-grained computation units, which are then braided to form a composite computation sequence. This compositional structure enables near-complete elimination of TP-related bubbles. Building upon this structure, we further design the PP schedule to minimize PP bubbles. Experimental results demonstrate that our approach improves training throughput by up to 12% for LLMs and 16% for MLLMs compared to existing scheduling methods. Our source code is avaiable at https://github.com/MICLAB-BUPT/STP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27257v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengshi Qi, Jiaxuan Peng, Jie Zhang, Juan Zhu, Yong Li, Huadong Ma</dc:creator>
    </item>
    <item>
      <title>A Digital Twin-based Multi-Agent Reinforcement Learning Framework for Vehicle-to-Grid Coordination</title>
      <link>https://arxiv.org/abs/2510.27289</link>
      <description>arXiv:2510.27289v1 Announce Type: new 
Abstract: The coordination of large-scale, decentralised systems, such as a fleet of Electric Vehicles (EVs) in a Vehicle-to-Grid (V2G) network, presents a significant challenge for modern control systems. While collaborative Digital Twins have been proposed as a solution to manage such systems without compromising the privacy of individual agents, deriving globally optimal control policies from the high-level information they share remains an open problem. This paper introduces Digital Twin Assisted Multi-Agent Deep Deterministic Policy Gradient (DT-MADDPG) algorithm, a novel hybrid architecture that integrates a multi-agent reinforcement learning framework with a collaborative DT network. Our core contribution is a simulation-assisted learning algorithm where the centralised critic is enhanced by a predictive global model that is collaboratively built from the privacy-preserving data shared by individual DTs. This approach removes the need for collecting sensitive raw data at a centralised entity, a requirement of traditional multi-agent learning algorithms. Experimental results in a simulated V2G environment demonstrate that DT-MADDPG can achieve coordination performance comparable to the standard MADDPG algorithm while offering significant advantages in terms of data privacy and architectural decentralisation. This work presents a practical and robust framework for deploying intelligent, learning-based coordination in complex, real-world cyber-physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27289v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengchang Hua, Panagiotis Oikonomou, Karim Djemame, Nikos Tziritas, Georgios Theodoropoulos</dc:creator>
    </item>
    <item>
      <title>Dynamic Service Scheduling and Resource Management in Energy-Harvesting Multi-access Edge Computing</title>
      <link>https://arxiv.org/abs/2510.27317</link>
      <description>arXiv:2510.27317v1 Announce Type: new 
Abstract: Multi-access Edge Computing (MEC) delivers low-latency services by hosting applications near end-users. To promote sustainability, these systems are increasingly integrated with renewable Energy Harvesting (EH) technologies, enabling operation where grid electricity is unavailable. However, balancing the intermittent nature of harvested energy with dynamic user demand presents a significant resource allocation challenge. This work proposes an online strategy for an MEC system powered exclusively by EH to address this trade-off. Our strategy dynamically schedules computational tasks with dependencies and governs energy consumption through real-time decisions on server frequency scaling and service module migration. Experiments using real-world datasets demonstrate our algorithm's effectiveness in efficiently utilizing harvested energy while maintaining low service latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27317v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuyi Chen, Panagiotis Oikonomou, Zhengchang Hua, Nikos Tziritas, Karim Djemame, Nan Zhang, Georgios Theodoropoulos</dc:creator>
    </item>
    <item>
      <title>ML-Based Optimum Sub-system Size Heuristic for the GPU Implementation of the Tridiagonal Partition Method</title>
      <link>https://arxiv.org/abs/2510.27351</link>
      <description>arXiv:2510.27351v1 Announce Type: new 
Abstract: This paper presents a machine learning (ML)-based heuristic for finding the optimum sub-system size for the CUDA implementation of the parallel partition algorithm. Computational experiments for different system of linear algebraic equation (SLAE) sizes are conducted, and the optimum sub-system size for each of them is found empirically. To estimate a model for the sub-system size, we perform the k-nearest neighbors (kNN) classification method. Statistical analysis of the results is done. By comparing the predicted values with the actual data, the algorithm is deemed to be acceptably good. Next, the heuristic is expanded to work for the recursive parallel partition algorithm as well. An algorithm for determining the optimum sub-system size for each recursive step is formulated. A kNN model for predicting the optimum number of recursive steps for a particular SLAE size is built.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27351v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Milena Veneva</dc:creator>
    </item>
    <item>
      <title>RDMA Point-to-Point Communication for LLM Systems</title>
      <link>https://arxiv.org/abs/2510.27656</link>
      <description>arXiv:2510.27656v1 Announce Type: new 
Abstract: Emerging Large Language Model (LLM) system patterns, such as disaggregated inference, Mixture-of-Experts (MoE) routing, and asynchronous reinforcement fine-tuning, require flexible point-to-point communication beyond simple collectives. Existing implementations are locked to specific Network Interface Controllers (NICs), hindering integration into inference engines and portability across hardware providers. We present TransferEngine, which bridges the functionality of common NICs to expose a uniform interface. TransferEngine exposes one-sided WriteImm operations with a ImmCounter primitive for completion notification, without ordering assumptions of network transport, transparently managing multiple NICs per GPU. We demonstrate peak throughput of 400 Gbps on both NVIDIA ConnectX-7 and AWS Elastic Fabric Adapter (EFA). We showcase TransferEngine through three production systems: (1) KvCache transfer for disaggregated inference with dynamic scaling, (2) RL weight updates achieving 1.3 seconds for trillion-parameter models, and (3) MoE dispatch/combine implementation exceeding DeepEP decode latency on ConnectX-7, with the first viable latencies on EFA. We demonstrate that our portable point-to-point communication complements collectives while avoiding lock-in.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27656v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nandor Licker (Perplexity AI), Kevin Hu (Perplexity AI), Vladimir Zaytsev (Perplexity AI), Lequn Chen (Perplexity AI)</dc:creator>
    </item>
    <item>
      <title>LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources</title>
      <link>https://arxiv.org/abs/2510.18477</link>
      <description>arXiv:2510.18477v2 Announce Type: cross 
Abstract: Large Language Models (LLMs) have shown great promise in automating data analytics tasks by interpreting natural language queries and generating multi-operation execution plans. However, existing LLM-agent-based analytics frameworks operate under the assumption of centralized data access, offering little to no privacy protection. In contrast, federated analytics (FA) enables privacy-preserving computation across distributed data sources, but lacks support for natural language input and requires structured, machine-readable queries. In this work, we present LAFA, the first system that integrates LLM-agent-based data analytics with FA. LAFA introduces a hierarchical multi-agent architecture that accepts natural language queries and transforms them into optimized, executable FA workflows. A coarse-grained planner first decomposes complex queries into sub-queries, while a fine-grained planner maps each subquery into a Directed Acyclic Graph of FA operations using prior structural knowledge. To improve execution efficiency, an optimizer agent rewrites and merges multiple DAGs, eliminating redundant operations and minimizing computational and communicational overhead. Our experiments demonstrate that LAFA consistently outperforms baseline prompting strategies by achieving higher execution plan success rates and reducing resource-intensive FA operations by a substantial margin. This work establishes a practical foundation for privacy-preserving, LLM-driven analytics that supports natural language input in the FA setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18477v2</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haichao Ji, Zibo Wang, Cheng Pan, Meng Han, Yifei Zhu, Dan Wang, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Secure Communication in the Presence of an RIS-Enhanced Eavesdropper in MIMO Networks</title>
      <link>https://arxiv.org/abs/2510.27147</link>
      <description>arXiv:2510.27147v1 Announce Type: cross 
Abstract: We pay our attention towards secure and robust communication in the presence of a Reconfigurable Intelligent Surface (RIS)-enhanced mobile eavesdropping attacker in Multiple-Input Multiple-Output (MIMO)wireless networks.Specifically,we first provide a unifying framework that generalizes specific intelligent wiretap model wherein the passive eavesdropper configured with any number of antennas is potentially mobile and can actively optimize its received signal strength with the help of RIS by intelligently manipulating wiretap channel characteristics.To effectively mitigate this intractable threat,we then propose a novel and lightweight secure communication scheme from the perspective of information theory.The main idea is that the data processing can in some cases be observed as communication channel,and a random bit-flipping scheme is then carefully involved for the legitimate transmitter to minimize the mutual information between the secret message and the passive eavesdropper's received data.The Singular Value Decomposition (SVD)-based precoding strategy is also implemented to optimize power allocation,and thus ensure that the legitimate receiver is not subject to interference from this random bit-flipping.The corresponding results depict that our secure communication scheme is practically desired, which does not require any a prior knowledge of the eavesdropper's full instantaneous Channel State Information (ICSI). Furthermore,we consider the RIS optimization problem from the eavesdropper's perspective,and provide RIS phase shift design solutions under different attacking scenarios.Finally,the optimal detection schemes respectively for the legitimate user and the eavesdropper are provided,and comprehensive simulations are presented to verify our theoretical analysis and show the effectiveness and robustness of our secure communication scheme across a wide range of attacking scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27147v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaoyuan Zhang, Ruisong Si, Boyuan Li, Zijian Li, Baofeng Ji, Chenqi Zhu, Tony Q. S. Quek</dc:creator>
    </item>
    <item>
      <title>Byzantine Attacks in RIS-Enhanced Cooperative Spectrum Sensing: A Decision Fusion Perspective</title>
      <link>https://arxiv.org/abs/2510.27175</link>
      <description>arXiv:2510.27175v1 Announce Type: cross 
Abstract: From the perspective of hard decision fusion, we investigate Byzantine attacks in Reconfigurable Intelligent Surface (RIS)-enhanced and decode-and-forward relay-assisted Cooperative Spectrum Sensing (CSS) for mobile Cognitive Radio Networks (CRNs) in this paper. Specially, a RIS-enhanced and decode-and-forward relay-assisted CSS configuration is first constructed under dynamic channel scenarios due to user mobility. Subsequently, the channel- and attack-aware hard decision fusion rules are developed, and the optimal channel-aware Byzantine attack strategies are then developed under both small-scale and large-scale attacking scenarios. The corresponding results depict that the optimal attack strategy does not require any a prior knowledge of the global instantaneous Channel State Information (ICSI) (e.g. false alarm probability and detection probability of all the secondary users), although perfect acquisition of ICSI is clearly always not affordable from the attacker perspective, which is further exacerbated by the RIS and decode-and-forward relays involved in CSS and the potential high mobility of secondary users that leads to fast fading channels. Furthermore, our counterintuitive results also indicate that, regardless of the attacker's awareness of the decision fusion rule, the optimal Byzantine attack can be achieved through a unifying framework, the explicit attack strategy may be not unique, and the attacking effectiveness is primarily determined by the fraction of the Byzantine nodes rather than the channel dynamics. That is, to make the channel-aware approach more practical, the challenge that the heavy reliance on the global ICSI and decision fusion rule in obtaining the Byzantine attacks is successfully relaxed. Finally, we empirically validate our theoretical analysis through extensive simulations across a wide range of attacking scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27175v1</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>math.IT</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gaoyuan Zhang, Gaolei Song, Boyuan Li, Zijian Li, Baofeng Ji, Ruijuan Zheng, Guoqiang Zheng, Tony Q. S. Quek</dc:creator>
    </item>
    <item>
      <title>Glia: A Human-Inspired AI for Automated Systems Design and Optimization</title>
      <link>https://arxiv.org/abs/2510.27176</link>
      <description>arXiv:2510.27176v1 Announce Type: cross 
Abstract: Can an AI autonomously design mechanisms for computer systems on par with the creativity and reasoning of human experts? We present Glia, an AI architecture for networked systems design that uses large language models (LLMs) in a human-inspired, multi-agent workflow. Each agent specializes in reasoning, experimentation, and analysis, collaborating through an evaluation framework that grounds abstract reasoning in empirical feedback. Unlike prior ML-for-systems methods that optimize black-box policies, Glia generates interpretable designs and exposes its reasoning process. When applied to a distributed GPU cluster for LLM inference, it produces new algorithms for request routing, scheduling, and auto-scaling that perform at human-expert levels in significantly less time, while yielding novel insights into workload behavior. Our results suggest that by combining reasoning LLMs with structured experimentation, an AI can produce creative and understandable designs for complex systems problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27176v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pouya Hamadanian, Pantea Karimi, Arash Nasr-Esfahany, Kimia Noorbakhsh, Joseph Chandler, Ali ParandehGheibi, Mohammad Alizadeh, Hari Balakrishnan</dc:creator>
    </item>
    <item>
      <title>SERFLOW: A Cross-Service Cost Optimization Framework for SLO-Aware Dynamic ML Inference</title>
      <link>https://arxiv.org/abs/2510.27182</link>
      <description>arXiv:2510.27182v1 Announce Type: cross 
Abstract: Dynamic offloading of Machine Learning (ML) model partitions across different resource orchestration services, such as Function-as-a-Service (FaaS) and Infrastructure-as-a-Service (IaaS), can balance processing and transmission delays while minimizing costs of adaptive inference applications. However, prior work often overlooks real-world factors, such as Virtual Machine (VM) cold starts, requests under long-tail service time distributions, etc. To tackle these limitations, we model each ML query (request) as traversing an acyclic sequence of stages, wherein each stage constitutes a contiguous block of sparse model parameters ending in an internal or final classifier where requests may exit. Since input-dependent exit rates vary, no single resource configuration suits all query distributions. IaaS-based VMs become underutilized when many requests exit early, yet rapidly scaling to handle request bursts reaching deep layers is impractical. SERFLOW addresses this challenge by leveraging FaaS-based serverless functions (containers) and using stage-specific resource provisioning that accounts for the fraction of requests exiting at each stage. By integrating this provisioning with adaptive load balancing across VMs and serverless functions based on request ingestion, SERFLOW reduces cloud costs by over $23\%$ while efficiently adapting to dynamic workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27182v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zongshun Zhang, Ibrahim Matta</dc:creator>
    </item>
    <item>
      <title>Recognizing Hereditary Properties in the Presence of Byzantine Nodes</title>
      <link>https://arxiv.org/abs/2312.07747</link>
      <description>arXiv:2312.07747v2 Announce Type: replace 
Abstract: Augustine et al. [DISC 2022] initiated the study of distributed graph algorithms in the presence of Byzantine nodes in the congested clique model. In this model, there is a set $B$ of Byzantine nodes, where $|B|$ is less than a third of the total number of nodes. These nodes have complete knowledge of the network and the state of other nodes, and they conspire to alter the output of the system. The authors addressed the connectivity problem, showing that it is solvable under the promise that either the subgraph induced by the honest nodes is connected, or the graph has $2|B|+1$ connected components.
  In the current work, we continue the study of the Byzantine congested clique model by considering the recognition of other graph properties, specifically hereditary properties. A graph property is hereditary if it is closed under taking induced subgraphs. Examples of hereditary properties include acyclicity, bipartiteness, planarity, and bounded (chromatic, independence) number, etc.
  For each class of graphs ${\bf G}$ satisfying a hereditary property (a hereditary graph-class), we propose a randomized algorithm which, with high probability, (1) accepts if the input graph $G$ belongs to ${\bf G}$, and (2) rejects if $G$ contains at least $|B| + 1$ disjoint subgraphs not belonging to ${\bf G}$. The round complexity of our algorithm is $$O\left(\left(\dfrac{\log \left(\left|{\bf G}_n\right|\right)}{n} +|B|\right)\cdot\textrm{polylog}(n)\right),$$ where ${\bf G}_n$ is the set of $n$-node graphs in ${\bf G}$.
  Finally, we obtain an impossibility result that proves that our result is tight. Indeed, we consider the hereditary class of acyclic graphs, and we prove that there is no algorithm that can distinguish between a graph being acyclic and a graph having $|B|$ disjoint cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07747v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Cifuentes-N\'u\~nez, Pedro Montealegre, Ivan Rapaport</dc:creator>
    </item>
    <item>
      <title>High-Performance Statistical Computing (HPSC): Challenges, Opportunities, and Future Directions</title>
      <link>https://arxiv.org/abs/2508.04013</link>
      <description>arXiv:2508.04013v3 Announce Type: replace 
Abstract: We recognize the emergence of a statistical computing community focused on working with large computing platforms and producing software and applications that exemplify high-performance statistical computing (HPSC). The statistical computing (SC) community develops software that is widely used across disciplines. However, it remains largely absent from the high-performance computing (HPC) landscape, particularly on platforms such as those featured on the Top500 or Green500 lists. Many disciplines already participate in HPC, mostly centered around simulation science, although data-focused efforts under the artificial intelligence (AI) label are gaining popularity. Bridging this gap requires both community adaptation and technical innovation to align statistical methods with modern HPC technologies. We can accelerate progress in fast and scalable statistical applications by building strong connections between the SC and HPC communities. We present a brief history of SC, a vision for how its strengths can contribute to statistical science in the HPC environment (such as HPSC), the challenges that remain, and the opportunities currently available, culminating in a possible roadmap toward a thriving HPSC community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04013v3</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sameh Abdulah, Mary Lai O. Salvana, Ying Sun, David E. Keyes, Marc G. Genton</dc:creator>
    </item>
    <item>
      <title>Declarative Data Pipeline for Large Scale ML Services</title>
      <link>https://arxiv.org/abs/2508.15105</link>
      <description>arXiv:2508.15105v2 Announce Type: replace 
Abstract: Modern distributed data processing systems face significant challenges in balancing system performance with code maintainability and developer productivity, particularly when integrating machine learning capabilities at scale. In large collaborative environments, these challenges are amplified by high communication overhead between teams and the complexity of coordinating development across multiple groups. This paper presents a novel "Declarative Data Pipeline" architecture that addresses these challenges while processing billions of records with high accuracy and efficiency. Our architecture introduces a modular framework that seamlessly integrates machine learning capabilities within Apache Spark by combining logical computation units that we refer as Pipes, departing from traditional microservice-based approaches. By establishing clear component boundaries and standardized interfaces, we achieve both modularity and system optimization without sacrificing maintainability. The enterprise case study demonstrate substantial improvements in multiple dimensions: development efficiency improved by 50%, collaboration/troubleshooting efforts compressed from weeks to days, performance improved by 500x in scalability and by 10x in throughput. The academic experiment also proves at least 5.7x faster in throughput with 99% CPU utilization than non-framework implementations. This paper details the architectural decisions, implementation strategies, and performance optimizations that enable these improvements, providing insights for building scalable, maintainable data processing systems that effectively balance system performance with development velocity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15105v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunzhao Yang, Runhui Wang, Xuanqing Liu, Adit Krishnan, Yefan Tao, Yuqian Deng, Kuangyou Yao, Peiyuan Sun, Henrik Johnson, Aditi sinha, Davor Golac, Gerald Friedland, Usman Shakeel, Daryl Cooke, Joe Sullivan, Chris Kong</dc:creator>
    </item>
    <item>
      <title>Accelerating Mixture-of-Experts Inference by Hiding Offloading Latency with Speculative Decoding</title>
      <link>https://arxiv.org/abs/2508.21706</link>
      <description>arXiv:2508.21706v2 Announce Type: replace 
Abstract: Recent advancements in Mixture of Experts (MoE) models have significantly increased their parameter scale as well as model performance. Extensive offloading techniques have been proposed to address the GPU memory limitations of MoE inference. However, due to the I/O bottleneck and sparse computation of MoE models, existing offloading techniques still suffer from low hardware utilization. To fully utilize the hardware resources, we propose SpecMoEOff, which employs the speculative decoding technique to enlarge the workload of each expert. SpecMoEOff orchestrates the GPU and CPU by both theoretical and empirical roofline analysis. In addition, we develop a dedicated CPU chunked attention verification kernel to fit the speculative decoding in offloading scenarios as well as minimizing the additional overhead led by draft models. SpecMoEOff further integrates an optimizer to automatically tune the hyperparameters of speculative decoding under given hardware and workload. Experimental results show that SpecMoEOff achieves up to 2.5x decode throughput improvement over the state-of-the-art MoE offloading techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21706v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhibin Wang, Zhonghui Zhang, Yuhang Zhou, Zibo Wang, Mo Zhou, Peng Jiang, Weilin Cai, Chengying Huan, Rong Gu, Sheng Zhong, Chen Tian</dc:creator>
    </item>
    <item>
      <title>Hybrid Dual-Batch and Cyclic Progressive Learning for Efficient Distributed Training</title>
      <link>https://arxiv.org/abs/2509.26092</link>
      <description>arXiv:2509.26092v2 Announce Type: replace 
Abstract: Distributed machine learning is critical for training deep learning models on large datasets with numerous parameters. Current research primarily focuses on leveraging additional hardware resources and powerful computing units to accelerate the training process. As a result, larger batch sizes are often employed to speed up training. However, training with large batch sizes can lead to lower accuracy due to poor generalization. To address this issue, we propose the dual-batch learning scheme, a distributed training method built on the parameter server framework. This approach maximizes training efficiency by utilizing the largest batch size that the hardware can support while incorporating a smaller batch size to enhance model generalization. By using two different batch sizes simultaneously, this method improves accuracy with minimal additional training time. Additionally, to mitigate the time overhead caused by dual-batch learning, we propose the cyclic progressive learning scheme. This technique repeatedly and gradually increases image resolution from low to high during training, thereby reducing training time. By combining cyclic progressive learning with dual-batch learning, our hybrid approach improves both model generalization and training efficiency. Experimental results with ResNet-18 demonstrate that, compared to conventional training methods, our approach improves accuracy by 3.3% while reducing training time by 10.1% on CIFAR-100, and further achieves a 34.8% reduction in training time on ImageNet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26092v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kuan-Wei Lu, Ding-Yong Hong, Pangfeng Liu, Jan-Jan Wu</dc:creator>
    </item>
    <item>
      <title>Tokencake: A KV-Cache-centric Serving Framework for LLM-based Multi-Agent Applications</title>
      <link>https://arxiv.org/abs/2510.18586</link>
      <description>arXiv:2510.18586v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are increasingly deployed in complex multi-agent applications that use external function calls. This workload creates severe performance challenges for the KV Cache: space contention leads to the eviction of critical agents' caches and time underutilization leaves the cache of agents stalled on long-running tool calls idling in GPU memory. We present Tokencake, a KV-Cache-centric serving framework that co-optimizes scheduling and memory management with an agent-aware design. Tokencake's Space Scheduler uses dynamic memory partitioning to shield critical agents from contention, while its Time Scheduler employs a proactive offload and predictive upload mechanism to repurpose GPU memory during function call stalls. Our evaluation on representative multi-agent benchmarks shows that Tokencake can reduce end-to-end latency by over 47.06%, improve effective GPU memory utilization by up to 16.9% compared to vLLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18586v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhuohang Bian, Feiyang Wu, Teng Ma, Youwei Zhuo</dc:creator>
    </item>
    <item>
      <title>Hybrid Decentralized Optimization: Leveraging Both First- and Zeroth-Order Optimizers for Faster Convergence</title>
      <link>https://arxiv.org/abs/2210.07703</link>
      <description>arXiv:2210.07703v4 Announce Type: replace-cross 
Abstract: Distributed optimization is the standard way of speeding up machine learning training, and most of the research in the area focuses on distributed first-order, gradient-based methods. Yet, there are settings where some computationally-bounded nodes may not be able to implement first-order, gradient-based optimization, while they could still contribute to joint optimization tasks. In this paper, we initiate the study of hybrid decentralized optimization, studying settings where nodes with zeroth-order and first-order optimization capabilities co-exist in a distributed system, and attempt to jointly solve an optimization task over some data distribution. We essentially show that, under reasonable parameter settings, such a system can not only withstand noisier zeroth-order agents but can even benefit from integrating such agents into the optimization process, rather than ignoring their information. At the core of our approach is a new analysis of distributed optimization with noisy and possibly-biased gradient estimators, which may be of independent interest. Our results hold for both convex and non-convex objectives. Experimental results on standard optimization tasks confirm our analysis, showing that hybrid first-zeroth order optimization can be practical, even when training deep neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.07703v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matin Ansaripour, Shayan Talaei, Giorgi Nadiradze, Dan Alistarh</dc:creator>
    </item>
    <item>
      <title>Resource-Adaptive Successive Doubling for Hyperparameter Optimization with Large Datasets on High-Performance Computing Systems</title>
      <link>https://arxiv.org/abs/2412.02729</link>
      <description>arXiv:2412.02729v2 Announce Type: replace-cross 
Abstract: On High-Performance Computing (HPC) systems, several hyperparameter configurations can be evaluated in parallel to speed up the Hyperparameter Optimization (HPO) process. State-of-the-art HPO methods follow a bandit-based approach and build on top of successive halving, where the final performance of a combination is estimated based on a lower than fully trained fidelity performance metric and more promising combinations are assigned more resources over time. Frequently, the number of epochs is treated as a resource, letting more promising combinations train longer. Another option is to use the number of workers as a resource and directly allocate more workers to more promising configurations via data-parallel training. This article proposes a novel Resource-Adaptive Successive Doubling Algorithm (RASDA), which combines a resource-adaptive successive doubling scheme with the plain Asynchronous Successive Halving Algorithm (ASHA). Scalability of this approach is shown on up to 1,024 Graphics Processing Units (GPUs) on modern HPC systems. It is applied to different types of Neural Networks (NNs) and trained on large datasets from the Computer Vision (CV), Computational Fluid Dynamics (CFD), and Additive Manufacturing (AM) domains, where performing more than one full training run is usually infeasible. Empirical results show that RASDA outperforms ASHA by a factor of up to 1.9 with respect to the runtime. At the same time, the solution quality of final ASHA models is maintained or even surpassed by the implicit batch size scheduling of RASDA. With RASDA, systematic HPO is applied to a terabyte-scale scientific dataset for the first time in the literature, enabling efficient optimization of complex models on massive scientific data. The implementation of RASDA is available on https://github.com/olympiquemarcel/rasda</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02729v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2025.108042</arxiv:DOI>
      <arxiv:journal_reference>Future Generation Computer Systems, Volume 175, February 2026</arxiv:journal_reference>
      <dc:creator>Marcel Aach, Rakesh Sarma, Helmut Neukirchen, Morris Riedel, Andreas Lintermann</dc:creator>
    </item>
    <item>
      <title>Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry</title>
      <link>https://arxiv.org/abs/2510.26008</link>
      <description>arXiv:2510.26008v2 Announce Type: replace-cross 
Abstract: Modern machine learning (ML) has grown into a tightly coupled, full-stack ecosystem that combines hardware, software, network, and applications. Many users rely on cloud providers for elastic, isolated, and cost-efficient resources. Unfortunately, these platforms as a service use virtualization, which means operators have little insight into the users' workloads. This hinders resource optimizations by the operator, which is essential to ensure cost efficiency and minimize execution time. In this paper, we argue that workload knowledge is unnecessary for system-level optimization. We propose Reveal, which takes a hardware-centric approach, relying only on hardware signals - fully accessible by operators. Using low-level signals collected from the system, Reveal detects anomalies through an unsupervised learning pipeline. The pipeline is developed by analyzing over 30 popular ML models on various hardware platforms, ensuring adaptability to emerging workloads and unknown deployment patterns. Using Reveal, we successfully identified both network and system configuration issues, accelerating the DeepSeek model by 5.97%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26008v2</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziji Chen, Steven W. D. Chien, Peng Qian, Noa Zilberman</dc:creator>
    </item>
    <item>
      <title>Environmental Impact of CI/CD Pipelines</title>
      <link>https://arxiv.org/abs/2510.26413</link>
      <description>arXiv:2510.26413v2 Announce Type: replace-cross 
Abstract: CI/CD pipelines are widely used in software development, yet their environmental impact, particularly carbon and water footprints (CWF), remains largely unknown to developers, as CI service providers typically do not disclose such information. With the growing environmental impact of cloud computing, understanding the CWF of CI/CD services has become increasingly important.
  This work investigates the CWF of using GitHub Actions, focusing on open-source repositories where usage is free and unlimited for standard runners. We build upon a methodology from the Cloud Carbon Footprint framework and we use the largest dataset of workflow runs reported in the literature to date, comprising over 2.2 million workflow runs from more than 18,000 repositories.
  Our analysis reveals that the GitHub Actions ecosystem results in a substantial CWF. Our estimates for the carbon footprint in 2024 range from 150.5 MTCO2e in the most optimistic scenario to 994.9 MTCO2e in the most pessimistic scenario, while the water footprint ranges from 1,989.6 to 37,664.5 kiloliters. The most likely scenario estimates are 456.9 MTCO2e for carbon footprint and 5,738.2 kiloliters for water footprint. To provide perspective, the carbon footprint in the most likely scenario is equivalent to the carbon captured by 7,615 urban trees in a year, and the water footprint is comparable to the water consumed by an average American family over 5,053 years.
  We explore strategies to mitigate this impact, primarily by reducing wasted computational resources. Key recommendations include deploying runners in regions whose energy production has a low environmental impact such as France and the United Kingdom, implementing stricter deactivation policies for scheduled runs and aligning their execution with periods when the regional energy mix is more environmentally favorable, and reducing the size of repositories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26413v2</guid>
      <category>cs.SE</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nuno Saavedra, Alexandra Mendes, Jo\~ao F. Ferreira</dc:creator>
    </item>
    <item>
      <title>An All-Reduce Compatible Top-K Compressor for Communication-Efficient Distributed Learning</title>
      <link>https://arxiv.org/abs/2510.26709</link>
      <description>arXiv:2510.26709v2 Announce Type: replace-cross 
Abstract: Communication remains a central bottleneck in large-scale distributed machine learning, and gradient sparsification has emerged as a promising strategy to alleviate this challenge. However, existing gradient compressors face notable limitations: Rand-$K$ discards structural information and performs poorly in practice, while Top-$K$ preserves informative entries but loses the contraction property and requires costly All-Gather operations. In this paper, we propose ARC-Top-$K$, an {All-Reduce}-Compatible Top-$K$ compressor that aligns sparsity patterns across nodes using a lightweight sketch of the gradient, enabling index-free All-Reduce while preserving globally significant information. ARC-Top-$K$ is provably contractive and, when combined with momentum error feedback (EF21M), achieves linear speedup and sharper convergence rates than the original EF21M under standard assumptions. Empirically, ARC-Top-$K$ matches the accuracy of Top-$K$ while reducing wall-clock training time by up to 60.7\%, offering an efficient and scalable solution that combines the robustness of Rand-$K$ with the strong performance of Top-$K$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26709v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chuyan Chen, Chenyang Ma, Zhangxin Li, Yutong He, Yanjie Dong, Kun Yuan</dc:creator>
    </item>
    <item>
      <title>Non-Convex Over-the-Air Heterogeneous Federated Learning: A Bias-Variance Trade-off</title>
      <link>https://arxiv.org/abs/2510.26722</link>
      <description>arXiv:2510.26722v2 Announce Type: replace-cross 
Abstract: Over-the-air (OTA) federated learning (FL) has been well recognized as a scalable paradigm that exploits the waveform superposition of the wireless multiple-access channel to aggregate model updates in a single use. Existing OTA-FL designs largely enforce zero-bias model updates by either assuming \emph{homogeneous} wireless conditions (equal path loss across devices) or forcing zero-bias updates to guarantee convergence. Under \emph{heterogeneous} wireless scenarios, however, such designs are constrained by the weakest device and inflate the update variance. Moreover, prior analyses of biased OTA-FL largely address convex objectives, while most modern AI models are highly non-convex. Motivated by these gaps, we study OTA-FL with stochastic gradient descent (SGD) for general smooth non-convex objectives under wireless heterogeneity. We develop novel OTA-FL SGD updates that allow a structured, time-invariant model bias while facilitating reduced variance updates. We derive a finite-time stationarity bound (expected time average squared gradient norm) that explicitly reveals a bias-variance trade-off. To optimize this trade-off, we pose a non-convex joint OTA power-control design and develop an efficient successive convex approximation (SCA) algorithm that requires only statistical CSI at the base station. Experiments on a non-convex image classification task validate the approach: the SCA-based design accelerates convergence via an optimized bias and improves generalization over prior OTA-FL baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.26722v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Mon, 03 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Faraz Ul Abrar, Nicol\`o Michelusi</dc:creator>
    </item>
  </channel>
</rss>
