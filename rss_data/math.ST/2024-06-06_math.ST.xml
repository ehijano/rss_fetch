<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Jun 2024 01:46:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Coefficients in Semi-Regular Models</title>
      <link>https://arxiv.org/abs/2406.02646</link>
      <description>arXiv:2406.02646v2 Announce Type: new 
Abstract: Statistical learning models such as multilayer neural networks and mixed distributions are widely used, and understanding the accuracy of these models is crucial for their use. Recent advances have clarified theoretical learning accuracy in Bayesian inference, where metrics such as generalization loss and free energy are used to measure the accuracy of predictive distributions. It has become clear that the asymptotic behavior of these metrics is determined by a rational number specific to each statistical model, known as the learning coefficient (real log canonical threshold).
  The problem of determining the learning coefficient is known to be reducible to the problem of finding the normal crossing of Kullback-Leibler divergence in relation to algebraic geometry. In this context, it is crucial to perform appropriate coordinate transformations and blow-ups. This paper attempts to derive appropriate variable transformations and blow-ups from the properties of the log-likelihood ratio function. That is, instead of dealing with the Kullback-Leibler information itself, it uses the properties of the log-likelihood ratio function before taking the expectation to calculate the real log canonical threshold. This approach has not been considered in previous research.
  Using these variable transformations and blow-ups, this paper provides the exact values of the learning coefficients and their calculation methods for statistical models that meet simple conditions next to the regular conditions (referred to as semi-regular models), and as specific examples, provides the learning coefficients for semi-regular models with two parameters and for those models where the random variables take a finite number of values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02646v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Kurumadani</dc:creator>
    </item>
    <item>
      <title>Asymptotic properties of parameter estimators in Vasicek model driven by tempered fractional Brownian motion</title>
      <link>https://arxiv.org/abs/2406.02800</link>
      <description>arXiv:2406.02800v1 Announce Type: new 
Abstract: The paper focuses on the Vasicek model driven by a tempered fractional Brownian motion. We derive the asymptotic distributions of the least-squares estimators (based on continuous-time observations) for the unknown drift parameters. This work continues the investigation by Mishura and Ralchenko (Fractal and Fractional, 8(2:79), 2024), where these estimators were introduced and their strong consistency was proved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02800v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuliya Mishura, Kostiantyn Ralchenko, Olena Dehtiar</dc:creator>
    </item>
    <item>
      <title>The Bunching and Monotonicity Properties of Families of Probability Distributions</title>
      <link>https://arxiv.org/abs/2406.02894</link>
      <description>arXiv:2406.02894v1 Announce Type: new 
Abstract: Measuring the concentration of random variables is a fundamental concept in probability and statistics. Here, we explore a type of concentration measure for continuous random variables with bounded support and use it to provide a notion of stochastic order by concentration. We give an application to the Beta family of distributions, and specifically to the one-parameter subfamily with constant mean. This leads to using U.S. household income data to fit generalized Beta distributions and offers a new measure of income concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02894v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Portnoy, N. Torrado, J. J. P. Veerman</dc:creator>
    </item>
    <item>
      <title>On determinantal point processes with nonsymmetric kernels</title>
      <link>https://arxiv.org/abs/2406.03360</link>
      <description>arXiv:2406.03360v1 Announce Type: new 
Abstract: Determinantal point processes (DPPs for short) are a class of repulsive point processes. They have found some statistical applications to model spatial point pattern datasets with repulsion between close points. In the case of DPPs on finite sets, they are defined by a matrix called the DPP kernel which is usually assumed to be symmetric. While there are a few known examples of DPPs with nonsymmetric kernels, not much is known on how this affects their usual properties. In this paper, we demonstrate how to adapt the results on $P_0$ matrices to the DPP setting in order to get necessary and sufficient conditions for the well-definedness of DPPs with nonsymmetric kernels. We also generalize various common results on DPPs. We then show how to use these results to construct attractive couplings of regular DPPs with symmetric kernels in order to model spatial marked point patterns with repulsion between points of the same mark and attraction between points of different marks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03360v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Poinas Arnaud</dc:creator>
    </item>
    <item>
      <title>Computational lower bounds for multi-frequency group synchronization</title>
      <link>https://arxiv.org/abs/2406.03424</link>
      <description>arXiv:2406.03424v1 Announce Type: new 
Abstract: We consider a group synchronization problem with multiple frequencies which involves observing pairwise relative measurements of group elements on multiple frequency channels, corrupted by Gaussian noise. We study the computational phase transition in the problem of detecting whether a structured signal is present in such observations by analyzing low-degree polynomial algorithms. We show that, assuming the low-degree conjecture, in synchronization models over arbitrary finite groups as well as over the circle group $SO(2)$, a simple spectral algorithm is optimal among algorithms of runtime $\exp(\tilde{\Omega}(n^{1/3}))$ for detection from an observation including a constant number of frequencies. Combined with an upper bound for the statistical threshold shown in Perry et al., our results indicate the presence of a statistical-to-computational gap in such models with a sufficiently large number of frequencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03424v1</guid>
      <category>math.ST</category>
      <category>cs.CC</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasia Kireeva, Afonso S. Bandeira, Dmitriy Kunisky</dc:creator>
    </item>
    <item>
      <title>PriME: Privacy-aware Membership profile Estimation in networks</title>
      <link>https://arxiv.org/abs/2406.02794</link>
      <description>arXiv:2406.02794v1 Announce Type: cross 
Abstract: This paper presents a novel approach to estimating community membership probabilities for network vertices generated by the Degree Corrected Mixed Membership Stochastic Block Model while preserving individual edge privacy. Operating within the $\varepsilon$-edge local differential privacy framework, we introduce an optimal private algorithm based on a symmetric edge flip mechanism and spectral clustering for accurate estimation of vertex community memberships. We conduct a comprehensive analysis of the estimation risk and establish the optimality of our procedure by providing matching lower bounds to the minimax risk under privacy constraints. To validate our approach, we demonstrate its performance through numerical simulations and its practical application to real-world data. This work represents a significant step forward in balancing accurate community membership estimation with stringent privacy preservation in network data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02794v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhinav Chakraborty, Sayak Chatterjee, Sagnik Nandy</dc:creator>
    </item>
    <item>
      <title>Statistical inference of convex order by Wasserstein projection</title>
      <link>https://arxiv.org/abs/2406.02840</link>
      <description>arXiv:2406.02840v1 Announce Type: cross 
Abstract: Ranking distributions according to a stochastic order has wide applications in diverse areas. Although stochastic dominance has received much attention,convex order, particularly in general dimensions, has yet to be investigated from a statistical point of view. This article addresses this gap by introducing a simple statistical test for convex order based on the Wasserstein projection distance. This projection distance not only encodes whether two distributions are indeed in convex order, but also quantifies the deviation from the desired convex order and produces an optimal convex order approximation. Lipschitz stability of the backward and forward Wasserstein projection distance is proved, which leads to elegant consistency results of the estimator we employ as our test statistic. Combining these with state of the art results regarding the convergence rate of empirical distributions, we also derive upper bounds for the $p$-value and type I error our test statistic, as well as upper bounds on the type II error for an appropriate class of strict alternatives. Lastly, we provide an efficient numerical scheme for our test statistic, by way of an entropic Frank-Wolfe algorithm. Some experiments based on synthetic data sets illuminates the success of our approach empirically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02840v1</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jakwang Kim, Young-Heon Kim, Yuanlong Ruan, Andrew Warren</dc:creator>
    </item>
    <item>
      <title>Mesoscopic Bayesian Inference by Solvable Models</title>
      <link>https://arxiv.org/abs/2406.02869</link>
      <description>arXiv:2406.02869v1 Announce Type: cross 
Abstract: The rapid advancement of data science and artificial intelligence has influenced physics in numerous ways, including the application of Bayesian inference. Our group has proposed Bayesian measurement, a framework that applies Bayesian inference to measurement science and is applicable across various natural sciences. This framework enables the determination of posterior probability distributions for system parameters, model selection, and the integration of multiple measurement datasets. However, a theoretical framework to address fluctuations in these results due to finite measurement data (N) is still needed. In this paper, we suggest a mesoscopic theoretical framework for the components of Bayesian measurement-parameter estimation, model selection, and Bayesian integration-within the mesoscopic region where (N) is finite. We develop a solvable theory for linear regression with Gaussian noise, which is practical for real-world measurements and as an approximation for nonlinear models with large (N). By utilizing mesoscopic Gaussian and chi-squared distributions, we aim to analytically evaluate the three components of Bayesian measurement. Our results offer a novel approach to understanding fluctuations in Bayesian measurement outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02869v1</guid>
      <category>physics.data-an</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shun Katakami, Shuhei Kashiwamura, Kenji Nagata, Masaichiro Mizumaki, Masato Okada</dc:creator>
    </item>
    <item>
      <title>Feature learning in finite-width Bayesian deep linear networks with multiple outputs and convolutional layers</title>
      <link>https://arxiv.org/abs/2406.03260</link>
      <description>arXiv:2406.03260v1 Announce Type: cross 
Abstract: Deep linear networks have been extensively studied, as they provide simplified models of deep learning. However, little is known in the case of finite-width architectures with multiple outputs and convolutional layers. In this manuscript, we provide rigorous results for the statistics of functions implemented by the aforementioned class of networks, thus moving closer to a complete characterization of feature learning in the Bayesian setting. Our results include: (i) an exact and elementary non-asymptotic integral representation for the joint prior distribution over the outputs, given in terms of a mixture of Gaussians; (ii) an analytical formula for the posterior distribution in the case of squared error loss function (Gaussian likelihood); (iii) a quantitative description of the feature learning infinite-width regime, using large deviation theory. From a physical perspective, deep architectures with multiple outputs or convolutional layers represent different manifestations of kernel shape renormalization, and our work provides a dictionary that translates this physics intuition and terminology into rigorous Bayesian statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03260v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federico Bassetti, Marco Gherardi, Alessandro Ingrosso, Mauro Pastore, Pietro Rotondo</dc:creator>
    </item>
    <item>
      <title>Combining an experimental study with external data: study designs and identification strategies</title>
      <link>https://arxiv.org/abs/2406.03302</link>
      <description>arXiv:2406.03302v1 Announce Type: cross 
Abstract: There is increasing interest in combining information from experimental studies, including randomized and single-group trials, with information from external experimental or observational data sources. Such efforts are usually motivated by the desire to compare treatments evaluated in different studies -- for instance, through the introduction of external treatment groups -- or to estimate treatment effects with greater precision. Proposals to combine experimental studies with external data were made at least as early as the 1970s, but in recent years have come under increasing consideration by regulatory agencies involved in drug and device evaluation, particularly with the increasing availability of rich observational data. In this paper, we describe basic templates of study designs and data structures for combining information from experimental studies with external data, and use the potential (counterfactual) outcomes framework to elaborate identification strategies for potential outcome means and average treatment effects in these designs. In formalizing designs and identification strategies for combining information from experimental studies with external data, we hope to provide a conceptual foundation to support the systematic use and evaluation of such efforts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03302v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lawson Ung, Guanbo Wang, Sebastien Haneuse, Miguel A. Hernan, Issa J. Dahabreh</dc:creator>
    </item>
    <item>
      <title>Bayesian Quantile Estimation and Regression with Martingale Posteriors</title>
      <link>https://arxiv.org/abs/2406.03358</link>
      <description>arXiv:2406.03358v1 Announce Type: cross 
Abstract: Quantile estimation and regression within the Bayesian framework is challenging as the choice of likelihood and prior is not obvious. In this paper, we introduce a novel Bayesian nonparametric method for quantile estimation and regression based on the recently introduced martingale posterior (MP) framework. The core idea of the MP is that posterior sampling is equivalent to predictive imputation, which allows us to break free of the stringent likelihood-prior specification. We demonstrate that a recursive estimate of a smooth quantile function, subject to a martingale condition, is entirely sufficient for full nonparametric Bayesian inference. We term the resulting posterior distribution as the quantile martingale posterior (QMP), which arises from an implicit generative predictive distribution. Associated with the QMP is an expedient, MCMC-free and parallelizable posterior computation scheme, which can be further accelerated with an asymptotic approximation based on a Gaussian process. Furthermore, the well-known issue of monotonicity in quantile estimation is naturally alleviated through increasing rearrangement due to the connections to the Bayesian bootstrap. Finally, the QMP has a particularly tractable form that allows for comprehensive theoretical study, which forms a main focus of the work. We demonstrate the ease of posterior computation in simulations and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03358v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edwin Fong, Andrew Yiu</dc:creator>
    </item>
    <item>
      <title>Posterior and variational inference for deep neural networks with heavy-tailed weights</title>
      <link>https://arxiv.org/abs/2406.03369</link>
      <description>arXiv:2406.03369v1 Announce Type: cross 
Abstract: We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random. Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation. We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces. While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network. We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03369v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isma\"el Castillo, Paul Egels</dc:creator>
    </item>
    <item>
      <title>How many labelers do you have? A closer look at gold-standard labels</title>
      <link>https://arxiv.org/abs/2206.12041</link>
      <description>arXiv:2206.12041v2 Announce Type: replace 
Abstract: The construction of most supervised learning datasets revolves around collecting multiple labels for each instance, then aggregating the labels to form a type of "gold-standard". We question the wisdom of this pipeline by developing a (stylized) theoretical model of this process and analyzing its statistical consequences, showing how access to non-aggregated label information can make training well-calibrated models more feasible than it is with gold-standard labels. The entire story, however, is subtle, and the contrasts between aggregated and fuller label information depend on the particulars of the problem, where estimators that use aggregated information exhibit robust but slower rates of convergence, while estimators that can effectively leverage all labels converge more quickly if they have fidelity to (or can learn) the true labeling process. The theory makes several predictions for real-world datasets, including when non-aggregate labels should improve learning performance, which we test to corroborate the validity of our predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.12041v2</guid>
      <category>math.ST</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chen Cheng, Hilal Asi, John Duchi</dc:creator>
    </item>
    <item>
      <title>Dynamic Ranking and Translation Synchronization</title>
      <link>https://arxiv.org/abs/2207.01455</link>
      <description>arXiv:2207.01455v4 Announce Type: replace 
Abstract: In many applications, such as sport tournaments or recommendation systems, we have at our disposal data consisting of pairwise comparisons between a set of $n$ items (or players). The objective is to use this data to infer the latent strength of each item and/or their ranking. Existing results for this problem predominantly focus on the setting consisting of a single comparison graph $G$. However, there exist scenarios (e.g., sports tournaments) where the the pairwise comparison data evolves with time. Theoretical results for this dynamic setting are relatively limited and is the focus of this paper.
  We study an extension of the \emph{translation synchronization} problem, to the dynamic setting. In this setup, we are given a sequence of comparison graphs $(G_t)_{t\in \mathcal{T}}$, where $\mathcal{T} \subset [0,1]$ is a grid representing the time domain, and for each item $i$ and time $t\in \mathcal{T}$ there is an associated unknown strength parameter $z^*_{t,i}\in \mathbb{R}$. We aim to recover, for $t\in\mathcal{T}$, the strength vector $z^*_t=(z^*_{t,1},\dots,z^*_{t,n})$ from noisy measurements of $z^*_{t,i}-z^*_{t,j}$, where $\{i,j\}$ is an edge in $G_t$. Assuming that $z^*_t$ evolves smoothly in $t$, we propose two estimators -- one based on a smoothness-penalized least squares approach and the other based on projection onto the low frequency eigenspace of a suitable smoothness operator. For both estimators, we provide finite sample bounds for the $\ell_2$ estimation error under the assumption that $G_t$ is connected for all $t\in \mathcal{T}$, thus proving the consistency of the proposed methods in terms of the grid size $|\mathcal{T}|$. We complement our theoretical findings with experiments on synthetic and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.01455v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Information and Inference: A Journal of the IMA, 12(3), 2023, 2224-2266</arxiv:journal_reference>
      <dc:creator>Ernesto Araya, Eglantine Karl\'e, Hemant Tyagi</dc:creator>
    </item>
    <item>
      <title>Dimension-free uniform concentration bound for logistic regression</title>
      <link>https://arxiv.org/abs/2405.18055</link>
      <description>arXiv:2405.18055v2 Announce Type: replace 
Abstract: We provide a novel dimension-free uniform concentration bound for the empirical risk function of constrained logistic regression. Our bound yields a milder sufficient condition for a uniform law of large numbers than conditions derived by the Rademacher complexity argument and McDiarmid's inequality. The derivation is based on the PAC-Bayes approach with second-order expansion and Rademacher-complexity-based bounds for the residual term of the expansion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18055v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shogo Nakakita</dc:creator>
    </item>
    <item>
      <title>Profiled Transfer Learning for High Dimensional Linear Model</title>
      <link>https://arxiv.org/abs/2406.00701</link>
      <description>arXiv:2406.00701v2 Announce Type: replace 
Abstract: We develop here a novel transfer learning methodology called Profiled Transfer Learning (PTL). The method is based on the \textit{approximate-linear} assumption between the source and target parameters. Compared with the commonly assumed \textit{vanishing-difference} assumption and \textit{low-rank} assumption in the literature, the \textit{approximate-linear} assumption is more flexible and less stringent. Specifically, the PTL estimator is constructed by two major steps. Firstly, we regress the response on the transferred feature, leading to the profiled responses. Subsequently, we learn the regression relationship between profiled responses and the covariates on the target data. The final estimator is then assembled based on the \textit{approximate-linear} relationship. To theoretically support the PTL estimator, we derive the non-asymptotic upper bound and minimax lower bound. We find that the PTL estimator is minimax optimal under appropriate regularity conditions. Extensive simulation studies are presented to demonstrate the finite sample performance of the new method. A real data example about sentence prediction is also presented with very encouraging results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00701v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqian Lin, Junlong Zhao, Fang Wang, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Almost exact recovery in noisy semi-supervised learning</title>
      <link>https://arxiv.org/abs/2007.14717</link>
      <description>arXiv:2007.14717v4 Announce Type: replace-cross 
Abstract: Graph-based semi-supervised learning methods combine the graph structure and labeled data to classify unlabeled data. In this work, we study the effect of a noisy oracle on classification. In particular, we derive the Maximum A Posteriori (MAP) estimator for clustering a Degree Corrected Stochastic Block Model (DC-SBM) when a noisy oracle reveals a fraction of the labels. We then propose an algorithm derived from a continuous relaxation of the MAP, and we establish its consistency. Numerical experiments show that our approach achieves promising performance on synthetic and real data sets, even in the case of very noisy labeled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.14717v4</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Avrachenkov, Maximilien Dreveton</dc:creator>
    </item>
    <item>
      <title>Weak limits of entropy regularized Optimal Transport; potentials, plans and divergences</title>
      <link>https://arxiv.org/abs/2207.07427</link>
      <description>arXiv:2207.07427v2 Announce Type: replace-cross 
Abstract: This work deals with the asymptotic distribution of both potentials and couplings of entropic regularized optimal transport for compactly supported probabilities in $\R^d$. We first provide the central limit theorem of the Sinkhorn potentials -- the solutions of the dual problem -- as a Gaussian process in $\Cs$. Then we obtain the weak limits of the couplings -- the solutions of the primal problem -- evaluated on integrable functions, proving a conjecture of \cite{ChaosDecom}. In both cases, their limit is a real Gaussian random variable. Finally we consider the weak limit of the entropic Sinkhorn divergence under both assumptions $H_0:\ {\rm P}={\rm Q}$ or $H_1:\ {\rm P}\neq{\rm Q}$. Under $H_0$ the limit is a quadratic form applied to a Gaussian process in a Sobolev space, while under $H_1$, the limit is Gaussian. We provide also a different characterisation of the limit under $H_0$ in terms of an infinite sum of an i.i.d. sequence of standard Gaussian random variables. Such results enable statistical inference based on entropic regularized optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.07427v2</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Gonzalez-Sanz, Jean-Michel Loubes, Jonathan Niles-Weed</dc:creator>
    </item>
    <item>
      <title>Improved Convergence of Score-Based Diffusion Models via Prediction-Correction</title>
      <link>https://arxiv.org/abs/2305.14164</link>
      <description>arXiv:2305.14164v3 Announce Type: replace-cross 
Abstract: Score-based generative models (SGMs) are powerful tools to sample from complex data distributions. Their underlying idea is to (i) run a forward process for time $T_1$ by adding noise to the data, (ii) estimate its score function, and (iii) use such estimate to run a reverse process. As the reverse process is initialized with the stationary distribution of the forward one, the existing analysis paradigm requires $T_1\to\infty$. This is however problematic: from a theoretical viewpoint, for a given precision of the score approximation, the convergence guarantee fails as $T_1$ diverges; from a practical viewpoint, a large $T_1$ increases computational costs and leads to error propagation. This paper addresses the issue by considering a version of the popular predictor-corrector scheme: after running the forward process, we first estimate the final distribution via an inexact Langevin dynamics and then revert the process. Our key technical contribution is to provide convergence guarantees which require to run the forward process only for a fixed finite time $T_1$. Our bounds exhibit a mild logarithmic dependence on the input dimension and the subgaussian norm of the target distribution, have minimal assumptions on the data, and require only to control the $L^2$ loss on the score approximation, which is the quantity minimized in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.14164v3</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Pedrotti, Jan Maas, Marco Mondelli</dc:creator>
    </item>
    <item>
      <title>ARK: Robust Knockoffs Inference with Coupling</title>
      <link>https://arxiv.org/abs/2307.04400</link>
      <description>arXiv:2307.04400v2 Announce Type: replace-cross 
Abstract: We investigate the robustness of the model-X knockoffs framework with respect to the misspecified or estimated feature distribution. We achieve such a goal by theoretically studying the feature selection performance of a practically implemented knockoffs algorithm, which we name as the approximate knockoffs (ARK) procedure, under the measures of the false discovery rate (FDR) and $k$-familywise error rate ($k$-FWER). The approximate knockoffs procedure differs from the model-X knockoffs procedure only in that the former uses the misspecified or estimated feature distribution. A key technique in our theoretical analyses is to couple the approximate knockoffs procedure with the model-X knockoffs procedure so that random variables in these two procedures can be close in realizations. We prove that if such coupled model-X knockoffs procedure exists, the approximate knockoffs procedure can achieve the asymptotic FDR or $k$-FWER control at the target level. We showcase three specific constructions of such coupled model-X knockoff variables, verifying their existence and justifying the robustness of the model-X knockoffs framework. Additionally, we formally connect our concept of knockoff variable coupling to a type of Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04400v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingying Fan, Lan Gao, Jinchi Lv</dc:creator>
    </item>
  </channel>
</rss>
