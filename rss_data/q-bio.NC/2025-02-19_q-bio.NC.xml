<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Feb 2025 05:01:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Explainable AI-Driven Neural Activity Analysis in Parkinsonian Rats under Electrical Stimulation</title>
      <link>https://arxiv.org/abs/2502.12471</link>
      <description>arXiv:2502.12471v1 Announce Type: new 
Abstract: Parkinson's disease (PD) is a neurodegenerative disorder characterized by motor dysfunction and abnormal neural oscillations. These symptoms can be modulated through electrical stimulation. Traditional neural activity analysis in PD has typically relied on statistical methods, which often introduce bias owing to the need for expert-driven feature extraction. To address this limitation, we explore an explainable artificial intelligence (XAI) approach to analyze neural activity in Parkinsonian rats receiving electrical stimulation. Electrocorticogram (ECoG) signals were collected before and after electrical stimulation using graphene-based electrodes that enable less-invasive monitoring and stimulation in PD. EEGNet, a convolutional neural network, classified these ECoG signals into pre- and post-stimulation states. We applied layer-wise relevance propagation, an XAI technique, to identify key neural inputs contributing to the model's decisions, incorporating the spatial electrode information matched to the cortex map. The XAI analysis highlighted area-specific importance in beta and gamma frequency bands, which could not be detected through mean comparison analyses relying on feature extraction. These findings demonstrate the potential of XAI in analyzing neural dynamics in neurodegenerative disorders such as PD, suggesting that the integration of graphene-based electrodes with advanced deep learning models offers a promising solution for real-time PD monitoring and therapy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12471v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jibum Kim, Hanseul Choi, Gaeun Kim, Sunggu Yang, Eunha Baeg, Donggue Kim, Seongwon Jin, Sangwon Byun</dc:creator>
    </item>
    <item>
      <title>Neuro-oscillatory models of cortical speech processing</title>
      <link>https://arxiv.org/abs/2502.12935</link>
      <description>arXiv:2502.12935v1 Announce Type: new 
Abstract: In this review, we examine computational models that explore the role of neural oscillations in speech perception, spanning from early auditory processing to higher cognitive stages. We focus on models that use rhythmic brain activities, such as gamma, theta, and delta oscillations, to encode phonemes, segment speech into syllables and words, and integrate linguistic elements to infer meaning. We analyze the mechanisms underlying these models, their biological plausibility, and their potential applications in processing and understanding speech in real time, a computational feature that is achieved by the human brain but not yet implemented in speech recognition models. Real-time processing enables dynamic adaptation to incoming speech, allowing systems to handle the rapid and continuous flow of auditory information required for effective communication, interactive applications, and accurate speech recognition in a variety of real-world settings. While significant progress has been made in modeling the neural basis of speech perception, challenges remain, particularly in accounting for the complexity of semantic processing and the integration of contextual influences. Moreover, the high computational demands of biologically realistic models pose practical difficulties for their implementation and analysis. Despite these limitations, these models provide valuable insights into the neural mechanisms of speech perception. We conclude by identifying current limitations, proposing future research directions, and suggesting how these models can be further developed to achieve a more comprehensive understanding of speech processing in the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12935v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olesia Dogonasheva, Denis Zakharov, Anne-Lise Giraud, Boris Gutkin</dc:creator>
    </item>
    <item>
      <title>Mind the Gap: Aligning the Brain with Language Models Requires a Nonlinear and Multimodal Approach</title>
      <link>https://arxiv.org/abs/2502.12771</link>
      <description>arXiv:2502.12771v1 Announce Type: cross 
Abstract: Self-supervised language and audio models effectively predict brain responses to speech. However, traditional prediction models rely on linear mappings from unimodal features, despite the complex integration of auditory signals with linguistic and semantic information across widespread brain networks during speech comprehension. Here, we introduce a nonlinear, multimodal prediction model that combines audio and linguistic features from pre-trained models (e.g., LLAMA, Whisper). Our approach achieves a 17.2% and 17.9% improvement in prediction performance (unnormalized and normalized correlation) over traditional unimodal linear models, as well as a 7.7% and 14.4% improvement, respectively, over prior state-of-the-art models. These improvements represent a major step towards future robust in-silico testing and improved decoding performance. They also reveal how auditory and semantic information are fused in motor, somatosensory, and higher-level semantic regions, aligning with existing neurolinguistic theories. Overall, our work highlights the often neglected potential of nonlinear and multimodal approaches to brain modeling, paving the way for future studies to embrace these strategies in naturalistic neurolinguistics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12771v1</guid>
      <category>cs.CL</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Danny Dongyeop Han, Yunju Cho, Jiook Cha, Jay-Yoon Lee</dc:creator>
    </item>
    <item>
      <title>Characterizing the Interaction of Cultural Evolution Mechanisms in Experimental Social Networks</title>
      <link>https://arxiv.org/abs/2502.12847</link>
      <description>arXiv:2502.12847v1 Announce Type: cross 
Abstract: Understanding how cognitive and social mechanisms shape the evolution of complex artifacts such as songs is central to cultural evolution research. Social network topology (what artifacts are available?), selection (which are chosen?), and reproduction (how are they copied?) have all been proposed as key influencing factors. However, prior research has rarely studied them together due to methodological challenges. We address this gap through a controlled naturalistic paradigm whereby participants (N=2,404) are placed in networks and are asked to iteratively choose and sing back melodies from their neighbors. We show that this setting yields melodies that are more complex and more pleasant than those found in the more-studied linear transmission setting, and exhibits robust differences across topologies. Crucially, these differences are diminished when selection or reproduction bias are eliminated, suggesting an interaction between mechanisms. These findings shed light on the interplay of mechanisms underlying the evolution of cultural artifacts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12847v1</guid>
      <category>cs.SI</category>
      <category>q-bio.NC</category>
      <category>q-bio.PE</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raja Marjieh, Manuel Anglada-Tort, Thomas L. Griffiths, Nori Jacoby</dc:creator>
    </item>
    <item>
      <title>Time-series attribution maps with regularized contrastive learning</title>
      <link>https://arxiv.org/abs/2502.12977</link>
      <description>arXiv:2502.12977v1 Announce Type: cross 
Abstract: Gradient-based attribution methods aim to explain decisions of deep learning models but so far lack identifiability guarantees. Here, we propose a method to generate attribution maps with identifiability guarantees by developing a regularized contrastive learning algorithm trained on time-series data plus a new attribution method called Inverted Neuron Gradient (collectively named xCEBRA). We show theoretically that xCEBRA has favorable properties for identifying the Jacobian matrix of the data generating process. Empirically, we demonstrate robust approximation of zero vs. non-zero entries in the ground-truth attribution map on synthetic datasets, and significant improvements across previous attribution methods based on feature ablation, Shapley values, and other gradient-based methods. Our work constitutes a first example of identifiable inference of time-series attribution maps and opens avenues to a better understanding of time-series data, such as for neural dynamics and decision-processes within neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12977v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>The 28th International Conference on Artificial Intelligence and Statistics 2025</arxiv:journal_reference>
      <dc:creator>Steffen Schneider, Rodrigo Gonz\'alez Laiz, Anastasiia Filippova, Markus Frey, Mackenzie Weygandt Mathis</dc:creator>
    </item>
    <item>
      <title>Emergence and maintenance of modularity in neural networks with Hebbian and anti-Hebbian inhibitory STDP</title>
      <link>https://arxiv.org/abs/2405.18587</link>
      <description>arXiv:2405.18587v3 Announce Type: replace 
Abstract: The modular and hierarchical organization of the brain is believed to support the coexistence of segregated (specialization) and integrated (binding) information processes. A relevant question is yet to understand how such architecture naturally emerges and is sustained over time, given the plastic nature of the brain's wiring. Following evidences that the sensory cortices organize into assemblies under selective stimuli, it has been shown that stable neuronal assemblies can emerge due to targeted stimulation, embedding various forms of synaptic plasticity in presence of homeostatic and/or control mechanisms. Here, we show that simple spike-timing-dependent plasticity (STDP) rules, based only on pre- and post-synaptic spike times, can also lead to the stable encoding of memories in the absence of any control mechanism. We develop a model of spiking neurons, trained by stimuli targeting different subpopulations. The model satisfies some biologically plausible features: (i) it contains excitatory and inhibitory neurons with Hebbian and anti-Hebbian STDP; (ii) neither the neuronal activity nor the synaptic weights are frozen after the learning phase. Instead, the neurons are allowed to fire spontaneously while synaptic plasticity remains active. We find that only the combination of two inhibitory STDP subpopulations allows for the formation of stable modules in the network, with each subpopulation playing a distinctive role. The Hebbian subpopulation controls for the firing activity, while the anti-Hebbian neurons promote pattern selectivity. After the learning phase, the network settles into an asynchronous irregular resting-state. This post-learning activity is associated with spontaneous memory recalls which turn out to be fundamental for the long-term consolidation of the learned memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18587v3</guid>
      <category>q-bio.NC</category>
      <category>nlin.AO</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rapha\"el Bergoin, Alessandro Torcini, Gustavo Deco, Mathias Quoy, Gorka Zamora-L\'opez</dc:creator>
    </item>
    <item>
      <title>From Thought to Action: How a Hierarchy of Neural Dynamics Supports Language Production</title>
      <link>https://arxiv.org/abs/2502.07429</link>
      <description>arXiv:2502.07429v2 Announce Type: replace 
Abstract: Humans effortlessly communicate their thoughts through intricate sequences of motor actions. Yet, the neural processes that coordinate language production remain largely unknown, in part because speech artifacts limit the use of neuroimaging. To elucidate the unfolding of language production in the brain, we investigate with magnetoencephalography (MEG) and electroencephalography (EEG) the neurophysiological activity of 35 skilled typists, while they typed sentences on a keyboard. This approach confirms the hierarchical predictions of linguistic theories: the neural activity preceding the production of each word is marked by the sequential rise and fall of context-, word-, syllable-, and letter-level representations. Remarkably, each of these neural representations is maintained over long time periods within each level of the language hierarchy. This phenomenon results in a superposition of successive representations that is supported by a hierarchy of dynamic neural codes. Overall, these findings provide a precise computational breakdown of the neural dynamics that coordinate the production of language in the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07429v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingfang Zhang, Jarod L\'evy, St\'ephane d'Ascoli, J\'er\'emy Rapin, F. -Xavier Alario, Pierre Bourdillon, Svetlana Pinet, Jean-R\'emi King</dc:creator>
    </item>
    <item>
      <title>Neuron Platonic Intrinsic Representation From Dynamics Using Contrastive Learning</title>
      <link>https://arxiv.org/abs/2502.10425</link>
      <description>arXiv:2502.10425v2 Announce Type: replace 
Abstract: The Platonic Representation Hypothesis suggests a universal, modality-independent reality representation behind different data modalities. Inspired by this, we view each neuron as a system and detect its multi-segment activity data under various peripheral conditions. We assume there's a time-invariant representation for the same neuron, reflecting its intrinsic properties like molecular profiles, location, and morphology. The goal of obtaining these intrinsic neuronal representations has two criteria: (I) segments from the same neuron should have more similar representations than those from different neurons; (II) the representations must generalize well to out-of-domain data. To meet these, we propose the NeurPIR (Neuron Platonic Intrinsic Representation) framework. It uses contrastive learning, with segments from the same neuron as positive pairs and those from different neurons as negative pairs. In implementation, we use VICReg, which focuses on positive pairs and separates dissimilar samples via regularization. We tested our method on Izhikevich model-simulated neuronal population dynamics data. The results accurately identified neuron types based on preset hyperparameters. We also applied it to two real-world neuron dynamics datasets with neuron type annotations from spatial transcriptomics and neuron locations. Our model's learned representations accurately predicted neuron types and locations and were robust on out-of-domain data (from unseen animals). This shows the potential of our approach for understanding neuronal systems and future neuroscience research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10425v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Wu, Can Liao, Zizhen Deng, Zhengrui Guo, Jinzhuo Wang</dc:creator>
    </item>
    <item>
      <title>Towards Homogeneous Lexical Tone Decoding from Heterogeneous Intracranial Recordings</title>
      <link>https://arxiv.org/abs/2410.12866</link>
      <description>arXiv:2410.12866v2 Announce Type: replace-cross 
Abstract: Recent advancements in brain-computer interfaces (BCIs) have enabled the decoding of lexical tones from intracranial recordings, offering the potential to restore the communication abilities of speech-impaired tonal language speakers. However, data heterogeneity induced by both physiological and instrumental factors poses a significant challenge for unified invasive brain tone decoding. Traditional subject-specific models, which operate under a heterogeneous decoding paradigm, fail to capture generalized neural representations and cannot effectively leverage data across subjects. To address these limitations, we introduce Homogeneity-Heterogeneity Disentangled Learning for neural Representations (H2DiLR), a novel framework that disentangles and learns both the homogeneity and heterogeneity from intracranial recordings across multiple subjects. To evaluate H2DiLR, we collected stereoelectroencephalography (sEEG) data from multiple participants reading Mandarin materials comprising 407 syllables, representing nearly all Mandarin characters. Extensive experiments demonstrate that H2DiLR, as a unified decoding paradigm, significantly outperforms the conventional heterogeneous decoding approach. Furthermore, we empirically confirm that H2DiLR effectively captures both homogeneity and heterogeneity during neural representation learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12866v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>q-bio.NC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Wu, Siyuan Li, Chen Feng, Lu Cao, Yue Zhang, Jie Yang, Mohamad Sawan</dc:creator>
    </item>
  </channel>
</rss>
