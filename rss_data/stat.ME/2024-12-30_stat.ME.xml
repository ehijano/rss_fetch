<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 03:20:51 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Trustworthy assessment of heterogeneous treatment effect estimator</title>
      <link>https://arxiv.org/abs/2412.18803</link>
      <description>arXiv:2412.18803v1 Announce Type: new 
Abstract: Accurate heterogeneous treatment effect (HTE) estimation is essential for personalized recommendations, making it important to evaluate and compare HTE estimators. Traditional assessment methods are inapplicable due to missing counterfactuals. Current HTE evaluation methods rely on additional estimation or matching on test data, often ignoring the uncertainty introduced and potentially leading to incorrect conclusions. We propose incorporating uncertainty quantification into HTE estimator comparisons. In addition, we suggest shifting the focus to the estimation and inference of the relative error between methods rather than their absolute errors. Methodology-wise, we develop a relative error estimator based on the efficient influence function and establish its asymptotic distribution for inference. Compared to absolute error-based methods, the relative error estimator (1) is less sensitive to the error of nuisance function estimators, satisfying a "global double robustness" property, and (2) its confidence intervals are often narrower, making it more powerful for determining the more accurate HTE estimator. Through extensive empirical study of the ACIC challenge benchmark datasets, we show that the relative error-based method more effectively identifies the better HTE estimator with statistical confidence, even with a moderately large test dataset or inaccurate nuisance estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18803v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Robust functional PCA for density data</title>
      <link>https://arxiv.org/abs/2412.19004</link>
      <description>arXiv:2412.19004v1 Announce Type: new 
Abstract: This paper introduces a robust approach to functional principal component analysis (FPCA) for compositional data, particularly density functions. While recent papers have studied density data within the Bayes space framework, there has been limited focus on developing robust methods to effectively handle anomalous observations and large noise. To address this, we extend the Mahalanobis distance concept to Bayes spaces, proposing its regularized version that accounts for the constraints inherent in density data. Based on this extension, we introduce a new method, robust density principal component analysis (RDPCA), for more accurate estimation of functional principal components in the presence of outliers. The method's performance is validated through simulations and real-world applications, showing its ability to improve covariance estimation and principal component analysis compared to traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19004v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Oguamalam, Peter Filzmoser, Karel Hron, Alessandra Menafoglio, Una Radoji\v{c}i\'c</dc:creator>
    </item>
    <item>
      <title>Dynamic networks clustering via mirror distance</title>
      <link>https://arxiv.org/abs/2412.19012</link>
      <description>arXiv:2412.19012v1 Announce Type: new 
Abstract: The classification of different patterns of network evolution, for example in brain connectomes or social networks, is a key problem in network inference and modern data science. Building on the notion of a network's Euclidean mirror, which captures its evolution as a curve in Euclidean space, we develop the Dynamic Network Clustering through Mirror Distance (DNCMD), an algorithm for clustering dynamic networks based on a distance measure between their associated mirrors. We provide theoretical guarantees for DNCMD to achieve exact recovery of distinct evolutionary patterns for latent position random networks both when underlying vertex features change deterministically and when they follow a stochastic process. We validate our theoretical results through numerical simulations and demonstrate the application of DNCMD to understand edge functions in Drosophila larval connectome data, as well as to analyze temporal patterns in dynamic trade networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19012v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Avanti Athreya, Marta Zlatic, Michael Clayton, Carey E. Priebe</dc:creator>
    </item>
    <item>
      <title>Functional structural equation modeling with latent variables</title>
      <link>https://arxiv.org/abs/2412.19242</link>
      <description>arXiv:2412.19242v1 Announce Type: new 
Abstract: Handling latent variables in Structural Equation Models (SEMs) in a case where both the latent variables and their corresponding indicators in the measurement error part of the model are random curves presents significant challenges, especially with sparse data. In this paper, we develop a novel family of Functional Structural Equation Models (FSEMs) that incorporate latent variables modeled as Gaussian Processes (GPs). The introduced FSEMs are built upon functional regression models having response variables modeled as underlying GPs. The model flexibly adapts to cases when the random curves' realizations are observed only over a sparse subset of the domain, and the inferential framework is based on a restricted maximum likelihood approach. The advantage of this framework lies in its ability and flexibility in handling various data scenarios, including regularly and irregularly spaced points and thus missing data. To extract smooth estimates for the functional parameters, we employ a penalized likelihood approach that selects the smoothing parameters using a cross-validation method. We evaluate the performance of the proposed model using simulation studies and a real data example, which suggests that our model performs well in practice. The uncertainty associated with the estimates of the functional coefficients is also assessed by constructing confidence regions for each estimate. The goodness of fit indices that are commonly used to evaluate the fit of SEMs are developed for the FSEMs introduced in this paper. Overall, the proposed method is a promising approach for modeling functional data in SEMs with functional latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19242v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Asgari, Valeria Vitelli, Uta Sailer</dc:creator>
    </item>
    <item>
      <title>Network double autoregression</title>
      <link>https://arxiv.org/abs/2412.19251</link>
      <description>arXiv:2412.19251v1 Announce Type: new 
Abstract: Modeling high-dimensional time series with simple structures is a challenging problem. This paper proposes a network double autoregression (NDAR) model, which combines the advantages of network structure and the double autoregression (DAR) model, to handle high-dimensional, conditionally heteroscedastic, and network-structured data within a simple framework. The parameters of the model are estimated using quasi-maximum likelihood estimation, and the asymptotic properties of the estimators are derived. The selection of the model's lag order will be based on the Bayesian information criterion. Finite-sample simulations show that the proposed model performs well even with moderate time dimensions and network sizes. Finally, the model is applied to analyze three different categories of stock data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19251v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tingting Li, Hao Wang</dc:creator>
    </item>
    <item>
      <title>Multimodal Symmetric Circular Distributions Based on Nonnegative Trigonometric Sums and a Likelihood Ratio Test for Reflective Symmetry</title>
      <link>https://arxiv.org/abs/2412.19501</link>
      <description>arXiv:2412.19501v1 Announce Type: new 
Abstract: Fern\'andez-Dur\'an (2004) developed a family of circular distributions based on nonnegative trigonometric sums (NNTS) which is flexible for modeling datasets exhibiting multimodality and asymmetry. Many datasets involving angles in the natural sciences, such as animal movement in biology, are expected to exhibit reflective symmetry with respect to a central angle (axis) of symmetry. Testing for symmetry in the underlying circular density from which these angles are generated is crucial. Additionally, such densities often display multimodality. This paper identifies the conditions under which NNTS distributions are reflective symmetric and develops a likelihood ratio test for reflective symmetry. The proposed methodology is demonstrated through applications to simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19501v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Jos\'e Fern\'andez-Dur\'an, Mar\'ia Mercedes Gregorio-Dom\'inguez</dc:creator>
    </item>
    <item>
      <title>Generalized Grade-of-Membership Estimation for High-dimensional Locally Dependent Data</title>
      <link>https://arxiv.org/abs/2412.19796</link>
      <description>arXiv:2412.19796v1 Announce Type: new 
Abstract: This work focuses on the mixed membership models for multivariate categorical data widely used for analyzing survey responses and population genetics data. These grade of membership (GoM) models offer rich modeling power but present significant estimation challenges for high-dimensional polytomous data. Popular existing approaches, such as Bayesian MCMC inference, are not scalable and lack theoretical guarantees in high-dimensional settings. To address this, we first observe that data from this model can be reformulated as a three-way (quasi-)tensor, with many subjects responding to many items with varying numbers of categories. We introduce a novel and simple approach that flattens the three-way quasi-tensor into a "fat" matrix, and then perform a singular value decomposition of it to estimate parameters by exploiting the singular subspace geometry. Our fast spectral method can accommodate a broad range of data distributions with arbitrarily locally dependent noise, which we formalize as the generalized-GoM models. We establish finite-sample entrywise error bounds for the generalized-GoM model parameters. This is supported by a new sharp two-to-infinity singular subspace perturbation theory for locally dependent and flexibly distributed noise, a contribution of independent interest. Simulations and applications to data in political surveys, population genetics, and single-cell sequencing demonstrate our method's superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19796v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Chen, Chengzhu Huang, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>An efficient search-and-score algorithm for ancestral graphs using multivariate information scores</title>
      <link>https://arxiv.org/abs/2412.17508</link>
      <description>arXiv:2412.17508v1 Announce Type: cross 
Abstract: We propose a greedy search-and-score algorithm for ancestral graphs, which include directed as well as bidirected edges, originating from unobserved latent variables. The normalized likelihood score of ancestral graphs is estimated in terms of multivariate information over relevant ``ac-connected subsets'' of vertices, C, that are connected through collider paths confined to the ancestor set of C. For computational efficiency, the proposed two-step algorithm relies on local information scores limited to the close surrounding vertices of each node (step 1) and edge (step 2). This computational strategy, although restricted to information contributions from ac-connected subsets containing up to two-collider paths, is shown to outperform state-of-the-art causal discovery methods on challenging benchmark datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17508v1</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nikita Lagrange, Herve Isambert</dc:creator>
    </item>
    <item>
      <title>Empirical likelihood for Fr\'echet means on open books</title>
      <link>https://arxiv.org/abs/2412.18818</link>
      <description>arXiv:2412.18818v1 Announce Type: cross 
Abstract: Empirical Likelihood (EL) is a type of nonparametric likelihood that is useful in many statistical inference problems, including confidence region construction and $k$-sample problems. It enjoys some remarkable theoretical properties, notably Bartlett correctability. One area where EL has potential but is under-developed is in non-Euclidean statistics where the Fr\'echet mean is the population characteristic of interest. Only recently has a general EL method been proposed for smooth manifolds. In this work, we continue progress in this direction and develop an EL method for the Fr\'echet mean on a stratified metric space that is not a manifold: the open book, obtained by gluing copies of a Euclidean space along their common boundaries. The structure of an open book captures the essential behaviour of the Fr\'echet mean around certain singular regions of more general stratified spaces for complex data objects, and relates intimately to the local geometry of non-binary trees in the well-studied phylogenetic treespace. We derive a version of Wilks' theorem for the EL statistic, and elucidate on the delicate interplay between the asymptotic distribution and topology of the neighbourhood around the population Fr\'echet mean. We then present a bootstrap calibration of the EL, which proves that under mild conditions, bootstrap calibration of EL confidence regions have coverage error of size $O(n^{-2})$ rather than $O(n^{-1})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18818v1</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karthik Bharath, Huiling Le, Andrew T A Wood, Xi Yan</dc:creator>
    </item>
    <item>
      <title>Priors for second-order unbiased Bayes estimators</title>
      <link>https://arxiv.org/abs/2412.19187</link>
      <description>arXiv:2412.19187v1 Announce Type: cross 
Abstract: Asymptotically unbiased priors, introduced by Hartigan (1965), are designed to achieve second-order unbiasedness of Bayes estimators. This paper extends Hartigan's framework to non-i.i.d. models by deriving a system of partial differential equations that characterizes asymptotically unbiased priors. Furthermore, we establish a necessary and sufficient condition for the existence of such priors and propose a simple procedure for constructing them.
  The proposed method is applied to several examples, including the linear regression model and the nested error regression (NER) model (also known as the random effects model). Simulation studies evaluate the frequentist properties of the Bayes estimator under the asymptotically unbiased prior for the NER model, highlighting its effectiveness in small-sample settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19187v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mana Sakai, Takeru Matsuda, Tatsuya Kubokawa</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Market Research: A Data-augmentation Approach</title>
      <link>https://arxiv.org/abs/2412.19363</link>
      <description>arXiv:2412.19363v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have transformed artificial intelligence by excelling in complex natural language processing tasks. Their ability to generate human-like text has opened new possibilities for market research, particularly in conjoint analysis, where understanding consumer preferences is essential but often resource-intensive. Traditional survey-based methods face limitations in scalability and cost, making LLM-generated data a promising alternative. However, while LLMs have the potential to simulate real consumer behavior, recent studies highlight a significant gap between LLM-generated and human data, with biases introduced when substituting between the two. In this paper, we address this gap by proposing a novel statistical data augmentation approach that efficiently integrates LLM-generated data with real data in conjoint analysis. Our method leverages transfer learning principles to debias the LLM-generated data using a small amount of human data. This results in statistically robust estimators with consistent and asymptotically normal properties, in contrast to naive approaches that simply substitute human data with LLM-generated data, which can exacerbate bias. We validate our framework through an empirical study on COVID-19 vaccine preferences, demonstrating its superior ability to reduce estimation error and save data and costs by 24.9\% to 79.8\%. In contrast, naive approaches fail to save data due to the inherent biases in LLM-generated data compared to human data. Another empirical study on sports car choices validates the robustness of our results. Our findings suggest that while LLM-generated data is not a direct substitute for human responses, it can serve as a valuable complement when used within a robust statistical framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19363v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengxin Wang (Naveen Jindal School of Management, The University of Texas at Dallas), Dennis J. Zhang (Olin School of Business, Washington University in St. Louis), Heng Zhang (W. P. Carey School of Business, Arizona State University)</dc:creator>
    </item>
    <item>
      <title>LASER: A new method for locally adaptive nonparametric regression</title>
      <link>https://arxiv.org/abs/2412.19802</link>
      <description>arXiv:2412.19802v1 Announce Type: cross 
Abstract: In this article, we introduce \textsf{LASER} (Locally Adaptive Smoothing Estimator for Regression), a computationally efficient locally adaptive nonparametric regression method that performs variable bandwidth local polynomial regression. We prove that it adapts (near-)optimally to the local H\"{o}lder exponent of the underlying regression function \texttt{simultaneously} at all points in its domain. Furthermore, we show that there is a single ideal choice of a global tuning parameter under which the above mentioned local adaptivity holds. Despite the vast literature on nonparametric regression, instances of practicable methods with provable guarantees of such a strong notion of local adaptivity are rare. The proposed method achieves excellent performance across a broad range of numerical experiments in comparison to popular alternative locally adaptive methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19802v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sabyasachi Chatterjee, Subhajit Goswami, Soumendu Sundar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Fast Nonseparable Gaussian Stochastic Process with Application to Methylation Level Interpolation</title>
      <link>https://arxiv.org/abs/1711.11501</link>
      <description>arXiv:1711.11501v4 Announce Type: replace 
Abstract: Gaussian stochastic process (GaSP) has been widely used as a prior over functions due to its flexibility and tractability in modeling. However, the computational cost in evaluating the likelihood is $O(n^3)$, where $n$ is the number of observed points in the process, as it requires to invert the covariance matrix. This bottleneck prevents GaSP being widely used in large-scale data. We propose a general class of nonseparable GaSP models for multiple functional observations with a fast and exact algorithm, in which the computation is linear ($O(n)$) and exact, requiring no approximation to compute the likelihood. We show that the commonly used linear regression and separable models are special cases of the proposed nonseparable GaSP model. Through the study of an epigenetic application, the proposed nonseparable GaSP model can accurately predict the genome-wide DNA methylation levels and compares favorably to alternative methods, such as linear regression, random forest and localized Kriging method. The algorithm for fast computation is implemented in the ${\tt FastGaSP}$ R package on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:1711.11501v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2019.1665534</arxiv:DOI>
      <arxiv:journal_reference>Journal of Computational and Graphical Statistics, 29:2, 250-260 (2020)</arxiv:journal_reference>
      <dc:creator>Mengyang Gu, Yanxun Xu</dc:creator>
    </item>
    <item>
      <title>Joint Dynamic Models and Statistical Inference for Recurrent Competing Risks, Longitudinal Marker, and Health Status</title>
      <link>https://arxiv.org/abs/2103.12903</link>
      <description>arXiv:2103.12903v3 Announce Type: replace 
Abstract: Consider a subject or unit in a longitudinal biomedical, public health, engineering, economic, or social science study which is being monitored over a possibly random duration. Over time this unit experiences competing recurrent events and a longitudinal marker transitions over a discrete state-space. In addition, its ``health or performance'' status also transitions over a discrete state-space with some states possibly absorbing states. A vector of covariates will also be associated with this unit. If there are absorbing states, of interest for this unit is its time-to-absorption of its health status process, which could be viewed as the unit's lifetime. Aside from being affected by its covariate vector, there could be associations among the recurrent competing risks processes, the longitudinal marker process, and the health status process in the sense that the time-evolution of each process is associated with the other processes. To obtain more realistic models and enhance inferential performance, a joint dynamic stochastic model for these components is proposed and statistical inference methods are developed. This joint model, formulated via counting processes and continuous-time Markov chains, has the potential of facilitating `personalized' interventions. This could enhance, for example, the implementation and adoption of precision medicine in medical settings. Semi-parametric and likelihood-based inferential methods for the model parameters are developed when a sample of these units is available. Finite-sample and asymptotic properties of estimators of model parameters, both finite- and infinite-dimensional, are obtained analytically or through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2103.12903v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lili Tong, Piaomu Liu, Edsel Pena</dc:creator>
    </item>
    <item>
      <title>Identification enhanced generalised linear model estimation with nonignorable missing outcomes</title>
      <link>https://arxiv.org/abs/2204.10508</link>
      <description>arXiv:2204.10508v4 Announce Type: replace 
Abstract: Missing data often result in undesirable bias and loss of efficiency. These issues become substantial when the response mechanism is nonignorable, meaning that the response model depends on unobserved variables. To manage nonignorable nonresponse, it is necessary to estimate the joint distribution of unobserved variables and response indicators. However, model misspecification and identification issues can prevent robust estimates, even with careful estimation of the target joint distribution. In this study, we modeled the distribution of the observed parts and derived sufficient conditions for model identifiability, assuming a logistic regression model as the response mechanism and generalized linear models as the main outcome model of interest. More importantly, the derived sufficient conditions do not require any instrumental variables, which are often assumed to guarantee model identifiability but cannot be practically determined beforehand. To analyze missing data in applications, we propose practical guidelines and sensitivity analysis to determine the response mechanism. Furthermore, we present the performance of the proposed estimators in numerical studies and apply the proposed method to two sets of real data: exit polls from the 19th South Korean election and public data collected from the Korean Survey of Household Finances and Living Conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.10508v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenji Beppu, Jinung Choi, Kosuke Morikawa, Jongho Im</dc:creator>
    </item>
    <item>
      <title>Dynamic Bayesian Learning for Spatiotemporal Mechanistic Models</title>
      <link>https://arxiv.org/abs/2208.06528</link>
      <description>arXiv:2208.06528v4 Announce Type: replace 
Abstract: We develop an approach for Bayesian learning of spatiotemporal dynamical mechanistic models. Such learning consists of statistical emulation of the mechanistic system that can efficiently interpolate the output of the system from arbitrary inputs. The emulated learner can then be used to train the system from noisy data achieved by melding information from observed data with the emulated mechanistic system. This joint melding of mechanistic systems employ hierarchical state-space models with Gaussian process regression. Assuming the dynamical system is controlled by a finite collection of inputs, Gaussian process regression learns the effect of these parameters through a number of training runs, driving the stochastic innovations of the spatiotemporal state-space component. This enables efficient modeling of the dynamics over space and time. This article details exact inference with analytically accessible posterior distributions in hierarchical matrix-variate Normal and Wishart models in designing the emulator. This step obviates expensive iterative algorithms such as Markov chain Monte Carlo or variational approximations. We also show how emulation is applicable to large-scale emulation by designing a dynamic Bayesian transfer learning framework. Inference on $\bm \eta$ proceeds using Markov chain Monte Carlo as a post-emulation step using the emulator as a regression component. We demonstrate this framework through solving inverse problems arising in the analysis of ordinary and partial nonlinear differential equations and, in addition, to a black-box computer model generating spatiotemporal dynamics across a graphical model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.06528v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sudipto Banerjee, Xiang Chen, Ian Frankenburg, Daniel Zhou</dc:creator>
    </item>
    <item>
      <title>Using Case Description Information to Reduce Sensitivity to Bias for the Attributable Fraction Among the Exposed</title>
      <link>https://arxiv.org/abs/2209.00781</link>
      <description>arXiv:2209.00781v5 Announce Type: replace 
Abstract: The attributable fraction among the exposed (\textbf{AF}$_e$), also known as the attributable risk or excess fraction among the exposed, is the proportion of disease cases among the exposed that could be avoided by eliminating the exposure. Understanding the \textbf{AF}$_e$ for different exposures helps guide public health interventions. The conventional approach to inference for the \textbf{AF}$_e$ assumes no unmeasured confounding and could be sensitive to hidden bias from unobserved covariates. In this paper, we propose a new approach to reduce sensitivity to hidden bias for conducting statistical inference on the \textbf{AF}$_e$ by leveraging case description information. Case description information is information that describes the case, e.g., the subtype of cancer. The exposure may have more of an effect on some types of cases than other types. We explore how leveraging case description information can reduce sensitivity to bias from unmeasured confounding through an asymptotic tool, design sensitivity, and simulation studies. We allow for the possibility that leveraging case definition information may introduce additional selection bias through an additional sensitivity parameter. The proposed methodology is illustrated by re-examining alcohol consumption and the risk of postmenopausal invasive breast cancer using case description information on the subtype of cancer (hormone-sensitive or insensitive) using data from the Women's Health Initiative (WHI) Observational Study (OS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.00781v5</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kan Chen, Jing Cheng, M. Elizabeth Halloran, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Semiparametric Efficient Fusion of Individual Data and Summary Statistics</title>
      <link>https://arxiv.org/abs/2210.00200</link>
      <description>arXiv:2210.00200v5 Announce Type: replace 
Abstract: Suppose we have individual data from an internal study and various summary statistics from relevant external studies. External summary statistics have the potential to improve statistical inference for the internal population; however, it may lead to efficiency loss or bias if not used properly. We study the fusion of individual data and summary statistics in a semiparametric framework to investigate the efficient use of external summary statistics. Under a weak transportability assumption, we establish the semiparametric efficiency bound for estimating a general functional of the internal data distribution, which is no larger than that using only internal data and underpins the potential efficiency gain of integrating individual data and summary statistics. We propose a data-fused efficient estimator that achieves this efficiency bound. In addition, an adaptive fusion estimator is proposed to eliminate the bias of the original data-fused estimator when the transportability assumption fails. We establish the asymptotic oracle property of the adaptive fusion estimator. Simulations and application to a Helicobacter pylori infection dataset demonstrate the promising numerical performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.00200v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenjie Hu, Ruoyu Wang, Wei Li, Wang Miao</dc:creator>
    </item>
    <item>
      <title>Deep Bayes Factors</title>
      <link>https://arxiv.org/abs/2312.05411</link>
      <description>arXiv:2312.05411v3 Announce Type: replace 
Abstract: The is no other model or hypothesis verification tool in Bayesian statistics that is as widely used as the Bayes factor. We focus on generative models that are likelihood-free and, therefore, render the computation of Bayes factors (marginal likelihood ratios) far from obvious. We propose a deep learning estimator of the Bayes factor based on simulated data from two competing models using the likelihood ratio trick. This estimator is devoid of summary statistics and obviates some of the difficulties with ABC model choice. We establish sufficient conditions for consistency of our Deep Bayes Factor estimator as well as its consistency as a model selection tool. We investigate the performance of our estimator on various examples using a wide range of quality metrics related to estimation and model decision accuracy. After training, our deep learning approach enables rapid evaluations of the Bayes factor estimator at any fictional data arriving from either hypothesized model, not just the observed data $Y_0$. This allows us to inspect entire Bayes factor distributions under the two models and to quantify the relative location of the Bayes factor evaluated at $Y_0$ in light of these distributions. Such tail area evaluations are not possible for Bayes factor estimators tailored to $Y_0$. We find the performance of our Deep Bayes Factors competitive with existing MCMC techniques that require the knowledge of the likelihood function. We also consider variants for posterior or intrinsic Bayes factors estimation. We demonstrate the usefulness of our approach on a relatively high-dimensional real data example about determining cognitive biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05411v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jungeum Kim, Veronika Rockova</dc:creator>
    </item>
    <item>
      <title>Robust Point Matching with Distance Profiles</title>
      <link>https://arxiv.org/abs/2312.12641</link>
      <description>arXiv:2312.12641v4 Announce Type: replace 
Abstract: We show the outlier robustness and noise stability of practical matching procedures based on distance profiles. Although the idea of matching points based on invariants like distance profiles has a long history in the literature, there has been little understanding of the theoretical properties of such procedures, especially in the presence of outliers and noise. We provide a theoretical analysis showing that under certain probabilistic settings, the proposed matching procedure is successful with high probability even in the presence of outliers and noise. We demonstrate the performance of the proposed method using a real data example and provide simulation studies to complement the theoretical findings. Lastly, we extend the concept of distance profiles to the abstract setting and connect the proposed matching procedure to the Gromov-Wasserstein distance and its lower bound, with a new sample complexity result derived based on the properties of distance profiles. This paper contributes to the literature by providing theoretical underpinnings of the matching procedures based on invariants like distance profiles, which have been widely used in practice but have rarely been analyzed theoretically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.12641v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>YoonHaeng Hur, Yuehaw Khoo</dc:creator>
    </item>
    <item>
      <title>Loss-based prior for tree topologies in BART models</title>
      <link>https://arxiv.org/abs/2404.00359</link>
      <description>arXiv:2404.00359v2 Announce Type: replace 
Abstract: We present a novel prior for tree topology within Bayesian Additive Regression Trees (BART) models. This approach quantifies the hypothetical loss in information and the loss due to complexity associated with choosing the wrong tree structure. The resulting prior distribution is compellingly geared toward sparsity, a critical feature considering BART models' tendency to overfit. Our method incorporates prior knowledge into the distribution via two parameters that govern the tree's depth and balance between its left and right branches. Additionally, we propose a default calibration for these parameters, offering an objective version of the prior. We demonstrate our method's efficacy on both simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00359v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>F. Serafini, F. Leisen, C. Villa, K. Wilson</dc:creator>
    </item>
    <item>
      <title>AutoGFI: Streamlined Generalized Fiducial Inference for Modern Inference Problems in Models with Additive Errors</title>
      <link>https://arxiv.org/abs/2404.08169</link>
      <description>arXiv:2404.08169v2 Announce Type: replace 
Abstract: The concept of fiducial inference was introduced by R. A. Fisher in the 1930s to address the perceived limitations of Bayesian inference, particularly the need for subjective prior distributions in cases with limited prior information. However, Fisher's fiducial approach lost favor due to complications, especially in multi-parameter problems. With renewed interest in fiducial inference in the 2000s, generalized fiducial inference (GFI) emerged as a promising extension of Fisher's ideas, offering new solutions for complex inference challenges. Despite its potential, GFI's adoption has been hindered by demanding mathematical derivations and complex implementation requirements, such as Markov Chain Monte Carlo (MCMC) algorithms. This paper introduces AutoGFI, a streamlined variant of GFI designed to simplify its application across various inference problems with additive noise. AutoGFI's accessibility lies in its simplicity-requiring only a fitting routine-making it a feasible option for a wider range of researchers and practitioners. To demonstrate its efficacy, AutoGFI is applied to three challenging problems: tensor regression, matrix completion, and network cohesion regression. These case studies showcase AutoGFI's competitive performance against specialized solutions, highlighting its potential to broaden the application of GFI in practical domains, ultimately enriching the statistical inference toolkit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08169v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10618600.2024.2441165</arxiv:DOI>
      <dc:creator>Wei Du, Jan Hannig, Thomas C. M. Lee, Yi Su, Chunzhe Zhang</dc:creator>
    </item>
    <item>
      <title>Covariate Selection for Optimizing Balance with an Innovative Adaptive Randomization Approach</title>
      <link>https://arxiv.org/abs/2406.08968</link>
      <description>arXiv:2406.08968v2 Announce Type: replace 
Abstract: Balancing influential covariates is crucial for valid treatment comparisons in clinical studies. While covariate-adaptive randomization is commonly used to achieve balance, its performance can be inadequate when the number of baseline covariates is large. It is therefore essential to identify the influential factors associated with the outcome and ensure balance among these critical covariates. In this article, we propose a novel adaptive randomization approach that integrates the patients' responses and covariates information to select sequentially significant covariates and maintain their balance. We establish theoretically the consistency of our covariate selection method and demonstrate that the improved covariate balancing, as evidenced by a faster convergence rate of the imbalance measure, leads to higher efficiency in estimating treatment effects. Furthermore, we provide extensive numerical and empirical studies to illustrate the benefits of our proposed method across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08968v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziqing Guo, Yang Liu, Lucy Xia</dc:creator>
    </item>
    <item>
      <title>Valid standard errors for Bayesian quantile regression with clustered and independent data</title>
      <link>https://arxiv.org/abs/2407.09772</link>
      <description>arXiv:2407.09772v4 Announce Type: replace 
Abstract: In Bayesian quantile regression, the most commonly used likelihood is the asymmetric Laplace (AL) likelihood. The reason for this choice is not that it is a plausible data-generating model but that the corresponding maximum likelihood estimator is identical to the classical estimator by Koenker and Bassett (1978), and in that sense, the AL likelihood can be thought of as a working likelihood. AL-based quantile regression has been shown to produce good finite-sample Bayesian point estimates and to be consistent. However, if the AL distribution does not correspond to the data-generating distribution, credible intervals based on posterior standard deviations can have poor coverage. Yang, Wang, and He (2016) proposed an adjustment to the posterior covariance matrix that produces asymptotically valid intervals. However, we show that this adjustment is sensitive to the choice of scale parameter for the AL likelihood and can lead to poor coverage when the sample size is small to moderate. We therefore propose using Infinitesimal Jackknife (IJ) standard errors (Giordano &amp; Broderick, 2023). These standard errors do not require resampling but can be obtained from a single MCMC run. We also propose a version of IJ standard errors for clustered data. Simulations and applications to real data show that the IJ standard errors have good frequentist properties, both for independent and clustered data. We provide an R-package, IJSE, that computes IJ standard errors for clustered or independent data after estimation with the brms wrapper in R for Stan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09772v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ji, JoonHo Lee, Sophia Rabe-Hesketh</dc:creator>
    </item>
    <item>
      <title>Non-zero block selector: A linear correlation coefficient measure for blocking-selection models</title>
      <link>https://arxiv.org/abs/2407.13302</link>
      <description>arXiv:2407.13302v2 Announce Type: replace 
Abstract: Multiple-group data is widely used in genomic studies, finance, and social science. This study investigates a block structure that consists of covariate and response groups. It examines the block-selection problem of high-dimensional models with group structures for both responses and covariates, where both the number of blocks and the dimension within each block are allowed to grow larger than the sample size. We propose a novel strategy for detecting the block structure, which includes the block-selection model and a non-zero block selector (NBS). We establish the uniform consistency of the NBS and propose three estimators based on the NBS to enhance modeling efficiency. We prove that the estimators achieve the oracle solution and show that they are consistent, jointly asymptotically normal, and efficient in modeling extremely high-dimensional data. Simulations generate complex data settings and demonstrate the superiority of the proposed method. A gene-data analysis also demonstrates its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13302v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixiong Liang, Yuehan Yang</dc:creator>
    </item>
    <item>
      <title>Modeling Zero-Inflated Correlated Dental Data through Gaussian Copulas and Approximate Bayesian Computation</title>
      <link>https://arxiv.org/abs/2410.13949</link>
      <description>arXiv:2410.13949v2 Announce Type: replace 
Abstract: We develop a new longitudinal count data regression model that accounts for zero-inflation and spatio-temporal correlation across responses. This project is motivated by an analysis of Iowa Fluoride Study (IFS) data, a longitudinal cohort study with data on caries (cavity) experience scores measured for each tooth across five time points. To that end, we use a hurdle model for zero-inflation with two parts: the presence model indicating whether a count is non-zero through logistic regression and the severity model that considers the non-zero counts through a shifted Negative Binomial distribution allowing overdispersion. To incorporate dependence across measurement occasion and teeth, these marginal models are embedded within a Gaussian copula that introduces spatio-temporal correlations. A distinct advantage of this formulation is that it allows us to determine covariate effects with population-level (marginal) interpretations in contrast to mixed model choices. Standard Bayesian sampling from such a model is infeasible, so we use approximate Bayesian computing for inference. This approach is applied to the IFS data to gain insight into the risk factors for dental caries and the correlation structure across teeth and time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13949v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anish Mukherjee, Jeremy T. Gaskins, Shoumi Sarkar, Steven Levy, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>Model checking for high dimensional generalized linear models based on random projections</title>
      <link>https://arxiv.org/abs/2412.10721</link>
      <description>arXiv:2412.10721v2 Announce Type: replace 
Abstract: Most existing tests in the literature for model checking do not work in high dimension settings due to challenges arising from the "curse of dimensionality", or dependencies on the normality of parameter estimators. To address these challenges, we proposed a new goodness of fit test based on random projections for generalized linear models, when the dimension of covariates may substantially exceed the sample size. The tests only require the convergence rate of parameter estimators to derive the limiting distribution. The growing rate of the dimension is allowed to be of exponential order in relation to the sample size. As random projection converts covariates to one-dimensional space, our tests can detect the local alternative departing from the null at the rate of $n^{-1/2}h^{-1/4}$ where $h$ is the bandwidth, and $n$ is the sample size. This sensitive rate is not related to the dimension of covariates, and thus the "curse of dimensionality" for our tests would be largely alleviated. An interesting and unexpected result is that for randomly chosen projections, the resulting test statistics can be asymptotic independent. We then proposed combination methods to enhance the power performance of the tests. Detailed simulation studies and a real data analysis are conducted to illustrate the effectiveness of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10721v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wen Chen, Jie Liu, Heng Peng, Falong Tan, Lixing Zhu</dc:creator>
    </item>
    <item>
      <title>Hierarchical Dirichlet Process Mixture of Products of Multinomial Distributions: Applications to Survey Data with Potentially Missing Values</title>
      <link>https://arxiv.org/abs/2412.17335</link>
      <description>arXiv:2412.17335v2 Announce Type: replace 
Abstract: In social science research, understanding latent structures in populations through survey data with categorical responses is a common and important task. Traditional methods like Factor Analysis and Latent Class Analysis have limitations, particularly in handling categorical data and accommodating mixed memberships in latent structures, respectively. Moreover, choosing the number of factors or latent classes is often subjective and can be challenging in the presence of missing values. This study introduces a Hierarchical Dirichlet Process Mixture of Products of Multinomial Distributions (HDPMPM) model, which leverages the flexibility of nonparametric Bayesian methods to address these limitations. The HDPMPM model allows for multiple latent classes within individuals and avoids fixing the number of mixture components at an arbitrary number. Additionally, it incorporates missing data imputation directly into the model's Gibbs sampling process. By applying a truncated stick-breaking representation of the Dirichlet process, we can derive a Gibbs sampling scheme for posterior inference. An application of the HDPMPM model to the 2016 American National Election Study (ANES) data demonstrates its effectiveness in identifying political profiles and handling missing data scenarios, including those that are missing at random (MAR) and missing completely at random (MCAR). The results show that the HDPMPM model successfully recovers dominant profiles and manages complex latent structures in survey data, providing an alternative tool for social science researchers in dealing with categorical data with missing values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17335v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chayut Wongkamthong</dc:creator>
    </item>
    <item>
      <title>Fast estimation of Kendall's Tau and conditional Kendall's Tau matrices under structural assumptions</title>
      <link>https://arxiv.org/abs/2204.03285</link>
      <description>arXiv:2204.03285v2 Announce Type: replace-cross 
Abstract: Kendall's tau and conditional Kendall's tau matrices are multivariate (conditional) dependence measures between the components of a random vector. For large dimensions, available estimators are computationally expensive and can be improved by averaging. Under structural assumptions on the underlying Kendall's tau and conditional Kendall's tau matrices, we introduce new estimators that have a significantly reduced computational cost while keeping a similar error level. In the unconditional setting we assume that, up to reordering, the underlying Kendall's tau matrix is block-structured with constant values in each of the off-diagonal blocks. Consequences on the underlying correlation matrix are then discussed. The estimators take advantage of this block structure by averaging over (part of) the pairwise estimates in each of the off-diagonal blocks. Derived explicit variance expressions show their improved efficiency. In the conditional setting, the conditional Kendall's tau matrix is assumed to have a constant block structure, independently of the conditioning variable. Conditional Kendall's tau matrix estimators are constructed similarly as in the unconditional case by averaging over (part of) the pairwise conditional Kendall's tau estimators. We establish their joint asymptotic normality, and show that the asymptotic variance is reduced compared to the naive estimators. Then, we perform a simulation study which displays the improved performance of both the unconditional and conditional estimators. Finally, the estimators are used for estimating the value at risk of a large stock portfolio; backtesting illustrates the obtained improvements compared to the previous estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.03285v2</guid>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rutger van der Spek, Alexis Derumigny</dc:creator>
    </item>
    <item>
      <title>From Contextual Data to Newsvendor Decisions: On the Actual Performance of Data-Driven Algorithms</title>
      <link>https://arxiv.org/abs/2302.08424</link>
      <description>arXiv:2302.08424v4 Announce Type: replace-cross 
Abstract: In this work, we explore a framework for contextual decision-making to study how the relevance and quantity of past data affects the performance of a data-driven policy. We analyze a contextual Newsvendor problem in which a decision-maker needs to trade-off between an underage and an overage cost in the face of uncertain demand. We consider a setting in which past demands observed under ``close by'' contexts come from close by distributions and analyze the performance of data-driven algorithms through a notion of context-dependent worst-case expected regret. We analyze the broad class of Weighted Empirical Risk Minimization (WERM) policies which weigh past data according to their similarity in the contextual space. This class includes classical policies such as ERM, k-Nearest Neighbors and kernel-based policies. Our main methodological contribution is to characterize exactly the worst-case regret of any WERM policy on any given configuration of contexts. To the best of our knowledge, this provides the first understanding of tight performance guarantees in any contextual decision-making problem, with past literature focusing on upper bounds via concentration inequalities. We instead take an optimization approach, and isolate a structure in the Newsvendor loss function that allows to reduce the infinite-dimensional optimization problem over worst-case distributions to a simple line search.
  This in turn allows us to unveil fundamental insights that were obfuscated by previous general-purpose bounds. We characterize actual guaranteed performance as a function of the contexts, as well as granular insights on the learning curve of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.08424v4</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Omar Besbes, Will Ma, Omar Mouchtaki</dc:creator>
    </item>
    <item>
      <title>Semiparametric Inference for Regression-Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2403.05803</link>
      <description>arXiv:2403.05803v3 Announce Type: replace-cross 
Abstract: Treatment effects in regression discontinuity designs (RDDs) are often estimated using local regression methods. \cite{Hahn:01} demonstrated that the identification of the average treatment effect at the cutoff in RDDs relies on the unconfoundedness assumption and that, without this assumption, only the local average treatment effect at the cutoff can be identified. In this paper, we propose a semiparametric framework tailored for identifying the average treatment effect in RDDs, eliminating the need for the unconfoundedness assumption. Our approach globally conceptualizes the identification as a partially linear modeling problem, with the coefficient of a specified polynomial function of propensity score in the linear component capturing the average treatment effect. This identification result underpins our semiparametric inference for RDDs, employing the $P$-spline method to approximate the nonparametric function and establishing a procedure for conducting inference within this framework. Through theoretical analysis, we demonstrate that our global approach achieves a faster convergence rate compared to the local method. Monte Carlo simulations further confirm that the proposed method consistently outperforms alternatives across various scenarios. Furthermore, applications to real-world datasets illustrate that our global approach can provide more reliable inference for practical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05803v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weiwei Jiang, Rong J. B. Zhu</dc:creator>
    </item>
    <item>
      <title>Geometric quantile-based measures of multivariate distributional characteristics</title>
      <link>https://arxiv.org/abs/2407.07297</link>
      <description>arXiv:2407.07297v2 Announce Type: replace-cross 
Abstract: Several new geometric quantile-based measures for multivariate dispersion, skewness, kurtosis, and spherical asymmetry are defined. These measures differ from existing measures, which use volumes and are easy to calculate. Some theoretical justification is given, followed by experiments illustrating that they are reasonable measures of these distributional characteristics and computing confidence regions with the desired coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07297v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Projected random forests and conformal prediction of circular data</title>
      <link>https://arxiv.org/abs/2410.24145</link>
      <description>arXiv:2410.24145v2 Announce Type: replace-cross 
Abstract: We apply split conformal prediction techniques to regression problems with circular responses by introducing a suitable conformity score, leading to prediction sets with adaptive arc length and finite-sample coverage guarantees for any circular predictive model under exchangeable data. Leveraging the high performance of existing predictive models designed for linear responses, we analyze a general projection procedure that converts any linear response regression model into one suitable for circular responses. When random forests serve as basis models in this projection procedure, we harness the out-of-bag dynamics to eliminate the necessity for a separate calibration sample in the construction of prediction sets. For synthetic and real datasets the resulting projected random forests model produces more efficient out-of-bag conformal prediction sets, with shorter median arc length, when compared to the split conformal prediction sets generated by two existing alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24145v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paulo C. Marques F., Rinaldo Artes, Helton Graziadei</dc:creator>
    </item>
  </channel>
</rss>
