<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Jan 2025 05:00:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Dissecting a Small Artificial Neural Network</title>
      <link>https://arxiv.org/abs/2501.08341</link>
      <description>arXiv:2501.08341v1 Announce Type: new 
Abstract: We investigate the loss landscape and backpropagation dynamics of convergence for the simplest possible artificial neural network representing the logical exclusive-OR (XOR) gate. Cross-sections of the loss landscape in the nine-dimensional parameter space are found to exhibit distinct features, which help understand why backpropagation efficiently achieves convergence toward zero loss, whereas values of weights and biases keep drifting. Differences in shapes of cross-sections obtained by nonrandomized and randomized batches are discussed. In reference to statistical physics we introduce the microcanonical entropy as a unique quantity that allows to characterize the phase behavior of the network. Learning in neural networks can thus be thought of as an annealing process that experiences the analogue of phase transitions known from thermodynamic systems. It also reveals how the loss landscape simplifies as more hidden neurons are added to the network, eliminating entropic barriers caused by finite-size effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08341v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/1751-8121/ad9dc6</arxiv:DOI>
      <arxiv:journal_reference>J. Phys. A: Math. Theor. 58 025001(1-18) (2025)</arxiv:journal_reference>
      <dc:creator>Xiguang Yang, Krish Arora, Michael Bachmann</dc:creator>
    </item>
    <item>
      <title>Observation of Impurity-Induced Scale-Free Localization in a Disordered Non-Hermitian Electrical Circuit</title>
      <link>https://arxiv.org/abs/2501.08594</link>
      <description>arXiv:2501.08594v1 Announce Type: new 
Abstract: One of unique features of non-Hermitian systems is the extreme sensitive to their boundary conditions, e.g., the emergence of non-Hermitian skin effect (NHSE) under the open boundary conditions, where most of bulk states become localized at the boundaries. In the presence of impurities, the scale-free localization can appear, which is qualitatively distinct from the NHSE. Here, we experimentally design a disordered non-Hermitian electrical circuits in the presence of a single non-Hermitian impurity and the nonreciprocal hopping. We observe the anomalous scale-free accumulation of eigenstates, opposite to the bulk hopping direction. The experimental results open the door to further explore the anomalous skin effects in non-Hermitian electrical circuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08594v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.mes-hall</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.15302/frontphys.2025.014203</arxiv:DOI>
      <arxiv:journal_reference>Frontiers of Physics, 20, 014203(2025)</arxiv:journal_reference>
      <dc:creator>Hao Wang, Jin Liu, Tao Liu, Wen-Bo Ju</dc:creator>
    </item>
    <item>
      <title>Amp\`ere phase in frustrated magnets</title>
      <link>https://arxiv.org/abs/2501.08859</link>
      <description>arXiv:2501.08859v1 Announce Type: cross 
Abstract: We report a new class of algebraic spin liquids, in which the macroscopically degenerate ground state manifold is not Coulombic, like in spin ices, but Amp\`ere-like. The local constraint characterizing an Amp\`ere phase is not a Gauss law, but rather an Amp\`ere law, i.e., a condition on the curl of the magnetization vector field and not on its divergence. As a consequence, the excitations evolving in such a manifold are not magnetically charged scalar quasiparticles, the so-called magnetic monopoles in Coulomb phases, but instead vectorial magnetic loops (or fictional current lines). We demonstrate analytically that in a macroscopically degenerate manifold inheriting the properties of a cooperative paramagnet and subject to a local curl-free contraint, magnetic correlations decay in space with a power law whose exponent is the space dimension d: the Amp\`ere phase is a d-algebraic spin liquid. Using Monte Carlo simulations with appropriate cluster dynamics, we confirm this physics numerically in two- and three-dimensional examples, and illustrate how the Amp\`ere phase compares to its Coulomb counterpart.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08859v1</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.other</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>N. Rougemaille, J. Coraux, B. Canals</dc:creator>
    </item>
    <item>
      <title>High-dimensional learning of narrow neural networks</title>
      <link>https://arxiv.org/abs/2409.13904</link>
      <description>arXiv:2409.13904v2 Announce Type: replace-cross 
Abstract: Recent years have been marked with the fast-pace diversification and increasing ubiquity of machine learning applications. Yet, a firm theoretical understanding of the surprising efficiency of neural networks to learn from high-dimensional data still proves largely elusive. In this endeavour, analyses inspired by statistical physics have proven instrumental, enabling the tight asymptotic characterization of the learning of neural networks in high dimensions, for a broad class of solvable models. This manuscript reviews the tools and ideas underlying recent progress in this line of work. We introduce a generic model -- the sequence multi-index model -- which encompasses numerous previously studied models as special instances. This unified framework covers a broad class of machine learning architectures with a finite number of hidden units, including multi-layer perceptrons, autoencoders, attention mechanisms; and tasks, including (un)supervised learning, denoising, contrastive learning, in the limit of large data dimension, and comparably large number of samples. We explicate in full detail the analysis of the learning of sequence multi-index models, using statistical physics techniques such as the replica method and approximate message-passing algorithms. This manuscript thus provides a unified presentation of analyses reported in several previous works, and a detailed overview of central techniques in the field of statistical physics of machine learning. This review should be a useful primer for machine learning theoreticians curious of statistical physics approaches; it should also be of value to statistical physicists interested in the transfer of such ideas to the study of neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13904v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugo Cui</dc:creator>
    </item>
    <item>
      <title>Learning by Confusion: The Phase Diagram of the Holstein Model</title>
      <link>https://arxiv.org/abs/2501.04681</link>
      <description>arXiv:2501.04681v3 Announce Type: replace-cross 
Abstract: We employ the "learning by confusion" technique, an unsupervised machine learning approach for detecting phase transitions, to analyze quantum Monte Carlo simulations of the two-dimensional Holstein model--a fundamental model for electron-phonon interactions on a lattice. Utilizing a convolutional neural network, we conduct a series of binary classification tasks to identify Holstein critical points based on the neural network's learning accuracy. We further evaluate the effectiveness of various training datasets, including snapshots of phonon fields and other measurements resolved in imaginary time, for predicting distinct phase transitions and crossovers. Our results culminate in the construction of the finite-temperature phase diagram of the Holstein model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04681v3</guid>
      <category>cond-mat.str-el</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 16 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>George Issa, Owen Bradley, Ehsan Khatami, Richard Scalettar</dc:creator>
    </item>
  </channel>
</rss>
