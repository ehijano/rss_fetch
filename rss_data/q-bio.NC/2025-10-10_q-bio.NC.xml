<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Beyond Grid-Locked Voxels: Neural Response Functions for Continuous Brain Encoding</title>
      <link>https://arxiv.org/abs/2510.07342</link>
      <description>arXiv:2510.07342v1 Announce Type: new 
Abstract: Neural encoding models aim to predict fMRI-measured brain responses to natural images. fMRI data is acquired as a 3D volume of voxels, where each voxel has a defined spatial location in the brain. However, conventional encoding models often flatten this volume into a 1D vector and treat voxel responses as independent outputs. This removes spatial context, discards anatomical information, and ties each model to a subject-specific voxel grid. We introduce the Neural Response Function (NRF), a framework that models fMRI activity as a continuous function over anatomical space rather than a flat vector of voxels. NRF represents brain activity as a continuous implicit function: given an image and a spatial coordinate (x, y, z) in standardized MNI space, the model predicts the response at that location. This formulation decouples predictions from the training grid, supports querying at arbitrary spatial resolutions, and enables resolution-agnostic analyses. By grounding the model in anatomical space, NRF exploits two key properties of brain responses: (1) local smoothness -- neighboring voxels exhibit similar response patterns; modeling responses continuously captures these correlations and improves data efficiency, and (2) cross-subject alignment -- MNI coordinates unify data across individuals, allowing a model pretrained on one subject to be fine-tuned on new subjects. In experiments, NRF outperformed baseline models in both intrasubject encoding and cross-subject adaptation, achieving high performance while reducing the data size needed by orders of magnitude. To our knowledge, NRF is the first anatomically aware encoding model to move beyond flattened voxels, learning a continuous mapping from images to brain responses in 3D space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07342v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haomiao Chen, Keith W Jamison, Mert R. Sabuncu, Amy Kuceyeski</dc:creator>
    </item>
    <item>
      <title>Monkey Perceptogram: Reconstructing Visual Representation and Presumptive Neural Preference from Monkey Multi-electrode Arrays</title>
      <link>https://arxiv.org/abs/2510.07576</link>
      <description>arXiv:2510.07576v1 Announce Type: new 
Abstract: Understanding how the primate brain transforms complex visual scenes into coherent perceptual experiences remains a central challenge in neuroscience. Here, we present a comprehensive framework for interpreting monkey visual processing by integrating encoding and decoding approaches applied to two large-scale spiking datasets recorded from macaque using THINGS images (THINGS macaque IT Dataset (TITD) and THINGS Ventral Stream Spiking Dataset (TVSD)). We leverage multi-electrode array recordings from the ventral visual stream--including V1, V4, and inferotemporal (IT) cortex--to investigate how distributed neural populations encode and represent visual information. Our approach employs linear models to decode spiking activity into multiple latent visual spaces (including CLIP and VDVAE embeddings) and reconstruct images using state-of-the-art generative models. We further utilize encoding models to map visual features back to neural activity, enabling visualization of the "preferred stimuli" that drive specific neural ensembles. Analyses of both datasets reveal that it is possible to reconstruct both low-level (e.g., color, texture) and high-level (e.g., semantic category) features of visual stimuli from population activity, with reconstructions preserving key perceptual attributes as quantified by feature-based similarity metrics. The spatiotemporal spike patterns reflect the ventral stream's hierarchical organization with anterior regions representing complex objects and categories. Functional clustering identifies feature-specific neural ensembles, with temporal dynamics show evolving feature selectivity post-stimulus. Our findings demonstrate feasible, generalizable perceptual reconstruction from large-scale monkey neural recordings, linking neural activity to perception.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07576v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Teng Fei, Srinivas Ravishankar, Hoko Nakada, Abhinav Uppal, Ian Jackson, Garrison W. Cottrell, Ryusuke Hayashi, Virginia R. de Sa</dc:creator>
    </item>
    <item>
      <title>State-dependent brain responsiveness, from local circuits to the whole brain</title>
      <link>https://arxiv.org/abs/2510.07956</link>
      <description>arXiv:2510.07956v1 Announce Type: new 
Abstract: The objective of this paper is to review physiological and computational aspects of the responsiveness of the cerebral cortex to stimulation, and how responsiveness depends on the state of the system. This correspondence between brain state and brain responsiveness (state-dependent responses) is outlined at different scales from the cellular and circuit level, to the mesoscale and macroscale level. At each scale, we review how quantitative methods can be used to characterize network states based on brain responses, such as the Perturbational Complexity Index (PCI). This description will compare data and models, systematically and at multiple scales, with a focus on the mechanisms that explain how brain responses depend on brain states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07956v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>A. Destexhe, J Goldman, N. Tort-Colet, A. Roques, J. Fousek, S. Petkoski, V. Jirsa, O. David, M. Jedynak, C. Capone, C. De Luca, G. De Bonis, P. S. Paolucci, E. Mikulan,  Pigorini, M Massimini, A. Galluzzi, A. Pazienti, M. Mattia, A. Arena, B. E. Juel, E. Hagen, J. F. Storm, E. Montagni, F. Resta, F. S. Pavone, A. L. Allegra Mascaro, A. Dwarakanath, T. I. Panagiotaropoulos, J. Senk, M. Diesmann, A. Camassa, L. Dalla Porta, A. Manasanch, M. V. Sanchez-Vives</dc:creator>
    </item>
    <item>
      <title>Optimizing BCI Rehabilitation Protocols for Stroke: Exploring Task Design and Training Duration</title>
      <link>https://arxiv.org/abs/2510.08082</link>
      <description>arXiv:2510.08082v1 Announce Type: new 
Abstract: Stroke is a leading cause of long-term disability and the second most common cause of death worldwide. Although acute treatments have advanced, recovery remains challenging and limited. Brain-computer interfaces (BCIs) have emerged as a promising tool for post-stroke rehabilitation by promoting neuroplasticity. However, clinical outcomes remain variable, and optimal protocols have yet to be established. This study explores strategies to optimize BCI-based rehabilitation by comparing motor imagery of affected hand movement versus rest, instead of the conventional left-versus-right motor imagery. This alternative aims to simplify the task and address the weak contralateral activation commonly observed in stroke patients. Two datasets, one from healthy individuals and one from stroke patients, were used to evaluate the proposed approach. The results showed improved performance using both FBCSP and EEGNet. Additionally, we investigated the impact of session duration and found that shorter training sessions produced better BCI performance than longer sessions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08082v1</guid>
      <category>q-bio.NC</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aniana Cruz, Marko Kuzmanoski, Gabriel Pires</dc:creator>
    </item>
    <item>
      <title>Autoencoding Coordinate Sequences from Psychophysiologic Signals</title>
      <link>https://arxiv.org/abs/2510.07415</link>
      <description>arXiv:2510.07415v1 Announce Type: cross 
Abstract: We present a method for converting 24 channels of psychophysiologic time series data collected from individual participants via electroencephalogram (EEG), electrocardiogram (ECG), electrodermal activity (EDA), respiration rate (RR) into trackable three dimensional (3D) coordinates sufficient to estimate participation in specific task and cognitive states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07415v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/RAPID64712.2025.11151495</arxiv:DOI>
      <dc:creator>Timothy L. Hutcheson, Anil K. Raj</dc:creator>
    </item>
    <item>
      <title>Spike-frequency and h-current based adaptation are dynamically equivalent in a Wilson-Cowan field model</title>
      <link>https://arxiv.org/abs/2510.08436</link>
      <description>arXiv:2510.08436v1 Announce Type: cross 
Abstract: During slow-wave sleep, the brain produces traveling waves of slow oscillations (SOs; $\leq 2$ Hz), characterized by the propagation of alternating high- and low-activity states. The question of internal mechanisms that modulate traveling waves of SOs is still unanswered although it is established that it is an adaptation mechanism that mediates them. One mechanism investigated is spike-frequency adaptation, a hyperpolarizing feedback current that is activated during periods of high-activity. An alternative mechanism is based on hyperpolarization-activated currents, which are positive feedback currents that are activated in low-activity states. Both adaptation mechanisms were shown to feature SO-like dynamics in neuronal populations, and the inclusion of a spatial domain seems to enhance observable differences in their effects. To investigate this in detail, we examine a spatially extended two-population Wilson-Cowan model with local spatial coupling and the excitatory populations equipped with either one of the two adaptation mechanisms. We describe them with the same dynamical equation and include the inverse mode of action by changing the signs of adaptation strength and gain. We show that the dynamical systems are mathematically equivalent under a compensatory external input, which depends on the adaptation strength, leading to a shift in state space of the otherwise equivalent bifurcation structure. Strong enough adaptation is required to induce traveling waves. Additionally, adaptation modulates the properties of the spatio-temporal activity patterns, such as temporal and spatial frequencies, and the speed of the traveling waves, all of which increase with increasing strength. Though being dynamically equivalent, our results also explain why location-dependent variations in feedback strength cause differences in the propagation of traveling waves between both adaptation mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08436v1</guid>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ronja Str\"omsd\"orfer, Klaus Obermayer</dc:creator>
    </item>
    <item>
      <title>Language learning shapes visual category-selectivity in deep neural networks</title>
      <link>https://arxiv.org/abs/2502.16456</link>
      <description>arXiv:2502.16456v2 Announce Type: replace 
Abstract: Category-selective regions in the human brain-such as the fusiform face area (FFA), extrastriate body area (EBA), parahippocampal place area (PPA), and visual word form area (VWFA)-support high-level visual recognition. Here, we investigate whether artificial neural networks (ANNs) exhibit analogous category-selective neurons and how these representations are shaped by language experience. Using an fMRI-inspired functional localizer approach, we identified face-, body-, place-, and word-selective neurons in deep networks presented with category images and scrambled controls. Both the purely visual ResNet and a linguistically supervised Lang-Learned ResNet contained category-selective neurons that increased in proportion across layers. However, compared to the vision-only model, the Lang-Learned ResNet showed a greater number but lower specificity of category-selective neurons, along with reduced spatial localization and attenuated activation strength-indicating a shift toward more distributed, semantically aligned coding. These effects were replicated in the large-scale vision-language model CLIP. Together, our findings reveal that language experience systematically reorganizes visual category representations in ANNs, providing a computational parallel to how linguistic context may shape categorical organization in the human brain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16456v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CV</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zitong Lu, Yuxin Wang</dc:creator>
    </item>
    <item>
      <title>Neural encoding of real world face perception</title>
      <link>https://arxiv.org/abs/2505.08831</link>
      <description>arXiv:2505.08831v2 Announce Type: replace 
Abstract: Social perception unfolds as we freely interact with people around us. We investigated the neural basis of real world face perception using multi electrode intracranial recordings in humans during spontaneous interactions with friends, family, and others. Computational models reconstructed the faces participants looked at during natural interactions, including facial expressions and motion, from brain activity alone. The results highlighted a critical role for the social vision pathway, a network of areas spanning parietal, temporal, and occipital cortex. This network was more sharply tuned to subtle expressions compared to intense expressions, which was confirmed with controlled psychophysical experiments. These findings reveal that the human social vision pathway encodes facial expressions and motion as deviations from a neutral expression prototype during natural social interactions in real life.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08831v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arish Alreja, Michael J. Ward, Lisa S. Parker, R. Mark Richardson, Louis-Philippe Morency, Taylor J. Abel, Avniel Singh Ghuman</dc:creator>
    </item>
    <item>
      <title>A Computational Perspective on NeuroAI and Synthetic Biological Intelligence</title>
      <link>https://arxiv.org/abs/2509.23896</link>
      <description>arXiv:2509.23896v2 Announce Type: replace 
Abstract: NeuroAI is an emerging field at the intersection of neuroscience and artificial intelligence, where insights from brain function guide the design of intelligent systems. A central area within this field is synthetic biological intelligence (SBI), which combines the adaptive learning properties of biological neural networks with engineered hardware and software. SBI systems provide a platform for modeling neural computation, developing biohybrid architectures, and enabling new forms of embodied intelligence. In this review, we organize the NeuroAI landscape into three interacting domains: hardware, software, and wetware. We outline computational frameworks that integrate biological and non-biological systems and highlight recent advances in organoid intelligence, neuromorphic computing, and neuro-symbolic learning. These developments collectively point toward a new class of systems that compute through interactions between living neural tissue and digital algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.23896v2</guid>
      <category>q-bio.NC</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhruvik Patel, Md Sayed Tanveer, Jesus Gonzalez-Ferrer, Alon Loeffler, Brett J. Kagan, Mohammed A. Mostajo-Radji, Ge Wang</dc:creator>
    </item>
    <item>
      <title>Stochastic Models of Neuronal Growth</title>
      <link>https://arxiv.org/abs/2205.10723</link>
      <description>arXiv:2205.10723v2 Announce Type: replace-cross 
Abstract: Neuronal circuits arise as axons and dendrites extend, navigate, and connect to target cells. Axonal growth, in particular, integrates deterministic guidance from substrate mechanics and geometry with stochastic fluctuations generated by signaling, molecular detection, cytoskeletal assembly, and growth cone dynamics. A comprehensive quantitative description of this process remains incomplete. We review stochastic models in which Langevin dynamics and the associates Fokker-Planck equation capture axonal motion and turning under combined biases and noise. Paired with experiments, these models yield key parameters, including effective diffusion (motility) coefficients, speed and angle distributions, mean-square displacement, and mechanical measures of cell-substrate coupling, thereby linking single-cell biophysics and intercellular interactions to collective growth statistics and network formation. We further couple the Fokker-Planck description to a mechanochemical actin-myosin-clutch model and perform a linear stability analysis of the resulting dynamics. Routh--Hurwitz criteria identify regimes of steady extension, damped oscillations, and Hopf bifurcations that generate sustained limit cycles. Together, these results clarify the mechanisms that govern axonal guidance and connectivity and inform the design of engineered substrates and neuroprosthetic scaffolds aimed at enhancing nerve repair and regeneration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.10723v2</guid>
      <category>physics.bio-ph</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 10 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cristian Staii</dc:creator>
    </item>
  </channel>
</rss>
