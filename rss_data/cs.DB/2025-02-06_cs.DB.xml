<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Feb 2025 02:46:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Accessible and Portable LLM Inference by Compiling Computational Graphs into SQL</title>
      <link>https://arxiv.org/abs/2502.02818</link>
      <description>arXiv:2502.02818v1 Announce Type: new 
Abstract: Serving large language models (LLMs) often demands specialized hardware, dedicated frameworks, and substantial development efforts, which restrict their accessibility, especially for edge devices and organizations with limited technical resources. We propose a novel compiler that translates LLM inference graphs into SQL queries, enabling relational databases, one of the most widely used and mature software systems globally, to serve as the runtime. By mapping neural operators such as matrix multiplication and attention into relational primitives like joins and aggregations, our approach leverages database capabilities, including disk-based data management and native caching. Supporting key transformer components, such as attention mechanisms and key-value caching, our system generates SQL pipelines for end-to-end LLM inference. Using the Llama3 family as a case study, we demonstrate up to 30x speedup in token generation for memory-constrained scenarios comparable to competitive CPU-based frameworks. Our work offers an accessible, portable, and efficient solution, facilitating the serving of LLMs across diverse deployment environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02818v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenbo Sun, Qiming Guo, Wenlu Wang, Rihan Hai</dc:creator>
    </item>
    <item>
      <title>Enforcing Demographic Coherence: A Harms Aware Framework for Reasoning about Private Data Release</title>
      <link>https://arxiv.org/abs/2502.02709</link>
      <description>arXiv:2502.02709v1 Announce Type: cross 
Abstract: The technical literature about data privacy largely consists of two complementary approaches: formal definitions of conditions sufficient for privacy preservation and attacks that demonstrate privacy breaches. Differential privacy is an accepted standard in the former sphere. However, differential privacy's powerful adversarial model and worst-case guarantees may make it too stringent in some situations, especially when achieving it comes at a significant cost to data utility. Meanwhile, privacy attacks aim to expose real and worrying privacy risks associated with existing data release processes but often face criticism for being unrealistic. Moreover, the literature on attacks generally does not identify what properties are necessary to defend against them.
  We address the gap between these approaches by introducing demographic coherence, a condition inspired by privacy attacks that we argue is necessary for data privacy. This condition captures privacy violations arising from inferences about individuals that are incoherent with respect to the demographic patterns in the data. Our framework focuses on confidence rated predictors, which can in turn be distilled from almost any data-informed process. Thus, we capture privacy threats that exist even when no attack is explicitly being carried out. Our framework not only provides a condition with respect to which data release algorithms can be analysed but suggests natural experimental evaluation methodologies that could be used to build practical intuition and make tangible assessment of risks. Finally, we argue that demographic coherence is weaker than differential privacy: we prove that every differentially private data release is also demographically coherent, and that there are demographically coherent algorithms which are not differentially private.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.02709v1</guid>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark Bun, Marco Carmosino, Palak Jain, Gabriel Kaptchuk, Satchit Sivakumar</dc:creator>
    </item>
    <item>
      <title>Group Trip Planning Query Problem with Multimodal Journey</title>
      <link>https://arxiv.org/abs/2502.03144</link>
      <description>arXiv:2502.03144v1 Announce Type: cross 
Abstract: In Group Trip Planning (GTP) Query Problem, we are given a city road network where a number of Points of Interest (PoI) have been marked with their respective categories (e.g., Cafeteria, Park, Movie Theater, etc.). A group of agents want to visit one PoI from every category from their respective starting location and once finished, they want to reach their respective destinations. This problem asks which PoI from every category should be chosen so that the aggregated travel cost of the group is minimized. This problem has been studied extensively in the last decade, and several solution approaches have been proposed. However, to the best of our knowledge, none of the existing studies have considered the different modalities of the journey, which makes the problem more practical. To bridge this gap, we introduce and study the GTP Query Problem with Multimodal Journey in this paper. Along with the other inputs of the GTP Query Problem, we are also given the different modalities of the journey that are available and their respective cost. Now, the problem is not only to select the PoIs from respective categories but also to select the modality of the journey. For this problem, we have proposed an efficient solution approach, which has been analyzed to understand their time and space requirements. A large number of experiments have been conducted using real-life datasets and the results have been reported. From the results, we observe that the PoIs and modality of journey recommended by the proposed solution approach lead to much less time and cost than the baseline methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03144v1</guid>
      <category>cs.MA</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dildar Ali, Suman Banerjee, Yamuna Prasad</dc:creator>
    </item>
    <item>
      <title>Data Dams: A Novel Framework for Regulating and Managing Data Flow in Large-Scale Systems</title>
      <link>https://arxiv.org/abs/2502.03218</link>
      <description>arXiv:2502.03218v1 Announce Type: cross 
Abstract: In the era of big data, managing dynamic data flows efficiently is crucial as traditional storage models struggle with real-time regulation and risk overflow. This paper introduces Data Dams, a novel framework designed to optimize data inflow, storage, and outflow by dynamically adjusting flow rates to prevent congestion while maximizing resource utilization. Inspired by physical dam mechanisms, the framework employs intelligent sluice controls and predictive analytics to regulate data flow based on system conditions such as bandwidth availability, processing capacity, and security constraints. Simulation results demonstrate that the Data Dam significantly reduces average storage levels (371.68 vs. 426.27 units) and increases total outflow (7999.99 vs. 7748.76 units) compared to static baseline models. By ensuring stable and adaptive outflow rates under fluctuating data loads, this approach enhances system efficiency, mitigates overflow risks, and outperforms existing static flow control strategies. The proposed framework presents a scalable solution for dynamic data management in large-scale distributed systems, paving the way for more resilient and efficient real-time processing architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03218v1</guid>
      <category>cs.IR</category>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Aly Bouke, Azizol Abdullah, Korhan Cengiz, Nikola Ivkovi\'c, Ivan Mihaljevi\'c, Mudathir Ahmed Mohamud, Ahmed Kowrina</dc:creator>
    </item>
    <item>
      <title>PalimpChat: Declarative and Interactive AI analytics</title>
      <link>https://arxiv.org/abs/2502.03368</link>
      <description>arXiv:2502.03368v1 Announce Type: cross 
Abstract: Thanks to the advances in generative architectures and large language models, data scientists can now code pipelines of machine-learning operations to process large collections of unstructured data. Recent progress has seen the rise of declarative AI frameworks (e.g., Palimpzest, Lotus, and DocETL) to build optimized and increasingly complex pipelines, but these systems often remain accessible only to expert programmers. In this demonstration, we present PalimpChat, a chat-based interface to Palimpzest that bridges this gap by letting users create and run sophisticated AI pipelines through natural language alone. By integrating Archytas, a ReAct-based reasoning agent, and Palimpzest's suite of relational and LLM-based operators, PalimpChat provides a practical illustration of how a chat interface can make declarative AI frameworks truly accessible to non-experts.
  Our demo system is publicly available online. At SIGMOD'25, participants can explore three real-world scenarios--scientific discovery, legal discovery, and real estate search--or apply PalimpChat to their own datasets. In this paper, we focus on how PalimpChat, supported by the Palimpzest optimizer, simplifies complex AI workflows such as extracting and analyzing biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03368v1</guid>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.IR</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chunwei Liu, Gerardo Vitagliano, Brandon Rose, Matt Prinz, David Andrew Samson, Michael Cafarella</dc:creator>
    </item>
    <item>
      <title>A Data Source Discovery Method using Several Domain Ontologies in P2P Environments (2014, IRIT research report)</title>
      <link>https://arxiv.org/abs/2411.19016</link>
      <description>arXiv:2411.19016v3 Announce Type: replace 
Abstract: Several data source discovery methods take into account the semantic heterogeneity problems by using several Domain Ontologies (DOs). However, most of them impose a topology of mapping links between DOs. DOs and mapping links are available on Internet but with an arbitrary topology. In this paper, we propose a data source Discovery method Adapted to any Mapping links Topology (DAMT) and taking into account semantic problems. Peers using the same DO are grouped in a Virtual Organization (VO) and connected in a Distributed Hash Table (DHT). Lookups within a same VO consists in a classical search in a DHT. Regarding the inter-VO discovery process, we propose an addressing system, based on the existing mapping links between DOs, to interconnect VOs. Furthermore, we adopt a lazy maintenance in order to reduce the number of messages required to update the system due to the dynamicity of peers. The performance analysis of the proposed method shows good results for inter-VO lookup queries. Also, it confirms a significant maintenance cost reduction when peers join and leave the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19016v3</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riad Mokadem (IRIT-PYRAMIDE, IRIT)</dc:creator>
    </item>
    <item>
      <title>RADx Data Hub: A Cloud Repository for FAIR, Harmonized COVID-19 Data</title>
      <link>https://arxiv.org/abs/2502.00265</link>
      <description>arXiv:2502.00265v2 Announce Type: replace 
Abstract: The COVID-19 pandemic highlighted the urgent need for robust systems to enable rapid data collection, integration, and analysis for public health responses. Existing approaches often relied on disparate, non-interoperable systems, creating bottlenecks in comprehensive analyses and timely decision-making. To address these challenges, the U.S. National Institutes of Health (NIH) launched the Rapid Acceleration of Diagnostics (RADx) initiative in 2020, with the RADx Data Hub, a centralized repository for de-identified and curated COVID-19 data, as its cornerstone. The RADx Data Hub hosts diverse study data, including clinical data, testing results, smart sensor outputs, self-reported symptoms, and information on social determinants of health. Built on cloud infrastructure, the RADx Data Hub integrates metadata standards, interoperable formats, and ontology-based tools to adhere to the FAIR (Findable, Accessible, Interoperable, Reusable) principles for data sharing. Initially developed for COVID-19 research, its architecture and processes are adaptable to other scientific disciplines. This paper provides an overview of the data hosted by the RADx Data Hub and describes the platform's capabilities and architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00265v2</guid>
      <category>cs.DB</category>
      <pubDate>Thu, 06 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcos Martinez-Romero, Matthew Horridge, Nilesh Mistry, Aubrie Weyhmiller, Jimmy K. Yu, Alissa Fujimoto, Aria Henry, Martin J. O'Connor, Ashley Sier, Stephanie Suber, Mete U. Akdogan, Yan Cao, Somu Valliappan, Joanna O. Mieczkowska, the RADx Data Hub team, Ashok Krishnamurthy, Michael A. Keller, Mark A. Musen</dc:creator>
    </item>
  </channel>
</rss>
