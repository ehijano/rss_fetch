<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Nov 2025 03:50:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Simple Circuit Extensions for XOR in PTIME</title>
      <link>https://arxiv.org/abs/2511.16903</link>
      <description>arXiv:2511.16903v1 Announce Type: new 
Abstract: The Minimum Circuit Size Problem for Partial Functions ($MCSP^*$) is hard assuming the Exponential Time Hypothesis (ETH) (Ilango, 2020). This breakthrough hardness result leveraged a characterization of the optimal $\{\land, \lor, \neg\}$ circuits for $n$-bit $OR$ ($OR_n$) and a reduction from the partial $f$-Simple Extension Problem where $f = OR_n$. It remains open to extend that reduction to show ETH-hardness of total $MCSP$. However, Ilango observed that the total $f$-Simple Extension Problem is easy whenever $f$ is computed by read-once formulas (like $OR_n$). Therefore, extending Ilango's proof to total $MCSP$ would require one to replace $OR_n$ with a slightly more complex but similarly well-understood Boolean function.
  This work shows that the $f$-Simple Extension problem remains easy when $f$ is the next natural candidate: $XOR_n$. We first develop a fixed-parameter tractable algorithm for the $f$-Simple Extension Problem that is efficient whenever the optimal circuits for $f$ are (1) linear in size, (2) polynomially "few" and efficiently enumerable in the truth-table size (up to isomorphism and permutation of inputs), and (3) all have constant bounded fan-out. $XOR_n$ satisfies all three of these conditions. When $\neg$ gates count towards circuit size, optimal $XOR_n$ circuits are binary trees of $n-1$ subcircuits computing $(\neg)XOR_2$ (Kombarov, 2011). We extend this characterization when $\neg$ gates do not contribute the circuit size. Thus, the $XOR$-Simple Extension Problem is in polynomial time under both measures of circuit complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16903v1</guid>
      <category>cs.CC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Carmosino, Ngu Dang, Tim Jackman</dc:creator>
    </item>
    <item>
      <title>A Lifting Theorem for Hybrid Classical-Quantum Communication Complexity</title>
      <link>https://arxiv.org/abs/2511.17227</link>
      <description>arXiv:2511.17227v1 Announce Type: new 
Abstract: We investigates a model of hybrid classical-quantum communication complexity, in which two parties first exchange classical messages and subsequently communicate using quantum messages. We study the trade-off between the classical and quantum communication for composed functions of the form $f\circ G^n$, where $f:\{0,1\}^n\to\{\pm1\}$ and $G$ is an inner product function of $\Theta(\log n)$ bits. To prove the trade-off, we establish a novel lifting theorem for hybrid communication complexity. This theorem unifies two previously separate lifting paradigms: the query-to-communication lifting framework for classical communication complexity and the approximate-degree-to-generalized-discrepancy lifting methods for quantum communication complexity. Our hybrid lifting theorem therefore offers a new framework for proving lower bounds in hybrid classical-quantum communication models.
  As a corollary, we show that any hybrid protocol communicating $c$ classical bits followed by $q$ qubits to compute $f\circ G^n$ must satisfy $c+q^2=\Omega\big(\max\{\mathrm{deg}(f),\mathrm{bs}(f)\}\cdot\log n\big)$, where $\mathrm{deg}(f)$ is the degree of $f$ and $\mathrm{bs}(f)$ is the block sensitivity of $f$. For read-once formula $f$, this yields an almost tight trade-off: either they have to exchange $\Theta\big(n\cdot\log n\big)$ classical bits or $\widetilde\Theta\big(\sqrt n\cdot\log n\big)$ qubits, showing that classical pre-processing cannot significantly reduce the quantum communication required. To the best of our knowledge, this is the first non-trivial trade-off between classical and quantum communication in hybrid two-way communication complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17227v1</guid>
      <category>cs.CC</category>
      <category>quant-ph</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xudong Wu, Guangxu Yang, Penghui Yao</dc:creator>
    </item>
    <item>
      <title>Lower Bounds for CSP Hierarchies Through Ideal Reduction</title>
      <link>https://arxiv.org/abs/2511.17272</link>
      <description>arXiv:2511.17272v1 Announce Type: new 
Abstract: We present a generic way to obtain level lower bounds for (promise) CSP hierarchies from degree lower bounds for algebraic proof systems. More specifically, we show that pseudo-reduction operators in the sense of Alekhnovich and Razborov [Proc. Steklov Inst. Math. 2003] can be used to fool the cohomological $k$-consistency algorithm. As applications, we prove optimal level lower bounds for $c$ vs. $\ell$-coloring for all $\ell \geq c \geq 3$, and give a simplified proof of the lower bounds for lax and null-constraining CSPs of Chan and Ng [STOC 2025].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17272v1</guid>
      <category>cs.CC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Conneryd, Yassine Ghannane, Shuo Pang</dc:creator>
    </item>
    <item>
      <title>Low-Sensitivity Matching via Sampling from Gibbs Distributions</title>
      <link>https://arxiv.org/abs/2511.16918</link>
      <description>arXiv:2511.16918v1 Announce Type: cross 
Abstract: In this work, we study the maximum matching problem from the perspective of sensitivity. The sensitivity of an algorithm $A$ on a graph $G$ is defined as the maximum Wasserstein distance between the output distributions of $A$ on $G$ and on $G - e$, where $G - e$ is the graph obtained by deleting an edge $e$ from $G$. The maximum is taken over all edges $e$, and the underlying metric for the Wasserstein distance is the Hamming distance.
  We first show that for any $\varepsilon &gt; 0$, there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\Delta^{O(1/\varepsilon)}$, where $\Delta$ is the maximum degree of the input graph. The algorithm is based on sampling from the Gibbs distribution over matchings and runs in time $O_{\varepsilon, \Delta}(m \log m)$, where $m$ is the number of edges in the graph. This result significantly improves the previously known sensitivity bounds.
  Next, we present significantly faster algorithms for planar and bipartite graphs as a function of $\varepsilon$ and $\Delta$, which run in time $\mathrm{poly}(n/\varepsilon)$. This improvement is achieved by designing a more efficient algorithm for sampling matchings from the Gibbs distribution in these graph classes, which improves upon the previous best in terms of running time.
  Finally, for general graphs with potentially unbounded maximum degree, we show that there exists a polynomial-time $(1 - \varepsilon)$-approximation algorithm with sensitivity $\sqrt{n} \cdot (\varepsilon^{-1} \log n)^{O(1/\varepsilon)}$, improving upon the previous best bound of $O(n^{1/(1+\varepsilon^2)})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16918v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuichi Yoshida, Zihan Zhang</dc:creator>
    </item>
    <item>
      <title>Fundamental Limitations of QAOA on Constrained Problems and a Route to Exponential Enhancement</title>
      <link>https://arxiv.org/abs/2511.17259</link>
      <description>arXiv:2511.17259v1 Announce Type: cross 
Abstract: We study fundamental limitations of the generic Quantum Approximate Optimization Algorithm (QAOA) on constrained problems where valid solutions form a low dimensional manifold inside the Boolean hypercube, and we present a provable route to exponential improvements via constraint embedding. Focusing on permutation constrained objectives, we show that the standard generic QAOA ansatz, with a transverse field mixer and diagonal r local cost, faces an intrinsic feasibility bottleneck: even after angle optimization, circuits whose depth grows at most linearly with n cannot raise the total probability mass on the feasible manifold much above the uniform baseline suppressed by the size of the full Hilber space. Against this envelope we introduce a minimal constraint enhanced kernel (CE QAOA) that operates directly inside a product one hot subspace and mixes with a block local XY Hamiltonian. For permutation constrained problems, we prove an angle robust, depth matched exponential enhancement where the ratio between the feasible mass from CE QAOA and generic QAOA grows exponentially in $n^2$ for all depths up to a linear fraction of n, under a mild polynomial growth condition on the interaction hypergraph. Thanks to the problem algorithm co design in the kernel construction, the techniques and guarantees extend beyond permutations to a broad class of NP-Hard constrained optimization problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17259v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.CE</category>
      <category>cs.DM</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinonso Onah, Kristel Michielsen</dc:creator>
    </item>
    <item>
      <title>The Complexity Classes of Hamming Distance Recoverable Robust Problems</title>
      <link>https://arxiv.org/abs/2209.06939</link>
      <description>arXiv:2209.06939v4 Announce Type: replace 
Abstract: In the well-known complexity class NP are combinatorial problems, whose optimization counterparts are important for many practical settings. These problems typically consider full knowledge about the input. In practical settings, however, uncertainty in the input data is a usual phenomenon, whereby this is normally not covered in optimization versions of NP problems. One concept to model the uncertainty in the input data, is recoverable robustness. The instance of the recoverable robust version of a combinatorial problem P is split into a base scenario $\sigma_0$ and an uncertainty scenario set $\textsf{S}$. The base scenario and all members of the uncertainty scenario set are instances of the original combinatorial problem P. The task is to calculate a solution $s_0$ for the base scenario $\sigma_0$ and solutions $s$ for all uncertainty scenarios $\sigma \in \textsf{S}$ such that $s_0$ and $s$ are not too far away from each other according to a distance measure, so $s_0$ can be easily adapted to $s$. This paper introduces Hamming Distance Recoverable Robustness, in which solutions $s_0$ and $s$ have to be calculated, such that $s_0$ and $s$ may only differ in at most $\kappa$ elements. We survey the complexity of Hamming distance recoverable robust versions of optimization problems, typically found in NP for different scenario encodings. The complexity is primarily situated in the lower levels of the polynomial hierarchy. The main contribution of the paper is a gadget reduction framework that shows that the recoverable robust versions of problems in a large class of combinatorial problems is $\Sigma^P_{3}$-complete. This class includes problems such as Vertex Cover, Coloring or Subset Sum. Additionally, we expand the results to $\Sigma^P_{2m+1}$-completeness for multi-stage recoverable robust problems with $m \in \mathbb{N}$ stages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.06939v4</guid>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gr\"une</dc:creator>
    </item>
    <item>
      <title>Enumeration and updates for conjunctive linear algebra queries through expressibility</title>
      <link>https://arxiv.org/abs/2310.04118</link>
      <description>arXiv:2310.04118v4 Announce Type: replace 
Abstract: Due to the importance of linear algebra and matrix operations in data analytics, there is significant interest in using relational query optimization and processing techniques for evaluating (sparse) linear algebra programs. In particular, in recent years close connections have been established between linear algebra programs and relational algebra that allow transferring optimization techniques of the latter to the former. In this paper, we ask ourselves which linear algebra programs in MATLANG correspond to the free-connex and q-hierarchical fragments of conjunctive first-order logic. Both fragments have desirable query processing properties: free-connex conjunctive queries support constant-delay enumeration after a linear-time preprocessing phase, and q-hierarchical conjunctive queries further allow constant-time updates. By characterizing the corresponding fragments of MATLANG, we hence identify the fragments of linear algebra programs that one can evaluate with constant-delay enumeration after linear-time preprocessing and with constant-time updates. To derive our results, we improve and generalize previous correspondences between MATLANG and relational algebra evaluated over semiring-annotated relations. In addition, we identify properties on semirings that allow to generalize the complexity bounds for free-connex and q-hierarchical conjunctive queries from Boolean annotations to general semirings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04118v4</guid>
      <category>cs.CC</category>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Mu\~noz, Cristian Riveros, Stijn Vansummeren</dc:creator>
    </item>
    <item>
      <title>Completeness in the Polynomial Hierarchy for many natural Problems in Bilevel and Robust Optimization</title>
      <link>https://arxiv.org/abs/2311.10540</link>
      <description>arXiv:2311.10540v4 Announce Type: replace 
Abstract: In bilevel and robust optimization we are concerned with combinatorial min-max problems, for example from the areas of min-max regret robust optimization, network interdiction, most vital vertex problems, blocker problems, and two-stage adjustable robust optimization. Even though these areas are well-researched for over two decades and one would naturally expect many (if not most) of the problems occurring in these areas to be complete for the classes $\Sigma^p_2$ or $\Sigma^p_3$ from the polynomial hierarchy, almost no hardness results in this regime are currently known. However, such complexity insights are important, since they imply that no polynomial-sized integer program for these min-max problems exist, and hence conventional IP-based approaches fail. We address this lack of knowledge by introducing over 70 new $\Sigma^p_2$-complete and $\Sigma^p_3$-complete problems. The majority of all earlier publications on $\Sigma^p_2$- and $\Sigma^p_3$-completeness in said areas are special cases of our meta-theorem. Precisely, we introduce a large list of problems for which the meta-theorem is applicable (including clique, vertex cover, knapsack, TSP, facility location and many more). We show that for every single of these problems, the corresponding min-max (i.e. interdiction/regret) variant is $\Sigma^p_2$- and the min-max-min (i.e. two-stage) variant is $\Sigma^p_3$-complete.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.10540v4</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Gr\"une, Lasse Wulf</dc:creator>
    </item>
    <item>
      <title>Debordering Closure Results in Determinantal and Pfaffian Ideals</title>
      <link>https://arxiv.org/abs/2511.16492</link>
      <description>arXiv:2511.16492v2 Announce Type: replace 
Abstract: One important question in algebraic complexity is understanding the complexity of polynomial ideals (Grochow, Bulletin of EATCS 131, 2020). Andrews and Forbes (STOC 2022) studied the determinantal ideals $I^{\det}_{n,m,r}$ generated by the $r\times r$ minors of $n\times m$ matrices. Over fields of characteristic zero or of sufficiently large characteristic, they showed that for any nonzero $f \in I^{\det}_{n,m,r}$, the determinant of a $t \times t$ matrix of variables with $t = \Theta(r^{1/3})$ is approximately computed by a constant-depth, polynomial-size $f$-oracle algebraic circuit, in the sense that the determinant lies in the border of such circuits. An analogous result was also obtained for Pfaffians in the same paper.
  In this work, we deborder the result of Andrews and Forbes by showing that when $f$ has polynomial degree, the determinant is in fact exactly computed by a constant-depth, polynomial-size $f$-oracle algebraic circuit. We further establish an analogous result for Pfaffian ideals.
  Our results are established using the isolation lemma, combined with a careful analysis of straightening-law expansions of polynomials in determinantal and Pfaffian ideals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16492v2</guid>
      <category>cs.CC</category>
      <category>math.AC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anakin Dey, Zeyu Guo</dc:creator>
    </item>
    <item>
      <title>Uniform Value and Decidability in Ergodic Blind Stochastic Games</title>
      <link>https://arxiv.org/abs/2405.12583</link>
      <description>arXiv:2405.12583v2 Announce Type: replace-cross 
Abstract: We study a class of two-player zero-sum stochastic games known as \textit{blind stochastic games}, where players neither observe the state nor receive any information about it during the game. A central concept for analyzing long-duration stochastic games is the \textit{uniform value}. A game has a uniform value $v$ if for every $\varepsilon&gt;0$, Player 1 (resp., Player 2) has a strategy such that, for all sufficiently large $n$, his average payoff over $n$ stages is at least $v-\varepsilon$ (resp., at most $v+\varepsilon$). Prior work has shown that the uniform value may not exist in general blind stochastic games. To address this, we introduce a subclass called \textit{ergodic blind stochastic games}, defined by imposing an ergodicity condition on the state transitions. For this subclass, we prove the existence of the uniform value and provide an algorithm to approximate it, establishing the \textit{decidability} of the approximation problem. Notably, this decidability result is novel even in the single-player setting of Partially Observable Markov Decision Processes (POMDPs). Furthermore, we show that no algorithm can compute the uniform value exactly, emphasizing the tightness of our result. Finally, we establish that the uniform value is independent of the initial belief.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12583v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krishnendu Chatterjee, David Lurie, Raimundo Saona, Bruno Ziliotto</dc:creator>
    </item>
    <item>
      <title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title>
      <link>https://arxiv.org/abs/2509.18057</link>
      <description>arXiv:2509.18057v5 Announce Type: replace-cross 
Abstract: Can AI based methods help us make advances in complexity theory? We provide evidence towards answering this in the affirmative, using AlphaEvolve (an LLM code mutation agent) to obtain new results in three settings:
  a) We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ vertices, and our upper bounds are obtained via analytical arguments.
  b) We obtain new inapproximability results for MAX-4-CUT and MAX-3-CUT, proving that it is NP-hard to approximate them within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of the SOTA of $16/17$ that relies on a custom PCP (rather than a reduction from ``standard'' H{\aa}stad-style PCPs).
  c) Inapproximability for the metric Traveling Salesman Problem (TSP): We show that it is NP-hard to approximate the minimum cost tour within a factor of $111/110$ using AlphaEvolve to discover a new gadget, thus improving the SOTA of $117/116$. Along the way, we provide new modular soundness and completeness arguments that can be of independent interest.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (sometimes requiring time exponential in the size of the construction). We used AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$ for our gadgets). Our results suggest that gadget based proofs would benefit from a pass through AI-based tools to obtain stronger results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18057v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</dc:creator>
    </item>
  </channel>
</rss>
