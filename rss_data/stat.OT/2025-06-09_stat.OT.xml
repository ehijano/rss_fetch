<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Jun 2025 02:56:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On the stability of global forecasting models</title>
      <link>https://arxiv.org/abs/2506.05776</link>
      <description>arXiv:2506.05776v1 Announce Type: cross 
Abstract: Forecast stability, that is the consistency of predictions over time, is essential in business settings where sudden shifts in forecasts can disrupt planning and erode trust in predictive systems. Despite its importance, stability is often overlooked in favor of accuracy, particularly in global forecasting models. In this study, we evaluate the stability of point and probabilistic forecasts across different retraining frequencies and ensemble strategies using two large retail datasets (M5 and VN1). To do this, we introduce a new metric for probabilistic stability (MQC) and analyze ten different global models and four ensemble configurations. The results show that less frequent retraining not only preserves but often improves forecast stability, while ensembles, especially those combining diverse pool of models, further enhance consistency without sacrificing accuracy. These findings challenge the need for continuous retraining and highlight ensemble diversity as a key factor in reducing forecast stability. The study promotes a shift toward stability-aware forecasting practices, offering practical guidelines for building more robust and sustainable prediction systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05776v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
    <item>
      <title>Conformal-DP: Data Density Aware Privacy on Riemannian Manifolds via Conformal Transformation</title>
      <link>https://arxiv.org/abs/2504.20941</link>
      <description>arXiv:2504.20941v2 Announce Type: replace-cross 
Abstract: Differential Privacy (DP) enables privacy-preserving data analysis by adding calibrated noise. While recent works extend DP to curved manifolds (e.g., diffusion-tensor MRI, social networks) by adding geodesic noise, these assume uniform data distribution. This assumption is not always practical, hence these approaches may introduce biased noise and suboptimal privacy-utility trade-offs for non-uniform data. To address this issue, we propose \emph{Conformal}-DP that utilizes conformal transformations on Riemannian manifolds. This approach locally equalizes sample density and redefines geodesic distances while preserving intrinsic manifold geometry. Our theoretical analysis demonstrates that the conformal factor, which is derived from local kernel density estimates, is data density-aware. We show that under these conformal metrics, \emph{Conformal}-DP satisfies $\varepsilon$-differential privacy on any complete Riemannian manifold and offers a closed-form expected geodesic error bound dependent only on the maximal density ratio, and not global curvature. We show through experiments on synthetic and real-world datasets that our mechanism achieves superior privacy-utility trade-offs, particularly for heterogeneous manifold data, and also is beneficial for homogeneous datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.20941v2</guid>
      <category>cs.CR</category>
      <category>math.DG</category>
      <category>stat.OT</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peilin He, Liou Tang, M. Amin Rahimian, James Joshi</dc:creator>
    </item>
  </channel>
</rss>
