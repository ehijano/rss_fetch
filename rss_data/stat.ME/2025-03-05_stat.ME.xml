<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Mar 2025 02:52:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Integrating Misclassified EHR Outcomes with Validated Outcomes from a Non-probability Sample</title>
      <link>https://arxiv.org/abs/2503.02071</link>
      <description>arXiv:2503.02071v1 Announce Type: new 
Abstract: Although increasingly used for research, electronic health records (EHR) often lack gold-standard assessment of key data elements. Linking EHRs to other data sources with higher-quality measurements can improve statistical inference, but such analyses must account for selection bias if the linked data source arises from a non-probability sample. We propose a set of novel estimators targeting the average treatment effect (ATE) that combine information from binary outcomes measured with error in a large, population-representative EHR database with gold-standard outcomes obtained from a smaller validation sample subject to selection bias. We evaluate our approach in extensive simulations and an analysis of data from the Adult Changes in Thought (ACT) study, a longitudinal study of incident dementia in a cohort of Kaiser Permanente Washington members with linked EHR data. For a subset of deceased ACT participants who consented to brain autopsy prior to death, gold-standard measures of Alzheimer's disease neuropathology are available. Our proposed estimators reduced bias and improved efficiency for the ATE, facilitating valid inference with EHR data when key data elements are ascertained with error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02071v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jenny Shen, Dane Isenberg, Kristin A. Linn, Rebecca A. Hubbard</dc:creator>
    </item>
    <item>
      <title>PanelMatch: Matching Methods for Causal Inference with Time-Series Cross-Section Data</title>
      <link>https://arxiv.org/abs/2503.02073</link>
      <description>arXiv:2503.02073v1 Announce Type: new 
Abstract: Analyzing time-series cross-sectional (also known as longitudinal or panel) data is an important process across a number of fields, including the social sciences, economics, finance, and medicine. PanelMatch is an R package that implements a set of tools enabling researchers to apply matching methods for causal inference with time-series cross-sectional data. Relative to other commonly used methods for longitudinal analyses, like regression with fixed effects, the matching-based approach implemented in PanelMatch makes fewer parametric assumptions and offers more diagnostics. In this paper, we discuss the PanelMatch package, showing users a recommended pipeline for doing causal inference analysis with it and highlighting useful diagnostic and visualization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02073v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam Rauh, In Song Kim, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>What Influences the Field Goal Attempts of Professional Players? Analysis of Basketball Shot Charts via Log Gaussian Cox Processes with Spatially Varying Coefficients</title>
      <link>https://arxiv.org/abs/2503.02137</link>
      <description>arXiv:2503.02137v1 Announce Type: new 
Abstract: Basketball shot charts provide valuable information regarding local patterns of in-game performance to coaches, players, sports analysts, and statisticians. The spatial patterns of where shots were attempted and whether the shots were successful suggest options for offensive and defensive strategies as well as historical summaries of performance against particular teams and players. The data represent a marked spatio-temporal point process with locations representing locations of attempted shots and an associated mark representing the shot's outcome (made/missed). Here, we develop a Bayesian log Gaussian Cox process model allowing joint analysis of the spatial pattern of locations and outcomes of shots across multiple games. We build a hierarchical model for the log intensity function using Gaussian processes, and allow spatially varying effects for various game-specific covariates. We aim to model the spatial relative risk under different covariate values. For inference via posterior simulation, we design a Markov chain Monte Carlo (MCMC) algorithm based on a kernel convolution approach. We illustrate the proposed method using extensive simulation studies. A case study analyzing the shot data of NBA legends Stephen Curry, LeBron James, and Michael Jordan highlights the effectiveness of our approach in real-world scenarios and provides practical insights into optimizing shooting strategies by examining how different playing conditions, game locations, and opposing team strengths impact shooting efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02137v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Cao, Qingpo Cai, Lance A. Waller, DeMarc A. Hickson, Guanyu Hu, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Generalized Tree-Informed Mixed Model Regression</title>
      <link>https://arxiv.org/abs/2503.02266</link>
      <description>arXiv:2503.02266v1 Announce Type: new 
Abstract: The standard regression tree method applied to observations within clusters poses both methodological and implementation challenges. Effectively leveraging these data requires methods that account for both individual-level and sample-level effects. We propose Generalized Tree-Informed Mixed Model (GTIMM), which replaces the linear fixed effect in a generalized linear mixed model (GLMM) with the output of a regression tree. Traditional parameter estimation and prediction techniques, such as the expectation-maximization algorithm, scale poorly in high-dimensional settings, creating a computational bottleneck. To address this, we employ a quasi-likelihood framework with stochastic gradient descent for optimized parameter estimation. Additionally, we establish a theoretical bound for the mean squared prediction error. The predictive performance of our method is evaluated through simulations and compared with existing approaches. Finally, we apply our model to predict country-level GDP based on trade, foreign direct investment, unemployment, inflation, and geographic region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02266v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremiah Allis, Xin Jin, Riddhi Ghosh</dc:creator>
    </item>
    <item>
      <title>Nonparametric Sequential Change-point Detection on High Order Compositional Time Series Models with Exogenous Variables</title>
      <link>https://arxiv.org/abs/2503.02349</link>
      <description>arXiv:2503.02349v1 Announce Type: new 
Abstract: Sequential change-point detection for time series is widely used in data monitoring in practice. In this work, we focus on sequential change-point detection on high-order compositional time series models. Under the regularity conditions, we prove that a process following the generalized Beta AR(p) model with exogenous variables is stationary and ergodic. We develop a nonparametric sequential change-point detection method for the generalized Beta AR(p) model, which does not rely on any strong assumptions about the sources of the change points. We show that the power of the test converges to one given that the amount of initial observations is large enough. We apply the nonparametric method to a rate of automobile crashes with alcohol involved, which is recorded monthly from January 2010 to December 2020; the exogenous variable is the price level of alcoholic beverages, which has a change point around August 2019. We fit a generalized Beta AR(p) model to the crash rate sequence, and we use the nonparametric sequential change-point detection method to successfully detect the change point.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02349v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yajun Liu, Beth Andrews</dc:creator>
    </item>
    <item>
      <title>Monge-Kantorovich quantiles and ranks for image data</title>
      <link>https://arxiv.org/abs/2503.02427</link>
      <description>arXiv:2503.02427v1 Announce Type: new 
Abstract: This paper defines quantiles, ranks and statistical depths for image data by leveraging ideas from measure transportation. The first step is to embed a distribution of images in a tangent space, with the framework of linear optimal transport. Therein, Monge-Kantorovich quantiles are shown to provide a meaningful ordering of image data, with outward images having unusual shapes. Numerical experiments showcase the relevance of the proposed procedure, for descriptive analysis, outlier detection or statistical testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02427v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gauthier Thurin (ENS-PSL)</dc:creator>
    </item>
    <item>
      <title>Robust Multi-Source Domain Adaptation under Label Shift</title>
      <link>https://arxiv.org/abs/2503.02506</link>
      <description>arXiv:2503.02506v1 Announce Type: new 
Abstract: As the volume of data continues to expand, it becomes increasingly common for data to be aggregated from multiple sources. Leveraging multiple sources for model training typically achieves better predictive performance on test datasets. Unsupervised multi-source domain adaptation aims to predict labels of unlabeled samples in the target domain by using labeled samples from source domains. This work focuses on robust multi-source domain adaptation for multi-category classification problems against the heterogeneity of label shift and data contamination. We investigate a domain-weighted empirical risk minimization framework for robust estimation of the target domain's class proportion. Inspired by outlier detection techniques, we propose a refinement procedure within this framework. With the estimated class proportion, robust classifiers for the target domain can be constructed. Theoretically, we study the finite-sample error bounds of the domain-weighted empirical risk minimization and highlight the improvement of the refinement step. Numerical simulations and real-data applications demonstrate the superiority of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02506v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Congbin Xu, Chengde Qian, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Fast and robust invariant generalized linear models</title>
      <link>https://arxiv.org/abs/2503.02611</link>
      <description>arXiv:2503.02611v1 Announce Type: new 
Abstract: Statistical integration of diverse data sources is an essential step in the building of generalizable prediction tools, especially in precision health. The invariant features model is a new paradigm for multi-source data integration which posits that a small number of covariates affect the outcome identically across all possible environments. Existing methods for estimating invariant effects suffer from immense computational costs or only offer good statistical performance under strict assumptions. In this work, we provide a general framework for estimation under the invariant features model that is computationally efficient and statistically flexible. We also provide a robust extension of our proposed method to protect against possibly corrupted or misspecified data sources. We demonstrate the robust properties of our method via simulations, and use it to build a transferable prediction model for end stage renal disease using electronic health records from the All of Us research program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02611v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parker Knight, Ndey Isatou Jobe, Rui Duan</dc:creator>
    </item>
    <item>
      <title>Seeded Poisson Factorization: Leveraging domain knowledge to fit topic models</title>
      <link>https://arxiv.org/abs/2503.02741</link>
      <description>arXiv:2503.02741v1 Announce Type: new 
Abstract: Topic models are widely used for discovering latent thematic structures in large text corpora, yet traditional unsupervised methods often struggle to align with predefined conceptual domains. This paper introduces Seeded Poisson Factorization (SPF), a novel approach that extends the Poisson Factorization framework by incorporating domain knowledge through seed words. SPF enables a more interpretable and structured topic discovery by modifying the prior distribution of topic-specific term intensities, assigning higher initial rates to predefined seed words. The model is estimated using variational inference with stochastic gradient optimization, ensuring scalability to large datasets.
  We apply SPF to an Amazon customer feedback dataset, leveraging predefined product categories as guiding structures. Our evaluation demonstrates that SPF achieves superior classification performance compared to alternative guided topic models, particularly in terms of computational efficiency and predictive performance. Furthermore, robustness checks highlight SPF's ability to adaptively balance domain knowledge and data-driven topic discovery, even in cases of imperfect seed word selection. These results establish SPF as a powerful and scalable alternative for integrating expert knowledge into topic modeling, enhancing both interpretability and efficiency in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02741v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bernd Prostmaier, Jan V\'avra, Bettina Gr\"un, Paul Hofmarcher</dc:creator>
    </item>
    <item>
      <title>A Statistical Interpretation of Multi-Item Rating and Recommendation Problems</title>
      <link>https://arxiv.org/abs/2503.02786</link>
      <description>arXiv:2503.02786v1 Announce Type: new 
Abstract: Ordinal user-provided ratings across multiple items are frequently encountered in both scientific and commercial applications. Whilst recommender systems are known to do well on these type of data from a predictive point of view, their typical reliance on large sample sizes and frequent lack of interpretability and uncertainty quantification limits their applicability in inferential problems. Taking a fully Bayesian approach, this article introduces a novel statistical method that is designed with interpretability and uncertainty quantification in mind. Whilst parametric assumptions ensure that the method is applicable to data with modest sample sizes, the model is simultaneously designed to remain flexible in order to handle a wide variety of situations. Model performance, i.e. parameter estimation and prediction, is shown by means of a simulation study, both on simulated data and against commonly used recommender systems on real data. These simulations indicate that the proposed method performs competitively. Finally, to illustrate the applicability of the proposed method on real life problems that are of interest to economists, the method is applied on speed dating data, where novel insights into the partner preference problem are obtained. An R package containing the proposed methodology can be found on https://CRAN.R-project.org/package=StatRec.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02786v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes</dc:creator>
    </item>
    <item>
      <title>Exact matching as an alternative to propensity score matching</title>
      <link>https://arxiv.org/abs/2503.02850</link>
      <description>arXiv:2503.02850v1 Announce Type: new 
Abstract: The comparison of different medical treatments from observational studies or across different clinical studies is often biased by confounding factors such as systematic differences in patient demographics or in the inclusion criteria for the trials. Propensity score matching is a popular method to adjust for such confounding. It compares weighted averages of patient responses. The weights are calculated from logistic regression models with the intention to reduce differences between the confounders in the treatment groups. However, the groups are only "roughly matched" with no generally accepted principle to determine when a match is "good enough".
  In this manuscript, we propose an alternative approach to the matching problem by considering it as a constrained optimization problem. We investigate the conditions for exact matching in the sense that the average values of confounders are identical in the treatment groups after matching. Our approach is similar to the matching-adjusted indirect comparison approach by Signorovitch et al. (2010) but with two major differences: First, we do not impose any specific functional form on the matching weights; second, the proposed approach can be applied to individual patient data from several treatment groups as well as to a mix of individual patient and aggregated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02850v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ekkehard Glimm, Lillian Yau</dc:creator>
    </item>
    <item>
      <title>Correcting Mode Proportion Bias in Generalized Bayesian Inference via a Weighted Kernel Stein Discrepancy</title>
      <link>https://arxiv.org/abs/2503.02108</link>
      <description>arXiv:2503.02108v1 Announce Type: cross 
Abstract: Generalized Bayesian Inference (GBI) provides a flexible framework for updating prior distributions using various loss functions instead of the traditional likelihoods, thereby enhancing the model robustness to model misspecification. However, GBI often suffers the problem associated with intractable likelihoods. Kernelized Stein Discrepancy (KSD), as utilized in a recent study, addresses this challenge by relying only on the gradient of the log-likelihood. Despite this innovation, KSD-Bayes suffers from critical pathologies, including insensitivity to well-separated modes in multimodal posteriors. To address this limitation, we propose a weighted KSD method that retains computational efficiency while effectively capturing multimodal structures. Our method improves the GBI framework for handling intractable multimodal posteriors while maintaining key theoretical properties such as posterior consistency and asymptotic normality. Experimental results demonstrate that our method substantially improves mode sensitivity compared to standard KSD-Bayes, while retaining robust performance in unimodal settings and in the presence of outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02108v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elham Afzali, Saman Muthukumarana, Liqun Wang</dc:creator>
    </item>
    <item>
      <title>Accounting for Missing Data in Public Health Research Using a Synthesis of Statistical and Mathematical Models</title>
      <link>https://arxiv.org/abs/2503.02789</link>
      <description>arXiv:2503.02789v1 Announce Type: cross 
Abstract: Introduction: Missing data is a challenge to medical research. Accounting for missing data by imputing or weighting conditional on covariates relies on the variable with missingness being observed at least some of the time for all unique covariate values. This requirement is referred to as positivity, and violations can result in bias. Here, we review a novel approach to addressing positivity violations in the context of systolic blood pressure. Methods: To illustrate the proposed approach, we estimate the mean systolic blood pressure among children and adolescents aged 2-17 years old in the United States using data from 2017-2018 National Health and Nutrition Examination Survey (NHANES). As blood pressure was never measured for those aged 2-7, there exists a positivity violation by design. Using a recently proposed synthesis of statistical and mathematical models, we integrate external information with NHANES to address our motivating question. Results: With the synthesis model, the estimated mean systolic blood pressure was 100.5 (95\% confidence interval: 99.9, 101.0), which is notably lower than either a complete-case analysis or extrapolation from a statistical model. The synthesis results were supported by a diagnostic comparing the performance of the mathematical model in the positive region. Conclusion: Positivity violations pose a threat to quantitative medical research, and standard approaches to addressing nonpositivity rely on restrictive untestable assumptions. Using a synthesis model, like the one detailed here, offers a viable alternative through integration of external information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02789v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul N Zivich, Bonnie E Shook-Sa, Stephen R Cole, Eric T Lofgren, Jessie K Edwards</dc:creator>
    </item>
    <item>
      <title>Spike-and-Slab Posterior Sampling in High Dimensions</title>
      <link>https://arxiv.org/abs/2503.02798</link>
      <description>arXiv:2503.02798v1 Announce Type: cross 
Abstract: Posterior sampling with the spike-and-slab prior [MB88], a popular multimodal distribution used to model uncertainty in variable selection, is considered the theoretical gold standard method for Bayesian sparse linear regression [CPS09, Roc18]. However, designing provable algorithms for performing this sampling task is notoriously challenging. Existing posterior samplers for Bayesian sparse variable selection tasks either require strong assumptions about the signal-to-noise ratio (SNR) [YWJ16], only work when the measurement count grows at least linearly in the dimension [MW24], or rely on heuristic approximations to the posterior. We give the first provable algorithms for spike-and-slab posterior sampling that apply for any SNR, and use a measurement count sublinear in the problem dimension. Concretely, assume we are given a measurement matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ and noisy observations $\mathbf{y} = \mathbf{X}\mathbf{\theta}^\star + \mathbf{\xi}$ of a signal $\mathbf{\theta}^\star$ drawn from a spike-and-slab prior $\pi$ with a Gaussian diffuse density and expected sparsity k, where $\mathbf{\xi} \sim \mathcal{N}(\mathbb{0}_n, \sigma^2\mathbf{I}_n)$. We give a polynomial-time high-accuracy sampler for the posterior $\pi(\cdot \mid \mathbf{X}, \mathbf{y})$, for any SNR $\sigma^{-1}$ &gt; 0, as long as $n \geq k^3 \cdot \text{polylog}(d)$ and $X$ is drawn from a matrix ensemble satisfying the restricted isometry property. We further give a sampler that runs in near-linear time $\approx nd$ in the same setting, as long as $n \geq k^5 \cdot \text{polylog}(d)$. To demonstrate the flexibility of our framework, we extend our result to spike-and-slab posterior sampling with Laplace diffuse densities, achieving similar guarantees when $\sigma = O(\frac{1}{k})$ is bounded.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02798v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Syamantak Kumar, Purnamrita Sarkar, Kevin Tian, Yusong Zhu</dc:creator>
    </item>
    <item>
      <title>Inductive randomness predictors</title>
      <link>https://arxiv.org/abs/2503.02803</link>
      <description>arXiv:2503.02803v1 Announce Type: cross 
Abstract: This paper introduces inductive randomness predictors, which form a superset of inductive conformal predictors. Its focus is on a very simple special case, binary inductive randomness predictors. It is interesting that binary inductive randomness predictors have an advantage over inductive conformal predictors, although they also have a serious disadvantage. This advantage will allow us to reach the surprising conclusion that non-trivial inductive conformal predictors are inadmissible in the sense of statistical decision theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02803v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
    <item>
      <title>Unsupervised Attributed Dynamic Network Embedding with Stability Guarantees</title>
      <link>https://arxiv.org/abs/2503.02859</link>
      <description>arXiv:2503.02859v1 Announce Type: cross 
Abstract: Stability for dynamic network embeddings ensures that nodes behaving the same at different times receive the same embedding, allowing comparison of nodes in the network across time. We present attributed unfolded adjacency spectral embedding (AUASE), a stable unsupervised representation learning framework for dynamic networks in which nodes are attributed with time-varying covariate information. To establish stability, we prove uniform convergence to an associated latent position model. We quantify the benefits of our dynamic embedding by comparing with state-of-the-art network representation learning methods on three real attributed networks. To the best of our knowledge, AUASE is the only attributed dynamic embedding that satisfies stability guarantees without the need for ground truth labels, which we demonstrate provides significant improvements for link prediction and node classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02859v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Ceccherini, Ian Gallagher, Andrew Jones, Daniel Lawson</dc:creator>
    </item>
    <item>
      <title>Wilcoxon-type Multivariate Cluster Elastic Net</title>
      <link>https://arxiv.org/abs/2209.13354</link>
      <description>arXiv:2209.13354v2 Announce Type: replace 
Abstract: We propose a method for high dimensional multivariate regression that is robust to random error distributions that are heavy-tailed or contain outliers, while preserving estimation accuracy in normal random error distributions. We extend the Wilcoxon-type regression to a multivariate regression model as a tuning-free approach to robustness. Furthermore, the proposed method regularizes the L1 and L2 terms of the clustering based on k-means, which is extended from the multivariate cluster elastic net. The estimation of the regression coefficient and variable selection are produced simultaneously. Moreover, considering the relationship among the correlation of response variables through the clustering is expected to improve the estimation performance. Numerical simulation demonstrates that our proposed method overperformed the multivariate cluster method and other methods of multiple regression in the case of heavy-tailed error distribution and outliers. It also showed stability in normal error distribution. Finally, we confirm the efficacy of our proposed method using a data example for the gene associated with breast cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.13354v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2025.129358</arxiv:DOI>
      <dc:creator>Mayu Hiraishi, Kensuke Tanioka, Hiroshi Yadohisa</dc:creator>
    </item>
    <item>
      <title>Regularized Principal Spline Functions to Mitigate Spatial Confounding</title>
      <link>https://arxiv.org/abs/2403.05373</link>
      <description>arXiv:2403.05373v2 Announce Type: replace 
Abstract: This paper proposes a new approach to address the problem of unmeasured confounding in spatial designs. Spatial confounding occurs when some confounding variables are unobserved and not included in the model, leading to distorted inferential results about the effect of an exposure on an outcome. We show the relationship existing between the confounding bias of a non-spatial model and that of a semi-parametric model that includes a basis matrix to represent the unmeasured confounder conditional on the exposure. This relationship holds for any basis expansion, however it is shown that using the semi-parametric approach guarantees a reduction in the confounding bias only under certain circumstances, which are related to the spatial structures of the exposure and the unmeasured confounder, the type of basis expansion utilized, and the regularization mechanism. To adjust for spatial confounding, and therefore try to recover the effect of interest, we propose a Bayesian semi-parametric regression model, where an expansion matrix of principal spline basis functions is used to approximate the unobserved factor, and spike-and-slab priors are imposed on the respective expansion coefficients in order to select the most important bases. From the results of an extensive simulation study, we conclude that our proposal is able to reduce the confounding bias more than competing approaches, and it also seems more robust to bias amplification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05373v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlo Zaccardi, Pasquale Valentini, Luigi Ippoliti, Alexandra M. Schmidt</dc:creator>
    </item>
    <item>
      <title>Targeted Sequential Indirect Experiment Design</title>
      <link>https://arxiv.org/abs/2405.19985</link>
      <description>arXiv:2405.19985v2 Announce Type: replace 
Abstract: Scientific hypotheses typically concern specific aspects of complex, imperfectly understood or entirely unknown mechanisms, such as the effect of gene expression levels on phenotypes or how microbial communities influence environmental health. Such queries are inherently causal (rather than purely associational), but in many settings, experiments can not be conducted directly on the target variables of interest, but are indirect. Therefore, they perturb the target variable, but do not remove potential confounding factors. If, additionally, the resulting experimental measurements are multi-dimensional and the studied mechanisms nonlinear, the query of interest is generally not identified. We develop an adaptive strategy to design indirect experiments that optimally inform a targeted query about the ground truth mechanism in terms of sequentially narrowing the gap between an upper and lower bound on the query. While the general formulation consists of a bi-level optimization procedure, we derive an efficiently estimable analytical kernel-based estimator of the bounds for the causal effect, a query of key interest, and demonstrate the efficacy of our approach in confounded, multivariate, nonlinear synthetic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19985v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elisabeth Ailer, Niclas Dern, Jason Hartford, Niki Kilbertus</dc:creator>
    </item>
    <item>
      <title>Improving Genomic Prediction using High-dimensional Secondary Phenotypes: the Genetic Latent Factor Approach</title>
      <link>https://arxiv.org/abs/2408.09876</link>
      <description>arXiv:2408.09876v2 Announce Type: replace 
Abstract: Decreasing costs and new technologies have led to an increase in the amount of data available to plant breeding programs. High-throughput phenotyping (HTP) platforms routinely generate high-dimensional datasets of secondary features that may be used to improve genomic prediction accuracy. However, integration of these data comes with challenges such as multicollinearity, parameter estimation in $p &gt; n$ settings, and the computational complexity of many standard approaches. Several methods have emerged to analyze such data, but interpretation of model parameters often remains challenging. We propose genetic latent factor best linear unbiased prediction (glfBLUP), a prediction pipeline that reduces the dimensionality of the original secondary HTP data using generative factor analysis. In short, glfBLUP uses redundancy filtered and regularized genetic and residual correlation matrices to fit a maximum likelihood factor model and estimate genetic latent factor scores. These latent factors are subsequently used in multi-trait genomic prediction. Our approach performs better than alternatives in extensive simulations and a real-world application, while producing easily interpretable and biologically relevant parameters. We discuss several possible extensions and highlight glfBLUP as the basis for a flexible and modular multi-trait genomic prediction framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09876v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Killian A. C. Melsen, Jonathan F. Kunst, Jos\'e Crossa, Margaret R. Krause, Fred A. van Eeuwijk, Willem Kruijer, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>Exact Bayesian Inference for Multivariate Spatial Data of Any Size with Application to Air Pollution Monitoring</title>
      <link>https://arxiv.org/abs/2410.02655</link>
      <description>arXiv:2410.02655v3 Announce Type: replace 
Abstract: Fine particulate matter and aerosol optical thickness are of interest to atmospheric scientists for understanding air quality and its various health/environmental impacts. The available data are extremely large, making uncertainty quantification in a fully Bayesian framework quite difficult, as traditional implementations do not scale reasonably to the size of the data. We specifically consider roughly 8 million observations obtained from NASA's Moderate Resolution Imaging Spectroradiometer (MODIS) instrument. To analyze data on this scale, we introduce Scalable Multivariate Exact Posterior Regression (SM-EPR) which combines the recently introduced data subset approach and Exact Posterior Regression (EPR). EPR is a new Bayesian hierarchical model where it is possible to sample independent replicates of fixed and random effects directly from the posterior without the use of Markov chain Monte Carlo (MCMC). We extend EPR to the multivariate spatial context, where the multiple variables may be distributed according to different distributions. The combination of the data subset approach with EPR allows one to perform exact Bayesian inference without MCMC for effectively any sample size. Additional motivation is provided via technical results illustrating favorable Kullback-Leibler and covariance properties. We demonstrate SM-EPR using a motivating big remote sensing data application and provide several simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02655v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Madelyn Clinch, Jonathan R. Bradley</dc:creator>
    </item>
    <item>
      <title>Scalable piecewise smoothing with BART</title>
      <link>https://arxiv.org/abs/2411.07984</link>
      <description>arXiv:2411.07984v2 Announce Type: replace 
Abstract: Although it is an extremely effective, easy-to-use, and increasingly popular tool for nonparametric regression, the Bayesian Additive Regression Trees (BART) model is limited by the fact that it can only produce discontinuous output. Initial attempts to overcome this limitation were based on regression trees that output Gaussian Processes instead of constants. Unfortunately, implementations of these extensions cannot scale to large datasets. We propose ridgeBART, an extension of BART built with trees that output linear combinations of ridge functions (i.e., a composition of an affine transformation of the inputs and non-linearity); that is, we build a Bayesian ensemble of localized neural networks with a single hidden layer. We develop a new MCMC sampler that updates trees in linear time and establish posterior contraction rates for estimating piecewise anisotropic H\"{o}lder functions and nearly minimax-optimal rates for estimating isotropic H\"{o}lder functions. We demonstrate ridgeBART's effectiveness on synthetic data and use it to estimate the probability that a professional basketball player makes a shot from any location on the court in a spatially smooth fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07984v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Yee, Soham Ghosh, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Scalable calibration of individual-based epidemic models through categorical approximations</title>
      <link>https://arxiv.org/abs/2501.03950</link>
      <description>arXiv:2501.03950v2 Announce Type: replace 
Abstract: Traditional compartmental models capture population-level dynamics but fail to characterize individual-level risk. The computational cost of exact likelihood evaluation for partially observed individual-based models, however, grows exponentially with the population size, necessitating approximate inference. Existing sampling-based methods usually require multiple simulations of the individuals in the population and rely on bespoke proposal distributions or summary statistics. We propose a deterministic approach to approximating the likelihood using categorical distributions. The approximate likelihood is amenable to automatic differentiation so that parameters can be estimated by maximization or posterior sampling using standard software libraries such as Stan or TensorFlow with little user effort. We prove the consistency of the maximum approximate likelihood estimator. We empirically test our approach on several classes of individual-based models for epidemiology: different sets of disease states, individual-specific transition rates, spatial interactions, under-reporting and misreporting. We demonstrate ground truth recovery and comparable marginal log-likelihood values at substantially reduced cost compared to competitor methods. Finally, we show the scalability and effectiveness of our approach with a real-world application on the 2001 UK Foot-and-Mouth outbreak, where the simplicity of the CAL allows us to include 162775 farms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03950v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Rimella, Nick Whiteley, Chris Jewell, Paul Fearnhead, Michael Whitehouse</dc:creator>
    </item>
    <item>
      <title>Distilling heterogeneous treatment effects: Stable subgroup estimation in causal inference</title>
      <link>https://arxiv.org/abs/2502.07275</link>
      <description>arXiv:2502.07275v2 Announce Type: replace 
Abstract: Recent methodological developments have introduced new black-box approaches to better estimate heterogeneous treatment effects; however, these methods fall short of providing interpretable characterizations of the underlying individuals who may be most at risk or benefit most from receiving the treatment, thereby limiting their practical utility. In this work, we introduce causal distillation trees (CDT) to estimate interpretable subgroups. CDT allows researchers to fit any machine learning model to estimate the individual-level treatment effect, and then leverages a simple, second-stage tree-based model to "distill" the estimated treatment effect into meaningful subgroups. As a result, CDT inherits the improvements in predictive performance from black-box machine learning models while preserving the interpretability of a simple decision tree. We derive theoretical guarantees for the consistency of the estimated subgroups using CDT, and introduce stability-driven diagnostics for researchers to evaluate the quality of the estimated subgroups. We illustrate our proposed method on a randomized controlled trial of antiretroviral treatment for HIV from the AIDS Clinical Trials Group Study 175 and show that CDT out-performs state-of-the-art approaches in constructing stable, clinically relevant subgroups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07275v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Huang, Tiffany M. Tang, Ana M. Kenney</dc:creator>
    </item>
    <item>
      <title>Bias in Gini coefficient estimation for gamma mixture populations</title>
      <link>https://arxiv.org/abs/2503.00690</link>
      <description>arXiv:2503.00690v2 Announce Type: replace 
Abstract: This paper examines the properties of the Gini coefficient estimator for gamma mixture populations and reveals the presence of bias. In contrast, we show that sampling from a gamma distribution yields an unbiased estimator, consistent with prior research (Baydil et al., 2025). We derive an explicit bias expression for the Gini coefficient in gamma mixture populations, which serves as the foundation for proposing a bias-corrected Gini estimator. We conduct a Monte Carlo simulation study to evaluate the behavior of the bias-corrected Gini estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.00690v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Vila, Helton Saulo</dc:creator>
    </item>
    <item>
      <title>Modelling pathwise uncertainty of Stochastic Differential Equations samplers via Probabilistic Numerics</title>
      <link>https://arxiv.org/abs/2401.03338</link>
      <description>arXiv:2401.03338v2 Announce Type: replace-cross 
Abstract: Probabilistic ordinary differential equation (ODE) solvers have been introduced over the past decade as uncertainty-aware numerical integrators. They typically proceed by assuming a functional prior to the ODE solution, which is then queried on a grid to form a posterior distribution over the ODE solution. As the queries span the integration interval, the approximate posterior solution then converges to the true deterministic one. Gaussian ODE filters, in particular, have enjoyed a lot of attention due to their computational efficiency, the simplicity of their implementation, as well as their provable fast convergence rates. In this article, we extend the methodology to stochastic differential equations (SDEs) and propose a probabilistic simulator for SDEs. Our approach involves transforming the SDE into a sequence of random ODEs using piecewise differentiable approximations of the Brownian motion. We then apply probabilistic ODE solvers to the individual ODEs, resulting in a pathwise probabilistic solution to the SDE\@. We establish worst-case strong $1.5$ local and $1.0$ global convergence orders for a specific instance of our method. We further show how we can marginalise the Brownian approximations, by incorporating its coefficients as part of the prior ODE model, allowing for computing exact transition densities under our model. Finally, we numerically validate the theoretical findings, showcasing reasonable weak convergence properties in the marginalised version.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03338v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yvann Le Fay, Simo S\"arkk\"a, Adrien Corenflos</dc:creator>
    </item>
    <item>
      <title>Causal Discovery over High-Dimensional Structured Hypothesis Spaces with Causal Graph Partitioning</title>
      <link>https://arxiv.org/abs/2406.06348</link>
      <description>arXiv:2406.06348v3 Announce Type: replace-cross 
Abstract: The aim in many sciences is to understand the mechanisms that underlie the observed distribution of variables, starting from a set of initial hypotheses. Causal discovery allows us to infer mechanisms as sets of cause and effect relationships in a generalized way -- without necessarily tailoring to a specific domain. Causal discovery algorithms search over a structured hypothesis space, defined by the set of directed acyclic graphs, to find the graph that best explains the data. For high-dimensional problems, however, this search becomes intractable and scalable algorithms for causal discovery are needed to bridge the gap. In this paper, we define a novel causal graph partition that allows for divide-and-conquer causal discovery with theoretical guarantees. We leverage the idea of a superstructure -- a set of learned or existing candidate hypotheses -- to partition the search space. We prove under certain assumptions that learning with a causal graph partition always yields the Markov Equivalence Class of the true causal graph. We show our algorithm achieves comparable accuracy and a faster time to solution for biologically-tuned synthetic networks and networks up to ${10^4}$ variables. This makes our method applicable to gene regulatory network inference and other domains with high-dimensional structured hypothesis spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06348v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashka Shah, Adela DePavia, Nathaniel Hudson, Ian Foster, Rick Stevens</dc:creator>
    </item>
    <item>
      <title>Conditional Rank-Rank Regression</title>
      <link>https://arxiv.org/abs/2407.06387</link>
      <description>arXiv:2407.06387v3 Announce Type: replace-cross 
Abstract: Rank-rank regression is commonly employed in economic research as a way of capturing the relationship between two economic variables. It frequently features in studies of intergenerational mobility as the resulting coefficient, capturing the rank correlation between the variables, is easy to interpret and measures overall persistence. However, in many applications it is common practice to include other covariates to account for differences in persistence levels between groups defined by the values of these covariates. In these instances the resulting coefficients can be difficult to interpret. We propose the conditional rank-rank regression, which uses conditional ranks instead of unconditional ranks, to measure average within-group persistence. The difference between conditional and unconditional rank-rank regression coefficients can then be interpreted as a measure of between-group persistence. We develop a flexible estimation approach using distribution regression and establish a theoretical framework for large sample inference. An empirical study on intergenerational income mobility in Switzerland demonstrates the advantages of this approach. The study reveals stronger intergenerational persistence between fathers and sons compared to fathers and daughters, with the within-group persistence explaining 62% of the overall income persistence for sons and 52% for daughters. Smaller families and those with highly educated fathers exhibit greater persistence in economic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06387v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Help Experimental Design for Causal Discovery?</title>
      <link>https://arxiv.org/abs/2503.01139</link>
      <description>arXiv:2503.01139v2 Announce Type: replace-cross 
Abstract: Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently difficult. Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional data to facilitate causal discovery. Previous approaches commonly utilize uncertainty or gradient signals to determine the intervention targets. However, numerical-based approaches may yield suboptimal results due to the inaccurate estimation of the guiding signals at the beginning when with limited interventional data. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in LLMs. Specifically, we present Large Language Model Guided Intervention Targeting (LeGIT) -- a robust framework that effectively incorporates LLMs to augment existing numerical approaches for the intervention targeting in causal discovery. Across 4 realistic benchmark scales, LeGIT demonstrates significant improvements and robustness over existing methods and even surpasses humans, which demonstrates the usefulness of LLMs in assisting with experimental design for scientific discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.01139v2</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 05 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyi Li, Yongqiang Chen, Chenxi Liu, Qianyi Cai, Tongliang Liu, Bo Han, Kun Zhang, Hui Xiong</dc:creator>
    </item>
  </channel>
</rss>
