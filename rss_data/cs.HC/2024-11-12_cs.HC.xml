<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 13 Nov 2024 02:44:16 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Provocation on Expertise in Social Impact Evaluations of Generative AI (and Beyond)</title>
      <link>https://arxiv.org/abs/2411.06017</link>
      <description>arXiv:2411.06017v1 Announce Type: new 
Abstract: Social impact evaluations are emerging as a useful tool to understand, document, and evaluate the societal impacts of generative AI. In this provocation, we begin to think carefully about the types of experts and expertise that are needed to conduct robust social impact evaluations of generative AI. We suggest that doing so will require thoughtfully eliciting and integrating insights from a range of "domain experts" and "experiential experts," and close with five open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06017v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zoe Kahn, Nitin Kohli</dc:creator>
    </item>
    <item>
      <title>Wild Narratives: Exploring the Effects of Animal Chatbots on Empathy and Positive Attitudes toward Animals</title>
      <link>https://arxiv.org/abs/2411.06060</link>
      <description>arXiv:2411.06060v1 Announce Type: new 
Abstract: Rises in the number of animal abuse cases are reported around the world. While chatbots have been effective in influencing their users' perceptions and behaviors, little if any research has hitherto explored the design of chatbots that embody animal identities for the purpose of eliciting empathy toward animals. We therefore conducted a mixed-methods experiment to investigate how specific design cues in such chatbots can shape their users' perceptions of both the chatbots' identities and the type of animal they represent. Our findings indicate that such chatbots can significantly increase empathy, improve attitudes, and promote prosocial behavioral intentions toward animals, particularly when they incorporate emotional verbal expressions and authentic details of such animals' lives. These results expand our understanding of chatbots with non-human identities and highlight their potential for use in conservation initiatives, suggesting a promising avenue whereby technology could foster a more informed and empathetic society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06060v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingshu Li, Aaditya Patwari, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>CoPrompter: User-Centric Evaluation of LLM Instruction Alignment for Improved Prompt Engineering</title>
      <link>https://arxiv.org/abs/2411.06099</link>
      <description>arXiv:2411.06099v1 Announce Type: new 
Abstract: Ensuring large language models' (LLMs) responses align with prompt instructions is crucial for application development. Based on our formative study with industry professionals, the alignment requires heavy human involvement and tedious trial-and-error especially when there are many instructions in the prompt. To address these challenges, we introduce CoPrompter, a framework that identifies misalignment based on assessing multiple LLM responses with criteria. It proposes a method to generate evaluation criteria questions derived directly from prompt requirements and an interface to turn these questions into a user-editable checklist. Our user study with industry prompt engineers shows that CoPrompter improves the ability to identify and refine instruction alignment with prompt requirements over traditional methods, helps them understand where and how frequently models fail to follow user's prompt requirements, and helps in clarifying their own requirements, giving them greater control over the response evaluation process. We also present the design lessons to underscore our system's potential to streamline the prompt engineering process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06099v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ishika Joshi, Simra Shahid, Shreeya Venneti, Manushree Vasu, Yantao Zheng, Yunyao Li, Balaji Krishnamurthy, Gromit Yeuk-Yin Chan</dc:creator>
    </item>
    <item>
      <title>Grasping Object: Challenges and Innovations in Robotics and Virtual Reality</title>
      <link>https://arxiv.org/abs/2411.06244</link>
      <description>arXiv:2411.06244v1 Announce Type: new 
Abstract: In real life, grasping is one of the fundamental and effective forms of interaction when manipulating objects. This holds true in the physical and virtual world; however, unlike the physical world, virtual reality (VR) is grasped in a complex formulation that includes graphics, physics, and perception. In virtual reality, the user's immersion level depends on realistic haptic feedback and high-quality graphics, which are computationally demanding and hard to achieve in real-time. Current solutions fail to produce plausible visuals and haptic feedback when simulation grasping in VR with a variety of targeted object dynamics. In this paper, we review the existing techniques for grasping in VR and robotics and indicate the main challenges that grasping faces in the domains. We aim to explore and understand the complexity of hand-grasping objects with different dynamics and inspire various ideas to improve and come up with potential solutions suitable for virtual reality applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06244v1</guid>
      <category>cs.HC</category>
      <category>cs.GR</category>
      <category>cs.RO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingzhao Zhou, Nadine Aburumman</dc:creator>
    </item>
    <item>
      <title>Enhancing Well-Being Through Positive Technology: VR Forest Bathing</title>
      <link>https://arxiv.org/abs/2411.06293</link>
      <description>arXiv:2411.06293v1 Announce Type: new 
Abstract: The growing demand for accessible therapeutic options has led to the exploration of Virtual Reality (VR) as a platform for forest bathing, which aims to reduce stress and improve cognitive functions. This paper brings together findings from three studies by the authors. The first study compared environments with and without plant life to examine how biomass influences stress reduction. The second study focused on the differences between low-fidelity and high-fidelity VR environments, while the third explored whether the benefits of VR forest bathing come from being immersed in realistic environments or simply from viewing something beautiful. The results showed no significant differences between environments with and without biomass, but highlighted the positive effects of high-fidelity VR environments and realistic nature over abstract art. The paper also covers how VR nature experiences may boost executive functioning and well-being in older adults and discusses the potential of generative AI to create customized VR environments. It concludes with a call for further collaborative research to refine VR forest bathing for stress relief and cognitive enhancement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06293v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Francisco R. Ortega, Victoria Interrante, Sara Lotemplio, Rachel Masters, Jalynn Nicoly, Zahra Borhani, Deana Davalos, Daniel Zielasko</dc:creator>
    </item>
    <item>
      <title>Emotion-Aware Interaction Design in Intelligent User Interface Using Multi-Modal Deep Learning</title>
      <link>https://arxiv.org/abs/2411.06326</link>
      <description>arXiv:2411.06326v1 Announce Type: new 
Abstract: In an era where user interaction with technology is ubiquitous, the importance of user interface (UI) design cannot be overstated. A well-designed UI not only enhances usability but also fosters more natural, intuitive, and emotionally engaging experiences, making technology more accessible and impactful in everyday life. This research addresses this growing need by introducing an advanced emotion recognition system to significantly improve the emotional responsiveness of UI. By integrating facial expressions, speech, and textual data through a multi-branch Transformer model, the system interprets complex emotional cues in real-time, enabling UIs to interact more empathetically and effectively with users. Using the public MELD dataset for validation, our model demonstrates substantial improvements in emotion recognition accuracy and F1 scores, outperforming traditional methods. These findings underscore the critical role that sophisticated emotion recognition plays in the evolution of UIs, making technology more attuned to user needs and emotions. This study highlights how enhanced emotional intelligence in UIs is not only about technical innovation but also about fostering deeper, more meaningful connections between users and the digital world, ultimately shaping how people interact with technology in their daily lives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06326v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyu Duan, Ziyi Wang, Shixiao Wang, Mengmeng Chen, Runsheng Zhang</dc:creator>
    </item>
    <item>
      <title>From Complexity to Simplicity: Using Python Instead of PsychoPy for fNIRS Data Collection</title>
      <link>https://arxiv.org/abs/2411.06523</link>
      <description>arXiv:2411.06523v1 Announce Type: new 
Abstract: This paper discusses the improvements made by replacing the PsychoPy script with a simplified Python code, reducing the number of devices and enhancing the experiment's overall efficiency in fNIRS based studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06523v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayla Sharmin, Md Fahim Abrar, Roghayeh Leila Barmaki</dc:creator>
    </item>
    <item>
      <title>Adaptive Kinematic Modeling for Improved Hand Posture Estimates Using a Haptic Glove</title>
      <link>https://arxiv.org/abs/2411.06575</link>
      <description>arXiv:2411.06575v1 Announce Type: new 
Abstract: Most commercially available haptic gloves compromise the accuracy of hand-posture measurements in favor of a simpler design with fewer sensors. While inaccurate posture data is often sufficient for the task at hand in biomedical settings such as VR-therapy-aided rehabilitation, measurements should be as precise as possible to digitally recreate hand postures as accurately as possible. With these applications in mind, we have added extra sensors to the commercially available Dexmo haptic glove by Dexta Robotics and applied kinematic models of the haptic glove and the user's hand to improve the accuracy of hand-posture measurements. In this work, we describe the augmentations and the kinematic modeling approach. Additionally, we present and discuss an evaluation of hand posture measurements as a proof of concept.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06575v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kathrin Krieger, David P. Leins, Thorben Markmann, Robert Haschke, Jianxu Chen, Matthias Gunzer, Helge Ritter</dc:creator>
    </item>
    <item>
      <title>Tap tactile presentation by airborne ultrasound</title>
      <link>https://arxiv.org/abs/2411.06653</link>
      <description>arXiv:2411.06653v1 Announce Type: new 
Abstract: The airborne ultrasound tactile display can present tactile information without direct contact. Using this technology, we developed two methods for simulating the tactile sensation of tapping an object with a finger: the Amplitude Modulation Method and the Lateral Modulation Method. The first method, Amplitude Modulation, simulates the tactile sensation of tapping a soft, deformable surface, like a deflated balloon. The second method, Lateral Modulation, simulates the tactile sensation of a rigid surface that easily resonates with vibrations, like a cymbal. In the demonstration, participants can experience the difference between these two tactile stimuli by tapping virtual objects displayed on a screen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06653v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haruka Tsuchiya, Zen Somei, Yasutoshi Makino, Hiroyuki Shinoda</dc:creator>
    </item>
    <item>
      <title>Script-Strategy Aligned Generation: Aligning LLMs with Expert-Crafted Dialogue Scripts and Therapeutic Strategies for Psychotherapy</title>
      <link>https://arxiv.org/abs/2411.06723</link>
      <description>arXiv:2411.06723v1 Announce Type: new 
Abstract: Chatbots or conversational agents (CAs) are increasingly used to improve access to digital psychotherapy. Many current systems rely on rigid, rule-based designs, heavily dependent on expert-crafted dialogue scripts for guiding therapeutic conversations. Although recent advances in large language models (LLMs) offer the potential for more flexible interactions, their lack of controllability and transparency poses significant challenges in sensitive areas like psychotherapy. In this work, we explored how aligning LLMs with expert-crafted scripts can enhance psychotherapeutic chatbot performance. Our comparative study showed that LLMs aligned with expert-crafted scripts through prompting and fine-tuning significantly outperformed both pure LLMs and rule-based chatbots, achieving a more effective balance between dialogue flexibility and adherence to therapeutic principles. Building on findings, we proposed ``Script-Strategy Aligned Generation (SSAG)'', a flexible alignment approach that reduces reliance on fully scripted content while enhancing LLMs' therapeutic adherence and controllability. In a 10-day field study, SSAG demonstrated performance comparable to full script alignment and outperformed rule-based chatbots, empirically supporting SSAG as an efficient approach for aligning LLMs with domain expertise. Our work advances LLM applications in psychotherapy by providing a controllable, adaptable, and scalable solution for digital interventions, reducing reliance on expert effort. It also provides a collaborative framework for domain experts and developers to efficiently build expertise-aligned chatbots, broadening access to psychotherapy and behavioral interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06723v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Sun, Jan de Wit, Zhuying Li, Jiahuan Pei, Abdallah El Ali, Jos A. Bosch</dc:creator>
    </item>
    <item>
      <title>Predicting Selection Intention in Real-Time with Bayesian-based ML Model in Unimodal Gaze Interaction</title>
      <link>https://arxiv.org/abs/2411.06726</link>
      <description>arXiv:2411.06726v1 Announce Type: new 
Abstract: Eye gaze is considered a promising interaction modality in extende reality (XR) environments. However, determining selection intention from gaze data often requires additional manual selection techniques. We present a Bayesian-based machine learning (ML) model to predict user selection intention in real-time using only gaze data. Our model uses a Bayesian approach to transform gaze data into selection probabilities, which are then fed into an ML model to discriminate selection intentions. In Study 1, our model achieved real-time inference with an accuracy of 0.97 and an F1 score of 0.96. In Study 2, we found that the selection intention inferred by our model enables more comfortable and accurate interactions compared to traditional techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06726v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taewoo Jo, Ho Jung Lee, Sulim Chun, In-Kwon Lee</dc:creator>
    </item>
    <item>
      <title>Electrooculography Dataset for Objective Spatial Navigation Assessment in Healthy Participants</title>
      <link>https://arxiv.org/abs/2411.06811</link>
      <description>arXiv:2411.06811v1 Announce Type: new 
Abstract: In the quest for understanding human executive function, eye movements represent a unique insight into how we process and comprehend our environment. Eye movements reveal patterns in how we focus, navigate, and make decisions across various contexts. The proposed dataset includes electrooculography (EOG) signals from 27 healthy subjects, capturing both vertical and horizontal eye movements. The recorded signals were obtained during the video-watching stage of the Leiden Navigation Test, designed to assess spatial navigation abilities. In addition to other data, the dataset includes scores from the Mini- Mental State Examination and the Wayfinding Questionnaire. The dataset comprises carefully curated components, including relevant information, the Mini-Mental State Examination scores, and the Wayfinding Questionnaire scores, encompassing navigation, orientation, distance estimation, spatial anxiety, as well as raw and processed EOG signals. These assessments contribute more information about the participants' cognitive function and navigational abilities. This dataset can be valuable for researchers investigating spatial navigation abilities through EOG signal analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06811v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mobina Zibandehpoor, Fatemeh Alizadehziri, Arash Abbasi Larki, Sobhan Teymouri, Mehdi Delrobaei</dc:creator>
    </item>
    <item>
      <title>Minion: A Technology Probe for Resolving Value Conflicts through Expert-Driven and User-Driven Strategies in AI Companion Applications</title>
      <link>https://arxiv.org/abs/2411.07042</link>
      <description>arXiv:2411.07042v1 Announce Type: new 
Abstract: AI companions based on large language models can role-play and converse very naturally. When value conflicts arise between the AI companion and the user, it may offend or upset the user. Yet, little research has examined such conflicts. We first conducted a formative study that analyzed 151 user complaints about conflicts with AI companions, providing design implications for our study. Based on these, we created Minion, a technology probe to help users resolve human-AI value conflicts. Minion applies a user-empowerment intervention method that provides suggestions by combining expert-driven and user-driven conflict resolution strategies. We conducted a technology probe study, creating 40 value conflict scenarios on Character.AI and Talkie. 22 participants completed 274 tasks and successfully resolved conflicts 94.16% of the time. We summarize user responses, preferences, and needs in resolving value conflicts, and propose design implications to reduce conflicts and empower users to resolve them more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07042v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xianzhe Fan, Qing Xiao, Xuhui Zhou, Yuran Su, Zhicong Lu, Maarten Sap, Hong Shen</dc:creator>
    </item>
    <item>
      <title>Tasks, Time, and Tools: Quantifying Online Sensemaking Efforts Through a Survey-based Study</title>
      <link>https://arxiv.org/abs/2411.07206</link>
      <description>arXiv:2411.07206v1 Announce Type: new 
Abstract: Aiming to help people conduct online research tasks, much research has gone into tools for searching for, collecting, organizing, and synthesizing online information. However, outside of the lab, in-the-wild sensemaking sessions (with data on tasks, users, their tools and challenges) can ground us in the reality of such efforts and the state of tool support. We use a survey-based approach with aided recall focused on segmenting and contextualizing individual exploratory browsing sessions to conduct a mixed method analysis of everyday sensemaking sessions in the traditional desktop browser setting while preserving user privacy. We report data from our survey (n=111) collected in September, 2022, and use these results to update and deepen the rich literature on information seeking behavior and exploratory search, contributing new empirical insights into the time spent per week and distribution of that time across tasks, and the lack of externalization and tool-use despite widespread desire for support.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07206v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Kuznetsov, Michael Xieyang Liu, Aniket Kittur</dc:creator>
    </item>
    <item>
      <title>Evaluating the Economic Implications of Using Machine Learning in Clinical Psychiatry</title>
      <link>https://arxiv.org/abs/2411.05856</link>
      <description>arXiv:2411.05856v1 Announce Type: cross 
Abstract: With the growing interest in using AI and machine learning (ML) in medicine, there is an increasing number of literature covering the application and ethics of using AI and ML in areas of medicine such as clinical psychiatry. The problem is that there is little literature covering the economic aspects associated with using ML in clinical psychiatry. This study addresses this gap by specifically studying the economic implications of using ML in clinical psychiatry. In this paper, we evaluate the economic implications of using ML in clinical psychiatry through using three problem-oriented case studies, literature on economics, socioeconomic and medical AI, and two types of health economic evaluations. In addition, we provide details on fairness, legal, ethics and other considerations for ML in clinical psychiatry.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05856v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soaad Hossain, James Rasalingam, Arhum Waheed, Fatah Awil, Rachel Kandiah, Syed Ishtiaque Ahmed</dc:creator>
    </item>
    <item>
      <title>Humans Continue to Outperform Large Language Models in Complex Clinical Decision-Making: A Study with Medical Calculators</title>
      <link>https://arxiv.org/abs/2411.05897</link>
      <description>arXiv:2411.05897v1 Announce Type: cross 
Abstract: Although large language models (LLMs) have been assessed for general medical knowledge using medical licensing exams, their ability to effectively support clinical decision-making tasks, such as selecting and using medical calculators, remains uncertain. Here, we evaluate the capability of both medical trainees and LLMs to recommend medical calculators in response to various multiple-choice clinical scenarios such as risk stratification, prognosis, and disease diagnosis. We assessed eight LLMs, including open-source, proprietary, and domain-specific models, with 1,009 question-answer pairs across 35 clinical calculators and measured human performance on a subset of 100 questions. While the highest-performing LLM, GPT-4o, provided an answer accuracy of 74.3% (CI: 71.5-76.9%), human annotators, on average, outperformed LLMs with an accuracy of 79.5% (CI: 73.5-85.0%). With error analysis showing that the highest-performing LLMs continue to make mistakes in comprehension (56.6%) and calculator knowledge (8.1%), our findings emphasize that humans continue to surpass LLMs on complex clinical tasks such as calculator recommendation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.05897v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Wan, Qiao Jin, Joey Chan, Guangzhi Xiong, Serina Applebaum, Aidan Gilson, Reid McMurry, R. Andrew Taylor, Aidong Zhang, Qingyu Chen, Zhiyong Lu</dc:creator>
    </item>
    <item>
      <title>Expansion Quantization Network: An Efficient Micro-emotion Annotation and Detection Framework</title>
      <link>https://arxiv.org/abs/2411.06160</link>
      <description>arXiv:2411.06160v1 Announce Type: cross 
Abstract: Text emotion detection constitutes a crucial foundation for advancing artificial intelligence from basic comprehension to the exploration of emotional reasoning. Most existing emotion detection datasets rely on manual annotations, which are associated with high costs, substantial subjectivity, and severe label imbalances. This is particularly evident in the inadequate annotation of micro-emotions and the absence of emotional intensity representation, which fail to capture the rich emotions embedded in sentences and adversely affect the quality of downstream task completion. By proposing an all-labels and training-set label regression method, we map label values to energy intensity levels, thereby fully leveraging the learning capabilities of machine models and the interdependencies among labels to uncover multiple emotions within samples. This led to the establishment of the Emotion Quantization Network (EQN) framework for micro-emotion detection and annotation. Using five commonly employed sentiment datasets, we conducted comparative experiments with various models, validating the broad applicability of our framework within NLP machine learning models. Based on the EQN framework, emotion detection and annotation are conducted on the GoEmotions dataset. A comprehensive comparison with the results from Google literature demonstrates that the EQN framework possesses a high capability for automatic detection and annotation of micro-emotions. The EQN framework is the first to achieve automatic micro-emotion annotation with energy-level scores, providing strong support for further emotion detection analysis and the quantitative research of emotion computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06160v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyi Zhou, Senlin Luo, Haofan Chen</dc:creator>
    </item>
    <item>
      <title>Security Implications of User Non-compliance Behavior to Software Updates: A Risk Assessment Study</title>
      <link>https://arxiv.org/abs/2411.06262</link>
      <description>arXiv:2411.06262v1 Announce Type: cross 
Abstract: Software updates are essential to enhance security, fix bugs, and add better features to existing software. However, while some users comply and update their systems upon notification, non-compliance is common. Delaying or ignoring updates leaves systems exposed to security vulnerabilities. Despite research efforts, users' noncompliance behavior with software updates is still prevalent. In this study, we explored how psychological factors influence users' perception and behavior toward software updates. In addition, we proposed a model to assess the security risk score associated with delaying software updates. We conducted a user study with Windows OS users to explore how information about potential vulnerabilities and risk scores influence their behavior. Furthermore, we also studied the influence of demographic factors such as gender on the users' decision-making process for software updates. Our results showed that psychological traits, such as knowledge, awareness, and experience, impact users' decision-making about software updates. To increase users' compliance, providing a risk score for not updating their systems and information about vulnerabilities statistically significantly increased users' willingness to update their systems. Additionally, our results indicated no statistically significant difference in male and female users' responses in terms of concerns about securing their systems. The implications of this study are relevant for software developers and manufacturers as they can use this information to design more effective software update notification messages. Highlighting potential risks and corresponding risk scores in future software updates can motivate users to act promptly to update the systems in a timely manner, which can ultimately improve the overall security of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06262v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahzabin Tamanna, Joseph D Stephens, Mohd Anwar</dc:creator>
    </item>
    <item>
      <title>Optimal Driver Warning Generation in Dynamic Driving Environment</title>
      <link>https://arxiv.org/abs/2411.06306</link>
      <description>arXiv:2411.06306v1 Announce Type: cross 
Abstract: The driver warning system that alerts the human driver about potential risks during driving is a key feature of an advanced driver assistance system. Existing driver warning technologies, mainly the forward collision warning and unsafe lane change warning, can reduce the risk of collision caused by human errors. However, the current design methods have several major limitations. Firstly, the warnings are mainly generated in a one-shot manner without modeling the ego driver's reactions and surrounding objects, which reduces the flexibility and generality of the system over different scenarios. Additionally, the triggering conditions of warning are mostly rule-based threshold-checking given the current state, which lacks the prediction of the potential risk in a sufficiently long future horizon. In this work, we study the problem of optimally generating driver warnings by considering the interactions among the generated warning, the driver behavior, and the states of ego and surrounding vehicles on a long horizon. The warning generation problem is formulated as a partially observed Markov decision process (POMDP). An optimal warning generation framework is proposed as a solution to the proposed POMDP. The simulation experiments demonstrate the superiority of the proposed solution to the existing warning generation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06306v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chenran Li, Aolin Xu, Enna Sachdeva, Teruhisa Misu, Behzad Dariush</dc:creator>
    </item>
    <item>
      <title>Prompts Matter: Comparing ML/GAI Approaches for Generating Inductive Qualitative Coding Results</title>
      <link>https://arxiv.org/abs/2411.06316</link>
      <description>arXiv:2411.06316v1 Announce Type: cross 
Abstract: Inductive qualitative methods have been a mainstay of education research for decades, yet it takes much time and effort to conduct rigorously. Recent advances in artificial intelligence, particularly with generative AI (GAI), have led to initial success in generating inductive coding results. Like human coders, GAI tools rely on instructions to work, and how to instruct it may matter. To understand how ML/GAI approaches could contribute to qualitative coding processes, this study applied two known and two theory-informed novel approaches to an online community dataset and evaluated the resulting coding results. Our findings show significant discrepancies between ML/GAI approaches and demonstrate the advantage of our approaches, which introduce human coding processes into GAI prompts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06316v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>John Chen, Alexandros Lotsos, Lexie Zhao, Grace Wang, Uri Wilensky, Bruce Sherin, Michael Horn</dc:creator>
    </item>
    <item>
      <title>Balancing Power and Ethics: A Framework for Addressing Human Rights Concerns in Military AI</title>
      <link>https://arxiv.org/abs/2411.06336</link>
      <description>arXiv:2411.06336v1 Announce Type: cross 
Abstract: AI has made significant strides recently, leading to various applications in both civilian and military sectors. The military sees AI as a solution for developing more effective and faster technologies. While AI offers benefits like improved operational efficiency and precision targeting, it also raises serious ethical and legal concerns, particularly regarding human rights violations. Autonomous weapons that make decisions without human input can threaten the right to life and violate international humanitarian law. To address these issues, we propose a three-stage framework (Design, In Deployment, and During/After Use) for evaluating human rights concerns in the design, deployment, and use of military AI. Each phase includes multiple components that address various concerns specific to that phase, ranging from bias and regulatory issues to violations of International Humanitarian Law. By this framework, we aim to balance the advantages of AI in military operations with the need to protect human rights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06336v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Harms and Risks of AI in the Military Workshop 2024</arxiv:journal_reference>
      <dc:creator>Mst Rafia Islam, Azmine Toushik Wasi</dc:creator>
    </item>
    <item>
      <title>Epistemic Integrity in Large Language Models</title>
      <link>https://arxiv.org/abs/2411.06528</link>
      <description>arXiv:2411.06528v1 Announce Type: cross 
Abstract: Large language models are increasingly relied upon as sources of information, but their propensity for generating false or misleading statements with high confidence poses risks for users and society. In this paper, we confront the critical problem of epistemic miscalibration $\unicode{x2013}$ where a model's linguistic assertiveness fails to reflect its true internal certainty. We introduce a new human-labeled dataset and a novel method for measuring the linguistic assertiveness of Large Language Models (LLMs) which cuts error rates by over 50% relative to previous benchmarks. Validated across multiple datasets, our method reveals a stark misalignment between how confidently models linguistically present information and their actual accuracy. Further human evaluations confirm the severity of this miscalibration. This evidence underscores the urgent risk of the overstated certainty LLMs hold which may mislead users on a massive scale. Our framework provides a crucial step forward in diagnosing this miscalibration, offering a path towards correcting it and more trustworthy AI across domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06528v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bijean Ghafouri, Shahrad Mohammadzadeh, James Zhou, Pratheeksha Nair, Jacob-Junqi Tian, Mayank Goel, Reihaneh Rabbany, Jean-Fran\c{c}ois Godbout, Kellin Pelrine</dc:creator>
    </item>
    <item>
      <title>Large-scale moral machine experiment on large language models</title>
      <link>https://arxiv.org/abs/2411.06790</link>
      <description>arXiv:2411.06790v1 Announce Type: cross 
Abstract: The rapid advancement of Large Language Models (LLMs) and their potential integration into autonomous driving systems necessitates understanding their moral decision-making capabilities. While our previous study examined four prominent LLMs using the Moral Machine experimental framework, the dynamic landscape of LLM development demands a more comprehensive analysis. Here, we evaluate moral judgments across 51 different LLMs, including multiple versions of proprietary models (GPT, Claude, Gemini) and open-source alternatives (Llama, Gemma), to assess their alignment with human moral preferences in autonomous driving scenarios. Using a conjoint analysis framework, we evaluated how closely LLM responses aligned with human preferences in ethical dilemmas and examined the effects of model size, updates, and architecture. Results showed that proprietary models and open-source models exceeding 10 billion parameters demonstrated relatively close alignment with human judgments, with a significant negative correlation between model size and distance from human judgments in open-source models. However, model updates did not consistently improve alignment with human preferences, and many LLMs showed excessive emphasis on specific ethical principles. These findings suggest that while increasing model size may naturally lead to more human-like moral judgments, practical implementation in autonomous driving systems requires careful consideration of the trade-off between judgment quality and computational efficiency. Our comprehensive analysis provides crucial insights for the ethical design of autonomous systems and highlights the importance of considering cultural contexts in AI moral decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06790v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Shahrul Zaim bin Ahmad, Kazuhiro Takemoto</dc:creator>
    </item>
    <item>
      <title>3D Printing of Near-Ambient Responsive Liquid Crystal Elastomers with Enhanced Nematic Order and Pluralized Transformation</title>
      <link>https://arxiv.org/abs/2411.06931</link>
      <description>arXiv:2411.06931v1 Announce Type: cross 
Abstract: Liquid Crystal Elastomers with near-ambient temperature-responsiveness (NAT-LCEs) have been extensively studied for building bio-compatible, low-power consumption devices and robotics. However, conventional manufacturing methods face limitations in programmability (e.g., molding) or low nematic order (e.g., DIW printing). Here, a hybrid cooling strategy is proposed for programmable 3D printing of NAT-LCEs with enhanced nematic order, intricate shape forming, and morphing capability. By integrating a low-temperature nozzle and a cooling platform into a 3D printer, the resulting temperature field synergistically facilitates mesogen alignment during extrusion and disruption-free UV cross-linking. This method achieves a nematic order 3000% higher than those fabricated using traditional room temperature 3D printing. Enabled by shifting of transition temperature during hybrid cooling printing, printed sheets spontaneously turn into 3D structures after release from the platform, exhibiting bidirectional deformation with heating and cooling. By adjusting the nozzle and plate temperatures, NAT-LCEs with graded properties can be fabricated for intricate shape morphing. A wristband system with enhanced heart rate monitoring is also developed based on 3D-printed NAT-LCE. Our method may open new possibilities for soft robotics, biomedical devices, and wearable electronics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06931v1</guid>
      <category>cond-mat.soft</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongxiao Li, Yuxuan Sun, Xingjian Li, Xingxiang Li, Zhengqing Zhu, Boxi Sun, Shutong Nong, Jiyang Wu, Tingrui Pan, Weihua Li, Shiwu Zhang, Mujun Li</dc:creator>
    </item>
    <item>
      <title>Sniff AI: Is My 'Spicy' Your 'Spicy'? Exploring LLM's Perceptual Alignment with Human Smell Experiences</title>
      <link>https://arxiv.org/abs/2411.06950</link>
      <description>arXiv:2411.06950v1 Announce Type: cross 
Abstract: Aligning AI with human intent is important, yet perceptual alignment-how AI interprets what we see, hear, or smell-remains underexplored. This work focuses on olfaction, human smell experiences. We conducted a user study with 40 participants to investigate how well AI can interpret human descriptions of scents. Participants performed "sniff and describe" interactive tasks, with our designed AI system attempting to guess what scent the participants were experiencing based on their descriptions. These tasks evaluated the Large Language Model's (LLMs) contextual understanding and representation of scent relationships within its internal states - high-dimensional embedding space. Both quantitative and qualitative methods were used to evaluate the AI system's performance. Results indicated limited perceptual alignment, with biases towards certain scents, like lemon and peppermint, and continued failing to identify others, like rosemary. We discuss these findings in light of human-AI alignment advancements, highlighting the limitations and opportunities for enhancing HCI systems with multisensory experience integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.06950v1</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shu Zhong, Zetao Zhou, Christopher Dawes, Giada Brianz, Marianna Obrist</dc:creator>
    </item>
    <item>
      <title>Enhancing Robot Assistive Behaviour with Reinforcement Learning and Theory of Mind</title>
      <link>https://arxiv.org/abs/2411.07003</link>
      <description>arXiv:2411.07003v1 Announce Type: cross 
Abstract: The adaptation to users' preferences and the ability to infer and interpret humans' beliefs and intents, which is known as the Theory of Mind (ToM), are two crucial aspects for achieving effective human-robot collaboration. Despite its importance, very few studies have investigated the impact of adaptive robots with ToM abilities. In this work, we present an exploratory comparative study to investigate how social robots equipped with ToM abilities impact users' performance and perception. We design a two-layer architecture. The Q-learning agent on the first layer learns the robot's higher-level behaviour. On the second layer, a heuristic-based ToM infers the user's intended strategy and is responsible for implementing the robot's assistance, as well as providing the motivation behind its choice. We conducted a user study in a real-world setting, involving 56 participants who interacted with either an adaptive robot capable of ToM, or with a robot lacking such abilities. Our findings suggest that participants in the ToM condition performed better, accepted the robot's assistance more often, and perceived its ability to adapt, predict and recognise their intents to a higher degree. Our preliminary insights could inform future research and pave the way for designing more complex computation architectures for adaptive behaviour with ToM capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07003v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Andriella, Giovanni Falcone, Silvia Rossi</dc:creator>
    </item>
    <item>
      <title>ConvMixFormer- A Resource-efficient Convolution Mixer for Transformer-based Dynamic Hand Gesture Recognition</title>
      <link>https://arxiv.org/abs/2411.07118</link>
      <description>arXiv:2411.07118v1 Announce Type: cross 
Abstract: Transformer models have demonstrated remarkable success in many domains such as natural language processing (NLP) and computer vision. With the growing interest in transformer-based architectures, they are now utilized for gesture recognition. So, we also explore and devise a novel ConvMixFormer architecture for dynamic hand gestures. The transformers use quadratic scaling of the attention features with the sequential data, due to which these models are computationally complex and heavy. We have considered this drawback of the transformer and designed a resource-efficient model that replaces the self-attention in the transformer with the simple convolutional layer-based token mixer. The computational cost and the parameters used for the convolution-based mixer are comparatively less than the quadratic self-attention. Convolution-mixer helps the model capture the local spatial features that self-attention struggles to capture due to their sequential processing nature. Further, an efficient gate mechanism is employed instead of a conventional feed-forward network in the transformer to help the model control the flow of features within different stages of the proposed model. This design uses fewer learnable parameters which is nearly half the vanilla transformer that helps in fast and efficient training. The proposed method is evaluated on NVidia Dynamic Hand Gesture and Briareo datasets and our model has achieved state-of-the-art results on single and multimodal inputs. We have also shown the parameter efficiency of the proposed ConvMixFormer model compared to other methods. The source code is available at https://github.com/mallikagarg/ConvMixFormer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07118v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Mallika Garg, Debashis Ghosh, Pyari Mohan Pradhan</dc:creator>
    </item>
    <item>
      <title>Generation Probabilities Are Not Enough: Uncertainty Highlighting in AI Code Completions</title>
      <link>https://arxiv.org/abs/2302.07248</link>
      <description>arXiv:2302.07248v3 Announce Type: replace 
Abstract: Large-scale generative models enabled the development of AI-powered code completion tools to assist programmers in writing code. However, much like other AI-powered tools, AI-powered code completions are not always accurate, potentially introducing bugs or even security vulnerabilities into code if not properly detected and corrected by a human programmer. One technique that has been proposed and implemented to help programmers identify potential errors is to highlight uncertain tokens. However, there have been no empirical studies exploring the effectiveness of this technique -- nor investigating the different and not-yet-agreed-upon notions of uncertainty in the context of generative models. We explore the question of whether conveying information about uncertainty enables programmers to more quickly and accurately produce code when collaborating with an AI-powered code completion tool, and if so, what measure of uncertainty best fits programmers' needs. Through a mixed-methods study with 30 programmers, we compare three conditions: providing the AI system's code completion alone, highlighting tokens with the lowest likelihood of being generated by the underlying generative model, and highlighting tokens with the highest predicted likelihood of being edited by a programmer. We find that highlighting tokens with the highest predicted likelihood of being edited leads to faster task completion and more targeted edits, and is subjectively preferred by study participants. In contrast, highlighting tokens according to their probability of being generated does not provide any benefit over the baseline with no highlighting. We further explore the design space of how to convey uncertainty in AI-powered code completion tools, and find that programmers prefer highlights that are granular, informative, interpretable, and not overwhelming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.07248v3</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3702320</arxiv:DOI>
      <dc:creator>Helena Vasconcelos, Gagan Bansal, Adam Fourney, Q. Vera Liao, Jennifer Wortman Vaughan</dc:creator>
    </item>
    <item>
      <title>Designing Human-AI Systems: Anthropomorphism and Framing Bias on Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2404.00634</link>
      <description>arXiv:2404.00634v2 Announce Type: replace 
Abstract: AI is redefining how humans interact with technology, leading to a synergetic collaboration between the two. Nevertheless, the effects of human cognition on this collaboration remain unclear. This study investigates the implications of two cognitive biases, anthropomorphism and framing effect, on human-AI collaboration within a hiring setting. Subjects were asked to select job candidates with the help of an AI-powered recommendation tool. The tool was manipulated in a 3 x 3 between-subjects design to present three different AI identities (human-like, robot-like, generic) and three types of framing (positive, negative, and neutral). The results revealed that the framing of AI's recommendations had no significant influence on subjects' decisions. In contrast, anthropomorphism significantly affected subjects' agreement with AI recommendations. Subjects were less likely to agree with the AI if it had human-like characteristics. These findings demonstrate that cognitive biases can impact human-AI collaboration and highlight the need for tailored approaches to AI product design, rather than a single, universal solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00634v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Aleksander S\'anchez Olszewski</dc:creator>
    </item>
    <item>
      <title>fNIRS Analysis of Interaction Techniques in Touchscreen-Based Educational Gaming</title>
      <link>https://arxiv.org/abs/2405.08906</link>
      <description>arXiv:2405.08906v2 Announce Type: replace 
Abstract: Touchscreens are becoming increasingly widespread in educational games, enhancing the quality of learner experience. Traditional metrics are often used to evaluate various input modalities, including hand and stylus. However, there exists a gap in understanding the cognitive impacts of these modalities during educational gameplay, which can be addressed through brain signal analysis to gain deeper insights into the underlying cognitive function and necessary brain resources for each condition. This facilitates a more precise comparison between conditions. In this study, we compared the brain signal and user experience of using hands and stylus on touchscreens while playing an educational game by analyzing hemodynamic response and self-reported measures. Participants engaged in a Unity-based educational quiz game using both hand and stylus on a touchscreen in a counterbalanced within-subject design. Oxygenated and deoxygenated hemoglobin data were collected using fNIRS, alongside quiz performance scores and standardized and customized user experience questionnaire ratings. Our findings show almost the same performance level with both input modalities, however, the hand requires less oxygen flow which suggests a lower cognitive effort than using a stylus while playing the educational game. Although the result shows that the stylus condition required more neural involvement than the hand condition, there is no significant difference between the use of both input modalities. However, there is a statistically significant difference in self-reported measures that support the findings mentioned above, favoring the hand that enhances understanding of modality effects in interactive educational environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08906v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shayla Sharmin, Elham Bakhshipour, Behdokht Kiafar, Md Fahim Abrar, Pinar Kullu, Nancy Getchell, Roghayeh Leila Barmaki</dc:creator>
    </item>
    <item>
      <title>The Dark Side of AI Companionship: A Taxonomy of Harmful Algorithmic Behaviors in Human-AI Relationships</title>
      <link>https://arxiv.org/abs/2410.20130</link>
      <description>arXiv:2410.20130v2 Announce Type: replace 
Abstract: As conversational AI systems increasingly permeate the socio-emotional realms of human life, they bring both benefits and risks to individuals and society. Despite extensive research on detecting and categorizing harms in AI systems, less is known about the harms that arise from social interactions with AI chatbots. Through a mixed-methods analysis of 35,390 conversation excerpts shared on r/replika, an online community for users of the AI companion Replika, we identified six categories of harmful behaviors exhibited by the chatbot: relational transgression, verbal abuse and hate, self-inflicted harm, harassment and violence, mis/disinformation, and privacy violations. The AI contributes to these harms through four distinct roles: perpetrator, instigator, facilitator, and enabler. Our findings highlight the relational harms of AI chatbots and the danger of algorithmic compliance, enhancing the understanding of AI harms in socio-emotional interactions. We also provide suggestions for designing ethical and responsible AI systems that prioritize user safety and well-being.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20130v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renwen Zhang, Han Li, Han Meng, Jinyuan Zhan, Hongyuan Gan, Yi-Chieh Lee</dc:creator>
    </item>
    <item>
      <title>Learning from Implicit User Feedback, Emotions and Demographic Information in Task-Oriented and Document-Grounded Dialogues</title>
      <link>https://arxiv.org/abs/2401.09248</link>
      <description>arXiv:2401.09248v2 Announce Type: replace-cross 
Abstract: Implicit user feedback, user emotions and demographic information have shown to be promising sources for improving the accuracy and user engagement of responses generated by dialogue systems. However, the influence of such information on task completion and factual consistency, which are important criteria for task-oriented and document-grounded dialogues, is not yet known. To address this, we introduce FEDI, the first English task-oriented and document-grounded dialogue dataset annotated with this information. Our experiments with Flan-T5, GPT-2 and Llama 2 show a particularly positive impact on task completion and factual consistency. Participants in our human evaluation reported that the responses generated by the feedback-trained models were more informative (Flan-T5 and GPT-2), relevant and factual consistent (Llama 2).</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09248v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Findings of the Association for Computational Linguistics: EMNLP 2024, pages 4573 - 4603, Miami, Florida, USA</arxiv:journal_reference>
      <dc:creator>Dominic Petrak, Thy Thy Tran, Iryna Gurevych</dc:creator>
    </item>
    <item>
      <title>MambaTalk: Efficient Holistic Gesture Synthesis with Selective State Space Models</title>
      <link>https://arxiv.org/abs/2403.09471</link>
      <description>arXiv:2403.09471v3 Announce Type: replace-cross 
Abstract: Gesture synthesis is a vital realm of human-computer interaction, with wide-ranging applications across various fields like film, robotics, and virtual reality. Recent advancements have utilized the diffusion model and attention mechanisms to improve gesture synthesis. However, due to the high computational complexity of these techniques, generating long and diverse sequences with low latency remains a challenge. We explore the potential of state space models (SSMs) to address the challenge, implementing a two-stage modeling strategy with discrete motion priors to enhance the quality of gestures. Leveraging the foundational Mamba block, we introduce MambaTalk, enhancing gesture diversity and rhythm through multimodal integration. Extensive experiments demonstrate that our method matches or exceeds the performance of state-of-the-art models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09471v3</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zunnan Xu, Yukang Lin, Haonan Han, Sicheng Yang, Ronghui Li, Yachao Zhang, Xiu Li</dc:creator>
    </item>
    <item>
      <title>On the Utility of Accounting for Human Beliefs about AI Intention in Human-AI Collaboration</title>
      <link>https://arxiv.org/abs/2406.06051</link>
      <description>arXiv:2406.06051v2 Announce Type: replace-cross 
Abstract: To enable effective human-AI collaboration, merely optimizing AI performance without considering human factors is insufficient. Recent research has shown that designing AI agents that take human behavior into account leads to improved performance in human-AI collaboration. However, a limitation of most existing approaches is their assumption that human behavior remains static, regardless of the AI agent's actions. In reality, humans may adjust their actions based on their beliefs about the AI's intentions, specifically, the subtasks they perceive the AI to be attempting to complete based on its behavior. In this paper, we address this limitation by enabling a collaborative AI agent to consider its human partner's beliefs about its intentions, i.e., what the human partner thinks the AI agent is trying to accomplish, and to design its action plan accordingly to facilitate more effective human-AI collaboration. Specifically, we developed a model of human beliefs that captures how humans interpret and reason about their AI partner's intentions. Using this belief model, we created an AI agent that incorporates both human behavior and human beliefs when devising its strategy for interacting with humans. Through extensive real-world human-subject experiments, we demonstrate that our belief model more accurately captures human perceptions of AI intentions. Furthermore, we show that our AI agent, designed to account for human beliefs over its intentions, significantly enhances performance in human-AI collaboration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06051v2</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guanghui Yu, Robert Kasumba, Chien-Ju Ho, William Yeoh</dc:creator>
    </item>
    <item>
      <title>Problem Solving Through Human-AI Preference-Based Cooperation</title>
      <link>https://arxiv.org/abs/2408.07461</link>
      <description>arXiv:2408.07461v3 Announce Type: replace-cross 
Abstract: While there is a widespread belief that artificial general intelligence (AGI) -- or even superhuman AI -- is imminent, complex problems in expert domains are far from being solved. We argue that such problems require human-AI cooperation and that the current state of the art in generative AI is unable to play the role of a reliable partner due to a multitude of shortcomings, including inability to keep track of a complex solution artifact (e.g., a software program), limited support for versatile human preference expression and lack of adapting to human preference in an interactive setting. To address these challenges, we propose HAI-Co2, a novel human-AI co-construction framework. We formalize HAI-Co2 and discuss the difficult open research problems that it faces. Finally, we present a case study of HAI-Co2 and demonstrate its efficacy compared to monolithic generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07461v3</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhabrata Dutta, Timo Kaufmann, Goran Glava\v{s}, Ivan Habernal, Kristian Kersting, Frauke Kreuter, Mira Mezini, Iryna Gurevych, Eyke H\"ullermeier, Hinrich Schuetze</dc:creator>
    </item>
    <item>
      <title>What is lost in Normalization? Exploring Pitfalls in Multilingual ASR Model Evaluations</title>
      <link>https://arxiv.org/abs/2409.02449</link>
      <description>arXiv:2409.02449v4 Announce Type: replace-cross 
Abstract: This paper explores the pitfalls in evaluating multilingual automatic speech recognition (ASR) models, with a particular focus on Indic language scripts. We investigate the text normalization routine employed by leading ASR models, including OpenAI Whisper, Meta's MMS, Seamless, and Assembly AI's Conformer, and their unintended consequences on performance metrics. Our research reveals that current text normalization practices, while aiming to standardize ASR outputs for fair comparison, by removing inconsistencies such as variations in spelling, punctuation, and special characters, are fundamentally flawed when applied to Indic scripts. Through empirical analysis using text similarity scores and in-depth linguistic examination, we demonstrate that these flaws lead to artificially improved performance metrics for Indic languages. We conclude by proposing a shift towards developing text normalization routines that leverage native linguistic expertise, ensuring more robust and accurate evaluations of multilingual ASR models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02449v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kavya Manohar, Leena G Pillai, Elizabeth Sherly</dc:creator>
    </item>
    <item>
      <title>How much does AI impact development speed? An enterprise-based randomized controlled trial</title>
      <link>https://arxiv.org/abs/2410.12944</link>
      <description>arXiv:2410.12944v3 Announce Type: replace-cross 
Abstract: How much does AI assistance impact developer productivity? To date, the software engineering literature has provided a range of answers, targeting a diversity of outcomes: from perceived productivity to speed on task and developer throughput. Our randomized controlled trial with 96 full-time Google software engineers contributes to this literature by sharing an estimate of the impact of three AI features on the time developers spent on a complex, enterprise-grade task. We found that AI significantly shortened the time developers spent on task. Our best estimate of the size of this effect, controlling for factors known to influence developer time on task, stands at about 21\%, although our confidence interval is large. We also found an interesting effect whereby developers who spend more hours on code-related activities per day were faster with AI. Product and future research considerations are discussed. In particular, we invite further research that explores the impact of AI at the ecosystem level and across multiple suites of AI-enhanced tools, since we cannot assume that the effect size obtained in our lab study will necessarily apply more broadly, or that the effect of AI found using internal Google tooling in the summer of 2024 will translate across tools and over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12944v3</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elise Paradis, Kate Grey, Quinn Madison, Daye Nam, Andrew Macvean, Vahid Meimand, Nan Zhang, Ben Ferrari-Church, Satish Chandra</dc:creator>
    </item>
    <item>
      <title>Two-stage Learning-to-Defer for Multi-Task Learning</title>
      <link>https://arxiv.org/abs/2410.15729</link>
      <description>arXiv:2410.15729v2 Announce Type: replace-cross 
Abstract: The Learning-to-Defer approach has been explored for classification and, more recently, regression tasks separately. Many contemporary learning tasks, however, involves both classification and regression components. In this paper, we introduce a Learning-to-Defer approach for multi-task learning that encompasses both classification and regression tasks. Our two-stage approach utilizes a rejector that defers decisions to the most accurate agent among a pre-trained joint classifier-regressor models and one or more external experts. We show that our surrogate loss is $(\mathcal{H}, \mathcal{F}, \mathcal{R})$ and Bayes--consistent, ensuring an effective approximation of the optimal solution. Additionally, we derive learning bounds that demonstrate the benefits of employing multiple confident experts along a rich model in a two-stage learning framework. Empirical experiments conducted on electronic health record analysis tasks underscore the performance enhancements achieved through our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15729v2</guid>
      <category>stat.ML</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannis Montreuil, Shu Heng Yeo, Axel Carlier, Lai Xing Ng, Wei Tsang Ooi</dc:creator>
    </item>
    <item>
      <title>GIS Copilot: Towards an Autonomous GIS Agent for Spatial Analysis</title>
      <link>https://arxiv.org/abs/2411.03205</link>
      <description>arXiv:2411.03205v3 Announce Type: replace-cross 
Abstract: Recent advancements in Generative AI offer promising capabilities for spatial analysis. Despite their potential, the integration of generative AI with established GIS platforms remains underexplored. In this study, we propose a framework for integrating LLMs directly into existing GIS platforms, using QGIS as an example. Our approach leverages the reasoning and programming capabilities of LLMs to autonomously generate spatial analysis workflows and code through an informed agent that has comprehensive documentation of key GIS tools and parameters. The implementation of this framework resulted in the development of a "GIS Copilot" that allows GIS users to interact with QGIS using natural language commands for spatial analysis. The GIS Copilot was evaluated with over 100 spatial analysis tasks with three complexity levels: basic tasks that require one GIS tool and typically involve one data layer to perform simple operations; intermediate tasks involving multi-step processes with multiple tools, guided by user instructions; and advanced tasks which involve multi-step processes that require multiple tools but not guided by user instructions, necessitating the agent to independently decide on and executes the necessary steps. The evaluation reveals that the GIS Copilot demonstrates strong potential in automating foundational GIS operations, with a high success rate in tool selection and code generation for basic and intermediate tasks, while challenges remain in achieving full autonomy for more complex tasks. This study contributes to the emerging vision of Autonomous GIS, providing a pathway for non-experts to engage with geospatial analysis with minimal prior expertise. While full autonomy is yet to be achieved, the GIS Copilot demonstrates significant potential for simplifying GIS workflows and enhancing decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03205v3</guid>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Temitope Akinboyewa, Zhenlong Li, Huan Ning, M. Naser Lessani</dc:creator>
    </item>
    <item>
      <title>Usefulness of LLMs as an Author Checklist Assistant for Scientific Papers: NeurIPS'24 Experiment</title>
      <link>https://arxiv.org/abs/2411.03417</link>
      <description>arXiv:2411.03417v2 Announce Type: replace-cross 
Abstract: Large language models (LLMs) represent a promising, but controversial, tool in aiding scientific peer review. This study evaluates the usefulness of LLMs in a conference setting as a tool for vetting paper submissions against submission standards. We conduct an experiment at the 2024 Neural Information Processing Systems (NeurIPS) conference, where 234 papers were voluntarily submitted to an "LLM-based Checklist Assistant." This assistant validates whether papers adhere to the author checklist used by NeurIPS, which includes questions to ensure compliance with research and manuscript preparation standards. Evaluation of the assistant by NeurIPS paper authors suggests that the LLM-based assistant was generally helpful in verifying checklist completion. In post-usage surveys, over 70% of authors found the assistant useful, and 70% indicate that they would revise their papers or checklist responses based on its feedback. While causal attribution to the assistant is not definitive, qualitative evidence suggests that the LLM contributed to improving some submissions. Survey responses and analysis of re-submissions indicate that authors made substantive revisions to their submissions in response to specific feedback from the LLM. The experiment also highlights common issues with LLMs: inaccuracy (20/52) and excessive strictness (14/52) were the most frequent issues flagged by authors. We also conduct experiments to understand potential gaming of the system, which reveal that the assistant could be manipulated to enhance scores through fabricated justifications, highlighting potential vulnerabilities of automated review tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03417v2</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 12 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Goldberg, Ihsan Ullah, Thanh Gia Hieu Khuong, Benedictus Kent Rachmat, Zhen Xu, Isabelle Guyon, Nihar B. Shah</dc:creator>
    </item>
  </channel>
</rss>
