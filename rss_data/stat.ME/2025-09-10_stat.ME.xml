<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 04:01:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Recursive Adaptive Importance Sampling with Optimal Replenishment</title>
      <link>https://arxiv.org/abs/2509.08102</link>
      <description>arXiv:2509.08102v1 Announce Type: new 
Abstract: Increased access to computing resources has led to the development of algorithms that can run efficiently on multi-core processing units or in distributed computing environments. In the context of Bayesian inference, many parallel computing approaches to fit statistical models have been proposed in the context of Markov Chain Monte Carlo methods, but they either have limited gains due to high latency cost or rely on model-specific decompositions. Alternatively, adaptive importance sampling, sequential Monte Carlo, and recursive Bayesian methods provide a parallel-friendly and asymptotically exact framework with well-developed theory for error estimation. We propose a recursive adaptive importance sampling approach that alternates between fast recursive weight updates and sample replenishment steps to balance computational efficiency while ensuring sample quality. We derive theoretical results to determine the optimal allocation of replenishing steps, and demonstrate the efficacy of our method in simulated experiments and an application of sea surface temperature prediction in the Gulf of Mexico using Gaussian processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08102v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel W\"urzler Barreto, Mevin B. Hooten</dc:creator>
    </item>
    <item>
      <title>Survival Analysis with Discrete Biomarkers Under a Semiparametric Bayesian Conditional Poisson Model</title>
      <link>https://arxiv.org/abs/2509.08162</link>
      <description>arXiv:2509.08162v1 Announce Type: new 
Abstract: Discrete biomarkers derived as cell densities or counts from tissue microarrays and immunostaining are widely used to study immune signatures in relation to survival outcomes in cancer. Although routinely collected, these signatures are not measured with exact precision because the sampling mechanism involves examination of small tissue cores from a larger section of interest. We model these error-prone biomarkers as Poisson processes with latent rates, inducing heteroscedasticity in their conditional variance. While critical for tumor histology, such measurement error frameworks remain understudied for conditionally Poisson-distributed covariates. To address this, we propose a Bayesian joint model that incorporates a Dirichlet process (DP) mixture to flexibly characterize the latent covariate distribution. The proposed approach is evaluated using simulation studies which demonstrate a superior bias reduction and robustness to the underlying model in realistic settings when compared to existing methods. We further incorporate Bayes factors for hypothesis testing in the Bayesian semiparametric joint model. The methodology is applied to a survival study of high-grade serous carcinoma where comparisons are made between the proposed and existing approaches. Accompanying R software is available at the GitHub repository listed in the Web Appendices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08162v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aijun Yang, Phineas T. Hamilton, Brad H. Nelson, Julian J. Lum, Mary Lesperance, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Functional Regression with Nonstationarity and Error Contamination: Application to the Economic Impact of Climate Change</title>
      <link>https://arxiv.org/abs/2509.08591</link>
      <description>arXiv:2509.08591v1 Announce Type: new 
Abstract: This paper studies a functional regression model with nonstationary dependent and explanatory functional observations, in which the nonstationary stochastic trends of the dependent variable are explained by those of the explanatory variable, and the functional observations may be error-contaminated. We develop novel autocovariance-based estimation and inference methods for this model. The methodology is broadly applicable to economic and statistical functional time series with nonstationary dynamics. To illustrate our methodology and its usefulness, we apply it to the evaluation of the global economic impact of climate change, an issue of intrinsic importance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08591v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kyungsik Nam, Won-Ki Seo</dc:creator>
    </item>
    <item>
      <title>Assessing Bias in the Variable Bandpass Periodic Block Bootstrap Method</title>
      <link>https://arxiv.org/abs/2509.08647</link>
      <description>arXiv:2509.08647v1 Announce Type: new 
Abstract: The Variable Bandpass Periodic Block Bootstrap(VBPBB) is an innovative method for time series with periodically correlated(PC) components. This method applies bandpass filters to extract specific PC components from datasets, effectively eliminating unwanted interference such as noise. It then bootstraps the PC components, maintaining their correlation structure while resampling and enabling a clearer analysis of the estimation of the statistical properties of periodic patterns in time series data. While its efficiency has been demonstrated in environmental and epidemiological research, the theoretical properties of VBPBB, particularly regarding its bias of the estimated sampling distributions, remain unexamined. This study investigates issues regarding biases in VBPBB, including overall mean bias and pointwise mean bias, across a range of time series models of varying complexity, all of which exhibit periodic components. Using the R programming language, we simulate various PC time series and apply VBPBB to assess its bias under different conditions. Our findings provide key insights into the validity of VBPBB for periodic time series analysis and offer practical recommendations for its implementation, as well as directions for future theoretical advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08647v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yanan Sun, Eric Rose, Kai Zhang, Edward Valachovic</dc:creator>
    </item>
    <item>
      <title>Generative AI as a Safety Net for Survey Question Refinement</title>
      <link>https://arxiv.org/abs/2509.08702</link>
      <description>arXiv:2509.08702v1 Announce Type: new 
Abstract: Writing survey questions that easily and accurately convey their intent to a variety of respondents is a demanding and high-stakes task. Despite the extensive literature on best practices, the number of considerations to keep in mind is vast and even small errors can render collected data unusable for its intended purpose. The process of drafting initial questions, checking for known sources of error, and developing solutions to those problems requires considerable time, expertise, and financial resources. Given the rising costs of survey implementation and the critical role that polls play in media, policymaking, and research, it is vital that we utilize all available tools to protect the integrity of survey data and the financial investments made to obtain it. Since its launch in 2022, ChatGPT and other generative AI model platforms have been integrated into everyday life processes and workflows, particularly pertaining to text revision. While many researchers have begun exploring how generative AI may assist with questionnaire design, we have implemented a prompt experiment to systematically test what kind of feedback on survey questions an average ChatGPT user can expect. Results from our zero--shot prompt experiment, which randomized the version of ChatGPT and the persona given to the model, shows that generative AI is a valuable tool today, even for an average AI user, and suggests that AI will play an increasingly prominent role in the evolution of survey development best practices as precise tools are developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08702v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erica Ann Metheney, Lauren Yehle</dc:creator>
    </item>
    <item>
      <title>Doubly robust average treatment effect estimation for survival data</title>
      <link>https://arxiv.org/abs/2509.08788</link>
      <description>arXiv:2509.08788v1 Announce Type: new 
Abstract: Considering censored outcomes in survival analysis can lead to quite complex results in the model setting of causal inference. Causal inference has attracted a lot of attention over the past few years, but little research has been done on survival analysis. Even for the only research conducted, the machine learning method was considered assuming a large sample, which is not suitable in that the actual data are high dimensional low sample size (HDLSS) method. Therefore, penalty is considered for numerous covariates, and the relationship between these covariates and treatment variables is reflected as a covariate balancing property score (CBPS). It also considers censored results. To this end, we will try to solve the above-mentioned problems by using penalized empirical likelihood, which considers both estimating equation and penalty. The proposed average treatment effect (ATE) estimator possesses the oracle property, exhibiting key characteristics such as double robustness for unbiasedness, sparsity in model selection, and asymptotic normality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08788v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Byeonghee Lee, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>On the inclusion of non-concurrent controls in platform trials with a futility interim analysis</title>
      <link>https://arxiv.org/abs/2509.08795</link>
      <description>arXiv:2509.08795v1 Announce Type: new 
Abstract: The analysis of platform trials can be enhanced by utilizing non-concurrent controls. Since including this data might also introduce bias in the treatment effect estimators if time trends are present, methods for incorporating non-concurrent controls adjusting for time have been proposed. However, so far their behavior has not been systematically investigated in platform trials that include interim analyses. To evaluate the impact of a futility interim analysis in trials utilizing non-concurrent controls, we consider a platform trial featuring two experimental arms and a shared control, with the second experimental arm entering later. We focus on a frequentist regression model that uses non-concurrent controls to estimate the treatment effect of the second arm and adjusts for time using a step function to account for temporal changes. We show that performing a futility interim analysis in Arm 1 may introduce bias in the point estimation of the effect in Arm 2, if the regression model is used without adjustment, and investigate how the marginal bias and bias conditional on the first arm continuing after the interim depend on different trial design parameters. Moreover, we propose a new estimator of the treatment effect in Arm 2, aiming to eliminate the bias introduced by both the interim analysis in Arm 1 and the time trends, and evaluate its performance in a simulation study. The newly proposed estimator is shown to substantially reduce the bias and type I error rate inflation while leading to power gains compared to an analysis using only concurrent controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08795v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pavla Krotka, Martin Posch, Marta Bofill Roig</dc:creator>
    </item>
    <item>
      <title>Selective Induction Heads: How Transformers Select Causal Structures In Context</title>
      <link>https://arxiv.org/abs/2509.08184</link>
      <description>arXiv:2509.08184v1 Announce Type: cross 
Abstract: Transformers have exhibited exceptional capabilities in sequence modeling tasks, leveraging self-attention and in-context learning. Critical to this success are induction heads, attention circuits that enable copying tokens based on their previous occurrences. In this work, we introduce a novel framework that showcases transformers' ability to dynamically handle causal structures. Existing works rely on Markov Chains to study the formation of induction heads, revealing how transformers capture causal dependencies and learn transition probabilities in-context. However, they rely on a fixed causal structure that fails to capture the complexity of natural languages, where the relationship between tokens dynamically changes with context. To this end, our framework varies the causal structure through interleaved Markov chains with different lags while keeping the transition probabilities fixed. This setting unveils the formation of Selective Induction Heads, a new circuit that endows transformers with the ability to select the correct causal structure in-context. We empirically demonstrate that transformers learn this mechanism to predict the next token by identifying the correct lag and copying the corresponding token from the past. We provide a detailed construction of a 3-layer transformer to implement the selective induction head, and a theoretical analysis proving that this mechanism asymptotically converges to the maximum likelihood solution. Our findings advance the understanding of how transformers select causal structures, providing new insights into their functioning and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08184v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco D'Angelo, Francesco Croce, Nicolas Flammarion</dc:creator>
    </item>
    <item>
      <title>kNNSampler: Stochastic Imputations for Recovering Missing Value Distributions</title>
      <link>https://arxiv.org/abs/2509.08366</link>
      <description>arXiv:2509.08366v1 Announce Type: cross 
Abstract: We study a missing-value imputation method, termed kNNSampler, that imputes a given unit's missing response by randomly sampling from the observed responses of the $k$ most similar units to the given unit in terms of the observed covariates. This method can sample unknown missing values from their distributions, quantify the uncertainties of missing values, and be readily used for multiple imputation. Unlike popular kNNImputer, which estimates the conditional mean of a missing response given an observed covariate, kNNSampler is theoretically shown to estimate the conditional distribution of a missing response given an observed covariate. Experiments demonstrate its effectiveness in recovering the distribution of missing values. The code for kNNSampler is made publicly available (https://github.com/SAP/knn-sampler).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08366v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parastoo Pashmchi, Jerome Benoit, Motonobu Kanagawa</dc:creator>
    </item>
    <item>
      <title>Bridging Binarization: Causal Inference with Dichotomized Continuous Exposures</title>
      <link>https://arxiv.org/abs/2405.07109</link>
      <description>arXiv:2405.07109v4 Announce Type: replace 
Abstract: The average treatment effect (ATE) is a common parameter estimated in causal inference literature, but it is only defined for binary exposures. Thus, despite concerns raised by some researchers, many studies seeking to estimate the causal effect of a continuous exposure create a new binary exposure variable by dichotomizing the continuous values into two categories. In this paper, we affirm binarization as a statistically valid method for answering causal questions about continuous exposures by showing the equivalence between the binarized ATE and the difference in the average outcomes of two specific modified treatment policies. These policies impose cut-offs corresponding to the binarized exposure variable and assume preservation of relative self-selection. Relative self-selection is the ratio of the probability density of an individual having an exposure equal to one value of the continuous exposure variable versus another. The policies assume that, for any two values of the exposure variable with non-zero probability density after the cut-off, this ratio will remain unchanged. Through this equivalence, we clarify the assumptions underlying binarization and discuss how to properly interpret the resulting estimator. Additionally, we introduce a new target parameter that can be computed after binarization that considers the observed world as a benchmark. We argue that this parameter addresses more relevant causal questions than the traditional binarized ATE parameter. We present a simulation study to illustrate the implications of these assumptions when analyzing data and to demonstrate how to correctly implement estimators of the parameters discussed. Finally, we present an application of this method to evaluate the effect of a law in the state of California which seeks to limit exposures to oil and gas wells on birth outcomes to further illustrate the underlying assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07109v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn J. Lee, Alan Hubbard, Alejandro Schuler</dc:creator>
    </item>
    <item>
      <title>Weighted Brier Score -- an Overall Summary Measure for Risk Prediction Models with Clinical Utility Consideration</title>
      <link>https://arxiv.org/abs/2408.01626</link>
      <description>arXiv:2408.01626v2 Announce Type: replace 
Abstract: As advancements in novel biomarker-based algorithms and models accelerate disease risk prediction and stratification in medicine, it is crucial to evaluate these models within the context of their intended clinical application. Prediction models output the absolute risk of disease; subsequently, patient counseling and shared decision-making are based on the estimated individual risk and cost-benefit assessment. The overall impact of the application is often referred to as clinical utility, which received significant attention in terms of model assessment lately. The classic Brier score is a popular measure of prediction accuracy; however, it is insufficient for effectively assessing clinical utility. To address this limitation, we propose a class of weighted Brier scores that aligns with the decision-theoretic framework of clinical utility. Additionally, we decompose the weighted Brier score into discrimination and calibration components, examining how weighting influences the overall score and its individual components. Through this decomposition, we link the weighted Brier score to the $H$ measure, which has been proposed as a coherent alternative to the area under the receiver operating characteristic curve. This theoretical link to the $H$ measure further supports our weighting method and underscores the essential elements of discrimination and calibration in risk prediction evaluation. The practical use of the weighted Brier score as an overall summary is demonstrated using data from the Prostate Cancer Active Surveillance Study (PASS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01626v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kehao Zhu, Yingye Zheng, Kwun Chuen Gary Chan</dc:creator>
    </item>
    <item>
      <title>Outlier-Robust Multi-Group Gaussian Mixture Modeling with Flexible Group Reassignment</title>
      <link>https://arxiv.org/abs/2504.02547</link>
      <description>arXiv:2504.02547v2 Announce Type: replace 
Abstract: Do expert-defined or diagnostically-labeled data groups align with clusters inferred through statistical modeling? If not, where do discrepancies between predefined labels and model-based groupings occur and why? In this work, we show how to address these questions using the multi-group Gaussian mixture model (MG-GMM). This novel model incorporates prior group information while allowing flexibility to reassign observations to alternative groups based on data-driven evidence. We achieve this by modeling the observations of each group as arising not from a single distribution, but from a Gaussian mixture comprising all group-specific distributions. Moreover, our model offers robustness against cellwise outliers that may obscure or distort the underlying group structure. We propose a new penalized likelihood approach, called cellMG-GMM, to jointly estimate mixture probabilities, location and scale parameters of the MG-GMM, and detect outliers through a penalty term on the number of flagged cellwise outliers in the objective function. We show that our estimator has good breakdown properties in presence of cellwise outliers. We develop a computationally-efficient EM-based algorithm for cellMG-GMM, and demonstrate its strong performance in identifying and diagnosing observations at the intersection of multiple groups through simulations and diverse applications in meteorology, medicine and oenology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02547v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patricia Puchhammer, Ines Wilms, Peter Filzmoser</dc:creator>
    </item>
    <item>
      <title>Introducing RobustiPy: An efficient next generation multiversal library with model selection, averaging, resampling, and explainable artificial intelligence</title>
      <link>https://arxiv.org/abs/2506.19958</link>
      <description>arXiv:2506.19958v2 Announce Type: replace 
Abstract: Scientific inference is often undermined by the vast but rarely explored "multiverse" of defensible modelling choices, which can generate results as variable as the phenomena under study. We introduce RobustiPy, an open-source Python library that systematizes multiverse analysis and model-uncertainty quantification at scale. RobustiPy unifies bootstrap-based inference, combinatorial specification search, model selection and averaging, joint-inference routines, and explainable AI methods within a modular, reproducible framework. Beyond exhaustive specification curves, it supports rigorous out-of-sample validation and quantifies the marginal contribution of each covariate. We demonstrate its utility across five simulation designs and ten empirical case studies spanning economics, sociology, psychology, and medicine, including a re-analysis of widely cited findings with documented discrepancies. Benchmarking on ~672 million simulated regressions shows that RobustiPy delivers state-of-the-art computational efficiency while expanding transparency in empirical research. By standardizing and accelerating robustness analysis, RobustiPy transforms how researchers interrogate sensitivity across the analytical multiverse, offering a practical foundation for more reproducible and interpretable computational science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19958v2</guid>
      <category>stat.ME</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Valdenegro Ibarra, Jiani Yan, Duiyi Dai, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>Adding structure to generalized additive models, with applications in ecology</title>
      <link>https://arxiv.org/abs/2508.07915</link>
      <description>arXiv:2508.07915v2 Announce Type: replace 
Abstract: Generalized additive models (GAMs) connecting a set of scalar covariates that map 1-1 to a response are commonly employed in ecology and beyond. However, covariates are often inherently non-scalar, taking multiple values for each observation of the response. They can sometimes have a temporal structure, e.g., a time series of temperatures, or a spatial structure, e.g., multiple soil pH measurements made at nearby locations. While aggregating or selectively summarizing such covariates to yield a scalar covariate allows the use of standard GAM fitting procedures, exactly how to do so can be problematic and information is necessarily lost. Naively including all $p$ components of a vector-valued covariate as $p$ separate covariates, say, without recognizing the structure, can lead to problems of multicollinearity, data sets that are excessively wide given the sample size, and difficulty extracting the primary signal provided by the covariate. Here we introduce three useful extensions to GAMs that efficiently and effectively handle vector-valued covariates without requiring one to choose aggregations or selective summarizations. These extensions are varying-coefficient, scalar-on-function and distributed lag models. While these models have existed for some time they remain relatively underused in ecology. This article aims to show when these models can be useful and how to fit them with the popular R package \mgcv{}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07915v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David L Miller, Ken Newman, Thomas Cornulier</dc:creator>
    </item>
    <item>
      <title>Variance Estimation for Weighted Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2508.08167</link>
      <description>arXiv:2508.08167v2 Announce Type: replace 
Abstract: Common variance estimation methods for weighted average treatment effects (WATEs) in observational studies include nonparametric bootstrap and model-based, closed-form sandwich variance estimation. However, the computational cost of bootstrap increases with the size of the data at hand. Besides, some replicates may exhibit random violations of the positivity assumption even when the original data do not. Sandwich variance estimation relies on regularity conditions that may be structurally violated. Moreover, the sandwich variance estimation is model-dependent on the propensity score model, the outcome model, or both; thus it does not have a unified closed-form expression. Recent studies have explored the use of wild bootstrap to estimate the variance of the average treatment effect on the treated (ATT). This technique adopts a one-dimensional, nonparametric, and computationally efficient resampling strategy. In this article, we propose a "post-weighting" bootstrap approach as an alternative to the conventional bootstrap, which helps avoid random positivity violations in replicates and improves computational efficiency. We also generalize the wild bootstrap algorithm from ATT to the broader class of WATEs by providing new justification for correctly accounting for sampling variability from multiple sources under different weighting functions. We evaluate the performance of all four methods through extensive simulation studies and demonstrate their application using data from the National Health and Nutrition Examination Survey (NHANES). Our findings offer several practical recommendations for the variance estimation of WATE estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08167v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huiyue Li, Yi Liu, Yunji Zhou, Jiajun Liu, Dezhao Fu, Roland A. Matsouaka</dc:creator>
    </item>
    <item>
      <title>Interpretable Scalar-on-Image Linear Regression Models via the Generalized Dantzig Selector</title>
      <link>https://arxiv.org/abs/2508.20278</link>
      <description>arXiv:2508.20278v2 Announce Type: replace 
Abstract: The scalar-on-image regression model examines the association between a scalar response and a bivariate function (e.g., images) through the estimation of a bivariate coefficient function. Existing approaches often impose smoothness constraints to control the bias-variance trade-off, and thus prevent overfitting. However, such assumptions can hinder interpretability, especially when only certain regions of an image influence changes in the response. In such a scenario, interpretability can be better captured by imposing sparsity assumptions on the coefficient function. To address this challenge, we propose the Generalized Dantzig Selector, a novel method that jointly enforces sparsity and smoothness on the coefficient function. The proposed approach enhances interpretability by accurately identifying regions with no contribution to the changes of response, while preserving stability in estimation. Extensive simulation studies and real data applications demonstrate that the new method is highly interpretable and achieves notable improvements over existing approaches. Moreover, we rigorously establish non-asymptotic bounds for the estimation error, providing strong theoretical guarantees for the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.20278v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sijia Liao, Xiaoxiao Sun, Ning Hao, Hao Helen Zhang</dc:creator>
    </item>
    <item>
      <title>PQMass: Probabilistic Assessment of the Quality of Generative Models using Probability Mass Estimation</title>
      <link>https://arxiv.org/abs/2402.04355</link>
      <description>arXiv:2402.04355v3 Announce Type: replace-cross 
Abstract: We propose a likelihood-free method for comparing two distributions given samples from each, with the goal of assessing the quality of generative models. The proposed approach, PQMass, provides a statistically rigorous method for assessing the performance of a single generative model or the comparison of multiple competing models. PQMass divides the sample space into non-overlapping regions and applies chi-squared tests to the number of data samples that fall within each region, giving a p-value that measures the probability that the bin counts derived from two sets of samples are drawn from the same multinomial distribution. PQMass does not depend on assumptions regarding the density of the true distribution, nor does it rely on training or fitting any auxiliary models. We evaluate PQMass on data of various modalities and dimensions, demonstrating its effectiveness in assessing the quality, novelty, and diversity of generated samples. We further show that PQMass scales well to moderately high-dimensional data and thus obviates the need for feature extraction in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04355v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pablo Lemos, Sammy Sharief, Nikolay Malkin, Salma Salhi, Connor Stone, Laurence Perreault-Levasseur, Yashar Hezaveh</dc:creator>
    </item>
    <item>
      <title>Unified calibration and spatial mapping of fine particulate matter data from multiple low-cost air pollution sensor networks in Baltimore, Maryland</title>
      <link>https://arxiv.org/abs/2412.13034</link>
      <description>arXiv:2412.13034v2 Announce Type: replace-cross 
Abstract: Low-cost air pollution sensor networks are increasingly being deployed globally, supplementing sparse regulatory monitoring with localized air quality data. In some areas, like Baltimore, Maryland, there are only few regulatory (reference) devices but multiple low-cost networks. While there are many available methods to calibrate data from each network individually, separate calibration of each network leads to conflicting air quality predictions. We develop a general Bayesian spatial filtering model combining data from multiple networks and reference devices, providing dynamic calibrations (informed by the latest reference data) and unified predictions (combining information from all available sensors) for the entire region. This method accounts for network-specific bias and noise (observation models), as different networks can use different types of sensors, and uses a Gaussian process (state-space model) to capture spatial correlations. We apply the method to calibrate PM$_{2.5}$ data from Baltimore in June and July 2023 -- a period including days of hazardous concentrations due to wildfire smoke. Our method helps mitigate the effects of preferential sampling of one network in Baltimore, results in better predictions and narrower confidence intervals. Our approach can be used to calibrate low-cost air pollution sensor data in Baltimore and any other areas with multiple low-cost networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13034v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire Heffernan, Kirsten Koehler, Drew R. Gentner, Roger D. Peng, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>A Multi-fidelity Estimator of the Expected Information Gain for Bayesian Optimal Experimental Design</title>
      <link>https://arxiv.org/abs/2501.10845</link>
      <description>arXiv:2501.10845v2 Announce Type: replace-cross 
Abstract: Optimal experimental design (OED) is a framework that leverages a mathematical model of the experiment to identify optimal conditions for conducting the experiment. Under a Bayesian approach, the design objective function is typically chosen to be the expected information gain (EIG). However, EIG is intractable for nonlinear models and must be estimated numerically. Estimating the EIG generally entails some variant of Monte Carlo sampling, requiring repeated data model and likelihood evaluations $\unicode{x2013}$ each involving solving the governing equations of the experimental physics $\unicode{x2013}$ under different sample realizations. This computation becomes impractical for high-fidelity models.
  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the approximate control variate (ACV) framework. This estimator is unbiased with respect to the high-fidelity mean, and minimizes variance under a given computational budget. We achieve this by first reparameterizing the EIG so that its expectations are independent of the data models, a requirement for compatibility with ACV. We then provide specific examples under different data model forms, as well as practical enhancements of sample size optimization and sample reuse techniques. We demonstrate the MF-EIG estimator in two numerical examples: a nonlinear benchmark and a turbulent flow problem involving the calibration of shear-stress transport turbulence closure model parameters within the Reynolds-averaged Navier-Stokes model. We validate the estimator's unbiasedness and observe one- to two-orders-of-magnitude variance reduction compared to existing single-fidelity EIG estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10845v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas E. Coons, Xun Huan</dc:creator>
    </item>
  </channel>
</rss>
