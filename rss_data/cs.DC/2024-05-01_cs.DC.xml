<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 02 May 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 02 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Experimental Evaluation of the PHP's cURL Library Performance</title>
      <link>https://arxiv.org/abs/2405.00001</link>
      <description>arXiv:2405.00001v1 Announce Type: new 
Abstract: cURL (libcurl) is a popular and widely used library distributed with the php interpreter. It allows php applications to connect to and communicate with external resources (servers) by using wide variety of communication protocols. In most cases it is the preferred way of consuming external REST web services. Programmers usually use it for granted without even thinking of any performance issues. During an experimental analysis of the Hadoop's WebHDFS API throughput, it has been noted that read (download) speed from WebHDFS reduces with increasing the file size. However, this issue does not happen when writing to WebHDFS. Since the communication between the php application and the WebHDFS API is handled by the php's cURL library, then the cause of the download speed decrease could be either the cURL library itself or the API. This paper presents a series of experimental analyses aiming to determine the cause of the download speed decrease in previous experiments - whether it is the WebHDFS API or the php's cURL library. Both parties are tested in multiple ways separately and independently of each other. Results clearly prove (in two different ways) that the cause of the download speed decrease is the php's cURL library itself, not the consumed API.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00001v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>PROCEEDINGS OF UNIVERSITY OF RUSE - 2023, volume 62, book 3.2., pp. 28-33</arxiv:journal_reference>
      <dc:creator>Yordan Kalmukov</dc:creator>
    </item>
    <item>
      <title>TALICS$^3$: Tape Library Cloud Storage System Simulator</title>
      <link>https://arxiv.org/abs/2405.00003</link>
      <description>arXiv:2405.00003v1 Announce Type: new 
Abstract: High performance computing data is surging fast into the exabyte-scale world, where tape libraries are the main platform for long-term durable data storage besides high-cost DNA. Tape libraries are extremely hard to model, but accurate modeling is critical for system administrators to obtain valid performance estimates for their designs. This research introduces a discrete event tape simulation platform that realistically models tape library behavior in a networked cloud environment, by incorporating real-world phenomena and effects. The platform addresses several challenges, including precise estimation of data access latency, rates of robot exchange, data collocation, deduplication/compression ratio, and attainment of durability goals through replication or erasure coding. The suggested simulator has the capability to compare the single enterprise configuration with multiple commodity library (RAIL) configurations, making it a useful tool for system administrators and reliability engineers. They can use the simulator to obtain practical and reliable performance estimates for their long-term, durable, and cost-effective cold data storage architecture designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00003v1</guid>
      <category>cs.DC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suayb S. Arslan, James Peng, Turguy Goker</dc:creator>
    </item>
    <item>
      <title>Self-healing Nodes with Adaptive Data-Sharding</title>
      <link>https://arxiv.org/abs/2405.00004</link>
      <description>arXiv:2405.00004v1 Announce Type: new 
Abstract: Data sharding, a technique for partitioning and distributing data among multiple servers or nodes, offers enhancements in the scalability, performance, and fault tolerance of extensive distributed systems. Nonetheless, this strategy introduces novel challenges, including load balancing among shards, management of node failures and data loss, and adaptation to evolving data and workload patterns. This paper proposes an innovative approach to tackle these challenges by empowering self-healing nodes with adaptive data sharding. Leveraging concepts such as self-replication, fractal regeneration, sentient data sharding, and symbiotic node clusters, our approach establishes a dynamic and resilient data sharding scheme capable of addressing diverse scenarios and meeting varied requirements. Implementation and evaluation of our approach involve a prototype system simulating a large-scale distributed database across various data sharding scenarios. Comparative analyses against existing data sharding techniques highlight the superior scalability, performance, fault tolerance, and adaptability of our approach. Additionally, the paper delves into potential applications and limitations, providing insights into the future research directions that can further advance this innovative approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00004v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ayush Thakur, Sanskar Chauhan, Ilisha Tomar, Vaibhavi Paul, Deepak Gupta</dc:creator>
    </item>
    <item>
      <title>Scheduling of Distributed Applications on the Computing Continuum: A Survey</title>
      <link>https://arxiv.org/abs/2405.00005</link>
      <description>arXiv:2405.00005v1 Announce Type: new 
Abstract: The demand for distributed applications has significantly increased over the past decade, with improvements in machine learning techniques fueling this growth. These applications predominantly utilize Cloud data centers for high-performance computing and Fog and Edge devices for low-latency communication for small-size machine learning model training and inference. The challenge of executing applications with different requirements on heterogeneous devices requires effective methods for solving NP-hard resource allocation and application scheduling problems. The state-of-the-art techniques primarily investigate conflicting objectives, such as the completion time, energy consumption, and economic cost of application execution on the Cloud, Fog, and Edge computing infrastructure. Therefore, in this work, we review these research works considering their objectives, methods, and evaluation tools. Based on the review, we provide a discussion on the scheduling methods in the Computing Continuum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00005v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3603166.3632540</arxiv:DOI>
      <dc:creator>Narges Mehran, Dragi Kimovski, Hermann Hellwagner, Dumitru Roman, Ahmet Soylu, Radu Prodan</dc:creator>
    </item>
    <item>
      <title>Service Level Agreements and Security SLA: A Comprehensive Survey</title>
      <link>https://arxiv.org/abs/2405.00009</link>
      <description>arXiv:2405.00009v1 Announce Type: new 
Abstract: A Service Level Agreement (SLA) is a formal contract between a service provider and a consumer, representing a crucial instrument to define, manage, and maintain relationships between these two parties. The SLA's ability to define the Quality of Service (QoS) expectations, standards, and accountability helps to deliver high-quality services and increase client confidence in disparate application domains, such as Cloud computing and the Internet of Things. An open research direction in this context is related to the possible integration of new metrics to address the security and privacy aspects of services, thus providing protection of sensitive information, mitigating risks, and building trust. This survey paper identifies state of the art covering concepts, approaches, and open problems of SLA management with a distinctive and original focus on the recent development of Security SLA (SecSLA). It contributes by carrying out a comprehensive review and covering the gap between the analyses proposed in existing surveys and the most recent literature on this topic, spanning from 2017 to 2023. Moreover, it proposes a novel classification criterium to organize the analysis based on SLA life cycle phases. This original point of view can help both academics and industrial practitioners to understand and properly locate existing contributions in the advancement of the different aspects of SLA technology. The present work highlights the importance of the covered topics and the need for new research improvements to tackle present and demanding challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00009v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Serena Nicolazzo, Antonino Nocera, Witold Pedrycz</dc:creator>
    </item>
    <item>
      <title>A Review on Industrial Augmented Reality Systems for the Industry 4.0 Shipyard</title>
      <link>https://arxiv.org/abs/2405.00010</link>
      <description>arXiv:2405.00010v1 Announce Type: new 
Abstract: Shipbuilding companies are upgrading their inner workings in order to create Shipyards 4.0, where the principles of Industry 4.0 are paving the way to further digitalized and optimized processes in an integrated network. Among the different Industry 4.0 technologies, this article focuses on Augmented Reality, whose application in the industrial field has led to the concept of Industrial Augmented Reality (IAR). This article first describes the basics of IAR and then carries out a thorough analysis of the latest IAR systems for industrial and shipbuilding applications. Then, in order to build a practical IAR system for shipyard workers, the main hardware and software solutions are compared. Finally, as a conclusion after reviewing all the aspects related to IAR for shipbuilding, it is proposed an IAR system architecture that combines Cloudlets and Fog Computing, which reduce latency response and accelerate rendering tasks while offloading compute intensive tasks from the Cloud.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00010v1</guid>
      <category>cs.DC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ACCESS.2018.2808326.</arxiv:DOI>
      <arxiv:journal_reference>P. Fraga-Lamas, T. M. Fernandez-Carames, O. Blanco-Novoa and M. A. Vilar-Montesinos, "A Review on Industrial Augmented Reality Systems for the Industry 4.0 Shipyard," in IEEE Access, vol. 6, pp. 13358-13375, 2018</arxiv:journal_reference>
      <dc:creator>Paula Fraga-Lamas, Tiago M Fernandez-Carames, Oscar Blanco-Novoa, Miguel Vilar-Montesinos</dc:creator>
    </item>
    <item>
      <title>The GA4GH Task Execution API: Enabling Easy Multi Cloud Task Execution</title>
      <link>https://arxiv.org/abs/2405.00013</link>
      <description>arXiv:2405.00013v1 Announce Type: new 
Abstract: The Global Alliance for Genomics and Health (GA4GH) Task Execution Service (TES) API is a standardized schema and API for describing and executing batch execution tasks. It provides a common way to submit and manage tasks to a variety of compute environments, including on premise High Performance Compute and High Throughput Computing (HPC/HTC) systems, Cloud computing platforms, and hybrid environments. The TES API is designed to be flexible and extensible, allowing it to be adapted to a wide range of use cases, such as "bringing compute to the data" solutions for federated and distributed data analysis or load balancing across multi cloud infrastructures. This API has been adopted by a number of different service providers and utilized by several workflow engines. Using its capabilities, genomes research institutes are building hybrid compute systems to study life science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00013v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Kanitz, Matthew H. McLoughlin, Liam Beckman, Venkat S. Malladi, Kyle P. Ellrott</dc:creator>
    </item>
    <item>
      <title>Experiences Porting Distributed Applications to Asynchronous Tasks: A Multidimensional FFT Case-study</title>
      <link>https://arxiv.org/abs/2405.00015</link>
      <description>arXiv:2405.00015v1 Announce Type: new 
Abstract: Parallel algorithms relying on synchronous parallelization libraries often experience adverse performance due to global synchronization barriers. Asynchronous many-task runtimes offer task futurization capabilities that minimize or remove the need for global synchronization barriers. This paper conducts a case study of the multidimensional Fast Fourier Transform to identify which applications will benefit from the asynchronous many-task model. Our basis is the popular FFTW library. We use the asynchronous many-task model HPX and a one-dimensional FFTW backend to implement multiple versions using different HPX features and highlight overheads and pitfalls during migration. Furthermore, we add an HPX threading backend to FFTW. The case study analyzes shared memory scaling properties between our HPX-based parallelization and FFTW with its pthreads, OpenMP, and HPX backends. The case study also compares FFTW's MPI+X backend to a purely HPX-based distributed implementation. The FFT application does not profit from asynchronous task execution. In contrast, enforcing task synchronization results in better cache performance and thus better runtime. Nonetheless, the HPX backend for FFTW is competitive with existing backends. Our distributed HPX implementation based on HPX collectives using MPI parcelport performs similarly to FFTW's MPI+OpenMP. However, the LCI parcelport of HPX accelerated communication up to a factor of 5.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00015v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Strack, Christopher Taylor, Patrick Diehl, Dirk Pfl\"uger</dc:creator>
    </item>
    <item>
      <title>HPX with Spack and Singularity Containers: Evaluating Overheads for HPX/Kokkos using an astrophysics application</title>
      <link>https://arxiv.org/abs/2405.00016</link>
      <description>arXiv:2405.00016v1 Announce Type: new 
Abstract: Cloud computing for high performance computing resources is an emerging topic. This service is of interest to researchers who care about reproducible computing, for software packages with complex installations, and for companies or researchers who need the compute resources only occasionally or do not want to run and maintain a supercomputer on their own. The connection between HPC and containers is exemplified by the fact that Microsoft Azure's Eagle cloud service machine is number three on the November 23 Top 500 list. For cloud services, the HPC application and dependencies are installed in containers, e.g. Docker, Singularity, or something else, and these containers are executed on the physical hardware. Although containerization leverages the existing Linux kernel and should not impose overheads on the computation, there is the possibility that machine-specific optimizations might be lost, particularly machine-specific installs of commonly used packages. In this paper, we will use an astrophysics application using HPX-Kokkos and measure overheads on homogeneous resources, e.g. Supercomputer Fugaku, using CPUs only and on heterogenous resources, e.g. LSU's hybrid CPU and GPU system. We will report on challenges in compiling, running, and using the containers as well as performance performance differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00016v1</guid>
      <category>cs.DC</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Diehl, Steven R. Brandt, Gregor Dai{\ss}, Hartmut Kaiser</dc:creator>
    </item>
    <item>
      <title>Queuing dynamics of asynchronous Federated Learning</title>
      <link>https://arxiv.org/abs/2405.00017</link>
      <description>arXiv:2405.00017v1 Announce Type: new 
Abstract: We study asynchronous federated learning mechanisms with nodes having potentially different computational speeds. In such an environment, each node is allowed to work on models with potential delays and contribute to updates to the central server at its own pace. Existing analyses of such algorithms typically depend on intractable quantities such as the maximum node delay and do not consider the underlying queuing dynamics of the system. In this paper, we propose a non-uniform sampling scheme for the central server that allows for lower delays with better complexity, taking into account the closed Jackson network structure of the associated computational graph. Our experiments clearly show a significant improvement of our method over current state-of-the-art asynchronous algorithms on an image classification problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00017v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Leconte, Matthieu Jonckheere, Sergey Samsonov, Eric Moulines</dc:creator>
    </item>
    <item>
      <title>Proof-of-concept: Using ChatGPT to Translate and Modernize an Earth System Model from Fortran to Python/JAX</title>
      <link>https://arxiv.org/abs/2405.00018</link>
      <description>arXiv:2405.00018v1 Announce Type: new 
Abstract: Earth system models (ESMs) are vital for understanding past, present, and future climate, but they suffer from legacy technical infrastructure. ESMs are primarily implemented in Fortran, a language that poses a high barrier of entry for early career scientists and lacks a GPU runtime, which has become essential for continued advancement as GPU power increases and CPU scaling slows. Fortran also lacks differentiability - the capacity to differentiate through numerical code - which enables hybrid models that integrate machine learning methods. Converting an ESM from Fortran to Python/JAX could resolve these issues. This work presents a semi-automated method for translating individual model components from Fortran to Python/JAX using a large language model (GPT-4). By translating the photosynthesis model from the Community Earth System Model (CESM), we demonstrate that the Python/JAX version results in up to 100x faster runtimes using GPU parallelization, and enables parameter estimation via automatic differentiation. The Python code is also easy to read and run and could be used by instructors in the classroom. This work illustrates a path towards the ultimate goal of making climate models fast, inclusive, and differentiable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00018v1</guid>
      <category>cs.DC</category>
      <category>physics.ao-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Zhou, Linnia Hawkins, Pierre Gentine</dc:creator>
    </item>
    <item>
      <title>Swarm UAVs Communication</title>
      <link>https://arxiv.org/abs/2405.00024</link>
      <description>arXiv:2405.00024v1 Announce Type: new 
Abstract: The advancement in cyber-physical systems has opened a new way in disaster management and rescue operations. The usage of UAVs is very promising in this context. UAVs, mainly quadcopters, are small in size and their payload capacity is limited. A single UAV can not traverse the whole area. Hence multiple UAVs or swarms of UAVs come into the picture managing the entire payload in a modular and equiproportional manner. In this work we have explored a vast topic related to UAVs. Among the UAVs quadcopter is the main focus. We explored the types of quadcopters, their flying strategy,their communication protocols, architecture and controlling techniques, followed by the swarm behaviour in nature and UAVs. Swarm behaviour and a few swarm optimization algorithms has been explored here. Swarm architecture and communication in between swarm UAV networks also got a special attention in our work. In disaster management the UAV swarm network must have to search a large area. And for this proper path planning algorithm is required. We have discussed the existing path planning algorithm, their advantages and disadvantages in great detail. Formation maintenance of the swarm network is an important issue which has been explored through leader-follower technique. The wireless path loss model has been modelled using friis and ground ray reflection model. Using this path loss models we have managed to create the link budget and simulate the variation of communication link performance with the variation of distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00024v1</guid>
      <category>cs.DC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arindam Majee, Rahul Saha, Snehasish Roy, Srilekha Mandal, Sayan Chatterjee</dc:creator>
    </item>
    <item>
      <title>MaRDIFlow: A CSE workflow framework for abstracting meta-data from FAIR computational experiments</title>
      <link>https://arxiv.org/abs/2405.00028</link>
      <description>arXiv:2405.00028v1 Announce Type: new 
Abstract: Numerical algorithms and computational tools are instrumental in navigating and addressing complex simulation and data processing tasks. The exponential growth of metadata and parameter-driven simulations has led to an increasing demand for automated workflows that can replicate computational experiments across platforms. In general, a computational workflow is defined as a sequential description for accomplishing a scientific objective, often described by tasks and their associated data dependencies. If characterized through input-output relation, workflow components can be structured to allow interchangeable utilization of individual tasks and their accompanying metadata. In the present work, we develop a novel computational framework, namely, MaRDIFlow, that focuses on the automation of abstracting meta-data embedded in an ontology of mathematical objects. This framework also effectively addresses the inherent execution and environmental dependencies by incorporating them into multi-layered descriptions. Additionally, we demonstrate a working prototype with example use cases and methodically integrate them into our workflow tool and data provenance framework. Furthermore, we show how to best apply the FAIR principles to computational workflows, such that abstracted components are Findable, Accessible, Interoperable, and Reusable in nature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00028v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pavan L. Veluvali, Jan Heiland, Peter Benner</dc:creator>
    </item>
    <item>
      <title>DeepOps &amp; SLURM: Your GPU Cluster Guide</title>
      <link>https://arxiv.org/abs/2405.00030</link>
      <description>arXiv:2405.00030v1 Announce Type: new 
Abstract: In the ever evolving landscape of deep learning, unlocking the potential of cutting-edge models demands computational resources that surpass the capabilities of individual machines. Enter the NVIDIA DeepOps Slurm cluster, a meticulously orchestrated symphony of high-performance nodes, each equipped with powerful GPUs and meticulously managed by the efficient Slurm resource allocation system. This guide serves as your comprehensive roadmap, empowering you to harness the immense parallel processing capabilities of this cluster and propel your deep learning endeavors to new heights. Whether you are a seasoned deep learning practitioner seeking to optimize performance or a newcomer eager to unlock the power of parallel processing, this guide caters to your needs. We wll delve into the intricacies of the cluster hardware architecture, exploring the capabilities of its GPUs and the underlying network fabric. You will master the art of leveraging DeepOps containers for efficient and reproducible workflows, fine-tune resource configurations for optimal performance, and confidently submit jobs to unleash the full potential of parallel processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00030v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arindam Majee</dc:creator>
    </item>
    <item>
      <title>A Communication Avoiding and Reducing Algorithm for Symmetric Eigenproblem for Very Small Matrices</title>
      <link>https://arxiv.org/abs/2405.00326</link>
      <description>arXiv:2405.00326v1 Announce Type: new 
Abstract: In this paper, a parallel symmetric eigensolver with very small matrices in massively parallel processing is considered. We define very small matrices that fit the sizes of caches per node in a supercomputer. We assume that the sizes also fit the exa-scale computing requirements of current production runs of an application. To minimize communication time, we added several communication avoiding and communication reducing algorithms based on Message Passing Interface (MPI) non-blocking implementations. A performance evaluation with up to full nodes of the FX10 system indicates that (1) the MPI non-blocking implementation is 3x as efficient as the baseline implementation, (2) the hybrid MPI execution is 1.9x faster than the pure MPI execution, (3) our proposed solver is 2.3x and 22x faster than a ScaLAPACK routine with optimized blocking size and cyclic-cyclic distribution, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00326v1</guid>
      <category>cs.DC</category>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takahiro Katagiri, Jun'ichi Iwata, Kazuyuki Uchida</dc:creator>
    </item>
    <item>
      <title>Porting HPC Applications to AMD Instinct$^\text{TM}$ MI300A Using Unified Memory and OpenMP</title>
      <link>https://arxiv.org/abs/2405.00436</link>
      <description>arXiv:2405.00436v1 Announce Type: new 
Abstract: AMD Instinct$^\text{TM}$ MI300A is the world's first data center accelerated processing unit (APU) with memory shared between the AMD "Zen 4" EPYC$^\text{TM}$ cores and third generation CDNA$^\text{TM}$ compute units. A single memory space offers several advantages: i) it eliminates the need for data replication and costly data transfers, ii) it substantially simplifies application development and allows an incremental acceleration of applications, iii) is easy to maintain, and iv) its potential can be well realized via the abstractions in the OpenMP 5.2 standard, where the host and the device data environments can be unified in a more performant way. In this article, we provide a blueprint of the APU programming model leveraging unified memory and highlight key distinctions compared to the conventional approach with discrete GPUs. OpenFOAM, an open-source C++ library for computational fluid dynamics, is presented as a case study to emphasize the flexibility and ease of offloading a full-scale production-ready application on MI300 APUs using directive-based OpenMP programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00436v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Suyash Tandon, Leopold Grinberg, Gheorghe-Teodor Bercea, Carlo Bertolli, Mark Olesen, Simone Bn\`a, Nicholas Malaya</dc:creator>
    </item>
    <item>
      <title>A Confirmation Rule for the Ethereum Consensus Protocol</title>
      <link>https://arxiv.org/abs/2405.00549</link>
      <description>arXiv:2405.00549v1 Announce Type: new 
Abstract: A Confirmation Rule, within blockchain networks, refers to an algorithm implemented by network nodes that determines (either probabilistically or deterministically) the permanence of certain blocks on the blockchain. An example of Confirmation Rule is the Bitcoin's longest chain Confirmation Rule where a block is confirmed (with high probability) when it has a sufficiently long chain of successors, its siblings have notably shorter successor chains, and network synchrony holds. In this work, we devise a Confirmation Rule for Ethereum's consensus protocol, Gasper. Initially, our focus is on developing a rule specifically for LMD-GHOST, the component of Gasper responsible for ensuring dynamic availability. This is done independently of the influence of FFG-Casper, which is designed to finalize the blocks produced by LMD-GHOST. Subsequently, we build upon this rule to consider FFG-Casper's impact, aiming to achieve fast block confirmations through a heuristic that balances confirmation speed with a trade-off in safety guarantees. This refined Confirmation Rule could potentially standardize fast block confirmation within Gasper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00549v1</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aditya Asgaonkar, Francesco D'Amato, Roberto Saltini, Luca Zanolini, Chenyi Zhang</dc:creator>
    </item>
    <item>
      <title>Improved Massively Parallel Triangle Counting in $O(1)$ Rounds</title>
      <link>https://arxiv.org/abs/2405.00262</link>
      <description>arXiv:2405.00262v1 Announce Type: cross 
Abstract: In this short note, we give a novel algorithm for $O(1)$ round triangle counting in bounded arboricity graphs. Counting triangles in $O(1)$ rounds (exactly) is listed as one of the interesting remaining open problems in the recent survey of Im et al. [IKLMV23]. The previous paper of Biswas et al. [BELMR20], which achieved the best bounds under this setting, used $O(\log \log n)$ rounds in sublinear space per machine and $O(m\alpha)$ total space where $\alpha$ is the arboricity of the graph and $n$ and $m$ are the number of vertices and edges in the graph, respectively. Our new algorithm is very simple, achieves the optimal $O(1)$ rounds without increasing the space per machine and the total space, and has the potential of being easily implementable in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00262v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quanquan C. Liu, C. Seshadhri</dc:creator>
    </item>
    <item>
      <title>On the Potential of RIS in the Context of PLA in Wireless Communication Systems</title>
      <link>https://arxiv.org/abs/2405.00426</link>
      <description>arXiv:2405.00426v1 Announce Type: cross 
Abstract: Re-configurable Intelligent Surfaces (RIS) technology has proven itself a promising candidate for the next generation of wireless networks through its enhanced performance in terms of throughput, spectral, and energy efficiency. However, the broadcast nature of RIS-assisted wireless communication makes it vulnerable to malicious attacks at the physical layer. On the other hand, physical layer authentication is an emerging area in the security domain to thwart different attacks such as cloning, spoofing, and impersonation by using the random features of the physical layer. In this paper, we investigate RIS-assisted wireless communication systems to unlock the potential of using RIS for physical layer authentication (PLA). Specifically, we exploit two distinct features of the physical layer: pathloss and channel impulse response (CIR) for PLA in RIS-assisted wireless communication. We construct hypothesis tests for the estimated features and derive the closed-form errors' expressions. Further, we chose the critical error, i.e., missed detection as our objective function for minimization by optimizing the phase shift of the RIS pannel. We compare the performance of our proposed mechanisms with baseline mechanisms which are PLA schemes using the same features but with no RIS assistance. Furthermore, we thoroughly evaluate our proposed schemes using performance metrics such as the probability of false alarm (PFA), the probability of missed detection (PMD), and the receiver operating characteristic (ROC) curves. The results demonstrate the significant positive impact of RIS on PLA, as it effectively reduces PMD values to zero when determining the optimal phase shift.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00426v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hama Amin, Waqas Aman, Saif Al-Kuwari</dc:creator>
    </item>
    <item>
      <title>Jolteon and Ditto: Network-Adaptive Efficient Consensus with Asynchronous Fallback</title>
      <link>https://arxiv.org/abs/2106.10362</link>
      <description>arXiv:2106.10362v3 Announce Type: replace 
Abstract: Existing committee-based Byzantine state machine replication (SMR) protocols, typically deployed in production blockchains, face a clear trade-off: (1) they either achieve linear communication cost in the happy path, but sacrifice liveness during periods of asynchrony, or (2) they are robust (progress with probability one) but pay quadratic communication cost. We believe this trade-off is unwarranted since existing linear protocols still have asymptotic quadratic cost in the worst case. We design Ditto, a Byzantine SMR protocol that enjoys the best of both worlds: optimal communication on and off the happy path (linear and quadratic, respectively) and progress guarantee under asynchrony and DDoS attacks. We achieve this by replacing the view-synchronization of partially synchronous protocols with an asynchronous fallback mechanism at no extra asymptotic cost. Specifically, we start from HotStuff, a state-of-the-art linear protocol, and gradually build Ditto. As a separate contribution and an intermediate step, we design a 2-chain version of HotStuff, Jolteon, which leverages a quadratic view-change mechanism to reduce the latency of the standard 3-chain HotStuff. We implement and experimentally evaluate all our systems. Notably, Jolteon's commit latency outperforms HotStuff by 200-300ms with varying system size. Additionally, Ditto adapts to the network and provides better performance than Jolteon under faulty conditions and better performance than VABA (a state-of-the-art asynchronous protocol) under faultless conditions. This proves our case that breaking the robustness-efficiency trade-off is in the realm of practicality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.10362v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rati Gelashvili, Lefteris Kokoris-Kogias, Alberto Sonnino, Alexander Spiegelman, Zhuolun Xiang</dc:creator>
    </item>
    <item>
      <title>FaaSKeeper: Learning from Building Serverless Services with ZooKeeper as an Example</title>
      <link>https://arxiv.org/abs/2203.14859</link>
      <description>arXiv:2203.14859v3 Announce Type: replace 
Abstract: FaaS (Function-as-a-Service) revolutionized cloud computing by replacing persistent virtual machines with dynamically allocated resources. This shift trades locality and statefulness for a pay-as-you-go model more suited to variable and infrequent workloads. However, the main challenge is to adapt services to the serverless paradigm while meeting functional, performance, and consistency requirements. In this work, we push the boundaries of FaaS computing by designing a serverless variant of ZooKeeper, a centralized coordination service with a safe and wait-free consensus mechanism. We define synchronization primitives to extend the capabilities of scalable cloud storage and outline a set of requirements for efficient computing with serverless. In FaaSKeeper, the first coordination service built on serverless functions and cloud-native services, we explore the limitations of serverless offerings and propose improvements essential for complex and latency-sensitive applications. We share serverless design lessons based on our experiences of implementing a ZooKeeper model deployable to clouds today. FaaSKeeper maintains the same consistency guarantees and interface as ZooKeeper, with a serverless price model that lowers costs up to 110-719x on infrequent workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.14859v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3625549.3658661</arxiv:DOI>
      <dc:creator>Marcin Copik, Alexandru Calotoiu, Pengyu Zhou, Konstantin Taranov, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Grassroots Social Networking: Where People have Agency over their Personal Information and Social Graph</title>
      <link>https://arxiv.org/abs/2306.13941</link>
      <description>arXiv:2306.13941v5 Announce Type: replace 
Abstract: Offering an architecture for social networking in which people have agency over their personal information and social graph is an open challenge. Here we present a grassroots architecture for serverless, permissionless, peer-to-peer social networks termed Grassroots Social Networking that aims to address this challenge. The architecture is geared for people with networked smartphones -- roaming (address-changing) computing devices communicating over an unreliable network (e.g., using UDP). The architecture incorporates (i) a decentralized social graph, where each person controls, maintains and stores only their local neighborhood in the graph; (iii) personal feeds, with authors and followers who create and store the feeds; and (ii) a grassroots dissemination protocol, in which communication among people occurs only along the edges of their social graph. The architecture realizes these components using the blocklace data structure -- a partially-ordered conflict-free counterpart of the totally-ordered conflict-based blockchain. We provide two example Grassroots Social Networking protocols -- Twitter-like and WhatsApp-like -- and address their security (safety, liveness and privacy), spam/bot/deep-fake resistance, and implementation, demonstrating how server-based social networks could be supplanted by a grassroots architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13941v5</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3599696.3612898</arxiv:DOI>
      <dc:creator>Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>Sui Lutris: A Blockchain Combining Broadcast and Consensus</title>
      <link>https://arxiv.org/abs/2310.18042</link>
      <description>arXiv:2310.18042v2 Announce Type: replace 
Abstract: Sui Lutris is the first smart-contract platform to sustainably achieve sub-second finality. It achieves this significant decrease by employing consensusless agreement not only for simple payments but for a large variety of transactions. Unlike prior work, Sui Lutris neither compromises expressiveness nor throughput and can run perpetually without restarts. Sui Lutris achieves this by safely integrating consensuless agreement with a high-throughput consensus protocol that is invoked out of the critical finality path but ensures that when a transaction is at risk of inconsistent concurrent accesses, its settlement is delayed until the total ordering is resolved. Building such a hybrid architecture is especially delicate during reconfiguration events, where the system needs to preserve the safety of the consensusless path without compromising the long-term liveness of potentially misconfigured clients. We thus develop a novel reconfiguration protocol, the first to provably show the safe and efficient reconfiguration of a consensusless blockchain. Sui Lutris is currently running in production and underpins the Sui smart-contract platform. Combined with the use of Objects instead of accounts it enables the safe execution of smart contracts that expose objects as a first-class resource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds for throughput up to 5,000 certificates per second (150k ops/s with transaction blocks), compared to the state-of-the-art real-world consensus latencies of 3 seconds. Furthermore, it gracefully handles validators crash-recovery and does not suffer visible performance degradation during reconfiguration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18042v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Same Blackshear, Andrey Chursin, George Danezis, Anastasios Kichidis, Lefteris Kokoris-Kogias, Xun Li, Mark Logan, Ashok Menon, Todd Nowacki, Alberto Sonnino, Brandon Williams, Lu Zhang</dc:creator>
    </item>
    <item>
      <title>Uniform Partitioning of a Bounded Region using Opaque ASYNC Luminous Mobile Robots</title>
      <link>https://arxiv.org/abs/2311.04536</link>
      <description>arXiv:2311.04536v2 Announce Type: replace 
Abstract: We are given $N$ autonomous mobile robots inside a bounded region. The robots are opaque which means that three collinear robots are unable to see each other as one of the robots acts as an obstruction for the other two. They operate in classical \emph{Look-Compute-Move} (LCM) activation cycles. Moreover, the robots are oblivious except for a persistent light (which is why they are called \emph{Luminous robots}) that can determine a color from a fixed color set. Obliviousness does not allow the robots to remember any information from past activation cycles. The Uniform Partitioning problem requires the robots to partition the whole region into sub-regions of equal area, each of which contains exactly one robot. Due to application-oriented motivation, we, in this paper consider the region to be well-known geometric shapes such as rectangle, square and circle. We investigate the problem in \emph{asynchronous} setting where there is no notion of common time and any robot gets activated at any time with a fair assumption that every robot needs to get activated infinitely often. To the best of our knowledge, this is the first attempt to study the Uniform Partitioning problem using oblivious opaque robots working under asynchronous settings. We propose three algorithms considering three different regions: rectangle, square and circle. The algorithms proposed for rectangular and square regions run in $O(N)$ epochs whereas the algorithm for circular regions runs in $O(N^2)$ epochs, where an epoch is the smallest unit of time in which all robots are activated at least once and execute their LCM cycles. The algorithms for the rectangular, square and circular regions require $2$ (which is optimal), $5$ and $8$ colors, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04536v2</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Pramanick, Saswata Jana, Adri Bhattacharya, Partha Sarathi Mandal</dc:creator>
    </item>
    <item>
      <title>Software Resource Disaggregation for HPC with Serverless Computing</title>
      <link>https://arxiv.org/abs/2401.10852</link>
      <description>arXiv:2401.10852v4 Announce Type: replace 
Abstract: Aggregated HPC resources have rigid allocation systems and programming models which struggle to adapt to diverse and changing workloads. Consequently, HPC systems fail to efficiently use the large pools of unused memory and increase the utilization of idle computing resources. Prior work attempted to increase the throughput and efficiency of supercomputing systems through workload co-location and resource disaggregation. However, these methods fall short of providing a solution that can be applied to existing systems without major hardware modifications and performance losses. In this paper, we improve the utilization of supercomputers by employing the new cloud paradigm of serverless computing. We show how serverless functions provide fine-grained access to the resources of batch-managed cluster nodes. We present an HPC-oriented Function-as-a-Service (FaaS) that satisfies the requirements of high-performance applications. We demonstrate a software resource disaggregation approach where placing functions on unallocated and underutilized nodes allows idle cores and accelerators to be utilized while retaining near-native performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10852v4</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcin Copik, Marcin Chrapek, Larissa Schmid, Alexandru Calotoiu, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>Automatic BLAS Offloading on Unified Memory Architecture: A Study on NVIDIA Grace-Hopper</title>
      <link>https://arxiv.org/abs/2404.13195</link>
      <description>arXiv:2404.13195v3 Announce Type: replace 
Abstract: Porting codes to GPU often requires major efforts. While several tools exist for automatically offload numerical libraries such as BLAS and LAPACK, they often prove impractical due to the high cost of mandatory data transfer. The new unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth cache-coherent memory access of all memory from both CPU and GPU, potentially eliminating bottleneck faced in conventional architecture. This breakthrough opens up new avenues for application development and porting strategies. In this study, we introduce a new tool for automatic BLAS offload, the tool leverages the high speed cache coherent NVLink C2C interconnect in Grace-Hopper, and enables performant GPU offload for BLAS heavy applications with no code changes or recompilation. The tool was tested on two quantum chemistry or physics codes, great performance benefits were observed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13195v3</guid>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junjie Li, Yinzhi Wang, Xiao Liang, Hang Liu</dc:creator>
    </item>
    <item>
      <title>Federated Transfer Component Analysis Towards Effective VNF Profiling</title>
      <link>https://arxiv.org/abs/2404.17553</link>
      <description>arXiv:2404.17553v2 Announce Type: replace 
Abstract: The increasing concerns of knowledge transfer and data privacy challenge the traditional gather-and-analyse paradigm in networks. Specifically, the intelligent orchestration of Virtual Network Functions (VNFs) requires understanding and profiling the resource consumption. However, profiling all kinds of VNFs is time-consuming. It is important to consider transferring the well-profiled VNF knowledge to other lack-profiled VNF types while keeping data private. To this end, this paper proposes a Federated Transfer Component Analysis (FTCA) method between the source and target VNFs. FTCA first trains Generative Adversarial Networks (GANs) based on the source VNF profiling data, and the trained GANs model is sent to the target VNF domain. Then, FTCA realizes federated domain adaptation by using the generated source VNF data and less target VNF profiling data, while keeping the raw data locally. Experiments show that the proposed FTCA can effectively predict the required resources for the target VNF. Specifically, the RMSE index of the regression model decreases by 38.5% and the R-squared metric advances up to 68.6%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17553v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xunzheng Zhang, Shadi Moazzeni, Juan Marcelo Parra-Ullauri, Reza Nejabati, Dimitra Simeonidou</dc:creator>
    </item>
    <item>
      <title>A Tight Lower Bound for 3-Coloring Grids in the Online-LOCAL Model</title>
      <link>https://arxiv.org/abs/2312.01384</link>
      <description>arXiv:2312.01384v4 Announce Type: replace-cross 
Abstract: Recently, \citeauthor*{akbari2021locality}~(ICALP 2023) studied the locality of graph problems in distributed, sequential, dynamic, and online settings from a {unified} point of view. They designed a novel $O(\log n)$-locality deterministic algorithm for proper 3-coloring bipartite graphs in the $\mathsf{Online}$-$\mathsf{LOCAL}$ model. In this work, we establish the optimality of the algorithm by showing a \textit{tight} deterministic $\Omega(\log n)$ locality lower bound, which holds even on grids. To complement this result, we have the following additional results:
  \begin{enumerate}
  \item We show a higher and {tight} $\Omega(\sqrt{n})$ lower bound for 3-coloring toroidal and cylindrical grids.
  \item Considering the generalization of $3$-coloring bipartite graphs to $(k+1)$-coloring $k$-partite graphs, %where $k \geq 2$ is a constant,
  we show that the problem also has $O(\log n)$ locality when the input is a $k$-partite graph that admits a \emph{locally inferable unique coloring}. This special class of $k$-partite graphs covers several fundamental graph classes such as $k$-trees and triangular grids. Moreover, for this special class of graphs, we show a {tight} $\Omega(\log n)$ locality lower bound.
  \item For general $k$-partite graphs with $k \geq 3$, we prove that the problem of $(2k-2)$-coloring $k$-partite graphs exhibits a locality of $\Omega(n)$ in the $\onlineLOCAL$ model, matching the round complexity of the same problem in the $\LOCAL$ model recently shown by \citeauthor*{coiteux2023no}~(STOC 2024). Consequently, the problem of $(k+1)$-coloring $k$-partite graphs admits a locality lower bound of $\Omega(n)$ when $k\geq 3$, contrasting sharply with the $\Theta(\log n)$ locality for the case of $k=2$. \end{enumerate}</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01384v4</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi-Jun Chang, Gopinath Mishra, Hung Thuan Nguyen, Mingyang Yang, Yu-Cheng Yeh</dc:creator>
    </item>
    <item>
      <title>On Replacing Cryptopuzzles with Useful Computation in Blockchain Proof-of-Work Protocols</title>
      <link>https://arxiv.org/abs/2404.15735</link>
      <description>arXiv:2404.15735v3 Announce Type: replace-cross 
Abstract: Proof-of-Work (PoW) blockchains have emerged as a robust and effective consensus mechanism in open environments like the Internet, leading to widespread deployment with numerous cryptocurrency platforms and substantial investments. However, the current PoW implementation primarily focuses on validating the discovery of a winning nonce. Exploring the notion of replacing cryptographic puzzles with useful computing tasks becomes compelling, given the substantial computational capacity of blockchain networks and the global pursuit of a more sustainable IT infrastructure. In this study, we conduct a comprehensive analysis of the prerequisites for alternative classes of tasks, examining proposed designs from existing literature in light of these requirements. We distill pertinent techniques and address gaps in the current state-of-the-art, providing valuable insights into the evolution of consensus mechanisms beyond traditional PoW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15735v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Merlina, Thiago Garrett, Roman Vitenberg</dc:creator>
    </item>
    <item>
      <title>Fast Abstracts and Student Forum Proceedings -- EDCC 2024 -- 19th European Dependable Computing Conference</title>
      <link>https://arxiv.org/abs/2404.17465</link>
      <description>arXiv:2404.17465v2 Announce Type: replace-cross 
Abstract: The goal of the Fast Abstracts track is to bring together researchers and practitioners working on dependable computing to discuss work in progress or opinion pieces. Contributions are welcome from academia and industry. Fast Abstracts aim to serve as a rapid and flexible mechanism to: (i) Report on current work that may or may not be complete; (ii) Introduce new ideas to the community; (iii) State positions on controversial issues or open problems; (iv) Share lessons learnt from real-word dependability engineering; and (v) Debunk or question results from other papers based on contra-indications. The Student Forum aims at creating a vibrant and friendly environment where students can present and discuss their work, and exchange ideas and experiences with other students, researchers and industry. One of the key goals of the Forum is to provide students with feedback on their preliminary results that might help with their future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17465v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simona Bernardi, Tommaso Zoppi</dc:creator>
    </item>
  </channel>
</rss>
