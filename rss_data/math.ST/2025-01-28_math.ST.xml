<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A characterization of uniform distribution using varextropy with application in testing uniformity</title>
      <link>https://arxiv.org/abs/2501.14797</link>
      <description>arXiv:2501.14797v1 Announce Type: new 
Abstract: In statistical analysis, quantifying uncertainties through measures such as entropy, extropy, varentropy, and varextropy is of fundamental importance for understanding distribution functions. This paper investigates several properties of varextropy and give a new characterization of uniform distribution using varextropy. The alredy proposed estimators are used as a test statistics. Building on the characterization of the uniform distribution using varextropy, we give a uniformity test. The critical value and power of the test statistics are derived. The proposed test procedure is applied to a real-world dataset to assess its performance and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14797v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santosh Kumar Chaudhary, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>The typicality principle and its implications for statistics and data science</title>
      <link>https://arxiv.org/abs/2501.14860</link>
      <description>arXiv:2501.14860v1 Announce Type: new 
Abstract: A central focus of data science is the transformation of empirical evidence into knowledge. As such, the key insights and scientific attitudes of deep thinkers like Fisher, Popper, and Tukey are expected to inspire exciting new advances in machine learning and artificial intelligence in years to come. Along these lines, the present paper advances a novel {\em typicality principle} which states, roughly, that if the observed data is sufficiently ``atypical'' in a certain sense relative to a posited theory, then that theory is unwarranted. This emphasis on typicality brings familiar but often overlooked background notions like model-checking to the inferential foreground. One instantiation of the typicality principle is in the context of parameter estimation, where we propose a new typicality-based regularization strategy that leans heavily on goodness-of-fit testing. The effectiveness of this new regularization strategy is illustrated in three non-trivial examples where ordinary maximum likelihood estimation fails miserably. We also demonstrate how the typicality principle fits within a bigger picture of reliable and efficient uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14860v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Zeyu Zhang, Ryan Martin, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Private Minimum Hellinger Distance Estimation via Hellinger Distance Differential Privacy</title>
      <link>https://arxiv.org/abs/2501.14974</link>
      <description>arXiv:2501.14974v1 Announce Type: new 
Abstract: Hellinger distance has been widely used to derive objective functions that are alternatives to maximum likelihood methods. Motivated by recent regulatory privacy requirements, estimators satisfying differential privacy constraints are being derived. In this paper, we describe different notions of privacy using divergences and establish that Hellinger distance minimizes the added variance within the class of power divergences for an additive Gaussian mechanism. We demonstrate that a new definition of privacy, namely Hellinger differential privacy, shares several features of the standard notion of differential privacy while allowing for sharper inference. Using these properties, we develop private versions of gradient descent and Newton-Raphson algorithms for obtaining private minimum Hellinger distance estimators, which are robust and first-order efficient. Using numerical experiments, we illustrate the finite sample performance and verify that they retain their robustness properties under gross-error contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14974v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fengnan Deng, Anand N. Vidyashankar</dc:creator>
    </item>
    <item>
      <title>Eigenvector fluctuations and limit results for random graphs with infinite rank kernels</title>
      <link>https://arxiv.org/abs/2501.15725</link>
      <description>arXiv:2501.15725v1 Announce Type: new 
Abstract: This paper systematically studies the behavior of the leading eigenvectors for independent edge undirected random graphs generated from a general latent position model whose link function is possibly infinite rank and also possibly indefinite. We first derive uniform error bounds in the two-to-infinity norm as well as row-wise normal approximations for the leading sample eigenvectors. We then build on these results to tackle two graph inference problems, namely (i) entrywise bounds for graphon estimation and (ii) testing for the equality of latent positions, the latter of which is achieved by proposing a rank-adaptive test statistic that converges in distribution to a weighted sum of independent chi-square random variables under the null hypothesis. Our fine-grained theoretical guarantees and applications differ from the existing literature which primarily considers first order upper bounds and more restrictive low rank or positive semidefinite model assumptions. Further, our results collectively quantify the statistical properties of eigenvector-based spectral embeddings with growing dimensionality for large graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15725v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minh Tang, Joshua R. Cape</dc:creator>
    </item>
    <item>
      <title>Minimax rates of convergence of a binary classification procedure for time-homogeneous S.D.E paths</title>
      <link>https://arxiv.org/abs/2501.15926</link>
      <description>arXiv:2501.15926v1 Announce Type: new 
Abstract: In the context of binary classification of trajectories generated by time-homogeneous stochastic differential equations, we consider a mixture model of two diffusion processes characterized by a stochastic differential equation whose drift coefficient depends on the class or label, which is modeled as a discrete random variable taking two possible values, and whose diffusion coefficient is independent of the class. We assume that the drift and diffusion coefficients are unknown as well as the law of the discrete random variable that models the class. In this paper, we study the minimax convergence rate of the resulting nonparametric plug-in classifier under different sets of assumptions on the mixture model considered. As the plug-in classifier is based on nonparametric estimators of the coefficients of the mixture model, we also provide a minimax convergence rate of the risk of estimation of the drift coefficients on the real line.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15926v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eddy Michel Ella Mintsa</dc:creator>
    </item>
    <item>
      <title>Minimax rates of convergence for the nonparametric estimation of the diffusion coefficient from time-homogeneous SDE paths</title>
      <link>https://arxiv.org/abs/2501.15933</link>
      <description>arXiv:2501.15933v1 Announce Type: new 
Abstract: Consider a diffusion process X, solution of a time-homogeneous stochastic differential equation. We assume that the diffusion process X is observed at discrete times, at high frequency, which means that the time step tends toward zero. In addition, the drift and diffusion coefficients of the process X are assumed to be unknown. In this paper, we study the minimax rates of convergence of the nonparametric estimators of the square of the diffusion coefficient. Two observation schemes are considered depending on the estimation interval. The square of the diffusion coefficient is estimated on the real line from repeated observations of the process X, where the number of diffusion paths tends to infinity. For the case of a compact estimation interval, we study the nonparametric estimation of the square of the diffusion coefficient constructed from a single diffusion path on one side and from repeated observations on the other side, where the number of trajectories tends to infinity. In each of these cases, we establish minimax convergence rates of the risk of estimation of the diffusion coefficient over a space of Holder functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15933v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eddy Michel Ella Mintsa</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Low-Rank Tensor Models</title>
      <link>https://arxiv.org/abs/2501.16223</link>
      <description>arXiv:2501.16223v1 Announce Type: new 
Abstract: Statistical inference for tensors has emerged as a critical challenge in analyzing high-dimensional data in modern data science. This paper introduces a unified framework for inferring general and low-Tucker-rank linear functionals of low-Tucker-rank signal tensors for several low-rank tensor models. Our methodology tackles two primary goals: achieving asymptotic normality and constructing minimax-optimal confidence intervals. By leveraging a debiasing strategy and projecting onto the tangent space of the low-Tucker-rank manifold, we enable inference for general and structured linear functionals, extending far beyond the scope of traditional entrywise inference. Specifically, in the low-Tucker-rank tensor regression or PCA model, we establish the computational and statistical efficiency of our approach, achieving near-optimal sample size requirements (in regression model) and signal-to-noise ratio (SNR) conditions (in PCA model) for general linear functionals without requiring sparsity in the loading tensor. Our framework also attains both computationally and statistically optimal sample size and SNR thresholds for low-Tucker-rank linear functionals. Numerical experiments validate our theoretical results, showcasing the framework's utility in diverse applications. This work addresses significant methodological gaps in statistical inference, advancing tensor analysis for complex and high-dimensional data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16223v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ke Xu, Elynn Chen, Yuefeng Han</dc:creator>
    </item>
    <item>
      <title>Assessing Skew Normality in Marks Distribution, a Comparative Analysis of Shapiro Wilk Tests</title>
      <link>https://arxiv.org/abs/2501.14845</link>
      <description>arXiv:2501.14845v1 Announce Type: cross 
Abstract: This paper investigates the distribution of marks obtained by students across multiple courses to explore whether the data conforms to a skew-normal distribution. Traditional methods for assessing normality, such as the Shapiro Wilk test, often reject normality in datasets with evident skewness. To address this, we apply a modified Shapiro Wilk test tailored for skew-normal distributions, as described in the literature, to evaluate the suitability of skew-normal models for these datasets. The analysis includes both classical and modified tests, complemented by visualizations such as histograms and Q-Q plots of transformed data. Our findings highlight the relevance of using specialized statistical methods for skew normality, offering valuable insights into the characteristics of academic performance data. This study provides a framework for robust statistical analysis in educational research, emphasizing the need to account for distributional properties when analyzing student performance metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14845v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Himadri Mukherjee, Pratham Bhonge</dc:creator>
    </item>
    <item>
      <title>Decision Making in Changing Environments: Robustness, Query-Based Learning, and Differential Privacy</title>
      <link>https://arxiv.org/abs/2501.14928</link>
      <description>arXiv:2501.14928v1 Announce Type: cross 
Abstract: We study the problem of interactive decision making in which the underlying environment changes over time subject to given constraints. We propose a framework, which we call \textit{hybrid Decision Making with Structured Observations} (hybrid DMSO), that provides an interpolation between the stochastic and adversarial settings of decision making. Within this framework, we can analyze local differentially private (LDP) decision making, query-based learning (in particular, SQ learning), and robust and smooth decision making under the same umbrella, deriving upper and lower bounds based on variants of the Decision-Estimation Coefficient (DEC). We further establish strong connections between the DEC's behavior, the SQ dimension, local minimax complexity, learnability, and joint differential privacy. To showcase the framework's power, we provide new results for contextual bandits under the LDP constraint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14928v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fan Chen, Alexander Rakhlin</dc:creator>
    </item>
    <item>
      <title>Random Processes with Stationary Increments and Intrinsic Random Functions on the Real Line</title>
      <link>https://arxiv.org/abs/2501.15680</link>
      <description>arXiv:2501.15680v1 Announce Type: cross 
Abstract: Random processes with stationary increments and intrinsic random processes are two concepts commonly used to deal with non-stationary random processes. They are broader classes than stationary random processes and conceptually closely related to each other. This paper illustrates the relationship between these two concepts of stochastic processes and shows that, under certain conditions, they are equivalent on the real line.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15680v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jongwook Kim, Chunfeng Huang</dc:creator>
    </item>
    <item>
      <title>A Unified Representation of Density-Power-Based Divergences Reducible to M-Estimation</title>
      <link>https://arxiv.org/abs/2501.16287</link>
      <description>arXiv:2501.16287v1 Announce Type: cross 
Abstract: Density-power-based divergences are known to provide robust inference procedures against outliers, and their extensions have been widely studied. A characteristic of successful divergences is that the estimation problem can be reduced to M-estimation. In this paper, we define a norm-based Bregman density power divergence (NB-DPD) -- density-power-based divergence with functional flexibility within the framework of Bregman divergences that can be reduced to M-estimation. We show that, by specifying the function $\phi_\gamma$, NB-DPD reduces to well-known divergences, such as the density power divergence and the $\gamma$-divergence. Furthermore, by examining the combinations of functions $\phi_\gamma$ corresponding to existing divergences, we show that a new divergence connecting these existing divergences can be derived. Finally, we show that the redescending property, one of the key indicators of robustness, holds only for the $\gamma$-divergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16287v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kobayashi</dc:creator>
    </item>
    <item>
      <title>A varifold-type estimation for data sampled on a rectifiable set</title>
      <link>https://arxiv.org/abs/2501.16315</link>
      <description>arXiv:2501.16315v1 Announce Type: cross 
Abstract: We investigate the inference of varifold structures in a statistical framework: assuming that we have access to i.i.d. samples in $\mathbb{R}^n$ obtained from an underlying $d$--dimensional shape $S$ endowed with a possibly non uniform density $\theta$, we propose and analyse an estimator of the varifold structure associated to $S$. The shape $S$ is assumed to be piecewise $C^{1,a}$ in a sense that allows for a singular set whose small enlargements are of small $d$--dimensional measure. The estimators are kernel--based both for infering the density and the tangent spaces and the convergence result holds for the bounded Lipschitz distance between varifolds, in expectation and in a noiseless model. The mean convergence rate involves the dimension $d$ of $S$, its regularity through $a \in (0, 1]$ and the regularity of the density $\theta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16315v1</guid>
      <category>math.CA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Blanche Buet, Charly Boricaud</dc:creator>
    </item>
    <item>
      <title>Plug-in error bounds for a mixing density estimate in $R^d,$ and for its derivatives</title>
      <link>https://arxiv.org/abs/1510.06940</link>
      <description>arXiv:1510.06940v2 Announce Type: replace 
Abstract: A mixture density, $f_p,$ is estimable in $R^d, \ d \ge 1,$ but an estimate for the mixing density, $p,$ is usually obtained only when $d$ is unity; $h$ is the mixture's kernel. When $f_p$'s estimate has form $f_{\hat p_n}$ and $p$ is $\tilde q$-smooth, vanishing outside a compact in $R^d,$ plug-in upper bounds are obtained herein for the $L_u$-error (and risk)of $\hat p_n$ and its derivatives; $d \ge 1, 1 \le u \le \infty.$ The bounds depend on $f_{\hat p_n}$'s $L_u$-error (or risk), $h$'s Fourier transform, $\tilde h,$ and the bandwidth of kernel $K$ used in approximations. The choice of $\hat p_n,$ via $f_{\hat p_n},$ suggests that $\hat p_n$'s error rate could be only nearly optimal when $f_{\hat p_n}$ is optimal, but competing estimates and their error rates may not be available for $d&gt;1.$ In examples with $d$ unity, the upper bound is optimal when $h$ is super smooth, misses the optimal rate by the factor $(\log n)^{\xi}, \ \xi&gt;0,$ when $h$ is smooth, and is satisfactory when $\tilde h$ has periodic zeros.</description>
      <guid isPermaLink="false">oai:arXiv.org:1510.06940v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yannis G. Yatracos</dc:creator>
    </item>
    <item>
      <title>Existence and uniqueness of weighted generalized $\psi$-estimators</title>
      <link>https://arxiv.org/abs/2211.06026</link>
      <description>arXiv:2211.06026v2 Announce Type: replace 
Abstract: We introduce the notions of generalized and weighted generalized $\psi$-estimators as unique points of sign change of some appropriate functions, and we give necessary as well as sufficient conditions for their existence. We also derive a set of sufficient conditions under which the so-called $\psi$-expectation function has a unique point of sign change. We present several examples from statistical estimation theory, where our results are well-applicable. For example, we consider the cases of empirical quantiles, empirical expectiles, some $\psi$-estimators that are important in robust statistics, and some examples from maximum likelihood theory as well. Further, we introduce Bajraktarevi\'c-type (in particular, quasi-arithmetic-type) $\psi$-estimators. Our results specialized to $\psi$-estimators with a function $\psi$ being continuous in its second variable provide new results for (usual) $\psi$-estimators (also called Z-estimators).</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06026v2</guid>
      <category>math.ST</category>
      <category>math.CA</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matyas Barczy, Zsolt P\'ales</dc:creator>
    </item>
    <item>
      <title>From sparse to dense functional data in high dimensions: Revisiting phase transitions from a non-asymptotic perspective</title>
      <link>https://arxiv.org/abs/2306.00476</link>
      <description>arXiv:2306.00476v4 Announce Type: replace 
Abstract: Nonparametric estimation of the mean and covariance functions is ubiquitous in functional data analysis and local linear smoothing techniques are most frequently used. Zhang and Wang (2016) explored different types of asymptotic properties of the estimation, which reveal interesting phase transition phenomena based on the relative order of the average sampling frequency per subject $T$ to the number of subjects $n$, partitioning the data into three categories: "sparse", "semi-dense", and "ultra-dense". In an increasingly available high-dimensional scenario, where the number of functional variables $p$ is large in relation to $n$, we revisit this open problem from a non-asymptotic perspective by deriving comprehensive concentration inequalities for the local linear smoothers. Besides being of interest by themselves, our non-asymptotic results lead to elementwise maximum rates of $L_2$ convergence and uniform convergence serving as a fundamentally important tool for further convergence analysis when $p$ grows exponentially with $n$ and possibly $T$. With the presence of extra $\log p$ terms to account for the high-dimensional effect, we then investigate the scaled phase transitions and the corresponding elementwise maximum rates from sparse to semi-dense to ultra-dense functional data in high dimensions. We also discuss a couple of applications of our theoretical results. Finally, numerical studies are carried out to confirm the established theoretical properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00476v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaojun Guo, Dong Li, Xinghao Qiao, Yizhu Wang</dc:creator>
    </item>
    <item>
      <title>Improved learning theory for kernel distribution regression with two-stage sampling</title>
      <link>https://arxiv.org/abs/2308.14335</link>
      <description>arXiv:2308.14335v2 Announce Type: replace 
Abstract: The distribution regression problem encompasses many important statistics and machine learning tasks, and arises in a large range of applications. Among various existing approaches to tackle this problem, kernel methods have become a method of choice. Indeed, kernel distribution regression is both computationally favorable, and supported by a recent learning theory. This theory also tackles the two-stage sampling setting, where only samples from the input distributions are available. In this paper, we improve the learning theory of kernel distribution regression. We address kernels based on Hilbertian embeddings, that encompass most, if not all, of the existing approaches. We introduce the novel near-unbiased condition on the Hilbertian embeddings, that enables us to provide new error bounds on the effect of the two-stage sampling, thanks to a new analysis. We show that this near-unbiased condition holds for three important classes of kernels, based on optimal transport and mean embedding. As a consequence, we strictly improve the existing convergence rates for these kernels. Our setting and results are illustrated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14335v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Bachoc, Louis B\'ethune, Alberto Gonz\'alez-Sanz, Jean-Michel Loubes</dc:creator>
    </item>
    <item>
      <title>An analysis of the noise schedule for score-based generative models</title>
      <link>https://arxiv.org/abs/2402.04650</link>
      <description>arXiv:2402.04650v4 Announce Type: replace 
Abstract: Score-based generative models (SGMs) aim at estimating a target data distribution by learning score functions using only noise-perturbed samples from the target.Recent literature has focused extensively on assessing the error between the target and estimated distributions, gauging the generative quality through the Kullback-Leibler (KL) divergence and Wasserstein distances.  Under mild assumptions on the data distribution, we establish an upper bound for the KL divergence between the target and the estimated distributions, explicitly depending on any time-dependent noise schedule. Under additional regularity assumptions, taking advantage of favorable underlying contraction mechanisms, we provide a tighter error bound in Wasserstein distance compared to state-of-the-art results. In addition to being tractable, this upper bound jointly incorporates properties of the target distribution and SGM hyperparameters that need to be tuned during training. Finally, we illustrate these bounds through numerical experiments using simulated and CIFAR-10 datasets, identifying an optimal range of noise schedules within a parametric family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04650v4</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislas Strasman (SU, LPSM), Antonio Ocello (CMAP), Claire Boyer (LPSM), Sylvain Le Corff (LPSM), Vincent Lemaire (LPSM)</dc:creator>
    </item>
    <item>
      <title>Multifunction Estimation in a Time-Discretized Skorokhod Reflection Problem</title>
      <link>https://arxiv.org/abs/2407.05011</link>
      <description>arXiv:2407.05011v3 Announce Type: replace 
Abstract: This paper deals with a consistent estimator of the multifunction involved in a time-discretized Skorokhod reflection problem defined by a stochastic differential equation and a Moreau sweeping process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05011v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Marie</dc:creator>
    </item>
    <item>
      <title>Marchenko-Pastur law for Daniell smoothed periodograms without simultaneous diagonalizability</title>
      <link>https://arxiv.org/abs/2408.14618</link>
      <description>arXiv:2408.14618v2 Announce Type: replace 
Abstract: The eigenvectors of a spectral density matrix $F(\theta)$ to a stationary Gaussian process $(X_t)_{t \in \mathbb{Z}}$ depend explicitly on the frequency $\theta \in [0,2\pi]$. The most commonly used estimator of the spectral density matrix $F(\theta)$ is the smoothed periodogram, which takes the form $ZZ^*$ for random matrices $Z$ with non-zero covariance between rows and columns. When the covariance matrices of the columns are not simultaneously diagonalizable, this covariance structure is non-separable and such matrices $ZZ^*$ are out of reach for the current state of random matrix theory. In this paper, we derive a Marchenko-Pastur law in this non-simultaneously diagonalizable case. The Marchenko-Pastur law emerges when the dimension $d$ of the process and the smoothing span $m$ of the smoothed periodogram grow at the same rate, which is slower than the number of observations $n$.
  On the technical level we prove a trace moment bound for matrices $YY^T$, where $Y$ is a matrix with correlated Gaussian entries. This allows for sub-polynomial error bounds in settings where the error $Y$ has correlations between different points in time as well as between features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14618v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ben Deitmar</dc:creator>
    </item>
    <item>
      <title>High-order Accurate Inference on Manifolds</title>
      <link>https://arxiv.org/abs/2501.06652</link>
      <description>arXiv:2501.06652v2 Announce Type: replace 
Abstract: We present a new framework for statistical inference on Riemannian manifolds that achieves high-order accuracy, addressing the challenges posed by non-Euclidean parameter spaces frequently encountered in modern data science. Our approach leverages a novel and computationally efficient procedure to reach higher-order asymptotic precision. In particular, we develop a bootstrap algorithm on Riemannian manifolds that is both computationally efficient and accurate for hypothesis testing and confidence region construction. Although locational hypothesis testing can be reformulated as a standard Euclidean problem, constructing high-order accurate confidence regions necessitates careful treatment of manifold geometry. To this end, we establish high-order asymptotics under a fixed normal chart centered at the true parameter, thereby enabling precise expansions that incorporate curvature effects. We demonstrate the versatility of this framework across various manifold settings-including spheres, the Stiefel manifold, fixed-rank matrices manifolds, and rank-one tensor manifolds-and, for Euclidean submanifolds, introduce a class of projection-like coordinate charts with strong consistency properties. Finally, numerical studies confirm the practical merits of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06652v2</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Chengzhu Huang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Approximate D-optimal design and equilibrium measure</title>
      <link>https://arxiv.org/abs/2409.04058</link>
      <description>arXiv:2409.04058v2 Announce Type: replace-cross 
Abstract: We introduce a minor variant of  the approximate D-optimal design of experiments with a more general information matrixthat takes into account the representation of the design space S. The main motivation (and result) is that if S in R^d is the unit ball, the unit box or the canonical simplex, then remarkably, for every dimension d and every degree n, one obtains an optimal solution in closed form, namely the equilibrium measure of S (in pluripotential theory). Equivalently, for each degree n, the unique optimal solution is the vector of moments (up to degree 2n) of the equilibrium measure of S. Hence finding an optimal design reduces to finding a cubature for the equilibrium measure, with atoms in S, positive weights, and exact up to degree 2n. In addition, any resulting sequence of atomic D-optimal measures converges to the equilibrium measure of S for the weak-star topology, as n increases. Links with Fekete sets of points are also discussed. More general compact basic semi-algebraic sets are also considered, and a previously developed two-step design algorithm is easily adapted to this new variant of D-optimal design problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04058v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Didier Henrion (LAAS-POP), Jean Bernard Lasserre (LAAS-POP, TSE-R)</dc:creator>
    </item>
    <item>
      <title>A Note on Doubly Robust Estimator in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2411.07978</link>
      <description>arXiv:2411.07978v4 Announce Type: replace-cross 
Abstract: This note introduces a doubly robust (DR) estimator for regression discontinuity (RD) designs. RD designs provide a quasi-experimental framework for estimating treatment effects, where treatment assignment depends on whether a running variable surpasses a predefined cutoff. A common approach in RD estimation is the use of nonparametric regression methods, such as local linear regression. However, the validity of these methods still relies on the consistency of the nonparametric estimators. In this study, we propose the DR-RD estimator, which combines two distinct estimators for the conditional expected outcomes. The primary advantage of the DR-RD estimator lies in its ability to ensure the consistency of the treatment effect estimation as long as at least one of the two estimators is consistent. Consequently, our DR-RD estimator enhances robustness of treatment effect estimators in RD designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07978v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Polynomial time sampling from log-smooth distributions in fixed dimension under semi-log-concavity of the forward diffusion with application to strongly dissipative distributions</title>
      <link>https://arxiv.org/abs/2501.00565</link>
      <description>arXiv:2501.00565v2 Announce Type: replace-cross 
Abstract: In this article, we provide a stochastic sampling algorithm with polynomial complexity in fixed dimension that leverages the recent advances on diffusion models where it is shown that under mild conditions, sampling can be achieved via an accurate estimation of intermediate scores across the marginals $(p_t)_{t\ge 0}$ of the standard Ornstein-Uhlenbeck process started at $\mu$, the density we wish to sample from. The heart of our method consists into approaching these scores via a computationally cheap estimator and relating the variance of this estimator to the smoothness properties of the forward process. Under the assumption that the density to sample from is $L$-log-smooth and that the forward process is semi-log-concave: $-\nabla^2 \log(p_t) \succeq -\beta I_d$ for some $\beta \geq 0$, we prove that our algorithm achieves an expected $\epsilon$ error in $\text{KL}$ divergence in $O(d^7(L+\beta)^2L^{d+2}\epsilon^{-2(d+3)}(d+m_2(\mu))^{2(d+1)})$ time with $m_2(\mu)$ the second order moment of $\mu$. In particular, our result allows to fully transfer the problem of sampling from a log-smooth distribution into a regularity estimate problem. As an application, we derive an exponential complexity improvement for the problem of sampling from an $L$-log-smooth distribution that is $\alpha$-strongly log-concave outside some ball of radius $R$: after proving that such distributions verify the semi-log-concavity assumption, a result which might be of independent interest, we recover a $poly(R, L, \alpha^{-1}, \epsilon^{-1})$ complexity in fixed dimension which exponentially improves upon the previously known $poly(e^{LR^2}, L,\alpha^{-1}, \log(\epsilon^{-1}))$ complexity in the low precision regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00565v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Adrien Vacher, Omar Chehab, Anna Korba</dc:creator>
    </item>
  </channel>
</rss>
