<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 14 Aug 2025 01:29:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Millisecond-scale Volatile Memory in HZO Ferroelectric Capacitors for Bio-inspired Temporal Computing</title>
      <link>https://arxiv.org/abs/2508.08973</link>
      <description>arXiv:2508.08973v1 Announce Type: new 
Abstract: With the broad recent research on ferroelectric hafnium oxide for non-volatile memory technology, depolarization effects in HfO2-based ferroelectric devices gained a lot of interest. Understanding the physical mechanisms regulating the retention of these devices provides an excellent opportunity for device optimization both towards non-volatile memory applications and towards real-time signal processing applications in which controlled time constants are of paramount importance. Indeed, we argue that ferroelectric devices, particularly HfO2-based, are an elegant solution to realize possibly arbitrary time constants in a single scaled memory device, which paves the way for temporal and brain-inspired computing in hardware. Here we present a ferroelectric capacitor stack realizing volatile memory due to its unique interface configuration. We provide electrical characterization of the device to motivate its use for realizing time constants in hardware, followed by an investigation of the electronic mechanisms and their possible relation to the observed retention times to facilitate further modeling of the retention process in HfO2-based ferroelectric capacitors. In the presented device, internal electric fields stabilize one polarization of the ferroelectric film, opening the possibility for unipolar operation with millisecond retention for the unstable polarization state. We show a dependence of the retention on both the polarization as well as the electrical stimuli, allowing us to exploit a range of time scales in a single device. Further, the intentionally defective interface in the presented material stack allows an insight into the interplay between retention loss in HfO2-based ferroelectric devices and the internal bias field, which we relate to the interface composition and the role of oxygen vacancies as a possible source of the internal bias fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08973v1</guid>
      <category>cs.ET</category>
      <category>cond-mat.mtrl-sci</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Fehlings, Thomas Mikolajick, Beatriz Noheda, Erika Covi</dc:creator>
    </item>
    <item>
      <title>EU Digital Regulation and Guatemala: AI, 5G, and Cybersecurity</title>
      <link>https://arxiv.org/abs/2508.08315</link>
      <description>arXiv:2508.08315v1 Announce Type: cross 
Abstract: The paper examines how EU rules in AI, 5G, and cybersecurity operate as transnational governance and shape policy in Guatemala. It outlines the AI Act's risk approach, the 5G Action Plan and Security Toolbox, and the cybersecurity regime built on ENISA, NIS2, the Cybersecurity Act, and the Cyber Resilience Act. It traces extraterritorial channels such as the Brussels effect, private standards, supply chain clauses, and data transfer controls. Guatemala specific impacts include SME compliance costs, procurement limits, environmental trade-offs in rollout, rights risks, and capacity gaps. The paper maps current national measures and proposes five guardrails: digital constitutionalism, green IT duties, third country impact assessment, standards co-design, and recognition of regulatory diversity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08315v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Lopez Juarez</dc:creator>
    </item>
    <item>
      <title>Architecting Long-Context LLM Acceleration with Packing-Prefetch Scheduler and Ultra-Large Capacity On-Chip Memories</title>
      <link>https://arxiv.org/abs/2508.08457</link>
      <description>arXiv:2508.08457v1 Announce Type: cross 
Abstract: Long-context Large Language Model (LLM) inference faces increasing compute bottlenecks as attention calculations scale with context length, primarily due to the growing KV-cache transfer overhead that saturates High Bandwidth Memory (HBM). While prefetching techniques mitigate cache misses by fetching KV data in advance, their spatial and temporal benefits present new opportunities to exploit. This work proposes a packing-prefetch scheduling architecture with monolithic 3D (M3D) back-end-of-line (BEOL) compatible embedded memories with ultra-large on-chip capacity to accelerate long-context LLM inference. Our optimizations demonstrate 8.06x decode speedup and 1.83x overall latency reduction on Llama3.1-8B using TPUv6e-like hardware with additional 512MB BEOL memories over the serial execution. Evaluations of multi-request workloads on TPU-like architectures show 1.7x-2.4x throughput improvement and 1.5x-2.4x HBM bandwidth reduction compared to packing-only methods on Llama3.1-8B and Llama3.1-70B models. With the co-design of packing, prefetching, and BEOL memories, our approach alleviates HBM constraints and enables efficient long-context LLM inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08457v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming-Yen Lee, Faaiq Waqar, Hanchen Yang, Muhammed Ahosan Ul Karim, Harsono Simka, Shimeng Yu</dc:creator>
    </item>
    <item>
      <title>OISMA: On-the-fly In-memory Stochastic Multiplication Architecture for Matrix-Multiplication Workloads</title>
      <link>https://arxiv.org/abs/2508.08822</link>
      <description>arXiv:2508.08822v1 Announce Type: cross 
Abstract: Artificial Intelligence models are currently driven by a significant up-scaling of their complexity, with massive matrix multiplication workloads representing the major computational bottleneck. In-memory computing architectures are proposed to avoid the Von Neumann bottleneck. However, both digital/binary-based and analogue in-memory computing architectures suffer from various limitations, which significantly degrade the performance and energy efficiency gains. This work proposes OISMA, a novel in-memory computing architecture that utilizes the computational simplicity of a quasi-stochastic computing domain (Bent-Pyramid system), while keeping the same efficiency, scalability, and productivity of digital memories. OISMA converts normal memory read operations into in-situ stochastic multiplication operations with a negligible cost. An accumulation periphery then accumulates the output multiplication bitstreams, achieving the matrix multiplication functionality. Extensive matrix multiplication benchmarking was conducted to analyze the accuracy of the Bent-Pyramid system, using matrix dimensions ranging from 4x4 to 512x512. The accuracy results show a significant decrease in the average relative Frobenius error, from 9.42% (for 4x4) to 1.81% (for 512x512), compared to 64-bit double precision floating-point format. A 1T1R OISMA array of 4 KB capacity was implemented using a commercial 180nm technology node and in-house RRAM technology. At 50 MHz, OISMA achieves 0.891 TOPS/W and 3.98 GOPS/mm2 for energy and area efficiency, respectively, occupying an effective computing area of 0.804241 mm2. Scaling OISMA from 180nm to 22nm technology shows a significant improvement of two orders of magnitude in energy efficiency and one order of magnitude in area efficiency, compared to dense matrix multiplication in-memory computing architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08822v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shady Agwa, Yihan Pan, Georgios Papandroulidakis, Themis Prodromakis</dc:creator>
    </item>
    <item>
      <title>LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning</title>
      <link>https://arxiv.org/abs/2508.06799</link>
      <description>arXiv:2508.06799v2 Announce Type: replace 
Abstract: Digital Twins (DTs) offer powerful tools for managing complex infrastructure systems, but their effectiveness is often limited by challenges in integrating unstructured knowledge. Recent advances in Large Language Models (LLMs) bring new potential to address this gap, with strong abilities in extracting and organizing diverse textual information. We therefore propose LSDTs (LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract planning knowledge from unstructured documents like environmental regulations and technical guidelines, and organize it into a formal ontology. This ontology forms a semantic layer that powers a digital twin-a virtual model of the physical system-allowing it to simulate realistic, regulation-aware planning scenarios. We evaluate LSDTs through a case study of offshore wind farm planning in Maryland, including its application during Hurricane Sandy. Results demonstrate that LSDTs support interpretable, regulation-aware layout optimization, enable high-fidelity simulation, and enhance adaptability in infrastructure planning. This work shows the potential of combining generative AI with digital twins to support complex, knowledge-driven planning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.06799v2</guid>
      <category>cs.ET</category>
      <category>cs.AI</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naiyi Li, Zihui Ma, Runlong Yu, Lingyao Li</dc:creator>
    </item>
    <item>
      <title>ObfusQate: Unveiling the First Quantum Program Obfuscation Framework</title>
      <link>https://arxiv.org/abs/2503.23785</link>
      <description>arXiv:2503.23785v2 Announce Type: replace-cross 
Abstract: This paper introduces ObfusQate, a novel tool that conducts obfuscations using quantum primitives to enhance the security of both classical and quantum programs. We have designed and implemented two primary categories of obfuscations: quantum circuit level obfuscation and code level obfuscation, encompassing a total of eight distinct methods. Quantum circuit-level obfuscation leverages on quantum gates and circuits, utilizing strategies such as quantum gate hiding and identity matrices to construct complex, non-intuitive circuits that effectively obscure core functionalities and resist reverse engineering, making the underlying code difficult to interpret. Meanwhile, code-level obfuscation manipulates the logical sequence of program operations through quantum-based opaque predicates, obfuscating execution paths and rendering program behavior more unpredictable and challenging to analyze. Additionally, ObfusQate can be used to obfuscate malicious code segments, making them harder to detect and analyze. These advancements establish a foundational framework for further exploration into the potential and limitations of quantum-based obfuscation techniques, positioning ObfusQate as a valuable tool for future developers to enhance code security in the evolving landscape of software development. To the best of our knowledge, ObfusQate represents the pioneering work in developing an automated framework for implementing obfuscations leveraging quantum primitives. Security evaluations show that obfuscations by ObfusQate maintain code behavior with polynomial overheads in space and time complexities. We have also demonstrated an offensive use case by embedding a keylogger into Shor's algorithm and obfuscating it using ObfusQate. Our results show that current Large language models like GPT 4o, GPT o3 mini and Grok 3 were not able to identify the malicious keylogger after obfuscation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23785v2</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <pubDate>Wed, 13 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nilhil Bartake, See Toh Zi Jie, Carmen Wong Jiawen, Michael Kasper, Vivek Balachandran</dc:creator>
    </item>
  </channel>
</rss>
