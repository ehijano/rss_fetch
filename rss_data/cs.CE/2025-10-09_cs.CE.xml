<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 09 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Attention-Enhanced Reinforcement Learning for Dynamic Portfolio Optimization</title>
      <link>https://arxiv.org/abs/2510.06466</link>
      <description>arXiv:2510.06466v1 Announce Type: new 
Abstract: We develop a deep reinforcement learning framework for dynamic portfolio optimization that combines a Dirichlet policy with cross-sectional attention mechanisms. The Dirichlet formulation ensures that portfolio weights are always feasible, handles tradability constraints naturally, and provides a stable way to explore the allocation space. The model integrates per-asset temporal encoders with a global attention layer, allowing it to capture sector relationships, factor spillovers, and other cross asset dependencies. The reward function includes transaction costs and portfolio variance penalties, linking the learning objective to traditional mean variance trade offs. The results show that attention based Dirichlet policies outperform equal-weight and standard reinforcement learning benchmarks in terms of terminal wealth and Sharpe ratio, while maintaining realistic turnover and drawdown levels. Overall, the study shows that combining principled action design with attention-based representations improves both the stability and interpretability of reinforcement learning for portfolio management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06466v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pei Xue, Yuanchun Ye</dc:creator>
    </item>
    <item>
      <title>A Higher-Order Time Domain Boundary Element Formulation based on Isogeometric Analysis and the Convolution Quadrature Method</title>
      <link>https://arxiv.org/abs/2510.06804</link>
      <description>arXiv:2510.06804v1 Announce Type: new 
Abstract: An isogeometric boundary element method (BEM) is presented to solve scattering problems in an isotropic homogeneous medium. We consider wave problems governed by the scalar wave equation as in acoustics and the Lam\'e-Navier equations for elastodynamics considering the theory of linear elasticity. The underlying boundary integral equations imply time-dependent convolution integrals and allow us to determine the sought quantities in the bounded interior or the unbounded exterior after solving for the unknown Cauchy data. In the present work, the time-dependent convolution integrals are approximated by multi-stage Runge-Kutta (RK) based convolution quadratures that involve steady-state solutions in the Laplace domain. The proposed method discretizes the spatial variables in the framework of isogeometric analysis (IGA), entailing a patchwise smooth spline basis. Overall, it enables high convergence rates in space and time. The implementation scheme follows an element structure defined by the non-empty knot spans in the knot vectors and local, uniform Bernstein polynomials as basis functions. The algorithms to localize the basis functions on the elements are outlined and explained. The solutions of the mixed problems are approximated by the BEM based on a symmetric Galerkin variational formulation and a collocation method. We investigate convergence rates of the approximative solutions in a mixed space and time error norm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06804v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thomas Kramer, Benjamin Marussig, Martin Schanz</dc:creator>
    </item>
    <item>
      <title>A Framework for Measuring How News Topics Drive Stock Movement</title>
      <link>https://arxiv.org/abs/2510.06864</link>
      <description>arXiv:2510.06864v1 Announce Type: new 
Abstract: In modern financial markets, news plays a critical role in shaping investor sentiment and influencing stock price movements. However, most existing studies aggregate daily news sentiment into a single score, potentially overlooking important variations in topic content and relevance. This simplification may mask nuanced relationships between specific news themes and market responses. To address this gap, this paper proposes a novel framework to examine how different news topics influence stock price movements. The framework encodes individual news headlines into dense semantic embeddings using a pretrained sentence transformer, then applies K-means clustering to identify distinct news topics. Topic exposures are incorporated as explanatory variables in an ordinary least squares regression to quantify their impact on daily stock returns. Applied to Apple Inc., the framework reveals that certain topics are significantly associated with positive or negative next-day returns, while others have no measurable effect. These findings highlight the importance of topic-level analysis in understanding the relationship between news content and financial markets. The proposed framework provides a scalable approach for both researchers and practitioners to assess the informational value of different news topics and suggests a promising direction for improving predictive models of stock price movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06864v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qizhao Chen</dc:creator>
    </item>
    <item>
      <title>TOMATOES: Topology and Material Optimization for Latent Heat Thermal Energy Storage Devices</title>
      <link>https://arxiv.org/abs/2510.07057</link>
      <description>arXiv:2510.07057v1 Announce Type: new 
Abstract: Latent heat thermal energy storage (LHTES) systems are compelling candidates for energy storage, primarily owing to their high storage density. Improving their performance is crucial for developing the next-generation efficient and cost effective devices. Topology optimization (TO) has emerged as a powerful computational tool to design LHTES systems by optimally distributing a high-conductivity material (HCM) and a phase change material (PCM). However, conventional TO typically limits to optimizing the geometry for a fixed, pre-selected materials. This approach does not leverage the large and expanding databases of novel materials. Consequently, the co-design of material and geometry for LHTES remains a challenge and unexplored.
  To address this limitation, we present an automated design framework for the concurrent optimization of material choice and topology. A key challenge is the discrete nature of material selection, which is incompatible with the gradient-based methods used for TO. We overcome this by using a data-driven variational autoencoder (VAE) to project discrete material databases for both the HCM and PCM onto continuous and differentiable latent spaces. These continuous material representations are integrated into an end-to-end differentiable, transient nonlinear finite-element solver that accounts for phase change. We demonstrate this framework on a problem aimed at maximizing the discharged energy within a specified time, subject to cost constraints. The effectiveness of the proposed method is validated through several illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07057v1</guid>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Kumar Padhy, Krishnan Suresh, Aaditya Chandrasekhar</dc:creator>
    </item>
    <item>
      <title>Bayesian Optimization under Uncertainty for Training a Scale Parameter in Stochastic Models</title>
      <link>https://arxiv.org/abs/2510.06439</link>
      <description>arXiv:2510.06439v1 Announce Type: cross 
Abstract: Hyperparameter tuning is a challenging problem especially when the system itself involves uncertainty. Due to noisy function evaluations, optimization under uncertainty can be computationally expensive. In this paper, we present a novel Bayesian optimization framework tailored for hyperparameter tuning under uncertainty, with a focus on optimizing a scale- or precision-type parameter in stochastic models. The proposed method employs a statistical surrogate for the underlying random variable, enabling analytical evaluation of the expectation operator. Moreover, we derive a closed-form expression for the optimizer of the random acquisition function, which significantly reduces computational cost per iteration. Compared with a conventional one-dimensional Monte Carlo-based optimization scheme, the proposed approach requires 40 times fewer data points, resulting in up to a 40-fold reduction in computational cost. We demonstrate the effectiveness of the proposed method through two numerical examples in computational engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06439v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akash Yadav, Ruda Zhang</dc:creator>
    </item>
    <item>
      <title>FEAorta: A Fully Automated Framework for Finite Element Analysis of the Aorta From 3D CT Images</title>
      <link>https://arxiv.org/abs/2510.06621</link>
      <description>arXiv:2510.06621v1 Announce Type: cross 
Abstract: Aortic aneurysm disease ranks consistently in the top 20 causes of death in the U.S. population. Thoracic aortic aneurysm is manifested as an abnormal bulging of thoracic aortic wall and it is a leading cause of death in adults. From the perspective of biomechanics, rupture occurs when the stress acting on the aortic wall exceeds the wall strength. Wall stress distribution can be obtained by computational biomechanical analyses, especially structural Finite Element Analysis. For risk assessment, probabilistic rupture risk of TAA can be calculated by comparing stress with material strength using a material failure model. Although these engineering tools are currently available for TAA rupture risk assessment on patient specific level, clinical adoption has been limited due to two major barriers: labor intensive 3D reconstruction current patient specific anatomical modeling still relies on manual segmentation, making it time consuming and difficult to scale to a large patient population, and computational burden traditional FEA simulations are resource intensive and incompatible with time sensitive clinical workflows. The second barrier was successfully overcome by our team through the development of the PyTorch FEA library and the FEA DNN integration framework. By incorporating the FEA functionalities within PyTorch FEA and applying the principle of static determinacy, we reduced the FEA based stress computation time to approximately three minutes per case. Moreover, by integrating DNN and FEA through the PyTorch FEA library, our approach further decreases the computation time to only a few seconds per case. This work focuses on overcoming the first barrier through the development of an end to end deep neural network capable of generating patient specific finite element meshes of the aorta directly from 3D CT images.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06621v1</guid>
      <category>eess.IV</category>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiasong Chen, Linchen Qian, Ruonan Gong, Christina Sun, Tongran Qin, Thuy Pham, Caitlin Martin, Mohammad Zafar, John Elefteriades, Wei Sun, Liang Liang</dc:creator>
    </item>
    <item>
      <title>Diffusion-Augmented Reinforcement Learning for Robust Portfolio Optimization under Stress Scenarios</title>
      <link>https://arxiv.org/abs/2510.07099</link>
      <description>arXiv:2510.07099v1 Announce Type: cross 
Abstract: In the ever-changing and intricate landscape of financial markets, portfolio optimisation remains a formidable challenge for investors and asset managers. Conventional methods often struggle to capture the complex dynamics of market behaviour and align with diverse investor preferences. To address this, we propose an innovative framework, termed Diffusion-Augmented Reinforcement Learning (DARL), which synergistically integrates Denoising Diffusion Probabilistic Models (DDPMs) with Deep Reinforcement Learning (DRL) for portfolio management. By leveraging DDPMs to generate synthetic market crash scenarios conditioned on varying stress intensities, our approach significantly enhances the robustness of training data. Empirical evaluations demonstrate that DARL outperforms traditional baselines, delivering superior risk-adjusted returns and resilience against unforeseen crises, such as the 2025 Tariff Crisis. This work offers a robust and practical methodology to bolster stress resilience in DRL-driven financial applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07099v1</guid>
      <category>stat.ML</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Himanshu Choudhary, Arishi Orra, Manoj Thakur</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Taskforce Collaboration: Self-Correction of Compounding Errors in Long-Form Literature Review Generation</title>
      <link>https://arxiv.org/abs/2508.04306</link>
      <description>arXiv:2508.04306v2 Announce Type: replace 
Abstract: Compounding error is critical in long-form literature review generation, where minor inaccuracies cascade and amplify across subsequent steps, severely compromising the faithfulness of the final output. To address this challenge, we propose the Multi-Agent Taskforce Collaboration (MATC) framework, which proactively mitigates errors by orchestrating LLM-based agents into three specialized taskforces: (1) an exploration taskforce that interleaves retrieval and outlining using a tree-based strategy to establish a grounded structure; (2) an exploitation taskforce that iteratively cycles between fact location and draft refinement to ensure evidential support; and (3) a feedback taskforce that leverages historical experience for self-correction before errors propagate. Experimental results show that MATC achieves state-of-the-art performance on existing benchmarks (AutoSurvey and SurveyEval), significantly outperforming strong baselines in both citation quality (e.g., +15.7% recall) and content quality. We further contribute TopSurvey, a new large-scale benchmark of 195 peer-reviewed survey topics, on which MATC maintains robust performance, demonstrating its generalizability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04306v2</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi Zhang, Yan Liu, Zhejing Hu, Gong Chen, Sheng-hua Zhong, Jiannong Cao</dc:creator>
    </item>
    <item>
      <title>GUIDe: Generative and Uncertainty-Informed Inverse Design for On-Demand Nonlinear Functional Responses</title>
      <link>https://arxiv.org/abs/2509.05641</link>
      <description>arXiv:2509.05641v2 Announce Type: replace 
Abstract: Inverse design is a common yet challenging engineering problem, particularly for nonlinear functional responses such as mechanical behavior or spectral analysis. Deep generative models are motivated by intractability, non-existence or non-uniqueness of solutions, and the need for rapid solution-space exploration. In this study, we show that deep generative model-based and optimization-based approaches can provide incomplete solutions or hallucinate given out-of-distribution targets. To address this, we propose the Generative and Uncertainty-informed Inverse Design (GUIDe) framework, which leverages probabilistic machine learning, statistical inference, and Markov chain Monte Carlo to generate designs with targeted nonlinear behaviors. Instead of inverse mappings, i.e., response $\mapsto$ design, GUIDe adopts design $\mapsto$ response: a forward model predicts each design's nonlinear functional response and evaluates the confidence under a user-specified tolerance. Sampling the solution space by this confidence yields diverse feasible designs. Our validation on nacre-inspired materials finds solutions beyond the training range, even under out-of-distribution targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.05641v2</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoxuan Dylan Mu, Mingjian Tang, Wei Gao, Wei "Wayne" Chen</dc:creator>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>https://arxiv.org/abs/2509.07627</link>
      <description>arXiv:2509.07627v4 Announce Type: replace 
Abstract: Designing full-length, epitope-specific TCR {\alpha}\b{eta} remains challenging due to vast sequence space, data biases and incomplete modeling of immunogenetic constraints. We present LSMTCR, a scalable multi-architecture framework that separates specificity from constraint learning to enable de novo, epitope-conditioned generation of paired, full-length TCRs. A diffusion-enhanced BERT encoder learns time-conditioned epitope representations; conditional GPT decoders, pretrained on CDR3\b{eta} and transferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modal conditioning with temperature-controlled diversity; and a gene-aware Transformer assembles complete {\alpha}/\b{eta} sequences by predicting V/J usage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and our curated dataset, LSMTCR achieves higher predicted binding than baselines on most datasets, more faithfully recovers positional and length grammars, and delivers superior, temperature-tunable diversity. For {\alpha}-chain generation, transfer learning improves predicted binding, length realism and diversity over representative methods. Full-length assembly from known or de novo CDR3s preserves k-mer spectra, yields low edit distances to references, and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higher pTM/ipTM than single-chain settings. LSMTCR outputs diverse, gene-contextualized, full-length TCR designs from epitope input alone, enabling high-throughput screening and iterative optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07627v4</guid>
      <category>cs.CE</category>
      <category>q-bio.BM</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihao Zhang, Xiao Liu</dc:creator>
    </item>
    <item>
      <title>Default Resilience and Worst-Case Effects in Financial Networks</title>
      <link>https://arxiv.org/abs/2403.10631</link>
      <description>arXiv:2403.10631v2 Announce Type: replace-cross 
Abstract: In this paper we analyze the resilience of a network of banks to joint price fluctuations of the external assets in which they have shared exposures, and evaluate the worst-case effects of the possible default contagion. Indeed, when the prices of certain external assets either decrease or increase, all banks exposed to them experience varying degrees of simultaneous shocks to their balance sheets. These coordinated and structured shocks have the potential to exacerbate the likelihood of defaults. In this context, we introduce first a concept of {default resilience margin}, $\epsilon^*$, i.e., the maximum amplitude of asset prices fluctuations that the network can tolerate without generating defaults. Such threshold value is computed by considering two different measures of price fluctuations, one based on the maximum individual variation of each asset, and the other based on the sum of all the asset's absolute variations. For any price perturbation having amplitude no larger than $\epsilon^*$, the network absorbs the shocks remaining default free. When the perturbation amplitude goes beyond $\epsilon^*$, however, defaults may occur. In this case we find the worst-case systemic loss, that is, the total unpaid debt under the most severe price variation of given magnitude. Computation of both the threshold level $\epsilon^*$ and of the worst-case loss and of a corresponding worst-case asset price scenario, amounts to solving suitable linear programming problems.}</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10631v2</guid>
      <category>q-fin.RM</category>
      <category>cs.CE</category>
      <category>math.OC</category>
      <category>q-fin.MF</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s41109-025-00728-5</arxiv:DOI>
      <arxiv:journal_reference>Applied Network Science 10, 48 (2025)</arxiv:journal_reference>
      <dc:creator>Giuseppe Calafiore, Giulia Fracastoro, Anton Proskurnikov</dc:creator>
    </item>
  </channel>
</rss>
