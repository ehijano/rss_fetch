<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Feb 2026 02:49:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Computational Explorations on Semifields</title>
      <link>https://arxiv.org/abs/2602.09577</link>
      <description>arXiv:2602.09577v1 Announce Type: new 
Abstract: A finite semifield is a division algebra over a finite field where multiplication is not necessarily associative. We consider here the complexity of the multiplication in small semifields and finite field extensions. For this operation, the number of required base field multiplications is the tensor rank, or the multiplicative complexity. The other base field operations are additions and scalings by constants, which together we refer to as the additive complexity. When used recursively, the tensor rank determines the exponent while the other operations determine the constant of the associated asymptotic complexity bounds. For small extensions, both measures are of similar importance.  In this paper, we establish the tensor rank of some semifields and finite fields of characteristics 2 and 3. We also propose new upper and lower bounds on their additive complexity, and give new associated algorithms improving on the state-of-the-art in terms of overall complexity. We achieve this by considering short straight line programs for encoding linear codes with given parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09577v1</guid>
      <category>cs.SC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Guillaume Dumas (UGA, LJK, CASC), Stefano Lia (UCD), John Sheekey (UCD)</dc:creator>
    </item>
    <item>
      <title>On semidefinite-representable sets over valued fields</title>
      <link>https://arxiv.org/abs/2602.09702</link>
      <description>arXiv:2602.09702v1 Announce Type: cross 
Abstract: Polyhedra and spectrahedra over the real numbers, or more generally their images under linear maps, are respectively the feasible sets of linear and semidefinite programming, and form the family of semidefinite-representable sets. This paper studies analogues of these sets, as well as the associated optimization problems, when the data are taken over a valued field $K$. For $K$-polyhedra and linear programming over $K$ we present an algorithm based on the computation of Smith normal forms. We prove that fundamental properties of semidefinite-representable sets extend to the valued setting. In particular, we exhibit examples of non-polyhedral $K$-spectrahedra, as well as sets that are semidefinite-representable over $K$ but are not $K$-spectrahedra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.09702v1</guid>
      <category>math.AG</category>
      <category>cs.SC</category>
      <category>math.OC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Corentin Cornou, Simone Naldi, Tristan Vaccon</dc:creator>
    </item>
    <item>
      <title>Artificial Intelligence Software Structured to Simulate Human Working Memory, Mental Imagery, and Mental Continuity</title>
      <link>https://arxiv.org/abs/2204.05138</link>
      <description>arXiv:2204.05138v3 Announce Type: replace-cross 
Abstract: This article presents an artificial intelligence (AI) architecture intended to simulate the iterative updating of the human working memory system. It features several interconnected neural networks designed to emulate the specialized modules of the cerebral cortex. These are structured hierarchically and integrated into a global workspace. They are capable of temporarily maintaining high-level representational patterns akin to the psychological items maintained in working memory. This maintenance is made possible by persistent neural activity in the form of two modalities: sustained neural firing (resulting in a focus of attention) and synaptic potentiation (resulting in a short-term store). Representations held in persistent activity are recursively replaced resulting in incremental changes to the content of the working memory system. As this content gradually evolves, successive processing states overlap and are continuous with one another. The present article will explore how this architecture can lead to iterative shift in the distribution of coactive representations, ultimately leading to mental continuity between processing states, and thus to human-like thought and cognition. Taken together, these components outline a biologically motivated route toward synthetic consciousness or artificial sentience and subjectivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.05138v3</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jared Edward Reser</dc:creator>
    </item>
    <item>
      <title>Breaking the Simplification Bottleneck in Amortized Neural Symbolic Regression</title>
      <link>https://arxiv.org/abs/2602.08885</link>
      <description>arXiv:2602.08885v3 Announce Type: replace-cross 
Abstract: Symbolic regression (SR) aims to discover interpretable analytical expressions that accurately describe observed data. Amortized SR promises to be much more efficient than the predominant genetic programming SR methods, but currently struggles to scale to realistic scientific complexity. We find that a key obstacle is the lack of a fast reduction of equivalent expressions to a concise normalized form. Amortized SR has addressed this by general-purpose Computer Algebra Systems (CAS) like SymPy, but the high computational cost severely limits training and inference speed. We propose SimpliPy, a rule-based simplification engine achieving a 100-fold speed-up over SymPy at comparable quality. This enables substantial improvements in amortized SR, including scalability to much larger training sets, more efficient use of the per-expression token budget, and systematic training set decontamination with respect to equivalent test expressions. We demonstrate these advantages in our Flash-ANSR framework, which achieves much better accuracy than amortized baselines (NeSymReS, E2E) on the FastSRB benchmark. Moreover, it performs on par with state-of-the-art direct optimization (PySR) while recovering more concise instead of more complex expressions with increasing inference budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08885v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SC</category>
      <pubDate>Wed, 11 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Paul Saegert, Ullrich K\"othe</dc:creator>
    </item>
  </channel>
</rss>
