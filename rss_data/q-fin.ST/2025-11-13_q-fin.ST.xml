<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 05:02:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Explainable Federated Learning for U.S. State-Level Financial Distress Modeling</title>
      <link>https://arxiv.org/abs/2511.08588</link>
      <description>arXiv:2511.08588v1 Announce Type: new 
Abstract: We present the first application of federated learning (FL) to the U.S. National Financial Capability Study, introducing an interpretable framework for predicting consumer financial distress across all 50 states and the District of Columbia without centralizing sensitive data. Our cross-silo FL setup treats each state as a distinct data silo, simulating real-world governance in nationwide financial systems. Unlike prior work, our approach integrates two complementary explainable AI techniques to identify both global (nationwide) and local (state-specific) predictors of financial hardship, such as contact from debt collection agencies. We develop a machine learning model specifically suited for highly categorical, imbalanced survey data. This work delivers a scalable, regulation-compliant blueprint for early warning systems in finance, demonstrating how FL can power socially responsible AI applications in consumer credit risk and financial inclusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08588v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Carta, Fernando Spadea, Oshani Seneviratne</dc:creator>
    </item>
    <item>
      <title>When Reasoning Fails: Evaluating 'Thinking' LLMs for Stock Prediction</title>
      <link>https://arxiv.org/abs/2511.08608</link>
      <description>arXiv:2511.08608v1 Announce Type: new 
Abstract: Problem. "Thinking" LLMs (TLLMs) expose explicit or hidden reasoning traces and are widely believed to generalize better on complex tasks than direct LLMs. Whether this promise carries to noisy, heavy-tailed and regime-switching financial data remains unclear.
  Approach. Using Indian equities (NIFTY constituents), we run a rolling 48m/1m walk-forward evaluation at horizon k = 1 day and dial cross-sectional complexity via the universe size U in {5, 11, 21, 36} while keeping the reasoning budget fixed (B = 512 tokens) for the TLLM. We compare a direct LLM (gpt-4o-mini), a TLLM (gpt-5), and classical learners (ridge, random forest) on cross-sectional ranking loss 1 - IC, MSE, and long/short backtests with realistic costs. Statistical confidence is measured with Diebold-Mariano, Pesaran-Timmermann, and SPA tests.
  Main findings.
  (i) As U grows under a fixed budget B, the TLLM's ranking quality deteriorates, whereas the direct LLM remains flat and classical baselines are stable.
  (ii) TLLM variance is higher, requiring ex-post calibration (winsorization and blending) for stability.
  (iii) Portfolio results under transaction costs do not support a net advantage for the TLLM.
  Hypotheses. Our results are consistent with the following testable hypotheses:
  H1 (Capacity-Complexity Mismatch): for fixed B, TLLM accuracy degrades superlinearly in cross-sectional complexity.
  H2 (Reasoning Variance): TLLM outputs exhibit higher dispersion date-by-date than direct LLMs, increasing error bars and turnover.
  H3 (Domain Misfit): next-token prediction objectives and token-budgeted inference are poorly aligned with heavy-tailed, weakly predictable stock returns.
  Implication. In our setting, "thinking" LLMs are not yet ready to replace classical or direct methods for short-horizon stock ranking; scaling the reasoning budget and/or re-aligning objectives appears necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08608v1</guid>
      <category>q-fin.ST</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rakeshkumar H Sodha</dc:creator>
    </item>
    <item>
      <title>Reasoning on Time-Series for Financial Technical Analysis</title>
      <link>https://arxiv.org/abs/2511.08616</link>
      <description>arXiv:2511.08616v1 Announce Type: new 
Abstract: While Large Language Models have been used to produce interpretable stock forecasts, they mainly focus on analyzing textual reports but not historical price data, also known as Technical Analysis. This task is challenging as it switches between domains: the stock price inputs and outputs lie in the time-series domain, while the reasoning step should be in natural language. In this work, we introduce Verbal Technical Analysis (VTA), a novel framework that combine verbal and latent reasoning to produce stock time-series forecasts that are both accurate and interpretable. To reason over time-series, we convert stock price data into textual annotations and optimize the reasoning trace using an inverse Mean Squared Error (MSE) reward objective. To produce time-series outputs from textual reasoning, we condition the outputs of a time-series backbone model on the reasoning-based attributes. Experiments on stock datasets across U.S., Chinese, and European markets show that VTA achieves state-of-the-art forecasting accuracy, while the reasoning traces also perform well on evaluation by industry experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08616v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kelvin J. L. Koa, Jan Chen, Yunshan Ma, Huanhuan Zheng, Tat-Seng Chua</dc:creator>
    </item>
    <item>
      <title>The LLM Pro Finance Suite: Multilingual Large Language Models for Financial Applications</title>
      <link>https://arxiv.org/abs/2511.08621</link>
      <description>arXiv:2511.08621v1 Announce Type: new 
Abstract: The financial industry's growing demand for advanced natural language processing (NLP) capabilities has highlighted the limitations of generalist large language models (LLMs) in handling domain-specific financial tasks. To address this gap, we introduce the LLM Pro Finance Suite, a collection of five instruction-tuned LLMs (ranging from 8B to 70B parameters) specifically designed for financial applications. Our approach focuses on enhancing generalist instruction-tuned models, leveraging their existing strengths in instruction following, reasoning, and toxicity control, while fine-tuning them on a curated, high-quality financial corpus comprising over 50% finance-related data in English, French, and German.
  We evaluate the LLM Pro Finance Suite on a comprehensive financial benchmark suite, demonstrating consistent improvement over state-of-the-art baselines in finance-oriented tasks and financial translation. Notably, our models maintain the strong general-domain capabilities of their base models, ensuring reliable performance across non-specialized tasks. This dual proficiency, enhanced financial expertise without compromise on general abilities, makes the LLM Pro Finance Suite an ideal drop-in replacement for existing LLMs in financial workflows, offering improved domain-specific performance while preserving overall versatility. We publicly release two 8B-parameters models to foster future research and development in financial NLP applications: https://huggingface.co/collections/DragonLLM/llm-open-finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08621v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>q-fin.CP</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ga\"etan Caillaut, Raheel Qader, Jingshu Liu, Mariam Nakhl\'e, Arezki Sadoune, Massinissa Ahmim, Jean-Gabriel Barthelemy</dc:creator>
    </item>
    <item>
      <title>Multi-period Learning for Financial Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2511.08622</link>
      <description>arXiv:2511.08622v1 Announce Type: new 
Abstract: Time series forecasting is important in finance domain. Financial time series (TS) patterns are influenced by both short-term public opinions and medium-/long-term policy and market trends. Hence, processing multi-period inputs becomes crucial for accurate financial time series forecasting (TSF). However, current TSF models either use only single-period input, or lack customized designs for addressing multi-period characteristics. In this paper, we propose a Multi-period Learning Framework (MLF) to enhance financial TSF performance. MLF considers both TSF's accuracy and efficiency requirements. Specifically, we design three new modules to better integrate the multi-period inputs for improving accuracy: (i) Inter-period Redundancy Filtering (IRF), that removes the information redundancy between periods for accurate self-attention modeling, (ii) Learnable Weighted-average Integration (LWI), that effectively integrates multi-period forecasts, (iii) Multi-period self-Adaptive Patching (MAP), that mitigates the bias towards certain periods by setting the same number of patches across all periods. Furthermore, we propose a Patch Squeeze module to reduce the number of patches in self-attention modeling for maximized efficiency. MLF incorporates multiple inputs with varying lengths (periods) to achieve better accuracy and reduces the costs of selecting input lengths during training. The codes and datasets are available at https://github.com/Meteor-Stars/MLF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08622v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3690624.3709422</arxiv:DOI>
      <dc:creator>Xu Zhang, Zhengang Huang, Yunzhi Wu, Xun Lu, Erpeng Qi, Yunkai Chen, Zhongya Xue, Qitong Wang, Peng Wang, Wei Wang</dc:creator>
    </item>
    <item>
      <title>"It Looks All the Same to Me": Cross-index Training for Long-term Financial Series Prediction</title>
      <link>https://arxiv.org/abs/2511.08658</link>
      <description>arXiv:2511.08658v1 Announce Type: new 
Abstract: We investigate a number of Artificial Neural Network architectures (well-known and more ``exotic'') in application to the long-term financial time-series forecasts of indexes on different global markets. The particular area of interest of this research is to examine the correlation of these indexes' behaviour in terms of Machine Learning algorithms cross-training. Would training an algorithm on an index from one global market produce similar or even better accuracy when such a model is applied for predicting another index from a different market? The demonstrated predominately positive answer to this question is another argument in favour of the long-debated Efficient Market Hypothesis of Eugene Fama.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08658v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-53969-5_26</arxiv:DOI>
      <dc:creator>Stanislav Selitskiy</dc:creator>
    </item>
    <item>
      <title>Online Ensemble Learning for Sector Rotation: A Gradient-Free Framework</title>
      <link>https://arxiv.org/abs/2304.09947</link>
      <description>arXiv:2304.09947v2 Announce Type: replace 
Abstract: We propose a gradient-free online ensemble learning algorithm that dynamically combines forecasts from a heterogeneous set of machine learning models based on their recent predictive performance, measured by out-of-sample R-squared. The ensemble is model-agnostic, requires no gradient access, and is designed for sequential forecasting under nonstationarity. It adaptively reweights 16 constituent models-three linear benchmarks (OLS, PCR, LASSO) and thirteen nonlinear learners including Random Forests, Gradient-Boosted Trees, and a hierarchy of neural networks (NN1-NN12). We apply the framework to sector rotation, using sector-level features aggregated from firm characteristics. Empirically, sector returns are more predictable and stable than individual asset returns, making them suitable for cross-sectional forecasting. The algorithm constructs sector-specific ensembles that assign adaptive weights in a rolling-window fashion, guided by forecast accuracy. Our key theoretical result bounds the online forecast regret directly in terms of realized out-of-sample R-squared, providing an interpretable guarantee that the ensemble performs nearly as well as the best model in hindsight. Empirically, the ensemble consistently outperforms individual models, equal-weighted averages, and traditional offline ensembles, delivering higher predictive accuracy, stronger risk-adjusted returns, and robustness across macroeconomic regimes, including during the COVID-19 crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09947v2</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 13 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaju Miao, Pawel Polak</dc:creator>
    </item>
  </channel>
</rss>
