<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 03 Dec 2024 04:25:48 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Deterministic and Probabilistic Rounding Error Analysis for Mixed-Precision Arithmetic on Modern Computing Units</title>
      <link>https://arxiv.org/abs/2411.18747</link>
      <description>arXiv:2411.18747v1 Announce Type: new 
Abstract: Modern computer architectures support low-precision arithmetic, which present opportunities for the adoption of mixed-precision algorithms to achieve high computational throughput and reduce energy consumption. As a growing number of scientific computations leverage specialized hardware accelerators, the risk of rounding errors increases, potentially compromising the reliability of models. This shift towards hardware-optimized, low-precision computations highlights the importance of rounding error analysis to ensure that performance gains do not come at the expense of accuracy, especially in high-stakes scientific applications. In this work, we conduct rounding error analysis on widely used operations such as fused multiply-add (FMA), mixed-precision FMA (MPFMA), and NVIDIA Tensor cores. We present a deterministic and probabilistic approach to quantifying the accumulated rounding errors. Numerical experiments are presented to perform the multiply and accumulate operation (MAC) and matrix-matrix multiplication using Tensor cores with random data. We show that probabilistic bounds produce tighter estimates by nearly an order of magnitude compared to deterministic ones for matrix-matrix multiplication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18747v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sahil Bhola, Karthik Duraisamy</dc:creator>
    </item>
    <item>
      <title>Penetrance Estimation in Family-based Studies with the penetrance R package</title>
      <link>https://arxiv.org/abs/2411.18816</link>
      <description>arXiv:2411.18816v1 Announce Type: new 
Abstract: Reliable methods for penetrance estimation are critical to improving clinical decision making and risk assessment for hereditary cancer syndromes. Penetrance is defined as the proportion of individuals who carry a genetic variant (i.e., genotype) that causes a trait and show symptoms of that trait, such as cancer (i.e., phenotype). We introduce penetrance, an open-source R package, to estimate age-specific penetrance from pedigree data. The package employs a Bayesian estimation approach, allowing for the incorporation of prior knowledge through the specification of priors for the parameters of the carrier distribution. It also includes options to impute missing ages during the estimation process, addressing incomplete age data in pedigree datasets. Our software provides a flexible and user-friendly tool for researchers to estimate penetrance in complex family-based studies, facilitating improved genetic risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18816v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Kubista, Danielle Braun, Giovanni Parmigiani</dc:creator>
    </item>
    <item>
      <title>Evaluating marginal likelihood approximations of dose-response relationship models in Bayesian benchmark dose methods for risk assessment</title>
      <link>https://arxiv.org/abs/2411.19066</link>
      <description>arXiv:2411.19066v1 Announce Type: new 
Abstract: Benchmark dose (BMD; a dose associated with a specified change in response) is used to determine the point of departure for the acceptable daily intake of substances for humans. Multiple dose-response relationship models are considered in the BMD method. Bayesian model averaging (BMA) is commonly used, where several models are averaged based on their posterior probabilities, which are determined by calculating the marginal likelihood (ML). Several ML approximation methods are employed in standard software packages, such as BBMD, \texttt{ToxicR}, and Bayesian BMD for the BMD method, because the ML cannot be analytically calculated. Although ML values differ among approximation methods, resulting in different posterior probabilities and BMD estimates, this phenomenon is neither widely recognized nor quantitatively evaluated. In this study, we evaluated the performance of five ML approximation methods: (1) maximum likelihood estimation (MLE)-based Schwarz criterion, (2) Markov chain Monte Carlo (MCMC)-based Schwarz criterion, (3) Laplace approximation, (4) density estimation, and (5) bridge sampling through numerical examples using four real experimental datasets. Eight models and three prior distributions used in BBMD and \texttt{ToxicR} were assumed. The approximation and estimation biases of bridge sampling were the smallest regardless of the dataset or prior distributions. Both the approximation and estimation biases of MCMC-based Schwarz criterion and Laplace approximation were large for some datasets. Thus, the approximation biases of the density estimation were relatively small but were large for some datasets. In terms of the accuracy of ML approximation methods, using Bayesian BMD, in which the bridge sampling is available, is preferred.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19066v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sota Minewaki, Tomohiro Ohigashi, Takashi Sozu</dc:creator>
    </item>
    <item>
      <title>Annealed variational mixtures for disease subtyping and biomarker discovery</title>
      <link>https://arxiv.org/abs/2411.19262</link>
      <description>arXiv:2411.19262v1 Announce Type: new 
Abstract: Cluster analyses of high-dimensional data are often hampered by the presence of large numbers of variables that do not provide relevant information, as well as the perennial issue of choosing an appropriate number of clusters. These challenges are frequently encountered when analysing `omics datasets, such as in molecular precision medicine, where a key goal is to identify disease subtypes and the biomarkers that define them. Here we introduce an annealed variational Bayes algorithm for fitting high-dimensional mixture models while performing variable selection. Our algorithm is scalable and computationally efficient, and we provide an open source Python implementation, VBVarSel. In a range of simulated and real biomedical examples, we show that VBVarSel outperforms the current state of the art, and demonstrate its use for cancer subtyping and biomarker discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19262v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emma Prevot, Rory Toogood, Filippo Pagani, Paul D. W. Kirk</dc:creator>
    </item>
    <item>
      <title>A Tidy Data Structure and Visualisations for Multiple Variable Correlations and Other Pairwise Scores</title>
      <link>https://arxiv.org/abs/2411.19830</link>
      <description>arXiv:2411.19830v1 Announce Type: new 
Abstract: We provide a pipeline for calculating, managing and visualising correlations and other pairwise scores for numerical and categorical data. We present a uniform interface for calculating a plethora of pairwise scores and a new tidy data structure for managing the results. We also provide new visualisations which simultaneously show multiple and/or grouped pairwise scores. The visualisations are far richer than a traditional heatmap of correlation scores, as they help identify relationships with categorical variables, numeric variable pairs with non-linear associations or those which exhibit Simpson's paradox. These methods are available in our R package bullseye.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19830v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amit Chinwan, Catherine B. Hurley</dc:creator>
    </item>
    <item>
      <title>Nonparametric Estimation for a Log-concave Distribution Function with Interval-censored Data</title>
      <link>https://arxiv.org/abs/2411.19878</link>
      <description>arXiv:2411.19878v1 Announce Type: new 
Abstract: We consider the nonparametric maximum likelihood estimation for the underlying event time based on mixed-case interval-censored data, under a log-concavity assumption on its distribution function. This generalized framework relaxes the assumptions of a log-concave density function or a concave distribution function considered in the literature. A log-concave distribution function is fulfilled by many common parametric families in survival analysis and also allows for multi-modal and heavy-tailed distributions. We establish the existence, uniqueness and consistency of the log-concave nonparametric maximum likelihood estimator. A computationally efficient procedure that combines an active set algorithm with the iterative convex minorant algorithm is proposed. Numerical studies demonstrate the advantages of incorporating additional shape constraint compared to the unconstrained nonparametric maximum likelihood estimator. The results also show that our method achieves a balance between efficiency and robustness compared to assuming log-concavity in the density. An R package iclogcondist is developed to implement our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19878v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi Wing Chu, Hok Kan Ling, Chaoyu Yuan</dc:creator>
    </item>
    <item>
      <title>Bayesian Cluster Weighted Gaussian Models</title>
      <link>https://arxiv.org/abs/2411.18957</link>
      <description>arXiv:2411.18957v1 Announce Type: cross 
Abstract: We introduce a novel class of Bayesian mixtures for normal linear regression models which incorporates a further Gaussian random component for the distribution of the predictor variables. The proposed cluster-weighted model aims to encompass potential heterogeneity in the distribution of the response variable as well as in the multivariate distribution of the covariates for detecting signals relevant to the underlying latent structure. Of particular interest are potential signals originating from: (i) the linear predictor structures of the regression models and (ii) the covariance structures of the covariates. We model these two components using a lasso shrinkage prior for the regression coefficients and a graphical-lasso shrinkage prior for the covariance matrices. A fully Bayesian approach is followed for estimating the number of clusters, by treating the number of mixture components as random and implementing a trans-dimensional telescoping sampler. Alternative Bayesian approaches based on overfitting mixture models or using information criteria to select the number of components are also considered. The proposed method is compared against EM type implementation, mixtures of regressions and mixtures of experts. The method is illustrated using a set of simulation studies and a biomedical dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18957v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panagiotis Papastamoulis, Konstantinos Perrakis</dc:creator>
    </item>
    <item>
      <title>Memristive Nanowire Network for Energy Efficient Audio Classification: Pre-Processing-Free Reservoir Computing with Reduced Latency</title>
      <link>https://arxiv.org/abs/2411.19611</link>
      <description>arXiv:2411.19611v1 Announce Type: cross 
Abstract: Speech recognition is a key challenge in natural language processing, requiring low latency, efficient computation, and strong generalization for real-time applications. While software-based artificial neural networks (ANNs) excel at this task, they are computationally intensive and depend heavily on data pre-processing. Neuromorphic computing, with its low-latency and energy-efficient advantages, holds promise for audio classification. Memristive nanowire networks, combined with pre-processing techniques like Mel-Frequency Cepstrum Coefficient extraction, have been widely used for associative learning, but such pre-processing can be power-intensive, undermining latency benefits. This study pioneers the use of memristive and spatio-temporal properties of nanowire networks for audio signal classification without pre-processing. A nanowire network simulation is paired with three linear classifiers for 10-class MNIST audio classification and binary speaker generalization tests. The hybrid system achieves significant benefits: excellent data compression with only 3% of nanowire output utilized, a 10-fold reduction in computational latency, and up to 28.5% improved classification accuracy (using a logistic regression classifier). Precision and recall improve by 10% and 17% for multispeaker datasets, and by 24% and 17% for individual speaker datasets, compared to raw data classifiers. This work provides a foundational proof of concept for utilizing memristive nanowire networks (NWN) in edge-computing devices, showcasing their potential for efficient, real-time audio signal processing with reduced computational overhead and power consumption, and enabling the development of advanced neuromorphic computing solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19611v1</guid>
      <category>cs.SD</category>
      <category>cond-mat.dis-nn</category>
      <category>eess.AS</category>
      <category>physics.app-ph</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Akshaya Rajesh (nano-Macro Reliability Laboratory), Pavithra Ananthasubramanian (nano-Macro Reliability Laboratory), Nagarajan Raghavan (nano-Macro Reliability Laboratory), Ankush Kumar (nano-Macro Reliability Laboratory, Centre for Nanotechnology, Indian Institute of Technology Roorkee, Roorkee, Uttrakhand, 247667, India)</dc:creator>
    </item>
    <item>
      <title>Gaussian multi-target filtering with target dynamics driven by a stochastic differential equation</title>
      <link>https://arxiv.org/abs/2411.19814</link>
      <description>arXiv:2411.19814v1 Announce Type: cross 
Abstract: This paper proposes multi-target filtering algorithms in which target dynamics are given in continuous time and measurements are obtained at discrete time instants. In particular, targets appear according to a Poisson point process (PPP) in time with a given Gaussian spatial distribution, targets move according to a general time-invariant linear stochastic differential equation, and the life span of each target is modelled with an exponential distribution. For this multi-target dynamic model, we derive the distribution of the set of new born targets and calculate closed-form expressions for the best fitting mean and covariance of each target at its time of birth by minimising the Kullback-Leibler divergence via moment matching. This yields a novel Gaussian continuous-discrete Poisson multi-Bernoulli mixture (PMBM) filter, and its approximations based on Poisson multi-Bernoulli and probability hypothesis density filtering. These continuous-discrete multi-target filters are also extended to target dynamics driven by nonlinear stochastic differential equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19814v1</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\'Angel F. Garc\'ia-Fern\'andez, Simo S\"arkk\"a</dc:creator>
    </item>
    <item>
      <title>No Free Lunch for Approximate MCMC</title>
      <link>https://arxiv.org/abs/2010.12514</link>
      <description>arXiv:2010.12514v2 Announce Type: replace 
Abstract: It is widely known that the performance of Markov chain Monte Carlo (MCMC) can degrade quickly when targeting computationally expensive posterior distributions, such as when the sample size is large. This has motivated the search for MCMC variants that scale well to large datasets. One popular general approach has been to look at only a subsample of the data at every step. In this note, we point out that well-known MCMC convergence results often imply that these ``subsampling'' MCMC algorithms cannot greatly improve performance. We apply these abstract results to realistic statistical problems and proposed algorithms, and also discuss some design principles suggested by the results. Finally, we develop estimates for the singular values of random matrices bounds that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.12514v2</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>James E. Johndrow, Natesh S. Pillai, Aaron Smith</dc:creator>
    </item>
    <item>
      <title>Cross Validation for Penalized Quantile Regression with a Case-Weight Adjusted Solution Path</title>
      <link>https://arxiv.org/abs/1902.07770</link>
      <description>arXiv:1902.07770v2 Announce Type: replace-cross 
Abstract: Cross validation is widely used for selecting tuning parameters in regularization methods, but it is computationally intensive in general. To lessen its computational burden, approximation schemes such as generalized approximate cross validation (GACV) are often employed. However, such approximations may not work well when non-smooth loss functions are involved. As a case in point, approximate cross validation schemes for penalized quantile regression do not work well for extreme quantiles. In this paper, we propose a new algorithm to compute the leave-one-out cross validation scores exactly for quantile regression with ridge penalty through a case-weight adjusted solution path. Resorting to the homotopy technique in optimization, we introduce a case weight for each individual data point as a continuous embedding parameter and decrease the weight gradually from one to zero to link the estimators based on the full data and those with a case deleted. This allows us to design a solution path algorithm to compute all leave-one-out estimators very efficiently from the full-data solution. We show that the case-weight adjusted solution path is piecewise linear in the weight parameter, and using the solution path, we examine case influences comprehensively and observe that different modes of case influences emerge, depending on the specified quantiles, data dimensions and penalty parameter. We further illustrate the utility of the proposed algorithm in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:1902.07770v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shanshan Tu, Yunzhang Zhu, Yoonkyung Lee, Qiuyu Gu, Haozhen Yu</dc:creator>
    </item>
    <item>
      <title>Fast post-process Bayesian inference with Variational Sparse Bayesian Quadrature</title>
      <link>https://arxiv.org/abs/2303.05263</link>
      <description>arXiv:2303.05263v3 Announce Type: replace-cross 
Abstract: In applied Bayesian inference scenarios, users may have access to a large number of pre-existing model evaluations, for example from maximum-a-posteriori (MAP) optimization runs. However, traditional approximate inference techniques make little to no use of this available information. We propose the framework of post-process Bayesian inference as a means to obtain a quick posterior approximation from existing target density evaluations, with no further model calls. Within this framework, we introduce Variational Sparse Bayesian Quadrature (VSBQ), a method for post-process approximate inference for models with black-box and potentially noisy likelihoods. VSBQ reuses existing target density evaluations to build a sparse Gaussian process (GP) surrogate model of the log posterior density function. Subsequently, we leverage sparse-GP Bayesian quadrature combined with variational inference to achieve fast approximate posterior inference over the surrogate. We validate our method on challenging synthetic scenarios and real-world applications from computational neuroscience. The experiments show that VSBQ builds high-quality posterior approximations by post-processing existing optimization traces, with no further model evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.05263v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengkun Li, Gr\'egoire Clart\'e, Martin J{\o}rgensen, Luigi Acerbi</dc:creator>
    </item>
    <item>
      <title>Weighted Particle-Based Optimization for Efficient Generalized Posterior Calibration</title>
      <link>https://arxiv.org/abs/2405.04845</link>
      <description>arXiv:2405.04845v4 Announce Type: replace-cross 
Abstract: In the realm of statistical learning, the increasing volume of accessible data and increasing model complexity necessitate robust methodologies. This paper explores two branches of robust Bayesian methods in response to this trend. The first is generalized Bayesian inference, which introduces a learning rate parameter to enhance robustness against model misspecifications. The second is Gibbs posterior inference, which formulates inferential problems using generic loss functions rather than probabilistic models. In such approaches, it is necessary to calibrate the spread of the posterior distribution by selecting a learning rate parameter. The study aims to enhance the generalized posterior calibration (GPC) algorithm proposed by [1]. Their algorithm chooses the learning rate to achieve the nominal frequentist coverage probability, but it is computationally intensive because it requires repeated posterior simulations for bootstrap samples. We propose a more efficient version of the GPC inspired by sequential Monte Carlo (SMC) samplers. A target distribution with a different learning rate is evaluated without posterior simulation as in the reweighting step in SMC sampling. Thus, the proposed algorithm can reach the desirable value within a few iterations. This improvement substantially reduces the computational cost of the GPC. Its efficacy is demonstrated through synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04845v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ICoDSA62899.2024.10651910</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 7th International Conference on Data Science and Its Applications 2024 (ICoDSA), pp. 515-521</arxiv:journal_reference>
      <dc:creator>Masahiro Tanaka</dc:creator>
    </item>
    <item>
      <title>Polynomial approximation of noisy functions</title>
      <link>https://arxiv.org/abs/2410.02317</link>
      <description>arXiv:2410.02317v2 Announce Type: replace-cross 
Abstract: Approximating a univariate function on the interval $[-1,1]$ with a polynomial is among the most classical problems in numerical analysis. When the function evaluations come with noise, a least-squares fit is known to reduce the effect of noise as more samples are taken. The generic algorithm for the least-squares problem requires $O(Nn^2)$ operations, where $N+1$ is the number of sample points and $n$ is the degree of the polynomial approximant. This algorithm is unstable when $n$ is large, for example $n\gg \sqrt{N}$ for equispaced sample points. In this study, we blend numerical analysis and statistics to introduce a stable and fast $O(N\log N)$ algorithm called NoisyChebtrunc based on the Chebyshev interpolation. It has the same error reduction effect as least-squares and the convergence is spectral until the error reaches $O(\sigma \sqrt{{n}/{N}})$, where $\sigma$ is the noise level, after which the error continues to decrease at the Monte-Carlo $O(1/\sqrt{N})$ rate. To determine the polynomial degree, NoisyChebtrunc employs a statistical criterion, namely Mallows' $C_p$. We analyze NoisyChebtrunc in terms of the variance and concentration in the infinity norm to the underlying noiseless function. These results show that with high probability the infinity-norm error is bounded by a small constant times $\sigma \sqrt{{n}/{N}}$, when the noise {is} independent and follows a subgaussian or subexponential distribution. We illustrate the performance of NoisyChebtrunc with numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02317v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeru Matsuda, Yuji Nakatsukasa</dc:creator>
    </item>
  </channel>
</rss>
