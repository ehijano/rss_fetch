<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.NA</link>
    <description>math.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Nov 2025 02:36:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A neural-network based nonlinear non-intrusive reduced basis method with online adaptation for parametrized partial differential equations</title>
      <link>https://arxiv.org/abs/2511.07684</link>
      <description>arXiv:2511.07684v1 Announce Type: new 
Abstract: We propose a nonlinear, non-intrusive reduced basis method with online adaptation for efficient approximation of parametrized partial differential equations. The method combines neural networks with reduced-order modeling and physics-informed training to enhance both accuracy and efficiency. In the offline stage, reduced basis functions are obtained via nonlinear dimension reduction, and a neural surrogate is trained to map parameters to approximate solutions. The surrogate employs a nonlinear reconstruction of the solution from the basis functions, enabling more accurate representation of complex solution structures than linear mappings. The model is further refined during the online stage using lightweight physics-informed neural network training. This offline-online framework enables accurate prediction especially in complex scenarios or with limited snapshot data. We demonstrate the performance and effectiveness of the proposed method through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07684v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingye Li, Alex Bespalov, Jinglai Li</dc:creator>
    </item>
    <item>
      <title>A New Initial Approximation Bound in the Durand Kerner Algorithm for Finding Polynomial Zeros</title>
      <link>https://arxiv.org/abs/2511.07728</link>
      <description>arXiv:2511.07728v1 Announce Type: new 
Abstract: The Durand-Kerner algorithm is a widely used iterative technique for simultaneously finding all the roots of a polynomial. However, its convergence heavily depends on the choice of initial approximations. This paper introduces two novel approaches for determining the initial values: New bound 1 and the lambda maximal bound, aimed at improving the stability and convergence speed of the algorithm. Theoretical analysis and numerical experiments were conducted to evaluate the effectiveness of these bounds. The lambda maximal bound consistently ensures that all the roots lie within the complex circle, leading to faster and more stable convergence. Comparative results demonstrate that while New bound 1 guarantees convergence, but it yields excessively large radii.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07728v1</guid>
      <category>math.NA</category>
      <category>cs.DS</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>B. A. Sanjoyo, M. Yunus, N. Hidayat</dc:creator>
    </item>
    <item>
      <title>Structure-Preserving Transfer of Grad-Shafranov Equilibria to Magnetohydrodynamic Solvers</title>
      <link>https://arxiv.org/abs/2511.07763</link>
      <description>arXiv:2511.07763v1 Announce Type: new 
Abstract: Magnetohydrodynamic (MHD) solvers used to study dynamic plasmas for magnetic confinement fusion typically rely on initial conditions that describe force balance, which are provided by an equilibrium solver based on the Grad-Shafranov (GS) equation. Transferring such equilibria from the GS discretization to the MHD discretization often introduces errors that lead to unwanted perturbations to the equilibria on the level of the MHD discretization. In this work, we identify and analyze sources of such errors in the context of finite element methods, with a focus on the force balance and divergence-free properties of the loaded equilibria. In particular, we reveal three main sources of errors: (1) the improper choice of finite element spaces in the MHD scheme relative to the poloidal flux and toroidal field function spaces in the GS scheme, (2) the misalignment of the meshes from two solvers, and (3) possibly under-resolved strong gradients near the separatrix. With this in mind, we study the impact of different choices of finite element spaces, including those based on compatible finite elements. In addition, we also investigate the impact of mesh misalignment and propose to conduct mesh refinement to resolve the strong gradients near the separatrix. Numerical experiments are conducted to demonstrate equilibria errors arising in the transferred initial conditions. Results show that force balance is best preserved when structure-preserving finite element spaces are used and when the MHD and GS meshes are both aligned and refined. Given that the poloidal flux is often computed in continuous Galerkin spaces, we further demonstrate that projecting the magnetic field into divergence-conforming spaces is optimal for preserving force balance, while projection into curl-conforming spaces, although less optimal for force balance, weakly preserves the divergence-free property.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07763v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rushan Zhang, Golo Wimmer, Qi Tang</dc:creator>
    </item>
    <item>
      <title>Fast Direct Solvers</title>
      <link>https://arxiv.org/abs/2511.07773</link>
      <description>arXiv:2511.07773v1 Announce Type: new 
Abstract: This survey describes a class of methods known as "fast direct solvers". These algorithms address the problem of solving a system of linear equations $\boldsymbol{Ax}=\boldsymbol{b}$ arising from the discretization of either an elliptic PDE or of an associated integral equation. The matrix $\boldsymbol{A}$ will be sparse when the PDE is discretized directly, and dense when an integral equation formulation is used. In either case, industry practice for large scale problems has for decades been to use iterative solvers such as multigrid, GMRES, or conjugate gradients. A direct solver, in contrast, builds an approximation to the inverse of $\boldsymbol{A}$, or alternatively, an easily invertible factorization (e.g. LU or Cholesky). A major development in numerical analysis in the last couple of decades has been the emergence of algorithms for constructing such factorizations or performing such inversions in linear or close to linear time. Such methods must necessarily exploit that the matrix $\boldsymbol{A}^{-1}$ is "data-sparse", typically in the sense that it can be tessellated into blocks that have low numerical rank. This survey provides a unifying context to both sparse and dense fast direct solvers, introduces key concepts with a minimum of notational overhead, and provides guidance to help a user determine the best method to use for a given application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07773v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>physics.comp-ph</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Per-Gunnar Martinsson, Michael O'Neil</dc:creator>
    </item>
    <item>
      <title>Hyperellipsoid Density Sampling: Exploitative Sequences to Accelerate High-Dimensional Optimization</title>
      <link>https://arxiv.org/abs/2511.07836</link>
      <description>arXiv:2511.07836v2 Announce Type: new 
Abstract: The curse of dimensionality presents a pervasive challenge in optimization problems, with exponential expansion of the search space rapidly causing traditional algorithms to become inefficient or infeasible. An adaptive sampling strategy is presented to accelerate optimization in this domain as an alternative to uniform quasi-Monte Carlo (QMC) methods.
  This method, referred to as Hyperellipsoid Density Sampling (HDS), generates its sequences by defining multiple hyperellipsoids throughout the search space. HDS uses three types of unsupervised learning algorithms to circumvent high-dimensional geometric calculations, producing an intelligent, non-uniform sample sequence that exploits statistically promising regions of the parameter space and improves final solution quality in high-dimensional optimization problems.
  A key feature of the method is optional Gaussian weights, which may be provided to influence the sample distribution towards known locations of interest. This capability makes HDS versatile for applications beyond optimization, providing a focused, denser sample distribution where models need to concentrate their efforts on specific, non-uniform regions of the parameter space.
  The method was evaluated against Sobol, a standard QMC method, using differential evolution (DE) on the 29 CEC2017 benchmark test functions. The results show statistically significant improvements in solution geometric mean error (p &lt; 0.05), with average performance gains ranging from 3% in 30D to 37% in 10D. This paper demonstrates the efficacy of HDS as a robust alternative to QMC sampling for high-dimensional optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07836v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.NE</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julian Soltes</dc:creator>
    </item>
    <item>
      <title>Derivation of resonance-based schemes via normal forms</title>
      <link>https://arxiv.org/abs/2511.07838</link>
      <description>arXiv:2511.07838v1 Announce Type: new 
Abstract: In this work, we propose a systematic derivation of resonance-based schemes via normal forms. The main idea is to use an arborification map on decorated trees together with a Butcher-Connes-Kreimer type coproduct and lower-dominant parts decompositions of the Fourier operator coming from the nonlinear interactions. This new family of low regularity schemes has explicit formulae for its coefficients and its local error. Under a mild assumption, one could expect these schemes to have a similar local error as the low regularity schemes proposed in arXiv:2005.01649.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07838v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.RA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yvain Bruned</dc:creator>
    </item>
    <item>
      <title>A Novel Block-Alternating Iterative Algorithm for Retrieving Top-$k$ Elements from Factorized Tensors</title>
      <link>https://arxiv.org/abs/2511.07898</link>
      <description>arXiv:2511.07898v1 Announce Type: new 
Abstract: Tensors, especially higher-order tensors, are typically represented in low-rank formats to preserve the main information of the high-dimensional data while saving memory space. In practice, only a small fraction elements in high-dimensional data are of interest, such as the $k$ largest or smallest elements. Thus, retrieving the $k$ largest/smallest elements from a low-rank tensor is a fundamental and important task in a wide variety of applications. In this paper, we first model the top-$k$ elements retrieval problem to a continuous constrained optimization problem. To address the equivalent optimization problem, we develop a block-alternating iterative algorithm that decomposes the original problem into a sequence of small-scale subproblems. Leveraging the separable summation structure of the objective function, a heuristic algorithm is proposed to solve these subproblems in an alternating manner. Numerical experiments with tensors from synthetic and real-world applications demonstrate that the proposed algorithm outperforms existing methods in terms of accuracy and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07898v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuanfu Xiao, Jiaxin Zeng</dc:creator>
    </item>
    <item>
      <title>Constructive quasi-uniform sequences over triangles</title>
      <link>https://arxiv.org/abs/2511.07909</link>
      <description>arXiv:2511.07909v1 Announce Type: new 
Abstract: In this paper, we develop constructive algorithms for generating quasi-uniform point sets and sequences over arbitrary two-dimensional triangular domains. Our proposed method, called the \emph{Voronoi-guided greedy packing} algorithm, iteratively selects the point farthest from the current set among a finite candidate set determined by the Voronoi diagram of the triangle. Our main theoretical result shows that, after a finite number of iterations, the mesh ratio of the generated point set is at most~2, which is known to be optimal. We further analyze two existing triangular low-discrepancy point sets and prove that their mesh ratios are uniformly bounded, thereby establishing their quasi-uniformity. Finally, through a series of numerical experiments, we demonstrate that the proposed method provides an efficient and practical strategy for generating high-quality point sets on individual triangles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07909v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hengjun Xu, Takashi Goda</dc:creator>
    </item>
    <item>
      <title>Standard versus Asymptotic Preserving Time Discretizations for the Poisson-Nernst-Planck System in the Quasi-Neutral Limit</title>
      <link>https://arxiv.org/abs/2511.07964</link>
      <description>arXiv:2511.07964v1 Announce Type: new 
Abstract: In this paper, we investigate the correlated diffusion of two ion species governed by a Poisson-Nernst-Planck (PNP) system. Here we further validate the numerical scheme recently proposed in \cite{astuto2025asymptotic}, where a time discretization method was shown to be Asymptotic-Preserving (AP) with respect to the Debye length. For vanishingly Debye lengths, the so called Quasi-Neutral limit can be adopted, reducing the system to a single diffusion equation with an effective diffusion coefficient \cite{CiCP-31-707}. Choosing small, but not negligible, Debye lengths, standard numerical methods suffer from severe stability restrictions and difficulties in handling initial conditions. IMEX schemes, on the other hand, are proved to be asymptotically stable for all Debye lengths, and do not require any assumption on the initial conditions. In this work, we compare different time discretizations to show their asymptotic behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07964v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clarissa Astuto</dc:creator>
    </item>
    <item>
      <title>Generalized Probability Density Approach to Histopolation Schemes of Arbitrary Order</title>
      <link>https://arxiv.org/abs/2511.07972</link>
      <description>arXiv:2511.07972v1 Announce Type: new 
Abstract: In this paper, we investigate the reconstruction of a bivariate function from weighted edge integrals on a triangular mesh, a problem of central importance in tomography, computer vision, and numerical approximation. Our approach is based on local histopolation methods defined through unisolvent triples, where the edge weights are induced by probability densities. We present a general strategy that applies to arbitrary polynomial order~$k$, in which edge moments are taken against orthogonal polynomials associated with the chosen densities. This yields a systematic framework for weighted reconstructions of any degree, with theoretical guarantees of unisolvency and fully explicit basis functions. As a concrete and flexible instance, we introduce a two-parameter family of Jacobi-type distributions on $[-1,1]$, together with its symmetric Gegenbauer subclass, and show how these densities generate new quadratic reconstruction operators that generalize the standard linear histopolation scheme while preserving its simplicity and locality. We employ an adaptive parameter selection algorithm for Jacobi densities, which automatically tunes the distribution parameters to minimize the global reconstruction error. This strategy enhances robustness and adaptivity across different function classes and mesh resolutions. The effectiveness of the proposed operators is demonstrated through extensive numerical experiments, which confirm their superior accuracy in approximating both smooth and highly oscillatory functions. Finally, the framework is sufficiently general to accommodate any admissible edge density, thus providing a flexible and broadly applicable tool for weighted function reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07972v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gradimir V. Milovanovic, Federico Nudo</dc:creator>
    </item>
    <item>
      <title>In-Memory Load Balancing for Discontinuous Galerkin Methods on Polytopal Meshes</title>
      <link>https://arxiv.org/abs/2511.08020</link>
      <description>arXiv:2511.08020v1 Announce Type: new 
Abstract: High-order accurate discontinuous Galerkin (DG) methods have emerged as powerful tools for solving partial differential equations such as the compressible Navier-Stokes equations due to their excellent dispersion-dissipation properties and scalability on modern hardware. The open-source DG framework FLEXI has recently been extended to support DG schemes on general polytopal elements including tetrahedra, prisms, and pyramids. This advancement enables simulations on complex geometries where purely hexahedral meshes are difficult or impossible to generate. However, the use of meshes with heterogeneous element types introduces a workload imbalance, a consequence of the temporal evolution of modal rather than nodal degrees of freedom and the accompanying transformations. In this work, we present a lightweight, system-agnostic in-memory load balancing strategy designed for high-order DG solvers. The method employs high-precision runtime measurements and efficient data redistribution to dynamically reassign mesh elements along a space-filling curve. We demonstrate the effectiveness of the approach through simulations of the Taylor-Green vortex and large-scale parallel runs on the EuroHPC pre-exascale system MareNostrum 5. Results show that the proposed strategy recovers a significant fraction of the lost efficiency on heterogeneous meshes while retaining excellent strong and weak scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08020v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Kopper, Anna Schwarz, Jens Keim, Andrea Beck</dc:creator>
    </item>
    <item>
      <title>A simple predictor-corrector scheme without order reduction for advection-diffusion-reaction problems</title>
      <link>https://arxiv.org/abs/2511.08164</link>
      <description>arXiv:2511.08164v1 Announce Type: new 
Abstract: Treating diffusion and advection/reaction separately is an effective strategy for solving semilinear advection-diffusion-reaction equations. However, such an approach is prone to suffer from order reduction, especially in the presence of inhomogeneous Dirichlet boundary conditions. In this paper, we extend an approach of Einkemmer and Ostermann [SIAM J. Sci. Comput. 37, A1577-A1592, 2015] to advection-diffusion-reaction problems, where the advection and reaction terms depend nonlinearly on both the solution and its gradient. Starting from a modified splitting method, we construct a predictor-corrector scheme that avoids order reduction and significantly improves accuracy. The predictor only requires the solution of a linear diffusion equation, while the corrector is simply an explicit Euler step of an advection-reaction equation. Under appropriate regularity assumptions on the exact solution, we rigorously establish second-order convergence for this scheme. Numerical experiments are presented to confirm the theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08164v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thi Tam Dang, Lukas Einkemmer, Alexander Ostermann</dc:creator>
    </item>
    <item>
      <title>A Stable Iterative Direct Sampling Method for Elliptic Inverse Problems with Partial Cauchy Data</title>
      <link>https://arxiv.org/abs/2511.08171</link>
      <description>arXiv:2511.08171v1 Announce Type: new 
Abstract: We develop a novel iterative direct sampling method (IDSM) for solving linear or nonlinear elliptic inverse problems with partial Cauchy data. It integrates three innovations: a data completion scheme to reconstruct missing boundary information, a heterogeneously regularized Dirichlet-to-Neumann map to enhance the near-orthogonality of probing functions, and a stabilization-correction strategy to ensure the numerical stability. The resulting method is remarkably robust with respect to measurement noise, is flexible with the measurement configuration, enjoys provable stability guarantee, and achieves enhanced resolution for recovering inhomogeneities. Numerical experiments in electrical impedance tomography, diffuse optical tomography, and cardiac electrophysiology show its effectiveness in accurately reconstructing the locations and geometries of inhomogeneities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08171v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bangti Jin, Fengru Wang, Jun Zou</dc:creator>
    </item>
    <item>
      <title>An Iterative Direct Sampling Method for Reconstructing Moving Inhomogeneities in Parabolic Problems</title>
      <link>https://arxiv.org/abs/2511.08197</link>
      <description>arXiv:2511.08197v1 Announce Type: new 
Abstract: We propose in this work a novel iterative direct sampling method for imaging moving inhomogeneities in parabolic problems using boundary measurements. It can efficiently identify the locations and shapes of moving inhomogeneities when very limited data are available, even with only one pair of lateral Cauchy data, and enjoys remarkable numerical stability for noisy data and over an extended time horizon. The method is formulated in an abstract framework, and is applicable to linear and nonlinear parabolic problems, including linear, nonlinear, and mixed-type inhomogeneities. Numerical experiments across diverse scenarios show its effectiveness and robustness against the data noise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08197v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bangti Jin, Fengru Wang, Jun Zou</dc:creator>
    </item>
    <item>
      <title>Pointwise A Posteriori Error Estimators for Multiple and Clustered Eigenvalue Computations</title>
      <link>https://arxiv.org/abs/2511.08259</link>
      <description>arXiv:2511.08259v1 Announce Type: new 
Abstract: In this work, we propose an a pointwise a posteriori error estimator for conforming finite element approximations of eigenfunctions corresponding to multiple and clustered eigenvalues of elliptic operators. It is proven that the pointwise a posteriori error estimator is reliable and efficient, up to some logarithmic factors of the mesh size. The constants involved in the reliability and efficiency are independent of the gaps among the targeted eigenvalues, the mesh size and the number of mesh level. Specially, we obtain a by-product that edge residuals dominate the a posteriori error in the sense of $L^{\infty}$-norm when the linear element is used. With the aid of the weighted Sobolev stability of the $L^2$-projection, we also propose a new method to prove the reliability of the a posteriori error estimator for higher order finite elements. A key ingredient in the a posteriori error analysis is some new estimates for regularized derivative Green's functions. Some numerical experiments verify our theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08259v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenglei Li, Qigang Liang, Xuejun Xu</dc:creator>
    </item>
    <item>
      <title>Numerical Approaches for Identifying the Time-Dependent Potential Coefficient in the Diffusion Equation</title>
      <link>https://arxiv.org/abs/2511.08302</link>
      <description>arXiv:2511.08302v1 Announce Type: new 
Abstract: We address the inverse problem of identifying a time-dependent potential coefficient in a one-dimensional diffusion equation subject to Dirichlet boundary conditions and a nonlocal integral overdetermination constraint reflecting spatially averaged measurements. After establishing well-posedness for the forward problem and deriving an a priori estimate that ensures uniqueness and continuous dependence on the data, we prove existence and uniqueness for the inverse problem. To compute numerically the unknown coefficient, we propose and compare three numerical methods: an integration-based scheme, a Newton-Raphson iterative solver, and a physics-informed neural network (PINN). Numerical experiments on both exact and noisy data demonstrate the accuracy, robustness, and efficiency of each approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08302v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arshyn Altybay, Michael Ruzhansky</dc:creator>
    </item>
    <item>
      <title>Fast integral methods for the Neumann Green's function: applications to capture and signaling problems in two dimensions</title>
      <link>https://arxiv.org/abs/2511.08458</link>
      <description>arXiv:2511.08458v2 Announce Type: new 
Abstract: We present a high order numerical method for the solution of the Neumann Green's function in two dimensions. For a general closed planar curve, our computational method resolves both the interior and exterior Green's functions with the source placed either in the bulk or on the surface -- yielding four distinct functions. Our method exactly represents the singular nature of the Green's function by decomposing the singular and regular components. In the case of the interior function, we exactly prescribe an integral constraint which is necessary to obtain a unique solution given the arbitrary constant solution associated with Neumann boundary conditions. Our implementation is based on a fast integral method for the regular part of the Green's function which allows for a rapid and high order discretization for general domains. We demonstrate the accuracy of our method for simple geometries such as disks and ellipses where closed form solutions are available. To exhibit the usefulness of these new routines, we demonstrate several applications to open problems in the capture of Brownian particles, specifically, how the small traps or boundary windows should be configured to maximize the capture rate of Brownian particles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08458v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sanchita Chakraborty, Jeremy Hoskins, Alan E. Lindsay</dc:creator>
    </item>
    <item>
      <title>Two Datasets Are Better Than One: Method of Double Moments for 3-D Reconstruction in Cryo-EM</title>
      <link>https://arxiv.org/abs/2511.07438</link>
      <description>arXiv:2511.07438v1 Announce Type: cross 
Abstract: Cryo-electron microscopy (cryo-EM) is a powerful imaging technique for reconstructing three-dimensional molecular structures from noisy tomographic projection images of randomly oriented particles. We introduce a new data fusion framework, termed the method of double moments (MoDM), which reconstructs molecular structures from two instances of the second-order moment of projection images obtained under distinct orientation distributions--one uniform, the other non-uniform and unknown. We prove that these moments generically uniquely determine the underlying structure, up to a global rotation and reflection, and we develop a convex-relaxation-based algorithm that achieves accurate recovery using only second-order statistics. Our results demonstrate the advantage of collecting and modeling multiple datasets under different experimental conditions, illustrating that leveraging dataset diversity can substantially enhance reconstruction quality in computational imaging tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07438v1</guid>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joe Kileel, Oscar Mickelin, Amit Singer, Sheng Xu</dc:creator>
    </item>
    <item>
      <title>Mechanistic multiphysics modeling reveals how blood pulsation drives CSF flow, pressure, and brain deformation under physiological and injection conditions</title>
      <link>https://arxiv.org/abs/2511.07705</link>
      <description>arXiv:2511.07705v1 Announce Type: cross 
Abstract: Intrathecal (IT) injection is an effective way to deliver drugs to the brain bypassing the blood-brain barrier. To evaluate and optimize IT drug delivery, it is necessary to understand the cerebrospinal fluid (CSF) dynamics in the central nervous system (CNS). In combination with experimental measurements, computational modeling plays an important role in reconstructing CSF flow in the CNS. Existing models have provided valuable insights into the CSF dynamics; however, most neglect the effects of tissue mechanics, focus on partial geometries, or rely on measured CSF flow rates under specific conditions, leaving full-CNS CSF flow field predictions across different physiological states underexplored. Here, we propose a comprehensive multiphysics computational model of the CNS with three key features: (1) it is implemented on a fully closed geometry of CNS; (2) it includes the interaction between CSF and poroelastic tissue as well as the compliant spinal dura mater; (3) it has potential for predictive simulations because it only needs data on cardiac blood pulsation into the brain. Our simulations under physiological conditions demonstrate that our model accurately reconstructs the CSF pulsation and captures both the craniocaudal attenuation and phase shift of CSF flow along the spinal subarachnoid space (SAS). When applied to the simulation of IT drug delivery, our model successfully captures the intracranial pressure (ICP) elevation during injection and subsequent recovery after injections. The proposed multiphysics model provides a unified and extensible framework that allows parametric studies of CSF flow dynamics and optimization of IT injections, serving as a strong foundation for integration of additional physiological mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07705v1</guid>
      <category>physics.med-ph</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.bio-ph</category>
      <category>physics.flu-dyn</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhuogen Li, Keyu Feng, Hector Gomez</dc:creator>
    </item>
    <item>
      <title>A control variate method based on polynomial approximation of Brownian path</title>
      <link>https://arxiv.org/abs/2511.08021</link>
      <description>arXiv:2511.08021v1 Announce Type: cross 
Abstract: We present a novel control variate technique for enhancing the efficiency of Monte Carlo (MC) estimation of expectations involving solutions to stochastic differential equations (SDEs). Our method integrates a primary fine-time-step discretization of the SDE with a control variate derived from a secondary coarse-time-step discretization driven by a piecewise parabolic approximation of Brownian motion. This approximation is conditioned on the same fine-scale Brownian increments, enabling strong coupling between the estimators. The expectation of the control variate is computed via an independent MC simulation using the coarse approximation. We characterize the minimized quadratic error decay as a function of the computational budget and the weak and strong orders of the primary and secondary discretization schemes. We demonstrate the method's effectiveness through numerical experiments on representative SDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08021v1</guid>
      <category>math.PR</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josselin Garnier, Laurent Mertz</dc:creator>
    </item>
    <item>
      <title>A Fast and Accurate Approach for Covariance Matrix Construction</title>
      <link>https://arxiv.org/abs/2511.08223</link>
      <description>arXiv:2511.08223v1 Announce Type: cross 
Abstract: Reichel (2025) defined the Bariance as $\mathrm{Bariance}(x)=\frac{1}{n(n-1)}\sum_{i&lt;j}(x_i-x_j)^2$, which admits an $O(n)$ reformulation using scalar sums. We extend this to the covariance matrix by showing that $\mathrm{Cov}(X)=\frac{1}{n-1}\!\left(X^\top X-\frac{1}{n}\,s\,s^\top\right)$ with $s=X^\top \mathbf{1}_n$ is algebraically identical to the pairwise-difference form yet avoids explicit centering. Computation reduces to a single $p\times p$ outer matrix product and one subtraction. Empirical benchmarks in Python show clear runtime gains over numpy.cov in non-BLAS-tuned settings. Faster Gram routines such as RXTX (Rybin et. al) for $XX^\top$ further reduce total cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.08223v1</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Structure-Preserving Numerical Methods for Two Nonlinear Systems of Dispersive Wave Equations</title>
      <link>https://arxiv.org/abs/2402.16669</link>
      <description>arXiv:2402.16669v2 Announce Type: replace 
Abstract: We use the general framework of summation-by-parts operators to construct conservative, energy-stable, and well-balanced semidiscretizations of two different nonlinear systems of dispersive shallow water equations with varying bathymetry: (i) a variant of the coupled Benjamin-Bona-Mahony (BBM) equations and (ii) a recently proposed model by Sv\"ard and Kalisch (2025) with enhanced dispersive behavior. Both models share the property of being conservative in terms of a nonlinear invariant, often interpreted as energy. This property is preserved exactly in our novel semidiscretizations. To obtain fully-discrete energy-stable schemes, we employ the relaxation method. Our novel methods generalize energy-conserving methods for the BBM-BBM system to variable bathymetries. Compared to the low-order, energy-dissipative finite volume method proposed by Sv\"ard and Kalisch, our schemes are arbitrary high-order accurate, energy-conservative or -stable, can deal with periodic and reflecting boundary conditions, and can be any method within the framework of summation-by-parts operators including finite difference and finite element schemes. We present improved numerical properties of our methods in some test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16669v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s44207-025-00006-3</arxiv:DOI>
      <arxiv:journal_reference>Computational Science and Engineering 2(2), 2025</arxiv:journal_reference>
      <dc:creator>Joshua Lampert, Hendrik Ranocha</dc:creator>
    </item>
    <item>
      <title>Simultaneous spatial-parametric collocation approximation for parametric PDEs with log-normal random inputs</title>
      <link>https://arxiv.org/abs/2502.07799</link>
      <description>arXiv:2502.07799v3 Announce Type: replace 
Abstract: We establish convergence rates for a fully discrete, multi-level, linear collocation method solving parametric elliptic PDEs on bounded polygonal domains with log-normal inputs. The method uses a finite set of function evaluations in the spatial-parametric domain. Compared with the best-known fully discrete collocation rates, these rates are significantly improved and, up to logarithmic factors, match the rates of best n-term approximations. The results follow from applying general multi-level linear sampling recovery theory in abstract Bochner spaces -- via extended least-squares -- to infinite-dimensional holomorphic functions. The abstract multi-level recovery in Bochner spaces guarantees yield the improved rates when specialized to the parametric PDE setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07799v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dinh D\~ung</dc:creator>
    </item>
    <item>
      <title>Scalable Signature Kernel Computations for Long Time Series via Local Neumann Series Expansions</title>
      <link>https://arxiv.org/abs/2502.20392</link>
      <description>arXiv:2502.20392v2 Announce Type: replace 
Abstract: The signature kernel is a recent state-of-the-art tool for analyzing high-dimensional sequential data, valued for its theoretical guarantees and strong empirical performance. In this paper, we present a novel method for efficiently computing the signature kernel of long, high-dimensional time series via adaptively truncated recursive local power series expansions. Building on the characterization of the signature kernel as the solution of a Goursat PDE, our approach employs tilewise Neumann-series expansions to derive rapidly converging power series approximations of the signature kernel that are locally defined on subdomains and propagated iteratively across the entire domain of the Goursat solution by exploiting the geometry of the time series. Algorithmically, this involves solving a system of interdependent Goursat PDEs via adaptively truncated local power series expansions and recursive propagation of boundary conditions along a directed graph in a topological ordering. This method strikes an effective balance between computational cost and accuracy, achieving substantial performance improvements over state-of-the-art approaches for computing the signature kernel. It offers (a) adjustable and superior accuracy, even for time series with very high roughness; (b) drastically reduced memory requirements; and (c) scalability to efficiently handle very long time series (one million data points or more) on a single GPU. As demonstrated in our benchmarks, these advantages make our method particularly well-suited for rough-path-assisted machine learning, financial modeling, and signal processing applications involving very long and highly volatile sequential data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20392v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Tamayo-Rios, Alexander Schell, Rima Alaifari</dc:creator>
    </item>
    <item>
      <title>A Machine Learning and Finite Element Framework for Inverse Elliptic PDEs via Dirichlet-to-Neumann Mapping</title>
      <link>https://arxiv.org/abs/2504.03895</link>
      <description>arXiv:2504.03895v3 Announce Type: replace 
Abstract: Inverse problems for Partial Differential Equations (PDEs) are crucial in numerous applications such as geophysics, biomedical imaging, and material science, where unknown physical properties must be inferred from indirect measurements. In this work, we present a new approach to solving the inverse problem for elliptic PDEs, using only boundary data. Our method leverages the Dirichlet-to-Neumann (DtN) map, which captures the relationship between boundary inputs and flux responses. This enables the reconstruction of the unknown physical properties within the domain from boundary measurements alone. Our framework employs a self-supervised machine learning algorithm that integrates a Finite Element Method (FEM) in the inner loop for the forward problem, ensuring high accuracy. Moreover, our approach illustrates its effectiveness in challenging scenarios with only partial boundary observations, which is often the case in real-world scenarios. In addition, the proposed algorithm effectively handles discontinuities by incorporating carefully designed loss functions. This combined FEM and machine learning approach offers a robust, accurate solution strategy for a broad range of inverse problems, enabling improved estimation of critical parameters in applications from medical diagnostics to subsurface exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03895v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dabin Park, Sanghyun Lee, Sunghwan Moon</dc:creator>
    </item>
    <item>
      <title>An initial-boundary corrected splitting method for diffusion-reaction problems</title>
      <link>https://arxiv.org/abs/2504.10125</link>
      <description>arXiv:2504.10125v2 Announce Type: replace 
Abstract: Strang splitting is a widely used second-order method for solving diffusion-reaction problems. However, its convergence order is often reduced to order $1$ for Dirichlet boundary conditions and to order $1.5$ for Neumann and Robin boundary conditions, leading to lower accuracy and reduced efficiency. In this paper, we consider a new splitting approach, called an initial-boundary corrected splitting, which avoids order reduction while improving computational efficiency for a wider range of applications. In contrast to the corrections proposed in the literature, it does not require the computation of correction terms that depend on the boundary conditions and boundary data. Through rigorous analytical convergence analysis and numerical experiments, we demonstrate the improved accuracy and performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10125v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thi Tam Dang, Lukas Einkemmer, Alexander Ostermann</dc:creator>
    </item>
    <item>
      <title>Dual-grid parameter choice method with application to image deblurring</title>
      <link>https://arxiv.org/abs/2504.10259</link>
      <description>arXiv:2504.10259v2 Announce Type: replace 
Abstract: Variational regularization of ill-posed inverse problems is based on minimizing the sum of a data fidelity term and a regularization term. The balance between them is tuned using a positive regularization parameter, whose automatic choice remains an open question in general. A novel approach for parameter choice is introduced, based on the use of two slightly different computational models for the same inverse problem. Small parameter values should give two very different reconstructions due to amplification of noise. Large parameter values lead to two identical but trivial reconstructions. Optimal parameter is chosen between the extremes by matching image similarity of the two reconstructions with a pre-defined value. Efficacy of the new method is demonstrated with image deblurring using measured data and two different regularizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10259v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3934/ammc.2025011</arxiv:DOI>
      <dc:creator>Markus Juvonen, Bj{\o}rn Jensen, Ilmari Pohjola, Yiqiu Dong, Samuli Siltanen</dc:creator>
    </item>
    <item>
      <title>A Taylor-Hood finite element method for the surface Stokes problem without penalization</title>
      <link>https://arxiv.org/abs/2506.20419</link>
      <description>arXiv:2506.20419v2 Announce Type: replace 
Abstract: Finite element approximation of the velocity-pressure formulation of the surfaces Stokes equations is challenging because it is typically not possible to enforce both tangentiality and $H^1$ conformity of the velocity field. Most previous works concerning finite element methods (FEMs) for these equations thus have weakly enforced one of these two constraints by penalization or a Lagrange multiplier formulation. Recently in [A tangential and penalty-free finite element method for the surface Stokes problem, SINUM 62(1):248-272, 2024], the authors constructed a surface Stokes FEM based on the MINI element which is tangentiality conforming and $H^1$ nonconforming, but possesses sufficient weak continuity properties to circumvent the need for penalization. The key to this method is construction of velocity degrees of freedom lying on element edges and vertices using an auxiliary Piola transform. In this work we extend this methodology to construct Taylor-Hood surface FEMs. The resulting method is shown to achieve optimal-order convergence when the edge degrees of freedom for the velocity space are placed at Gauss-Lobatto nodes. Numerical experiments confirm that this nonstandard placement of nodes is necessary to achieve optimal convergence orders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20419v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Demlow, Michael Neilan</dc:creator>
    </item>
    <item>
      <title>Physics-informed deep learning and compressive collocation for high-dimensional diffusion-reaction equations: practical existence theory and numerics</title>
      <link>https://arxiv.org/abs/2406.01539</link>
      <description>arXiv:2406.01539v3 Announce Type: replace-cross 
Abstract: On the forefront of scientific computing, Deep Learning (DL), i.e., machine learning with Deep Neural Networks (DNNs), has emerged a powerful new tool for solving Partial Differential Equations (PDEs). It has been observed that DNNs are particularly well suited to weakening the effect of the curse of dimensionality, a term coined by Richard E. Bellman in the late `50s to describe challenges such as the exponential dependence of the sample complexity, i.e., the number of samples required to solve an approximation problem, on the dimension of the ambient space. However, although DNNs have been used to solve PDEs since the `90s, the literature underpinning their mathematical efficiency in terms of numerical analysis (i.e., stability, accuracy, and sample complexity), is only recently beginning to emerge. In this paper, we leverage recent advancements in function approximation using sparsity-based techniques and random sampling to develop and analyze an efficient high-dimensional PDE solver based on DL. We show, both theoretically and numerically, that it can compete with a novel stable and accurate compressive spectral collocation method for the solution of high-dimensional, steady-state diffusion-reaction equations with periodic boundary conditions. In particular, we demonstrate a new practical existence theorem, which establishes the existence of a class of trainable DNNs with suitable bounds on the network architecture and a sufficient condition on the sample complexity, with logarithmic or, at worst, linear scaling in dimension, such that the resulting networks stably and accurately approximate a diffusion-reaction PDE with high probability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01539v3</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simone Brugiapaglia, Nick Dexter, Samir Karam, Weiqi Wang</dc:creator>
    </item>
    <item>
      <title>GDNSQ: Gradual Differentiable Noise Scale Quantization for Low-bit Neural Networks</title>
      <link>https://arxiv.org/abs/2508.14004</link>
      <description>arXiv:2508.14004v2 Announce Type: replace-cross 
Abstract: Quantized neural networks can be viewed as a chain of noisy channels, where rounding in each layer reduces capacity as bit-width shrinks; the floating-point (FP) checkpoint sets the maximum input rate. We track capacity dynamics as the average bit-width decreases and identify resulting quantization bottlenecks by casting fine-tuning as a smooth, constrained optimization problem. Our approach employs a fully differentiable Straight-Through Estimator (STE) with learnable bit-width, noise scale and clamp bounds, and enforces a target bit-width via an exterior-point penalty; mild metric smoothing (via distillation) stabilizes training. Despite its simplicity, the method attains competitive accuracy down to the extreme W1A1 setting while retaining the efficiency of STE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14004v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NA</category>
      <category>math.IT</category>
      <category>math.NA</category>
      <pubDate>Wed, 12 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sergey Salishev, Ian Akhremchik</dc:creator>
    </item>
  </channel>
</rss>
