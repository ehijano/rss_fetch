<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Nov 2025 05:01:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Use of multi-pollutant air sensor data and geometric non-negative matrix factorization for source apportionment of air pollution burden in Curtis Bay, Baltimore, USA</title>
      <link>https://arxiv.org/abs/2511.11833</link>
      <description>arXiv:2511.11833v1 Announce Type: new 
Abstract: Air sensor networks provide hyperlocal, high temporal resolution data on multiple pollutants that can support credible identification of common pollution sources. Source apportionment using least squares-based non-negative matrix factorization is non-unique and often does not scale. A recent geometric source apportionment framework focuses inference on the source attribution matrix, which is shown to remain identifiable even when the factorization is not. Recognizing that the method scales with and benefits from large data volumes, we use this geometric method to analyze 451,946 one-minute air sensor records from Curtis Bay (Baltimore, USA), collected from October 21, 2022 to June 16, 2023, covering size-resolved particulate matter (PM), black carbon (BC), carbon monoxide (CO), nitric oxide (NO), and nitrogen dioxide (NO2). The analysis identifies three stable sources. Source 1 explains &gt; 70% of fine and coarse PM and ~30% of BC. Source 2 dominates CO and contributes ~70% of BC, NO, and NO2. Source 3 is specific to the larger PM fractions, PM10 to PM40. Regression analyses show Source 1 and Source 3 rise during bulldozer activity at a nearby coal terminal and under winds from the terminal, indicating a direct coal terminal influence, while Source 2 exhibits diurnal patterns consistent with traffic. A case-study on the day with a known bulldozer incident at the coal terminal further confirms the association of terminal activities with Sources 1 and 3. The results are stable under sensitivity analyses. The analysis demonstrates that geometric source apportionment, paired with high temporal resolution data from multi-pollutant air sensor networks, delivers scalable and reliable evidence to inform mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11833v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bora Jin, Bonita D. Salmer\'on, David McClosky, David H. Hagan, Russell R. Dickerson, Nicholas J. Spada, Lauren N. Deanes, Matthew A. Aubourg, Laura E. Schmidt, Gregory G. Sawtell, Christopher D. Heaney, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Violent event-related fatality patterns in Ethiopia: a Bayesian spatiotemporal perspective</title>
      <link>https://arxiv.org/abs/2511.12219</link>
      <description>arXiv:2511.12219v1 Announce Type: new 
Abstract: Fatalities resulting from violence in armed conflict have long been a significant public health issue in Ethiopia. Despite the severity of this problem, more comprehensive quantitative scientific studies need to be conducted to elucidate the sequence and dynamics of these occurrences. In response, this study introduces a spatio-temporal statistical method designed to uncover the patterns of fatalities associated with violent events in Ethiopia. The research employs a two-part zero-inflated Bayesian generalized additive mixed model, which integrates a spatio-temporal component to map the fatality patterns across Ethiopian regions. The dataset utilized originates from the Armed Conflict Location and Event Data Project, covering fatality counts related to violent events from 1997 to 2022. The analysis revealed that nine out of thirteen administrative regions exhibited a probability greater than 0.6 for fatality occurrence due to violent events, with five regions surpassing a 0.7 probability threshold. These five regions include Benishangul Gumz, Gambela, Oromia, Somali, and the South West Ethiopian People's Region. Notably, the Tigray region displayed the highest probability (0.558) of experiencing more than 20 deaths per violent event, followed by the Benishangul Gumz region with a probability of 0.306. Encouragingly, the findings also indicate an average decline in fatalities per violent event over time. Specifically, the probability of more than 20 deaths per event was 0.401 in 2020, which decreased to 0.148 by 2022. These insights are invaluable for the government, policymakers, political leaders, and traditional or religious authorities in Ethiopia, enabling them to make informed, strategic decisions to mitigate and ultimately prevent violence-related fatalities in the country.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12219v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Osafu Augustine Egbon, Asrat Mekonnen Belachew, Ezra Gayawan, Francisco Louzada</dc:creator>
    </item>
    <item>
      <title>A Review of Statistical and Machine Learning Approaches for Coral Bleaching Assessment</title>
      <link>https://arxiv.org/abs/2511.12234</link>
      <description>arXiv:2511.12234v1 Announce Type: new 
Abstract: Coral bleaching is a major concern for marine ecosystems; more than half of the world's coral reefs have either bleached or died over the past three decades. Increasing sea surface temperatures, along with various spatiotemporal environmental factors, are considered the primary reasons behind coral bleaching. The statistical and machine learning communities have focused on multiple aspects of the environment in detail. However, the literature on various stochastic modeling approaches for assessing coral bleaching is extremely scarce. Data-driven strategies are crucial for effective reef management, and this review article provides an overview of existing statistical and machine learning methods for assessing coral bleaching. Statistical frameworks, including simple regression models, generalized linear models, generalized additive models, Bayesian regression models, spatiotemporal models, and resilience indicators, such as Fisher's Information and Variance Index, are commonly used to explore how different environmental stressors influence coral bleaching. On the other hand, machine learning methods, including random forests, decision trees, support vector machines, and spatial operators, are more popular for detecting nonlinear relationships, analyzing high-dimensional data, and allowing integration of heterogeneous data from diverse sources. In addition to summarizing these models, we also discuss potential data-driven future research directions, with a focus on constructing statistical and machine learning models in specific contexts related to coral bleaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12234v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Sarkar, Arnab Hazra</dc:creator>
    </item>
    <item>
      <title>Stochastic Predictive Analytics for Stocks in the Newsvendor Problem</title>
      <link>https://arxiv.org/abs/2511.12397</link>
      <description>arXiv:2511.12397v1 Announce Type: new 
Abstract: This work addresses a key challenge in inventory management by developing a stochastic model that describes the dynamic distribution of inventory stock over time without assuming a specific demand distribution. Our model provides a flexible and applicable solution for situations with limited historical data and short-term predictions, making it well-suited for the Newsvendor problem. We evaluate our model's performance using real-world data from a large electronic marketplace, demonstrating its effectiveness in a practical forecasting scenario.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12397v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Pedro A. Pury</dc:creator>
    </item>
    <item>
      <title>Do Nineteenth-Century Graphics Still Work for Today's Readers?</title>
      <link>https://arxiv.org/abs/2511.12510</link>
      <description>arXiv:2511.12510v1 Announce Type: new 
Abstract: Do nineteenth-century graphics still work for today's readers? To investigate this question, we conducted a controlled experiment evaluating three canonical historical visualizations- Nightingale's polar area diagram, Playfair's trade balance chart, and Minard's campaign map-against modern redesigns. Fifty-four participants completed structured question-answering tasks, allowing us to measure accuracy, response time, and perceived workload (NASA-TLX). We used mixed-effects regression models to find: Nightingale's diagram remained consistently effective across versions, achieving near-ceiling accuracy and low workload; Playfair's dual-axis redesign underperformed relative to both its historical and alternative versions; and Minard's map showed large accuracy gains under redesign but continued to impose high workload and long response times. These results demonstrate that some nineteenth-century designs remain effective, others degrade under certain modernizations, and some benefit from careful redesign. The findings indicate how perceptual encoding choices, task alignment, and cognitive load determine whether historical charts survive or require adaptation for contemporary use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12510v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingke He</dc:creator>
    </item>
    <item>
      <title>A spatio-temporal statistical model for property valuation at country-scale with adjustments for regional submarkets</title>
      <link>https://arxiv.org/abs/2511.12625</link>
      <description>arXiv:2511.12625v1 Announce Type: new 
Abstract: Valuing residential property is inherently complex, requiring consideration of numerous environmental, economic, and property-specific factors. These complexities present significant challenges for automated valuation models (AVMs), which are increasingly used to provide objective assessments for property taxation and mortgage financing. The challenge of obtaining accurate and objective valuations for properties at a country level, and not just within major cities, is further compounded by the presence of multiple localised submarkets-spanning urban, suburban, and rural contexts-where property features contribute differently to value. Existing AVMs often struggle in such settings: traditional hedonic regression models lack the flexibility to capture spatial variation, while advanced machine learning approaches demand extensive datasets that are rarely available. In this article, we address these limitations by developing a robust statistical framework for property valuation in the Irish housing market. We segment the country into six submarkets encompassing cities, large towns, and rural areas, and employ a generalized additive model that captures non-linear effects of property characteristics while allowing feature contributions to vary across submarkets. Our approach outperforms both machine learning-based and traditional hedonic regression models, particularly in data-sparse regions. In out-of-sample validation, our model achieves R-squared values of 0.70, 0.84, and 0.83 for rural areas, towns, and Dublin, respectively, compared to 0.52, 0.71, and 0.82 from a random forest benchmark. Furthermore, the temporal dynamics of our model align closely with reported inflation figures for the study period, providing additional validation of its accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12625v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brian O'Donovan, Andrew Finley, James Sweeney</dc:creator>
    </item>
    <item>
      <title>Change-Point Detection Utilizing Normalized Entropy as a Fundamental Metric</title>
      <link>https://arxiv.org/abs/2511.12703</link>
      <description>arXiv:2511.12703v1 Announce Type: new 
Abstract: This paper introduces a concept for change-point detection based on normalized entropy as a fundamental metric, aiming to overcome the dependence of traditional entropy methods on assumptions about data distribution and absolute scales. Normalized entropy maps entropy values to the [0,1] interval through standardization, accurately capturing relative changes in data complexity. By utilizing a sliding window to compute normalized entropy, this approach transforms the challenge of detecting change points in complex time series, arising from variations in scale, distribution, and diversity, into the task of identifying significant features within the normalized entropy sequence, thereby avoiding interference from parametric assumptions and effectively highlighting distributional shifts. Experimental results show that normalized entropy exhibits significant numerical fluctuation characteristics and patterns near change points across various distributions and parameter combinations. The average deviation between fluctuation moments and actual change points is only 2.4% of the sliding window size, demonstrating strong adaptability. This paper provides theoretical support for change-point detection in complex data environments and lays a methodological foundation for precise and automated detection based on normalized entropy as a fundamental metric.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12703v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 17th International Conference on Pattern Recognition and Information Processing (PRIP 2025), pp. 288-291, 2025</arxiv:journal_reference>
      <dc:creator>Qingqing Song, Shaoliang Xia</dc:creator>
    </item>
    <item>
      <title>Scalable Vision-Guided Crop Yield Estimation</title>
      <link>https://arxiv.org/abs/2511.12999</link>
      <description>arXiv:2511.12999v1 Announce Type: new 
Abstract: Precise estimation and uncertainty quantification for average crop yields are critical for agricultural monitoring and decision making. Existing data collection methods, such as crop cuts in randomly sampled fields at harvest time, are relatively time-consuming. Thus, we propose an approach based on prediction-powered inference (PPI) to supplement these crop cuts with less time-consuming field photos. After training a computer vision model to predict the ground truth crop cut yields from the photos, we learn a ``control function" that recalibrates these predictions with the spatial coordinates of each field. This enables fields with photos but not crop cuts to be leveraged to improve the precision of zone-wide average yield estimates. Our control function is learned by training on a dataset of nearly 20,000 real crop cuts and photos of rice and maize fields in sub-Saharan Africa. To improve precision, we pool training observations across different zones within the same first-level subdivision of each country. Our final PPI-based point estimates of the average yield are provably asymptotically unbiased and cannot increase the asymptotic variance beyond that of the natural baseline estimator -- the sample average of the crop cuts -- as the number of fields grows. We also propose a novel bias-corrected and accelerated (BCa) bootstrap to construct accompanying confidence intervals. Even in zones with as few as 20 fields, the point estimates show significant empirical improvement over the baseline, increasing the effective sample size by as much as 73% for rice and by 12-23% for maize. The confidence intervals are accordingly shorter at minimal cost to empirical finite-sample coverage. This demonstrates the potential for relatively low-cost images to make area-based crop insurance more affordable and thus spur investment into sustainable agricultural practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12999v1</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison H. Li, Medhanie Irgau, Nabil Janmohamed, Karen Solveig Rieckmann, David B. Lobell</dc:creator>
    </item>
    <item>
      <title>TacEleven: generative tactic discovery for football open play</title>
      <link>https://arxiv.org/abs/2511.13326</link>
      <description>arXiv:2511.13326v1 Announce Type: new 
Abstract: Creating offensive advantages during open play is fundamental to football success. However, due to the highly dynamic and long-sequence nature of open play, the potential tactic space grows exponentially as the sequence progresses, making automated tactic discovery extremely challenging. To address this, we propose TacEleven, a generative framework for football open-play tactic discovery developed in close collaboration with domain experts from AJ Auxerre, designed to assist coaches and analysts in tactical decision-making. TacEleven consists of two core components: a language-controlled tactical generator that produces diverse tactical proposals, and a multimodal large language model-based tactical critic that selects the optimal proposal aligned with a high-level stylistic tactical instruction. The two components enables rapid exploration of tactical proposals and discovery of alternative open-play offensive tactics. We evaluate TacEleven across three tasks with progressive tactical complexity: counterfactual exploration, single-step discovery, and multi-step discovery, through both quantitative metrics and a questionnaire-based qualitative assessment. The results show that the TacEleven-discovered tactics exhibit strong realism and tactical creativity, with 52.50% of the multi-step tactical alternatives rated adoptable in real-world elite football scenarios, highlighting the framework's ability to rapidly generate numerous high-quality tactics for complex long-sequence open-play situations. TacEleven demonstrates the potential of creatively leveraging domain data and generative models to advance tactical analysis in sports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13326v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyao Zhao, Hao Ma, Zhiqiang Pu, Jingjing Huang, Yi Pan, Zhi Ming</dc:creator>
    </item>
    <item>
      <title>Variance Stabilizing Transformations for Electricity Price Forecasting in Periods of Increased Volatility</title>
      <link>https://arxiv.org/abs/2511.13603</link>
      <description>arXiv:2511.13603v1 Announce Type: new 
Abstract: Accurate day-ahead electricity price forecasts are critical for power system operation and market participation, yet growing renewable penetration and recent crises have caused unprecedented volatility that challenges standard models. This paper revisits variance-stabilizing transformations (VSTs) as a preprocessing tool by introducing a novel parametrization of the asinh transformation, systematically analyzing parameter sensitivity and calibration window size, and explicitly testing performance under volatile market regimes. Using data from Germany, Spain, and France over 2015-2024 with two model classes (NARX and LEAR), we show that VSTs substantially reduce forecast errors, with gains of up to 14.6% for LEAR and 8.7% for NARX relative to untransformed benchmarks. The new parametrized asinh consistently outperforms its standard form, while rolling averaging across transformations delivers the most robust improvements, reducing errors by up to 17.7%. Results demonstrate that VSTs are especially valuable in volatile regimes, making them a powerful tool for enhancing electricity price forecasting in today's power markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13603v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bartosz Uniejewski</dc:creator>
    </item>
    <item>
      <title>Phase I Distribution-Free Control Charts for Individual Observations Using Runs and Patterns</title>
      <link>https://arxiv.org/abs/2511.13672</link>
      <description>arXiv:2511.13672v1 Announce Type: new 
Abstract: Phase I distribution-free runs- and patterns-type control charts are proposed for monitoring the unknown target value (or location parameter) for both continuous and discrete individual observations. Our approach maintains the nominal in-control signal probability at a prescribed level by employing the finite Markov chain imbedding technique combined with random permutation and conditioning arguments. To elucidate the methodology, we examine two popular runs- and patterns-type statistics: the number of success runs and the scan statistic. Numerical results indicate that the performance of our proposed control charts is comparable to that of existing Phase I nonparametric control charts for individual observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13672v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tung-Lung Wu</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines</title>
      <link>https://arxiv.org/abs/2511.11613</link>
      <description>arXiv:2511.11613v1 Announce Type: cross 
Abstract: Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11613v1</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pouya Taraghi, Yong Li, Samer Adeeb</dc:creator>
    </item>
    <item>
      <title>A Bayesian Model for Multi-stage Censoring</title>
      <link>https://arxiv.org/abs/2511.11684</link>
      <description>arXiv:2511.11684v1 Announce Type: cross 
Abstract: Many sequential decision settings in healthcare feature funnel structures characterized by a series of stages, such as screenings or evaluations, where the number of patients who advance to each stage progressively decreases and decisions become increasingly costly. For example, an oncologist may first conduct a breast exam, followed by a mammogram for patients with concerning exams, followed by a biopsy for patients with concerning mammograms. A key challenge is that the ground truth outcome, such as the biopsy result, is only revealed at the end of this funnel. The selective censoring of the ground truth can introduce statistical biases in risk estimation, especially in underserved patient groups, whose outcomes are more frequently censored. We develop a Bayesian model for funnel decision structures, drawing from prior work on selective labels and censoring. We first show in synthetic settings that our model is able to recover the true parameters and predict outcomes for censored patients more accurately than baselines. We then apply our model to a dataset of emergency department visits, where in-hospital mortality is observed only for those who are admitted to either the hospital or ICU. We find that there are gender-based differences in hospital and ICU admissions. In particular, our model estimates that the mortality risk threshold to admit women to the ICU is higher for women (5.1%) than for men (4.5%).</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11684v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuvom Sadhuka, Sophia Lin, Emma Pierson, Bonnie Berger</dc:creator>
    </item>
    <item>
      <title>Group Identification and Variable Selection in Multivariable Mendelian Randomization with Highly-Correlated Exposures</title>
      <link>https://arxiv.org/abs/2511.12375</link>
      <description>arXiv:2511.12375v1 Announce Type: cross 
Abstract: Multivariable Mendelian Randomization (MVMR) estimates the direct causal effects of multiple risk factors on an outcome using genetic variants as instruments. The growing availability of summary-level genetic data has created opportunities to apply MVMR in high-dimensional settings with many strongly correlated candidate risk factors. However, existing methods face three major limitations: weak instrument bias, limited interpretability, and the absence of valid post-selection inference. Here we introduce MVMR-PACS, a method that identifies signal-groups -- sets of causal risk factors with high genetic correlation or indistinguishable causal effects -- and estimates the direct effect of each group. MVMR-PACS minimizes a debiased objective function that reduces weak instrument bias while yielding interpretable estimates with theoretical guarantees for variable selection. We adapt a data-thinning strategy to summary-data MVMR to enable valid post-selection inference. In simulations, MVMR-PACS outperforms existing approaches in both estimation accuracy and variable selection. When applied to 27 lipoprotein subfraction traits and coronary artery disease risk, MVMR-PACS identifies biologically meaningful and robust signal-groups with interpretable direct causal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12375v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yinxiang Wu, Neil M. Davies, Ting Ye</dc:creator>
    </item>
    <item>
      <title>The Probabilistic Foundations of Surveillance Failure: From False Alerts to Structural Bias</title>
      <link>https://arxiv.org/abs/2511.12459</link>
      <description>arXiv:2511.12459v1 Announce Type: cross 
Abstract: For decades, forensic statisticians have debated whether searching large DNA databases undermines the evidential value of a match. Modern surveillance faces an exponentially harder problem: screening populations across thousands of attributes using threshold rules rather than exact matching. Intuition suggests that requiring many coincidental matches should make false alerts astronomically unlikely. This intuition fails.
  Consider a system that monitors 1,000 attributes, each with a 0.5 percent innocent match rate. Matching 15 pre-specified attributes has probability \(10^{-35}\), one in 30 decillion, effectively impossible. But operational systems require no such specificity. They might flag anyone who matches \emph{any} 15 of the 1,000. In a city of one million innocent people, this produces about 226 false alerts. A seemingly impossible event becomes all but guaranteed. This is not an implementation flaw but a mathematical consequence of high-dimensional screening.
  We identify fundamental probabilistic limits on screening reliability. Systems undergo sharp transitions from reliable to unreliable with small increases in data scale, a fragility worsened by data growth and correlations. As data accumulate and correlation collapses effective dimensionality, systems enter regimes where alerts lose evidential value even when individual coincidences remain vanishingly rare. This framework reframes the DNA database controversy as a shift between operational regimes. Unequal surveillance exposures magnify failure, making ``structural bias'' mathematically inevitable. These limits are structural: beyond a critical scale, failure cannot be prevented through threshold adjustment or algorithmic refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12459v1</guid>
      <category>stat.ME</category>
      <category>cs.CY</category>
      <category>math.PR</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Pollanen</dc:creator>
    </item>
    <item>
      <title>Modeling group heterogeneity in spatio-temporal data via physics-informed semiparametric regression</title>
      <link>https://arxiv.org/abs/2511.13203</link>
      <description>arXiv:2511.13203v1 Announce Type: cross 
Abstract: In this work we propose a novel approach for modeling spatio-temporal data characterized by group structures. In particular, we extend classical mixed effect regression models by introducing a space-time nonparametric component, regularized through a partial differential equation, to embed the physical dynamics of the underlying process, while random effects capture latent variability associated with the group structure present in the data. We propose a two-step procedure to estimate the fixed and random components of the model, relying on a functional version of the Iterative Reweighted Least Squares algorithm. We investigate the asymptotic properties of both fixed and random components, and we assess the performance of the proposed model through a simulation study, comparing it with state-of-the-art alternatives from the literature. The proposed methodology is finally applied to the study of hourly nitrogen dioxide concentration data in Lombardy (Italy), using random effects to account for measurement heterogeneity across monitoring stations equipped with different sensor technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13203v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco F. De Sanctis, Eleonora Arnone, Francesca Ieva, Laura M. Sangalli</dc:creator>
    </item>
    <item>
      <title>The Shape of Data: Topology Meets Analytics. A Practical Introduction to Topological Analytics and the Stability Index (TSI) in Business</title>
      <link>https://arxiv.org/abs/2511.13503</link>
      <description>arXiv:2511.13503v1 Announce Type: cross 
Abstract: Modern business and economic datasets often exhibit nonlinear, multi-scale structures that traditional linear tools under-represent. Topological Data Analysis (TDA) offers a geometric lens for uncovering robust patterns, such as connected components, loops and voids, across scales. This paper provides an intuitive, figure-driven introduction to persistent homology and a practical, reproducible TDA pipeline for applied analysts. Through comparative case studies in consumer behavior, equity markets (SAX/eSAX vs.\ TDA) and foreign exchange dynamics, we demonstrate how topological features can reveal segmentation patterns and structural relationships beyond classical statistical methods. We discuss methodological choices regarding distance metrics, complex construction and interpretation, and we introduce the \textit{Topological Stability Index} (TSI), a simple yet interpretable indicator of structural variability derived from persistence lifetimes. We conclude with practical guidelines for TDA implementation, visualization and communication in business and economic analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13503v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Diamantis</dc:creator>
    </item>
    <item>
      <title>Making Evidence Actionable in Adaptive Learning Closing the Diagnostic Pedagogical Loop</title>
      <link>https://arxiv.org/abs/2511.13542</link>
      <description>arXiv:2511.13542v1 Announce Type: cross 
Abstract: Adaptive learning often diagnoses precisely yet intervenes weakly, producing help that is mistimed or misaligned. This study presents evidence supporting an instructor-governed feedback loop that converts concept-level assessment evidence into vetted microinterventions. The adaptive learning algorithm includes three safeguards: adequacy as a hard guarantee of gap closure, attention as a budgeted limit for time and redundancy, and diversity as protection against overfitting to a single resource. We formulate intervention assignment as a binary integer program with constraints for coverage, time, difficulty windows derived from ability estimates, prerequisites encoded by a concept matrix, and anti-redundancy with diversity. Greedy selection serves low-richness and tight-latency settings, gradient-based relaxation serves rich repositories, and a hybrid switches along a richness-latency frontier. In simulation and in an introductory physics deployment with 1204 students, both solvers achieved full skill coverage for nearly all learners within bounded watch time. The gradient-based method reduced redundant coverage by about 12 percentage points relative to greedy and produced more consistent difficulty alignment, while greedy delivered comparable adequacy at lower computational cost in resource-scarce environments. Slack variables localized missing content and guided targeted curation, sustaining sufficiency across student subgroups. The result is a tractable and auditable controller that closes the diagnostic pedagogical loop and enables equitable, load-aware personalization at the classroom scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13542v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirreza Mehrabi, Jason Wade Morphew, Breejha Quezada, N. Sanjay Rebello</dc:creator>
    </item>
    <item>
      <title>Rate-optimal and computationally efficient nonparametric estimation on the circle and the sphere</title>
      <link>https://arxiv.org/abs/2511.13664</link>
      <description>arXiv:2511.13664v1 Announce Type: cross 
Abstract: We investigate the problem of density estimation on the unit circle and the unit sphere from a computational perspective. Our primary goal is to develop new density estimators that are both rate-optimal and computationally efficient for direct implementation. After establishing these estimators, we derive closed-form expressions for probability estimates over regions of the circle and the sphere. Then, the proposed theories are supported by extensive simulation studies. The considered settings naturally arise when analyzing phenomena on the Earth's surface or in the sky (sphere), as well as directional or periodic phenomena (circle). The proposed approaches are broadly applicable, and we illustrate their usefulness through case studies in zoology, climatology, geophysics, and astronomy, which may be of independent interest. The methodologies developed here can be readily applied across a wide range of scientific domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13664v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Athanasios G. Georgiadis, Andrew P. Percival</dc:creator>
    </item>
    <item>
      <title>Forecasting Mortality Rates: A Linear Mixed-Effects Model</title>
      <link>https://arxiv.org/abs/2311.18668</link>
      <description>arXiv:2311.18668v2 Announce Type: replace 
Abstract: A linear mixed-effects (LME) model is proposed for modelling and forecasting single and multi-population age-specific death rates (ASDRs). The innovative approach that we take in this study treats age, the interaction between gender and age, their interactions with predictors, and cohort as fixed effects. Furthermore, we incorporate additional random effects to account for variations in the intercept, predictor coefficients, and cohort effects among different age groups of females and males across various countries. In the single-population case, we will see how the random effects of intercept and slope change over different age groups. We will show that the LME model is identifiable. Using simulating parameter uncertainty in the LME model, we will calculate 95% uncertainty intervals for death rate forecasts. We will use data from the Human Mortality Database (HMD) to illustrate the procedure. We assess the predictive performance of the LME model in comparison to the Lee-Carter (LC) models fitted to individual populations. Additionally, we evaluate the predictive accuracy of the LME model relative to the Li-Lee (LL) model. Our results indicate that the LME model provides a more precise representation of observed mortality rates within the HMD, demonstrates robustness in calibration rate selection, and exhibits superior performance when contrasted with the LC and LL models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18668v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Dastranj, Martin Kolar</dc:creator>
    </item>
    <item>
      <title>Mortality Forecasting with Generalized Additive Mixed Models</title>
      <link>https://arxiv.org/abs/2311.18698</link>
      <description>arXiv:2311.18698v2 Announce Type: replace 
Abstract: This study introduces a novel generalized additive mixed model (GAMM) for mortality modelling, utilizing the mortality covariate $k_t$ as proposed by Dastranj-Kolar. Our findings indicate that the GAMM effectively addresses this shortcoming. Given that ASDRs constitute longitudinal data, as noted in the LME framework, the GAMM offers a more flexible and suitable approach for both modeling and forecasting mortality rates. Empirical evaluations using data from the Human Mortality Database (HMD) demonstrate the GAMM's strong ability to reproduce observed mortality patterns with high precision. Comparative analyses show that the GAMM consistently outperforms the LL model in both in-sample fit and out-of-sample forecasting across multiple populations. These results highlight the GAMM's potential as a robust and reliable tool for mortality modeling and long-term demographic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18698v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Reza Dastranj, Martin Kolar</dc:creator>
    </item>
    <item>
      <title>Sequential Federated Analysis of Early Outbreak Data Applied to Incubation Period Estimation</title>
      <link>https://arxiv.org/abs/2404.14895</link>
      <description>arXiv:2404.14895v3 Announce Type: replace 
Abstract: Early outbreak data analysis is critical for informing about their potential impact and interventions. However, data obtained early in outbreaks are often sensitive and subject to strict privacy restrictions. Thus, federated analysis, which implies decentralised collaborative analysis where no raw data sharing is required, emerged as an attractive paradigm to solve issues around data privacy and confidentiality. In the present study, we propose two approaches which require neither data sharing nor direct communication between devices/servers. The first approach approximates the joint posterior distributions via a multivariate normal distribution and uses this information to update prior distributions sequentially. The second approach uses summaries from parameters' posteriors obtained locally at different locations (sites) to perform a meta-analysis via a hierarchical model. We test these models on simulated and on real outbreak data to estimate the incubation period of multiple infectious diseases. Results indicate that both approaches can recover incubation period parameters accurately, but they present different inferential advantages. While the approximation approach permits to work with full posterior distributions, thus providing a better quantification of uncertainty; the meta-analysis approach allows for an explicit hierarchical structure, which can make some parameters more interpretable. We provide a framework for federated analysis of early outbreak data where the public health contexts are complex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14895v3</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Simon Busch-Moreno, Moritz U. G. Kraemer</dc:creator>
    </item>
    <item>
      <title>Seeding with Differentially Private Network Information</title>
      <link>https://arxiv.org/abs/2305.16590</link>
      <description>arXiv:2305.16590v5 Announce Type: replace-cross 
Abstract: In public health interventions such as distributing preexposure prophylaxis (PrEP) for HIV prevention, decision makers often use seeding algorithms to identify key individuals who can amplify intervention impact. However, building a complete sexual activity network is typically infeasible due to privacy concerns. Instead, contact tracing can provide influence samples, observed sequences of sexual contacts, without full network reconstruction. This raises two challenges: protecting individual privacy in these samples and adapting seeding algorithms to incomplete data. We study differential privacy guarantees for influence maximization when the input consists of randomly collected cascades. Building on recent advances in costly seeding, we propose privacy-preserving algorithms that introduce randomization in data or outputs and bound the privacy loss of each node. Theoretical analysis and simulations on synthetic and real-world sexual contact data show that performance degrades gracefully as privacy budgets tighten, with central privacy regimes achieving better trade-offs than local ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16590v5</guid>
      <category>cs.SI</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxin Liu, M. Amin Rahimian, Fang-Yi Yu</dc:creator>
    </item>
    <item>
      <title>Regularised Canonical Correlation Analysis: graphical lasso, biplots and beyond</title>
      <link>https://arxiv.org/abs/2403.02979</link>
      <description>arXiv:2403.02979v2 Announce Type: replace-cross 
Abstract: Recent developments in regularized Canonical Correlation Analysis (CCA) promise powerful methods for high-dimensional, multiview data analysis. However, justifying the structural assumptions behind many popular approaches remains a challenge, and features of realistic biological datasets pose practical difficulties that are seldom discussed. We propose a novel CCA estimator rooted in an assumption of conditional independencies and based on the Graphical Lasso. Our method has desirable theoretical guarantees and good empirical performance, demonstrated through extensive simulations and real-world biological datasets. Recognizing the difficulties of model selection in high dimensions and other practical challenges of applying CCA in real-world settings, we introduce a novel framework for evaluating and interpreting regularized CCA models in the context of Exploratory Data Analysis (EDA), which we hope will empower researchers and pave the way for wider adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02979v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lennie Wells, Kumar Thurimella, Sergio Bacallado</dc:creator>
    </item>
    <item>
      <title>DeepMIDE: A Multi-Output Spatio-Temporal Method for Ultra-Scale Offshore Wind Energy Forecasting</title>
      <link>https://arxiv.org/abs/2410.20166</link>
      <description>arXiv:2410.20166v2 Announce Type: replace-cross 
Abstract: To unlock access to stronger winds, the offshore wind industry is advancing towards significantly larger and taller wind turbines. This massive upscaling motivates a departure from wind forecasting methods that traditionally focused on a single representative height. To fill this gap, we propose DeepMIDE--a statistical deep learning method which jointly models the offshore wind speeds across space, time, and height. DeepMIDE is formulated as a multi-output integro-difference equation model with a multivariate nonstationary kernel characterized by a set of advection vectors that encode the physics of wind field formation and propagation. Embedded within DeepMIDE, an advanced deep learning architecture learns these advection vectors from high-dimensional streams of exogenous weather information, which, along with other parameters, are plugged back into the statistical model for probabilistic multi-height space-time forecasting. Tested on real-world data from offshore wind energy areas in the Northeastern United States, the wind speed and power forecasts from DeepMIDE are shown to outperform those from prevalent time series, spatio-temporal, and deep learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20166v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Feng Ye, Xinxi Zhang, Michael Stein, Ahmed Aziz Ezzat</dc:creator>
    </item>
    <item>
      <title>Static marginal expected shortfall: Systemic risk measurement under dependence uncertainty</title>
      <link>https://arxiv.org/abs/2504.19953</link>
      <description>arXiv:2504.19953v2 Announce Type: replace-cross 
Abstract: Measuring the contribution of a bank or an insurance company to overall systemic risk is a key concern, particularly in the aftermath of the 2007--2009 financial crisis and the 2020 downturn. In this paper, we derive worst-case and best-case bounds for the marginal expected shortfall (MES) -- a key measure of systemic risk contribution -- under the assumption that individual firms' risk distributions are known but their dependence structure is not. We further derive tighter MES bounds when partial information on companies' risk exposures, and thus their dependence, is available. To represent this partial information, we employ three standard factor models: additive, minimum-based, and multiplicative background risk models. Additionally, we propose an alternative set of improved MES bounds based on a linear relationship between firm-specific and market-wide risks, consistent with the Capital Asset Pricing Model in finance and the Weighted Insurance Pricing Model in insurance. Finally, empirical analyses demonstrate the practical relevance of the theoretical bounds for industry practitioners and policymakers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19953v2</guid>
      <category>q-fin.RM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinghui Chen, Edward Furman, X. Sheldon Lin</dc:creator>
    </item>
    <item>
      <title>Extreme Conformal Prediction: Reliable Intervals for High-Impact Events</title>
      <link>https://arxiv.org/abs/2505.08578</link>
      <description>arXiv:2505.08578v2 Announce Type: replace-cross 
Abstract: Conformal prediction is a popular method to construct prediction intervals for black-box machine learning models with marginal coverage guarantees. In applications with potentially high-impact events, such as flooding or financial crises, regulators often require very high confidence for such intervals. However, if the desired level of confidence is too large relative to the amount of data used for calibration, then classical conformal methods provide infinitely wide, thus, uninformative prediction intervals. In this paper, we propose a new method to overcome this limitation. We bridge extreme value statistics and conformal prediction to provide reliable and informative prediction intervals with high-confidence coverage, which can be constructed using any black-box extreme quantile regression method. A weighted version of our approach can account for nonstationary data. The advantages of our extreme conformal prediction method are illustrated in a simulation study and in an application to flood risk forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08578v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olivier C. Pasche, Henry Lam, Sebastian Engelke</dc:creator>
    </item>
    <item>
      <title>Whitened Score Diffusion: A Structured Prior for Imaging Inverse Problems</title>
      <link>https://arxiv.org/abs/2505.10311</link>
      <description>arXiv:2505.10311v4 Announce Type: replace-cross 
Abstract: Conventional score-based diffusion models (DMs) may struggle with anisotropic Gaussian diffusion processes due to the required inversion of covariance matrices in the denoising score matching training objective \cite{vincent_connection_2011}. We propose Whitened Score (WS) diffusion models, a novel framework based on stochastic differential equations that learns the Whitened Score function instead of the standard score. This approach circumvents covariance inversion, extending score-based DMs by enabling stable training of DMs on arbitrary Gaussian forward noising processes. WS DMs establish equivalence with flow matching for arbitrary Gaussian noise, allow for tailored spectral inductive biases, and provide strong Bayesian priors for imaging inverse problems with structured noise. We experiment with a variety of computational imaging tasks using the CIFAR, CelebA ($64\times64$), and CelebA-HQ ($256\times256$) datasets and demonstrate that WS diffusion priors trained on anisotropic Gaussian noising processes consistently outperform conventional diffusion priors based on isotropic Gaussian noise. Our code is open-sourced at \href{https://github.com/jeffreyalido/wsdiffusion}{\texttt{github.com/jeffreyalido/wsdiffusion}}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10311v4</guid>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeffrey Alido, Tongyu Li, Yu Sun, Lei Tian</dc:creator>
    </item>
    <item>
      <title>Graph topology estimation of power grids using pairwise mutual information of time series data</title>
      <link>https://arxiv.org/abs/2505.11517</link>
      <description>arXiv:2505.11517v3 Announce Type: replace-cross 
Abstract: The topology of a power grid is estimated using an information theoretic approach. By modeling the grid as a graph and using voltage magnitude data of individual nodes in the grid, the mutual information between pairs of nodes is computed using different approximation methods. Using the well-known Chow-Liu algorithm, a maximum spanning tree based on mutual information is computed to estimate the power grid topology. This manuscript explores the application of this method to different datasets and explores the domain of applicability. The data quality, precision, time windows, frequency and the method for calculating the mutual information are varied to see the effect on the successful reconstruction of the graph and it's leaf nodes. Success is shown for IEEE networks generated with MATPOWER and data generated using GridLAB-D. The algorithm is then cross-validated on IEEE networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11517v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.CE</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel T. Speckhard</dc:creator>
    </item>
    <item>
      <title>From Path Coefficients to Targeted Estimands: A Comparison of Structural Equation Models (SEM) and Targeted Maximum Likelihood Estimation (TMLE)</title>
      <link>https://arxiv.org/abs/2511.01040</link>
      <description>arXiv:2511.01040v2 Announce Type: replace-cross 
Abstract: Structural Equation Modeling (SEM) has gained popularity in the social sciences and causal inference due to its flexibility in modeling complex relationships between variables and its availability in modern statistical software. To move beyond the parametric assumptions of SEM, this paper reviews targeted maximum likelihood estimation (TMLE), a doubly robust, machine learning-based approach that builds on nonparametric SEM. We demonstrate that both TMLE and SEM can be used to estimate standard causal effects and show that TMLE is robust to model misspecification. We conducted simulation studies under both correct and misspecified model conditions, implementing SEM and TMLE to estimate these causal effects. The simulations confirm that TMLE consistently outperforms SEM under misspecification in terms of bias, mean squared error, and the validity of confidence intervals. We applied both approaches to a real-world dataset to analyze the mediation effects of poverty on access to high school, revealing that the direct effect is no longer significant under TMLE, whereas SEM indicates significance. We conclude with practical guidance on using SEM and TMLE in light of recent developments in targeted learning for causal inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01040v2</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 18 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junjie Ma, Xiaoya Zhang, Guangye He, Yuting Han, Ting Ge, Feng Ji</dc:creator>
    </item>
  </channel>
</rss>
