<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 25 Mar 2025 03:04:44 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Change of some cropping systems in a long-term trial comparing different systems: rationale and implications for statistical analysis</title>
      <link>https://arxiv.org/abs/2503.16571</link>
      <description>arXiv:2503.16571v1 Announce Type: new 
Abstract: The project Agriculture 4.0 without chemical synthetical plant protection (NOcsPS) tests a number of cropping systems that avoid the use of chemical synthetical pesticides while at the same time using mineral fertilizers. The experiment started in 2020 (sowing fall 2019). In 2024 (sowing fall 2023), some of the cropping systems were modified. Analysis of this experiment may be done using linear mixed models. In order to include the data from 2020-2023 in joint analyses with the data collected for the modified systems from 2024 onwards, the mixed modelling approach needs to be reconsidered. In this paper, we develop models for this purpose. A key feature is the use of network meta-analytic concepts that allow a combination of direct and indirect comparisons among systems from the different years. The approach is first illustrated using a toy example. This is followed by detailed analyses of data from two the two trials sites Dahnsdorf and Hohenheim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16571v1</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hans-Peter Piepho, Ingrid Cla{\ss}-Mahler, Beate Zimmermann, Wilfried Hermann, J\"urgen Schwarz, Enno Bahrs</dc:creator>
    </item>
    <item>
      <title>Efficient automated cut-point finder in high-dimensional Cox models: a simple but promising approach</title>
      <link>https://arxiv.org/abs/2503.16687</link>
      <description>arXiv:2503.16687v1 Announce Type: new 
Abstract: We introduce Binacox+, an advanced extension of the Binacox method for prognostic analysis of high-dimensional survival data, enabling the detection of multiple cut-points per feature. The original Binacox method leverages the Cox proportional hazards model, combining one-hot encoding with the binarsity penalty to simultaneously perform feature selection and cut-point detection. In this work, we enhance Binacox by incorporating a novel penalty term based on the L1 norm of coefficients for cumulative binarization, defined over a set of pre-specified, context-dependent cut-point candidates. This new penalty not only improves interpretability but also significantly reduces computational time and enhances prediction performance compared to the original method. We conducted extensive simulation studies to evaluate the statistical and computational properties of Binacox+ in comparison to Binacox. Our simulation results demonstrate that Binacox+ achieves superior performance in important cut-point detection, particularly in high-dimensional settings, while drastically reducing computation time. As a case study, we applied both methods to three real-world genomic cancer datasets from The Cancer Genome Atlas (TCGA). The empirical results confirm that Binacox+ outperforms Binacox+ in risk prediction accuracy and computational efficiency, making it a powerful tool for survival analysis in high-dimensional biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16687v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdollah Safari, Hamed Halisaz, Peter Loewen</dc:creator>
    </item>
    <item>
      <title>Scalable community detection in massive networks via predictive assignment</title>
      <link>https://arxiv.org/abs/2503.16730</link>
      <description>arXiv:2503.16730v1 Announce Type: new 
Abstract: Massive network datasets are becoming increasingly common in scientific applications. Existing community detection methods encounter significant computational challenges for such massive networks due to two reasons. First, the full network needs to be stored and analyzed on a single server, leading to high memory costs. Second, existing methods typically use matrix factorization or iterative optimization using the full network, resulting in high runtimes. We propose a strategy called \textit{predictive assignment} to enable computationally efficient community detection while ensuring statistical accuracy. The core idea is to avoid large-scale matrix computations by breaking up the task into a smaller matrix computation plus a large number of vector computations that can be carried out in parallel. Under the proposed method, community detection is carried out on a small subgraph to estimate the relevant model parameters. Next, each remaining node is assigned to a community based on these estimates. We prove that predictive assignment achieves strong consistency under the stochastic blockmodel and its degree-corrected version. We also demonstrate the empirical performance of predictive assignment on simulated networks and two large real-world datasets: DBLP (Digital Bibliography \&amp; Library Project), a computer science bibliographical database, and the Twitch Gamers Social Network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16730v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Bhadra, Marianna Pensky, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Leveraging Two-Phase Data for Improved Prediction of Survival Outcomes with Application to Nasopharyngeal Cancer</title>
      <link>https://arxiv.org/abs/2503.16732</link>
      <description>arXiv:2503.16732v2 Announce Type: new 
Abstract: Accurate survival predicting models are essential for improving targeted cancer therapies and clinical care among cancer patients. In this article, we investigate and develop a method to improve predictions of survival in cancer by leveraging two-phase data with expert knowledge and prognostic index. Our work is motivated by two-phase data in nasopharyngeal cancer (NPC), where traditional covariates are readily available for all subjects, but the primary viral factor, Human Papillomavirus (HPV), is substantially missing. To address this challenge, we propose an expert guided method that incorporates prognostic index based on the observed covariates and clinical importance of key factors. The proposed method makes efficient use of available data, not simply discarding patients with unknown HPV status. We apply the proposed method and evaluate it against other existing approaches through a series of simulation studies and real data example of NPC patients. Under various settings, the proposed method consistently outperforms competing methods in terms of c-index, calibration slope, and integrated Brier score. By efficiently leveraging two-phase data, the model provides a more accurate and reliable predictive ability of survival models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16732v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Jeong Oh, Seungjun Ahn, Tristan Tham, Min Qian</dc:creator>
    </item>
    <item>
      <title>Modeling and forecasting subnational age distribution of death counts</title>
      <link>https://arxiv.org/abs/2503.16744</link>
      <description>arXiv:2503.16744v1 Announce Type: new 
Abstract: This paper presents several forecasting methods to model and forecast subnational age distribution of death counts. The age distribution of death counts has many similarities to probability density functions, which are nonnegative and have a constrained integral, and thus live in a constrained nonlinear space. To address the nonlinear nature of objects, we implement a cumulative distribution function transformation that has an additional monotonicity. Using the Japanese subnational life-table death counts obtained from the Japanese Mortality Database (2025), we evaluate the forecast accuracy of the transformation and forecasting methods. The improved forecast accuracy of life-table death counts implemented here will be of great interest to demographers in estimating regional age-specific survival probabilities and life expectancy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16744v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Cristian F. Jim\'enez-Var\'on</dc:creator>
    </item>
    <item>
      <title>Multi-View Orthogonal Projection Regression with Application in Multi-omics integration</title>
      <link>https://arxiv.org/abs/2503.16807</link>
      <description>arXiv:2503.16807v1 Announce Type: new 
Abstract: Multi-omics integration offers novel insights into complex biological mechanisms by utlizing the fused information from various omics datasets. However, the inherent within- and inter-modality correlations in multi-omics data present significant challenges for traditional variable selection methods, such as Lasso regression. These correlations can lead to multicollinearity, compromising the stability and interpretability of selected variables. To address these problems, we introduce the Multi-View Orthogonal Projection Regression (MVOPR), a novel approach for variable selection in multi-omics analysis. MVOPR leverages the unidirectional associations among omics layers, inspired by the Central Dogma of Molecular Biology, to transform predictors into an uncorrelated feature space. This orthogonal projection framework effectively mitigates the correlations, allowing penalized regression models to operate on independent components. Through simulations under both well-specified and misspecified scenarios, MVOPR demonstrates superior performance in variable selection, outperforming traditional Lasso-based methods and factor-based models. In real-data analysis on the CAARS dataset, MVOPR consistently identifies biologically relevant features, including the Bacteroidaceae family and key metabolites which align well with known asthma biomarkers. These findings illustrate MVOPR's ability to enhance variable selection while offering biologically interpretable insights, offering a robust tool for integrative multi-omics research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16807v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zongrui Dai, Yvonne J. Huang, Gen Li</dc:creator>
    </item>
    <item>
      <title>A categorization of performance measures for estimated non-linear associations between an outcome and continuous predictors</title>
      <link>https://arxiv.org/abs/2503.16981</link>
      <description>arXiv:2503.16981v1 Announce Type: new 
Abstract: In regression analysis, associations between continuous predictors and the outcome are often assumed to be linear. However, modeling the associations as non-linear can improve model fit. Many flexible modeling techniques, like (fractional) polynomials and spline-based approaches, are available. Such methods can be systematically compared in simulation studies, which require suitable performance measures to evaluate the accuracy of the estimated curves against the true data-generating functions. Although various measures have been proposed in the literature, no systematic overview exists so far. To fill this gap, we introduce a categorization of performance measures for evaluating estimated non-linear associations between an outcome and continuous predictors. This categorization includes many commonly used measures. The measures can not only be used in simulation studies, but also in application studies to compare different estimates to each other. We further illustrate and compare the behavior of different performance measures through some examples and a Shiny app.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16981v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theresa Ullmann, Georg Heinze, Michal Abrahamowicz, Aris Perperoglou, Willi Sauerbrei, Matthias Schmid, Daniela Dunkler, for TG2 of the STRATOS initiative</dc:creator>
    </item>
    <item>
      <title>An improved nonparametric test and sample size procedures for the randomized complete block designs</title>
      <link>https://arxiv.org/abs/2503.17179</link>
      <description>arXiv:2503.17179v1 Announce Type: new 
Abstract: The Friedman test has been extensively applied as a nonparametric alternative to the conventional F procedure for comparing treatment effects in randomized complete block designs. A chi-square distribution provides a convenient approximation to determining the critical values for the Friedman procedure in hypothesis testing. However, the chi-square approximation is generally conservative and the accuracy declines with increasing number of treatments. This paper describes an alternative transformation of the Friedman statistic along with an approximate F distribution that has the same numerator degrees of freedom as the ANOVA F test. Moreover, two approximate noncentral F distributions are presented for the proposed F-transformation under the alternative hypothesis of heterogeneous location shifts. Explicit power functions are derived when the underlying populations have the uniform, normal, Laplace, and exponential distributions. Theoretical examination and empirical assessment are presented to validate the advantages of the proposed approaches over the existing methods of the Friedman test. The developed test and power procedures are recommended due to their consistently acceptable Type I error rates and accurate power calculations for the location shift structures and population distributions considered here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17179v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Show-Li Jan, Gwowen Shieh</dc:creator>
    </item>
    <item>
      <title>Integrated Subset Selection and Bandwidth Estimation Algorithm for Geographically Weighted Regression</title>
      <link>https://arxiv.org/abs/2503.17253</link>
      <description>arXiv:2503.17253v1 Announce Type: new 
Abstract: This study proposes a mathematical programming-based algorithm for the integrated selection of variable subsets and bandwidth estimation in geographically weighted regression, a local regression method that allows the kernel bandwidth and regression coefficients to vary across study areas. Unlike standard approaches in the literature, in which bandwidth and regression parameters are estimated separately for each focal point on the basis of different criteria, our model uses a single objective function for the integrated estimation of regression and bandwidth parameters across all focal points, based on the regression likelihood function and variance modeling. The proposed model further integrates a procedure to select a single subset of independent variables for all focal points, whereas existing approaches may return heterogeneous subsets across focal points. We then propose an alternative direction method to solve the nonconvex mathematical model and show that it converges to a partial minimum. The computational experiment indicates that the proposed algorithm provides competitive explanatory power with stable spatially varying patterns, with the ability to select the best subset and account for additional constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17253v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyunwoo Lee, Young Woong Park</dc:creator>
    </item>
    <item>
      <title>Spatial-temporal models for forest inventory data</title>
      <link>https://arxiv.org/abs/2503.16691</link>
      <description>arXiv:2503.16691v1 Announce Type: cross 
Abstract: The USDA Forest Inventory and Analysis (FIA) program conducts a national forest inventory for the United States through a network of permanent field plots. FIA produces estimates of area averages/totals for plot-measured forest variables through design-based inference, assuming a fixed population and a probability sample of field plot locations. The fixed-population assumption and characteristics of the FIA sampling scheme make it difficult to estimate change in forest variables over time using design-based inference. We propose spatial-temporal models based on Gaussian processes as a flexible tool for forest inventory data, capable of inferring forest variables and change thereof over arbitrary spatial and temporal domains. It is shown to be beneficial for the covariance function governing the latent Gaussian process to account for variation at multiple scales, separating spatially local variation from ecosystem-scale variation. We demonstrate a model for forest biomass density, inferring 20 years of biomass change within two US National Forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16691v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul B. May, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Sparse Additive Contextual Bandits: A Nonparametric Approach for Online Decision-making with High-dimensional Covariates</title>
      <link>https://arxiv.org/abs/2503.16941</link>
      <description>arXiv:2503.16941v1 Announce Type: cross 
Abstract: Personalized services are central to today's digital landscape, where online decision-making is commonly formulated as contextual bandit problems. Two key challenges emerge in modern applications: high-dimensional covariates and the need for nonparametric models to capture complex reward-covariate relationships. We address these challenges by developing a contextual bandit algorithm based on sparse additive reward models in reproducing kernel Hilbert spaces. We establish statistical properties of the doubly penalized method applied to random regions, introducing novel analyses under bandit feedback. Our algorithm achieves sublinear cumulative regret over the time horizon $T$ while scaling logarithmically with covariate dimensionality $d$. Notably, we provide the first regret upper bound with logarithmic growth in $d$ for nonparametric contextual bandits with high-dimensional covariates. We also establish a lower bound, with the gap to the upper bound vanishing as smoothness increases. Extensive numerical experiments demonstrate our algorithm's superior performance in high-dimensional settings compared to existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16941v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenjia Wang, Qingwen Zhang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators</title>
      <link>https://arxiv.org/abs/2503.17290</link>
      <description>arXiv:2503.17290v1 Announce Type: cross 
Abstract: The partitioning of data for estimation and calibration critically impacts the performance of propensity score based estimators like inverse probability weighting (IPW) and double/debiased machine learning (DML) frameworks. We extend recent advances in calibration techniques for propensity score estimation, improving the robustness of propensity scores in challenging settings such as limited overlap, small sample sizes, or unbalanced data. Our contributions are twofold: First, we provide a theoretical analysis of the properties of calibrated estimators in the context of DML. To this end, we refine existing calibration frameworks for propensity score models, with a particular emphasis on the role of sample-splitting schemes in ensuring valid causal inference. Second, through extensive simulations, we show that calibration reduces variance of inverse-based propensity score estimators while also mitigating bias in IPW, even in small-sample regimes. Notably, calibration improves stability for flexible learners (e.g., gradient boosting) while preserving the doubly robust properties of DML. A key insight is that, even when methods perform well without calibration, incorporating a calibration step does not degrade performance, provided that an appropriate sample-splitting approach is chosen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17290v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Rabenseifner, Sven Klaassen, Jannis Kueck, Philipp Bach</dc:creator>
    </item>
    <item>
      <title>Statistical exploration of the Manifold Hypothesis</title>
      <link>https://arxiv.org/abs/2208.11665</link>
      <description>arXiv:2208.11665v5 Announce Type: replace 
Abstract: The Manifold Hypothesis is a widely accepted tenet of Machine Learning which asserts that nominally high-dimensional data are in fact concentrated near a low-dimensional manifold, embedded in high-dimensional space. This phenomenon is observed empirically in many real world situations, has led to development of a wide range of statistical methods in the last few decades, and has been suggested as a key factor in the success of modern AI technologies. We show that rich and sometimes intricate manifold structure in data can emerge from a generic and remarkably simple statistical model -- the Latent Metric Model -- via elementary concepts such as latent variables, correlation and stationarity. This establishes a general statistical explanation for why the Manifold Hypothesis seems to hold in so many situations. Informed by the Latent Metric Model we derive procedures to discover and interpret the geometry of high-dimensional data, and explore hypotheses about the data generating mechanism. These procedures operate under minimal assumptions and make use of well known graph-analytic algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11665v5</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick Whiteley, Annie Gray, Patrick Rubin-Delanchy</dc:creator>
    </item>
    <item>
      <title>On the efficiency-loss free ordering-robustness of product-PCA</title>
      <link>https://arxiv.org/abs/2302.11124</link>
      <description>arXiv:2302.11124v3 Announce Type: replace 
Abstract: This article studies the robustness of the eigenvalue ordering, an important issue when estimating the leading eigen-subspace by principal component analysis (PCA). In Yata and Aoshima (2010), cross-data-matrix PCA (CDM-PCA) was proposed and shown to have smaller bias than PCA in estimating eigenvalues. While CDM-PCA has the potential to achieve better estimation of the leading eigen-subspace than the usual PCA, its robustness is not well recognized. In this article, we first develop a more stable variant of CDM-PCA, which we call product-PCA (PPCA), that provides a more convenient formulation for theoretical investigation. Secondly, we prove that, in the presence of outliers, PPCA is more robust than PCA in maintaining the correct ordering of leading eigenvalues. The robustness gain in PPCA comes from the random data partition, and it does not rely on a data down-weighting scheme as most robust statistical methods do. This enables us to establish the surprising finding that, when there are no outliers, PPCA and PCA share the same asymptotic distribution. That is, the robustness gain of PPCA in estimating the leading eigen-subspace has no efficiency loss in comparison with PCA. Simulation studies and a face data example are presented to show the merits of PPCA. In conclusion, PPCA has a good potential to replace the role of the usual PCA in real applications whether outliers are present or not.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11124v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hung Hung, Su-Yun Huang</dc:creator>
    </item>
    <item>
      <title>A fully Bayesian approach for the imputation and analysis of derived outcome variables with missingness</title>
      <link>https://arxiv.org/abs/2404.09966</link>
      <description>arXiv:2404.09966v2 Announce Type: replace 
Abstract: Derived variables are variables that are constructed from one or more source variables through established mathematical operations or algorithms. For example, body mass index (BMI) is a derived variable constructed from two source variables: weight and height. When using a derived variable as the outcome in a statistical model, complications arise when some of the source variables have missing values. In this paper, we propose how one can define a single fully Bayesian model to simultaneously impute missing values and sample from the posterior. We compare our proposed method with alternative approaches that rely on multiple imputation with examples including an analysis to estimate the risk of microcephaly (a derived variable based on sex, gestational age and head circumference at birth) in newborns exposed to the ZIKA virus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09966v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harlan Campbell, Tim Morris, Paul Gustafson</dc:creator>
    </item>
    <item>
      <title>Network-based Neighborhood regression</title>
      <link>https://arxiv.org/abs/2407.04104</link>
      <description>arXiv:2407.04104v2 Announce Type: replace 
Abstract: Given the ubiquity of modularity in biological systems, module-level regulation analysis is vital for understanding biological systems across various levels and their dynamics. Current statistical analysis on biological modules predominantly focuses on either detecting the functional modules in biological networks or sub-group regression on the biological features without using the network data. This paper proposes a novel network-based neighborhood regression framework whose regression functions depend on both the global community-level information and local connectivity structures among entities. An efficient community-wise least square optimization approach is developed to uncover the strength of regulation among the network modules while enabling asymptotic inference. With random graph theory, we derive non-asymptotic estimation error bounds for the proposed estimator, achieving exact minimax optimality. Unlike the root-n consistency typical in canonical linear regression, our model exhibits linear consistency in the number of nodes n, highlighting the advantage of incorporating neighborhood information. The effectiveness of the proposed framework is further supported by extensive numerical experiments. Application to whole-exome sequencing and RNA-sequencing Autism datasets demonstrates the usage of the proposed method in identifying the association between the gene modules of genetic variations and the gene modules of genomic differential expressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04104v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yaoming Zhen, Jin-Hong Du</dc:creator>
    </item>
    <item>
      <title>Early and Late Buzzards: Comparing Different Approaches for Quantile-based Multiple Testing in Heavy-Tailed Wildlife Research Data</title>
      <link>https://arxiv.org/abs/2409.14926</link>
      <description>arXiv:2409.14926v2 Announce Type: replace 
Abstract: In medical, ecological and psychological research, there is a need for methods to handle multiple testing, for example to consider group comparisons with more than two groups. Typical approaches that deal with multiple testing are mean or variance based which can be less effective in the context of heavy-tailed and skewed data. Here, the median is the preferred measure of location and the interquartile range (IQR) is an adequate alternative to the variance. Therefore, it may be fruitful to formulate research questions of interest in terms of the median or the IQR. For this reason, we compare different inference approaches for two-sided and non-inferiority hypotheses formulated in terms of medians or IQRs in an extensive simulation study. We consider multiple contrast testing procedures combined with a bootstrap method as well as testing procedures with Bonferroni correction. As an example of a multiple testing problem based on heavy-tailed data we analyse an ecological trait variation in early and late breeding in a medium-sized bird of prey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.14926v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Marl\'ene Baumeister, Merle Munko, Kai-Philipp Gladow, Marc Ditzhaus, Nayden Chakarov, Markus Pauly</dc:creator>
    </item>
    <item>
      <title>Prediction-Centric Uncertainty Quantification via MMD</title>
      <link>https://arxiv.org/abs/2410.11637</link>
      <description>arXiv:2410.11637v2 Announce Type: replace 
Abstract: Deterministic mathematical models, such as those specified via differential equations, are a powerful tool to communicate scientific insight. However, such models are necessarily simplified descriptions of the real world. Generalised Bayesian methodologies have been proposed for inference with misspecified models, but these are typically associated with vanishing parameter uncertainty as more data are observed. In the context of a misspecified deterministic mathematical model, this has the undesirable consequence that posterior predictions become deterministic and certain, while being incorrect. Taking this observation as a starting point, we propose Prediction-Centric Uncertainty Quantification, where a mixture distribution based on the deterministic model confers improved uncertainty quantification in the predictive context. Computation of the mixing distribution is cast as a (regularised) gradient flow of the maximum mean discrepancy (MMD), enabling consistent numerical approximations to be obtained. Results are reported on both a toy model from population ecology and a real model of protein signalling in cell biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11637v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zheyang Shen, Jeremias Knoblauch, Sam Power, Chris. J. Oates</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Causal Estimand Selection</title>
      <link>https://arxiv.org/abs/2410.12093</link>
      <description>arXiv:2410.12093v2 Announce Type: replace 
Abstract: Estimating the causal effect of a treatment or health policy with observational data can be challenging due to an imbalance of and a lack of overlap between treated and control covariate distributions. In the presence of limited overlap, researchers choose between 1) methods (e.g., inverse probability weighting) that imply traditional estimands but whose estimators are at risk of considerable bias and variance; and 2) methods (e.g., overlap weighting) which imply a different estimand, thereby modifying the target population to reduce variance. We propose a framework for navigating the tradeoffs between variance and bias due to imbalance and lack of overlap and the targeting of the estimand of scientific interest. We introduce a bias decomposition that encapsulates bias due to 1) the statistical bias of the estimator; and 2) estimand mismatch, i.e., deviation from the population of interest. We propose two design-based metrics and an estimand selection procedure that help illustrate the tradeoffs between these sources of bias and variance of the resulting estimators. Our procedure allows analysts to incorporate their domain-specific preference for preservation of the original research population versus reduction of statistical bias. We demonstrate how to select an estimand based on these preferences with an application to right heart catheterization data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12093v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martha Barnard, Jared D. Huling, Julian Wolfson</dc:creator>
    </item>
    <item>
      <title>Topological Clustering of Agents in Hidden Information Contagions: Application to Financial Markets</title>
      <link>https://arxiv.org/abs/2410.21104</link>
      <description>arXiv:2410.21104v2 Announce Type: replace 
Abstract: This study proposes a strategy based on the Mapper algorithm, which utilizes topological data analysis to identify symptomatic agents in contagions by leveraging expert knowledge. The context of our paper is financial markets, where insiders may share private information through social links, and other agents may exhibit positive symptoms by opportunistically trading on this information. We verify and demonstrate our methods using both synthetic and empirical data on insider networks and stock market transactions. Recognizing the sensitive nature of insider trading cases, we design a conservative approach to minimize false positives, ensuring that innocent agents are not wrongfully implicated. The mapper-based method systematically outperforms other methods on synthetic data with ground truth. We also apply the method to empirical data and verify the results using a statistical validation method based on persistence homology. Our findings highlight that the proposed mapper-based technique successfully identifies a subpopulation of opportunistic agents within the information cascades. The adaptability of this method to diverse data types and sizes is demonstrated, with potential for tailoring for specific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21104v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anubha Goel, Henri Hansen, Juho Kanniainen</dc:creator>
    </item>
    <item>
      <title>Alignment and matching tests for high-dimensional tensor signals via tensor contraction</title>
      <link>https://arxiv.org/abs/2411.01732</link>
      <description>arXiv:2411.01732v2 Announce Type: replace 
Abstract: We consider two hypothesis testing problems for low-rank and high-dimensional tensor signals, namely the tensor signal alignment and tensor signal matching problems. These problems are challenging due to the high dimension of tensors and lack of meaningful test statistics. By exploiting a recent tensor contraction method, we propose and validate relevant test statistics using eigenvalues of a data matrix resulting from the tensor contraction. The matrix has a long range dependence among its entries, which makes the analysis of the matrix challenging, involved and distinct from standard random matrix theory. Our approach provides a novel framework for addressing hypothesis testing problems in the context of high-dimensional tensor signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01732v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruihan Liu, Zhenggang Wang, Jianfeng Yao</dc:creator>
    </item>
    <item>
      <title>Data collaboration for causal inference from limited medical testing and medication data</title>
      <link>https://arxiv.org/abs/2501.06511</link>
      <description>arXiv:2501.06511v2 Announce Type: replace 
Abstract: Observational studies enable causal inferences when randomized controlled trials (RCTs) are not feasible. However, integrating sensitive medical data across multiple institutions introduces significant privacy challenges. The data collaboration quasi-experiment (DC-QE) framework addresses these concerns by sharing "intermediate representations" -- dimensionality-reduced data derived from raw data -- instead of the raw data. While the DC-QE can estimate treatment effects, its application to medical data remains unexplored. This study applied the DC-QE framework to medical data from a single institution to simulate distributed data environments under independent and identically distributed (IID) and non-IID conditions. We propose a novel method for generating intermediate representations within the DC-QE framework. Experimental results demonstrated that DC-QE consistently outperformed individual analyses across various accuracy metrics, closely approximating the performance of centralized analysis. The proposed method further improved performance, particularly under non-IID conditions. These outcomes highlight the potential of the DC-QE framework as a robust approach for privacy-preserving causal inferences in healthcare. Broader adoption of this framework and increased use of intermediate representations could grant researchers access to larger, more diverse datasets while safeguarding patient confidentiality. This approach may ultimately aid in identifying previously unrecognized causal relationships, support drug repurposing efforts, and enhance therapeutic interventions for rare diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06511v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomoru Nakayama, Yuji Kawamata, Akihiro Toyoda, Akira Imakura, Rina Kagawa, Masaru Sanuki, Ryoya Tsunoda, Kunihiro Yamagata, Tetsuya Sakurai, Yukihiko Okada</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v2 Announce Type: replace-cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
  </channel>
</rss>
