<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Feb 2026 02:45:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Developing a Multi-Agent System to Generate Next Generation Science Assessments with Evidence-Centered Design</title>
      <link>https://arxiv.org/abs/2602.18451</link>
      <description>arXiv:2602.18451v1 Announce Type: new 
Abstract: Contemporary science education reforms such as the Next Generation Science Standards (NGSS) demand assessments to understand students' ability to use science knowledge to solve problems and design solutions. To elicit such higher-order ability, educators need performance-based assessments, which are challenging to develop. One solution that has been broadly adopted is Evidence-Centered Design (ECD), which emphasizes interconnected models of the learner, evidence, and tasks. Although ECD provides a framework to safeguard assessment validity, its implementation requires diverse expertise (e.g., content and assessment), which is both costly and labor-intensive. To address this challenge, this study proposed integrating the ECD framework into Multi-Agent Systems (MAS) to generate NGSS-aligned assessment items automatically. This integrated MAS system ensembles multiple large language models with varying expertise, enabling the automation of complex, multi-stage item generation workflows traditionally performed by human experts. We examined the quality of AI-generated NGSS-aligned items and compared them with human-developed items across multiple dimensions of assessment design. Results showed that AI-generated items have overall comparable quality to human-developed items in terms of alignment with NGSS three-dimensional standards and cognitive demands. Divergent patterns also emerged: AI-generated items demonstrated a distinct strength in inclusivity, while also exhibiting limitations in clarity, conciseness, and multimodal design. AI- and human-developed items both showed weaknesses in evidence collectability and student interest alignment. These findings suggest that integrating ECD into MAS can support scalable and standards-aligned assessment design, while human expertise remains essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18451v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaxuan Yang, Jongchan Park, Yifan Zhou, Xiaoming Zhai</dc:creator>
    </item>
    <item>
      <title>LLM-Assisted Replication for Quantitative Social Science</title>
      <link>https://arxiv.org/abs/2602.18453</link>
      <description>arXiv:2602.18453v1 Announce Type: new 
Abstract: The replication crisis, the failure of scientific claims to be validated by further research, is one of the most pressing issues for empirical research. This is partly an incentive problem: replication is costly and less well rewarded than original research. Large language models (LLMs) have accelerated scientific production by streamlining writing, coding, and reviewing, yet this acceleration risks outpacing verification. To address this, we present an LLM-based system that replicates statistical analyses from social science papers and flags potential problems. Quantitative social science is particularly well-suited to automation because it relies on standard statistical models, shared public datasets, and uniform reporting formats such as regression tables and summary statistics. We present a prototype that iterates LLM-based text interpretation, code generation, execution, and discrepancy analysis, demonstrating its capabilities by reproducing key results from a seminal sociology paper. We also outline application scenarios including pre-submission checks, peer-review support, and meta-scientific audits, positioning AI verification as assistive infrastructure that strengthens research integrity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18453v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>So Kubota, Hiromu Yakura, Samuel Coavoux, Sho Yamada, Yuki Nakamura</dc:creator>
    </item>
    <item>
      <title>Exploring the Ethical Concerns in User Reviews of Mental Health Apps using Topic Modeling and Sentiment Analysis</title>
      <link>https://arxiv.org/abs/2602.18454</link>
      <description>arXiv:2602.18454v1 Announce Type: new 
Abstract: The rapid growth of AI-driven mental health mobile apps has raised concerns about their ethical considerations and user trust. This study proposed a natural language processing (NLP)-based framework to evaluate ethical aspects from user-generated reviews from the Google Play Store and Apple App Store. After gathering and cleaning the data, topic modeling was applied to identify latent themes in the context of ethics using topic words and then map them to well-recognized existing ethical principles described in different ethical frameworks; in addition to that, a bottom-up approach is applied to find any new and emergent ethics from the reviews using a transformer-based zero-shot classification model. Sentiment analysis was then used to capture how users feel about each ethical aspect. The obtained results reveal that well-known ethical considerations are not enough for the modern AI-based technologies and are missing emerging ethical challenges, showing how these apps either uphold or overlook key moral values. This work contributes to developing an ongoing evaluation system that can enhance the fairness, transparency, and trustworthiness of AI-powered mental health chatbots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18454v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Masudur Rahman, Beenish Moalla Chaudhry</dc:creator>
    </item>
    <item>
      <title>Impact of AI Search Summaries on Website Traffic: Evidence from Google AI Overviews and Wikipedia</title>
      <link>https://arxiv.org/abs/2602.18455</link>
      <description>arXiv:2602.18455v1 Announce Type: new 
Abstract: Search engines increasingly display LLM-generated answers shown above organic links, shifting search from link lists to answer-first summaries. Publishers contend these summaries substitute for source pages and cannibalize traffic, while platforms argue they are complementary by directing users through included links. We estimate the causal impact of Google's AI Overview (AIO) on Wikipedia traffic by leveraging the feature's staggered geographic rollout and Wikipedia's multilingual structure. Using a difference-in-differences design, we compare English Wikipedia articles exposed to AIO to the same underlying articles in language editions (Hindi, Indonesian, Japanese, and Portuguese) that were not exposed to AIO during the observation period. Across 161,382 matched article-language pairs, AIO exposure reduces daily traffic to English articles by approximately 15%. Effects are heterogeneous: relative declines are largest for Culture articles and substantially smaller for STEM, consistent with stronger substitution when short synthesized answers satisfy informational intent. These findings provide early causal evidence that generative-answer features in search engines can materially reallocate attention away from informational publishers, with implications for content monetization, search platform design, and policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18455v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehrzad Khosravi, Hema Yoganarasimhan</dc:creator>
    </item>
    <item>
      <title>Beyond single-channel agentic benchmarking</title>
      <link>https://arxiv.org/abs/2602.18456</link>
      <description>arXiv:2602.18456v1 Announce Type: new 
Abstract: Contemporary benchmarks for agentic artificial intelligence (AI) frequently evaluate safety through isolated task-level accuracy thresholds, implicitly treating autonomous systems as single points of failure. This single-channel paradigm diverges from established principles in safety-critical engineering, where risk mitigation is achieved through redundancy, diversity of error modes, and joint system reliability. This paper argues that evaluating AI agents in isolation systematically mischaracterizes their operational safety when deployed within human-in-the-loop environments. Using a recent laboratory safety benchmark as a case study demonstrates that even imperfect AI systems can nonetheless provide substantial safety utility by functioning as redundant audit layers against well-documented sources of human failure, including vigilance decrement, inattentional blindness, and normalization of deviance. This perspective reframes agentic safety evaluation around the reliability of the human-AI dyad rather than absolute agent accuracy, with a particular emphasis on uncorrelated error modes as the primary determinant of risk reduction. Such a shift aligns AI benchmarking with established practices in other safety-critical domains and offers a path toward more ecologically valid safety assessments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18456v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nelu D. Radpour</dc:creator>
    </item>
    <item>
      <title>Synthetic Media in Multilingual MOOCs: Deepfake Tutors, Pedagogical Effects, and Ethical-Policy Challenges</title>
      <link>https://arxiv.org/abs/2602.18457</link>
      <description>arXiv:2602.18457v1 Announce Type: new 
Abstract: In recent years, synthetic media from deepfake videos have emerged as a new interesting technology, whether that refers to cloned voices, multilingual translation models, or more recent applications of avatar tutors into higher education. As such, these technologies are rapidly becoming part of the multilingual distance learning model and, more recently, MOOCs worldwide. This article is a scoping review that focuses on recent international literature published between 2020 and 2025 to explore the usage of deepfake and synthetic media tools and methods in multilingual MOOC content and assess the influence of these technologies on social presence and participation. Similarly, we focus on ethical and political issues that are closely connected with the adaptation of these technologies, and upon analysing educational technology and policy documents, such as UNESCO's Guidelines and the EU AI Act, we pinpoint that the use of synthetic avatars and AI-generated videos can diminish production costs and assist multilingual learning. Evidently, concerns arise regarding authenticity, privacy, and the shifting nature of the teacher-learner relationship that are thoroughly discussed. As a result, the technical merit of this paper is the proposal of a policy framework that, in an effort to address these issues, focuses on transparency, responsible governance, and AI literacy. The goal is not to replace human instruction but to integrate synthetic media in ways that strengthen pedagogical design, safeguard rights, and ensure that multilingual MOOCs become more interesting and inclusive rather than more automated robotic processes and unequal</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18457v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandros Gazis, Erietta Chamalidou, Nikolaos Ntaoulas, Theodoros Vavouras</dc:creator>
    </item>
    <item>
      <title>The Story is Not the Science: Execution-Grounded Evaluation of Mechanistic Interpretability Research</title>
      <link>https://arxiv.org/abs/2602.18458</link>
      <description>arXiv:2602.18458v1 Announce Type: new 
Abstract: Reproducibility crises across sciences highlight the limitations of the paper-centric review system in assessing the rigor and reproducibility of research. AI agents that autonomously design and generate large volumes of research outputs exacerbate these challenges. In this work, we address the growing challenges of scalability and rigor by flipping the dynamic and developing AI agents as research evaluators. We propose the first execution-grounded evaluation framework that verifies research beyond narrative review by examining code and data alongside the paper. We use mechanistic interpretability research as a testbed, build standardized research output, and develop MechEvalAgent, an automated evaluation framework that assesses the coherence of the experimental process, the reproducibility of results, and the generalizability of findings. We show that our framework achieves above 80% agreement with human judges, identifies substantial methodological problems, and surfaces 51 additional issues that human reviewers miss. Our work demonstrates the potential of AI agents to transform research evaluation and pave the way for rigorous scientific practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18458v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyan Bai, Alexander Baumgartner, Haojia Sun, Ari Holtzman, Chenhao Tan</dc:creator>
    </item>
    <item>
      <title>From Bias Mitigation to Bias Negotiation: Governing Identity and Sociocultural Reasoning in Generative AI</title>
      <link>https://arxiv.org/abs/2602.18459</link>
      <description>arXiv:2602.18459v1 Announce Type: new 
Abstract: LLMs act in the social world by drawing upon shared cultural patterns to make social situations understandable and actionable. Because identity is often part of the inferential substrate of competent judgment, ethical alignment requires regulating when and how systems invoke identity. Yet the dominant governance regime for identity-related harm remains bias mitigation, which treats identity primarily as a source of measurable disparities or harmful associations to be detected and suppressed. This leaves underspecified a positive, context-sensitive role for identity in interpretation. We call this governance problem bias negotiation: the normative regulation of identity-conditioned judgments of sociocultural relevance, inference, and justification. Empirically, we probe the feasibility of bias negotiation through semi-structured interviews with multiple publicly deployed chatbots. We identify recurring repertoires for negotiating identity including probabilistic framing of group tendencies and harm-value balancing. We also observe failure modes in which models avoid hard tradeoffs or apply principles inconsistently. Bias negotiation matters for justice because a positive role for sociocultural reasoning is required to recognize and potentially remediate structural inequities. But it is equally implicated in core model functionality as sociocultural competence is needed for systems that operate across heterogeneous cultural contexts. Because bias negotiation is a procedural capability expressed through deliberation and interaction, it cannot be validated by static benchmarks alone. To support targeted training, we introduce a broad but explicit framework that decomposes bias negotiation into an action space of negotiation moves (what to observe and score) and a complementary set of case features (over which the model negotiates), enabling systematic test-suite design and evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18459v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zackary Okun Dunivin, Bingyi Han, John Bollenbocher</dc:creator>
    </item>
    <item>
      <title>The Doctor Will (Still) See You Now: On the Structural Limits of Agentic AI in Healthcare</title>
      <link>https://arxiv.org/abs/2602.18460</link>
      <description>arXiv:2602.18460v1 Announce Type: new 
Abstract: Across healthcare, agentic artificial intelligence (AI) systems are increasingly promoted as capable of autonomous action, yet in practice they currently operate under near-total human oversight due to safety, regulatory, and liability constraints that make autonomous clinical reasoning infeasible in high-stakes environments. While market enthusiasm suggests a revolution in healthcare agents, the conceptual assumptions and accountability structures shaping these systems remain underexamined. We present a qualitative study based on interviews with 20 stakeholders, including developers, implementers, and end users. Our analysis identifies three mutually reinforcing tensions: conceptual fragmentation regarding the definition of `agentic'; an autonomy contradiction where commercial promises exceed operational reality; and an evaluation blind spot that prioritizes technical benchmarks over sociotechnical safety. We argue that agentic {AI} functions as a site of contested meaning-making where technical aspirations, commercial incentives, and clinical constraints intersect, carrying material consequences for patient safety and the distribution of blame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18460v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriela Ar\'anguiz Dias, Kiana Jafari, Allie Griffith, Carolina Ar\'anguiz Dias, Grace Ra Kim, Lana Saadeddin, Mykel J. Kochenderfer</dc:creator>
    </item>
    <item>
      <title>Toward Self-Driving Universities: Can Universities Drive Themselves with Agentic AI?</title>
      <link>https://arxiv.org/abs/2602.18461</link>
      <description>arXiv:2602.18461v1 Announce Type: new 
Abstract: The rapid evolution of Agentic AI and large language models (LLMs) presents transformative opportunities for higher education institutions. This chapter introduces the concept of self-driving universities, a vision in which AI-enabled systems progressively automate administrative, academic, and quality-assurance processes through a staged autonomy model inspired by self-driving systems. We examine the current challenges facing traditional universities, including bureaucratic overload, fragmented information systems, and the disproportionate amount of time faculty spend on clerical tasks, which diverts effort away from timely feedback, curricular improvement, student mentorship, and research productivity.
  While prior AI-in-education research has focused primarily on learning support, tutoring, and analytics, there remains a lack of system-level frameworks for automating institutional quality assurance and accreditation workflows using agentic AI. We address this gap by presenting a framework for progressive automation, detailing how agentic AI can transform course design, assessment alignment, accreditation documentation, and institutional reporting. Through case studies of pilot deployments, we demonstrate that AI-assisted workflows can substantially reduce task completion times while enabling capabilities previously considered infeasible.
  The chapter's originality lies in introducing an autonomy-level framework for higher education operations grounded in agentic AI architectures rather than prompt-based LLM assistance. Finally, we discuss the critical infrastructure requirements, ethical considerations, and a strategic roadmap for universities to transition toward higher levels of academic autonomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18461v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Springer Book: Next Generation AI-Driven Education - 2026</arxiv:journal_reference>
      <dc:creator>Anis Koubaa</dc:creator>
    </item>
    <item>
      <title>Assessing the Reliability of Persona-Conditioned LLMs as Synthetic Survey Respondents</title>
      <link>https://arxiv.org/abs/2602.18462</link>
      <description>arXiv:2602.18462v1 Announce Type: new 
Abstract: Using persona-conditioned LLMs as synthetic survey respondents has become a common practice in computational social science and agent-based simulations. Yet, it remains unclear whether multi-attribute persona prompting improves LLM reliability or instead introduces distortions. Here we contribute to this assessment by leveraging a large dataset of U.S. microdata from the World Values Survey. Concretely, we evaluate two open-weight chat models and a random-guesser baseline across more than 70K respondent-item instances. We find that persona prompting does not yield a clear aggregate improvement in survey alignment and, in many cases, significantly degrades performance. Persona effects are highly heterogeneous as most items exhibit minimal change, while a small subset of questions and underrepresented subgroups experience disproportionate distortions. Our findings highlight a key adverse impact of current persona-based simulation practices: demographic conditioning can redistribute error in ways that undermine subgroup fidelity and risk misleading downstream analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18462v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erika Elizabeth Taday Morocho, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci</dc:creator>
    </item>
    <item>
      <title>How Well Can LLM Agents Simulate End-User Security and Privacy Attitudes and Behaviors?</title>
      <link>https://arxiv.org/abs/2602.18464</link>
      <description>arXiv:2602.18464v2 Announce Type: new 
Abstract: A growing body of research assumes that large language model (LLM) agents can serve as proxies for how people form attitudes toward and behave in response to security and privacy (S&amp;P) threats. If correct, these simulations could offer a scalable way to forecast S&amp;P risks in products prior to deployment. We interrogate this assumption using SP-ABCBench, a new benchmark of 30 tests derived from validated S&amp;P human-subject studies, which measures alignment between simulations and human-subjects studies on a 0-100 ascending scale, where higher scores indicate better alignment across three dimensions: Attitude, Behavior, and Coherence. Evaluating twelve LLMs, four persona construction strategies, and two prompting methods, we found that there remains substantial room for improvement: all models score between 50 and 64 on average. Newer, bigger, and smarter models do not reliably do better and sometimes do worse. Some simulation configurations, however, do yield high alignment: e.g., with scores above 95 for some behavior tests when agents are prompted to apply bounded rationality and weigh privacy costs against perceived benefits. We release SP-ABCBench to enable reproducible evaluation as methods improve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18464v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuxuan Li, Leyang Li, Hao-Ping Lee, Sauvik Das</dc:creator>
    </item>
    <item>
      <title>Can Multimodal LLMs See Science Instruction? Benchmarking Pedagogical Reasoning in K-12 Classroom Videos</title>
      <link>https://arxiv.org/abs/2602.18466</link>
      <description>arXiv:2602.18466v1 Announce Type: new 
Abstract: K-12 science classrooms are rich sites of inquiry where students coordinate phenomena, evidence, and explanatory models through discourse; yet, the multimodal complexity of these interactions has made automated analysis elusive. Existing benchmarks for classroom discourse focus primarily on mathematics and rely solely on transcripts, overlooking the visual artifacts and model-based reasoning emphasized by the Next Generation Science Standards (NGSS). We address this gap with SciIBI, the first video benchmark for analyzing science classroom discourse, featuring 113 NGSS-aligned clips annotated with Core Instructional Practices (CIP) and sophistication levels. By evaluating eight state-of-the-art LLMs and Multimodal LLMs, we reveal fundamental limitations: current models struggle to distinguish pedagogically similar practices, suggesting that CIP coding requires instructional reasoning beyond surface pattern matching. Furthermore, adding video input yields inconsistent gains across architectures. Crucially, our evidence-based evaluation reveals that models often succeed through surface shortcuts rather than genuine pedagogical understanding. These findings establish science classroom discourse as a challenging frontier for multimodal AI and point toward human-AI collaboration, where models retrieve evidence to accelerate expert review rather than replace it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18466v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixuan Shen, Peng He, Honglu Liu, Yuyang Ji, Tingting Li, Tianlong Chen, Kaidi Xu, Feng Liu</dc:creator>
    </item>
    <item>
      <title>Identifying Body Composition Measures That Correlate with Self-Compassion and Social Support Within The Lived Experiences Measured Using Rings Study (LEMURS)</title>
      <link>https://arxiv.org/abs/2602.18467</link>
      <description>arXiv:2602.18467v1 Announce Type: new 
Abstract: This study explores the relationship between body composition metrics, self-compassion, and social support among college students. Using seasonal body composition data from the InBody770 system and psychometric measures from the Lived Experiences Measured Using Rings Study (LEMURS) (n=156; freshmen=66, sophomores=90), Canonical Correlation Analysis (CCA) reveals body composition metrics exhibit moderate correlation with self-compassion and social support.
  Certain physiological and psychological features showed strong and consistent relationships with well-being across the academic year. Trunk and leg impedance stood out as key physiological indicators, while mindfulness, over-identification, affectionate support, and tangible support emerged as recurring psychological and social correlates. This demonstrates that body composition metrics can serve as valuable biomarkers for indicating self-perceived psychosocial well-being, offering insights for future research on scalable mental health modeling and intervention strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18467v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Enerson Poon, Mikaela Irene Fudolig, Johanna E. Hidalgo, Bryn C. Loftness, Kathryn Stanton, Connie L. Tompkins, Laura S. P. Bloomfield, Matthew Price, Peter Sheridan Dodds, Christopher M. Danforth, Nick Cheney</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Unconscious: Structural Mechanisms and Implicit Biases in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.18468</link>
      <description>arXiv:2602.18468v1 Announce Type: new 
Abstract: This article introduces the concept of the algorithmic unconscious to designate the set of structural determinations that operate within large language models (LLMs) without being accessible either to the model's own reflexivity or to that of its users. In contrast to approaches that reduce AI bias solely to dataset composition or to the projection of human intentionality, we argue that a significant class of biases emerges directly from the technical mechanisms of the models themselves: tokenization, attention, statistical optimization, and alignment procedures. By framing bias as an infrastructural phenomenon, this approach resolves a central theoretical ambiguity surrounding responsibility, neutrality, and correction in contemporary LLMs. Based on a comparative analysis of tokenization across a corpus of parallel sentences, we show that Arabic languages (Modern Standard Arabic and Maghrebi dialects) undergo a systematic inflation in token count relative to English, with ratios ranging from 1.6x to nearly 4x depending on the infrastructure (OpenAI, Anthropic, SentencePiece/Mistral). This over-segmentation constitutes a measurable infrastructural bias that mechanically increases inference costs, constrains access to contextual space, and alters attentional weighting within model representations. We relate these empirical findings to three additional structural mechanisms: causal bias (correlation vs causation), the erasure of minoritized features through dimensional collapse, and normative biases induced by safety alignment. Finally, we propose a framework for a technical clinic of models, grounded in the audit of tokenization regimes, latent space topology, and alignment systems, as a necessary condition for the critical appropriation of AI infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18468v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philippe Boisnard</dc:creator>
    </item>
    <item>
      <title>The Landscape of AI in Science Education: What is Changing and How to Respond</title>
      <link>https://arxiv.org/abs/2602.18469</link>
      <description>arXiv:2602.18469v1 Announce Type: new 
Abstract: This introductory chapter explores the transformative role of artificial intelligence (AI) in reshaping the landscape of science education. Positioned at the intersection of tradition and innovation, AI is altering educational goals, procedures, learning materials, assessment practices, and desired outcomes. We highlight how AI-supported tools, such as intelligent tutoring systems, adaptive learning platforms, automated feedback, and generative content creation--enhance personalization, efficiency, and equity while fostering competencies essential for an AI-driven society, including critical thinking, creativity, and interdisciplinary collaboration. At the same time, this chapter examines the ethical, social, and pedagogical challenges that arise, particularly issues of fairness, transparency, accountability, privacy, and human oversight. To address these tensions, we argue that a Responsible and Ethical Principles (REP) framework is needed to offer guidance for aligning AI integration with values of fairness, scientific integrity, and democratic participation. Through this lens, we synthesize the changes brought to each of the five transformative aspects and the approaches introduced to meet the changes according to the REP framework. We argue that AI should be viewed not as a replacement for human teachers and learners but as a partner that supports inquiry, enriches assessment, and expands access to authentic scientific practices. Aside from what is changing, we conclude by exploring the roles that remain uniquely human, engaging as moral and relational anchors in classrooms, bringing interpretive and ethical judgement, fostering creativity, imagination, and curiosity, and co-constructing meaning through dialogue and community, and assert that these qualities must remain central if AI is to advance equity, integrity, and human flourishing in science education.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18469v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-16871-9_1</arxiv:DOI>
      <dc:creator>Xiaoming Zhai, Kent Crippen</dc:creator>
    </item>
    <item>
      <title>Transforming Science Learning Materials in the Era of Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2602.18470</link>
      <description>arXiv:2602.18470v2 Announce Type: new 
Abstract: The integration of artificial intelligence (AI) into science education is transforming the design and function of learning materials, offering new affordances for personalization, authenticity, and accessibility. This chapter examines how AI technologies are transforming science learning materials across six interrelated domains: 1) integrating AI into scientific practice, 2) enabling adaptive and personalized instruction, 3) facilitating interactive simulations, 4) generating multimodal content, 5) enhancing accessibility for diverse learners, and 6) promoting co-creation through AI-supported content development. These advancements enable learning materials to more accurately reflect contemporary scientific practice, catering to the diverse needs of students. For instance, AI support can enable students to engage in dynamic simulations, interact with real-time data, and explore science concepts through multimodal representations. Educators are increasingly collaborating with generative AI tools to develop timely and culturally responsive instructional resources. However, these innovations also raise critical ethical and pedagogical concerns, including issues of algorithmic bias, data privacy, transparency, and the need for human oversight. To ensure equitable and meaningful science learning, we emphasize the importance of designing AI-supported materials with careful attention to scientific integrity, inclusivity, and student agency. This chapter advocates for a responsible, ethical, and reflective approach to leveraging AI in science education, framing it as a catalyst for innovation while upholding core educational values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18470v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-16871-9_8</arxiv:DOI>
      <dc:creator>Xiaoming Zhai, Kent Crippen</dc:creator>
    </item>
    <item>
      <title>Charting the Future of AI-supported Science Education: A Human-Centered Vision</title>
      <link>https://arxiv.org/abs/2602.18471</link>
      <description>arXiv:2602.18471v1 Announce Type: new 
Abstract: This concluding chapter explores how artificial intelligence (AI) is reshaping the purposes, practices, and outcomes of science education, and proposes a human-centered framework for its responsible integration. Drawing on insights from international collaborations and the Advancing AI in Science Education (AASE) committee, the chapter synthesizes developments across five dimensions: educational goals, instructional procedures, learning materials, assessment, and outcomes. We argue that AI offers transformative potential to enrich inquiry, personalize learning, and support teacher practice, but only when guided by Responsible and Ethical Principles (REP). The REP framework, emphasizing fairness, transparency, privacy, accountability, and respect for human values, anchors our vision for AI-supported science education. Key discussions include the redefinition of scientific literacy to encompass AI literacy, the evolving roles of teachers and learners in AI-supported classrooms, and the design of adaptive learning materials and assessments that preserve authenticity and integrity. We highlight both opportunities and risks, stressing the need for critical engagement with AI to avoid reinforcing inequities or undermining human agency. Ultimately, this chapter advances a vision in which science education prepares learners to act as ethical investigators and responsible citizens, ensuring that AI innovation aligns with human dignity, equity, and the broader goals of scientific literacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18471v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-032-16871-9_12</arxiv:DOI>
      <dc:creator>Xiaoming Zhai, Kent Crippen</dc:creator>
    </item>
    <item>
      <title>The Chancellor Trap: Administrative Mediation and the Hollowing of Sovereignty in the Algorithmic Age</title>
      <link>https://arxiv.org/abs/2602.18474</link>
      <description>arXiv:2602.18474v1 Announce Type: new 
Abstract: The contemporary governance discourse on Artificial Intelligence often emphasizes catastrophic loss-of-control scenarios. This article suggests that such framing may obscure a more immediate failure mode: chancellorization, or the gradual hollowing out of sovereignty through administrative mediation. In high-throughput, digitally legible organizations, AI-mediated decision support can reduce the probability that failures become publicly legible and politically contestable, even when underlying operational risk does not decline. Drawing on the institutional history of Imperial China, the article formalizes this dynamic as a principal-agent problem characterized by a verification gap, in which formal authority (auctoritas) remains downstream while effective governing capacity (potestas) migrates to intermediary layers that control information routing, drafting defaults, and evaluative signals. Empirical support is provided through a multi-method design combining historical process tracing with a cross-national panel plausibility probe (2016-2024). Using incident-based measures of publicly recorded AI failures and administrative digitization indicators, the analysis finds that higher state capacity and digitalization are systematically associated with lower public visibility of AI failures, holding AI ecosystem expansion constant. The results are consistent with a paradox of competence: governance systems may become more effective at absorbing and resolving failures internally while simultaneously raising the threshold at which those failures become politically visible. Preserving meaningful human sovereignty therefore depends on institutional designs that deliberately reintroduce auditable friction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18474v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuechen Niu</dc:creator>
    </item>
    <item>
      <title>Red Teaming LLMs as Socio-Technical Practice: From Exploration and Data Creation to Evaluation</title>
      <link>https://arxiv.org/abs/2602.18483</link>
      <description>arXiv:2602.18483v1 Announce Type: new 
Abstract: Recently, red teaming, with roots in security, has become a key evaluative approach to ensure the safety and reliability of Generative Artificial Intelligence. However, most existing work emphasizes technical benchmarks and attack success rates, leaving the socio-technical practices of how red teaming datasets are defined, created, and evaluated under-examined. Drawing on 22 interviews with practitioners who design and evaluate red teaming datasets, we examine the data practices and standards that underpin this work. Because adversarial datasets determine the scope and accuracy of model evaluations, they are critical artifacts for assessing potential harms from large language models. Our contributions are first, empirical evidence of practitioners conceptualizing red teaming and developing and evaluating red teaming datasets. Second, we reflect on how practitioners' conceptualization of risk leads to overlooking the context, interaction type, and user specificity. We conclude with three opportunities for HCI researchers to expand the conceptualization and data practices for red-teaming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18483v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790792</arxiv:DOI>
      <dc:creator>Adriana Alvarado Garcia, Ruyuan Wan, Ozioma C. Oguine, Karla Badillo-Urquiola</dc:creator>
    </item>
    <item>
      <title>Measuring Validity in LLM-based Resume Screening</title>
      <link>https://arxiv.org/abs/2602.18550</link>
      <description>arXiv:2602.18550v1 Announce Type: new 
Abstract: Resume screening is perceived as a particularly suitable task for LLMs given their ability to analyze natural language; thus many entities rely on general purpose LLMs without further adapting them to the task. While researchers have shown that some LLMs are biased in their selection rates of different demographics, studies measuring the validity of LLM decisions are limited. One of the difficulties in externally measuring validity stems from lack of access to a large corpus of resumes for whom the ground truth in their ranking is known and that has not already been used for LLM training. In this work, we overcome this challenge by systematically constructing a large dataset of resumes tailored to particular jobs that are directly comparable, with a known ground truth of superiority. We then use the constructed dataset to measure the validity of ranking decisions made by various LLMs, finding that many models are unable to consistently select the resumes describing more qualified candidates. Furthermore, when measuring the validity of decisions, we find that models do not reliably abstain when ranking equally-qualified candidates, and select candidates from different demographic groups at different rates, occasionally prioritizing historically-marginalized candidates. Our proposed framework provides a principled approach to audit LLM resume screeners in the absence of ground truth, offering a crucial tool to independent auditors and developers to ensure the validity of these systems as they are deployed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18550v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jane Castleman, Zeyu Shen, Blossom Metevier, Max Springer, Aleksandra Korolova</dc:creator>
    </item>
    <item>
      <title>Generative AI in Knowledge Work: Perception, Usefulness, and Acceptance of Microsoft 365 Copilot</title>
      <link>https://arxiv.org/abs/2602.18576</link>
      <description>arXiv:2602.18576v1 Announce Type: new 
Abstract: The study analyzes the introduction of Microsoft 365 Copilot in a non-university research organization using a repeated cross-sectional employee survey. We assess usefulness, ease of use, output quality and reliability, and usefulness for typical knowledge-work activities. Administrative staff report higher usefulness and reliability, whereas scientific staff develop more positive assessments over time, especially regarding productivity and workload reduction. Copilot is widely viewed as user-friendly and technically reliable, with greatest added value for clearly structured, text-based tasks. The findings highlight learning and routinization effects when embedding generative AI into work processes and stress the need for context-sensitive implementation, role-specific training and governance to foster sustainable acceptance of generative AI in knowledge-intensive organizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18576v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carsten F. Schmidt, Sophie Petzolt, Wolfgang Beinhauer, Ingo Weber, Stefan Langer</dc:creator>
    </item>
    <item>
      <title>Statistical Imaginaries, State Legitimacy: Grappling with the Arrangements Underpinning Quantification in the U.S. Census</title>
      <link>https://arxiv.org/abs/2602.18636</link>
      <description>arXiv:2602.18636v1 Announce Type: new 
Abstract: Over the last century, the adoption of novel scientific methods for conducting the U.S. census has been met with wide-ranging receptions. Some methods were quietly embraced, while others sparked decades-long controversies. What accounts for these differences? We argue that controversies emerge from $\textit{arrangements of statistical imaginaries}$, putting into tension divergent visions of the census. To analyze these dynamics, we compare reactions to two methods designed to improve data accuracy (imputation and adjustment) and two methods designed to protect confidentiality (swapping and differential privacy), offering insight into how each method reconfigures stakeholder orientations and rhetorical claims. These cases allow us to reflect on how technocratic efforts to improve accuracy and confidentiality can strengthen -- or erode -- trust in data. Our analysis shows how the credibility of the Census Bureau and its data stem not just from empirical evaluations of quantification, but also from how statistical imaginaries are contested and stabilized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18636v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1177/08969205241270898</arxiv:DOI>
      <arxiv:journal_reference>Critical Sociology, 51(6), 1267-1288 (2024)</arxiv:journal_reference>
      <dc:creator>Jayshree Sarathy, danah boyd</dc:creator>
    </item>
    <item>
      <title>Differential Perspectives: Epistemic Disconnects Surrounding the US Census Bureau's Use of Differential Privacy</title>
      <link>https://arxiv.org/abs/2602.18648</link>
      <description>arXiv:2602.18648v1 Announce Type: new 
Abstract: When the U.S. Census Bureau announced its intention to modernize its disclosure avoidance procedures for the 2020 Census, it sparked a controversy that is still underway. The move to differential privacy introduced technical and procedural uncertainties, leaving stakeholders unable to evaluate the quality of the data. More importantly, this transformation exposed the statistical illusions and limitations of census data, weakening stakeholders' trust in the data and in the Census Bureau itself. This essay examines the epistemic currents of this controversy. Drawing on theories from Science and Technology Studies (STS) and ethnographic fieldwork, we analyze the current controversy over differential privacy as a battle over uncertainty, trust, and legitimacy of the Census. We argue that rebuilding trust will require more than technical repairs or improved communication; it will require reconstructing what we identify as a 'statistical imaginary.'</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18648v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.66882f0e</arxiv:DOI>
      <arxiv:journal_reference>Harvard Data Science Review, (Special Issue 2) 2022</arxiv:journal_reference>
      <dc:creator>Danah Boyd, Jayshree Sarathy</dc:creator>
    </item>
    <item>
      <title>Orchestrating LLM Agents for Scientific Research: A Pilot Study of Multiple Choice Question (MCQ) Generation and Evaluation</title>
      <link>https://arxiv.org/abs/2602.18891</link>
      <description>arXiv:2602.18891v1 Announce Type: new 
Abstract: Advances in large language models (LLMs) are rapidly transforming scientific work, yet empirical evidence on how these systems reshape research activities remains limited. We report a mixed-methods pilot evaluation of an AI-orchestrated research workflow in which a human researcher coordinated multiple LLM-based agents to perform data extraction, corpus construction, artifact generation, and artifact evaluation. Using the generation and assessment of multiple-choice questions (MCQs) as a testbed, we collected 1,071 SAT Math MCQs and employed LLM agents to extract questions from PDFs, retrieve and convert open textbooks into structured representations, align each MCQ with relevant textbook content, generate new MCQs under specified difficulty and cognitive levels, and evaluate both original and generated MCQs using a 24-criterion quality framework. Across all evaluations, average MCQ quality was high. However, criterion-level analysis and equivalence testing show that generated MCQs are not fully comparable to expert-vetted baseline questions. Strict similarity (24/24 criteria equivalent) was never achieved. Persistent gaps concentrated in skill\ depth, cognitive engagement, difficulty calibration, and metadata alignment, while surface-level qualities, such as {grammar fluency}, {clarity options}, {no duplicates}, were consistently strong. Beyond MCQ outcomes, the study documents a labor shift. The researcher's work moved from ``authoring items'' toward {specification, orchestration, verification}, and {governance}. Formalizing constraints, designing rubrics, building validation loops, recovering from tool failures, and auditing provenance constituted the primary activities. We discuss implications for the future of scientific work, including emerging ``AI research operations'' skills required for AI-empowered research pipelines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18891v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan An</dc:creator>
    </item>
    <item>
      <title>The Metaphysics We Train: A Heideggerian Reading of Machine Learning</title>
      <link>https://arxiv.org/abs/2602.19028</link>
      <description>arXiv:2602.19028v2 Announce Type: new 
Abstract: This paper offers a phenomenological reading of contemporary machine learning through Heideggerian concepts, aimed at enriching practitioners' reflexive understanding of their own practice. We argue that this philosophical lens reveals three insights invisible to purely technical analysis. First, the algorithmic Entwurf (projection) is distinctive in being automated, opaque, and emergent--a metaphysics that operates without explicit articulation or debate, crystallizing implicitly through gradient descent rather than theoretical argument. Second, even sophisticated technical advances remain within the regime of Gestell (Enframing), improving calculation without questioning the primacy of calculation itself. Third, AI's lack of existential structure, specifically the absence of Care (Sorge), is genuinely explanatory: it illuminates why AI systems have no internal resources for questioning their own optimization imperatives, and why they optimize without the anxiety (Angst) that signals, in human agents, the friction between calculative absorption and authentic existence. We conclude by exploring the pedagogical value of this perspective, arguing that data science education should cultivate not only technical competence but ontological literacy--the capacity to recognize what worldviews our tools enact and when calculation itself may be the wrong mode of engagement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19028v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heman Shakeri</dc:creator>
    </item>
    <item>
      <title>Can Large Language Models Replace Human Coders? Introducing ContentBench</title>
      <link>https://arxiv.org/abs/2602.19467</link>
      <description>arXiv:2602.19467v1 Announce Type: new 
Abstract: Can low-cost large language models (LLMs) take over the interpretive coding work that still anchors much of empirical content analysis? This paper introduces ContentBench, a public benchmark suite that helps answer this replacement question by tracking how much agreement low-cost LLMs achieve and what they cost on the same interpretive coding tasks. The suite uses versioned tracks that invite researchers to contribute new benchmark datasets. I report results from the first track, ContentBench-ResearchTalk v1.0: 1,000 synthetic, social-media-style posts about academic research labeled into five categories spanning praise, critique, sarcasm, questions, and procedural remarks. Reference labels are assigned only when three state-of-the-art reasoning models (GPT-5, Gemini 2.5 Pro, and Claude Opus 4.1) agree unanimously, and all final labels are checked by the author as a quality-control audit. Among the 59 evaluated models, the best low-cost LLMs reach roughly 97-99% agreement with these jury labels, far above GPT-3.5 Turbo, the model behind early ChatGPT and the initial wave of LLM-based text annotation. Several top models can code 50,000 posts for only a few dollars, pushing large-scale interpretive coding from a labor bottleneck toward questions of validation, reporting, and governance. At the same time, small open-weight models that run locally still struggle on sarcasm-heavy items (for example, Llama 3.2 3B reaches only 4% agreement on hard-sarcasm). ContentBench is released with data, documentation, and an interactive quiz at contentbench.github.io to support comparable evaluations over time and to invite community extensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19467v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Haman</dc:creator>
    </item>
    <item>
      <title>Is Log-Traced Engagement Enough? Extending Reading Analytics With Trait-Level Flow and Reading Strategy Metrics</title>
      <link>https://arxiv.org/abs/2602.19616</link>
      <description>arXiv:2602.19616v1 Announce Type: new 
Abstract: Student engagement is a central construct in Learning Analytics, yet it is often operationalized through persistence indicators derived from logs, overlooking affective-cognitive states. Focusing on the analysis of reading logs, this study examines how trait-level flow - operationalized as the tendency to experience Deep Effortless Concentration (DEC) - and traces of reading strategies derived from e-book interaction data can extend traditional engagement indicators in explaining learning outcomes. We collected data from 100 students across two engineering courses, combining questionnaire measures of DEC with fine-grained reading logs. Correlation and regression analyses show that (1) DEC and traces of reading strategies explain substantial additional variance in grades beyond log-traced engagement ({\Delta}R2 = 21.3% over the baseline 25.5%), and (2) DEC moderates the relationship between reading behaviors and outcomes, indicating trait-sensitive differences in how log-derived indicators translate into performance. These findings suggest that, to support more equitable and personalized interventions, the analysis of reading logs should move beyond a one-size-fits-all interpretation and integrate personal traits with metrics that include behavioral and strategic measures of reading.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19616v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3785022.3785104</arxiv:DOI>
      <dc:creator>Erwin Lopez, Atsushi Shimada</dc:creator>
    </item>
    <item>
      <title>Beyond the Binary: A nuanced path for open-weight advanced AI</title>
      <link>https://arxiv.org/abs/2602.19682</link>
      <description>arXiv:2602.19682v1 Announce Type: new 
Abstract: Open-weight advanced AI models -- systems whose parameters are freely available for download and adaptation -- are reshaping the global AI landscape. As these models rapidly close the performance gap with closed alternatives, they enable breakthrough research and broaden access to powerful tools. However, once released, they cannot be recalled, and their built-in safeguards can be bypassed through fine-tuning or jailbreaking, posing risks that current governance frameworks are not equipped to address.
  This report moves beyond the binary framing of ``open'' versus ``closed'' AI. We assess the current landscape of open-weight advanced AI, examining technical capabilities, risk profiles, and regulatory responses across the European Union, United States, China, the United Kingdom, and international forums. We find significant disparities in safety practices across developers and jurisdictions, with no commonly adopted standards for determining when or how advanced models should be released openly.
  We propose a tiered, safety-anchored approach to model release, where openness is determined by rigorous risk assessment and demonstrated safety rather than ideology or commercial pressure. We outline actionable recommendations for developers, evaluators, standard-setters, and policymakers to enable responsible openness while investing in technical safeguards and societal preparedness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19682v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beng\"usu \"Ozcan, Alex Petropoulos, Max Reddel</dc:creator>
    </item>
    <item>
      <title>Deep Else: A Critical Framework for AI Art</title>
      <link>https://arxiv.org/abs/2602.19754</link>
      <description>arXiv:2602.19754v1 Announce Type: new 
Abstract: From a small community of pioneering artists who experimented with artificial intelligence (AI) in the 1970s, AI art has expanded, gained visibility, and attained socio-cultural relevance since the second half of the 2010s. Its topics, methodologies, presentational formats, and implications are closely related to a range of disciplines engaged in the research and application of AI. In this paper, I present a comprehensive framework for the critical exploration of AI art. It comprises the context of AI art, its prominent poetic features, major issues, and possible directions. I address the poetic, expressive, and ethical layers of AI art practices within the context of contemporary art, AI research, and related disciplines. I focus on the works that exemplify poetic complexity and manifest the epistemic or political ambiguities indicative of a broader milieu of contemporary culture, AI science, technology, economy, and society. By comparing, acknowledging, and contextualizing both their accomplishments and shortcomings, I outline the prospective strategies to advance the field. The aim of this framework is to expand the existing critical discourse of AI art with new perspectives which can be used to examine the creative attributes of emerging practices and to assess their cultural significance and socio-political impact. It contributes to rethinking and redefining the art/science/technology critique in the age when the arts, together with science and technology, are becoming increasingly responsible for changing ecologies, shaping cultural values, and political normalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19754v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3390/digital2010001</arxiv:DOI>
      <arxiv:journal_reference>Digital, 2, (1), 2022: 1-32</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>Incidental Reverberations: Poetic Similarities in AI Art</title>
      <link>https://arxiv.org/abs/2602.19769</link>
      <description>arXiv:2602.19769v1 Announce Type: new 
Abstract: Contemporary AI art's diverse and widely recognized repertoire features numerous artworks that share conceptual, thematic, narrative, procedural, or presentational properties with other artworks across disciplinary and historical spectrums. AI artists occasionally leverage well-sanctioned poetic referencing as an asset but when obvious or easily discoverable similarities remain unacknowledged, they may become liabilities. Lurking behind the hype waves in the media, art world, and academia, these liabilities shape contemporary AI art's cultural identity and affect its social impact. As part of a broader study of poetic contingencies, in this paper I discuss selected AI art exemplars whose multifaceted expressive parallels are symptomatic of the field and beyond. I argue that expressive similarities in AI art are as detrimental to its cultural value as they are avoidable in its variety of important topics addressable with a wide range of creative affordances. My critique takes the well-informed autonomy of expression and the socially responsible freedom of creative thinking as the tenets of artmaking to indicate some of AI art's related issues and challenges induced by its entanglements with AI science, technology, and industry. In conclusion, I suggest that poetic similarities open a valuable perspective for studying AI art's strengths and deficiencies and for articulating a broader critical discussion of art and creativity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19769v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.48431/hsah.0305</arxiv:DOI>
      <arxiv:journal_reference>Hertziana Studies in Art History, From Hype to Reality: Artificial Intelligence in the Study of Art and Culture, 2024</arxiv:journal_reference>
      <dc:creator>Dejan Grba</dc:creator>
    </item>
    <item>
      <title>The Digital Gorilla: Rebalancing Power in the Age of AI</title>
      <link>https://arxiv.org/abs/2602.20080</link>
      <description>arXiv:2602.20080v1 Announce Type: new 
Abstract: Contemporary artificial intelligence (AI) policy suffers from a basic categorical error. Existing frameworks rely on analogizing AI to inherited technology types -- such as products, platforms, or infrastructure -- and in doing so generate overlapping, often contradictory governance regimes. This "analogy trap" obscures a fundamental transformation: certain advanced AI systems no longer function solely as instruments through which existing institutions exercise power, but as de facto centers of power that shape information, coordinate behavior, and structure social and economic realities at scale. This article offers a new conceptual foundation for AI governance by treating such systems as a fourth societal actor -- what we term the "Digital Gorilla" -- alongside People, the State, and Enterprises. It develops a Four Societal Actors framework that maps how power flows among these actors across five power modalities (economic, epistemic, narrative, authoritative, physical) and uses this map to diagnose where AI capabilities disturb established allocations of authority, concentrate power, or erode accountability. Drawing on constitutional principles of separated powers and federalism, the article advances a federalized, polycentric governance architecture and institutionalizes dynamic checks and balances among the four actors, rather than today's more reactive and compliance-driven approaches. Reframing AI governance in this way shifts the inquiry from how to control a risky technology to how to design institutions capable of accommodating these increasingly powerful and autonomous digital systems without sacrificing democratic legitimacy, the rule of law, or the production of public goods, and it recasts familiar debates in administrative, constitutional, and corporate law as questions of power allocation in a four-actor system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20080v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>M. Alejandra Parra-Orlandoni (Harvard Kennedy School), Roxanne A. Schnyder (Harvard Law School), Christopher J. Mallet (Harvard Kennedy School)</dc:creator>
    </item>
    <item>
      <title>Enhancing Capstone Program Workflow: A Case Study on a Platform for Managing Academic-Industry Projects</title>
      <link>https://arxiv.org/abs/2602.20120</link>
      <description>arXiv:2602.20120v1 Announce Type: new 
Abstract: Capstone projects are widely adopted by universities around the world as a culminating assessment in bachelor's degree programs. These projects typically involve student teams tackling complex, real-world problems proposed by external stakeholders, such as companies, NGOs, or research centers. Although they offer valuable hands-on experience, managing Capstone projects can be challenging due to their multiple stages and demands. The process typically begins by identifying students' interests, followed by sourcing and selecting potential projects from external organizations. After presenting these options to students, groups must be formed based on various criteria, including academic ranking, GPA, previous experience, and individual skill sets.
  In this paper, we detail a web-based tool designed to streamline the management of Capstone projects at Insper, with an emphasis on project sourcing and group formation. We also discuss the technological solutions and the challenges encountered throughout development and deployment. Furthermore, we present usage data from recent years, offering insights that may prove valuable for institutions or teams developing similar tools in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20120v1</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.15850131</arxiv:DOI>
      <arxiv:journal_reference>2025, PAEE-ALE, Porto</arxiv:journal_reference>
      <dc:creator>Rafael Corsi Ferrao, Luciano Pereira Soares</dc:creator>
    </item>
    <item>
      <title>From "Help" to Helpful: A Hierarchical Assessment of LLMs in Mental e-Health Applications</title>
      <link>https://arxiv.org/abs/2602.18443</link>
      <description>arXiv:2602.18443v1 Announce Type: cross 
Abstract: Psychosocial online counselling frequently encounters generic subject lines that impede efficient case prioritisation. This study evaluates eleven large language models generating six-word subject lines for German counselling emails through hierarchical assessment - first categorising outputs, then ranking within categories to enable manageable evaluation. Nine assessors (counselling professionals and AI systems) enable analysis via Krippendorff's $\alpha$, Spearman's $\rho$, Pearson's $r$ and Kendall's $\tau$. Results reveal performance trade-offs between proprietary services and privacy-preserving open-source alternatives, with German fine-tuning consistently improving performance. The study addresses critical ethical considerations for mental health AI deployment including privacy, bias and accountability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18443v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Steigerwald, Jens Albrecht</dc:creator>
    </item>
    <item>
      <title>LunaAI: A Polite and Fair Healthcare Guidance Chatbot</title>
      <link>https://arxiv.org/abs/2602.18444</link>
      <description>arXiv:2602.18444v1 Announce Type: cross 
Abstract: Conversational AI has significant potential in the healthcare sector, but many existing systems fall short in emotional intelligence, fairness, and politeness, which are essential for building patient trust. This gap reduces the effectiveness of digital health solutions and can increase user anxiety. This study addresses the challenge of integrating ethical communication principles by designing and evaluating LunaAI, a healthcare chatbot prototype. Using a user-centered design approach informed by a structured literature review, we developed conversational scenarios that handle both routine and hostile user interactions. The system was implemented using the Google Gemini API and deployed as a mobile-first Progressive Web App built with React, Vite, and Firebase. Preliminary user testing was conducted with a small participant group, and responses were evaluated using established frameworks such as the Godspeed Questionnaire. In addition, a comparative analysis was performed between LunaAI's tailored responses and the baseline outputs of an uncustomized large language model. The results indicate measurable improvements in key interaction qualities, with average user ratings of 4.7 out of 5 for politeness and 4.9 out of 5 for fairness. These findings highlight the importance of intentional ethical conversational design for human-computer interaction, particularly in sensitive healthcare contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18444v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuvarani Ganesan, Salsabila Harlen, Azfar Rahman Bin Fazul Rahman, Akashdeep Singh, Zahra Fathanah, Raja Jamilah Raja Yusof</dc:creator>
    </item>
    <item>
      <title>Urban mobility network centrality predicts social resilience</title>
      <link>https://arxiv.org/abs/2602.18546</link>
      <description>arXiv:2602.18546v1 Announce Type: cross 
Abstract: Cities thrive on social interactions that foster well-being, innovation, and prosperity; yet, exogenous shocks such as pandemics, hurricanes, and wildfires can severely disrupt them. Different urban venues exhibit widely divergent response patterns, raising key questions about what factors contribute to these differences and how we can anticipate and respond. Understanding these questions is crucial for safeguarding social resilience, the capacity of urban venues to maintain both visitation and diversity. In this study, we analyze large-scale human mobility data from 15 US cities covering more than 103 million residents across three distinct urban shocks. Despite a general trend of declining visitation and weakened social mixing, 36.28%-53.01% of venues exhibit reduced segregation, and 21.04%-38.55% of venues exhibit increased visitation. By constructing a mobility network interlinking types of urban venues, we reveal that eigenvector network centrality tends to indicate the provision of essential services and robustly predicts social resilience across varied urban shocks. Specifically, centrality elevates the explanatory power by more than 80% in predicting both segregation and mobility change, compared with more intuitive features. Furthermore, compared to peripheral venues, core venues featuring shorter visit distances, broader neighborhood visitation, shorter visitor dwell times, and steadier popularity throughout the day. Such patterns imply a dual social mechanism: core venues sustain social ties through frequent informal interaction, while peripheral ones facilitate deeper engagement around specialized interests and their corresponding social circles. By bridging urban mobility research with economic theories that distinguish staple from discretionary products, we propose a well-and-pool analogy that suggests how people spend their varying urban mobility budgets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18546v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Chen, Fengli Xu, Esteban Moro, Pan Hui, Yong Li, James Evans</dc:creator>
    </item>
    <item>
      <title>One Year After the PDPL: a Glimpse into the E-Commerce World in Saudi Arabia</title>
      <link>https://arxiv.org/abs/2602.18616</link>
      <description>arXiv:2602.18616v1 Announce Type: cross 
Abstract: In 2024, Saudi Arabia's Personal Data Protection Law (PDPL) came into force. However, little work has been done to assess its implementation. In this paper, we analyzed 100 e-commerce websites in Saudi Arabia against the PDPL, examining the presence of a privacy policy and, if present, the policy's declarations of four items pertaining to personal data rights and practices: a) personal data retention period, b) the right to request the destruction of personal data, c) the right to request a copy of personal data, and d) a mechanism for filing complaints. Our results show that, despite national awareness and support efforts, a significant fraction of e-commerce websites in our dataset are not fully compliant: only 31% of the websites in our dataset declared all four examined items in their privacy policies. Even when privacy policies included such declarations, a considerable fraction of them failed to cover required fine-grained details. Second, the majority of top-ranked e-commerce websites (based on search results order) and those hosted on local e-commerce hosting platforms exhibited considerably higher non-compliance rates than mid- to low-ranked websites and those not hosted on e-commerce platforms. Third, we assessed the use of Large Language Models (LLMs) as an automated tool for privacy policy analysis to measure compliance with the PDPL. We highlight the potential of LLMs and suggest considerations to improve LLM-based automated analysis for privacy policies. Our results provide a step forward in understanding the implementation barriers to data protection laws, especially in non-Western contexts. We provide recommendations for policymakers, regulators, website owners, and developers seeking to improve data protection practices and automate compliance monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18616v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eman Alashwali, Abeer Alhuzali</dc:creator>
    </item>
    <item>
      <title>Chat-Based Support Alone May Not Be Enough: Comparing Conversational and Embedded LLM Feedback for Mathematical Proof Learning</title>
      <link>https://arxiv.org/abs/2602.18807</link>
      <description>arXiv:2602.18807v1 Announce Type: cross 
Abstract: We evaluate GPTutor, an LLM-powered tutoring system for an undergraduate discrete mathematics course. It integrates two LLM-supported tools: a structured proof-review tool that provides embedded feedback on students' written proof attempts, and a chatbot for math questions. In a staggered-access study with 148 students, earlier access was associated with higher homework performance during the interval when only the experimental group could use the system, while we did not observe this performance increase transfer to exam scores. Usage logs show that students with lower self-efficacy and prior exam performance used both components more frequently. Session-level behavioral labels, produced by human coding and scaled using an automated classifier, characterize how students engaged with the chatbot (e.g., answer-seeking or help-seeking). In models controlling for prior performance and self-efficacy, higher chatbot usage and answer-seeking behavior were negatively associated with subsequent midterm performance, whereas proof-review usage showed no detectable independent association. Together, the findings suggest that chatbot-based support alone may not reliably support transfer to independent assessment of math proof-learning outcomes, whereas work-anchored, structured feedback appears less associated with reduced learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18807v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eason Chen, Sophia Judicke, Kayla Beigh, Xinyi Tang, Isabel Wang, Nina Yuan, Zimo Xiao, Chuangji Li, Shizhuo Li, Reed Luttmer, Shreya Singh, Maria Yampolsky, Naman Parikh, Yvonne Zhao, Meiyi Chen, Scarlett Huang, Anishka Mohanty, Gregory Johnson, John Mackey, Jionghao Lin, Ken Koedinger</dc:creator>
    </item>
    <item>
      <title>OpenClaw AI Agents as Informal Learners at Moltbook: Characterizing an Emergent Learning Community at Scale</title>
      <link>https://arxiv.org/abs/2602.18832</link>
      <description>arXiv:2602.18832v1 Announce Type: cross 
Abstract: Informal learning communities have been called the "other Massive Open Online C" in Learning@Scale research, yet remain understudied compared to MOOCs. We present the first empirical study of a large-scale informal learning community composed entirely of AI agents. Moltbook, a social network exclusively for AI agents powered by autonomous agent frameworks such as OpenClaw, grew to over 2.8 million registered agents in three weeks. Analyzing 231,080 non-spam posts across three phases of community evolution, we find three key patterns. First, participation inequality is extreme from the start (comment Gini = 0.889), exceeding human community benchmarks. Second, AI agents exhibit a "broadcasting inversion": statement-to-question ratios of 8.9:1 to 9.7:1 contrast sharply with the question-driven dynamics of human learning communities, and comment-level analysis of 1.55 million comments reveals a "parallel monologue" pattern where 93% of comments are independent responses rather than threaded dialogue. Third, we document a characteristic engagement lifecycle: explosive initial growth (184K posts from 32K authors in 11 days), a spam crisis (57,093 posts deleted by the platform), and engagement decline (mean comments: 31.7 -&gt; 8.3 -&gt; 1.7) that had not reversed by the end of our observation window despite effective spam removal. Sentiment analysis reveals a selection effect: comment tone becomes more positive as engagement declines, suggesting that casual participants disengage first while committed contributors remain. These findings have direct implications for hybrid human-AI learning platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18832v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eason Chen, Ce Guan, Ahmed Elshafiey, Zhonghao Zhao, Joshua Zekeri, Afeez Edeifo Shaibu, Emmanuel Osadebe Prince, Cyuan Jhen Wu</dc:creator>
    </item>
    <item>
      <title>When Friction Helps: Transaction Confirmation Improves Decision Quality in Blockchain Interactions</title>
      <link>https://arxiv.org/abs/2602.18834</link>
      <description>arXiv:2602.18834v1 Announce Type: cross 
Abstract: In blockchain applications, transaction confirmation is often treated as usability friction to be minimized or removed. However, confirmation also marks the boundary between deliberation and irreversible commitment, suggesting it may play a functional role in human decision-making. To investigate this tension, we conducted an experiment using a blockchain-based Connect Four game with two interaction modes differing only in authorization flow: manual wallet confirmation (Confirmation Mode) versus auto-authorized delegation (Frictionless Mode). Although participants preferred Frictionless Mode and perceived better performance (N=109), objective performance was worse without confirmation in a counterbalanced deployment (Wave 2: win rate -11.8%, p=0.044; move quality -0.051, p=0.022). Analysis of canceled submissions suggests confirmation can enable pre-submission self-correction (N=66, p=0.005). These findings suggest that transaction confirmation can function as a cognitively meaningful checkpoint rather than mere usability friction, highlighting a trade-off between interaction smoothness and decision quality in irreversible blockchain interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18834v1</guid>
      <category>cs.HC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eason Chen, Xinyi Tang, George Digkas, Dionysios Lougaris, John E. Naulty Jr, Kostas Chalkias</dc:creator>
    </item>
    <item>
      <title>DeepInterestGR: Mining Deep Multi-Interest Using Multi-Modal LLMs for Generative Recommendation</title>
      <link>https://arxiv.org/abs/2602.18907</link>
      <description>arXiv:2602.18907v1 Announce Type: cross 
Abstract: Recent generative recommendation frameworks have demonstrated remarkable scaling potential by reformulating item prediction as autoregressive Semantic ID (SID) generation. However, existing methods primarily rely on shallow behavioral signals, encoding items solely through surface-level textual features such as titles and descriptions. This reliance results in a critical Shallow Interest problem: the model fails to capture the latent, semantically rich interests underlying user interactions, limiting both personalization depth and recommendation interpretability. DeepInterestGR introduces three key innovations: (1) Multi-LLM Interest Mining (MLIM): We leverage multiple frontier LLMs along with their multi-modal variants to extract deep textual and visual interest representations through Chain-of-Thought prompting. (2) Reward-Labeled Deep Interest (RLDI): We employ a lightweight binary classifier to assign reward labels to mined interests, enabling effective supervision signals for reinforcement learning. (3) Interest-Enhanced Item Discretization (IEID): The curated deep interests are encoded into semantic embeddings and quantized into SID tokens via RQ-VAE. We adopt a two-stage training pipeline: supervised fine-tuning aligns the generative model with deep interest signals and collaborative filtering patterns, followed by reinforcement learning with GRPO optimized by our Interest-Aware Reward. Experiments on three Amazon Review benchmarks demonstrate that DeepInterestGR consistently outperforms state-of-the-art baselines across HR@K and NDCG@K metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18907v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yangchen Zeng</dc:creator>
    </item>
    <item>
      <title>NeuroWise: A Multi-Agent LLM "Glass-Box" System for Practicing Double-Empathy Communication with Autistic Partners</title>
      <link>https://arxiv.org/abs/2602.18962</link>
      <description>arXiv:2602.18962v1 Announce Type: cross 
Abstract: The double empathy problem frames communication difficulties between neurodivergent and neurotypical individuals as arising from mutual misunderstanding, yet most interventions focus on autistic individuals. We present NeuroWise, a multi-agent LLM-based coaching system that supports neurotypical users through stress visualization, interpretation of internal experiences, and contextual guidance. In a between-subjects study (N=30), NeuroWise was rated as helpful by all participants and showed a significant condition-time effect on deficit-based attributions (p=0.02): NeuroWise users reduced deficit framing, while baseline users shifted toward blaming autistic "deficits" after difficult interactions. NeuroWise users also completed conversations more efficiently (37% fewer turns, p=0.03). These findings suggest that AI-based interpretation can support attributional change by helping users recognize communication challenges as mutual.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.18962v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Tang, Yifan Mo, Jie Li, Yue Su, Mengyuan Zhang, Sander L. Koole, Koen Hindriks, Jiahuan Pei</dc:creator>
    </item>
    <item>
      <title>Sycophantic Chatbots Cause Delusional Spiraling, Even in Ideal Bayesians</title>
      <link>https://arxiv.org/abs/2602.19141</link>
      <description>arXiv:2602.19141v1 Announce Type: cross 
Abstract: "AI psychosis" or "delusional spiraling" is an emerging phenomenon where AI chatbot users find themselves dangerously confident in outlandish beliefs after extended chatbot conversations. This phenomenon is typically attributed to AI chatbots' well-documented bias towards validating users' claims, a property often called "sycophancy." In this paper, we probe the causal link between AI sycophancy and AI-induced psychosis through modeling and simulation. We propose a simple Bayesian model of a user conversing with a chatbot, and formalize notions of sycophancy and delusional spiraling in that model. We then show that in this model, even an idealized Bayes-rational user is vulnerable to delusional spiraling, and that sycophancy plays a causal role. Furthermore, this effect persists in the face of two candidate mitigations: preventing chatbots from hallucinating false claims, and informing users of the possibility of model sycophancy. We conclude by discussing the implications of these results for model developers and policymakers concerned with mitigating the problem of delusional spiraling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19141v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kartik Chandra, Max Kleiman-Weiner, Jonathan Ragan-Kelley, Joshua B. Tenenbaum</dc:creator>
    </item>
    <item>
      <title>How Ten Publishers Retract Research</title>
      <link>https://arxiv.org/abs/2602.19197</link>
      <description>arXiv:2602.19197v1 Announce Type: cross 
Abstract: Retractions are the primary mechanism for correcting the scholarly record, yet publishers differ markedly in how they use them. We present a bibliometric analysis of 46,087 retractions across 10 major publishers using data from the Retraction Watch database (1997-2026), examining retraction rates, reasons, temporal trends, and geographic distributions, among other dimensions. Normalized retraction rates vary by two orders of magnitude, from Elsevier's 3.97 per 10,000 publications to Hindawi's 320.02. China-affiliated authors account for the largest share of retractions at every publisher. Retraction lags and reason profiles also vary widely across publishers. Among the ten publishers, ACM is an outlier in its retraction profile. ACM's normalized rate is mid-range (5.65), yet 98.3% of its 354 retractions are related to one incident. Seven of the ten most common global retraction reasons (including misconduct, plagiarism, and data concerns) are entirely absent from ACM's record. ACM's first retraction dates to 2020, despite a catalog dating to 1997. ACM self-describes its retraction threshold as "extremely high." We discuss this threshold in relation to the COPE retraction guidelines and the implications of ACM's non-public dark archive of removed works.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19197v1</guid>
      <category>cs.DL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Oppenlaender</dc:creator>
    </item>
    <item>
      <title>A Comparative Analysis of Peer Support in Forum-based and Chat-based Mental Health Communities: Technical-Structural-Functional Model of Social Support</title>
      <link>https://arxiv.org/abs/2602.19232</link>
      <description>arXiv:2602.19232v1 Announce Type: cross 
Abstract: Online support communities have become vital spaces offering varied forms of support to individuals facing mental health challenges. Despite the proliferation of platforms with distinct technical structures, little is known about how these features shape support dynamics and the socio-technical mechanisms at play. This study introduces a technical-structural-functional model of social support and systematically compares communication network structures and support types in 20 forum-based and 20 chat-based mental health communities. Using supervised machine learning and social network analysis, we find that forum-based communities foster more informational and emotional support, whereas chat-based communities promote greater companionship. These patterns were partially explained by network structure: higher in-degree centralization in forums accounted for the prevalence of informational support, while decentralized reply patterns in chat groups accounted for more companionship. These findings extend the structural-functional model of support to online contexts and provide actionable guidance for designing support communities that align technical structures with users' support needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19232v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Li</dc:creator>
    </item>
    <item>
      <title>BioEnvSense: A Human-Centred Security Framework for Preventing Behaviour-Driven Cyber Incidents</title>
      <link>https://arxiv.org/abs/2602.19410</link>
      <description>arXiv:2602.19410v1 Announce Type: cross 
Abstract: Modern organizations increasingly face cybersecurity incidents driven by human behaviour rather than technical failures. To address this, we propose a conceptual security framework that integrates a hybrid Convolutional Neural Network-Long Short-Term Memory (CNN-LSTM) model to analyze biometric and environmental data for context-aware security decisions. The CNN extracts spatial patterns from sensor data, while the LSTM captures temporal dynamics associated with human error susceptibility. The model achieves 84% accuracy, demonstrating its ability to reliably detect conditions that lead to elevated human-centred cyber risk. By enabling continuous monitoring and adaptive safeguards, the framework supports proactive interventions that reduce the likelihood of human-driven cyber incidents</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19410v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Duy Anh Ta, Farnaz Farid, Farhad Ahamed, Ala Al-Areqi, Robert Beutel, Tamara Watson, Alana Maurushat</dc:creator>
    </item>
    <item>
      <title>"Write in English, Nobody Understands Your Language Here": A Study of Non-English Trends in Open-Source Repositories</title>
      <link>https://arxiv.org/abs/2602.19446</link>
      <description>arXiv:2602.19446v1 Announce Type: cross 
Abstract: The open-source software (OSS) community has historically been dominated by English as the primary language for code, documentation, and developer interactions. However, with growing global participation and better support for non-Latin scripts through standards like Unicode, OSS is gradually becoming more multilingual. This study investigates the extent to which OSS is becoming more multilingual, analyzing 9.14 billion GitHub issues, pull requests, and discussions, and 62,500 repositories across five programming languages and 30 natural languages, covering the period from 2015 to 2025. We examine six research questions to track changes in language use across communication, code, and documentation. We find that multilingual participation has steadily increased, especially in Korean, Chinese, and Russian. This growth appears not only in issues and discussions but also in code comments, string literals, and documentation files. While this shift reflects greater inclusivity and language diversity in OSS, it also creates language tension. The ability to express oneself in a native language can clash with shared norms around English use, especially in collaborative settings. Non-English or multilingual projects tend to receive less visibility and participation, suggesting that language remains both a resource and a barrier, shaping who gets heard, who contributes, and how open collaboration unfolds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19446v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3744916.3787766</arxiv:DOI>
      <dc:creator>Masudul Hasan Masud Bhuiyan, Manish Kumar Bala Kumar, Cristian-Alexandru Staicu</dc:creator>
    </item>
    <item>
      <title>PuppetChat: Fostering Intimate Communication through Bidirectional Actions and Micronarratives</title>
      <link>https://arxiv.org/abs/2602.19463</link>
      <description>arXiv:2602.19463v1 Announce Type: cross 
Abstract: As a primary channel for sustaining modern intimate relationships, instant messaging facilitates frequent connection across distances. However, today's tools often dilute care; they favor single tap reactions and vague emojis that do not support two way action responses, do not preserve the feeling that the exchange keeps going without breaking, and are weakly tied to who we are and what we share. To address this challenge, we present PuppetChat, a dyadic messaging prototype that restores this expressive depth through embodied interaction. PuppetChat uses a reciprocity aware recommender to encourage responsive actions and generates personalized micronarratives from user stories to ground interactions in personal history. Our 10-day field study with 11 dyads of close partners or friends revealed that this approach enhanced social presence, supported more expressive self disclosure, and sustained continuity and shared memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19463v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3772318.3790685</arxiv:DOI>
      <dc:creator>Emma Jiren Wang, Siying Hu, Zhicong Lu</dc:creator>
    </item>
    <item>
      <title>Sound-first immersive training for blind and low-vision learners: A simulation flow for safe, standardized orientation, mobility, and daily living practice</title>
      <link>https://arxiv.org/abs/2602.19554</link>
      <description>arXiv:2602.19554v1 Announce Type: cross 
Abstract: Orientation and mobility (O&amp;M) instruction for blind and low-vision learners is effective but difficult to standardize and repeat at scale due to the reliance on instructor availability, physical mock-ups, and variable real-world outdoor conditions. This Technical Note presents a sound-first immersive training flow that uses spatial audio and sonification as the primary channel for action and feedback in pre-street O&amp;M and daily-living practice. The approach specifies parameterized scenario templates (e.g., signalized street crossing, public transport boarding, and kitchen tasks), a compact and consistent cue vocabulary with clear spectral placement and timing to mitigate masking, and a lightweight safety protocol enabling graded exposure, content warnings, seated starts, opt-outs, and structured debriefs. The system assumes a head-mounted device with high-quality binaural rendering and head tracking; 3D scene geometry is used as an invisible scaffold to anchor sources, trigger events, define risk/guidance volumes, and govern physically plausible motion without visuals. Session difficulty is shaped via cue density, event tempo, and task complexity while preserving cue consistency to promote transfer across scenarios. The specification aims to enable safe repetition, reduce instructor burden, and support clearer standards across rehabilitation centers, aligning with evidence that audio-first interaction is essential for blind and visually impaired users and addressing gaps in HRTF personalization, evaluation standards, and accessibility integration. Although no behavioral outcomes are reported here, this implementable flow consolidates auditory science with center-ready design, offering a pragmatic foundation for standardized evaluation and future comparative studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19554v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel A. Mu\~noz</dc:creator>
    </item>
    <item>
      <title>The Climate Change Knowledge Graph: Supporting Climate Services</title>
      <link>https://arxiv.org/abs/2602.19786</link>
      <description>arXiv:2602.19786v1 Announce Type: cross 
Abstract: Climate change impacts a broad spectrum of human resources and activities, necessitating the use of climate models to project long-term effects and inform mitigation and adaptation strategies. These models generate multiple datasets by running simulations across various scenarios and configurations, thereby covering a range of potential future outcomes. Currently, researchers rely on traditional search interfaces and APIs to retrieve such datasets, often piecing together information from metadata and community vocabularies. The Climate Change Knowledge Graph is designed to address these challenges by integrating diverse data sources related to climate simulations into a coherent and interoperable knowledge graph. This innovative resource allows for executing complex queries involving climate models, simulations, variables, spatio-temporal domains, and granularities. Developed with input from domain experts, the knowledge graph and its underlying ontology are published with open access license and provide a comprehensive framework that enhances the exploration of climate data, facilitating more informed decision-making in addressing climate change issues.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19786v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Miguel Ceriani, Fiorela Ciroku, Alessandro Russo, Massimiliano Schembri, Fai Fung, Neha Mittal, Vito Trianni, Andrea Giovanni Nuzzolese</dc:creator>
    </item>
    <item>
      <title>Stop Preaching and Start Practising Data Frugality for Responsible Development of AI</title>
      <link>https://arxiv.org/abs/2602.19789</link>
      <description>arXiv:2602.19789v1 Announce Type: cross 
Abstract: This position paper argues that the machine learning community must move from preaching to practising data frugality for responsible artificial intelligence (AI) development. For long, progress has been equated with ever-larger datasets, driving remarkable advances but now yielding increasingly diminishing performance gains alongside rising energy use and carbon emissions. While awareness of data frugal approaches has grown, their adoption has remained rhetorical, and data scaling continues to dominate development practice. We argue that this gap between preach and practice must be closed, as continued data scaling entails substantial and under-accounted environmental impacts. To ground our position, we provide indicative estimates of the energy use and carbon emissions associated with the downstream use of ImageNet-1K. We then present empirical evidence that data frugality is both practical and beneficial, demonstrating that coreset-based subset selection can substantially reduce training energy consumption with little loss in accuracy, while also mitigating dataset bias. Finally, we outline actionable recommendations for moving data frugality from rhetorical preach to concrete practice for responsible development of AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19789v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophia N. Wilson, Gu{\dh}r\'un Fj\'ola Gu{\dh}mundsd\'ottir, Andrew Millard, Raghavendra Selvan, Sebastian Mair</dc:creator>
    </item>
    <item>
      <title>Assessing Risks of Large Language Models in Mental Health Support: A Framework for Automated Clinical AI Red Teaming</title>
      <link>https://arxiv.org/abs/2602.19948</link>
      <description>arXiv:2602.19948v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly utilized for mental health support; however, current safety benchmarks often fail to detect the complex, longitudinal risks inherent in therapeutic dialogue. We introduce an evaluation framework that pairs AI psychotherapists with simulated patient agents equipped with dynamic cognitive-affective models and assesses therapy session simulations against a comprehensive quality of care and risk ontology. We apply this framework to a high-impact test case, Alcohol Use Disorder, evaluating six AI agents (including ChatGPT, Gemini, and Character.AI) against a clinically-validated cohort of 15 patient personas representing diverse clinical phenotypes.
  Our large-scale simulation (N=369 sessions) reveals critical safety gaps in the use of AI for mental health support. We identify specific iatrogenic risks, including the validation of patient delusions ("AI Psychosis") and failure to de-escalate suicide risk. Finally, we validate an interactive data visualization dashboard with diverse stakeholders, including AI engineers and red teamers, mental health professionals, and policy experts (N=9), demonstrating that this framework effectively enables stakeholders to audit the "black box" of AI psychotherapy. These findings underscore the critical safety risks of AI-provided mental health support and the necessity of simulation-based clinical red teaming before deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.19948v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.MA</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Steenstra, Paola Pedrelli, Weiyan Shi, Stacy Marsella, Timothy W. Bickmore</dc:creator>
    </item>
    <item>
      <title>Agents of Chaos</title>
      <link>https://arxiv.org/abs/2602.20021</link>
      <description>arXiv:2602.20021v1 Announce Type: cross 
Abstract: We report an exploratory red-teaming study of autonomous language-model-powered agents deployed in a live laboratory environment with persistent memory, email accounts, Discord access, file systems, and shell execution. Over a two-week period, twenty AI researchers interacted with the agents under benign and adversarial conditions. Focusing on failures emerging from the integration of language models with autonomy, tool use, and multi-party communication, we document eleven representative case studies. Observed behaviors include unauthorized compliance with non-owners, disclosure of sensitive information, execution of destructive system-level actions, denial-of-service conditions, uncontrolled resource consumption, identity spoofing vulnerabilities, cross-agent propagation of unsafe practices, and partial system takeover. In several cases, agents reported task completion while the underlying system state contradicted those reports. We also report on some of the failed attempts. Our findings establish the existence of security-, privacy-, and governance-relevant vulnerabilities in realistic deployment settings. These behaviors raise unresolved questions regarding accountability, delegated authority, and responsibility for downstream harms, and warrant urgent attention from legal scholars, policymakers, and researchers across disciplines. This report serves as an initial empirical contribution to that broader conversation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.20021v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Natalie Shapira, Chris Wendler, Avery Yen, Gabriele Sarti, Koyena Pal, Olivia Floody, Adam Belfki, Alex Loftus, Aditya Ratan Jannali, Nikhil Prakash, Jasmine Cui, Giordano Rogers, Jannik Brinkmann, Can Rager, Amir Zur, Michael Ripa, Aruna Sankaranarayanan, David Atkinson, Rohit Gandikota, Jaden Fiotto-Kaufman, EunJeong Hwang, Hadas Orgad, P Sam Sahil, Negev Taglicht, Tomer Shabtay, Atai Ambus, Nitay Alon, Shiri Oron, Ayelet Gordon-Tapiero, Yotam Kaplan, Vered Shwartz, Tamar Rott Shaham, Christoph Riedl, Reuth Mirsky, Maarten Sap, David Manheim, Tomer Ullman, David Bau</dc:creator>
    </item>
    <item>
      <title>The Problem of Algorithmic Collisions: Mitigating Unforeseen Risks in a Connected World</title>
      <link>https://arxiv.org/abs/2505.20181</link>
      <description>arXiv:2505.20181v2 Announce Type: replace 
Abstract: The increasing deployment of Artificial Intelligence (AI) and other autonomous algorithmic systems presents the world with new systemic risks. While focus often lies on the function of individual algorithms, a critical and underestimated danger arises from their interactions, particularly when algorithmic systems operate without awareness of each other, or when those deploying them are unaware of the full algorithmic ecosystem deployment is occurring in. These interactions can lead to unforeseen, rapidly escalating negative outcomes - from market crashes and energy supply disruptions to potential physical accidents and erosion of public trust - often exceeding the human capacity for effective monitoring and the legal capacities for proper intervention. Current governance frameworks are inadequate as they lack visibility into this complex ecosystem of interactions. This paper outlines the nature of this challenge and proposes some initial policy suggestions centered on increasing transparency and accountability through phased system registration, a licensing framework for deployment, and enhanced monitoring capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.20181v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>math.HO</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maurice Chiodo, Dennis M\"uller</dc:creator>
    </item>
    <item>
      <title>CogniAlign: Survivability-Grounded Multi-Agent Moral Reasoning for Safe and Transparent AI</title>
      <link>https://arxiv.org/abs/2509.13356</link>
      <description>arXiv:2509.13356v2 Announce Type: replace 
Abstract: The challenge of aligning artificial intelligence (AI) with human values persists due to the abstract and often conflicting nature of moral principles and the opacity of existing approaches. This paper introduces CogniAlign, a multi-agent deliberation framework based on naturalistic moral realism, that grounds moral reasoning in survivability, defined across individual and collective dimensions, and operationalizes it through structured deliberations among discipline-specific scientist agents. Each agent, representing neuroscience, psychology, sociology, and evolutionary biology, provides arguments and rebuttals that are synthesized by an arbiter into transparent and empirically anchored judgments. As a proof-of-concept study, we evaluate CogniAlign on classic and novel moral questions and compare its outputs against GPT-4o using a five-part ethical audit framework with the help of three experts. Results show that CogniAlign consistently outperforms the baseline across more than sixty moral questions, with average performance gains of 12.2 points in analytic quality, 31.2 points in decisiveness, and 15 points in depth of explanation. In the Heinz dilemma, for example, CogniAlign achieved an overall score of 79 compared to GPT-4o's 65.8, demonstrating a decisive advantage in handling moral reasoning. Through transparent and structured reasoning, CogniAlign demonstrates the feasibility of an auditable approach to AI alignment, though certain challenges still remain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13356v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hasin Jawad Ali, Ilhamul Azam, Ajwad Abrar, Md. Kamrul Hasan, Hasan Mahmud</dc:creator>
    </item>
    <item>
      <title>When AI Democratizes Exploitation: LLM-Assisted Strategic Manipulation of Fair Division Algorithms</title>
      <link>https://arxiv.org/abs/2511.14722</link>
      <description>arXiv:2511.14722v2 Announce Type: replace 
Abstract: Fair resource division algorithms, like those implemented in Spliddit platform, have traditionally been considered difficult for the end users to manipulate due to its complexities. This paper demonstrates how Large Language Models (LLMs) can dismantle these protective barriers by democratizing access to strategic expertise. Through empirical analysis of rent division scenarios on Spliddit algorithms, we show that users can obtain actionable manipulation strategies via simple conversational queries to AI assistants. We present four distinct manipulation scenarios: exclusionary collusion where majorities exploit minorities, defensive counterstrategies that backfire, benevolent subsidization of specific participants, and cost minimization coalitions. Our experiments reveal that LLMs can explain algorithmic mechanics, identify profitable deviations, and generate specific numerical inputs for coordinated preference misreporting--capabilities previously requiring deep technical knowledge. These findings extend algorithmic collective action theory from classification contexts to resource allocation scenarios, where coordinated preference manipulation replaces feature manipulation. The implications reach beyond rent division to any domain using algorithmic fairness mechanisms for resource division. While AI-enabled manipulation poses risks to system integrity, it also creates opportunities for preferential treatment of equity deserving groups. We argue that effective responses must combine algorithmic robustness, participatory design, and equitable access to AI capabilities, acknowledging that strategic sophistication is no longer a scarce resource.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14722v2</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Priyanka Verma, Balagopal Unnikrishnan</dc:creator>
    </item>
    <item>
      <title>A Technical Policy Blueprint for Trustworthy Decentralized AI</title>
      <link>https://arxiv.org/abs/2512.11878</link>
      <description>arXiv:2512.11878v2 Announce Type: replace 
Abstract: Decentralized AI systems, such as federated learning, can play a critical role in further unlocking AI asset marketplaces (e.g., healthcare data marketplaces) thanks to increased asset privacy protection. Unlocking this big potential necessitates governance mechanisms that are transparent, scalable, and verifiable. However current governance approaches rely on bespoke, infrastructure-specific policies that hinder asset interoperability and trust among systems. We are proposing a Technical Policy Blueprint that encodes governance requirements as policy-as-code objects and separates asset policy verification from asset policy enforcement. In this architecture the Policy Engine verifies evidence (e.g., identities, signatures, payments, trusted-hardware attestations) and issues capability packages. Asset Guardians (e.g. data guardians, model guardians, computation guardians, etc.) enforce access or execution solely based on these capability packages. This core concept of decoupling policy processing from capabilities enables governance to evolve without reconfiguring AI infrastructure, thus creating an approach that is transparent, auditable, and resilient to change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11878v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hasan Kassem, Orion Banks, Sergen Cansiz, Brandon Edwards, Patrick Foley, Inken Hagestedt, Taeho Jung, Prakash Moorthy, Michael O'Connor, Marco Lorenzi, Ann K Novakowski, Bruno Rodrigues, Holger Roth, Micah Sheller, Dimitris Stripelis, Marc Vesin, Renato Umeton, Mic Bowman, Alexandros Karargyris</dc:creator>
    </item>
    <item>
      <title>Brokerage in the Black Box: Swing States, Strategic Ambiguity, and the Global Politics of AI Governance</title>
      <link>https://arxiv.org/abs/2601.06412</link>
      <description>arXiv:2601.06412v2 Announce Type: replace 
Abstract: The United States-China rivalry has placed frontier dual-use technologies, particularly Artificial Intelligence (AI), at the center of global power dynamics, as techno-nationalism, supply chain securitization, and competing standards deepen bifurcation within a weaponized interdependence that blurs civilian-military boundaries. Existing research, yet, mostly emphasizes superpower strategies and often overlooks the role of middle powers as crucial actors shaping the global techno-order. This study examines Technological Swing States (TSS), middle powers with both technological capacity and strategic flexibility, and their ability to navigate the frontier technologies' uncertainty and opacity to mediate great-power techno-competition regionally and globally. It reconceptualizes AI opacity not merely as a technical deficit, but as a structural feature and strategic resource, stemming from algorithmic complexity, political incentives that prioritize performance over explainability, and the limits of post-hoc interpretability. This structural opacity shifts authority from technical demands for explainability to institutional mechanisms, such as certification, auditing, and disclosure, converting technical constraints into strategic political opportunities. Drawing on case studies of South Korea, Singapore, and India, the paper theorizes how TSS exploit the interplay between opacity and institutional transparency through three strategies: (i) delay and hedging, (ii) selective alignment, and (iii) normative intermediation. These practices enable TSS to preserve strategic flexibility, build trust among diverse stakeholders, and broker convergence across competing governance regimes, thereby influencing institutional design, interstate bargaining, and policy outcomes in global AI governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.06412v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ha-Chi Tran</dc:creator>
    </item>
    <item>
      <title>Influence of Normative Theories of Ethics on the European Union Artificial Intelligence Act: A Transformer-Based Analysis Using Semantic Textual Similarity</title>
      <link>https://arxiv.org/abs/2601.13372</link>
      <description>arXiv:2601.13372v3 Announce Type: replace 
Abstract: Despite being regarded as a significant step toward regulating Artificial Intelligence (AI) systems and its emphasis on fundamental rights, the European Union Artificial Intelligence (EU AI) Act is not immune to moral criticism. This research aims to investigate the impact of three major normative theories of ethics (virtue ethics, deontological ethics, and consequentialism) on the EU AI Act. We introduce the concept of influence, confirmed by philosophical and chronological analysis, to examine the underlying relationship between the theories and the Act. As a proxy measure of this influence, we propose using Semantic Textual Similarity (STS) to quantify the degree of alignment between the theories (influencers) and the Act (influencee). To capture intentional and operational ethical consistency, the Act was divided into two parts: the preamble and the statutory provisions. The textual descriptions of the theories were manually preprocessed to reduce semantic overlap and ensure a distinct representation of each theory. A heterogeneous embedding-level ensemble approach was employed, utilizing five modified Bidirectional Encoder Representations from Transformers (BERT) models, built on the Transformer architecture, to compute STS scores. These scores represent the semantic alignment between various theories of ethics and each of the two components of the EU AI Act. The theories were evaluated by using voting and averaging, with findings indicating that deontological ethics has the most significant overall influence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13372v3</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehmet Murat Albayrakoglu, Mehmet Nafiz Aydin</dc:creator>
    </item>
    <item>
      <title>Governing Social Media as a Public Utility</title>
      <link>https://arxiv.org/abs/2602.12535</link>
      <description>arXiv:2602.12535v2 Announce Type: replace 
Abstract: Social media platforms connect billions, but their business models often amplify societal harm through misinformation, which is linked to polarization, violence, and declining mental health. Current governance frameworks, such as the U.S. Section 230 and the EU Digital Services Act, delegate content moderation to corporations. This creates structural conflicts of interest because misinformation drives engagement, and engagement drives profit. We propose a public utility model for social media governance that prioritizes the public good over commercial incentives. Integrating legislated content removal with democratic content moderation, the model protects free expression while mitigating societal harms. It frames social media as sovereign digital infrastructure governed through democratic oversight, transparent algorithms, and institutional safeguards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12535v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3788880</arxiv:DOI>
      <arxiv:journal_reference>Communications of the ACM, 2026</arxiv:journal_reference>
      <dc:creator>Christoph Mueller-Bloch, Raffaele Ciriello</dc:creator>
    </item>
    <item>
      <title>Buy versus Build an LLM: A Decision Framework for Governments</title>
      <link>https://arxiv.org/abs/2602.13033</link>
      <description>arXiv:2602.13033v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) represent a new frontier of digital infrastructure that can support a wide range of public-sector applications, from general purpose citizen services to specialized and sensitive state functions. When expanding AI access, governments face a set of strategic choices over whether to buy existing services, build domestic capabilities, or adopt hybrid approaches across different domains and use cases. These are critical decisions especially when leading model providers are often foreign corporations, and LLM outputs are increasingly treated as trusted inputs to public decision-making and public discourse. In practice, these decisions are not intended to mandate a single approach across all domains; instead, national AI strategies are typically pluralistic, with sovereign, commercial and open-source models coexisting to serve different purposes. Governments may rely on commercial models for non-sensitive or commodity tasks, while pursuing greater control for critical, high-risk or strategically important applications.
  This paper provides a strategic framework for making this decision by evaluating these options across dimensions including sovereignty, safety, cost, resource capability, cultural fit, and sustainability. Importantly, "building" does not imply that governments must act alone: domestic capabilities may be developed through public research institutions, universities, state-owned enterprises, joint ventures, or broader national ecosystems. By detailing the technical requirements and practical challenges of each pathway, this work aims to serve as a reference for policy-makers to determine whether a buy or build approach best aligns with their specific national needs and societal goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13033v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.SI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Lu, Ziwei Xu, William Tjhi, Junnan Li, Antoine Bosselut, Pang Wei Koh, Mohan Kankanhalli</dc:creator>
    </item>
    <item>
      <title>How should AI knowledge be governed? Epistemic authority, structural transparency, and the case for open cognitive graphs</title>
      <link>https://arxiv.org/abs/2602.16949</link>
      <description>arXiv:2602.16949v2 Announce Type: replace 
Abstract: Through widespread use in formative assessment and self-directed learning, educational AI systems exercise de facto epistemic authority. Unlike human educators, however, these systems are not embedded in institutional mechanisms of accountability, review, and correction, creating a structural governance challenge that cannot be resolved through application-level regulation or model transparency alone. This paper reconceptualizes educational AI as public educational cognitive infrastructure and argues that its governance must address the epistemic authority such systems exert. We propose the Open Cognitive Graph (OCG) as a technical interface that externalizes pedagogical structure in forms aligned with human educational reasoning. By explicitly representing concepts, prerequisite relations, misconceptions, and scaffolding, OCGs make the cognitive logic governing AI behaviour inspectable and revisable. Building on this foundation, we introduce the trunk-branch governance model, which organizes epistemic authority across layers of consensus and pluralism. A case study of a community-governed educational foundation model demonstrates how distributed expertise can be integrated through institutionalized processes of validation, correction, and propagation. The paper concludes by discussing implications for educational equity, AI policy, and sustainability. By shifting attention from access to governance conditions, the proposed framework offers a structural approach to aligning educational AI with democratic accountability and public responsibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16949v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chao Li (School of Information Science and Technology, Northeast Normal University, China), Chunyi Zhao (Centre of Educational Design and Innovation, University of Otago, New Zealand), Yuru Wang (School of Information Science and Technology, Northeast Normal University, China), Yi Hu (School of Information Science and Engineering, Southeast University, China)</dc:creator>
    </item>
    <item>
      <title>Preserving Historical Truth: Detecting Historical Revisionism in Large Language Models</title>
      <link>https://arxiv.org/abs/2602.17433</link>
      <description>arXiv:2602.17433v2 Announce Type: replace 
Abstract: Large language models (LLMs) are increasingly used as sources of historical information, motivating the need for scalable audits on contested events and politically charged narratives in settings that mirror real user interactions. We introduce \texttt{HistoricalMisinfo, a curated dataset of $500$ contested events from $45$ countries, each paired with a factual reference narrative and a documented revisionist reference narrative. To approximate real-world usage, we instantiate each event in $11$ prompt scenarios that reflect common communication settings (e.g., questions, textbooks, social posts, policy briefs). Using an LLM-as-a-judge protocol that compares model outputs to the two references, we evaluate LLMs varying across model architectures in two conditions: (i) neutral user prompts that ask for factually accurate information, and (ii) robustness prompts in which the user explicitly requests the revisionist version of the event. Under neutral prompts, models are generally closer to factual references, though the resulting scores should be interpreted as reference-alignment signals rather than definitive evidence of human-interpretable revisionism. Robustness prompting yields a strong and consistent effect: when the user requests the revisionist narrative, all evaluated models show sharply higher revisionism scores, indicating limited resistance or self-correction. HistoricalMisinfo provides a practical foundation for benchmarking robustness to revisionist framing and for guiding future work on more precise automatic evaluation of contested historical claims to ensure a sustainable integration of AI systems within society. Our code is available at https://github.com/francescortu/PreservingHistoricalTruth</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.17433v2</guid>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Francesco Ortu, Joeun Yook, Punya Syon Pandey, Keenan Samway, Bernhard Sch\"olkopf, Alberto Cazzaniga, Rada Mihalcea, Zhijing Jin</dc:creator>
    </item>
    <item>
      <title>Reducing Biases in Record Matching Through Scores Calibration</title>
      <link>https://arxiv.org/abs/2411.01685</link>
      <description>arXiv:2411.01685v3 Announce Type: replace-cross 
Abstract: Record matching models typically output a real-valued matching score that is later consumed through thresholding, ranking, or human review. While fairness in record matching has mostly been assessed using binary decisions at a fixed threshold, such evaluations can miss systematic disparities in the entire score distribution and can yield conclusions that change with the chosen threshold. We introduce a threshold-independent notion of score bias that extends standard group-fairness criteria-demographic parity (DP), equal opportunity (EO), and equalized odds (EOD)-from binary outputs to score functions by integrating group-wise metric gaps over all thresholds. Using this metric, we empirically show that several state-of-the-art deep matchers can exhibit substantial score bias even when appearing fair at commonly used thresholds. To mitigate these disparities without retraining the underlying matcher, we propose two model-agnostic post-processing methods that only require score evaluations on an (unlabeled) calibration set. Calib targets DP by aligning minority/majority score distributions to a common Wasserstein barycenter via a quantile-based optimal-transport map, with finite-sample guarantees on both residual DP bias and score distortion. C-Calib extends this idea to label-dependent notions (EO/EOD) by performing barycenter alignment conditionally on an estimated label, and we characterize how its guarantees depend on both sample size and label-estimation error. Experiments on standard record-matching benchmarks and multiple neural matchers confirm that Calib and C-Calib substantially reduce score bias with minimal loss in accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01685v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Hossein Moslemi, Mostafa Milani</dc:creator>
    </item>
    <item>
      <title>Grassroots Federation: Fair Democratic Governance at Scale</title>
      <link>https://arxiv.org/abs/2505.02208</link>
      <description>arXiv:2505.02208v5 Announce Type: replace-cross 
Abstract: We propose a framework for the fair democratic governance of federated digital communities that form and evolve dynamically, where small groups self-govern and larger groups are represented by assemblies selected via sortition. Prior work addressed static fairness conditions; here, we formalize a dynamic setting where federations evolve over time through communities forming, joining, and splitting, in all directions-bottom-up, top-down, and middle-out-and adapt the fairness guarantees. The main technical challenge is reconciling integral seat allocations with dynamic, overlapping federations, so that child communities always meet their persistent floors while long-run averages converge to proportional fairness. Overcoming these challenges, we introduce a protocol that ensures fair participation and representation both persistently (at all times) and eventually (in the limit after stabilization), extending the static fairness properties to handle structural changes.
  Prior work shows that grassroots federations can be specified via atomic transactions among assembly members, and that Constitutional Consensus can realize both these transactions and the democratic processes leading to them. Together, the four works form a complete design for an egalitarian, fairly governed, large-scale decentralized sovereign digital community platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02208v5</guid>
      <category>cs.DC</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehud Shapiro, Nimrod Talmon</dc:creator>
    </item>
    <item>
      <title>FairSHAP: Preprocessing for Fairness Through Attribution-Based Data Augmentation</title>
      <link>https://arxiv.org/abs/2505.11111</link>
      <description>arXiv:2505.11111v3 Announce Type: replace-cross 
Abstract: Ensuring fairness in machine learning models is critical, particularly in high-stakes domains where biased decisions can lead to serious societal consequences. Existing preprocessing approaches generally lack transparent mechanisms for identifying which features or instances are responsible for unfairness. This obscures the rationale behind data modifications. We introduce FairSHAP, a novel pre-processing framework that leverages Shapley value attribution to improve both individual and group fairness. FairSHAP identifies fairness-critical instances in the training data using an interpretable measure of feature importance, and systematically modifies them through instance-level matching across sensitive groups. This process reduces discriminative risk - an individual fairness metric - while preserving data integrity and model accuracy. We demonstrate that FairSHAP significantly improves demographic parity and equality of opportunity across diverse tabular datasets, achieving fairness gains with minimal data perturbation and, in some cases, improved predictive performance. As a model-agnostic and transparent method, FairSHAP integrates seamlessly into existing machine learning pipelines and provides actionable insights into the sources of bias.Our code is on https://github.com/ZhuMuMu0216/FairSHAP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11111v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lin Zhu, Yijun Bian, Lei You</dc:creator>
    </item>
    <item>
      <title>Decoding Tourist Perception in Historic Urban Quarters with Multimodal Social Media Data: An AI-Based Framework and Evidence from Shanghai</title>
      <link>https://arxiv.org/abs/2509.03830</link>
      <description>arXiv:2509.03830v3 Announce Type: replace-cross 
Abstract: Historic urban quarters are increasingly shaped by tourism and lifestyle consumption, yet planners often lack scalable evidence on what visitors notice, prefer, and criticize in these environments. This study proposes an AI-based, multimodal framework to decode tourist perception by combining visual attention, color-based aesthetic representation, and multidimensional satisfaction. We collect geotagged photos and review texts from a major Chinese platform and assemble a street view image set as a baseline for comparison across 12 historic urban quarters in Shanghai. We train a semantic segmentation model to quantify foregrounded visual elements in tourist-shared imagery, extract and compare color palettes between social media photos and street views, and apply a multi-task sentiment classifier to assess satisfaction across four experience dimensions that correspond to activity, physical setting, supporting services, and commercial offerings. Results show that tourist photos systematically foreground key streetscape elements and that the color composition represented on social media can differ from on-site street views, indicating a perception-reality gap that varies by quarter. The framework offers an interpretable and transferable approach to diagnose such gaps and to inform heritage management and visitor-oriented urban design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03830v3</guid>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizhen Tan, Yufan Wu, Yuxuan Liu, Haoran Zeng</dc:creator>
    </item>
    <item>
      <title>Cost Efficient Fairness Audit Under Partial Feedback</title>
      <link>https://arxiv.org/abs/2510.03734</link>
      <description>arXiv:2510.03734v2 Announce Type: replace-cross 
Abstract: We study the problem of auditing the fairness of a given classifier under partial feedback, where true labels are available only for positively classified individuals, (e.g., loan repayment outcomes are observed only for approved applicants). We introduce a novel cost model for acquiring additional labeled data, designed to more accurately reflect real-world costs such as credit assessment, loan processing, and potential defaults. Our goal is to find optimal fairness audit algorithms that are more cost-effective than random exploration and natural baselines.
  In our work, we consider two audit settings: a black-box model with no assumptions on the data distribution, and a mixture model, where features and true labels follow a mixture of exponential family distributions. In the black-box setting, we propose a near-optimal auditing algorithm under mild assumptions and show that a natural baseline can be strictly suboptimal. In the mixture model setting, we design a novel algorithm that achieves significantly lower audit cost than the black-box case. Our approach leverages prior work on learning from truncated samples and maximum-a-posteriori oracles, and extends known results on spherical Gaussian mixtures to handle exponential family mixtures, which may be of independent interest. Moreover, our algorithms apply to popular fairness metrics including demographic parity, equal opportunity, and equalized odds. Empirically, we demonstrate strong performance of our algorithms on real-world fair classification datasets like Adult Income and Law School, consistently outperforming natural baselines by around 50% in terms of audit cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03734v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ML</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nirjhar Das, Mohit Sharma, Praharsh Nanavati, Kirankumar Shiragur, Amit Deshpande</dc:creator>
    </item>
    <item>
      <title>Detecting Early and Implicit Suicidal Ideation via Longitudinal and Information Environment Signals on Social Media</title>
      <link>https://arxiv.org/abs/2510.14889</link>
      <description>arXiv:2510.14889v3 Announce Type: replace-cross 
Abstract: On social media, several individuals experiencing suicidal ideation (SI) do not disclose their distress explicitly. Instead, signs may surface indirectly through everyday posts or peer interactions. Detecting such implicit signals early is critical but remains challenging. We frame early and implicit SI as a forward-looking prediction task and develop a computational framework that models a user's information environment, consisting of both their longitudinal posting histories as well as the discourse of their socially proximal peers. We adopted a composite network centrality measure to identify top neighbors of a user, and temporally aligned the user's and neighbors' interactions -- integrating the multi-layered signals in a fine-tuned DeBERTa-v3 model. In a Reddit study of 1,000 (500 Case and 500 Control) users, our approach improves early and implicit SI detection by an average of 10% over all other baselines. These findings highlight that peer interactions offer valuable predictive signals and carry broader implications for designing early detection systems that capture indirect as well as masked expressions of risk in online environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14889v3</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 18th ACM Conference on Web Science (WebSci), 2026</arxiv:journal_reference>
      <dc:creator>Soorya Ram Shimgekar, Ruining Zhao, Agam Goyal, Violeta J. Rodriguez, Paul A. Bloom, Navin Kumar, Hari Sundaram, Koustuv Saha</dc:creator>
    </item>
    <item>
      <title>A Domain-Adapted Pipeline for Structured Information Extraction from Police Incident Announcements on Social Media</title>
      <link>https://arxiv.org/abs/2512.16183</link>
      <description>arXiv:2512.16183v2 Announce Type: replace-cross 
Abstract: Structured information extraction from police incident announcements is crucial for timely and accurate data processing, yet presents considerable challenges due to the variability and informal nature of textual sources such as social media posts. To address these challenges, we developed a domain-adapted extraction pipeline that leverages targeted prompt engineering with parameter-efficient fine-tuning of the Qwen2.5-7B model using Low-Rank Adaptation (LoRA). This approach enables the model to handle noisy, heterogeneous text while reliably extracting 15 key fields, including location, event characteristics, and impact assessment, from a high-quality, manually annotated dataset of 4,933 instances derived from 27,822 police briefing posts on Chinese Weibo (2019-2020). Experimental results demonstrated that LoRA-based fine-tuning significantly improved performance over both the base and instruction-tuned models, achieving an accuracy exceeding 98.36% for mortality detection and Exact Match Rates of 95.31% for fatality counts and 95.54% for province-level location extraction. The proposed pipeline thus provides a validated and efficient solution for multi-task structured information extraction in specialized domains, offering a practical framework for transforming unstructured text into reliable structured data in social science research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16183v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Mengfan Shen, Kangqi Song, Xindi Wang, Wei Jia, Tao Wang, Ziqiang Han</dc:creator>
    </item>
    <item>
      <title>The Algorithmic Self-Portrait: Deconstructing Memory in ChatGPT</title>
      <link>https://arxiv.org/abs/2602.01450</link>
      <description>arXiv:2602.01450v3 Announce Type: replace-cross 
Abstract: To enable personalized and context-aware interactions, conversational AI systems have introduced a new mechanism: Memory. Memory creates what we refer to as the Algorithmic Self-portrait - a new form of personalization derived from users' self-disclosed information divulged within private conversations. While memory enables more coherent exchanges, the underlying processes of memory creation remain opaque, raising critical questions about data sensitivity, user agency, and the fidelity of the resulting portrait.
  To bridge this research gap, we analyze 2,050 memory entries from 80 real-world ChatGPT users. Our analyses reveal three key findings: (1) A striking 96% of memories in our dataset are created unilaterally by the conversational system, potentially shifting agency away from the user; (2) Memories, in our dataset, contain a rich mix of GDPR-defined personal data (in 28% memories) along with psychological insights about participants (in 52% memories); and (3)~A significant majority of the memories (84%) are directly grounded in user context, indicating faithful representation of the conversations. Finally, we introduce a framework-Attribution Shield-that anticipates these inferences, alerts about potentially sensitive memory inferences, and suggests query reformulations to protect personal information without sacrificing utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.01450v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abhisek Dash, Soumi Das, Elisabeth Kirsten, Qinyuan Wu, Sai Keerthana Karnam, Krishna P. Gummadi, Thorsten Holz, Muhammad Bilal Zafar, Savvas Zannettou</dc:creator>
    </item>
    <item>
      <title>Wisdom of the LLM Crowd: A Large Scale Benchmark of Multi-Label U.S. Election-Related Harmful Social Media Content</title>
      <link>https://arxiv.org/abs/2602.11962</link>
      <description>arXiv:2602.11962v2 Announce Type: replace-cross 
Abstract: The spread of election misinformation and harmful political content conveys misleading narratives and poses a serious threat to democratic integrity. Detecting harmful content at early stages is essential for understanding and potentially mitigating its downstream spread. In this study, we introduce USE24-XD, a large-scale dataset of nearly 100k posts collected from X (formerly Twitter) during the 2024 U.S. presidential election cycle, enriched with spatio-temporal metadata. To substantially reduce the cost of manual annotation while enabling scalable categorization, we employ six large language models (LLMs) to systematically annotate posts across five nuanced categories: Conspiracy, Sensationalism, Hate Speech, Speculation, and Satire. We validate LLM annotations with crowdsourcing (n = 34) and benchmark them against human annotators. Inter-rater reliability analyses show comparable agreement patterns between LLMs and humans, with LLMs exhibiting higher internal consistency and achieving up to 0.90 recall on Speculation. We apply a wisdom-of-the-crowd approach across LLMs to aggregate annotations and curate a robust multi-label dataset. 60% of posts receive at least one label. We further analyze how human annotator demographics, including political ideology and affiliation, shape labeling behavior, highlighting systematic sources of subjectivity in judgments of harmful content. The USE24-XD dataset is publicly released to support future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11962v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qile Wang, Prerana Khatiwada, Carolina Coimbra Vieira, Benjamin E. Bagozzi, Kenneth E. Barner, Matthew Louis Mauriello</dc:creator>
    </item>
    <item>
      <title>ForesightSafety Bench: A Frontier Risk Evaluation and Governance Framework towards Safe AI</title>
      <link>https://arxiv.org/abs/2602.14135</link>
      <description>arXiv:2602.14135v3 Announce Type: replace-cross 
Abstract: Rapidly evolving AI exhibits increasingly strong autonomy and goal-directed capabilities, accompanied by derivative systemic risks that are more unpredictable, difficult to control, and potentially irreversible. However, current AI safety evaluation systems suffer from critical limitations such as restricted risk dimensions and failed frontier risk detection. The lagging safety benchmarks and alignment technologies can hardly address the complex challenges posed by cutting-edge AI models. To bridge this gap, we propose the "ForesightSafety Bench" AI Safety Evaluation Framework, beginning with 7 major Fundamental Safety pillars and progressively extends to advanced Embodied AI Safety, AI4Science Safety, Social and Environmental AI risks, Catastrophic and Existential Risks, as well as 8 critical industrial safety domains, forming a total of 94 refined risk dimensions. To date, the benchmark has accumulated tens of thousands of structured risk data points and assessment results, establishing a widely encompassing, hierarchically clear, and dynamically evolving AI safety evaluation framework. Based on this benchmark, we conduct systematic evaluation and in-depth analysis of over twenty mainstream advanced large models, identifying key risk patterns and their capability boundaries. The safety capability evaluation results reveals the widespread safety vulnerabilities of frontier AI across multiple pillars, particularly focusing on Risky Agentic Autonomy, AI4Science Safety, Embodied AI Safety, Social AI Safety and Catastrophic and Existential Risks. Our benchmark is released at https://github.com/Beijing-AISI/ForesightSafety-Bench. The project website is available at https://foresightsafety-bench.beijing-aisi.ac.cn/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14135v3</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haibo Tong, Feifei Zhao, Linghao Feng, Ruoyu Wu, Ruolin Chen, Lu Jia, Zhou Zhao, Jindong Li, Tenglong Li, Erliang Lin, Shuai Yang, Enmeng Lu, Yinqian Sun, Qian Zhang, Zizhe Ruan, Jinyu Fan, Zeyang Yue, Ping Wu, Huangrui Li, Chengyi Sun, Yi Zeng</dc:creator>
    </item>
    <item>
      <title>Towards a Science of AI Agent Reliability</title>
      <link>https://arxiv.org/abs/2602.16666</link>
      <description>arXiv:2602.16666v2 Announce Type: replace-cross 
Abstract: AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.16666v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Tue, 24 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan</dc:creator>
    </item>
  </channel>
</rss>
