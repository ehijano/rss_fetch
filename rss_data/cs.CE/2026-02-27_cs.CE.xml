<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 27 Feb 2026 05:00:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An automatic counting algorithm for the quantification and uncertainty analysis of the number of microglial cells trainable in small and heterogeneous datasets</title>
      <link>https://arxiv.org/abs/2602.22974</link>
      <description>arXiv:2602.22974v1 Announce Type: new 
Abstract: Counting immunopositive cells on biological tissues generally requires either manual annotation or (when available) automatic rough systems, for scanning signal surface and intensity in whole slide imaging. In this work, we tackle the problem of counting microglial cells in lumbar spinal cord cross-sections of rats by omitting cell detection and focusing only on the counting task. Manual cell counting is, however, a time-consuming task and additionally entails extensive personnel training. The classic automatic color-based methods roughly inform about the total labeled area and intensity (protein quantification) but do not specifically provide information on cell number. Since the images to be analyzed have a high resolution but a huge amount of pixels contain just noise or artifacts, we first perform a pre-processing generating several filtered images {(providing a tailored, efficient feature extraction)}. Then, we design an automatic kernel counter that is a non-parametric and non-linear method. The proposed scheme can be easily trained in small datasets since, in its basic version, it relies only on one hyper-parameter. However, being non-parametric and non-linear, the proposed algorithm is flexible enough to express all the information contained in rich and heterogeneous datasets as well (providing the maximum overfit if required). Furthermore, the proposed kernel counter also provides uncertainty estimation of the given prediction, and can directly tackle the case of receiving several expert opinions over the same image. Different numerical experiments with artificial and real datasets show very promising results. Related Matlab code is also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22974v1</guid>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.eswa.2025.129208</arxiv:DOI>
      <arxiv:journal_reference>Expert Systems With Applications, Volume 296, Part D, 2026. Num. 129208</arxiv:journal_reference>
      <dc:creator>L. Martino, M. M. Garcia, P. S. Paradas, E. Curbelo</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Calculation of Analytical Gradients of Matrix-Interpolatory Reduced-Order Models for Efficient Structural Optimization</title>
      <link>https://arxiv.org/abs/2602.23314</link>
      <description>arXiv:2602.23314v1 Announce Type: new 
Abstract: This paper presents an adaptive sampling algorithm tailored for the optimization of parametrized dynamical systems using projection-based model order reduction. Unlike classical sampling strategies, this framework does not aim for a small approximation error in the global sense but focuses on identifying and refining promising regions early on while reducing expensive full order model evaluations. The algorithm is tested on two models: a Timoshenko beam and a Kelvin cell, which ought to be optimized in terms of the system output in the frequency domain. For that, different norms of the transfer function are used as the objective function, while up to two geometrical parameters form the vector of design variables. The sampled full order models are reduced using the iterative rational Krylov algorithm and reprojected into a global basis. Subsequently, the models are parametrized by performing sparse Bayesian regression on matrix entry level of the reduced operators. Thompson sampling is carried out using the posterior distribution of the polynomial coefficients in order to account for uncertainties in the trained regression models. The strategy deployed for sample acquisition incorporates a gradient-based search on the parametrized reduced order model, which involves analytical gradients obtained via adjoint sensitivity analysis. By adding the found optimum to the sample set, the sample set is iteratively refined. Results demonstrate robust convergence towards the global optimum but highlight the computational cost introduced by the gradient-based optimization. The probabilistic extensions seamlessly integrate into existing matrix-interpolatory reduction frameworks and enable the analytical calculation of gradients under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.23314v1</guid>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcel Warzecha, Sebastian Resch-Schopper, Gerhard M\"uller</dc:creator>
    </item>
    <item>
      <title>Optimized Disaster Recovery for Distributed Storage Systems: Lightweight Metadata Architectures to Overcome Cryptographic Hashing Bottleneck</title>
      <link>https://arxiv.org/abs/2602.22237</link>
      <description>arXiv:2602.22237v1 Announce Type: cross 
Abstract: Distributed storage architectures are foundational to modern cloud-native infrastructure, yet a critical operational bottleneck persists within disaster recovery (DR) workflows: the dependence on content-based cryptographic hashing for data identification and synchronization. While hash-based deduplication is effective for storage efficiency in steady-state operation, it becomes a systemic liability during failover and failback events when hash indexes are stale, incomplete, or must be rebuilt following a crash. This paper precisely characterizes the operational conditions under which full or partial re-hashing becomes unavoidable. The paper also analyzes the downstream impact of cryptographic re-hashing on Recovery Time Objective (RTO) compliance, and proposes a generalized architectural shift toward deterministic, metadata-driven identification. The proposed framework assigns globally unique composite identifiers to data blocks at ingestion time-independent of content analysis enabling instantaneous delta computation during DR without any cryptographic overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22237v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prasanna Kumar, Nishank Soni, Gaurang Munje</dc:creator>
    </item>
    <item>
      <title>The AI Research Assistant: Promise, Peril, and a Proof of Concept</title>
      <link>https://arxiv.org/abs/2602.22842</link>
      <description>arXiv:2602.22842v1 Announce Type: cross 
Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22842v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tan Bui-Thanh</dc:creator>
    </item>
    <item>
      <title>Effective sample size approximations as entropy measures</title>
      <link>https://arxiv.org/abs/2602.22954</link>
      <description>arXiv:2602.22954v1 Announce Type: cross 
Abstract: In this work, we analyze alternative effective sample size (ESS) metrics for importance sampling algorithms, and discuss a possible extended range of applications. We show the relationship between the ESS expressions used in the literature and two entropy families, the R\'enyi and Tsallis entropy. The R\'enyi entropy is connected to the Huggins-Roy's ESS family introduced in \cite{Huggins15}. We prove that that all the ESS functions included in the Huggins-Roy's family fulfill all the desirable theoretical conditions. We analyzed and remark the connections with several other fields, such as the Hill numbers introduced in ecology, the Gini inequality coefficient employed in economics, and the Gini impurity index used mainly in machine learning, to name a few.
  Finally, by numerical simulations, we study the performance of different ESS expressions contained in the previous ESS families in terms of approximation of the theoretical ESS definition, and show the application of ESS formulas in a variable selection problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22954v1</guid>
      <category>math.ST</category>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-025-01665-8</arxiv:DOI>
      <arxiv:journal_reference>Computational Statistics, Volume 40, pages 5433-5464, 2025</arxiv:journal_reference>
      <dc:creator>L. Martino, V. Elvira</dc:creator>
    </item>
    <item>
      <title>A note on the area under the likelihood and the fake evidence for model selection</title>
      <link>https://arxiv.org/abs/2602.22965</link>
      <description>arXiv:2602.22965v1 Announce Type: cross 
Abstract: Improper priors are not allowed for the computation of the Bayesian evidence $Z=p({\bf y})$ (a.k.a., marginal likelihood), since in this case $Z$ is not completely specified due to an arbitrary constant involved in the computation. However, in this work, we remark that they can be employed in a specific type of model selection problem: when we have several (possibly infinite) models belonging to the same parametric family (i.e., for tuning parameters of a parametric model). However, the quantities involved in this type of selection cannot be considered as Bayesian evidences: we suggest to use the name ``fake evidences'' (or ``areas under the likelihood'' in the case of uniform improper priors). We also show that, in this model selection scenario, using a diffuse prior and increasing its scale parameter asymptotically to infinity, we cannot recover the value of the area under the likelihood, obtained with a uniform improper prior. We first discuss it from a general point of view. Then we provide, as an applicative example, all the details for Bayesian regression models with nonlinear bases, considering two cases: the use of a uniform improper prior and the use of a Gaussian prior, respectively. A numerical experiment is also provided confirming and checking all the previous statements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22965v1</guid>
      <category>stat.ME</category>
      <category>cs.CE</category>
      <category>eess.SP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s00180-025-01641-2</arxiv:DOI>
      <arxiv:journal_reference>Computational Statistics, Volume 40, pages 4799-4824, year 2025</arxiv:journal_reference>
      <dc:creator>L. Martino, F. Llorente</dc:creator>
    </item>
    <item>
      <title>A Reduced Magnetic Vector Potential Approach with Higher-Order Splines</title>
      <link>https://arxiv.org/abs/2602.22997</link>
      <description>arXiv:2602.22997v1 Announce Type: cross 
Abstract: This work presents a high-order isogeometric formulation for magnetoquasistatic eddy-current problems based on a decomposition into Biot-Savart-driven source fields and finite-element reaction fields. Building upon a recently proposed surface-only Biot-Savart evaluation, we generalize the reduced magnetic vector potential framework to the quasistatic regime and introduce a consistent high-order spline discretization. The resulting method avoids coil meshing, supports arbitrary winding paths, and enables high-order field approximation within a reduced computational domain. Beyond establishing optimal convergence rates, the numerical investigation identifies the requirements necessary to recover high-order accuracy in practice, including geometric regularity of the enclosing interface, accurate kernel quadrature, and compatible trace spaces for the source-reaction coupling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.22997v1</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <pubDate>Fri, 27 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Merle Backmeyer, Laura A. M. D'Angelo, Brahim Ramdane, Sebastian Sch\"ops</dc:creator>
    </item>
  </channel>
</rss>
