<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Jan 2026 03:30:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Model-based clustering using a new mixture of circular regressions</title>
      <link>https://arxiv.org/abs/2601.05345</link>
      <description>arXiv:2601.05345v1 Announce Type: new 
Abstract: Regression models, where the response variable is circular, are common in areas such as biology, geology and meteorology. A typical model assumes that the conditional distribution of the response follows a von-Mises distribution. However, this assumption is inadequate when the response variable is multimodal. For this reason, in this paper, a finite mixture of regressions model is proposed for the case of a circular response variable and a set of circular and/or linear covariates. Mixture models are very useful when the underlying population is multimodal. Despite the prevalence of multimodality in regression modelling of circular data, the use of mixtures of regressions has received no attention in the literature. This paper aims to close this knowledge gap. To estimate the proposed model, we develop a maximum likelihood estimation procedure via the Expectation-Maximization algorithm. An extensive simulation study is used to demonstrate the practical use and performance of the proposed model and estimation procedure. In addition, the model is shown to be useful as a model-based clustering tool. Lastly, the model is applied to a real dataset from a wind farm in South Africa.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05345v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sphiwe B. Skhosana, Najmeh Nakhaei Rad</dc:creator>
    </item>
    <item>
      <title>Archetypal cases for questionnaires with nominal multiple choice questions</title>
      <link>https://arxiv.org/abs/2601.05392</link>
      <description>arXiv:2601.05392v1 Announce Type: new 
Abstract: Archetypal analysis serves as an exploratory tool that interprets a collection of observations as convex combinations of pure (extreme) patterns. When these patterns correspond to actual observations within the sample, they are termed archetypoids. For the first time, we propose applying archetypoid analysis to nominal observations, specifically for identifying archetypal cases from questionnaires featuring nominal multiple-choice questions with a single possible answer. This approach can enhance our understanding of a nominal data set, similar to its application in multivariate contexts. We compare this methodology with the use of archetype analysis and probabilistic archetypal analysis and demonstrate the benefits of this methodology using a real-world example: the German credit dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05392v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleix Alcacer, Irene Epifanio</dc:creator>
    </item>
    <item>
      <title>Uncertainty Analysis of Experimental Parameters for Reducing Warpage in Injection Molding</title>
      <link>https://arxiv.org/abs/2601.05396</link>
      <description>arXiv:2601.05396v1 Announce Type: new 
Abstract: Injection molding is a critical manufacturing process, but controlling warpage remains a major challenge due to complex thermomechanical interactions. Simulation-based optimization is widely used to address this, yet traditional methods often overlook the uncertainty in model parameters. In this paper, we propose a data-driven framework to minimize warpage and quantify the uncertainty of optimal process settings. We employ polynomial regression models as surrogates for the injection molding simulations of a box-shaped part. By adopting a Bayesian framework, we estimate the posterior distribution of the regression coefficients. This approach allows us to generate a distribution of optimal decisions rather than a single point estimate, providing a measure of solution robustness. Furthermore, we develop a Monte Carlo-based boundary analysis method. This method constructs confidence bands for the zero-level sets of the response surfaces, helping to visualize the regions where warpage transitions between convex and concave profiles. We apply this framework to optimize four key process parameters: mold temperature, injection speed, packing pressure, and packing time. The results show that our approach finds stable process settings and clearly marks the boundaries of defects in the parameter space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05396v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yezhuo Li, Fan Zhang, Dhanashree Shinde, Qiong Zhang, Sai Pradeep, Srikanth Pilla, Gang Li</dc:creator>
    </item>
    <item>
      <title>Multi-Group Quadratic Discriminant Analysis via Projection</title>
      <link>https://arxiv.org/abs/2601.05415</link>
      <description>arXiv:2601.05415v1 Announce Type: new 
Abstract: Multi-group classification arises in many prediction and decision-making problems, including applications in epidemiology, genomics, finance, and image recognition. Although classification methods have advanced considerably, much of the literature focuses on binary problems, and available extensions often provide limited flexibility for multi-group settings. Recent work has extended linear discriminant analysis to multiple groups, but more general methods are still needed to handle complex structures such as nonlinear decision boundaries and group-specific covariance patterns.
  We develop Multi-Group Quadratic Discriminant Analysis (MGQDA), a method for multi-group classification built on quadratic discriminant analysis. MGQDA projects high-dimensional predictors onto a lower-dimensional subspace, which enables accurate classification while capturing nonlinearity and heterogeneity in group-specific covariance structures. We derive theoretical guarantees, including variable selection consistency, to support the reliability of the procedure. In simulations and a gene-expression application, MGQDA achieves competitive or improved predictive performance compared with existing methods while selecting group-specific informative variables, indicating its practical value for high-dimensional multi-group classification problems. Supplementary materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05415v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchao Wang, Tianying Wang</dc:creator>
    </item>
    <item>
      <title>Minimax Optimal Robust Sparse Regression with Heavy-Tailed Designs: A Gradient-Based Approach</title>
      <link>https://arxiv.org/abs/2601.05669</link>
      <description>arXiv:2601.05669v1 Announce Type: new 
Abstract: We investigate high-dimensional sparse regression when both the noise and the design matrix exhibit heavy-tailed behavior. Standard algorithms typically fail in this regime, as heavy-tailed covariates distort the empirical risk geometry. We propose a unified framework, Robust Iterative Gradient descent with Hard Thresholding (RIGHT), which employs a robust gradient estimator to bypass the need for higher-order moment conditions. Our analysis reveals a fundamental decoupling phenomenon: in linear regression, the estimation error rate is governed by the noise tail index, while the sample complexity required for stability is governed by the design tail index. This implies that while heavy-tailed noise limits precision, heavy-tailed designs primarily raise the sample size barrier for convergence. In contrast, for logistic regression, we show that the bounded gradient naturally robustifies the estimator against heavy-tailed designs, restoring standard parametric rates. We derive matching minimax lower bounds to prove that RIGHT achieves optimal estimation accuracy and sample complexity across these regimes, without requiring sample splitting or the existence of the population risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05669v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiyuan Zhou, Xiaoyu Zhang, Wenyang Zhang, Di Wang</dc:creator>
    </item>
    <item>
      <title>Conditional Cauchy-Schwarz Divergence for Time Series Analysis: Kernelized Estimation and Applications in Clustering and Fraud Detection</title>
      <link>https://arxiv.org/abs/2601.05711</link>
      <description>arXiv:2601.05711v1 Announce Type: new 
Abstract: We study the conditional Cauchy-Schwarz divergence (C-CSD) as a symmetric and density-free measure for time series analysis. We derive a practical kernel based estimator using radial basis function kernels on both the condition and output spaces, together with numerical stabilizations including a symmetric logarithmic form with an epsilon ridge and a robust bandwidth selection rule based on the interquartile range. Median heuristic bandwidths are applied to window vectors, and effective rank filtering is used to avoid degenerate kernels.
  We demonstrate the framework in two applications. In time series clustering, conditioning on the time index and comparing scalar series values yields a pairwise C-CSD dissimilarity. Bandwidths are selected on the training split, after which precomputed distance k-medoids clustering is performed on the test split and evaluated using normalized mutual information. In fraud detection, conditioning on sliding transaction windows and comparing the magnitude of value changes with categorical and merchant change indicators, each query window is scored by contrasting a global normal reference mixture against a same account local history mixture with recency decay and change flag weighting. Account level decisions are obtained by aggregating window scores using the maximum value. Experiments on benchmark time series datasets and a transactional fraud detection dataset demonstrate stable estimation and effective performance under a strictly leak free evaluation protocol.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05711v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayi Wang</dc:creator>
    </item>
    <item>
      <title>Estimating optimal interpretable individualized treatment regimes from a classification perspective using adaptive LASSO</title>
      <link>https://arxiv.org/abs/2601.05875</link>
      <description>arXiv:2601.05875v1 Announce Type: new 
Abstract: Real-world data (RWD) gains growing interests to provide a representative sample of the population for selecting the optimal treatment options. However, existing complex black box methods for estimating individualized treatment rules (ITR) from RWD have problems in interpretability and convergence. Providing an interpretable and sparse ITR can be used to overcome the limitation of existing methods. We developed an algorithm using Adaptive LASSO to predict optimal interpretable linear ITR in the RWD. To encourage sparsity, we obtain an ITR by minimizing the risk function with various types of penalties and different methods of contrast estimation. Simulation studies were conducted to select the best configuration and to compare the novel algorithm with the existing state-of-the-art methods. The proposed algorithm was applied to RWD to predict the optimal interpretable ITR. Simulations show that adaptive LASSO had the highest rates of correctly selected variables and augmented inverse probability weighting with Super Learner performed best for estimating treatment contrast. Our method had a better performance than causal forest and R-learning in terms of the value function and variable selection. The proposed algorithm can strike a balance between the interpretability of estimated ITR (by selecting a small set of important variables) and its value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05875v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunshu Zhang, Shu Yang, Wendy Ye, Ilya Lipkovich, Douglas E. Faries</dc:creator>
    </item>
    <item>
      <title>Negative binomial models for development triangles of counts</title>
      <link>https://arxiv.org/abs/2601.05964</link>
      <description>arXiv:2601.05964v1 Announce Type: new 
Abstract: Prediction of outstanding claims has been done via nonparametric models (chain ladder), semiparametric models (overdispersed poisson) or fully parametric models. In this paper, we propose models based on negative binomial distributions for the prediction of outstanding number of claims, which are particularly useful to account for overdispersion. We first assume independence of random variables and introduce appropriate notation. Later, we generalise the model to account for dependence across development years. In both cases, the marginal distributions are negative binomials. We study the properties of the models and carry out bayesian inference. We illustrate the performance of the models with simulated and real datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05964v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luis E. Nieto-Barajas, Rodrigo S. Targino</dc:creator>
    </item>
    <item>
      <title>A Bayesian Generative Modeling Approach for Arbitrary Conditional Inference</title>
      <link>https://arxiv.org/abs/2601.05355</link>
      <description>arXiv:2601.05355v1 Announce Type: cross 
Abstract: Modern data analysis increasingly requires flexible conditional inference P(X_B | X_A) where (X_A, X_B) is an arbitrary partition of observed variable X. Existing conditional inference methods lack this flexibility as they are tied to a fixed conditioning structure and cannot perform new conditional inference once trained. To solve this, we propose a Bayesian generative modeling (BGM) approach for arbitrary conditional inference without retraining. BGM learns a generative model of X through an iterative Bayesian updating algorithm where model parameters and latent variables are updated until convergence. Once trained, any conditional distribution can be obtained without retraining. Empirically, BGM achieves superior prediction performance with well calibrated predictive intervals, demonstrating that a single learned model can serve as a universal engine for conditional prediction with uncertainty quantification. We provide theoretical guarantees for the convergence of the stochastic iterative algorithm, statistical consistency and conditional-risk bounds. The proposed BGM framework leverages the power of AI to capture complex relationships among variables while adhering to Bayesian principles, emerging as a promising framework for advancing various applications in modern data science. The code for BGM is freely available at https://github.com/liuq-lab/bayesgm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05355v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiao Liu, Wing Hung Wong</dc:creator>
    </item>
    <item>
      <title>The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection</title>
      <link>https://arxiv.org/abs/2601.05371</link>
      <description>arXiv:2601.05371v1 Announce Type: cross 
Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05371v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Shafiqul Islam, Shakti Prasad Padhy, Douglas Allaire, Raymundo Arr\'oyave</dc:creator>
    </item>
    <item>
      <title>Representing asymmetric relationships by h-plots. Discovering the archetypal patterns of cross-journal citation relationships</title>
      <link>https://arxiv.org/abs/2601.05400</link>
      <description>arXiv:2601.05400v1 Announce Type: cross 
Abstract: This work approaches the multidimensional scaling problem from a novel angle. We introduce a scalable method based on the h-plot, which inherently accommodates asymmetric proximity data. Instead of embedding the objects themselves, the method embeds the variables that define the proximity to or from each object. It is straightforward to implement, and the quality of the resulting representation can be easily evaluated. The methodology is illustrated by visualizing the asymmetric relationships between the citing and cited profiles of journals on a common map. Two profiles that are far apart (or close together) in the h-plot, as measured by Euclidean distance, are different (or similar), respectively. This representation allows archetypoid analysis (ADA) to be calculated. ADA is used to find archetypal journals (or extreme cases). We can represent the dataset as convex combinations of these archetypal journals, making the results easy to interpret, even for non-experts. Comparisons with other methodologies are carried out, showing the good performance of our proposal. Code and data are available for reproducibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05400v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Aleix Alcacer, Irene Epifanio</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Noisy LLM-as-a-Judge Evaluation</title>
      <link>https://arxiv.org/abs/2601.05420</link>
      <description>arXiv:2601.05420v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05420v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqun T Chen, Sizhu Lu, Sijia Li, Moran Guo, Shengyi Li</dc:creator>
    </item>
    <item>
      <title>Poisson Hyperplane Processes with Rectified Linear Units</title>
      <link>https://arxiv.org/abs/2601.05586</link>
      <description>arXiv:2601.05586v1 Announce Type: cross 
Abstract: Neural networks have shown state-of-the-art performances in various classification and regression tasks. Rectified linear units (ReLU) are often used as activation functions for the hidden layers in a neural network model. In this article, we establish the connection between the Poisson hyperplane processes (PHP) and two-layer ReLU neural networks. We show that the PHP with a Gaussian prior is an alternative probabilistic representation to a two-layer ReLU neural network. In addition, we show that a two-layer neural network constructed by PHP is scalable to large-scale problems via the decomposition propositions. Finally, we propose an annealed sequential Monte Carlo algorithm for Bayesian inference. Our numerical experiments demonstrate that our proposed method outperforms the classic two-layer ReLU neural network. The implementation of our proposed model is available at https://github.com/ShufeiGe/Pois_Relu.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05586v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shufei Ge, Shijia Wang, Lloyd Elliott</dc:creator>
    </item>
    <item>
      <title>A latent factor approach to hyperspectral time series data for multivariate genomic prediction of grain yield in wheat</title>
      <link>https://arxiv.org/abs/2601.05842</link>
      <description>arXiv:2601.05842v1 Announce Type: cross 
Abstract: High-dimensional time series phenotypic data is becoming increasingly common within plant breeding programmes. However, analysing and integrating such data for genetic analysis and genomic prediction remains difficult. Here we show how factor analysis with Procrustes rotation on the genetic correlation matrix of hyperspectral secondary phenotype data can help in extracting relevant features for within-trial prediction. We use a subset of Centro Internacional de Mejoramiento de Ma\'iz y Trigo (CIMMYT) elite yield wheat trial of 2014-2015, consisting of 1,033 genotypes. These were measured across three irrigation treatments at several timepoints during the season, using manned airplane flights with hyperspectral sensors capturing 62 bands in the spectrum of 385-850 nm. We perform multivariate genomic prediction using latent variables to improve within-trial genomic predictive ability (PA) of wheat grain yield within three distinct watering treatments. By integrating latent variables of the hyperspectral data in a multivariate genomic prediction model, we are able to achieve an absolute gain of .1 to .3 (on the correlation scale) in PA compared to univariate genomic prediction. Furthermore, we show which timepoints within a trial are important and how these relate to plant growth stages. This paper showcases how domain knowledge and data-driven approaches can be combined to increase PA and gain new insights from sensor data of high-throughput phenotyping platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05842v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jonathan F. Kunst, Killian A. C. Melsen, Willem Kruijer, Jos\'e Crossa, Chris Maliepaard, Fred A. van Eeuwijk, Carel F. W. Peeters</dc:creator>
    </item>
    <item>
      <title>A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link</title>
      <link>https://arxiv.org/abs/2601.05845</link>
      <description>arXiv:2601.05845v1 Announce Type: cross 
Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05845v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Weine, Peter Carbonetto, Rafael A. Irizarry, Matthew Stephens</dc:creator>
    </item>
    <item>
      <title>A non-parametric approach for estimating consumer valuation distributions using second price auctions</title>
      <link>https://arxiv.org/abs/2312.07882</link>
      <description>arXiv:2312.07882v2 Announce Type: replace 
Abstract: We focus on online second price auctions, where bids are made sequentially, and the winning bidder pays the maximum of the second-highest bid and a seller specified reserve price. For many such auctions, the seller does not see all the bids or the total number of bidders accessing the auction, and only observes the current selling prices throughout the course of the auction. We develop a novel non-parametric approach to estimate the underlying consumer valuation distribution based on this data. Previous non-parametric approaches in the literature only use the final selling price and assume knowledge of the total number of bidders. The resulting estimate, in particular, can be used by the seller to compute the optimal profit-maximizing price for the product. Our approach is free of tuning parameters, and we demonstrate its computational and statistical efficiency in a variety of simulation settings, and also on an Xbox 7-day auction dataset on eBay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.07882v2</guid>
      <category>stat.ME</category>
      <category>cs.GT</category>
      <category>stat.AP</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Mukherjee, Ziqian Yang, Rohit K Patra, Kshitij Khare</dc:creator>
    </item>
    <item>
      <title>Testing for sufficient follow-up in survival data with a cure fraction</title>
      <link>https://arxiv.org/abs/2403.16832</link>
      <description>arXiv:2403.16832v2 Announce Type: replace 
Abstract: In order to estimate the proportion of `immune' or `cured' subjects who will never experience failure, a sufficiently long follow-up period is required. Several statistical tests have been proposed in the literature for assessing the assumption of sufficient follow-up, meaning that the study duration is longer than the support of the survival times for the uncured subjects. These tests do not perform satisfactorily, especially in terms of Type I error. In addition, they are constructed based on the assumption that the survival time for the uncured subjects has a compact support, i.e. the existence of a `cure time'. However, for practical purposes, the assumption of `cure time' is not realistic and the follow-up would be considered sufficiently long if the probability for the event to happen after the end of the study is very small. Based on this observation, we formulate a more relaxed notion of `practically' sufficient follow-up characterized by the quantiles of the distribution and develop a novel nonparametric statistical test. The proposed method relies mainly on the assumption of a non-increasing density function in the tail of the distribution. The test is then based on a shape constrained density estimator such as the Grenander or the kernel smoothed Grenander estimator and a bootstrap procedure is used for computation of the critical values. The performance of the test is investigated through an extensive simulation study, and the method is illustrated on breast cancer data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16832v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tsz Pang Yuen, Eni Musta</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Covariate-Adjusted and Interpretable Generalized Factor Model with Application to Testing Fairness</title>
      <link>https://arxiv.org/abs/2404.16745</link>
      <description>arXiv:2404.16745v3 Announce Type: replace 
Abstract: Latent variable models are popularly used to measure latent factors (e.g., abilities and personalities) from large-scale assessment data. Beyond understanding these latent factors, the covariate effect on responses controlling for latent factors is also of great scientific interest and has wide applications, such as evaluating the fairness of educational testing, where the covariate effect reflects whether a test question is biased toward certain individual characteristics (e.g., gender and race), taking into account their latent abilities. However, the large sample sizes and test lengths pose challenges to developing efficient methods and drawing valid inferences. Moreover, to accommodate the commonly encountered discrete responses, nonlinear latent factor models are often assumed, adding further complexity. To address these challenges, we consider a covariate-adjusted generalized factor model and develop novel and interpretable conditions to address the identifiability issue. Based on the identifiability conditions, we propose a joint maximum likelihood estimation method and establish estimation consistency and asymptotic normality results for the covariate effects. Furthermore, we derive estimation and inference results for latent factors and the factor loadings. We illustrate the finite sample performance of the proposed method through extensive numerical studies and an educational assessment dataset from the Programme for International Student Assessment (PISA).</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16745v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Ouyang, Chengyu Cui, Kean Ming Tan, Gongjun Xu</dc:creator>
    </item>
    <item>
      <title>Semiparametric causal mediation analysis of cluster-randomized trials for indirect and spillover effects</title>
      <link>https://arxiv.org/abs/2404.18256</link>
      <description>arXiv:2404.18256v3 Announce Type: replace 
Abstract: In cluster-randomized trials (CRTs), there is emerging interest in exploring the causal mechanism in which a cluster-level treatment affects the outcome through an intermediate outcome. The majority of existing causal mediation methods are applicable to independent data and only a few exceptions have considered assessing causal mediation in CRTs, all of which heavily depend on parametric assumptions. In this article, we develop a formal semiparametric efficiency theory to motivate new doubly-robust methods for addressing different mediation effect estimands -- the natural indirect effect, individual mediation effect, and spillover mediation effect (the extent to which one's outcome is influenced by others' mediators). We derive the efficient influence function for each estimand, and carefully parameterize each efficient influence function to motivate practical estimators. We consider both parametric working models and data-adaptive machine learners to estimate the nuisance functions, and obtain the semiparametric efficient estimators in the latter case. We conduct simulation studies to demonstrate the finite-sample performance of our new estimators and illustrate our proposed methods by reanalyzing a real-world CRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18256v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Fan Li</dc:creator>
    </item>
    <item>
      <title>Searching for local associations while controlling the false discovery rate</title>
      <link>https://arxiv.org/abs/2412.02182</link>
      <description>arXiv:2412.02182v3 Announce Type: replace 
Abstract: We introduce local conditional hypotheses that express how the relation between explanatory variables and outcomes changes across different contexts, described by covariates. By expanding upon the model-X knockoff filter, we show how to adaptively discover these local associations, all while controlling the false discovery rate. Our enhanced inferences can help explain sample heterogeneity and uncover interactions, making better use of the capabilities offered by modern machine learning models. Specifically, our method is able to leverage any model for the identification of data-driven hypotheses pertaining to different contexts. Then, it rigorously test these hypotheses without succumbing to selection bias. Importantly, our approach is efficient and does not require sample splitting. We demonstrate the effectiveness of our method through numerical experiments and by studying the genetic architecture of Waist-Hip-Ratio across different sexes in the UKBiobank.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02182v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paula Gablenz, Matteo Sesia, Tianshu Sun, Chiara Sabatti</dc:creator>
    </item>
    <item>
      <title>Beyond forecast leaderboards: Measuring individual model importance based on contribution to ensemble accuracy</title>
      <link>https://arxiv.org/abs/2412.08916</link>
      <description>arXiv:2412.08916v3 Announce Type: replace 
Abstract: Ensemble forecasts often outperform forecasts from individual standalone models, and have been used to support decision-making and policy planning in various fields. As collaborative forecasting efforts to create effective ensembles grow, so does interest in understanding individual models' relative importance in the ensemble. To this end, we propose two practical methods that measure the difference between ensemble performance when a given model is or is not included in the ensemble: a leave-one-model-out algorithm and a leave-all-subsets-of-models-out algorithm, which is based on the Shapley value. We explore the relationship between these metrics, forecast accuracy, and the similarity of errors, both analytically and through simulations. We illustrate this measure of the value a component model adds to an ensemble in the presence of other models using US COVID-19 death probabilistic forecasts. This study offers valuable insight into individual models' unique features within an ensemble, which standard accuracy metrics alone cannot reveal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08916v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsu Kim, Evan L. Ray, Nicholas G. Reich</dc:creator>
    </item>
    <item>
      <title>A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification</title>
      <link>https://arxiv.org/abs/2412.16065</link>
      <description>arXiv:2412.16065v3 Announce Type: replace 
Abstract: We present BayesPIM, a Bayesian prevalence-incidence mixture model for estimating time- and covariate-dependent disease incidence from screening and surveillance data. The method is particularly suited to settings where some individuals may have the disease at baseline, baseline tests may be missing or incomplete, and the screening test has imperfect test sensitivity. This setting was present in data from high-risk colorectal cancer (CRC) surveillance through colonoscopy, where adenomas, precursors of CRC, were already present at baseline and remained undetected due to imperfect test sensitivity. By including covariates, the model can quantify heterogeneity in disease risk, thereby informing personalized screening strategies. Internally, BayesPIM uses a Metropolis-within-Gibbs sampler with data augmentation and weakly informative priors on the incidence and prevalence model parameters. In simulations based on the real-world CRC surveillance data, we show that BayesPIM estimates model parameters without bias while handling latent prevalence and imperfect test sensitivity. However, informative priors on the test sensitivity are needed to stabilize estimation and mitigate non-convergence issues. We also show how conditioning incidence and prevalence estimates on covariates explains heterogeneity in adenoma risk and how model fit is assessed using information criteria and a non-parametric estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16065v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Klausch, Birgit I. Lissenberg-Witte, Veerle M. Coup\'e</dc:creator>
    </item>
    <item>
      <title>Inference on multiple quantiles in regression models by a rank-score approach</title>
      <link>https://arxiv.org/abs/2511.07999</link>
      <description>arXiv:2511.07999v2 Announce Type: replace 
Abstract: This paper tackles the challenge of performing multiple quantile regressions across different quantile levels and the associated problem of controlling the familywise error rate, an issue that is generally overlooked in practice. We propose a multivariate extension of the rank-score test and embed it within a closed-testing procedure to efficiently account for multiple testing. Then we further generalize the multivariate test to enhance statistical power against alternatives in selected directions. Theoretical foundations and simulation studies demonstrate that our method effectively controls the familywise error rate while achieving higher power than traditional corrections, such as Bonferroni.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07999v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Riccardo De Santis, Anna Vesely, Angela Andreella</dc:creator>
    </item>
    <item>
      <title>Selection-Induced Contraction of Innovation Statistics in Gated Kalman Filters</title>
      <link>https://arxiv.org/abs/2512.18508</link>
      <description>arXiv:2512.18508v2 Announce Type: replace 
Abstract: Validation gating is a fundamental component of classical Kalman-based tracking systems. Only measurements whose normalized innovation squared (NIS) falls below a prescribed threshold are considered for state update. While this procedure is statistically motivated by the chi-square distribution, it implicitly replaces the unconditional innovation process with a conditionally observed one, restricted to the validation event. This paper shows that innovation statistics computed after gating converge to gate-conditioned rather than nominal quantities. Under classical linear--Gaussian assumptions, we derive exact expressions for the first- and second-order moments of the innovation conditioned on ellipsoidal gating, and show that gating induces a deterministic, dimension-dependent contraction of the innovation covariance. The analysis is extended to NN association, which is shown to act as an additional statistical selection operator. We prove that selecting the minimum-norm innovation among multiple in-gate measurements introduces an unavoidable energy contraction, implying that nominal innovation statistics cannot be preserved under nontrivial gating and association. Closed-form results in the two-dimensional case quantify the combined effects and illustrate their practical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.18508v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Barak Or</dc:creator>
    </item>
    <item>
      <title>Predictive Assessment and Comparison of Bayesian Survival Models for Cancer Recurrence</title>
      <link>https://arxiv.org/abs/2601.01662</link>
      <description>arXiv:2601.01662v2 Announce Type: replace 
Abstract: Complex data features, such as unmodelled censored event times and variables with time-dependent effects, are common in cancer recurrence studies and pose challenges for Bayesian survival modelling. Current methodologies for predictive model checking and comparison often fail to adequately address these features. This paper bridges that gap by introducing new, targeted recommendations for predictive assessment and comparison of Bayesian survival models. Our recommendations cover a variety of different scenarios and models. Accompanying code together with our implementations to open source software help in replicating the results and applying our recommendations in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01662v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saku Suorsa, Aki Vehtari</dc:creator>
    </item>
    <item>
      <title>Unsupervised Domain Adaptation for Binary Classification with an Unobservable Source Subpopulation</title>
      <link>https://arxiv.org/abs/2509.20587</link>
      <description>arXiv:2509.20587v2 Announce Type: replace-cross 
Abstract: We study an unsupervised domain adaptation problem where the source domain consists of subpopulations defined by the binary label $Y$ and a binary background (or environment) $A$. We focus on a challenging setting in which one such subpopulation in the source domain is unobservable. Naively ignoring this unobserved group can result in biased estimates and degraded predictive performance. Despite this structured missingness, we show that the prediction in the target domain can still be recovered. Specifically, we rigorously derive both background-specific and overall prediction models for the target domain. For practical implementation, we propose the distribution matching method to estimate the subpopulation proportions. We provide theoretical guarantees for the asymptotic behavior of our estimator, and establish an upper bound on the prediction error. Experiments on both synthetic and real-world datasets show that our method outperforms the naive benchmark that does not account for this unobservable source subpopulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.20587v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chao Ying, Jun Jin, Haotian Zhang, Qinglong Tian, Yanyuan Ma, Yixuan Li, Jiwei Zhao</dc:creator>
    </item>
    <item>
      <title>Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2512.24139</link>
      <description>arXiv:2512.24139v3 Announce Type: replace-cross 
Abstract: While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \(1-\alpha \pm \delta\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24139v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 12 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
  </channel>
</rss>
