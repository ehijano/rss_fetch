<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Jul 2025 01:37:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Predictive Causal Inference via Spatio-Temporal Modeling and Penalized Empirical Likelihood</title>
      <link>https://arxiv.org/abs/2507.08896</link>
      <description>arXiv:2507.08896v1 Announce Type: new 
Abstract: This study introduces an integrated framework for predictive causal inference designed to overcome limitations inherent in conventional single model approaches. Specifically, we combine a Hidden Markov Model (HMM) for spatial health state estimation with a Multi Task and Multi Graph Convolutional Network (MTGCN) for capturing temporal outcome trajectories. The framework asymmetrically treats temporal and spatial information regarding them as endogenous variables in the outcome regression, and exogenous variables in the propensity score model, thereby expanding the standard doubly robust treatment effect estimation to jointly enhance bias correction and predictive accuracy. To demonstrate its utility, we focus on clinical domains such as cancer, dementia, and Parkinson disease, where treatment effects are challenging to observe directly. Simulation studies are conducted to emulate latent disease dynamics and evaluate the model performance under varying conditions. Overall, the proposed framework advances predictive causal inference by structurally adapting to spatiotemporal complexities common in biomedical data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08896v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Byunghee Lee, Hye Yeon Sin, Joonsung Kang</dc:creator>
    </item>
    <item>
      <title>Modeling Latent Underdispersion with Discrete Order Statistics</title>
      <link>https://arxiv.org/abs/2507.09032</link>
      <description>arXiv:2507.09032v1 Announce Type: new 
Abstract: The Poisson distribution is the default choice of likelihood for probabilistic models of count data. However, due to the equidispersion contraint of the Poisson, such models may have predictive uncertainty that is artificially inflated. While overdispersion has been extensively studied, conditional underdispersion -- where latent structure renders data more regular than Poisson -- remains underexplored, in part due to the lack of tractable modeling tools. We introduce a new class of models based on discrete order statistics, where observed counts are assumed to be an order statistic (e.g., minimum, median, maximum) of i.i.d. draws from some discrete parent, such as the Poisson or negative binomial. We develop a general data augmentation scheme that is modular with existing tools tailored to the parent distribution, enabling parameter estimation or posterior inference in a wide range of such models. We characterize properties of Poisson and negative binomial order statistics, exposing interpretable knobs on their dispersion. We apply our framework to four case studies -- i.e., to commercial flight times, COVID-19 case counts, Finnish bird abundance, and RNA sequencing data -- and illustrate the flexibility and generality of the proposed framework. Our results suggest that order statistic models can be built, used, and interpreted in much the same way as commonly-used alternatives, while often obtaining better fit, and offer promise in the wide range of applications in which count data arise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09032v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jimmy Lederman, Aaron Schein</dc:creator>
    </item>
    <item>
      <title>A monotone single index model for spatially-referenced multistate current status data</title>
      <link>https://arxiv.org/abs/2507.09057</link>
      <description>arXiv:2507.09057v1 Announce Type: new 
Abstract: Assessment of multistate disease progression is commonplace in biomedical research, such as, in periodontal disease (PD). However, the presence of multistate current status endpoints, where only a single snapshot of each subject's progression through disease states is available at a random inspection time after a known starting state, complicates the inferential framework. In addition, these endpoints can be clustered, and spatially associated, where a group of proximally located teeth (within subjects) may experience similar PD status, compared to those distally located. Motivated by a clinical study recording PD progression, we propose a Bayesian semiparametric accelerated failure time model with an inverse-Wishart proposal for accommodating (spatial) random effects, and flexible errors that follow a Dirichlet process mixture of Gaussians. For clinical interpretability, the systematic component of the event times is modeled using a monotone single index model, with the (unknown) link function estimated via a novel integrated basis expansion and basis coefficients endowed with constrained Gaussian process priors. In addition to establishing parameter identifiability, we present scalable computing via a combination of elliptical slice sampling, fast circulant embedding techniques, and smoothing of hard constraints, leading to straightforward estimation of parameters, and state occupation and transition probabilities. Using synthetic data, we study the finite sample properties of our Bayesian estimates, and their performance under model misspecification. We also illustrate our method via application to the real clinical PD dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09057v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Snigdha Das, Minwoo Chae, Debdeep Pati, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Convex Clustering</title>
      <link>https://arxiv.org/abs/2507.09077</link>
      <description>arXiv:2507.09077v1 Announce Type: new 
Abstract: This survey reviews a clustering method based on solving a convex optimization problem. Despite the plethora of existing clustering methods, convex clustering has several uncommon features that distinguish it from prior art. The optimization problem is free of spurious local minima, and its unique global minimizer is stable with respect to all its inputs, including the data, a tuning parameter, and weight hyperparameters. Its single tuning parameter controls the number of clusters and can be chosen using standard techniques from penalized regression. We give intuition into the behavior and theory for convex clustering as well as practical guidance. We highlight important algorithms and give insight into how their computational costs scale with the problem size. Finally, we highlight the breadth of its uses and flexibility to be combined and integrated with other inferential methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09077v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Eric C. Chi, Aaron J. Molstad, Zheming Gao</dc:creator>
    </item>
    <item>
      <title>Sharp Trade-Offs in High-Dimensional Inference via 2-Level SLOPE</title>
      <link>https://arxiv.org/abs/2507.09110</link>
      <description>arXiv:2507.09110v1 Announce Type: new 
Abstract: Among techniques for high-dimensional linear regression, Sorted L-One Penalized Estimation (SLOPE) generalizes the LASSO via an adaptive $l_1$ regularization that applies heavier penalties to larger coefficients in the model. To achieve such adaptivity, SLOPE requires the specification of a complex hierarchy of penalties, i.e., a monotone penalty sequence in $R^p$, in contrast to a single penalty scalar for LASSO. Tuning this sequence when $p$ is large poses a challenge, as brute force search over a grid of values is computationally prohibitive. In this work, we study the 2-level SLOPE, an important subclass of SLOPE, with only three hyperparameters. We demonstrate both empirically and analytically that 2-level SLOPE not only preserves the advantages of general SLOPE -- such as improved mean squared error and overcoming the Donoho-Tanner power limit -- but also exhibits computational benefits by reducing the penalty hyperparameter space. In particular, we prove that 2-level SLOPE admits a sharp, theoretically tight characterization of the trade-off between true positive proportion (TPP) and false discovery proportion (FDP), contrasting with general SLOPE where only upper and lower bounds are known. Empirical evaluations further underscore the effectiveness of 2-level SLOPE in settings where predictors exhibit high correlation, when the noise is large, or when the underlying signal is not sparse. Our results suggest that 2-level SLOPE offers a robust, scalable alternative to both LASSO and general SLOPE, making it particularly suited for practical high-dimensional data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09110v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqi Bu, Jason M. Klusowski, Cynthia Rush, Ruijia Wu</dc:creator>
    </item>
    <item>
      <title>A Moment-Based Generalization to Post-Prediction Inference</title>
      <link>https://arxiv.org/abs/2507.09119</link>
      <description>arXiv:2507.09119v1 Announce Type: new 
Abstract: Artificial intelligence (AI) and machine learning (ML) are increasingly used to generate data for downstream analyses, yet naively treating these predictions as true observations can lead to biased results and incorrect inference. Wang et al. (2020) proposed a method, post-prediction inference, which calibrates inference by modeling the relationship between AI/ML-predicted and observed outcomes in a small, gold-standard sample. Since then, several methods have been developed for inference with predicted data. We revisit Wang et al. in light of these recent developments. We reflect on their assumptions and offer a simple extension of their method which relaxes these assumptions. Our extension (1) yields unbiased point estimates under standard conditions and (2) incorporates a simple scaling factor to preserve calibration variability. In extensive simulations, we show that our method maintains nominal Type I error rates, reduces bias, and achieves proper coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09119v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Salerno, Kentaro Hoffman, Awan Afiaz, Anna Neufeld, Tyler H. McCormick, Jeffrey T. Leek</dc:creator>
    </item>
    <item>
      <title>Poisson Approximate Likelihood versus the block particle filter for a spatiotemporal measles model</title>
      <link>https://arxiv.org/abs/2507.09121</link>
      <description>arXiv:2507.09121v1 Announce Type: new 
Abstract: Filtering algorithms for high-dimensional nonlinear non-Gaussian partially observed stochastic processes provide access to the likelihood function and hence enable likelihood-based or Bayesian inference for this methodologically challenging class of models. A novel Poisson approximate likelihood (PAL) filter was introduced by Whitehouse et al.\ (2023). PAL employs a Poisson approximation to conditional densities, offering a fast approximation to the likelihood function for a certain subset of partially observed Markov process models. PAL was demonstrated on an epidemiological metapopulation model for measles, specifically, a spatiotemporal model for disease transmission within and between cities. At face value, Table\ 3 of Whitehouse et al.\ (2023) suggests that PAL considerably out-performs previous analysis as well as an ARMA benchmark model. We show that PAL does not outperform a block particle filter and that the lookahead component of PAL was implemented in a way that introduces substantial positive bias in the log-likelihood estimates. Therefore, the results of Table\ 3 of Whitehouse et al.\ (2023) do not accurately represent the true capabilities of PAL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09121v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kunyang He, Yize Hao, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Robust designs for Gaussian process emulation of computer experiments</title>
      <link>https://arxiv.org/abs/2507.09156</link>
      <description>arXiv:2507.09156v1 Announce Type: new 
Abstract: We study in this paper two classes of experimental designs, support points and projected support points, which can provide robust and effective emulation of computer experiments with Gaussian processes. These designs have two important properties that are appealing for surrogate modeling of computer experiments. First, the proposed designs are robust: they enjoy good emulation performance over a wide class of smooth and rugged response surfaces. Second, they can be efficiently generated for large designs in high dimensions using difference-of-convex programming. In this work, we present a theoretical framework that investigates the above properties, then demonstrate their effectiveness for Gaussian process emulation in a suite of numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09156v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Mak, V. Roshan Joseph</dc:creator>
    </item>
    <item>
      <title>The BdryMat\'ern GP: Reliable incorporation of boundary information on irregular domains for Gaussian process modeling</title>
      <link>https://arxiv.org/abs/2507.09178</link>
      <description>arXiv:2507.09178v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are broadly used as surrogate models for expensive computer simulators of complex phenomena. However, a key bottleneck is that its training data are generated from this expensive simulator and thus can be highly limited. A promising solution is to supplement the learning model with boundary information from scientific knowledge. However, despite recent work on boundary-integrated GPs, such models largely cannot accommodate boundary information on irregular (i.e., non-hypercube) domains, and do not provide sample path smoothness control or approximation error analysis, both of which are important for reliable surrogate modeling. We thus propose a novel BdryMat\'ern GP modeling framework, which can reliably integrate Dirichlet, Neumann and Robin boundaries on an irregular connected domain with a boundary set that is twice-differentiable almost everywhere. Our model leverages a new BdryMat\'ern covariance kernel derived in path integral form via a stochastic partial differential equation formulation. Similar to the GP with Mat\'ern kernel, we prove that sample paths from the BdryMat\'ern GP satisfy the desired boundaries with smoothness control on its derivatives. We further present an efficient approximation procedure for the BdryMat\'ern kernel using finite element modeling with rigorous error analysis. Finally, we demonstrate the effectiveness of the BdryMat\'ern GP in a suite of numerical experiments on incorporating broad boundaries on irregular domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09178v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liang Ding, Simon Mak, C. F. Jeff Wu</dc:creator>
    </item>
    <item>
      <title>The Multiplicative Instrumental Variable Model</title>
      <link>https://arxiv.org/abs/2507.09302</link>
      <description>arXiv:2507.09302v1 Announce Type: new 
Abstract: The instrumental variable (IV) design is a common approach to address hidden confounding bias. For validity, an IV must impact the outcome only through its association with the treatment. In addition, IV identification has required a homogeneity condition such as monotonicity or no unmeasured common effect modifier between the additive effect of the treatment on the outcome, and that of the IV on the treatment. In this work, we introduce a novel identifying condition of no multiplicative interaction between the instrument and the unmeasured confounder in the treatment model, which we establish nonparametrically identifies the average treatment effect on the treated (ATT). For inference, we propose an estimator that is multiply robust and semiparametric efficient, while allowing for the use of machine learning to adaptively estimate required nuisance functions via cross-fitting. Finally, we illustrate the methods in extended simulations and an application on the causal impact of a job training program on subsequent earnings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09302v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiewen Liu, Chan Park, Yonghoon Lee, Yunshu Zhang, Mengxin Yu, James M. Robins, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>An Integrated and Coherent Framework for Point Estimation and Hypothesis Testing with Concurrent Controls in Platform Trials</title>
      <link>https://arxiv.org/abs/2507.09358</link>
      <description>arXiv:2507.09358v1 Announce Type: new 
Abstract: A platform trial with a master protocol provides an infrastructure to ethically and efficiently evaluate multiple treatment options in multiple diseases. Given that certain study drugs can enter or exit a platform trial, the randomization ratio is possible to change over time, and this potential modification is not necessarily dependent on accumulating outcomes data. It is recommended that the analysis should account for time periods with different randomization ratios, with possible approaches such as Inverse Probability of Treatment Weighting (IPTW) or a weighted approach by the time period. To guide practical implementation, we specifically investigate the relationship between these two estimators, and further derive an optimal estimator within this class to gain efficacy. Practical guidance is provided on how to construct estimators based on observed data to approximate this unknown weight. The connection between the proposed method and the weighted least squares is also studied. We conduct simulation studies to demonstrate that the proposed method can control type I error rate with a reduced estimation bias, and can also achieve satisfactory power and mean squared error (MSE) with computational efficiency. Another appealing feature of our framework is the ability to provide consistent conclusions for both point estimation and hypothesis testing. This is critical to the interpretation of clinical trial results. The proposed method is further applied to the Accelerating COVID-19 Therapeutic Interventions and Vaccines (ACTIV) platform trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09358v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhan, Jane Zhang, Lei Shu, Yihua Gu</dc:creator>
    </item>
    <item>
      <title>A Latent Position Co-Clustering Model for Multiplex Networks</title>
      <link>https://arxiv.org/abs/2507.09370</link>
      <description>arXiv:2507.09370v1 Announce Type: new 
Abstract: Multiplex networks are increasingly common across diverse domains, motivating the development of clustering methods that uncover patterns at multiple levels. Existing approaches typically focus on clustering either entire networks or nodes within a single network. We address the lack of a unified latent space framework for simultaneous network- and node-level clustering by proposing a latent position co-clustering model (LaPCoM), based on a hierarchical mixture-of-mixtures formulation. LaPCoM enables co-clustering of networks and their constituent nodes, providing joint dimension reduction and two-level cluster detection. At the network level, it identifies global homogeneity in topological patterns by grouping networks that share similar latent representations. At the node level, it captures local connectivity and community patterns. The model adopts a Bayesian nonparametric framework using a mixture of finite mixtures, which places priors on the number of clusters at both levels and incorporates sparse priors to encourage parsimonious clustering. Inference is performed via Markov chain Monte Carlo with automatic selection of the number of clusters. LaPCoM accommodates both binary and count-valued multiplex data. Simulation studies and comparisons with existing methods demonstrate accurate recovery of latent structure and clusters. Applications to real-world social multiplexes reveal interpretable network-level clusters aligned with context-specific patterns, and node-level clusters reflecting social patterns and roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09370v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C. J. Clarke, Michael Fop</dc:creator>
    </item>
    <item>
      <title>Robust Spatiotemporal Epidemic Modeling with Integrated Adaptive Outlier Detection</title>
      <link>https://arxiv.org/abs/2507.09380</link>
      <description>arXiv:2507.09380v1 Announce Type: new 
Abstract: In epidemic modeling, outliers can distort parameter estimation and ultimately lead to misguided public health decisions. Although there are existing robust methods that can mitigate this distortion, the ability to simultaneously detect outliers is equally vital for identifying potential disease hotspots. In this work, we introduce a robust spatiotemporal generalized additive model (RST-GAM) to address this need. We accomplish this with a mean-shift parameter to quantify and adjust for the effects of outliers and rely on adaptive Lasso regularization to model the sparsity of outlying observations. We use univariate polynomial splines and bivariate penalized splines over triangulations to estimate the functional forms and a data-thinning approach for data-adaptive weight construction. We derive a scalable proximal algorithm to estimate model parameters by minimizing a convex negative log-quasi-likelihood function. Our algorithm uses adaptive step-sizes to ensure global convergence of the resulting iterate sequence. We establish error bounds and selection consistency for the estimated parameters and demonstrate our model's effectiveness through numerical studies under various outlier scenarios. Finally, we demonstrate the practical utility of RST-GAM by analyzing county-level COVID-19 infection data in the United States, highlighting its potential to inform public health decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09380v1</guid>
      <category>stat.ME</category>
      <category>physics.soc-ph</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Haoming Shi, Shan Yu, Eric C. Chi</dc:creator>
    </item>
    <item>
      <title>Semiparametric Regression Models for Explanatory Variables with Missing Data due to Detection Limit</title>
      <link>https://arxiv.org/abs/2507.09468</link>
      <description>arXiv:2507.09468v1 Announce Type: new 
Abstract: Detection limit (DL) has become an increasingly ubiquitous issue in statistical analyses of biomedical studies, such as cytokine, metabolite and protein analysis. In regression analysis, if an explanatory variable is left-censored due to concentrations below the DL, one may limit analyses to observed data. In many studies, additional, or surrogate, variables are available to model, and incorporating such auxiliary modeling information into the regression model can improve statistical power. Although methods have been developed along this line, almost all are limited to parametric models for both the regression and left-censored explanatory variable. While some recent work has considered semiparametric regression for the censored DL-effected explanatory variable, the regression of primary interest is still left parametric, which not only makes it prone to biased estimates, but also suffers from high computational cost and inefficiency due to maximizing an extremely complex likelihood function and bootstrap inference. In this paper, we propose a new approach by considering semiparametric generalized linear models (SPGLM) for the primary regression and parametric or semiparametric models for DL-effected explanatory variable. The semiparametric and semiparametric combination provides the most robust inference, while the semiparametric and parametric case enables more efficient inference. The proposed approach is also much easier to implement and allows for leveraging sample splitting and cross fitting (SSCF) to improve computational efficiency in variance estimation. In particular, our approach improves computational efficiency over bootstrap by 450 times. We use simulated and real study data to illustrate the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09468v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jasen Zhang, Lucy Shao, Kun Yang, Natalie E. Quach, Shengjia Tu, Ruohui Chen, Tsungchin Wu, Jinyuan Liu, Justin Tu, Jose R. Suarez-Lopez, Xinlian Zhang, Tuo Lin, Xin M. Tu</dc:creator>
    </item>
    <item>
      <title>The Use of Variational Inference for Lifetime Data with Spatial Correlations</title>
      <link>https://arxiv.org/abs/2507.09559</link>
      <description>arXiv:2507.09559v1 Announce Type: new 
Abstract: Lifetime data with spatial correlations are often collected for analysis in modern engineering, clinical, and medical applications. For such spatial lifetime data, statistical models usually account for the spatial dependence through spatial random effects, such as the cumulative exposure model and the proportional hazards model. For these models, the Bayesian estimation is commonly used for model inference, but often encounters computational challenges when the number of spatial locations is large. The conventional Markov Chain Monte Carlo (MCMC) methods for sampling the posterior can be time-consuming. In this case-study paper, we investigate the capability of variational inference (VI) for the model inference on spatial lifetime data, aiming for a good balance between the estimation accuracy and computational efficiency. Specifically, the VI methods with different divergence metrics are investigated for the spatial lifetime models. In the case study, the Titan GPU lifetime data and the pine tree lifetime data are used to examine the VI methods in terms of their computational advantage and estimation accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09559v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyao Wang, Yili Hong, Laura Freeman, Xinwei Deng</dc:creator>
    </item>
    <item>
      <title>Correction for Weak IV Bias and Winner's Curse in Mendelian Randomization Egger Regression: Rerandomized Egger estimator</title>
      <link>https://arxiv.org/abs/2507.09634</link>
      <description>arXiv:2507.09634v1 Announce Type: new 
Abstract: In two-sample Mendelian randomization (MR), Egger regression is widely used as a sensitivity analysis when directional pleiotropy is detected. However, the increasing complexity of modern MR studies, characterized by many weak instruments, renders the original Egger method less efficient. We first identify the source of weak instrument bias in Egger regression and introduce a debiased Egger (dEgger) estimator that restores consistency and asymptotic normality under substantially weaker conditions. To boost statistical power and ensure the validity of results, we then embed a random instrument selection procedure and present the rerandomized Egger (REgger) estimator along with an associated directional pleiotropy test. Recognizing the challenge of obtaining closed-form variances, we derive simple regression-residual-based variance estimators by truncating higher-order terms. The REgger estimator simultaneously removes the weak instrument bias and winner's curse while retaining robustness to directional pleiotropy, and is asymptotically normal when the effective sample size and post-selection instrument count are sufficiently large. Under balanced pleiotropy, REgger matches the rerandomized inverse-variance-weighted estimator, differing only in having marginally wider confidence intervals; under directional pleiotropy, it achieves substantially greater precision. Extensive simulations and real-data analyses confirm REgger's superior statistical properties, making it a valuable addition to two-sample MR sensitivity analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09634v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Youpeng Su, Yilei Ma, Ping Yin, Peng Wang</dc:creator>
    </item>
    <item>
      <title>Bridging Structural Causal Inference and Machine Learning The S-DIDML Estimator for Heterogeneous Treatment Effects</title>
      <link>https://arxiv.org/abs/2507.09718</link>
      <description>arXiv:2507.09718v1 Announce Type: new 
Abstract: In response to the increasing complexity of policy environments and the proliferation of high-dimensional data, this paper introduces the S-DIDML estimator a framework grounded in structure and semiparametrically flexible for causal inference. By embedding Difference-in-Differences (DID) logic within a Double Machine Learning (DML) architecture, the S-DIDML approach combines the strengths of temporal identification, machine learning-based nuisance adjustment, and orthogonalized estimation. We begin by identifying critical limitations in existing methods, including the lack of structural interpretability in ML models, instability of classical DID under high-dimensional confounding, and the temporal rigidity of standard DML frameworks. Building on recent advances in staggered adoption designs and Neyman orthogonalization, S-DIDML offers a five-step estimation pipeline that enables robust estimation of heterogeneous treatment effects (HTEs) while maintaining interpretability and scalability. Demonstrative applications are discussed across labor economics, education, taxation, and environmental policy. The proposed framework contributes to the methodological frontier by offering a blueprint for policy-relevant, structurally interpretable, and statistically valid causal analysis in complex data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09718v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yile Yu, Anzhi Xu</dc:creator>
    </item>
    <item>
      <title>FLAT: Fused Lasso Regression with Adaptive Minimum Spanning Tree with Applications on Thermohaline Circulation</title>
      <link>https://arxiv.org/abs/2507.09800</link>
      <description>arXiv:2507.09800v1 Announce Type: new 
Abstract: Spatial heterogeneity widely exists in many applications, such as in ocean science, where the temperature-salinity (T-S) relationship in thermohaline circulation varies across different geographical locations and depths. While spatial regression models are powerful tools for this purpose, they often face challenges in simultaneously estimating spatial parameters, detecting heterogeneity boundaries, and performing adaptive modeling, especially in complex systems. This paper proposes a Fused Lasso regression model with an Adaptive minimum spanning Tree (FLAT) to address these challenges in a unified framework. Specifically, FLAT constructs an adaptive minimum spanning tree guided by both spatial proximity and coefficient dissimilarity, and incorporates a spatial heterogeneity penalty to capture the underlying structure. A subsequent spatial clustering algorithm then identifies discrete heterogeneity boundaries, such as oceanic thermohaline fronts. Numerical simulations confirm that FLAT significantly outperforms classic spatial regression models in both coefficient estimation and heterogeneity detection. An empirical analysis with Atlantic Ocean data further demonstrates FLAT's capability to elucidate region-specific thermohaline compensation mechanisms and to detect surfaces with inverse T-S relationships. These findings advance the mechanistic understanding of T-S compensation dynamics in the Antarctic Intermediate Water region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09800v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cuiwen Che, Yifan Chen, Zhaoyu Xing, Wei Zhong</dc:creator>
    </item>
    <item>
      <title>Discrete Hamiltonian-Assisted Metropolis Sampling</title>
      <link>https://arxiv.org/abs/2507.09807</link>
      <description>arXiv:2507.09807v1 Announce Type: new 
Abstract: Gradient-based Markov Chain Monte Carlo methods have recently received much attention for sampling discrete distributions, with interesting connections to their continuous counterparts. For examples, there are two discrete analogues to the Metropolis-adjusted Langevin Algorithm (MALA). As motivated by Hamiltonian-Assisted Metropolis Sampling (HAMS), we propose Discrete HAMS (DHAMS), a discrete sampler which, for the first time, not only exploits gradient information but also incorporates a Gaussian momentum variable and samples a Hamiltonian as an augmented distribution. DHAMS is derived through several steps, including an auxiliary-variable proposal scheme, negation and gradient correction for the momentum variable, and over-relaxation for the state variable. Two distinctive properties are achieved simultaneously. One is generalized detailed balance, which enables irreversible exploration of the target distribution. The other is a rejection-free property for a target distribution with a linear potential function. In experiments involving both ordinal and binary distributions, DHAMS algorithms consistently yield superior performance compared with existing algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09807v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuze Zhou, Zhiqiang Tan</dc:creator>
    </item>
    <item>
      <title>Exploring the effects of mechanical ventilator settings with modified vector-valued treatment policies</title>
      <link>https://arxiv.org/abs/2507.09809</link>
      <description>arXiv:2507.09809v1 Announce Type: new 
Abstract: Mechanical ventilation is critical for managing respiratory failure, but inappropriate ventilator settings can lead to ventilator-induced lung injury (VILI), increasing patient morbidity and mortality. Evaluating the causal impact of ventilator settings is challenging due to the complex interplay of multiple treatment variables and strong confounding due to ventilator guidelines. In this paper, we propose a modified vector-valued treatment policy (MVTP) framework coupled with energy balancing weights to estimate causal effects involving multiple continuous ventilator parameters simultaneously in addition to sensitivity analysis to unmeasured confounding. Our approach mitigates common challenges in causal inference for vector-valued treatments, such as infeasible treatment combinations, stringent positivity assumptions, and interpretability concerns. Using the MIMIC-III database, our analyses suggest that equal reductions in the total power of ventilation (i.e., the mechanical power) through different ventilator parameters result in different expected patient outcomes. Specifically, lowering airway pressures may yield greater reductions in patient mortality compared to proportional adjustments of tidal volume alone. Moreover, controlling for respiratory-system compliance and minute ventilation, we found a significant benefit of reducing driving pressure in patients with acute respiratory distress syndrome (ARDS). Our analyses help shed light on the contributors to VILI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09809v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziren Jiang, Philip S. Crooke, John J. Marini, Jared D. Huling</dc:creator>
    </item>
    <item>
      <title>High-Dimensional Multi-Study Multi-Modality Covariate-Augmented Generalized Factor Model</title>
      <link>https://arxiv.org/abs/2507.09889</link>
      <description>arXiv:2507.09889v1 Announce Type: new 
Abstract: Latent factor models that integrate data from multiple sources/studies or modalities have garnered considerable attention across various disciplines. However, existing methods predominantly focus either on multi-study integration or multi-modality integration, rendering them insufficient for analyzing the diverse modalities measured across multiple studies. To address this limitation and cater to practical needs, we introduce a high-dimensional generalized factor model that seamlessly integrates multi-modality data from multiple studies, while also accommodating additional covariates. We conduct a thorough investigation of the identifiability conditions to enhance the model's interpretability. To tackle the complexity of high-dimensional nonlinear integration caused by four large latent random matrices, we utilize a variational lower bound to approximate the observed log-likelihood by employing a variational posterior distribution. By profiling the variational parameters, we establish the asymptotical properties of estimators for model parameters using M-estimation theory. Furthermore, we devise a computationally efficient variational EM algorithm to execute the estimation process and a criterion to determine the optimal number of both study-shared and study-specific factors. Extensive simulation studies and a real-world application show that the proposed method significantly outperforms existing methods in terms of estimation accuracy and computational efficiency. The R package for the proposed method is publicly accessible at https://CRAN.R-project.org/package=MMGFM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09889v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wei Liu, Qingzhi Zhong</dc:creator>
    </item>
    <item>
      <title>Statistical Inference for Conditional Group Distributionally Robust Optimization with Cross-Entropy Loss</title>
      <link>https://arxiv.org/abs/2507.09905</link>
      <description>arXiv:2507.09905v1 Announce Type: new 
Abstract: In multi-source learning with discrete labels, distributional heterogeneity across domains poses a central challenge to developing predictive models that transfer reliably to unseen domains. We study multi-source unsupervised domain adaptation, where labeled data are drawn from multiple source domains and only unlabeled data from a target domain. To address potential distribution shifts, we propose a novel Conditional Group Distributionally Robust Optimization (CG-DRO) framework that learns a classifier by minimizing the worst-case cross-entropy loss over the convex combinations of the conditional outcome distributions from the sources. To solve the resulting minimax problem, we develop an efficient Mirror Prox algorithm, where we employ a double machine learning procedure to estimate the risk function. This ensures that the errors of the machine learning estimators for the nuisance models enter only at higher-order rates, thereby preserving statistical efficiency under covariate shift. We establish fast statistical convergence rates for the estimator by constructing two surrogate minimax optimization problems that serve as theoretical bridges. A distinguishing challenge for CG-DRO is the emergence of nonstandard asymptotics: the empirical estimator may fail to converge to a standard limiting distribution due to boundary effects and system instability. To address this, we introduce a perturbation-based inference procedure that enables uniformly valid inference, including confidence interval construction and hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09905v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijian Guo, Zhenyu Wang, Yifan Hu, Francis Bach</dc:creator>
    </item>
    <item>
      <title>New Equivalence Tests for Hardy-Weinberg Equilibrium and Multiple Alleles</title>
      <link>https://arxiv.org/abs/2507.10077</link>
      <description>arXiv:2507.10077v1 Announce Type: new 
Abstract: We consider testing equivalence to Hardy-Weinberg Equilibrium in case of multiple alleles. Two different test statistics are proposed for this test problem. The asymptotic distribution of the test statistics is derived. The corresponding tests can be carried out using asymptotic approximation. Alternatively, the variance of the test statistics can be estimated by the bootstrap method. The proposed tests are applied to three real data sets. The finite sample performance of the tests is studied by simulations, which are inspired by the real data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10077v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/stats3010004</arxiv:DOI>
      <arxiv:journal_reference>Stats 2020, 3(1), 34-39</arxiv:journal_reference>
      <dc:creator>Vladimir Ostrovski</dc:creator>
    </item>
    <item>
      <title>Semiparametric empirical likelihood inference for abundance from one-inflated capture-recapture data</title>
      <link>https://arxiv.org/abs/2507.10388</link>
      <description>arXiv:2507.10388v1 Announce Type: new 
Abstract: Abundance estimation from capture-recapture data is of great importance in many disciplines. Analysis of capture-recapture data is often complicated by the existence of one-inflation and heterogeneity problems. Simultaneously taking these issues into account, existing abundance estimation methods are usually constructed on the basis of conditional likelihood (CL) under one-inflated zero-truncated count models. However, the resulting Horvitz-Thompson-type estimators may be unstable, and the resulting Wald-type confidence intervals may exhibit severe undercoverage. In this paper, we propose a semiparametric empirical likelihood (EL) approach to abundance estimation under one-inflated binomial and Poisson regression models. We show that the maximum EL estimator for the abundance follows an asymptotically normal distribution and that the EL ratio statistic of abundance follows a limiting chi-square distribution with one degree of freedom. To facilitate computation of the EL method, we develop an expectation-maximization (EM) algorithm, and establish its appealing convergence property. We also propose a new score test for the existence of one-inflation and prove its asymptotic normality. Our simulation studies indicate that compared with CL-based methods, the maximum EL estimator has a smaller mean square error, the EL ratio confidence interval has a remarkable gain in coverage probability, and the proposed score test is more powerful. The advantages of the proposed approaches are further demonstrated by analyses of prinia data from Hong Kong and drug user data from Bangkok.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10388v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/bimj.202100231</arxiv:DOI>
      <arxiv:journal_reference>Biometrical Journal (2022), 64, 1040-1055</arxiv:journal_reference>
      <dc:creator>Yang Liu, Pengfei Li, Yukun Liu, Riquan Zhang</dc:creator>
    </item>
    <item>
      <title>Two-step semiparametric empirical likelihood inference from capture-recapture data with missing covariates</title>
      <link>https://arxiv.org/abs/2507.10404</link>
      <description>arXiv:2507.10404v1 Announce Type: new 
Abstract: Missing covariates are not uncommon in capture-recapture studies. When covariate information is missing at random in capture-recapture data, an empirical full likelihood method has been demonstrated to outperform conditional-likelihood-based methods in abundance estimation. However, the fully observed covariates must be discrete, and the method is not directly applicable to continuous-time capture-recapture data. Based on the Binomial and Poisson regression models, we propose a two-step semiparametric empirical likelihood approach for abundance estimation in the presence of missing covariates, regardless of whether the fully observed covariates are discrete or continuous. We show that the maximum semiparametric empirical likelihood estimators for the underlying parameters and the abundance are asymptotically normal, and more efficient than the counterpart for a completely known non-missingness probability. After scaling, the empirical likelihood ratio test statistic for abundance follows a limiting chi-square distribution with one degree of freedom. The proposed approach is further extended to one-inflated count regression models, and a score-like test is constructed to assess whether one-inflation exists among the number of captures. Our simulation shows that, compared with the previous method, the proposed method not only performs better in correcting bias, but also has a more accurate coverage in the presence of fully observed continuous covariates, although there may be a slight efficiency loss when the fully observed covariates are only discrete. The performance of the new method is illustrated by an analysis of the Hong Kong prinia data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10404v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-024-00921-1</arxiv:DOI>
      <arxiv:journal_reference>Test (2024), 33, 786-808</arxiv:journal_reference>
      <dc:creator>Yang Liu, Yukun Liu, Pengfei Li, Riquan Zhang</dc:creator>
    </item>
    <item>
      <title>Flexible Modeling of Multivariate Skewed and Heavy-Tailed Data via a Non-Central Skew t Distribution: Application to Tumor Shape Data</title>
      <link>https://arxiv.org/abs/2507.10465</link>
      <description>arXiv:2507.10465v1 Announce Type: new 
Abstract: We propose a flexible formulation of the multivariate non-central skew t (NCST) distribution, defined by scaling skew-normal random vectors with independent chi-squared variables. This construction extends the classical multivariate t family by allowing both asymmetry and non-centrality, which provides an alternative to existing skew t models that often rely on restrictive assumptions for tractability. We derive key theoretical properties of the NCST distribution, which includes its moment structure, affine transformation behavior, and the distribution of quadratic forms. Due to the lack of a closed-form density, we implement a Monte Carlo likelihood approximation to enable maximum likelihood estimation and evaluate its performance through simulation studies. To demonstrate practical utility, we apply the NCST model to breast cancer diagnostic data, modeling multiple features of tumor shape. The NCST model achieves a superior fit based on information criteria and visual diagnostics, particularly in the presence of skewness and heavy tails compared to standard alternatives, including the multivariate normal, skew normal, and Azzalini's skew $t$ distribution. Our findings suggest that the NCST distribution offers a useful and interpretable choice for modeling complex multivariate data, which highlights promising directions for future development in likelihood inference, Bayesian computation, and applications involving asymmetry and non-Gaussian dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10465v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abeer M. Hasan, Ying-Ju Chen</dc:creator>
    </item>
    <item>
      <title>Constructing Confidence Intervals for Infinite-Dimensional Functional Prameters by Highly Adaptive Lasso</title>
      <link>https://arxiv.org/abs/2507.10511</link>
      <description>arXiv:2507.10511v2 Announce Type: new 
Abstract: Estimating the conditional mean function is a central task in statistical learning. In this paper, we consider estimation and inference for a nonparametric class of real-valued c\`adl\`ag functions with bounded sectional variation (Gill et al., 1995), using the Highly Adaptive Lasso (HAL) (van der Laan, 2015; Benkeser and van der Laan, 2016; van der Laan, 2023), a flexible empirical risk minimizer over linear combinations of tensor products of zero- or higher-order spline basis functions under an L1 norm constraint. Building on recent theoretical advances in asymptotic normality and uniform convergence rates for higher-order spline HAL estimators (van der Laan, 2023), this work focuses on constructing robust confidence intervals for HAL-based conditional mean estimators. To address regularization bias, we propose a targeted HAL with a debiasing step to remove bias for the conditional mean, and also consider a relaxed HAL estimator to reduce bias. We also introduce both global and local undersmoothing strategies to adaptively select the working model, reducing bias relative to variance. Combined with delta-method-based variance estimation, we construct confidence intervals for conditional means based on HAL. Through simulations, we evaluate combinations of estimation and model selection strategies, showing that our methods substantially reduce bias and yield confidence intervals with coverage rates close to nominal levels across scenarios. We also provide recommendations for different estimation objectives and illustrate the generality of our framework by applying it to estimate conditional average treatment effect (CATE) functions, highlighting how HAL-based inference extends to other infinite-dimensional, non-pathwise differentiable parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10511v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxin Zhang, Junming Shi, Alan Hubbard, Mark van der Laan</dc:creator>
    </item>
    <item>
      <title>Physics-informed machine learning: A mathematical framework with applications to time series forecasting</title>
      <link>https://arxiv.org/abs/2507.08906</link>
      <description>arXiv:2507.08906v1 Announce Type: cross 
Abstract: Physics-informed machine learning (PIML) is an emerging framework that integrates physical knowledge into machine learning models. This physical prior often takes the form of a partial differential equation (PDE) system that the regression function must satisfy. In the first part of this dissertation, we analyze the statistical properties of PIML methods. In particular, we study the properties of physics-informed neural networks (PINNs) in terms of approximation, consistency, overfitting, and convergence. We then show how PIML problems can be framed as kernel methods, making it possible to apply the tools of kernel ridge regression to better understand their behavior. In addition, we use this kernel formulation to develop novel physics-informed algorithms and implement them efficiently on GPUs. The second part explores industrial applications in forecasting energy signals during atypical periods. We present results from the Smarter Mobility challenge on electric vehicle charging occupancy and examine the impact of mobility on electricity demand. Finally, we introduce a physics-constrained framework for designing and enforcing constraints in time series, applying it to load forecasting and tourism forecasting in various countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08906v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathan Doum\`eche</dc:creator>
    </item>
    <item>
      <title>Possibilistic inferential models: a review</title>
      <link>https://arxiv.org/abs/2507.09007</link>
      <description>arXiv:2507.09007v1 Announce Type: cross 
Abstract: An inferential model (IM) is a model describing the construction of provably reliable, data-driven uncertainty quantification and inference about relevant unknowns. IMs and Fisher's fiducial argument have similar objectives, but a fundamental distinction between the two is that the former doesn't require that uncertainty quantification be probabilistic, offering greater flexibility and allowing for a proof of its reliability. Important recent developments have been made thanks in part to newfound connections with the imprecise probability literature, in particular, possibility theory. The brand of possibilistic IMs studied here are straightforward to construct, have very strong frequentist-like reliability properties, and offer fully conditional, Bayesian-like (imprecise) probabilistic reasoning. This paper reviews these key recent developments, describing the new theory, methods, and computational tools. A generalization of the basic possibilistic IM is also presented, making new and unexpected connections with ideas in modern statistics and machine learning, e.g., bootstrap and conformal prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09007v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Optimal Differentially Private Ranking from Pairwise Comparisons</title>
      <link>https://arxiv.org/abs/2507.09388</link>
      <description>arXiv:2507.09388v1 Announce Type: cross 
Abstract: Data privacy is a central concern in many applications involving ranking from incomplete and noisy pairwise comparisons, such as recommendation systems, educational assessments, and opinion surveys on sensitive topics. In this work, we propose differentially private algorithms for ranking based on pairwise comparisons. Specifically, we develop and analyze ranking methods under two privacy notions: edge differential privacy, which protects the confidentiality of individual comparison outcomes, and individual differential privacy, which safeguards potentially many comparisons contributed by a single individual. Our algorithms--including a perturbed maximum likelihood estimator and a noisy count-based method--are shown to achieve minimax optimal rates of convergence under the respective privacy constraints. We further demonstrate the practical effectiveness of our methods through experiments on both simulated and real-world data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09388v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Tony Cai, Abhinav Chakraborty, Yichen Wang</dc:creator>
    </item>
    <item>
      <title>An Algorithm for Identifying Interpretable Subgroups With Elevated Treatment Effects</title>
      <link>https://arxiv.org/abs/2507.09494</link>
      <description>arXiv:2507.09494v1 Announce Type: cross 
Abstract: We introduce an algorithm for identifying interpretable subgroups with elevated treatment effects, given an estimate of individual or conditional average treatment effects (CATE). Subgroups are characterized by ``rule sets'' -- easy-to-understand statements of the form (Condition A AND Condition B) OR (Condition C) -- which can capture high-order interactions while retaining interpretability. Our method complements existing approaches for estimating the CATE, which often produce high dimensional and uninterpretable results, by summarizing and extracting critical information from fitted models to aid decision making, policy implementation, and scientific understanding. We propose an objective function that trades-off subgroup size and effect size, and varying the hyperparameter that controls this trade-off results in a ``frontier'' of Pareto optimal rule sets, none of which dominates the others across all criteria. Valid inference is achievable through sample splitting. We demonstrate the utility and limitations of our method using simulated and empirical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09494v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Albert Chiu</dc:creator>
    </item>
    <item>
      <title>Edgeworth corrections for the spiked eigenvalues of non-Gaussian sample covariance matrices with applications</title>
      <link>https://arxiv.org/abs/2507.09584</link>
      <description>arXiv:2507.09584v1 Announce Type: cross 
Abstract: Yang and Johnstone (2018) established an Edgeworth correction for the largest sample eigenvalue in a spiked covariance model under the assumption of Gaussian observations, leaving the extension to non-Gaussian settings as an open problem. In this paper, we address this issue by establishing first-order Edgeworth expansions for spiked eigenvalues in both single-spike and multi-spike scenarios with non-Gaussian data. Leveraging these expansions, we construct more accurate confidence intervals for the population spiked eigenvalues and propose a novel estimator for the number of spikes. Simulation studies demonstrate that our proposed methodology outperforms existing approaches in both robustness and accuracy across a wide range of settings, particularly in low-dimensional cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09584v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yashi Wei, Jiang Hu, Zhidong Bai</dc:creator>
    </item>
    <item>
      <title>Spatial Dependencies in Item Response Theory: Gaussian Process Priors for Geographic and Cognitive Measurement</title>
      <link>https://arxiv.org/abs/2507.09824</link>
      <description>arXiv:2507.09824v1 Announce Type: cross 
Abstract: Measurement validity in Item Response Theory depends on appropriately modeling dependencies between items when these reflect meaningful theoretical structures rather than random measurement error. In ecological assessment, citizen scientists identifying species across geographic regions exhibit systematic spatial patterns in task difficulty due to environmental factors. Similarly, in Author Recognition Tests, literary knowledge organizes by genre, where familiarity with science fiction authors systematically predicts recognition of other science fiction authors. Current spatial Item Response Theory methods, represented by the 1PLUS, 2PLUS, and 3PLUS model family, address these dependencies but remain limited by (1) binary response restrictions, and (2) conditional autoregressive priors that impose rigid local correlation assumptions, preventing effective modeling of complex spatial relationships. Our proposed method, Spatial Gaussian Process Item Response Theory (SGP-IRT), addresses these limitations by replacing conditional autoregressive priors with flexible Gaussian process priors that adapt to complex dependency structures while maintaining principled uncertainty quantification. SGP-IRT accommodates polytomous responses and models spatial dependencies in both geographic and abstract cognitive spaces, where items cluster by theoretical constructs rather than physical proximity. Simulation studies demonstrate improved parameter recovery, particularly for item difficulty estimation. Empirical applications show enhanced recovery of meaningful difficulty surfaces and improved measurement precision across psychological, educational, and ecological research applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09824v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mingya Huang, Soham Ghosh</dc:creator>
    </item>
    <item>
      <title>Radial Neighborhood Smoothing Recommender System</title>
      <link>https://arxiv.org/abs/2507.09952</link>
      <description>arXiv:2507.09952v1 Announce Type: cross 
Abstract: Recommender systems inherently exhibit a low-rank structure in latent space. A key challenge is to define meaningful and measurable distances in the latent space to capture user-user, item-item, user-item relationships effectively. In this work, we establish that distances in the latent space can be systematically approximated using row-wise and column-wise distances in the observed matrix, providing a novel perspective on distance estimation. To refine the distance estimation, we introduce the correction based on empirical variance estimator to account for noise-induced non-centrality. The novel distance estimation enables a more structured approach to constructing neighborhoods, leading to the Radial Neighborhood Estimator (RNE), which constructs neighborhoods by including both overlapped and partially overlapped user-item pairs and employs neighborhood smoothing via localized kernel regression to improve imputation accuracy. We provide the theoretical asymptotic analysis for the proposed estimator. We perform evaluations on both simulated and real-world datasets, demonstrating that RNE achieves superior performance compared to existing collaborative filtering and matrix factorization methods. While our primary focus is on distance estimation in latent space, we find that RNE also mitigates the ``cold-start'' problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09952v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zerui Zhang, Yumou Qiu</dc:creator>
    </item>
    <item>
      <title>Gradient boosted multi-population mortality modelling with high-frequency data</title>
      <link>https://arxiv.org/abs/2507.09983</link>
      <description>arXiv:2507.09983v1 Announce Type: cross 
Abstract: High-frequency mortality data remains an understudied yet critical research area. While its analysis can reveal short-term health impacts of climate extremes and enable more timely mortality forecasts, its complex temporal structure poses significant challenges to traditional mortality models. To leverage the power of high-frequency mortality data, this paper introduces a novel integration of gradient boosting techniques into traditional stochastic mortality models under a multi-population setting. Our key innovation lies in using the Li and Lee model as the weak learner within the gradient boosting framework, replacing conventional decision trees. Empirical studies are conducted using weekly mortality data from 30 countries (Human Mortality Database, 2015--2019). The proposed methodology not only enhances model fit by accurately capturing underlying mortality trends and seasonal patterns, but also achieves superior forecast accuracy, compared to the benchmark models. We also investigate a key challenge in multi-population mortality modelling: how to select appropriate sub-populations with sufficiently similar mortality experiences. A comprehensive clustering exercise is conducted based on mortality improvement rates and seasonal strength. The results demonstrate the robustness of our proposed model, yielding stable forecast accuracy under different clustering configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09983v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziting Miao, Han Li, Yuyu Chen</dc:creator>
    </item>
    <item>
      <title>MF-GLaM: A multifidelity stochastic emulator using generalized lambda models</title>
      <link>https://arxiv.org/abs/2507.10303</link>
      <description>arXiv:2507.10303v1 Announce Type: cross 
Abstract: Stochastic simulators exhibit intrinsic stochasticity due to unobservable, uncontrollable, or unmodeled input variables, resulting in random outputs even at fixed input conditions. Such simulators are common across various scientific disciplines; however, emulating their entire conditional probability distribution is challenging, as it is a task traditional deterministic surrogate modeling techniques are not designed for. Additionally, accurately characterizing the response distribution can require prohibitively large datasets, especially for computationally expensive high-fidelity (HF) simulators. When lower-fidelity (LF) stochastic simulators are available, they can enhance limited HF information within a multifidelity surrogate modeling (MFSM) framework. While MFSM techniques are well-established for deterministic settings, constructing multifidelity emulators to predict the full conditional response distribution of stochastic simulators remains a challenge. In this paper, we propose multifidelity generalized lambda models (MF-GLaMs) to efficiently emulate the conditional response distribution of HF stochastic simulators by exploiting data from LF stochastic simulators. Our approach builds upon the generalized lambda model (GLaM), which represents the conditional distribution at each input by a flexible, four-parameter generalized lambda distribution. MF-GLaMs are non-intrusive, requiring no access to the internal stochasticity of the simulators nor multiple replications of the same input values. We demonstrate the efficacy of MF-GLaM through synthetic examples of increasing complexity and a realistic earthquake application. Results show that MF-GLaMs can achieve improved accuracy at the same cost as single-fidelity GLaMs, or comparable performance at significantly reduced cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10303v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Giannoukou, X. Zhu, S. Marelli, B. Sudret</dc:creator>
    </item>
    <item>
      <title>Post-reduction inference for confidence sets of models</title>
      <link>https://arxiv.org/abs/2507.10373</link>
      <description>arXiv:2507.10373v1 Announce Type: cross 
Abstract: Sparsity in a regression context makes the model itself an object of interest, pointing to a confidence set of models as the appropriate presentation of evidence. A difficulty in areas such as genomics, where the number of candidate variables is vast, arises from the need for preliminary reduction prior to the assessment of models. The present paper considers a resolution using inferential separations fundamental to the Fisherian approach to conditional inference, namely, the sufficiency/co-sufficiency separation, and the ancillary/co-ancillary separation. The advantage of these separations is that no direction for departure from any hypothesised model is needed, avoiding issues that would otherwise arise from using the same data for reduction and for model assessment. In idealised cases with no nuisance parameters, the separations extract all the information in the data, solely for the purpose for which it is useful, without loss or redundancy. The extent to which estimation of nuisance parameters affects the idealised information extraction is illustrated in detail for the normal-theory linear regression model, extending immediately to a log-normal accelerated-life model for time-to-event outcomes. This idealised analysis provides insight into when sample-splitting is likely to perform as well as, or better than, the co-sufficient or ancillary tests, and when it may be unreliable. The considerations involved in extending the detailed implementation to canonical exponential-family and more general regression models are briefly discussed. As part of the analysis for the Gaussian model, we introduce a modified version of the refitted cross-validation estimator of Fan et al. (2012), whose distribution theory is exact in an appropriate conditional sense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10373v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heather Battey, Daniel Garcia Rasines, Yanbo Tang</dc:creator>
    </item>
    <item>
      <title>Mapping food insecurity in the Brazilian Amazon using a spatial item factor analysis model</title>
      <link>https://arxiv.org/abs/1809.03905</link>
      <description>arXiv:1809.03905v2 Announce Type: replace 
Abstract: Food insecurity, a latent construct defined as the lack of consistent access to sufficient and nutritious food, is a pressing global issue with serious health and social justice implications. Item factor analysis is commonly used to study such latent constructs, but it typically assumes independence between sampling units. In the context of food insecurity, this assumption is often unrealistic, as food access is linked to socio-economic conditions and social relations that are spatially structured. To address this, we propose a spatial item factor analysis model that captures spatial dependence, allowing us to predict latent factors at unsampled locations and identify food insecurity hotspots. We develop a Bayesian sampling scheme for inference and illustrate the explanatory strength of our model by analysing household perceptions of food insecurity in Ipixuna, a remote river-dependent urban centre in the Brazilian Amazon. Our approach is implemented in the R package spifa, with further details provided in the Supplementary Material. This spatial extension offers policymakers and researchers a stronger tool for understanding and addressing food insecurity to locate and prioritise areas in greatest need. Our proposed methodology can be applied more widely to other spatially structured latent constructs.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.03905v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erick A. Chac\'on-Montalv\'an, Luke Parry, Emanuele Giorgi, Patricia Torres, Jesem D. Orellana, Paula Moraga, Benjamin M. Taylor</dc:creator>
    </item>
    <item>
      <title>Donor's Deferral and Return Behavior: Partial Identification from a Regression Discontinuity Design with Manipulation</title>
      <link>https://arxiv.org/abs/1910.02170</link>
      <description>arXiv:1910.02170v4 Announce Type: replace 
Abstract: Volunteer labor can temporarily yield lower benefits to charities than its costs. In such instances, organizations may wish to defer volunteer donations to a later date. Exploiting a discontinuity in blood donations' eligibility criteria, we show that deferring donors reduces their future volunteerism. In our setting, medical staff manipulates donors' reported hemoglobin levels over a threshold to facilitate donation. Such manipulation invalidates standard regression discontinuity design. To circumvent this issue, we propose a procedure for obtaining partial identification bounds where manipulation is present. Our procedure is applicable in various regression discontinuity settings where the running variable is manipulated.</description>
      <guid isPermaLink="false">oai:arXiv.org:1910.02170v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Evan Rosenman, Karthik Rajkumar, Romain Gauriot, Robert Slonim</dc:creator>
    </item>
    <item>
      <title>GEEPERs: Principal Stratification using Principal Scores and Stacked Estimating Equations</title>
      <link>https://arxiv.org/abs/2212.10406</link>
      <description>arXiv:2212.10406v3 Announce Type: replace 
Abstract: Principal stratification is a framework for making sense of causal effects conditioned on variables that themselves may have been affected by treatment. For instance, one component of an educational computer application is the availability of ``bottom-out'' hints that provide the answer. In evaluating a recent experimental evaluation against alternative programs without bottom-out hints, researchers may be interested in estimating separate average treatment effects for students who, if given the opportunity, would request bottom-out hints frequently, and for students who would not. Most principal stratification estimators rely on strong structural or modeling assumptions, and many require advanced statistical training to fit and check. In this paper, we introduce a new M-estimation principal effect estimator for one-way noncompliance based on a binary indicator. Estimates may be computed using conventional regressions (though the standard errors require a specialized sandwich formula) and do not rely on distributional assumptions. We present a simulation study that demonstrates the novel method's greater robustness compared to popular alternatives and illustrate the method through two real-data analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10406v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adam C. Sales, Kirk P. Vanacore, Erin R. Ottmar</dc:creator>
    </item>
    <item>
      <title>Demystifying Spatial Confounding</title>
      <link>https://arxiv.org/abs/2309.16861</link>
      <description>arXiv:2309.16861v3 Announce Type: replace 
Abstract: Spatial confounding is a fundamental issue in spatial regression models which arises because spatial random effects, included to approximate unmeasured spatial variation, are typically not independent of covariates in the model. This can lead to significant bias in covariate effect estimates. The problem is complex and has been the topic of extensive research with sometimes puzzling and seemingly contradictory results. Here, we develop a broad theoretical framework that brings mathematical clarity to the mechanisms of spatial confounding, providing explicit analytical expressions for the resulting bias. We see that the problem is directly linked to spatial smoothing and identify exactly how the size and occurrence of bias relate to the features of the spatial model as well as the underlying confounding scenario. Using our results, we can explain subtle and counter-intuitive behaviours. Finally, we propose a general approach for dealing with spatial confounding bias in practice, applicable for any spatial model specification. When a covariate has non-spatial information, we show that a general form of the so-called spatial+ method can be used to eliminate bias. When no such information is present, the situation is more challenging but, under the assumption of unconfounded high frequencies, we develop a procedure in which multiple capped versions of spatial+ are applied to assess the bias in this case. We illustrate our approach with an application to air temperature in Germany.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.16861v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emiko Dupont, Isa Marques, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>Price Experimentation and Interference</title>
      <link>https://arxiv.org/abs/2310.17165</link>
      <description>arXiv:2310.17165v4 Announce Type: replace 
Abstract: In this paper, we examine the biases that arise when firms run A/B tests on continuous parameters to estimate global treatment effects on performance metrics of interest; we particularly focus on price experiments to measure the price impact on quantity demanded, and on profit. In canonical A/B experimental estimators, biases emerge due to interference between market participants. We employ structural modeling and differential calculus to derive intuitive characterizations of these biases. We then specialize our general model to the standard revenue-management pricing problem. This setting highlights a fundamental risk innate to A/B pricing experiments: that the canonical estimator for the expected change in profits, counterintuitively, can have the em wrong sign in expectation. In other words, following the guidance of canonical estimators may lead firms to move prices (or fees) in the wrong direction, inadvertently decreasing profits. We introduce a novel debiasing technique for these canonical experiments, requiring only that firms equally split units between treatment and control. We apply these results to a two-sided market model, and demonstrate how the "change of sign" regime depends on market factors such as the supply/demand imbalance, and the price markup. We conclude by calibrating our two-sided market model to published empirical estimates from Airbnb marketplaces, demonstrating that estimators with the wrong sign are not a knife-edge issue, and that they may be prevalent enough to be of concern to practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17165v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ramesh Johari, Orrie B. Page, Gabriel Y. Weintraub</dc:creator>
    </item>
    <item>
      <title>Principal component analysis for max-stable distributions</title>
      <link>https://arxiv.org/abs/2408.10650</link>
      <description>arXiv:2408.10650v3 Announce Type: replace 
Abstract: Principal component analysis (PCA) is one of the most popular dimension reduction techniques in statistics and is especially powerful when a multivariate distribution is concentrated near a lower-dimensional subspace. Multivariate extreme value distributions have turned out to provide challenges for the application of PCA since their constraint support impedes the detection of lower-dimensional structures and heavy-tails can imply that second moments do not exist, thereby preventing the application of classical variance-based techniques for PCA. We adapt PCA to max-stable distributions using a regression setting and employ max-linear maps to project the random vector to a lower-dimensional space while preserving max-stability. We also provide a characterization of those distributions which allow for a perfect reconstruction from the lower-dimensional representation. Finally, we demonstrate how an optimal projection matrix can be consistently estimated and show viability in practice with a simulation study and application to a benchmark dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10650v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Reinbott, Anja Jan{\ss}en</dc:creator>
    </item>
    <item>
      <title>Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes</title>
      <link>https://arxiv.org/abs/2409.19241</link>
      <description>arXiv:2409.19241v3 Announce Type: replace 
Abstract: Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19241v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Na Bo, Ying Ding</dc:creator>
    </item>
    <item>
      <title>Testing conditional independence under isotonicity</title>
      <link>https://arxiv.org/abs/2501.06133</link>
      <description>arXiv:2501.06133v2 Announce Type: replace 
Abstract: We propose a test of the conditional independence of random variables $X$ and $Y$ given $Z$ under the additional assumption that $X$ is stochastically increasing in $Z$. The well-documented hardness of testing conditional independence means that some further restriction on the null hypothesis parameter space is required, but in contrast to existing approaches based on parametric models, smoothness assumptions, or approximations to the conditional distribution of $X$ given $Z$ and/or $Y$ given $Z$, our test requires only the stochastic monotonicity assumption. Our procedure, called PairSwap-ICI, determines the significance of a statistic by randomly swapping the $X$ values within ordered pairs of $Z$ values. The matched pairs and the test statistic may depend on both $Y$ and $Z$, providing the analyst with significant flexibility in constructing a powerful test. Our test offers finite-sample Type I error control, and provably achieves high power against a large class of alternatives that are not too close to the null. We validate our theoretical findings through a series of simulations and real data experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06133v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rohan Hore, Jake A. Soloff, Rina Foygel Barber, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>Gaussian Rank Verification</title>
      <link>https://arxiv.org/abs/2501.14142</link>
      <description>arXiv:2501.14142v3 Announce Type: replace 
Abstract: Statistical experiments often seek to identify random variables with the largest population means. This inferential task, known as rank verification, has been well-studied on Gaussian data with equal variances. This work provides the first treatment of the unequal variances case, utilizing ideas from the selective inference literature. We design a hypothesis test that verifies the rank of the largest observed value without losing power due to multiple testing corrections. This test is subsequently extended for two procedures: Identifying some number of correctly-ordered Gaussian means, and validating the top-K set. The testing procedures are validated on NHANES survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14142v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy Goldwasser, Will Fithian, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>Nonparametric Smoothing of Directional and Axial Data</title>
      <link>https://arxiv.org/abs/2501.17463</link>
      <description>arXiv:2501.17463v4 Announce Type: replace 
Abstract: We discuss generalized linear models for directional data where the conditional distribution of the response is a von Mises-Fisher distribution in arbitrary dimension or a Bingham distribution on the unit circle. To do this properly, we parametrize von Mises-Fisher distributions by Euclidean parameters and investigate computational aspects of this parametrization. Then we modify this approach for local polynomial regression as a means of nonparametric smoothing of distributional data. The methods are illustrated with simulated data and a data set from planetary sciences involving covariate vectors on a sphere with axial response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17463v4</guid>
      <category>stat.ME</category>
      <category>astro-ph.EP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lutz Duembgen, Caroline Haslebacher</dc:creator>
    </item>
    <item>
      <title>GLM Inference with AI-Generated Synthetic Data Using Misspecified Linear Regression</title>
      <link>https://arxiv.org/abs/2503.21968</link>
      <description>arXiv:2503.21968v2 Announce Type: replace 
Abstract: Data privacy concerns have led to the growing interest in synthetic data, which strives to preserve the statistical properties of the original dataset while ensuring privacy by excluding real records. Recent advances in deep neural networks and generative artificial intelligence have facilitated the generation of synthetic data. However, although prediction with synthetic data has been the focus of recent research, statistical inference with synthetic data remains underdeveloped. In particular, in many settings, including generalized linear models (GLMs), the estimator obtained using synthetic data converges much more slowly than in standard settings. To address these limitations, we propose a method that leverages summary statistics from the original data. Using a misspecified linear regression estimator, we then develop inference that greatly improves the convergence rate and restores the standard root-$n$ behavior for GLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21968v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nir Keret, Ali Shojaie</dc:creator>
    </item>
    <item>
      <title>Conditional Data Synthesis Augmentation</title>
      <link>https://arxiv.org/abs/2504.07426</link>
      <description>arXiv:2504.07426v2 Announce Type: replace 
Abstract: Reliable machine learning and statistical analysis rely on diverse, well-distributed training data. However, real-world datasets are often limited in size and exhibit underrepresentation across key subpopulations, leading to biased predictions and reduced performance, particularly in supervised tasks such as classification. To address these challenges, we propose Conditional Data Synthesis Augmentation (CoDSA), a novel framework that leverages generative models, such as diffusion models, to synthesize high-fidelity data for improving model performance across multimodal domains including tabular, textual, and image data. CoDSA generates synthetic samples that faithfully capture the conditional distributions of the original data, with a focus on under-sampled or high-interest regions. Through transfer learning, CoDSA fine-tunes pre-trained generative models to enhance the realism of synthetic data and increase sample density in sparse areas. This process preserves inter-modal relationships, mitigates data imbalance, improves domain adaptation, and boosts generalization. We also introduce a theoretical framework that quantifies the statistical accuracy improvements enabled by CoDSA as a function of synthetic sample volume and targeted region allocation, providing formal guarantees of its effectiveness. Extensive experiments demonstrate that CoDSA consistently outperforms non-adaptive augmentation strategies and state-of-the-art baselines in both supervised and unsupervised settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07426v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Tian, Xiaotong Shen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Instrumental Variable Inference with Many Weak Instruments</title>
      <link>https://arxiv.org/abs/2505.07729</link>
      <description>arXiv:2505.07729v2 Announce Type: replace 
Abstract: We study inference on linear functionals in the nonparametric instrumental variable (NPIV) problem with a discretely-valued instrument under a many-weak-instruments asymptotic regime, where the number of instrument values grows with the sample size. A key motivating example is estimating long-term causal effects in a new experiment with only short-term outcomes, using past experiments to instrument for the effect of short- on long-term outcomes. Here, the assignment to a past experiment serves as the instrument: we have many past experiments but only a limited number of units in each. Since the structural function is nonparametric but constrained by only finitely many moment restrictions, point identification typically fails. To address this, we consider linear functionals of the minimum-norm solution to the moment restrictions, which is always well-defined. As the number of instrument levels grows, these functionals define an approximating sequence to a target functional, replacing point identification with a weaker asymptotic notion suited to discrete instruments. Extending the Jackknife Instrumental Variable Estimator (JIVE) beyond the classical parametric setting, we propose npJIVE, a nonparametric estimator for solutions to linear inverse problems with many weak instruments. We construct automatic debiased machine learning estimators for linear functionals of both the structural function and its minimum-norm projection, and establish their efficiency in the many-weak-instruments regime. To do so, we develop a general semiparametric efficiency theory for regular estimators under weak identification and many-weak-instrument asymptotics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.07729v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Confidence sequences with informative, bounded-influence priors</title>
      <link>https://arxiv.org/abs/2506.22925</link>
      <description>arXiv:2506.22925v2 Announce Type: replace 
Abstract: Confidence sequences are collections of confidence regions that simultaneously cover the true parameter for every sample size at a prescribed confidence level. Tightening these sequences is of practical interest and can be achieved by incorporating prior information through the method of mixture martingales. However, confidence sequences built from informative priors are vulnerable to misspecification and may become vacuous when the prior is poorly chosen. We study this trade-off for Gaussian observations with known variance. By combining the method of mixtures with a global informative prior whose tails are polynomial or exponential and the extended Ville's inequality, we construct confidence sequences that are sharper than their non-informative counterparts whenever the prior is well specified, yet remain bounded under arbitrary misspecification. The theory is illustrated with several classical priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22925v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefano Cortinovis, Valentin Kilian, Fran\c{c}ois Caron</dc:creator>
    </item>
    <item>
      <title>A Test for Jumps in Metric-Space Conditional Means</title>
      <link>https://arxiv.org/abs/2507.04560</link>
      <description>arXiv:2507.04560v2 Announce Type: replace 
Abstract: Standard methods for detecting discontinuities in conditional means are not applicable to outcomes that are complex, non-Euclidean objects like distributions, networks, or covariance matrices. This article develops a nonparametric test for jumps in conditional means when outcomes lie in a non-Euclidean metric space. Using local Fr\'echet regression, the method estimates a mean path on either side of a candidate cutoff. This extends existing $k$-sample tests to a non-parametric regression setting with metric-space valued outcomes. I establish the asymptotic distribution of the test and its consistency against contiguous alternatives. For this, I derive a central limit theorem for the local estimator of the conditional Fr\'echet variance and a consistent estimator of its asymptotic variance. Simulations confirm nominal size control and robust power in finite samples. Two empirical illustrations demonstrate the method's ability to reveal discontinuities missed by scalar-based tests. I find sharp changes in (i) work-from-home compositions at an income threshold for non-compete enforceability and (ii) national input-output networks following the loss of preferential U.S. trade access. These findings show the value of analyzing regression outcomes in their native metric spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04560v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Total/dual correlation/coherence, redundancy/synergy, complexity, and O-information for real and complex valued multivariate data</title>
      <link>https://arxiv.org/abs/2507.08773</link>
      <description>arXiv:2507.08773v2 Announce Type: replace 
Abstract: Firstly, assuming Gaussianity, equations for the following information theory measures are presented: total correlation/coherence (TC), dual total correlation/coherence (DTC), O-information, TSE complexity, and redundancy-synergy index (RSI). Since these measures are functions of the covariance matrix "S" and its inverse "S^-1", the associated Wishart and inverse-Wishart distributions are of note. DTC is shown to be the Kullback-Leibler (KL) divergence for the inverse-Wishart pair "(S^-1)" and its diagonal matrix "D=diag(S^-1)", shedding light on its interpretation as a measure of "total partial correlation", -lndetP, with test hypothesis H0: P=I, where "P" is the standardized inverse covariance (i.e. P=(D^-1/2)(S^-1)(D^-1/2). The second aim of this paper introduces a generalization of all these measures for structured groups of variables. For instance, consider three or more groups, each consisting of three or more variables, with predominant redundancy within each group, but with synergistic interactions between groups. O-information will miss the between group synergy (since redundancy occurs more often in the system). In contrast, the structured O-information measure presented here will correctly report predominant synergy between groups. This is a relevant generalization towards structured multivariate information measures. A third aim is the presentation of a framework for quantifying the contribution of "connections" between variables, to the system's TC, DTC, O-information, and TSE complexity. A fourth aim is to present a generalization of the redundancy-synergy index for quantifying the contribution of a group of variables to the system's redundancy-synergy balance. Finally, it is shown that the expressions derived here directly apply to data from several other elliptical distributions. All program codes, data files, and executables are available (https://osf.io/jd37g/).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.08773v2</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roberto D. Pascual-Marqui, Kieko Kochi, Toshihiko Kinoshita</dc:creator>
    </item>
    <item>
      <title>Elementary proofs of several results on false discovery rate</title>
      <link>https://arxiv.org/abs/2201.09350</link>
      <description>arXiv:2201.09350v3 Announce Type: replace-cross 
Abstract: We collect self-contained elementary proofs of four results in the literature on the false discovery rate of the Benjamini-Hochberg (BH) procedure for independent or positive-regression dependent p-values, the Benjamini-Yekutieli correction for arbitrarily dependent p-values, and the e-BH procedure for arbitrarily dependent e-values. As a corollary, the above proofs also lead to some inequalities of Simes and Hommel.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.09350v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Score Attack: A Lower Bound Technique for Optimal Differentially Private Learning</title>
      <link>https://arxiv.org/abs/2303.07152</link>
      <description>arXiv:2303.07152v2 Announce Type: replace-cross 
Abstract: Achieving optimal statistical performance while ensuring the privacy of personal data is a challenging yet crucial objective in modern data analysis. However, characterizing the optimality, particularly the minimax lower bound, under privacy constraints is technically difficult. To address this issue, we propose a novel approach called the score attack, which provides a lower bound on the differential-privacy-constrained minimax risk of parameter estimation. The score attack method is based on the tracing attack concept in differential privacy and can be applied to any statistical model with a well-defined score statistic. It can optimally lower bound the minimax risk of estimating unknown model parameters, up to a logarithmic factor, while ensuring differential privacy for a range of statistical problems. We demonstrate the effectiveness and optimality of this general method in various examples, such as the generalized linear model in both classical and high-dimensional sparse settings, the Bradley-Terry-Luce model for pairwise comparisons, and non-parametric regression over the Sobolev class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07152v2</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>T. Tony Cai, Yichen Wang, Linjun Zhang</dc:creator>
    </item>
    <item>
      <title>Principal Component Copulas for Capital Modelling and Systemic Risk</title>
      <link>https://arxiv.org/abs/2312.13195</link>
      <description>arXiv:2312.13195v3 Announce Type: replace-cross 
Abstract: We introduce a class of copulas that we call Principal Component Copulas (PCCs). This class combines the strong points of copula-based techniques with principal component analysis (PCA), which results in flexibility when modelling tail dependence along the most important directions in high-dimensional data. We obtain theoretical results for PCCs that are important for practical applications. In particular, we derive tractable expressions for the high-dimensional copula density, which can be represented in terms of characteristic functions. We also develop algorithms to perform Maximum Likelihood and Generalized Method of Moment estimation in high-dimensions and show very good performance in simulation experiments. Finally, we apply the copula to the international stock market to study systemic risk. We find that PCCs lead to excellent performance on measures of systemic risk due to their ability to distinguish between parallel and orthogonal movements in the global market, which have a different impact on systemic risk and diversification. As a result, we consider the PCC promising for capital models, which financial institutions use to protect themselves against systemic risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13195v3</guid>
      <category>q-fin.RM</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. B. Gubbels, J. Y. Ypma, C. W. Oosterlee</dc:creator>
    </item>
    <item>
      <title>Winners with Confidence: Discrete Argmin Inference with an Application to Model Selection</title>
      <link>https://arxiv.org/abs/2408.02060</link>
      <description>arXiv:2408.02060v3 Announce Type: replace-cross 
Abstract: We study the problem of finding the index of the minimum value of a vector from noisy observations. This problem is relevant in population/policy comparison, discrete maximum likelihood, and model selection. We develop an asymptotically normal test statistic, even in high-dimensional settings and with potentially many ties in the population mean vector, by integrating concepts and tools from cross-validation and differential privacy. The key technical ingredient is a central limit theorem for globally dependent data. We also propose practical ways to select the tuning parameter that adapts to the signal landscape. Numerical experiments and data examples demonstrate the ability of the proposed method to achieve a favorable bias-variance trade-off in practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02060v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Hao Lee, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Unconditional Randomization Tests for Interference</title>
      <link>https://arxiv.org/abs/2409.09243</link>
      <description>arXiv:2409.09243v3 Announce Type: replace-cross 
Abstract: Researchers are often interested in the existence and extent of interference between units when conducting causal inference or designing policy. However, testing for interference presents significant econometric challenges, particularly due to complex clustering patterns and dependencies that can invalidate standard methods. This paper introduces the pairwise imputation-based randomization test (PIRT), a general and robust framework for assessing the existence and extent of interference in experimental settings. PIRT employs unconditional randomization testing and pairwise comparisons, enabling straightforward implementation and ensuring finite-sample validity under minimal assumptions about network structure. The method's practical value is demonstrated through an application to a large-scale policing experiment in Bogota, Colombia (Blattman et al., 2021), which evaluates the effects of hotspot policing on crime at the street segment level. The analysis reveals that increased police patrolling in hotspots significantly displaces violent crime, but not property crime. Simulations calibrated to this context further underscore the power and robustness of PIRT.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09243v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Zhong</dc:creator>
    </item>
    <item>
      <title>LITE: Efficiently Estimating Gaussian Probability of Maximality</title>
      <link>https://arxiv.org/abs/2501.13535</link>
      <description>arXiv:2501.13535v3 Announce Type: replace-cross 
Abstract: We consider the problem of computing the probability of maximality (PoM) of a Gaussian random vector, i.e., the probability for each dimension to be maximal. This is a key challenge in applications ranging from Bayesian optimization to reinforcement learning, where the PoM not only helps with finding an optimal action, but yields a fine-grained analysis of the action domain, crucial in tasks such as drug discovery. Existing techniques are costly, scaling polynomially in computation and memory with the vector size. We introduce LITE, the first approach for estimating Gaussian PoM with almost-linear time and memory complexity. LITE achieves SOTA accuracy on a number of tasks, while being in practice several orders of magnitude faster than the baselines. This also translates to a better performance on downstream tasks such as entropy estimation and optimal control of bandits. Theoretically, we cast LITE as entropy-regularized UCB and connect it to prior PoM estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.13535v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Menet (ETH Z\"urich), Jonas H\"ubotter (ETH Z\"urich), Parnian Kassraie (ETH Z\"urich), Andreas Krause (ETH Z\"urich)</dc:creator>
    </item>
    <item>
      <title>Multivariate Species Sampling Models</title>
      <link>https://arxiv.org/abs/2503.24004</link>
      <description>arXiv:2503.24004v2 Announce Type: replace-cross 
Abstract: Species sampling processes have long served as the fundamental framework for modeling random discrete distributions and exchangeable sequences. However, data arising from distinct but related sources require a broader notion of probabilistic invariance, making partial exchangeability a natural choice. Countless models for partially exchangeable data, collectively known as dependent nonparametric priors, have been proposed. These include hierarchical, nested and additive processes, widely used in statistics and machine Learning. Still, a unifying framework is lacking and key questions about their underlying learning mechanisms remain unanswered. We fill this gap by introducing multivariate species sampling models, a new general class of nonparametric priors that encompasses most existing finite- and infinite-dimensional dependent processes. They are characterized by the induced partially exchangeable partition probability function encoding their multivariate clustering structure. We establish their core distributional properties and analyze their dependence structure, demonstrating that borrowing of information across groups is entirely determined by shared ties. This provides new insights into the underlying learning mechanisms, offering, for instance, a principled rationale for the previously unexplained correlation structure observed in existing models. 
Beyond providing a cohesive theoretical foundation, our approach serves as a constructive tool for developing new models and opens novel research directions to capture richer dependence structures beyond the framework of multivariate species sampling processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.24004v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beatrice Franzolini, Antonio Lijoi, Igor Pr\"unster, Giovanni Rebaudo</dc:creator>
    </item>
    <item>
      <title>Moment Restrictions for Nonlinear Panel Data Models with Feedback</title>
      <link>https://arxiv.org/abs/2506.12569</link>
      <description>arXiv:2506.12569v2 Announce Type: replace-cross 
Abstract: Many panel data methods, while allowing for general dependence between covariates and time-invariant agent-specific heterogeneity, place strong a priori restrictions on feedback: how past outcomes, covariates, and heterogeneity map into future covariate levels. Ruling out feedback entirely, as often occurs in practice, is unattractive in many dynamic economic settings. We provide a general characterization of all feedback and heterogeneity robust (FHR) moment conditions for nonlinear panel data models and present constructive methods to derive feasible moment-based estimators for specific models. We also use our moment characterization to compute semiparametric efficiency bounds, allowing for a quantification of the information loss associated with accommodating feedback, as well as providing insight into how to construct estimators with good efficiency properties in practice. Our results apply both to the finite dimensional parameter indexing the parametric part of the model as well as to estimands that involve averages over the distribution of unobserved heterogeneity. We illustrate our methods by providing a complete characterization of all FHR moment functions in the multi-spell mixed proportional hazards model. We compute efficient moment functions for both model parameters and average effects in this setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12569v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>St\'ephane Bonhomme, Kevin Dano, Bryan S. Graham</dc:creator>
    </item>
  </channel>
</rss>
