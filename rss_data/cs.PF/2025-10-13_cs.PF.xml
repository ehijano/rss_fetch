<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.PF updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.PF</link>
    <description>cs.PF updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.PF" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 13 Oct 2025 04:01:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Prioritizing Latency with Profit: A DRL-Based Admission Control for 5G Network Slices</title>
      <link>https://arxiv.org/abs/2510.08769</link>
      <description>arXiv:2510.08769v1 Announce Type: cross 
Abstract: 5G networks enable diverse services such as eMBB, URLLC, and mMTC through network slicing, necessitating intelligent admission control and resource allocation to meet stringent QoS requirements while maximizing Network Service Provider (NSP) profits. However, existing Deep Reinforcement Learning (DRL) frameworks focus primarily on profit optimization without explicitly accounting for service delay, potentially leading to QoS violations for latency-sensitive slices. Moreover, commonly used epsilon-greedy exploration of DRL often results in unstable convergence and suboptimal policy learning. To address these gaps, we propose DePSAC -- a Delay and Profit-aware Slice Admission Control scheme. Our DRL-based approach incorporates a delay-aware reward function, where penalties due to service delay incentivize the prioritization of latency-critical slices such as URLLC. Additionally, we employ Boltzmann exploration to achieve smoother and faster convergence. We implement and evaluate DePSAC on a simulated 5G core network substrate with realistic Network Slice Request (NSLR) arrival patterns. Experimental results demonstrate that our method outperforms the DSARA baseline in terms of overall profit, reduced URLLC slice delays, improved acceptance rates, and improved resource consumption. These findings validate the effectiveness of the proposed DePSAC in achieving better QoS-profit trade-offs for practical 5G network slicing scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.08769v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Proggya Chakraborty, Aaquib Asrar, Jayasree Sengupta, Sipra Das Bit</dc:creator>
    </item>
    <item>
      <title>Assessing the Impact of Post-Quantum Digital Signature Algorithms on Blockchains</title>
      <link>https://arxiv.org/abs/2510.09271</link>
      <description>arXiv:2510.09271v1 Announce Type: cross 
Abstract: The advent of quantum computing threatens the security of traditional encryption algorithms, motivating the development of post-quantum cryptography (PQC). In 2024, the National Institute of Standards and Technology (NIST) standardized several PQC algorithms, marking an important milestone in the transition toward quantum-resistant security. Blockchain systems fundamentally rely on cryptographic primitives to guarantee data integrity and transaction authenticity. However, widely used algorithms such as ECDSA, employed in Bitcoin, Ethereum, and other networks, are vulnerable to quantum attacks. Although adopting PQC is essential for long-term security, its computational overhead in blockchain environments remains largely unexplored. In this work, we propose a methodology for benchmarking both PQC and traditional cryptographic algorithms in blockchain contexts. We measure signature generation and verification times across diverse computational environments and simulate their impact at scale. Our evaluation focuses on PQC digital signature schemes (ML-DSA, Dilithium, Falcon, Mayo, SLH-DSA, SPHINCS+, and Cross) across security levels 1 to 5, comparing them to ECDSA, the current standard in Bitcoin and Ethereum. Our results indicate that PQC algorithms introduce only minor performance overhead at security level 1, while in some scenarios they significantly outperform ECDSA at higher security levels. For instance, ML-DSA achieves a verification time of 0.14 ms on an ARM-based laptop at level 5, compared to 0.88 ms for ECDSA. We also provide an open-source implementation to ensure reproducibility and encourage further research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09271v1</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.PF</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alison Gon\c{c}alves Schemitt (PUCRS), Henrique Fan da Silva (UNIPAMPA), Roben Castagna Lunardi (PUCRS, IFRS), Diego Kreutz (UNIPAMPA), Rodrigo Brand\~ao Mansilha (UNIPAMPA), Avelino Francisco Zorzo (PUCRS)</dc:creator>
    </item>
    <item>
      <title>A Framework for Distributed Resource Allocation in Quantum Networks</title>
      <link>https://arxiv.org/abs/2510.09371</link>
      <description>arXiv:2510.09371v1 Announce Type: cross 
Abstract: We introduce a distributed resource allocation framework for the Quantum Internet that relies on feedback-based, fully decentralized coordination to serve multiple co-existing applications. We develop quantum network control algorithms under the mathematical framework of Quantum Network Utility Maximization (QNUM), where utility functions quantify network performance by mapping entanglement rate and quality into a joint optimization objective. We then introduce QPrimal-Dual, a decentralized, scalable algorithm that solves QNUM by strategically placing network controllers that operate using local state information and limited classical message exchange. We prove global asymptotic stability for concave, separable utility functions, and provide sufficient conditions for local stability for broader non-concave cases. To reduce control overhead and account for quantum memory decoherence, we also propose schemes that locally approximate global quantities and prevent congestion in the network. We evaluate the performance of our approach via simulations in realistic quantum network architectures. Results show that QPrimalDual significantly outperforms baseline allocation strategies, scales with network size, and is robust to latency and decoherence. Our observations suggest that QPrimalDual could be a practical, high-performance foundation for fully distributed resource allocation in quantum networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.09371v1</guid>
      <category>quant-ph</category>
      <category>cs.PF</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nitish K. Panigrahy, Leonardo Bacciottini, C. V. Hollot, Emily A. Van Milligen, Matheus Guedes de Andrade, Nageswara S. V. Rao, Gayane Vardoyan, Don Towsley</dc:creator>
    </item>
    <item>
      <title>Prompt-Aware Scheduling for Low-Latency LLM Serving</title>
      <link>https://arxiv.org/abs/2510.03243</link>
      <description>arXiv:2510.03243v2 Announce Type: replace-cross 
Abstract: Efficient scheduling of LLM inference tasks is essential for achieving low latency and high throughput, particularly with the growing use of reasoning-capable LLMs. Traditional strategies like First-Come-First-Serve (FCFS) often suffer from Head-of-Line (HOL) blocking, where long-running tasks delay shorter ones queued behind them. In this paper, we introduce PARS, a prompt-aware LLM task scheduler that improves serving efficiency by approximating shortest-job-first (SJF) scheduling through pairwise ranking with margin ranking loss. PARS focuses on impactful scheduling decisions and is seamlessly integrated into the state-of-the-art LLM serving system vLLM. It effectively predicts response-length-based task ordering, reducing latency with minimal overhead. Extensive experiments across multiple LLMs and real-world inference datasets show that PARS significantly improves performance, including for reasoning workloads. Furthermore, our cross-model evaluations demonstrate that the design generalizes well, enabling effective scheduling even when predictors are trained on different LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03243v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 13 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiheng Tao, Yihe Zhang, Matthew T. Dearing, Xin Wang, Yuping Fan, Zhiling Lan</dc:creator>
    </item>
  </channel>
</rss>
