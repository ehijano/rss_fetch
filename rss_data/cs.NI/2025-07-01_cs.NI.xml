<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Jul 2025 01:33:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Golden Ratio Assisted Localization for Wireless Sensor Network</title>
      <link>https://arxiv.org/abs/2506.22464</link>
      <description>arXiv:2506.22464v1 Announce Type: new 
Abstract: This paper presents a novel localization algorithm for wireless sensor networks (WSNs) called Golden Ratio Localization (GRL), which leverages the mathematical properties of the golden ratio (phi 1.618) to optimize both node placement and communication range. GRL introduces phi-based anchor node deployment and hop-sensitive weighting using phi-exponents to improve localization accuracy while minimizing energy consumption. Through extensive simulations conducted on a 100 m * 100 m sensor field with 100 nodes and 10 anchors, GRL achieved an average localization error of 2.35 meters, outperforming DV- Hop (3.87 meters) and Centroid (4.95 meters). In terms of energy efficiency, GRL reduced localization energy consumption to 1.12 microJ per node, compared to 1.78 microJ for DV-Hop and 1.45 microJ for Centroid. These results confirm that GRL provides a more balanced and efficient localization approach, making it especially suitable for energy-constrained and large-scale WSN deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22464v1</guid>
      <category>cs.NI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hitesh Mohapatra</dc:creator>
    </item>
    <item>
      <title>Reliable Transmission of LTP Using Reinforcement Learning-Based Adaptive FEC</title>
      <link>https://arxiv.org/abs/2506.22470</link>
      <description>arXiv:2506.22470v1 Announce Type: new 
Abstract: Delay/Disruption Tolerant Networking (DTN) employs the Licklider Transmission Protocol (LTP) with Automatic Repeat reQuest (ARQ) for reliable data delivery in challenging interplanetary networks. While previous studies have integrated packet-level Forward Erasure Correction (FEC) into LTP to reduce retransmission time costs, existing static and delay-feedback-based dynamic coding methods struggle with highly variable and unpredictable deep space channel conditions. This paper proposes a reinforcement learning (RL)-based adaptive FEC algorithm to address these limitations. The algorithm utilizes historical feedback and system state to predict future channel conditions and proactively adjust the code rate. This approach aims to anticipate channel quality degradation, thereby preventing decoding failures and subsequent LTP retransmissions and improving coding efficiency by minimizing redundancy during favorable channel conditions. Performance evaluations conducted in simulated Earth-Moon and Earth-Mars link scenarios demonstrate this algorithm's effectiveness in optimizing data transmission for interplanetary networks. Compared to existing methods, this approach demonstrates significant improvement, with matrix decoding failures reduced by at least 2/3.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22470v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Chen, Yu Song, Kanglian Zhao, Juan A. Fraire, Wenfeng Li</dc:creator>
    </item>
    <item>
      <title>RL-based Adaptive Task Offloading in Mobile-Edge Computing for Future IoT Networks</title>
      <link>https://arxiv.org/abs/2506.22474</link>
      <description>arXiv:2506.22474v1 Announce Type: new 
Abstract: The Internet of Things (IoT) has been increasingly used in our everyday lives as well as in numerous industrial applications. However, due to limitations in computing and power capabilities, IoT devices need to send their respective tasks to cloud service stations that are usually located at far distances. Having to transmit data far distances introduces challenges for services that require low latency such as industrial control in factories and plants as well as artificial intelligence assisted autonomous driving. To solve this issue, mobile edge computing (MEC) is deployed at the networks edge to reduce transmission time. In this regard, this study proposes a new offloading scheme for MEC-assisted ultra dense cellular networks using reinforcement learning (RL) techniques. The proposed scheme enables efficient resource allocation and dynamic offloading decisions based on varying network conditions and user demands. The RL algorithm learns from the networks historical data and adapts the offloading decisions to optimize the networks overall performance. Non-orthogonal multiple access is also adopted to improve resource utilization among the IoT devices. Simulation results demonstrate that the proposed scheme outperforms other stateof the art offloading algorithms in terms of energy efficiency, network throughput, and user satisfaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22474v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziad Qais Al Abbasi, Khaled M. Rabie, Senior Member, Xingwang Li, Senior Member, Wali Ullah Khan, Asma Abu Samah</dc:creator>
    </item>
    <item>
      <title>Innovative Research on IoT Architecture and Robotic Operating Platforms: Applications of Large Language Models and Generative AI</title>
      <link>https://arxiv.org/abs/2506.22477</link>
      <description>arXiv:2506.22477v1 Announce Type: new 
Abstract: This paper introduces an innovative design for robotic operating platforms, underpinned by a transformative Internet of Things (IoT) architecture, seamlessly integrating cutting-edge technologies such as large language models (LLMs), generative AI, edge computing, and 5G networks. The proposed platform aims to elevate the intelligence and autonomy of IoT systems and robotics, enabling them to make real-time decisions and adapt dynamically to changing environments. Through a series of compelling case studies across industries including smart manufacturing, healthcare, and service sectors, this paper demonstrates the substantial potential of IoT-enabled robotics to optimize operational workflows, enhance productivity, and deliver innovative, scalable solutions. By emphasizing the roles of LLMs and generative AI, the research highlights how these technologies drive the evolution of intelligent robotics and IoT, shaping the future of industry-specific advancements. The findings not only showcase the transformative power of these technologies but also offer a forward-looking perspective on their broader societal and industrial implications, positioning them as catalysts for next-generation automation and technological convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22477v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.RO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/RICAI64321.2024.10911316</arxiv:DOI>
      <arxiv:journal_reference>2024 6th International Conference on Robotics, Intelligent Control and Artificial Intelligence (RICAI), 2024, IEEE Xplore, pp. 881-886</arxiv:journal_reference>
      <dc:creator>Huiwen Han</dc:creator>
    </item>
    <item>
      <title>Service Placement in Small Cell Networks Using Distributed Best Arm Identification in Linear Bandits</title>
      <link>https://arxiv.org/abs/2506.22480</link>
      <description>arXiv:2506.22480v1 Announce Type: new 
Abstract: As users in small cell networks increasingly rely on computation-intensive services, cloud-based access often results in high latency. Multi-access edge computing (MEC) mitigates this by bringing computational resources closer to end users, with small base stations (SBSs) serving as edge servers to enable low-latency service delivery. However, limited edge capacity makes it challenging to decide which services to deploy locally versus in the cloud, especially under unknown service demand and dynamic network conditions. To tackle this problem, we model service demand as a linear function of service attributes and formulate the service placement task as a linear bandit problem, where SBSs act as agents and services as arms. The goal is to identify the service that, when placed at the edge, offers the greatest reduction in total user delay compared to cloud deployment. We propose a distributed and adaptive multi-agent best-arm identification (BAI) algorithm under a fixed-confidence setting, where SBSs collaborate to accelerate learning. Simulations show that our algorithm identifies the optimal service with the desired confidence and achieves near-optimal speedup, as the number of learning rounds decreases proportionally with the number of SBSs. We also provide theoretical analysis of the algorithm's sample complexity and communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22480v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariam Yahya, Aydin Sezgin, Setareh Maghsudi</dc:creator>
    </item>
    <item>
      <title>Wireless Home Automation Using Social Networking Websites</title>
      <link>https://arxiv.org/abs/2506.22482</link>
      <description>arXiv:2506.22482v1 Announce Type: new 
Abstract: With the advent of Internet of Things, Wireless Home Automation Systems WHAS are gradually gaining popularity. These systems are faced with multiple challenges such as security; controlling a variety of home appliances with a single interface and user friendliness. In this paper we propose a system that uses secure authentication systems of social networking websites such as Twitter, tracks the end-users activities on the social network and then control his or her domestic appliances. At the end, we highlight the applications of the proposed WHAS and compare the advantages of our proposed system over traditional home automation systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22482v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ADCOM.2014.7103241</arxiv:DOI>
      <dc:creator>Divya Alok Gupta, Dwith Chenna, B. Aditya Vighnesh Ramakanth</dc:creator>
    </item>
    <item>
      <title>An Urban Multi-Operator QoE-Aware Dataset for Cellular Networks in Dense Environments</title>
      <link>https://arxiv.org/abs/2506.22484</link>
      <description>arXiv:2506.22484v1 Announce Type: new 
Abstract: Urban cellular networks face complex performance challenges due to high infrastructure density, varied user mobility, and diverse service demands. While several datasets address network behaviour across different environments, there is a lack of datasets that captures user centric Quality of Experience (QoE), and diverse mobility patterns needed for efficient network planning and optimization solutions, which are important for QoE driven optimizations and mobility management. This study presents a curated dataset of 30,925 labelled records, collected using GNetTrack Pro within a 2 km2 dense urban area, spanning three major commercial network operators. The dataset captures key signal quality parameters (e.g., RSRP, RSRQ, SNR), across multiple real world mobility modes including pedestrian routes, canopy walkways, shuttle buses, and Bus Rapid Transit (BRT) routes. It also includes diverse network traffic scenarios including (1) FTP upload and download, (2) video streaming, and (3) HTTP browsing. A total of 132 physical cell sites were identified and validated through OpenCellID and on-site field inspections, illustrating the high cell density characteristic of 5G and emerging heterogeneous network deployment. The dataset is particularly suited for machine learning applications, such as handover optimization, signal quality prediction, and multi operator performance evaluation. Released in a structured CSV format with accompanying preprocessing and visualization scripts, this dataset offers a reproducible, application ready resource for researchers and practitioners working on urban cellular network planning and optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22484v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Muhammad Kabeer, Rosdiadee Nordin, Mehran Behjati, Farah Yasmin binti Mohd Shaharuddin</dc:creator>
    </item>
    <item>
      <title>AGI Enabled Solutions For IoX Layers Bottlenecks In Cyber-Physical-Social-Thinking Space</title>
      <link>https://arxiv.org/abs/2506.22487</link>
      <description>arXiv:2506.22487v1 Announce Type: new 
Abstract: The integration of the Internet of Everything (IoX) and Artificial General Intelligence (AGI) has given rise to a transformative paradigm aimed at addressing critical bottlenecks across sensing, network, and application layers in Cyber-Physical-Social Thinking (CPST) ecosystems. In this survey, we provide a systematic and comprehensive review of AGI-enhanced IoX research, focusing on three key components: sensing-layer data management, network-layer protocol optimization, and application-layer decision-making frameworks. Specifically, this survey explores how AGI can mitigate IoX bottlenecks challenges by leveraging adaptive sensor fusion, edge preprocessing, and selective attention mechanisms at the sensing layer, while resolving network-layer issues such as protocol heterogeneity and dynamic spectrum management, neuro-symbolic reasoning, active inference, and causal reasoning, Furthermore, the survey examines AGI-enabled frameworks for managing identity and relationship explosion. Key findings suggest that AGI-driven strategies, such as adaptive sensor fusion, edge preprocessing, and semantic modeling, offer novel solutions to sensing-layer data overload, network-layer protocol heterogeneity, and application-layer identity explosion. The survey underscores the importance of cross-layer integration, quantum-enabled communication, and ethical governance frameworks for future AGI-enabled IoX systems. Finally, the survey identifies unresolved challenges, such as computational requirements, scalability, and real-world validation, calling for further research to fully realize AGI's potential in addressing IoX bottlenecks. we believe AGI-enhanced IoX is emerging as a critical research field at the intersection of interconnected systems and advanced AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22487v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amar Khelloufi, Huansheng Ning, Sahraoui Dhelim, Jianguo Ding</dc:creator>
    </item>
    <item>
      <title>Integrated Multimodal Sensing and Communication: Challenges, Technologies, and Architectures</title>
      <link>https://arxiv.org/abs/2506.22507</link>
      <description>arXiv:2506.22507v1 Announce Type: new 
Abstract: The evolution towards 6G networks requires the intelligent integration of communication and sensing capabilities to support diverse and complex applications, such as autonomous driving and immersive services. However, existing integrated sensing and communication (ISAC) systems predominantly rely on single-modal sensors as primary participants, which leads to a limited representation of environmental features and significant performance bottlenecks under the emerging requirements of 6G applications. This limitation motivates a paradigm shift from single-modal to multimodal ISAC. In this article, we first analyze the key challenges in realizing multimodal ISAC, including the fusion of heterogeneous multimodal data, the high communication overhead among distributed sensors, and the design of efficient and scalable system architectures. We then introduce several enabling technologies, such as large AI models, semantic communication, and multi-agent systems, that hold promise for addressing these challenges. To operationalize these technologies, we zoom into three architectural paradigms: fusion-based multimodal ISAC (F-MAC), interaction-based multimodal ISAC (I-MAC), and relay-based multimodal ISAC (R-MAC), each tailored to organize devices and modalities for efficient collaboration in different scenarios. Thereafter, a case study is presented based on the F-MAC scheme, demonstrating that the scheme achieves more comprehensive sensing and improves sensing accuracy by approximately 80% compared to conventional single-modal ISAC systems. Finally, we discuss several open issues to be addressed in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22507v1</guid>
      <category>cs.NI</category>
      <category>cs.MA</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yubo Peng, Luping Xiang, Kun Yang, Feibo Jiang, Kezhi Wang, Christos Masouros</dc:creator>
    </item>
    <item>
      <title>Towards an Optimized Multi-Cyclic Queuing and Forwarding in Time Sensitive Networking with Time Injection</title>
      <link>https://arxiv.org/abs/2506.22671</link>
      <description>arXiv:2506.22671v1 Announce Type: new 
Abstract: Cyclic Queuing and Forwarding (CQF) is a Time-Sensitive Networking (TSN) shaping mechanism that provides bounded latency and deterministic Quality of Service (QoS). However, CQF's use of a single cycle restricts its ability to support TSN traffic with diverse timing requirements. Multi-Cyclic Queuing and Forwarding (Multi-CQF) is a new and emerging TSN shaping mechanism that uses multiple cycles on the same egress port, allowing it to accommodate TSN flows with varied timing requirements more effectively than CQF. Despite its potential, current Multi-CQF configuration studies are limited, leading to a lack of comprehensive research, poor understanding of the mechanism, and limited adoption of Multi-CQF in practical applications. Previous work has shown the impact of Time Injection (TI), defined as the start time of Time-Triggered (TT) flows at the source node, on CQF queue resource utilization. However, the impact of TI has not yet been explored in the context of Multi-CQF. This paper introduces a set of constraints and leverages Domain Specific Knowledge (DSK) to reduce the search space for Multi-CQF configuration. Building on this foundation, we develop an open-source Genetic Algorithm (GA) and a hybrid GA-Simulated Annealing (GASA) approach to efficiently configure Multi-CQF networks and introduce TI in Multi-CQF to enhance schedulability. Experimental results show that our proposed algorithms significantly increase the number of scheduled TT flows compared to the baseline Simulated Annealing (SA) model, improving scheduling by an average of 15%. Additionally, GASA achieves a 20% faster convergence rate and lower time complexity, outperforming the SA model in speed, and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22671v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rubi Debnath, Mohammadreza Barzegaran, Sebastian Steinhorst</dc:creator>
    </item>
    <item>
      <title>Trusted Routing for Blockchain-Enabled Low-Altitude Intelligent Networks</title>
      <link>https://arxiv.org/abs/2506.22745</link>
      <description>arXiv:2506.22745v1 Announce Type: new 
Abstract: Due to the scalability and portability, the low-altitude intelligent networks (LAINs) are essential in various fields such as surveillance and disaster rescue. However, in LAINs, unmanned aerial vehicles (UAVs) are characterized by the distributed topology and high dynamic mobility, and vulnerable to security threats, which may degrade the routing performance for data transmission. Hence, how to ensure the routing stability and security of LAINs is a challenge. In this paper, we focus on the routing process in LAINs with multiple UAV clusters and propose the blockchain-enabled zero-trust architecture to manage the joining and exiting of UAVs. Furthermore, we formulate the routing problem to minimize the end-to-end (E2E) delay, which is an integer linear programming and intractable to solve. Therefore, considering the distribution of LAINs, we reformulate the routing problem into a decentralized partially observable Markov decision process. With the proposed soft hierarchical experience replay buffer, the multi-agent double deep Q-network based adaptive routing algorithm is designed. Finally, simulations are conducted and numerical results show that the total E2E delay of the proposed mechanism decreases by 22.38\% than the benchmark on average.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22745v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sijie He, Ziye Jia, Qiuming Zhu, Fuhui Zhou, Qihui Wu</dc:creator>
    </item>
    <item>
      <title>Offline Reinforcement Learning for Mobility Robustness Optimization</title>
      <link>https://arxiv.org/abs/2506.22793</link>
      <description>arXiv:2506.22793v1 Announce Type: new 
Abstract: In this work we revisit the Mobility Robustness Optimisation (MRO) algorithm and study the possibility of learning the optimal Cell Individual Offset tuning using offline Reinforcement Learning. Such methods make use of collected offline datasets to learn the optimal policy, without further exploration. We adapt and apply a sequence-based method called Decision Transformers as well as a value-based method called Conservative Q-Learning to learn the optimal policy for the same target reward as the vanilla rule-based MRO. The same input features related to failures, ping-pongs, and other handover issues are used. Evaluation for realistic New Radio networks with 3500 MHz carrier frequency on a traffic mix including diverse user service types and a specific tunable cell-pair shows that offline-RL methods outperform rule-based MRO, offering up to 7% improvement. Furthermore, offline-RL can be trained for diverse objective functions using the same available dataset, thus offering operational flexibility compared to rule-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22793v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pegah Alizadeh, Anastasios Giovanidis, Pradeepa Ramachandra, Vasileios Koutsoukis, Osama Arouk</dc:creator>
    </item>
    <item>
      <title>Reliable Image Transmission in CPS-based Pub/Sub</title>
      <link>https://arxiv.org/abs/2506.22875</link>
      <description>arXiv:2506.22875v1 Announce Type: new 
Abstract: Developments in communication and automation have driven the expansion of distributed networks, essential for IoT and CPS development in industrial applications requiring reliable image processing and real-time adaptability. Although broadly adopted, there is a literature gap regarding the performance of MQTT protocol for image sharing and transmission under high-traffic scenarios with intermittent connectivity, restricting its use in critical IoT and CPS applications. In this context, the present work examines the reliability of real-time image transmission in IoT and CPS industrial systems that utilize the MQTT-based publish/subscribe communication model. It focuses on scenarios with network interruptions and high data traffic, evaluating the performance of a distributed system through a series of controlled testbed validation experiments. Experimental validation demonstrated that while the MQTT-based system sustains reliable transmission under normal conditions, its recovery capability depends on the failure point, with complete restoration occurring when disruptions affect the Orchestrator Node and partial recovery when the Producer Node or Broker are affected. The study also confirmed that the system prevents duplicate errors and adapts well to increasing network demands, reinforcing its suitability for industrial applications that require efficient and resilient data handling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22875v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Everson Flores, Bruna Guterres, Thomaz Pereira Junior, Paula Barros, Alberto Cabral, Cristiana Lima Dora, Marcelo Malheiros, Marcelo Pias</dc:creator>
    </item>
    <item>
      <title>Resilient-Native and Intelligent Next-Generation Wireless Systems: Key Enablers, Foundations, and Applications</title>
      <link>https://arxiv.org/abs/2506.22991</link>
      <description>arXiv:2506.22991v1 Announce Type: new 
Abstract: Just like power, water, and transportation systems, wireless networks are a crucial societal infrastructure. As natural and human-induced disruptions continue to grow, wireless networks must be resilient. This requires them to withstand and recover from unexpected adverse conditions, shocks, unmodeled disturbances and cascading failures. Unlike robustness and reliability, resilience is based on the understanding that disruptions will inevitably happen. Resilience, as elasticity, focuses on the ability to bounce back to favorable states, while resilience as plasticity involves agents and networks that can flexibly expand their states and hypotheses through real-time adaptation and reconfiguration. This situational awareness and active preparedness, adapting world models and counterfactually reasoning about potential system failures and the best responses, is a core aspect of resilience. This article will first disambiguate resilience from reliability and robustness, before delving into key mathematical foundations of resilience grounded in abstraction, compositionality and emergence. Subsequently, we focus our attention on a plethora of techniques and methodologies pertaining to the unique characteristics of resilience, as well as their applications through a comprehensive set of use cases. Ultimately, the goal of this paper is to establish a unified foundation for understanding, modeling, and engineering resilience in wireless communication systems, while laying a roadmap for the next-generation of resilient-native and intelligent wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22991v1</guid>
      <category>cs.NI</category>
      <category>cs.LO</category>
      <category>cs.MA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mehdi Bennis, Sumudu Samarakoon, Tamara Alshammari, Chathuranga Weeraddana, Zhoujun Tian, Chaouki Ben Issaid</dc:creator>
    </item>
    <item>
      <title>Model-Based Diagnosis: Automating End-to-End Diagnosis of Network Failures</title>
      <link>https://arxiv.org/abs/2506.23083</link>
      <description>arXiv:2506.23083v1 Announce Type: new 
Abstract: Fast diagnosis and repair of enterprise network failures is critically important since disruptions cause major business impacts. Prior works focused on diagnosis primitives or procedures limited to a subset of the problem, such as only data plane or only control plane faults. This paper proposes a new paradigm, model-based network diagnosis, that provides a systematic way to derive automated procedures for identifying the root cause of network failures, based on reports of end-to-end user-level symptoms. The diagnosis procedures are systematically derived from a model of packet forwarding and routing, covering hardware, firmware, and software faults in both the data plane and distributed control plane. These automated procedures replace and dramatically accelerate diagnosis by an experienced human operator. Model-based diagnosis is inspired by, leverages, and is complementary to recent work on network verification. We have built NetDx, a proof-of-concept implementation of model-based network diagnosis. We deployed NetDx on a new emulator of networks consisting of P4 switches with distributed routing software. We validated the robustness and coverage of NetDx with an automated fault injection campaign, in which 100% of faults were diagnosed correctly. Furthermore, on a data set of 33 faults from a large cloud provider that are within the domain targeted by NetDx, 30 are efficiently diagnosed in seconds instead of hours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23083v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Changrong Wu, Yiyao Yu, Myungjin Lee, Jayanth Srinivasa, Ennan Zhai, George Varghese, Yuval Tamir</dc:creator>
    </item>
    <item>
      <title>Autonomous Vision-Aided UAV Positioning for Obstacle-Aware Wireless Connectivity</title>
      <link>https://arxiv.org/abs/2506.23190</link>
      <description>arXiv:2506.23190v1 Announce Type: new 
Abstract: Unmanned Aerial Vehicles (UAVs) offer a promising solution for enhancing wireless connectivity and Quality of Service (QoS) in urban environments, acting as aerial Wi-Fi access points or cellular base stations. Their flexibility and rapid deployment capabilities make them suitable for addressing infrastructure gaps and traffic surges. However, optimizing UAV positions to maintain Line of Sight (LoS) links with ground User Equipment (UEs) remains challenging in obstacle-dense urban scenarios. This paper proposes VTOPA, a Vision-Aided Traffic- and Obstacle-Aware Positioning Algorithm that autonomously extracts environmental information -- such as obstacles and UE locations -- via computer vision and optimizes UAV positioning accordingly. The algorithm prioritizes LoS connectivity and dynamically adapts to user traffic demands in real time. Evaluated through simulations in ns-3, VTOPA achieves up to a 50% increase in aggregate throughput and a 50% reduction in delay, without compromising fairness, outperforming benchmark approaches in obstacle-rich environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23190v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamran Shafafi, Manuel Ricardo, Rui Campos</dc:creator>
    </item>
    <item>
      <title>On the Resilience of Underwater Semantic Wireless Communications</title>
      <link>https://arxiv.org/abs/2506.23350</link>
      <description>arXiv:2506.23350v1 Announce Type: new 
Abstract: Underwater wireless communications face significant challenges due to propagation constraints, limiting the effectiveness of traditional radio and optical technologies. Long-range acoustic communications support distances up to a few kilometers, but suffer from low bandwidth, high error ratios, and multipath interference. Semantic communications, which focus on transmitting extracted semantic features rather than raw data, present a promising solution by significantly reducing the volume of data transmitted over the wireless link.
  This paper evaluates the resilience of SAGE, a semantic-oriented communications framework that combines semantic processing with Generative Artificial Intelligence (GenAI) to compress and transmit image data as textual descriptions over acoustic links. To assess robustness, we use a custom-tailored simulator that introduces character errors observed in underwater acoustic channels. Evaluation results show that SAGE can successfully reconstruct meaningful image content even under varying error conditions, highlighting its potential for robust and efficient underwater wireless communication in harsh environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23350v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jo\~ao Pedro Loureiro, Patr\'icia Delgado, Tom\'as Feliciano Ribeiro, Filipe B. Teixeira, Rui Campos</dc:creator>
    </item>
    <item>
      <title>Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent Metasurfaces</title>
      <link>https://arxiv.org/abs/2506.23488</link>
      <description>arXiv:2506.23488v1 Announce Type: new 
Abstract: Wireless communication systems face significant challenges in meeting the increasing demands for higher data rates and more reliable connectivity in complex environments. Stacked intelligent metasurfaces (SIMs) have emerged as a promising technology for realizing wave-domain signal processing, with mobile SIMs offering superior communication performance compared to their fixed counterparts. In this paper, we investigate a novel unmanned aerial vehicle (UAV)-mounted SIMs (UAV-SIMs) assisted communication system within the low-altitude economy (LAE) networks paradigm, where UAVs function as both base stations that cache SIM-processed data and mobile platforms that flexibly deploy SIMs to enhance uplink communications from ground users. To maximize network capacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that comprehensively addresses three critical aspects: the association between UAV-SIMs and users, the three-dimensional positioning of UAV-SIMs, and the phase shifts across multiple SIM layers. Due to the inherent non-convexity and NP-hardness of USBJOP, we decompose it into three sub-optimization problems, \textit{i.e.}, association between UAV-SIMs and users optimization problem (AUUOP), UAV location optimization problem (ULOP), and UAV-SIM phase shifts optimization problem (USPSOP), and solve them using an alternating optimization strategy. Specifically, we transform AUUOP and ULOP into convex forms solvable by the CVX tool, while addressing USPSOP through a generative artificial intelligence (GAI)-based hybrid optimization algorithm. Simulations demonstrate that our proposed approach significantly outperforms benchmark schemes, achieving approximately 1.5 times higher network capacity compared to suboptimal alternatives. Additionally, our proposed GAI method reduces the algorithm runtime by 10\% while maintaining solution quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23488v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geng Sun, Mingzhe Fan, Lei Zhang, Hongyang Pan, Jiahui Li, Chuang Zhang, Linyao Li, Changyuan Zhao, Chau Yuen</dc:creator>
    </item>
    <item>
      <title>Securing the Sky: Integrated Satellite-UAV Physical Layer Security for Low-Altitude Wireless Networks</title>
      <link>https://arxiv.org/abs/2506.23493</link>
      <description>arXiv:2506.23493v1 Announce Type: new 
Abstract: Low-altitude wireless networks (LAWNs) have garnered significant attention in the forthcoming 6G networks. In LAWNs, satellites with wide coverage and unmanned aerial vehicles (UAVs) with flexible mobility can complement each other to form integrated satellite-UAV networks, providing ubiquitous and high-speed connectivity for low-altitude operations. However, the higher line-of-sight probability in low-altitude airspace increases transmission security concerns. In this work, we present a collaborative beamforming-based physical layer security scheme for LAWNs. We introduce the fundamental aspects of integrated satellite-UAV networks, physical layer security, UAV swarms, and collaborative beamforming for LAWN applications. Following this, we highlight several opportunities for collaborative UAV swarm secure applications enabled by satellite networks, including achieving physical layer security in scenarios involving data dissemination, data relay, eavesdropper collusion, and imperfect eavesdropper information. Next, we detail two case studies: a secure relay system and a two-way aerial secure communication framework specifically designed for LAWN environments. Simulation results demonstrate that these physical layer security schemes are effective and beneficial for secure low-altitude wireless communications. A short practicality analysis shows that the proposed method is applicable to LAWN scenarios. Finally, we discuss current challenges and future research directions for enhancing security in LAWNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23493v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahui Li, Geng Sun, Xiaoyu Sun, Fang Mei, Jingjing Wang, Xiangwang Hou, Daxin Tian, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>The Kubernetes Network Driver Model: A Composable Architecture for High-Performance Networking</title>
      <link>https://arxiv.org/abs/2506.23628</link>
      <description>arXiv:2506.23628v1 Announce Type: new 
Abstract: Traditional Kubernetes networking struggles to meet the escalating demands of AI/ML and evolving Telco infrastructure. This paper introduces Kubernetes Network Drivers (KNDs), a transformative, modular, and declarative architecture designed to overcome current imperative provisioning and API limitations. KNDs integrate network resource management into Kubernetes' core by utilizing Dynamic Resource Allocation (DRA), Node Resource Interface (NRI) improvements, and upcoming OCI Runtime Specification changes. Our DraNet implementation demonstrates declarative attachment of network interfaces, including Remote Direct Memory Access (RDMA) devices, significantly boosting high-performance AI/ML workloads. This capability enables sophisticated cloud-native applications and lays crucial groundwork for future Telco solutions, fostering a "galaxy" of specialized KNDs for enhanced application delivery and reduced operational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23628v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Ojea</dc:creator>
    </item>
    <item>
      <title>Geminet: Learning the Duality-based Iterative Process for Lightweight Traffic Engineering in Changing Topologies</title>
      <link>https://arxiv.org/abs/2506.23640</link>
      <description>arXiv:2506.23640v1 Announce Type: new 
Abstract: Recently, researchers have explored ML-based Traffic Engineering (TE), leveraging neural networks to solve TE problems traditionally addressed by optimization. However, existing ML-based TE schemes remain impractical: they either fail to handle topology changes or suffer from poor scalability due to excessive computational and memory overhead. To overcome these limitations, we propose Geminet, a lightweight and scalable ML-based TE framework that can handle changing topologies. Geminet is built upon two key insights: (i) a methodology that decouples neural networks from topology by learning an iterative gradient-descent-based adjustment process, as the update rule of gradient descent is topology-agnostic, relying only on a few gradient-related quantities; (ii) shifting optimization from path-level routing weights to edge-level dual variables, reducing memory consumption by leveraging the fact that edges are far fewer than paths. Evaluations on WAN and data center datasets show that Geminet significantly improves scalability. Its neural network size is only 0.04% to 7% of existing schemes, while handling topology variations as effectively as HARP, a state-of-the-art ML-based TE approach, without performance degradation. When trained on large-scale topologies, Geminet consumes under 10 GiB of memory, more than eight times less than the 80-plus GiB required by HARP, while achieving 5.45 times faster convergence speed, demonstrating its potential for large-scale deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23640v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ximeng Liu, Shizhen Zhao, Xinbing Wang</dc:creator>
    </item>
    <item>
      <title>Campus5G: A Campus Scale Private 5G Open RAN Testbed</title>
      <link>https://arxiv.org/abs/2506.23740</link>
      <description>arXiv:2506.23740v1 Announce Type: new 
Abstract: Mobile networks are embracing disaggregation, reflected by the industry trend towards Open RAN. Private 5G networks are viewed as particularly suitable contenders as early adopters of Open RAN, owing to their setting, high degree of control, and opportunity for innovation they present. Motivated by this, we have recently deployed Campus5G, the first of its kind campus-wide, O-RAN-compliant private 5G testbed across the central campus of the University of Edinburgh. We present in detail our process developing the testbed, from planning, to architecting, to deployment, and measuring the testbed performance. We then discuss the lessons learned from building the testbed, and highlight some research opportunities that emerged from our deployment experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23740v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrew E. Ferguson, Ujjwal Pawar, Tianxin Wang, Mahesh K. Marina</dc:creator>
    </item>
    <item>
      <title>How Long Can I Transmit? A Mobility Aware mmWave-based UAV Communication Framework</title>
      <link>https://arxiv.org/abs/2506.23755</link>
      <description>arXiv:2506.23755v1 Announce Type: new 
Abstract: One primary focus of next generation wireless communication networks is the millimeterwave (mmWave) spectrum, typically considered in the 30 GHz to 300 GHz frequency range. Despite their promise of high data rates, mmWaves suffer from severe attenuation while passing through obstacles. Unmanned aerial vehicles (UAVs) have been proposed to offset this limitation on account of their additional degrees of freedom, which can be leveraged to provide line of sight (LoS) transmission paths. While some prior works have proposed analytical frameworks to compute the LoS probability for static ground users and a UAV, the same is lacking for mobile users on the ground. In this paper, we consider the popular Manhattan point line process (MPLP) to model an urban environment, within which a ground user moves with a known velocity for a small time interval along the roads. We derive an expression for the expected duration of LoS between a static UAV in the air and a mobile ground user, and validate the same through simulations. To demonstrate the efficacy of the proposed analysis, we propose a simple user association algorithm that greedily assigns the UAVs to users with the highest expected LoS time, and show that it outperforms the existing benchmark schemes that assign the users to the nearest UAVs with LoS without considering the user mobility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23755v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shawon Mitra, Subhojit Sarkar, Sasthi C. Ghosh</dc:creator>
    </item>
    <item>
      <title>Learning Constraints Directly from Network Data</title>
      <link>https://arxiv.org/abs/2506.23964</link>
      <description>arXiv:2506.23964v1 Announce Type: new 
Abstract: Network data conforms to a wide range of rules that arise from protocols, design principles, and deployment decisions (e.g., a packet's queuing delay must be less than its end-to-end delay). Formalizing such rules as logic constraints can (i) improve the quality of synthetic data, (ii) reduce the brittleness of machine learning (ML) models, and (iii) improve semantic understanding of network measurements. However, these benefits remain out of reach if rule extraction is manual or solely reliant on ML, as both approaches yield incomplete, unreliable, and/or inaccurate rules.
  This paper formulates rule extraction as a constraint modeling problem and introduces NetNomos that learns propositional logic constraints directly from raw network measurements. Constraint modeling in this domain is uniquely challenging due to the scale of the data, the inherent learning complexity and passive environment, and the lack of ground truth supervision. NetNomos addresses these challenges via a lattice-based search structured by constraint specificity and succinctness. Our approach reduces learning complexity from superquadratic to logarithmic and enables efficient traversal in combinatorial search space.
  Our evaluations on diverse network datasets show that NetNomos learns all benchmark rules, including those associated with as little as 0.01% of data points, in under three hours. In contrast, baseline methods discover less than 25% of the rules and require several days to run. Through three case studies, we show that: NetNomos (i) finds rule violations in the outputs of all seven synthetic traffic generators, hence can be used to assess and guide their generation process; (ii) detects semantic differences in traffic, hence can be used for anomaly detection; and (iii) automatically finds rules used for telemetry imputation, hence can support monitoring through inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23964v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyu H\`e, Minhao Jin, Maria Apostolaki</dc:creator>
    </item>
    <item>
      <title>Continual Learning for Wireless Channel Prediction</title>
      <link>https://arxiv.org/abs/2506.22471</link>
      <description>arXiv:2506.22471v1 Announce Type: cross 
Abstract: Modern 5G/6G deployments routinely face cross-configuration handovers--users traversing cells with different antenna layouts, carrier frequencies, and scattering statistics--which inflate channel-prediction NMSE by $37.5\%$ on average when models are naively fine-tuned. The proposed improvement frames this mismatch as a continual-learning problem and benchmarks three adaptation families: replay with loss-aware reservoirs, synaptic-importance regularization, and memory-free learning-without-forgetting. Across three representative 3GPP urban micro scenarios, the best replay and regularization schemes cut the high-SNR error floor by up to 2~dB ($\approx 35\%$), while even the lightweight distillation recovers up to $30\%$ improvement over baseline handover prediction schemes. These results show that targeted rehearsal and parameter anchoring are essential for handover-robust CSI prediction and suggest a clear migration path for embedding continual-learning hooks into current channel prediction efforts in 3GPP--NR and O-RAN. The full codebase can be found at https://github.com/ahmd-mohsin/continual-learning-channel-prediction.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22471v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Ahmed Mohsin, Muhammad Umer, Ahsan Bilal, Muhammad Ali Jamshed, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Coexistence analysis of Wi-Fi 6E and 5G NR-U in the 6 GHz band</title>
      <link>https://arxiv.org/abs/2506.22844</link>
      <description>arXiv:2506.22844v1 Announce Type: cross 
Abstract: The ever-increasing demand for broadband and IoT wireless connectivity has recently urged the regulators around the world to start opening the 6 GHz spectrum for unlicensed use. These bands will, for example, permit the use of additional 1.2 GHz in the US and 500 MHz in Europe for unlicensed radio access technologies (RATs) such as Wi-Fi and 5G New Radio Unlicensed (5G NR-U). To support QoS-sensitive applications with both technologies, fair and efficient coexistence approaches between the two RATs, as well as with incumbents already operating in the 6 GHz band, are crucial. In this paper, we study through extensive simulations the achievable mean downlink throughput of both Wi-Fi 6E APs and 5G NR-U gNBs when they are co-deployed in a dense residential scenario under high-interference conditions. We also explore how different parameter settings e.g., MAC frame aggregation, energy detection threshold and maximum channel occupancy time (MCOT) affect the coexistence. Our findings give important insights into how to tune the key parameters to design fair coexistence policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22844v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Navid Keshtiarast, Marina Petrova</dc:creator>
    </item>
    <item>
      <title>Performance Measurements in the AI-Centric Computing Continuum Systems</title>
      <link>https://arxiv.org/abs/2506.22884</link>
      <description>arXiv:2506.22884v1 Announce Type: cross 
Abstract: Over the Eight decades, computing paradigms have shifted from large, centralized systems to compact, distributed architectures, leading to the rise of the Distributed Computing Continuum (DCC). In this model, multiple layers such as cloud, edge, Internet of Things (IoT), and mobile platforms work together to support a wide range of applications. Recently, the emergence of Generative AI and large language models has further intensified the demand for computational resources across this continuum. Although traditional performance metrics have provided a solid foundation, they need to be revisited and expanded to keep pace with changing computational demands and application requirements. Accurate performance measurements benefit both system designers and users by supporting improvements in efficiency and promoting alignment with system goals. In this context, we review commonly used metrics in DCC and IoT environments. We also discuss emerging performance dimensions that address evolving computing needs, such as sustainability, energy efficiency, and system observability. We also outline criteria and considerations for selecting appropriate metrics, aiming to inspire future research and development in this critical area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22884v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Praveen Kumar Donta, Qiyang Zhang, Schahram Dustdar</dc:creator>
    </item>
    <item>
      <title>All Proof of Work But No Proof of Play</title>
      <link>https://arxiv.org/abs/2506.23435</link>
      <description>arXiv:2506.23435v1 Announce Type: cross 
Abstract: Speedrunning is a competition that emerged from communities of early video games such as Doom (1993). Speedrunners try to finish a game in minimal time. Provably verifying the authenticity of submitted speedruns is an open problem. Traditionally, best-effort speedrun verification is conducted by on-site human observers, forensic audio analysis, or a rigorous mathematical analysis of the game mechanics. Such methods are tedious, fallible, and, perhaps worst of all, not cryptographic. Motivated by naivety and the Dunning-Kruger effect, we attempt to build a system that cryptographically proves the authenticity of speedruns. This paper describes our attempted solutions and ways to circumvent them. Through a narration of our failures, we attempt to demonstrate the difficulty of authenticating live and interactive human input in untrusted environments, as well as the limits of signature schemes, game integrity, and provable play.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23435v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>CFAIL 2025</arxiv:journal_reference>
      <dc:creator>Hayder Tirmazi</dc:creator>
    </item>
    <item>
      <title>E-WAN: Efficient Communication in Energy Harvesting Low-Power Networks</title>
      <link>https://arxiv.org/abs/2506.23788</link>
      <description>arXiv:2506.23788v1 Announce Type: cross 
Abstract: The ever-increasing number of distributed embedded systems in the context of the Internet of Things (IoT), Wireless Sensor Networks (WSN), and Cyber-Physical Systems (CPS) rely on wireless communication to collect and exchange data. Nodes can employ single-hop communication which, despite its ease, may necessitate energy-intensive long-range communication to cover long distances. Conversely, multi-hop communication allows for more energy-efficient short-range communication since nodes can rely on other nodes to forward their data. Yet, this approach requires relay nodes to be available and continuous maintenance of a dynamically changing distributed state. At the same time, energy harvesting has the potential to outperform traditional battery-based systems by improving their lifetime, scalability with lower maintenance costs, and environmental impact. However, the limited and temporally and spatially variable harvested energy poses significant challenges for networking in energy harvesting networks, particularly considering the energy demands and characteristics of both multi-hop and single-hop communication. We propose E-WAN, a protocol for energy harvesting wide-area low-power networks that builds on the concept of \emph{virtual sub-networks} to enable resource-efficient multi-hop communication when possible and reliable however energy-intensive point-to-point communication otherwise. Nodes autonomously and dynamically move between the two and adjust to changing network states and resources based only on easily obtainable network state information. We illustrate E-WAN's advantages both in terms of efficiency and adaptability in various communication and harvesting scenarios. Furthermore, we demonstrate E-WAN operating in a realistic setting by deploying an energy harvesting network in a real-world indoor environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23788v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Naomi Stricker, David Blaser, Andres Gomez, Lothar Thiele</dc:creator>
    </item>
    <item>
      <title>Retrieval Augmented Generation Based LLM Evaluation For Protocol State Machine Inference With Chain-of-Thought Reasoning</title>
      <link>https://arxiv.org/abs/2502.15727</link>
      <description>arXiv:2502.15727v2 Announce Type: replace 
Abstract: This paper presents a novel approach to evaluate the efficiency of a RAG-based agentic Large Language Model (LLM) architecture for network packet seed generation and enrichment. Enhanced by chain-of-thought (COT) prompting techniques, the proposed approach focuses on the improvement of the seeds' structural quality in order to guide protocol fuzzing frameworks through a wide exploration of the protocol state space. Our method leverages RAG and text embeddings to dynamically reference to the Request For Comments (RFC) documents knowledge base for answering queries regarding the protocol's Finite State Machine (FSM), then iteratively reasons through the retrieved knowledge, for output refinement and proper seed placement. We then evaluate the response structure quality of the agent's output, based on metrics as BLEU, ROUGE, and Word Error Rate (WER) by comparing the generated packets against the ground-truth packets. Our experiments demonstrate significant improvements of up to 18.19%, 14.81%, and 23.45% in BLEU, ROUGE, and WER, respectively, over baseline models. These results confirm the potential of such approach, improving LLM-based protocol fuzzing frameworks for the identification of hidden vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15727v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Youssef Maklad, Fares Wael, Wael Elsersy, Ali Hamdi</dc:creator>
    </item>
    <item>
      <title>Wi-Fi 6 Cross-Technology Interference Detection and Mitigation by OFDMA: an Experimental Study</title>
      <link>https://arxiv.org/abs/2503.05429</link>
      <description>arXiv:2503.05429v2 Announce Type: replace 
Abstract: Cross-Technology Interference (CTI) poses challenges for the performance and robustness of wireless networks. There are opportunities for better cooperation if the spectral occupation and technology of the interference can be detected. Namely, this information can help the Orthogonal Frequency Division Multiple Access (OFDMA) scheduler in IEEE 802.11ax (Wi-Fi 6) to efficiently allocate resources to multiple users inthe frequency domain. This work shows that a single Channel State Information (CSI) snapshot, which is used for packet demodulation in the receiver, is enough to detect and classify the type of CTI on low-cost Wi-Fi 6 hardware. We show the classification accuracy of a small Convolutional Neural Network (CNN) for different Signal-to-Noise Ratio (SNR) and Signal-to-Interference Ratio (SIR) with simulated data, as well as using a wired and over-the-air test with a professional wireless connectivity tester, while running the inference on the low-cost device. Furthermore, we use openwifi, a full-stack Wi-Fi transceiver running on software-defined radio (SDR) available in the w-iLab.t testbed, as Access Point (AP) to implement a CTI-aware multi-user OFDMA scheduler when the clients send CTI detection feedback to the AP. We show experimentally that it can fully mitigate the 35% throughput loss caused by CTI when the AP applies the appropriate scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05429v2</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/EuCNC/6GSummit63408.2025.11037177</arxiv:DOI>
      <dc:creator>Thijs Havinga, Xianjun Jiao, Wei Liu, Baiheng Chen, Adnan Shahid, Ingrid Moerman</dc:creator>
    </item>
    <item>
      <title>Bridging Subjective and Objective QoE: Operator-Level Aggregation Using LLM-Based Comment Analysis and Network MOS Comparison</title>
      <link>https://arxiv.org/abs/2506.00924</link>
      <description>arXiv:2506.00924v2 Announce Type: replace 
Abstract: This paper introduces a dual-layer framework for network operator-side quality of experience (QoE) assessment that integrates both objective network modeling and subjective user perception extracted from live-streaming platforms. On the objective side, we develop a machine learning model trained on mean opinion scores (MOS) computed via the ITU-T P.1203 reference implementation, allowing accurate prediction of user-perceived video quality using only network parameters such as packet loss, delay, jitter, and throughput without reliance on video content or client-side instrumentation. On the subjective side, we present a semantic filtering and scoring pipeline that processes user comments from live streams to extract performance-related feedback. A large language model is used to assign scalar MOS scores to filtered comments in a deterministic and reproducible manner. To support scalable and interpretable analysis, we construct a labeled dataset of 47,894 live-stream comments, of which about 34,000 are identified as QoE-relevant through multi-layer semantic filtering. Each comment is enriched with simulated Internet Service Provider attribution and temporally aligned using synthetic timestamps in 5-min intervals. The resulting dataset enables operator-level aggregation and time-series analysis of user-perceived quality. A delta MOS metric is proposed to measure each Internet service provider's deviation from platform-wide sentiment, allowing detection of localized degradations even in the absence of direct network telemetry. A controlled outage simulation confirms the framework's effectiveness in identifying service disruptions through comment-based trends alone. The system provides each operator with its own subjective MOS and the global platform average per interval, enabling real-time interpretation of performance deviations and comparison with objective network-based QoE estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00924v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parsa Hassani Shariat Panahi, Amir Hossein Jalilvand, M. Hassan Najafi</dc:creator>
    </item>
    <item>
      <title>Adaptive Rank Allocation for Federated Parameter-Efficient Fine-Tuning of Language Models</title>
      <link>https://arxiv.org/abs/2501.14406</link>
      <description>arXiv:2501.14406v3 Announce Type: replace-cross 
Abstract: Pre-trained Language Models (PLMs) have demonstrated their superiority and versatility in modern Natural Language Processing (NLP), effectively adapting to various downstream tasks through further fine-tuning. Federated Parameter-Efficient Fine-Tuning (FedPEFT) has emerged as a promising solution to address privacy and efficiency challenges in distributed training for PLMs on resource-constrained local devices. However, our measurements reveal two key limitations of FedPEFT: heterogeneous data across devices exacerbates performance degradation of low-rank adaptation, and a fixed parameter configuration results in communication inefficiency. To overcome these limitations, we propose FedARA, a novel Adaptive Rank Allocation framework for federated parameter-efficient fine-tuning of language models. Specifically, FedARA employs truncated Singular Value Decomposition (SVD) adaptation to enhance similar feature representation across clients, significantly mitigating the adverse effects of data heterogeneity. Subsequently, it utilizes dynamic rank allocation to progressively identify critical ranks, effectively improving communication efficiency. Lastly, it leverages rank-based module pruning to automatically remove inactive modules, steadily reducing local computational cost and memory usage in each federated learning round. Extensive experiments show that FedARA consistently outperforms baselines by an average of 6.95% to 8.49% across various datasets and models under heterogeneous data while significantly improving communication efficiency by 2.40$ \times$. Moreover, experiments on various edge devices demonstrate substantial decreases in total training time and energy consumption by up to 48.90% and 46.95%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14406v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fei Wu, Jia Hu, Geyong Min, Shiqiang Wang</dc:creator>
    </item>
    <item>
      <title>SLED: A Speculative LLM Decoding Framework for Efficient Edge Serving</title>
      <link>https://arxiv.org/abs/2506.09397</link>
      <description>arXiv:2506.09397v3 Announce Type: replace-cross 
Abstract: The growing gap between the increasing complexity of large language models (LLMs) and the limited computational budgets of edge devices poses a key challenge for efficient on-device inference, despite gradual improvements in hardware capabilities. Existing strategies, such as aggressive quantization, pruning, or remote inference, trade accuracy for efficiency or lead to substantial cost burdens. This position paper introduces a new framework that leverages speculative decoding, previously viewed primarily as a decoding acceleration technique for autoregressive generation of LLMs, as a promising approach specifically adapted for edge computing by orchestrating computation across heterogeneous devices. We propose \acronym, a framework that allows lightweight edge devices to draft multiple candidate tokens locally using diverse draft models, while a single, shared edge server verifies the tokens utilizing a more precise target model. To further increase the efficiency of verification, the edge server batch the diverse verification requests from devices. This approach supports device heterogeneity and reduces server-side memory footprint by sharing the same upstream target model across multiple devices. Our initial experiments with Jetson Orin Nano, Raspberry Pi 4B/5, and an edge server equipped with 4 Nvidia A100 GPUs indicate substantial benefits: 2.2 more system throughput, 2.8 more system capacity, and better cost efficiency, all without sacrificing model accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09397v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiangchen Li, Dimitrios Spatharakis, Saeid Ghafouri, Jiakun Fan, Hans Vandierendonck, Deepu John, Bo Ji, Dimitrios Nikolopoulos</dc:creator>
    </item>
  </channel>
</rss>
