<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:00:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Scalable Bayesian Inference for Bradley--Terry Models with Ties: An Application to Honour Based Abuse</title>
      <link>https://arxiv.org/abs/2405.13399</link>
      <description>arXiv:2405.13399v1 Announce Type: new 
Abstract: Honour based abuse covers a wide range of family abuse including female genital mutilation and forced marriage. Safeguarding professionals need to identify where abuses are happening in their local community to best support those at risk of these crimes and take preventative action. However, there is little local data about these kinds of crime. To tackle this problem, we ran comparative judgement surveys to map abuses at local level. In previous comparative judgement studies, participants reported fatigue associated with comparisons between areas with similar levels of abuse. Allowing for ties reduces fatigue, but increase the computational complexity when fitting the model. We designed an efficient Markov Chain Monte Carlo algorithm to fit the model, allowing for a wide range of prior distributions on the model parameters. Working with South Yorkshire Police and Oxford Against Cutting, we mapped the risk of honour based abuse at community level in two counties in the UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13399v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rowland G Seymour, Fabian Hernandez</dc:creator>
    </item>
    <item>
      <title>Representative electricity price profiles for European day-ahead and intraday spot markets</title>
      <link>https://arxiv.org/abs/2405.14403</link>
      <description>arXiv:2405.14403v1 Announce Type: new 
Abstract: We propose a method to construct representative price profiles of the day-ahead (DA) and the intraday (ID) electricity spot markets and use this method to provide examples of ready-to-use price data sets. In contrast to common scenario generation approaches, the method is deterministic and relies on a small number of degrees of freedom, with the aim to be well defined and easy to use. We thereby target an enhanced comparability of future research studies on demand-side management and energy cost optimization. We construct the price profiles based on historical time series from the spot markets of interest, e.g., European Power Exchange (EPEX) spot. To this end, we extract key price components from the data while also accounting for known dominant mechanisms in the price variation. Further, the method is able to preserve key statistical features of the historical data (e.g., mean and standard deviation) when constructing the benchmark profile. Finally, our approach ensures comparability of ID and DA price profiles by design, as their cumulative (integral) price can be made identical if needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14403v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chrysanthi Papadimitriou, Jan C. Schulze, Alexander Mitsos</dc:creator>
    </item>
    <item>
      <title>A Bayesian Approach to Estimate Causal Peer Influence Accounting for Latent Network Homophily</title>
      <link>https://arxiv.org/abs/2405.14789</link>
      <description>arXiv:2405.14789v1 Announce Type: new 
Abstract: Researchers have focused on understanding how individual's behavior is influenced by the behaviors of their peers in observational studies of social networks. Identifying and estimating causal peer influence, however, is challenging due to confounding by homophily, where people tend to connect with those who share similar characteristics with them. Moreover, since all the attributes driving homophily are generally not always observed and act as unobserved confounders, identifying and estimating causal peer influence becomes infeasible using standard causal identification assumptions. In this paper, we address this challenge by leveraging latent locations inferred from the network itself to disentangle homophily from causal peer influence, and we extend this approach to multiple networks by adopting a Bayesian hierarchical modeling framework. To accommodate the nonlinear dependency of peer influence on individual behavior, we employ a Bayesian nonparametric method, specifically Bayesian Additive Regression Trees (BART), and we propose a Bayesian framework that accounts for the uncertainty in inferring latent locations. We assess the operating characteristics of the estimator via extensive simulation study. Finally, we apply our method to estimate causal peer influence in advice-seeking networks of teachers in secondary schools, in order to assess whether the teachers' belief about mathematics education is influenced by the beliefs of their peers from whom they receive advice. Our results suggest that, overlooking latent homophily can lead to either underestimation or overestimation of causal peer influence, accompanied by considerable estimation uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14789v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seungha Um, Tracy Sweet, Samrachana Adhikari</dc:creator>
    </item>
    <item>
      <title>Computing the Instantaneous Collision Probability between Satellites using Characteristic Function Inversion</title>
      <link>https://arxiv.org/abs/2405.12230</link>
      <description>arXiv:2405.12230v1 Announce Type: cross 
Abstract: The probability that two satellites overlap in space at a specified instant of time is called their instantaneous collision probability. Assuming Gaussian uncertainties and spherical satellites, this probability is the integral of a Gaussian distribution over a sphere. This paper shows how to compute the probability using an established numerical procedure called characteristic function inversion. The collision probability in the short-term encounter scenario is also evaluated with this approach, where the instant at which the probability is computed is the time of closest approach between the objects. Python and R code is provided to evaluate the probability in practice. Overall, the approach has been established for over fifty years, is implemented in existing software, does not rely on analytical approximations, and can be used to evaluate two and three dimensional collision probabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12230v1</guid>
      <category>astro-ph.IM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason Bernstein</dc:creator>
    </item>
    <item>
      <title>Data Assimilation with Machine Learning Surrogate Models: A Case Study with FourCastNet</title>
      <link>https://arxiv.org/abs/2405.13180</link>
      <description>arXiv:2405.13180v1 Announce Type: cross 
Abstract: Modern data-driven surrogate models for weather forecasting provide accurate short-term predictions but inaccurate and nonphysical long-term forecasts. This paper investigates online weather prediction using machine learning surrogates supplemented with partial and noisy observations. We empirically demonstrate and theoretically justify that, despite the long-time instability of the surrogates and the sparsity of the observations, filtering estimates can remain accurate in the long-time horizon. As a case study, we integrate FourCastNet, a state-of-the-art weather surrogate model, within a variational data assimilation framework using partial, noisy ERA5 data. Our results show that filtering estimates remain accurate over a year-long assimilation window and provide effective initial conditions for forecasting tasks, including extreme event prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13180v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melissa Adrian, Daniel Sanz-Alonso, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>Valores extremos de inflaci\'on en Costa Rica</title>
      <link>https://arxiv.org/abs/2405.13251</link>
      <description>arXiv:2405.13251v1 Announce Type: cross 
Abstract: Maintaining low, non-negative and stable inflation levels is a necessary condition for the stability of the economy as a whole, because the monetary authorities of most industrialized countries, including the Central Bank of Costa Rica since 2005, they have oriented their monetary policy precisely to that task. Still Thus, both in Costa Rica and internationally, most of the statistical modeling of inflation has been limited to modeling their expectancy conditional on different covariates using linear models. This implies a lack of knowledge of the dynamics of the extreme values of the inflation rate and how these are related with other macroeconomic variables. In Costa Rica this is of particular importance since in several periods Negative quarter-on-quarter inflation rates have recently been experienced, which can be problematic if this becomes a recurring phenomenon. Therefore, in this work we propose to answer what is the relationship between the gap of GDP, inflation expectations, imported inflation rate, and the extreme values of the inflation rate in Costa Rica. That is, the main objective is to determine the relationship between the extreme values of the the inflation rate, GDP gap, inflation expectations and imported inflation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13251v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Daniel Aguilar, Breyner Chac\'on</dc:creator>
    </item>
    <item>
      <title>Ensemble size dependence of the logarithmic score for forecasts issued as multivariate normal distributions</title>
      <link>https://arxiv.org/abs/2405.13400</link>
      <description>arXiv:2405.13400v1 Announce Type: cross 
Abstract: Multivariate probabilistic verification is concerned with the evaluation of joint probability distributions of vector quantities such as a weather variable at multiple locations or a wind vector for instance. The logarithmic score is a proper score that is useful in this context. In order to apply this score to ensemble forecasts, a choice for the density is required. Here, we are interested in the specific case when the density is multivariate normal with mean and covariance given by the ensemble mean and ensemble covariance, respectively. Under the assumptions of multivariate normality and exchangeability of the ensemble members, a relationship is derived which describes how the logarithmic score depends on ensemble size. It permits to estimate the score in the limit of infinite ensemble size from a small ensemble and thus produces a fair logarithmic score for multivariate ensemble forecasts under the assumption of normality. This generalises a study from 2018 which derived the ensemble size adjustment of the logarithmic score in the univariate case.
  An application to medium-range forecasts examines the usefulness of the ensemble size adjustments when multivariate normality is only an approximation. Predictions of vectors consisting of several different combinations of upper air variables are considered. Logarithmic scores are calculated for these vectors using ECMWF's daily extended-range forecasts which consist of a 100-member ensemble. The probabilistic forecasts of these vectors are verified against operational ECMWF analyses in the Northern mid-latitudes in autumn 2023. Scores are computed for ensemble sizes from 8 to 100. The fair logarithmic scores of ensembles with different cardinalities are very close, in contrast to the unadjusted scores which decrease considerably with ensemble size. This provides evidence for the practical usefulness of the derived relationships.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13400v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Leutbecher, S\'andor Baran</dc:creator>
    </item>
    <item>
      <title>Machine Learning for Exoplanet Detection in High-Contrast Spectroscopy: Revealing Exoplanets by Leveraging Hidden Molecular Signatures in Cross-Correlated Spectra with Convolutional Neural Networks</title>
      <link>https://arxiv.org/abs/2405.13469</link>
      <description>arXiv:2405.13469v1 Announce Type: cross 
Abstract: The new generation of observatories and instruments (VLT/ERIS, JWST, ELT) motivate the development of robust methods to detect and characterise faint and close-in exoplanets. Molecular mapping and cross-correlation for spectroscopy use molecular templates to isolate a planet's spectrum from its host star. However, reliance on signal-to-noise ratio (S/N) metrics can lead to missed discoveries, due to strong assumptions of Gaussian independent and identically distributed noise. We introduce machine learning for cross-correlation spectroscopy (MLCCS); the method aims to leverage weak assumptions on exoplanet characterisation, such as the presence of specific molecules in atmospheres, to improve detection sensitivity for exoplanets. MLCCS methods, including a perceptron and unidimensional convolutional neural networks, operate in the cross-correlated spectral dimension, in which patterns from molecules can be identified. We test on mock datasets of synthetic planets inserted into real noise from SINFONI at K-band. The results from MLCCS show outstanding improvements. The outcome on a grid of faint synthetic gas giants shows that for a false discovery rate up to 5%, a perceptron can detect about 26 times the amount of planets compared to an S/N metric. This factor increases up to 77 times with convolutional neural networks, with a statistical sensitivity shift from 0.7% to 55.5%. In addition, MLCCS methods show a drastic improvement in detection confidence and conspicuity on imaging spectroscopy. Once trained, MLCCS methods offer sensitive and rapid detection of exoplanets and their molecular species in the spectral dimension. They handle systematic noise and challenging seeing conditions, can adapt to many spectroscopic instruments and modes, and are versatile regarding atmospheric characteristics, which can enable identification of various planets in archival and future data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13469v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Emily O. Garvin, Markus J. Bonse, Jean Hayoz, Gabriele Cugno, Jonas Spiller, Polychronis A. Patapis, Dominique Petit Dit de la Roche, Rakesh Nath-Ranga, Olivier Absil, Nicolai F. Meinshausen, Sascha P. Quanz</dc:creator>
    </item>
    <item>
      <title>Sequential Bayesian inference for stochastic epidemic models of cumulative incidence</title>
      <link>https://arxiv.org/abs/2405.13537</link>
      <description>arXiv:2405.13537v1 Announce Type: cross 
Abstract: Epidemics are inherently stochastic, and stochastic models provide an appropriate way to describe and analyse such phenomena. Given temporal incidence data consisting of, for example, the number of new infections or removals in a given time window, a continuous-time discrete-valued Markov process provides a natural description of the dynamics of each model component, typically taken to be the number of susceptible, exposed, infected or removed individuals. Fitting the SEIR model to time-course data is a challenging problem due incomplete observations and, consequently, the intractability of the observed data likelihood. Whilst sampling based inference schemes such as Markov chain Monte Carlo are routinely applied, their computational cost typically restricts analysis to data sets of no more than a few thousand infective cases. Instead, we develop a sequential inference scheme that makes use of a computationally cheap approximation of the most natural Markov process model. Crucially, the resulting model allows a tractable conditional parameter posterior which can be summarised in terms of a set of low dimensional statistics. This is used to rejuvenate parameter samples in conjunction with a novel bridge construct for propagating state trajectories conditional on the next observation of cumulative incidence. The resulting inference framework also allows for stochastic infection and reporting rates. We illustrate our approach using synthetic and real data applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13537v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sam A. Whitaker, Andrew Golightly, Colin S. Gillespie, Theodore Kypraios</dc:creator>
    </item>
    <item>
      <title>Hidden semi-Markov models with inhomogeneous state dwell-time distributions</title>
      <link>https://arxiv.org/abs/2405.13553</link>
      <description>arXiv:2405.13553v1 Announce Type: cross 
Abstract: The well-established methodology for the estimation of hidden semi-Markov models (HSMMs) as hidden Markov models (HMMs) with extended state spaces is further developed to incorporate covariate influences across all aspects of the state process model, in particular, regarding the distributions governing the state dwell time. The special case of periodically varying covariate effects on the state dwell-time distributions - and possibly the conditional transition probabilities - is examined in detail to derive important properties of such models, namely the periodically varying unconditional state distribution as well as the overall state dwell-time distribution. Through simulation studies, we ascertain key properties of these models and develop recommendations for hyperparameter settings. Furthermore, we provide a case study involving an HSMM with periodically varying dwell-time distributions to analyse the movement trajectory of an arctic muskox, demonstrating the practical relevance of the developed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13553v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Ole Koslik</dc:creator>
    </item>
    <item>
      <title>A Deep Learning Approach to Multi-Fiber Parameter Estimation and Uncertainty Quantification in Diffusion MRI</title>
      <link>https://arxiv.org/abs/2405.13655</link>
      <description>arXiv:2405.13655v1 Announce Type: cross 
Abstract: Diffusion MRI (dMRI) is the primary imaging modality used to study brain microstructure in vivo. Reliable and computationally efficient parameter inference for common dMRI biophysical models is a challenging inverse problem, due to factors such as variable dimensionalities (reflecting the unknown number of distinct white matter fiber populations in a voxel), low signal-to-noise ratios, and non-linear forward models. These challenges have led many existing methods to use biologically implausible simplified models to stabilize estimation, for instance, assuming shared microstructure across all fiber populations within a voxel. In this work, we introduce a novel sequential method for multi-fiber parameter inference that decomposes the task into a series of manageable subproblems. These subproblems are solved using deep neural networks tailored to problem-specific structure and symmetry, and trained via simulation. The resulting inference procedure is largely amortized, enabling scalable parameter estimation and uncertainty quantification across all model parameters. Simulation studies and real imaging data analysis using the Human Connectome Project (HCP) demonstrate the advantages of our method over standard alternatives. In the case of the standard model of diffusion, our results show that under HCP-like acquisition schemes, estimates for extra-cellular parallel diffusivity are highly uncertain, while those for the intra-cellular volume fraction can be estimated with relatively high precision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13655v1</guid>
      <category>eess.IV</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Consagra, Lipeng Ning, Yogesh Rathi</dc:creator>
    </item>
    <item>
      <title>Normalizing Basis Functions: Approximate Stationary Models for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.13821</link>
      <description>arXiv:2405.13821v1 Announce Type: cross 
Abstract: In geostatistics, traditional spatial models often rely on the Gaussian Process (GP) to fit stationary covariances to data. It is well known that this approach becomes computationally infeasible when dealing with large data volumes, necessitating the use of approximate methods. A powerful class of methods approximate the GP as a sum of basis functions with random coefficients. Although this technique offers computational efficiency, it does not inherently guarantee a stationary covariance. To mitigate this issue, the basis functions can be "normalized" to maintain a constant marginal variance, avoiding unwanted artifacts and edge effects. This allows for the fitting of nearly stationary models to large, potentially non-stationary datasets, providing a rigorous base to extend to more complex problems. Unfortunately, the process of normalizing these basis functions is computationally demanding. To address this, we introduce two fast and accurate algorithms to the normalization step, allowing for efficient prediction on fine grids. The practical value of these algorithms is showcased in the context of a spatial analysis on a large dataset, where significant computational speedups are achieved. While implementation and testing are done specifically within the LatticeKrig framework, these algorithms can be adapted to other basis function methods operating on regular grids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13821v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antony Sikorski, Daniel McKenzie, Douglas Nychka</dc:creator>
    </item>
    <item>
      <title>Watermarking Generative Tabular Data</title>
      <link>https://arxiv.org/abs/2405.14018</link>
      <description>arXiv:2405.14018v1 Announce Type: cross 
Abstract: In this paper, we introduce a simple yet effective tabular data watermarking mechanism with statistical guarantees. We show theoretically that the proposed watermark can be effectively detected, while faithfully preserving the data fidelity, and also demonstrates appealing robustness against additive noise attack. The general idea is to achieve the watermarking through a strategic embedding based on simple data binning. Specifically, it divides the feature's value range into finely segmented intervals and embeds watermarks into selected ``green list" intervals. To detect the watermarks, we develop a principled statistical hypothesis-testing framework with minimal assumptions: it remains valid as long as the underlying data distribution has a continuous density function. The watermarking efficacy is demonstrated through rigorous theoretical analysis and empirical validation, highlighting its utility in enhancing the security of synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14018v1</guid>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hengzhi He, Peiyu Yu, Junpeng Ren, Ying Nian Wu, Guang Cheng</dc:creator>
    </item>
    <item>
      <title>A Direct Importance Sampling-based Framework for Rare Event Uncertainty Quantification in Non-Gaussian Spaces</title>
      <link>https://arxiv.org/abs/2405.14149</link>
      <description>arXiv:2405.14149v1 Announce Type: cross 
Abstract: This work introduces a novel framework for precisely and efficiently estimating rare event probabilities in complex, high-dimensional non-Gaussian spaces, building on our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) approach. An unnormalized sampling target is first constructed and sampled, relaxing the optimal importance sampling distribution and appropriately designed for non-Gaussian spaces. Post-sampling, its normalizing constant is estimated using a stable inverse importance sampling procedure, employing an importance sampling density based on the already available samples. The sought probability is then computed based on the estimates evaluated in these two stages. The proposed estimator is theoretically analyzed, proving its unbiasedness and deriving its analytical coefficient of variation. To sample the constructed target, we resort to our developed Quasi-Newton mass preconditioned Hamiltonian MCMC (QNp-HMCMC) and we prove that it converges to the correct stationary target distribution. To avoid the challenging task of tuning the trajectory length in complex spaces, QNp-HMCMC is effectively utilized in this work with a single-step integration. We thus show the equivalence of QNp-HMCMC with single-step implementation to a unique and efficient preconditioned Metropolis-adjusted Langevin algorithm (MALA). An optimization approach is also leveraged to initiate QNp-HMCMC effectively, and the implementation of the developed framework in bounded spaces is eventually discussed. A series of diverse problems involving high dimensionality (several hundred inputs), strong nonlinearity, and non-Gaussianity is presented, showcasing the capabilities and efficiency of the suggested framework and demonstrating its advantages compared to relevant state-of-the-art sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14149v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elsayed Eshra, Konstantinos G. Papakonstantinou, Hamed Nikbakht</dc:creator>
    </item>
    <item>
      <title>Multilevel functional data analysis modeling of human glucose response to meal intake</title>
      <link>https://arxiv.org/abs/2405.14690</link>
      <description>arXiv:2405.14690v1 Announce Type: cross 
Abstract: Glucose meal response information collected via Continuous Glucose Monitoring (CGM) is relevant to the assessment of individual metabolic status and the support of personalized diet prescriptions. However, the complexity of the data produced by CGM monitors pushes the limits of existing analytic methods. CGM data often exhibits substantial within-person variability and has a natural multilevel structure. This research is motivated by the analysis of CGM data from individuals without diabetes in the AEGIS study. The dataset includes detailed information on meal timing and nutrition for each individual over different days. The primary focus of this study is to examine CGM glucose responses following patients' meals and explore the time-dependent associations with dietary and patient characteristics. Motivated by this problem, we propose a new analytical framework based on multilevel functional models, including a new functional mixed R-square coefficient. The use of these models illustrates 3 key points: (i) The importance of analyzing glucose responses across the entire functional domain when making diet recommendations; (ii) The differential metabolic responses between normoglycemic and prediabetic patients, particularly with regards to lipid intake; (iii) The importance of including random, person-level effects when modelling this scientific problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14690v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Joe Sartini, Francisco Gude</dc:creator>
    </item>
    <item>
      <title>Zero-inflation in the Multivariate Poisson Lognormal Family</title>
      <link>https://arxiv.org/abs/2405.14711</link>
      <description>arXiv:2405.14711v1 Announce Type: cross 
Abstract: Analyzing high-dimensional count data is a challenge and statistical model-based approaches provide an adequate and efficient framework that preserves explainability. The (multivariate) Poisson-Log-Normal (PLN) model is one such model: it assumes count data are driven by an underlying structured latent Gaussian variable, so that the dependencies between counts solely stems from the latent dependencies. However PLN doesn't account for zero-inflation, a feature frequently observed in real-world datasets. Here we introduce the Zero-Inflated PLN (ZIPLN) model, adding a multivariate zero-inflated component to the model, as an additional Bernoulli latent variable. The Zero-Inflation can be fixed, site-specific, feature-specific or depends on covariates. We estimate model parameters using variational inference that scales up to datasets with a few thousands variables and compare two approximations: (i) independent Gaussian and Bernoulli variational distributions or (ii) Gaussian variational distribution conditioned on the Bernoulli one. The method is assessed on synthetic data and the efficiency of ZIPLN is established even when zero-inflation concerns up to $90\%$ of the observed counts. We then apply both ZIPLN and PLN to a cow microbiome dataset, containing $90.6\%$ of zeroes. Accounting for zero-inflation significantly increases log-likelihood and reduces dispersion in the latent space, thus leading to improved group discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14711v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bastien Batardi\`ere, Julien Chiquet, Fran\c{c}ois Gindraud, Mahendra Mariadassou</dc:creator>
    </item>
    <item>
      <title>An assessment of racial disparities in pretrial decision-making using misclassification models</title>
      <link>https://arxiv.org/abs/2309.08599</link>
      <description>arXiv:2309.08599v2 Announce Type: replace 
Abstract: Pretrial risk assessment tools are used in jurisdictions across the country to assess the likelihood of "pretrial failure," the event where defendants either fail to appear for court or reoffend. Judicial officers, in turn, use these assessments to determine whether to release or detain defendants during trial. While algorithmic risk assessment tools were designed to predict pretrial failure with greater accuracy relative to judges, there is still concern that both risk assessment recommendations and pretrial decisions are biased against minority groups. In this paper, we develop methods to investigate the association between risk factors and pretrial failure, while simultaneously estimating misclassification rates of pretrial risk assessments and of judicial decisions as a function of defendant race. This approach adds to a growing literature that makes use of outcome misclassification methods to answer questions about fairness in pretrial decision-making. We give a detailed simulation study for our proposed methodology and apply these methods to data from the Virginia Department of Criminal Justice Services. We estimate that the VPRAI algorithm has near-perfect specificity, but its sensitivity differs by defendant race. Judicial decisions also display evidence of bias; we estimate wrongful detention rates of 39.7% and 51.4% among white and Black defendants, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.08599v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kimberly A. Hochstedler Webb, Sarah A. Riley, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Long-range Ising model for regional-scale seismic risk analysis</title>
      <link>https://arxiv.org/abs/2403.11429</link>
      <description>arXiv:2403.11429v2 Announce Type: replace 
Abstract: This study introduces the long-range Ising model from statistical mechanics to the Performance-Based Earthquake Engineering (PBEE) framework for regional seismic damage analysis. The application of the PBEE framework at a regional scale involves estimating the damage states of numerous structures, typically performed using fragility function-based stochastic simulations. However, these simulations often assume conditional independence or employ simplistic dependency models among the damage states of structures, leading to significant misrepresentation of regional risk. The Ising model addresses this issue by converting the available information on binary damage states (safe or failure) into a joint probability mass function, leveraging the principle of maximum entropy. The Ising model offers two main benefits: (1) it requires only the first- and second-order cross-moments, enabling seamless integration with the existing PBEE framework, and (2) it provides meaningful physical interpretations of the model parameters, facilitating the uncovering of insights not apparent from data. To demonstrate the proposed method, we applied the Ising model to 156 buildings in Antakya, Turkey, using post-hazard damage evaluation data, and to 182 buildings in Pacific Heights, San Francisco, using simulated data from the Regional Resilience Determination (R2D) tool. In both instances, the Ising model accurately reproduces the provided information and generates meaningful insights into regional damage. The study also investigates the change in Ising model parameters under varying earthquake magnitudes, along with the mean-field approximation, further facilitating the applicability of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11429v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebin Oh, Sang-ri Yi, Ziqi Wang</dc:creator>
    </item>
    <item>
      <title>Shift-invariant homogeneous classes of random fields</title>
      <link>https://arxiv.org/abs/2111.00792</link>
      <description>arXiv:2111.00792v4 Announce Type: replace-cross 
Abstract: Given an $R^d$-valued random field (rf) $Z(t),t\in T$ and an $\alpha$-homogeneous mapping $\kappa$ we define the corresponding equivalent class of rf's (denoted by $K_\alpha$) which include representers of the same tail measure $\nu_Z$. When $T$ is an additive group, tractable equivalent classes of interest are the shift-invariant ones, which contain in particular all independent random shifts of $Z$. This contribution is mainly concerned with the investigation of the probabilistic properties of shift-invariant $K_\alpha$'s. Important objects introduced in our setting are tail and spectral tail rf's. Further, the class of universal maps $U$ acting on elements of $K_\alpha$ turns out to be crucial for properties of functionals of $Z$. Applications of our findings concern max-stable and symmetric $\alpha$-stable rf's, their maximal indices as well as their random shift-representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.00792v4</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jmaa.2024.128517</arxiv:DOI>
      <dc:creator>Enkelejd Hashorva</dc:creator>
    </item>
    <item>
      <title>Sample size determination via learning-type curves</title>
      <link>https://arxiv.org/abs/2303.09575</link>
      <description>arXiv:2303.09575v2 Announce Type: replace-cross 
Abstract: This paper is concerned with sample size determination methodology for prediction models. We propose combining the individual calculations via a learning-type curve. We suggest two distinct ways of doing so, a deterministic skeleton of a learning curve and a Gaussian process centred upon its deterministic counterpart. We employ several learning algorithms for modelling the primary endpoint and distinct measures for trial efficacy. We find that the performance may vary with the sample size, but borrowing information across sample size universally improves the performance of such calculations. The Gaussian process-based learning curve appears more robust and statistically efficient, while computational efficiency is comparable. We suggest that anchoring against historical evidence when extrapolating sample sizes should be adopted when such data are available. The methods are illustrated on binary and survival endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.09575v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.10121</arxiv:DOI>
      <dc:creator>Alimu Dayimu, Nikola Simidjievski, Nikolaos Demiris, Jean Abraham</dc:creator>
    </item>
    <item>
      <title>A Physics-Informed, Deep Double Reservoir Network for Forecasting Boundary Layer Velocity</title>
      <link>https://arxiv.org/abs/2311.05728</link>
      <description>arXiv:2311.05728v2 Announce Type: replace-cross 
Abstract: When a fluid flows over a solid surface, it creates a thin boundary layer where the flow velocity is influenced by the surface through viscosity, and can transition from laminar to turbulent at sufficiently high speeds. Understanding and forecasting the fluid dynamics under these conditions is one of the most challenging scientific problems in fluid dynamics. It is therefore of high interest to formulate models able to capture the nonlinear spatio-temporal velocity structure as well as produce forecasts in a computationally efficient manner. Traditional statistical approaches are limited in their ability to produce timely forecasts of complex, nonlinear spatio-temporal structures which are at the same time able to incorporate the underlying flow physics. In this work, we propose a model to accurately forecast boundary layer velocities with a deep double reservoir computing network which is capable of capturing the complex, nonlinear dynamics of the boundary layer while at the same time incorporating physical constraints via a penalty obtained by a Partial Differential Equation (PDE). Simulation studies on a one-dimensional viscous fluid demonstrate how the proposed model is able to produce accurate forecasts while simultaneously accounting for energy loss. The application focuses on boundary layer data on a water tunnel with a PDE penalty derived from an appropriate simplification of the Navier-Stokes equations, showing forecasts improved by 33.7% and 80.0% in terms of mass conservation and variability of velocity fluctuation, respectfully, against non physics-informed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05728v2</guid>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Bonas, David H. Richter, Stefano Castruccio</dc:creator>
    </item>
    <item>
      <title>Biarchetype analysis: simultaneous learning of observations and features based on extremes</title>
      <link>https://arxiv.org/abs/2311.11153</link>
      <description>arXiv:2311.11153v2 Announce Type: replace-cross 
Abstract: We introduce a novel exploratory technique, termed biarchetype analysis, which extends archetype analysis to simultaneously identify archetypes of both observations and features. This innovative unsupervised machine learning tool aims to represent observations and features through instances of pure types, or biarchetypes, which are easily interpretable as they embody mixtures of observations and features. Furthermore, the observations and features are expressed as mixtures of the biarchetypes, which makes the structure of the data easier to understand. We propose an algorithm to solve biarchetype analysis. Although clustering is not the primary aim of this technique, biarchetype analysis is demonstrated to offer significant advantages over biclustering methods, particularly in terms of interpretability. This is attributed to biarchetypes being extreme instances, in contrast to the centroids produced by biclustering, which inherently enhances human comprehension. The application of biarchetype analysis across various machine learning challenges underscores its value, and both the source code and examples are readily accessible in R and Python at https://github.com/aleixalcacer/JA-BIAA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.11153v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TPAMI.2024.3400730</arxiv:DOI>
      <dc:creator>Aleix Alcacer, Irene Epifanio, Ximo Gual-Arnau</dc:creator>
    </item>
    <item>
      <title>Decoding Social Sentiment in DAO: A Comparative Analysis of Blockchain Governance Communities</title>
      <link>https://arxiv.org/abs/2311.14676</link>
      <description>arXiv:2311.14676v2 Announce Type: replace-cross 
Abstract: Blockchain technology is leading a revolutionary transformation across diverse industries, with effective governance being critical for the success and sustainability of blockchain projects. Community forums, pivotal in engaging decentralized autonomous organizations (DAOs), significantly impact blockchain governance decisions. Concurrently, Natural Language Processing (NLP), particularly sentiment analysis, provides powerful insights from textual data. While prior research has explored the potential of NLP tools in social media sentiment analysis, there is a gap in understanding the sentiment landscape of blockchain governance communities. The evolving discourse and sentiment dynamics on the forums of top DAOs remain largely unknown. This paper delves deep into the evolving discourse and sentiment dynamics on the public forums of leading DeFi projects: Aave, Uniswap, Curve DAO, Yearn.finance, Merit Circle, and Balancer, focusing primarily on discussions related to governance issues. Our study shows that participants in decentralized communities generally express positive sentiments during Discord discussions. Furthermore, there is a potential interaction between discussion intensity and sentiment dynamics; higher discussion volume may contribute to a more stable sentiment from code analysis. The insights gained from this study are valuable for decision-makers in blockchain governance, underscoring the pivotal role of sentiment analysis in interpreting community emotions and its evolving impact on the landscape of blockchain governance. This research significantly contributes to the interdisciplinary exploration of the intersection of blockchain and society, specifically emphasizing the decentralized blockchain governance ecosystem. We provide our data and code for replicability as open access on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14676v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.31219/osf.io/bq6tu</arxiv:DOI>
      <dc:creator>Yutong Quan, Xintong Wu, Wanlin Deng, Luyao Zhang</dc:creator>
    </item>
    <item>
      <title>Extreme value statistics of nerve transmission delay</title>
      <link>https://arxiv.org/abs/2402.00484</link>
      <description>arXiv:2402.00484v2 Announce Type: replace-cross 
Abstract: Nerve transmission delay is an important topic in neuroscience. Spike signals fired or received at the dendrites of a neuron travel from the axon to the presynaptic cell. The spike signal triggers a chemical reaction at the synapse, wherein a presynaptic cell transfers neurotransmitters to the postsynaptic cell, and regenerates electrical signals by a chemical reaction process through ion channels and transmits it to neighboring neurons. In the context of describing the complex physiological reaction process as a stochastic process, this study aimed to show that the distribution of the maximum time interval of spike signals follows extreme order statistics. By considering the statistical variance in the time constant of the Leaky Integrate-and-Fire model, which is a deterministic time evolution model of spike signals, we enabled randomness in the time interval of spike signals. When the time constant follows an exponential distribution function, the time interval of the spike signal also follows an exponential distribution. In this case, our theory and simulations confirmed that the histogram of the maximum time interval follows the Gumbel distribution, which is one of the three types of extreme value statistics. We also confirmed that the histogram of the maximum time interval follows a Fr\'{e}chet distribution when the time interval of the spike signal follows a Pareto distribution. These findings confirm that nerve transmission delay can be described using extreme value statistics and could, therefore, be used as a new indicator for transmission delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00484v2</guid>
      <category>q-bio.NC</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satori Tsuzuki</dc:creator>
    </item>
    <item>
      <title>Causal Discovery in Multivariate Extremes with a Hydrological Analysis of Swiss River Discharges</title>
      <link>https://arxiv.org/abs/2405.10371</link>
      <description>arXiv:2405.10371v2 Announce Type: replace-cross 
Abstract: Causal asymmetry is based on the principle that an event is a cause only if its absence would not have been a cause. From there, uncovering causal effects becomes a matter of comparing a well-defined score in both directions. Motivated by studying causal effects at extreme levels of a multivariate random vector, we propose to construct a model-agnostic causal score relying solely on the assumption of the existence of a max-domain of attraction. Based on a representation of a Generalized Pareto random vector, we construct the causal score as the Wasserstein distance between the margins and a well-specified random variable. The proposed methodology is illustrated on a hydrologically simulated dataset of different characteristics of catchments in Switzerland: discharge, precipitation, and snowmelt.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10371v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linda Mhalla, Val\'erie Chavez-Demoulin, Philippe Naveau</dc:creator>
    </item>
  </channel>
</rss>
