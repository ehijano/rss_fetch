<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Apr 2025 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Clustering Does Not Always Imply Latent Geometry</title>
      <link>https://arxiv.org/abs/2503.05369</link>
      <description>arXiv:2503.05369v1 Announce Type: cross 
Abstract: The latent space approach to complex networks has revealed fundamental principles and symmetries, enabling geometric methods. However, the conditions under which network topology implies geometricity remain unclear. We provide a mathematical proof and empirical evidence showing that the multiscale self-similarity of complex networks is a crucial factor in implying latent geometry. Using degree-thresholding renormalization, we prove that any random scale-free graph in a $d$-dimensional homogeneous and isotropic manifold is self-similar when interactions are pairwise. Hence, both clustering and self-similarity are required to imply geometricity. Our findings highlight that correlated links can lead to finite clustering without self-similarity, and therefore without inherent latent geometry. The implications are significant for network mapping and ensemble equivalence between graphs and continuous spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05369v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roya Aliakbarisani, Mari\'an Bogu\~n\'a, M. \'Angeles Serrano</dc:creator>
    </item>
    <item>
      <title>Bayesian critical points in classical lattice models</title>
      <link>https://arxiv.org/abs/2504.01264</link>
      <description>arXiv:2504.01264v1 Announce Type: cross 
Abstract: The Boltzmann distribution encodes our subjective knowledge of the configuration in a classical lattice model, given only its Hamiltonian. If we acquire further information about the configuration from measurement, our knowledge is updated according to Bayes' theorem. We examine the resulting "conditioned ensembles", finding that they show many new phase transitions and new renormalization-group fixed points. (Similar conditioned ensembles also describe "partial quenches" in which some of the system's degrees of freedom are instantaneously frozen, while the others continue to evolve.) After describing general features of the replica field theories for these problems, we analyze the effect of measurement on illustrative critical systems, including: critical Ising and Potts models, which show surprisingly rich phase diagrams, with RG fixed points at weak, intermediate, and infinite measurement strength; various models involving free fields, XY spins, or flux lines in 2D or 3D; and geometrical models such as polymers or clusters. We make connections with quantum dynamics, in particular with "charge sharpening" in 1D, by giving a formalism for measurement of classical stochastic processes: e.g. we give a purely hydrodynamic derivation of the known effective field theory for charge sharpening. We discuss qualitative differences between RG flows for the above measured systems, described by $N\to 1$ replica limits, and those for disordered systems, described by $N\to 0$ limits. In addition to discussing measurement of critical states, we give a unifying treatment of a family of inference problems for non-critical states. These are related to the Nishimori line in the phase diagram of the random-bond Ising model, and are relevant to various quantum error correction problems. We describe distinct physical interpretations of conditioned ensembles and note interesting open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01264v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>quant-ph</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Nahum, Jesper Lykke Jacobsen</dc:creator>
    </item>
    <item>
      <title>Learning and criticality in a self-organizing model of connectome growth</title>
      <link>https://arxiv.org/abs/2504.01565</link>
      <description>arXiv:2504.01565v1 Announce Type: cross 
Abstract: The exploration of brain networks has reached an important milestone as relatively large and reliable information has been gathered for connectomes of different species. Analyses of connectome data sets reveal that the structural length and the distributions of in- and out-node strengths follow heavy-tailed lognormal statistics, while the functional network properties exhibit powerlaw tails, suggesting that the brain operates close to a critical point where computational capabilities and sensitivity to stimulus is optimal. Because these universal network features emerge from bottom-up (self-)organization, one can pose the question of whether they can be modeled via a common framework, particularly through the lens of criticality of statistical physical systems. Here, we simultaneously reproduce the powerlaw statistics of connectome edge weights and the lognormal distributions of node strengths from an avalanche-type model with learning that operates on baseline networks that mimic the neuronal circuitry. We observe that the avalanches created by a sandpile-like model on simulated neurons connected by a hierarchical modular network (HMN) produce robust powerlaw avalanche size distributions with critical exponents of 3/2 characteristic of neuronal systems. Introducing Hebbian learning, wherein neurons that `fire together, wire together,' recovers the powerlaw distribution of edge weights and the lognormal distributions of node degrees, comparable to those obtained from connectome data. Our results strengthen the notion of a critical brain, one whose local interactions drive connectivity and learning without a need for external intervention and precise tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01565v1</guid>
      <category>physics.bio-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michelle T. Cirunay, Rene C. Batac, G\'eza Od\'or</dc:creator>
    </item>
    <item>
      <title>Dissipation induced transition between delocalization and localization in the three-dimensional Anderson model</title>
      <link>https://arxiv.org/abs/2409.20319</link>
      <description>arXiv:2409.20319v2 Announce Type: replace 
Abstract: We investigate the probable delocalization-localization transition in open quantum systems with disorder. The disorder can induce localization in isolated quantum systems and it is generally recognized that localization is fragile under the action of dissipations from the external environment due to its interfering nature. Recent work [Y. Liu, et al, Phys. Rev. Lett. 132, 216301 (2024).] found that a one-dimensional quasiperiodic system can be driven into the localization phase by a tailored local dissipation where a dissipation-induced delocalized-localized transition is proposed. Based on this, we consider a more realistic system and show that a dissipation-induced transition between delocalization and localization appears in the three-dimensional (3D) Anderson model. By tuning local dissipative operators acting on nearest neighboring sites, we find that the system can relax to localized states dominated steady state instead of the choice of initial conditions and dissipation strengths. Moreover, we can also realize a delocalized states predominated steady-state from a localized initial state by using a kind of dissipation operators acting on next nearest neighboring sites. Our results enrich the applicability of dissipation-induced localization and identify the transition between delocalized and localized phases in 3D disordered systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.20319v2</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanpu Yang, Xiang-Ping Jiang, Zijun Wei, Yucheng Wang, Lei Pan</dc:creator>
    </item>
    <item>
      <title>Phases and Phase Transitions of the Disordered Quantum Clock Model</title>
      <link>https://arxiv.org/abs/2410.07640</link>
      <description>arXiv:2410.07640v2 Announce Type: replace 
Abstract: We study the phases and phase transitions of a disordered one-dimensional quantum $q$-state clock Hamiltonian using large-scale Monte Carlo simulations. Making contact with earlier results, we confirm that the clean, translational invariant version of the model, for $q=6$, hosts an intermediate emergent quasi-long-range ordered (QLRO) phase between the symmetry-broken true long-range ordered (TLRO) phase and the disordered (paramagnetic) phase. With increasing disorder strength, the quasi-long-range ordered phase shrinks and finally vanishes at a multi-critical point, beyond which there is a direct transition from the TLRO phase to the paramagnetic phase. After establishing the phase diagram, we characterize the critical behaviors of the various quantum phase transitions in the model. We find that weak disorder is an irrelevant perturbation of the Berezinskii-Kosterlitz-Thouless transitions that separate the QLRO phase from the TLRO and paramagnetic phases. For stronger disorder, some of the critical exponents become disorder-dependent already before the system reaches the multicritical point. We also show that beyond the multicritical point, the direct transition from the TLRO phase to the paramagnetic phase is governed by an infinite-randomness critical point in line with strong-disorder renormalization group predictions. While our numerical results are for $q=6$, we expect the qualitative features of the behavior to hold for all $q&gt;4$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07640v2</guid>
      <category>cond-mat.dis-nn</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.111.094212</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 111, 094212 (2025)</arxiv:journal_reference>
      <dc:creator>Vishnu Pulloor Kuttanikkad, Gaurav Khairnar, Rajesh Narayanan, Thomas Vojta</dc:creator>
    </item>
    <item>
      <title>Fundamental computational limits of weak learnability in high-dimensional multi-index models</title>
      <link>https://arxiv.org/abs/2405.15480</link>
      <description>arXiv:2405.15480v4 Announce Type: replace-cross 
Abstract: Multi-index models - functions which only depend on the covariates through a non-linear transformation of their projection on a subspace - are a useful benchmark for investigating feature learning with neural nets. This paper examines the theoretical boundaries of efficient learnability in this hypothesis class, focusing on the minimum sample complexity required for weakly recovering their low-dimensional structure with first-order iterative algorithms, in the high-dimensional regime where the number of samples $n\!=\!\alpha d$ is proportional to the covariate dimension $d$. Our findings unfold in three parts: (i) we identify under which conditions a trivial subspace can be learned with a single step of a first-order algorithm for any $\alpha\!&gt;\!0$; (ii) if the trivial subspace is empty, we provide necessary and sufficient conditions for the existence of an easy subspace where directions that can be learned only above a certain sample complexity $\alpha\!&gt;\!\alpha_c$, where $\alpha_{c}$ marks a computational phase transition. In a limited but interesting set of really hard directions -- akin to the parity problem -- $\alpha_c$ is found to diverge. Finally, (iii) we show that interactions between different directions can result in an intricate hierarchical learning phenomenon, where directions can be learned sequentially when coupled to easier ones. We discuss in detail the grand staircase picture associated to these functions (and contrast it with the original staircase one). Our theory builds on the optimality of approximate message-passing among first-order iterative methods, delineating the fundamental learnability limit across a broad spectrum of algorithms, including neural networks trained with gradient descent, which we discuss in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15480v4</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CC</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emanuele Troiani, Yatin Dandi, Leonardo Defilippis, Lenka Zdeborov\'a, Bruno Loureiro, Florent Krzakala</dc:creator>
    </item>
    <item>
      <title>Streamlined optical training of large-scale modern deep learning architectures with direct feedback alignment</title>
      <link>https://arxiv.org/abs/2409.12965</link>
      <description>arXiv:2409.12965v2 Announce Type: replace-cross 
Abstract: Modern deep learning relies nearly exclusively on dedicated electronic hardware accelerators. Photonic approaches, with low consumption and high operation speed, are increasingly considered for inference but, to date, remain mostly limited to relatively basic tasks. Simultaneously, the problem of training deep and complex neural networks, overwhelmingly performed through backpropagation, remains a significant limitation to the size and, consequently, the performance of current architectures and a major compute and energy bottleneck. Here, we experimentally implement a versatile and scalable training algorithm, called direct feedback alignment, on a hybrid electronic-photonic platform. An optical processing unit performs large-scale random matrix multiplications, which is the central operation of this algorithm, at speeds up to 1500 TeraOPS under 30 Watts of power. We perform optical training of modern deep learning architectures, including Transformers, with more than 1B parameters, and obtain good performances on language, vision, and diffusion-based generative tasks. We study the scaling of the training time, and demonstrate a potential advantage of our hybrid opto-electronic approach for ultra-deep and wide neural networks, thus opening a promising route to sustain the exponential growth of modern artificial intelligence beyond traditional von Neumann approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12965v2</guid>
      <category>cs.ET</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>physics.app-ph</category>
      <category>physics.optics</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziao Wang, Kilian M\"uller, Matthew Filipovich, Julien Launay, Ruben Ohana, Gustave Pariente, Safa Mokaadi, Charles Brossollet, Fabien Moreau, Alessandro Cappelli, Iacopo Poli, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan</dc:creator>
    </item>
    <item>
      <title>Dissipation-Driven Transition of Particles from Dispersive to Flat Bands</title>
      <link>https://arxiv.org/abs/2504.00796</link>
      <description>arXiv:2504.00796v2 Announce Type: replace-cross 
Abstract: Flat bands (FBs) play a crucial role in condensed matter physics, offering an ideal platform to study strong correlation effects and enabling applications in diffraction-free photonics and quantum devices. However, the study and application of FB properties are susceptible to interference from dispersive bands. Here, we explore the impact of bond dissipation on systems hosting both flat and dispersive bands by calculating the steady-state density matrix. We demonstrate that bond dissipation can drive particles from dispersive bands into FBs and establish the general conditions for this phenomenon to occur. Our results demonstrate that dissipation can facilitate FB preparation, property measurement, and utilization. This opens a new avenue for exploring FB physics in open quantum systems, with potential implications for strongly correlated physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00796v2</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>physics.optics</category>
      <pubDate>Thu, 03 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yutao Hu, Chao Yang, Yucheng Wang</dc:creator>
    </item>
  </channel>
</rss>
