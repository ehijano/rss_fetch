<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 May 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>DSAM: A Deep Learning Framework for Analyzing Temporal and Spatial Dynamics in Brain Networks</title>
      <link>https://arxiv.org/abs/2405.15805</link>
      <description>arXiv:2405.15805v1 Announce Type: new 
Abstract: Resting-state functional magnetic resonance imaging (rs-fMRI) is a noninvasive technique pivotal for understanding human neural mechanisms of intricate cognitive processes. Most rs-fMRI studies compute a single static functional connectivity matrix across brain regions of interest, or dynamic functional connectivity matrices with a sliding window approach. These approaches are at risk of oversimplifying brain dynamics and lack proper consideration of the goal at hand. While deep learning has gained substantial popularity for modeling complex relational data, its application to uncovering the spatiotemporal dynamics of the brain is still limited. We propose a novel interpretable deep learning framework that learns goal-specific functional connectivity matrix directly from time series and employs a specialized graph neural network for the final classification. Our model, DSAM, leverages temporal causal convolutional networks to capture the temporal dynamics in both low- and high-level feature representations, a temporal attention unit to identify important time points, a self-attention unit to construct the goal-specific connectivity matrix, and a novel variant of graph neural network to capture the spatial dynamics for downstream classification. To validate our approach, we conducted experiments on the Human Connectome Project dataset with 1075 samples to build and interpret the model for the classification of sex group, and the Adolescent Brain Cognitive Development Dataset with 8520 samples for independent testing. Compared our proposed framework with other state-of-art models, results suggested this novel approach goes beyond the assumption of a fixed connectivity matrix and provides evidence of goal-specific brain connectivity patterns, which opens up the potential to gain deeper insights into how the human brain adapts its functional connectivity specific to the task at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15805v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bishal Thapaliya, Robyn Miller, Jiayu Chen, Yu-Ping Wang, Esra Akbas, Ram Sapkota, Bhaskar Ray, Pranav Suresh, Santosh Ghimire, Vince Calhoun, Jingyu Liu</dc:creator>
    </item>
    <item>
      <title>Peripheral Nervous System Responses to Food Stimuli: Analysis Using Data Science Approaches</title>
      <link>https://arxiv.org/abs/2405.15810</link>
      <description>arXiv:2405.15810v1 Announce Type: new 
Abstract: In the field of food, as in other fields, the measurement of emotional responses to food and their sensory properties is a major challenge. In the present protocol, we propose a step-by-step procedure that allows a physiological description of odors, aromas, and their hedonic properties. The method rooted in subgroup discovery belongs to the field of data science and especially data mining. It is still little used in the field of food and is based on a descriptive modeling of emotions on the basis of human physiological responses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15810v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-1-0716-2934-5_18</arxiv:DOI>
      <arxiv:journal_reference>Basic Protocols on Emotions, Senses, and Foods, Springer US; Springer US, pp.233-246, 2023, Methods and Protocols in Food Science, 978-1-0716-2933-8</arxiv:journal_reference>
      <dc:creator>Maelle Moranges (CRNL, LIRIS), Marc Plantevit (LIRIS, LRE), Moustafa Bensafi (CRNL)</dc:creator>
    </item>
    <item>
      <title>Pseudo Channel: Time Embedding for Motor Imagery Decoding</title>
      <link>https://arxiv.org/abs/2405.15812</link>
      <description>arXiv:2405.15812v1 Announce Type: new 
Abstract: Motor imagery (MI) based EEG represents a frontier in enabling direct neural control of external devices and advancing neural rehabilitation. This study introduces a novel time embedding technique, termed traveling-wave based time embedding, utilized as a pseudo channel to enhance the decoding accuracy of MI-EEG signals across various neural network architectures. Unlike traditional neural network methods that fail to account for the temporal dynamics in MI-EEG in individual difference, our approach captures time-related changes for different participants based on a priori knowledge. Through extensive experimentation with multiple participants, we demonstrate that this method not only improves classification accuracy but also exhibits greater adaptability to individual differences compared to position encoding used in Transformer architecture. Significantly, our results reveal that traveling-wave based time embedding crucially enhances decoding accuracy, particularly for participants typically considered "EEG-illiteracy". As a novel direction in EEG research, the traveling-wave based time embedding not only offers fresh insights for neural network decoding strategies but also expands new avenues for research into attention mechanisms in neuroscience and a deeper understanding of EEG signals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15812v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengqing Miao, Meirong Zhao</dc:creator>
    </item>
    <item>
      <title>Tangent space functional reconfigurations in individuals at risk for alcohol use disorder</title>
      <link>https://arxiv.org/abs/2405.15905</link>
      <description>arXiv:2405.15905v1 Announce Type: new 
Abstract: Human brain function dynamically adjusts to ever-changing stimuli from the external environment. Studies characterizing brain functional reconfiguration are nevertheless scarce. Here we present a principled mathematical framework to quantify brain functional reconfiguration when engaging and disengaging from a stop signal task (SST). We apply tangent space projection (a Riemannian geometry mapping technique) to transform functional connectomes (FCs) and quantify functional reconfiguration using the correlation distance of the resulting tangent-FCs. Our goal was to compare functional reconfigurations in individuals at risk for alcohol use disorder (AUD). We hypothesized that functional reconfigurations when transitioning in/from a task would be influenced by family history of alcohol use disorder (FHA) and other AUD risk factors. Multilinear regression model results showed that engaging and disengaging functional reconfiguration were driven by different AUD risk factors. Functional reconfiguration when engaging in the SST was negatively associated with recent drinking. When disengaging from the SST, however, functional reconfiguration was negatively associated with FHA. In both models, several other factors contributed to the explanation of functional reconfiguration. This study demonstrates that tangent-FCs can characterize task-induced functional reconfiguration, and that it is related to AUD risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15905v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Moghaddam, Mario Dzemidzic, Daniel Guerrero, Mintao Liu, Jonathan Alessi, Martin H. Plawecki, Jaroslaw Harezlak, David Kareken, Joaqu\'in Go\~ni</dc:creator>
    </item>
    <item>
      <title>Exploring the Enigma of Neural Dynamics Through A Scattering-Transform Mixer Landscape for Riemannian Manifold</title>
      <link>https://arxiv.org/abs/2405.16357</link>
      <description>arXiv:2405.16357v1 Announce Type: new 
Abstract: The human brain is a complex inter-wired system that emerges spontaneous functional fluctuations. In spite of tremendous success in the experimental neuroscience field, a system-level understanding of how brain anatomy supports various neural activities remains elusive. Capitalizing on the unprecedented amount of neuroimaging data, we present a physics-informed deep model to uncover the coupling mechanism between brain structure and function through the lens of data geometry that is rooted in the widespread wiring topology of connections between distant brain regions. Since deciphering the puzzle of self-organized patterns in functional fluctuations is the gateway to understanding the emergence of cognition and behavior, we devise a geometric deep model to uncover manifold mapping functions that characterize the intrinsic feature representations of evolving functional fluctuations on the Riemannian manifold. In lieu of learning unconstrained mapping functions, we introduce a set of graph-harmonic scattering transforms to impose the brain-wide geometry on top of manifold mapping functions, which allows us to cast the manifold-based deep learning into a reminiscent of MLP-Mixer architecture (in computer vision) for Riemannian manifold. As a proof-of-concept approach, we explore a neural-manifold perspective to understand the relationship between (static) brain structure and (dynamic) function, challenging the prevailing notion in cognitive neuroscience by proposing that neural activities are essentially excited by brain-wide oscillation waves living on the geometry of human connectomes, instead of being confined to focal areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16357v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingting Dan, Ziquan Wei, Won Hwa Kim, Guorong Wu</dc:creator>
    </item>
    <item>
      <title>Oscillations in neuronal activity: a neuron-centered spatiotemporal model of the Unfolded Protein Response in prion diseases</title>
      <link>https://arxiv.org/abs/2405.16695</link>
      <description>arXiv:2405.16695v1 Announce Type: new 
Abstract: Many neurodegenerative diseases (NDs) are characterized by the slow spatial spread of toxic protein species in the brain. The toxic proteins can induce neuronal stress, triggering the Unfolded Protein Response (UPR), which slows or stops protein translation and can indirectly reduce the toxic load. However, the UPR may also trigger processes leading to apoptotic cell death and the UPR is implicated in the progression of several NDs. In this paper, we develop a novel mathematical model to describe the spatiotemporal dynamics of the UPR mechanism for prion diseases. Our model is centered around a single neuron, with representative proteins P (healthy) and S (toxic) interacting with heterodimer dynamics (S interacts with P to form two S's). The model takes the form of a coupled system of nonlinear reaction-diffusion equations with a delayed, nonlinear flux for P (delay from the UPR). Through the delay, we find parameter regimes that exhibit oscillations in the P- and S-protein levels. We find that oscillations are more pronounced when the S-clearance rate and S-diffusivity are small in comparison to the P-clearance rate and P-diffusivity, respectively. The oscillations become more pronounced as delays in initiating the UPR increase. We also consider quasi-realistic clinical parameters to understand how possible drug therapies can alter the course of a prion disease. We find that decreasing the production of P, decreasing the recruitment rate, increasing the diffusivity of S, increasing the UPR S-threshold, and increasing the S clearance rate appear to be the most powerful modifications to reduce the mean UPR intensity and potentially moderate the disease progression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16695v1</guid>
      <category>q-bio.NC</category>
      <category>math.DS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elliot M. Miller, Tat Chung D. Chan, Carlos Montes-Matamoros, Omar Sharif, Laurent Pujo-Menjouet, Michael R. Lindstrom</dc:creator>
    </item>
    <item>
      <title>An Investigation of Conformal Isometry Hypothesis for Grid Cells</title>
      <link>https://arxiv.org/abs/2405.16865</link>
      <description>arXiv:2405.16865v1 Announce Type: new 
Abstract: This paper investigates the conformal isometry hypothesis as a potential explanation for the emergence of hexagonal periodic patterns in the response maps of grid cells. The hypothesis posits that the activities of the population of grid cells form a high-dimensional vector in the neural space, representing the agent's self-position in 2D physical space. As the agent moves in the 2D physical space, the vector rotates in a 2D manifold in the neural space, driven by a recurrent neural network. The conformal isometry hypothesis proposes that this 2D manifold in the neural space is a conformally isometric embedding of the 2D physical space, in the sense that local displacements of the vector in neural space are proportional to local displacements of the agent in the physical space. Thus the 2D manifold forms an internal map of the 2D physical space, equipped with an internal metric. In this paper, we conduct numerical experiments to show that this hypothesis underlies the hexagon periodic patterns of grid cells. We also conduct theoretical analysis to further support this hypothesis. In addition, we propose a conformal modulation of the input velocity of the agent so that the recurrent neural network of grid cells satisfies the conformal isometry hypothesis automatically. To summarize, our work provides numerical and theoretical evidences for the conformal isometry hypothesis for grid cells and may serve as a foundation for further development of normative models of grid cells and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16865v1</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dehong Xu, Ruiqi Gao, Wen-Hao Zhang, Xue-Xin Wei, Ying Nian Wu</dc:creator>
    </item>
    <item>
      <title>Theories of synaptic memory consolidation and intelligent plasticity for continual learning</title>
      <link>https://arxiv.org/abs/2405.16922</link>
      <description>arXiv:2405.16922v1 Announce Type: new 
Abstract: Humans and animals learn throughout life. Such continual learning is crucial for intelligence. In this chapter, we examine the pivotal role plasticity mechanisms with complex internal synaptic dynamics could play in enabling this ability in neural networks. By surveying theoretical research, we highlight two fundamental enablers for continual learning. First, synaptic plasticity mechanisms must maintain and evolve an internal state over several behaviorally relevant timescales. Second, plasticity algorithms must leverage the internal state to intelligently regulate plasticity at individual synapses to facilitate the seamless integration of new memories while avoiding detrimental interference with existing ones. Our chapter covers successful applications of these principles to deep neural networks and underscores the significance of synaptic metaplasticity in sustaining continual learning capabilities. Finally, we outline avenues for further research to understand the brain's superb continual learning abilities and harness similar mechanisms for artificial intelligence systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16922v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Friedemann Zenke, Axel Laborieux</dc:creator>
    </item>
    <item>
      <title>Biological Neurons Compete with Deep Reinforcement Learning in Sample Efficiency in a Simulated Gameworld</title>
      <link>https://arxiv.org/abs/2405.16946</link>
      <description>arXiv:2405.16946v1 Announce Type: new 
Abstract: How do biological systems and machine learning algorithms compare in the number of samples required to show significant improvements in completing a task? We compared the learning efficiency of in vitro biological neural networks to the state-of-the-art deep reinforcement learning (RL) algorithms in a simplified simulation of the game `Pong'. Using DishBrain, a system that embodies in vitro neural networks with in silico computation using a high-density multi-electrode array, we contrasted the learning rate and the performance of these biological systems against time-matched learning from three state-of-the-art deep RL algorithms (i.e., DQN, A2C, and PPO) in the same game environment. This allowed a meaningful comparison between biological neural systems and deep RL. We find that when samples are limited to a real-world time course, even these very simple biological cultures outperformed deep RL algorithms across various game performance characteristics, implying a higher sample efficiency. Ultimately, even when tested across multiple types of information input to assess the impact of higher dimensional data input, biological neurons showcased faster learning than all deep reinforcement learning agents.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16946v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Moein Khajehnejad, Forough Habibollahi, Aswin Paul, Adeel Razi, Brett J. Kagan</dc:creator>
    </item>
    <item>
      <title>Metric structural human connectomes: localization and multifractality of eigenmodes</title>
      <link>https://arxiv.org/abs/2405.17349</link>
      <description>arXiv:2405.17349v1 Announce Type: new 
Abstract: In this study, we explore the fundamental principles behind the architecture of the human brain's structural connectome, from the perspective of spectral analysis of Laplacian and adjacency matrices. Building on the idea that the brain strikes a balance between efficient information processing and minimizing wiring costs, we aim to understand the impact of the metric properties of the connectome and how they relate to the existence of an inherent scale. We demonstrate that a simple generative model, combining nonlinear preferential attachment with an exponential penalty for spatial distance between nodes, can effectively reproduce several key characteristics of the human connectome, including spectral density, edge length distribution, eigenmode localization and local clustering properties. We also delve into the finer spectral properties of the human structural connectomes by evaluating the inverse participation ratios ($\text{IPR}_q$) across various parts of the spectrum. Our analysis reveals that the level statistics in the soft cluster region of the Laplacian spectrum deviate from a purely Poisson distribution due to interactions between clusters. Additionally, we identified scar-like localized modes with large IPR values in the continuum spectrum. We identify multiple fractal eigenmodes distributed across different parts of the spectrum, evaluate their fractal dimensions and find a power-law relationship in the return probability, which is a hallmark of critical behavior. We discuss the conjectures that a brain operates in the Griffiths or multifractal phases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17349v1</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Bobyleva, Alexander Gorsky, Sergei Nechaev, Olga Valba, Nikita Pospelov</dc:creator>
    </item>
    <item>
      <title>CrEIMBO: Cross Ensemble Interactions in Multi-view Brain Observations</title>
      <link>https://arxiv.org/abs/2405.17395</link>
      <description>arXiv:2405.17395v1 Announce Type: new 
Abstract: Modern recordings of neural activity provide diverse observations of neurons across brain areas, behavioral conditions, and subjects -- thus presenting an exciting opportunity to reveal the fundamentals of brain-wide dynamics underlying cognitive function. Current methods, however, often fail to fully harness the richness of such data as they either provide an uninterpretable representation (e.g., via "black box" deep networks) or over-simplify the model (e.g., assume stationary dynamics or analyze each session independently). Here, instead of regarding asynchronous recordings that lack alignment in neural identity or brain areas as a limitation, we exploit these diverse views of the same brain system to learn a unified model of brain dynamics. We assume that brain observations stem from the joint activity of a set of functional neural ensembles (groups of co-active neurons) that are similar in functionality across recordings, and propose to discover the ensemble and their non-stationary dynamical interactions in a new model we term CrEIMBO (Cross-Ensemble Interactions in Multi-view Brain Observations). CrEIMBO identifies the composition of the per-session neural ensembles through graph-driven dictionary learning and models the ensemble dynamics as a latent sparse time-varying decomposition of global sub-circuits, thereby capturing non-stationary dynamics. CrEIMBO identifies multiple co-active sub-circuits while maintaining representation interpretability due to sharing sub-circuits across sessions. CrEIMBO distinguishes session-specific from global (session-invariant) computations by exploring when distinct sub-circuits are active. We demonstrate CrEIMBO's ability to recover ground truth components in synthetic data and uncover meaningful brain dynamics, capturing cross-subject and inter- and intra-area variability, in high-density electrode recordings of humans performing a memory task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17395v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Mudrik, Ryan Ly, Oliver Ruebel, Adam S. Charles</dc:creator>
    </item>
    <item>
      <title>When does compositional structure yield compositional generalization? A kernel theory</title>
      <link>https://arxiv.org/abs/2405.16391</link>
      <description>arXiv:2405.16391v1 Announce Type: cross 
Abstract: Compositional generalization (the ability to respond correctly to novel combinations of familiar components) is thought to be a cornerstone of intelligent behavior. Compositionally structured (e.g. disentangled) representations are essential for this; however, the conditions under which they yield compositional generalization remain unclear. To address this gap, we present a general theory of compositional generalization in kernel models with fixed, potentially nonlinear representations (which also applies to neural networks in the "lazy regime"). We prove that these models are functionally limited to adding up values assigned to conjunctions/combinations of components that have been seen during training ("conjunction-wise additivity"), and identify novel compositionality failure modes that arise from the data and model structure, even for disentangled inputs. For models in the representation learning (or "rich") regime, we show that networks can generalize on an important non-additive task (associative inference), and give a mechanistic explanation for why. Finally, we validate our theory empirically, showing that it captures the behavior of deep neural networks trained on a set of compositional tasks. In sum, our theory characterizes the principles giving rise to compositional generalization in kernel models and shows how representation learning can overcome their limitations. We further provide a formally grounded, novel generalization class for compositional tasks that highlights fundamental differences in the required learning mechanisms (conjunction-wise additivity).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16391v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Lippl, Kim Stachenfeld</dc:creator>
    </item>
    <item>
      <title>Crafting Interpretable Embeddings by Asking LLMs Questions</title>
      <link>https://arxiv.org/abs/2405.16714</link>
      <description>arXiv:2405.16714v1 Announce Type: cross 
Abstract: Large language models (LLMs) have rapidly improved text embeddings for a growing array of natural-language processing tasks. However, their opaqueness and proliferation into scientific domains such as neuroscience have created a growing need for interpretability. Here, we ask whether we can obtain interpretable embeddings through LLM prompting. We introduce question-answering embeddings (QA-Emb), embeddings where each feature represents an answer to a yes/no question asked to an LLM. Training QA-Emb reduces to selecting a set of underlying questions rather than learning model weights.
  We use QA-Emb to flexibly generate interpretable models for predicting fMRI voxel responses to language stimuli. QA-Emb significantly outperforms an established interpretable baseline, and does so while requiring very few questions. This paves the way towards building flexible feature spaces that can concretize and evaluate our understanding of semantic brain representations. We additionally find that QA-Emb can be effectively approximated with an efficient model, and we explore broader applications in simple NLP tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16714v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vinamra Benara, Chandan Singh, John X. Morris, Richard Antonello, Ion Stoica, Alexander G. Huth, Jianfeng Gao</dc:creator>
    </item>
    <item>
      <title>Uncovering multiscale structure in the variability of larval zebrafish navigation</title>
      <link>https://arxiv.org/abs/2405.17143</link>
      <description>arXiv:2405.17143v1 Announce Type: cross 
Abstract: Animals chain movements into long-lived motor strategies, exhibiting variability across scales that reflects the interplay between internal states and environmental cues. To reveal structure in such variability, we build Markov models of movement sequences that bridges across time scales and enables a quantitative comparison of behavioral phenotypes among individuals. Applied to larval zebrafish responding to diverse sensory cues, we uncover a hierarchy of long-lived motor strategies, dominated by changes in orientation distinguishing cruising versus wandering strategies. Environmental cues induce preferences along these modes at the population level: while fish cruise in the light, they wander in response to aversive stimuli, or in search for appetitive prey. As our method encodes the behavioral dynamics of each individual fish in the transitions among coarse-grained motor strategies, we use it to uncover a hierarchical structure in the phenotypic variability that reflects exploration-exploitation trade-offs. Across a wide range of sensory cues, a major source of variation among fish is driven by prior and/or immediate exposure to prey that induces exploitation phenotypes. A large degree of variability that is not explained by environmental cues unravels motivational states that override the sensory context to induce contrasting exploration-exploitation phenotypes. Altogether, by extracting the timescales of motor strategies deployed during navigation, our approach exposes structure among individuals and reveals internal states tuned by prior experience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17143v1</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.NC</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Sridhar, Massimo Vergassola, Joao C. Marques, Michael B. Orger, Antonio Carlos Costa, Claire Wyart</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian inference for community detection and connectivity of functional brain networks</title>
      <link>https://arxiv.org/abs/2301.07386</link>
      <description>arXiv:2301.07386v2 Announce Type: replace 
Abstract: Many functional magnetic resonance imaging (fMRI) studies rely on estimates of hierarchically organised brain networks whose segregation and integration reflect the dynamic transitions of latent cognitive states. However, most existing methods for estimating the community structure of networks from both individual and group-level analysis neglect the variability between subjects and lack validation. In this paper, we develop a new multilayer community detection method based on Bayesian latent block modelling. The method can robustly detect the group-level community structure of weighted functional networks that give rise to hidden brain states with an unknown number of communities and retain the variability of individual networks. For validation, we propose a new community structure-based multivariate Gaussian generative model to simulate synthetic signal. Our result shows that the inferred community memberships using hierarchical Bayesian analysis are consistent with the predefined node labels in the generative model. The method is also tested using real working memory task-fMRI data of 100 unrelated healthy subjects from the Human Connectome Project. The results show distinctive community structure patterns between 2-back, 0-back, and fixation conditions, which may reflect cognitive and behavioural states under working memory task conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.07386v2</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingbin Bian, Nizhuan Wang, Leonardo Novelli, Jonathan Keith, Adeel Razi</dc:creator>
    </item>
    <item>
      <title>Operators' cognitive performance under extreme hot-humid exposure and its physiological-psychological mechanism based on ECG, fNIRS, and Eye Tracking</title>
      <link>https://arxiv.org/abs/2403.00020</link>
      <description>arXiv:2403.00020v2 Announce Type: replace 
Abstract: Operators' cognitive functions are impaired significantly under extreme heat stress, potentially resulting in more severe secondary disasters. This research investigated the impact of elevated temperature and humidity (25 60%RH, 30 70%RH, 35 80%RH, 40 90%RH) on the cognitive functions and performance of operators. Meanwhile, we explored the psychological-physiological mechanism underlying the change in performance by electrocardiogram (ECG), functional near-infrared spectroscopy (fNIRS), and eye tracking physiologically. Psychological aspects such as situation awareness, workload, and working memory were assessed. Eventually, we verified and extended the maximal adaptability model to the extreme condition. Unexpectedly, a temporary improvement in simple reaction tasks but rapid impairment in advanced cognitive functions (i.e. situation awareness, communication, working memory) was obtained above 35 WBGT. The best performance in a suitable environment was due to more effective activation in the prefrontal cortex (PFC). With temperature increasing, more mistakes occurred and comprehension was impaired due to drowsiness and lower arousal levels, according to evidence of compensatory effect in fNIRS. In the extreme environment, the enhanced PFC cooperation with higher functional connectivity resulted in a temporary improvement, while depressed activation in PFC, heavy physical load, and poor regulation of the cardiovascular system restricted it. Our results provide a detailed study of the process of operators' performance and cognitive functions when encountering increasing heat stress, as well as its underlying mechanisms from a neuroergonomics perspective. This can contribute to a better understanding of the interaction between operators' performance and workplace conditions, and help to achieve a more reliable human-centered production system in the promising era of Industry 5.0.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00020v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhang, Ming Jia, Meng Li, Jianyu Wang, Xiangmin Hu, Zhihui Xu, Tao Chen</dc:creator>
    </item>
    <item>
      <title>A Concept-Value Network as a Brain Model</title>
      <link>https://arxiv.org/abs/1904.04579</link>
      <description>arXiv:1904.04579v3 Announce Type: replace-cross 
Abstract: This paper suggests a statistical framework for describing the relations between the physical and conceptual entities of a brain-like model. Features and concept instances are put into context, where the paper suggests that features may be the electrical wiring, although chemical connections are also possible. With this idea, the actual length of the connection is important, because it is related to firing rates and neuron synchronization, but the signal type is less important. The paper then suggests that concepts are neuron groups that link feature sets and concept instances are determined by chemical signals from those groups. Therefore, features become the static horizontal framework of the neural system and concepts are vertically interconnected combinations of these. This would also suggest that features can be distributed entities and not concentrated to a single area.</description>
      <guid isPermaLink="false">oai:arXiv.org:1904.04579v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kieran Greer</dc:creator>
    </item>
    <item>
      <title>Semi-supervised Multimodal Representation Learning through a Global Workspace</title>
      <link>https://arxiv.org/abs/2306.15711</link>
      <description>arXiv:2306.15711v2 Announce Type: replace-cross 
Abstract: Recent deep learning models can efficiently combine inputs from different modalities (e.g., images and text) and learn to align their latent representations, or to translate signals from one domain to another (as in image captioning, or text-to-image generation). However, current approaches mainly rely on brute-force supervised training over large multimodal datasets. In contrast, humans (and other animals) can learn useful multimodal representations from only sparse experience with matched cross-modal data. Here we evaluate the capabilities of a neural network architecture inspired by the cognitive notion of a "Global Workspace": a shared representation for two (or more) input modalities. Each modality is processed by a specialized system (pretrained on unimodal data, and subsequently frozen). The corresponding latent representations are then encoded to and decoded from a single shared workspace. Importantly, this architecture is amenable to self-supervised training via cycle-consistency: encoding-decoding sequences should approximate the identity function. For various pairings of vision-language modalities and across two datasets of varying complexity, we show that such an architecture can be trained to align and translate between two modalities with very little need for matched data (from 4 to 7 times less than a fully supervised approach). The global workspace representation can be used advantageously for downstream classification tasks and for robust transfer learning. Ablation studies reveal that both the shared workspace and the self-supervised cycle-consistency training are critical to the system's performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15711v2</guid>
      <category>cs.AI</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Devillers, L\'eopold Mayti\'e, Rufin VanRullen</dc:creator>
    </item>
    <item>
      <title>A brief tutorial on information theory</title>
      <link>https://arxiv.org/abs/2402.16556</link>
      <description>arXiv:2402.16556v2 Announce Type: replace-cross 
Abstract: At the 2023 Les Houches Summer School on Theoretical Biological Physics, several students asked for some background on information theory, and so we added a tutorial to the scheduled lectures. This is largely a transcript of that tutorial, lightly edited. It covers basic definitions and context rather than detailed calculations. We hope to have maintained the informality of the presentation, including exchanges with the students, while still being useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16556v2</guid>
      <category>physics.bio-ph</category>
      <category>q-bio.MN</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tarek Tohme, William Bialek</dc:creator>
    </item>
  </channel>
</rss>
