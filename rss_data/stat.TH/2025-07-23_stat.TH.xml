<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 24 Jul 2025 04:03:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Weighted Likelihood Approach Based on Statistical Data Depths</title>
      <link>https://arxiv.org/abs/2507.16998</link>
      <description>arXiv:2507.16998v1 Announce Type: new 
Abstract: We propose a general approach to construct weighted likelihood estimating equations with the aim of obtaining robust parameter estimates. We modify the standard likelihood equations by incorporating a weight that reflects the statistical depth of each data point relative to the model, as opposed to the sample. An observation is considered regular when the corresponding difference of these two depths is close to zero. When this difference is large the observation score contribution is downweighted. We study the asymptotic properties of the proposed estimator, including consistency and asymptotic normality, for a broad class of weight functions. In particular, we establish asymptotic normality under the standard regularity conditions typically assumed for the maximum likelihood estimator (MLE). Our weighted likelihood estimator achieves the same asymptotic efficiency as the MLE in the absence of contamination, while maintaining a high degree of robustness in contaminated settings. In stark contrast to the traditional minimum divergence/disparity estimators, our results hold even if the dimension of the data diverges with the sample size, without requiring additional assumptions on the existence or smoothness of the underlying densities. We also derive the finite sample breakdown point of our estimator for both location and scatter matrix in the elliptically symmetric model. Detailed results and examples are presented for robust parameter estimation in the multivariate normal model. Robustness is further illustrated using two real data sets and a Monte Carlo simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16998v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudio Agostinelli, Ayanendranath Basu, Giulia Bertagnolli, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Approximation Techniques for the Reconstruction of the Probability Measure and the Coupling Parameters in a Curie-Weiss Model for Large Populations</title>
      <link>https://arxiv.org/abs/2507.17073</link>
      <description>arXiv:2507.17073v1 Announce Type: new 
Abstract: The Curie-Weiss model, originally used to study phase transitions in statistical mechanics, has been adapted to model phenomena in social sciences where many agents interact with each other. Reconstructing the probability measure of a Curie-Weiss model via the maximum likelihood method runs into the problem of computing the partition function which scales exponentially with the population. We study the estimation of the coupling parameters of a multi-group Curie-Weiss model using large population asymptotic approximations for the relevant moments of the probability distribution in the case that there are no interactions between groups. As a result, we obtain an estimator which can be calculated at a low and constant computational cost for any size of the population. The estimator is consistent (under the added assumption that the population is large enough), asymptotically normal, and satisfies large deviation principles. The estimator is potentially useful in political science, sociology, automated voting, and in any application where the degree of social cohesion in a population has to be identified. The Curie-Weiss model's coupling parameters provide a natural measure of social cohesion. We discuss the problem of estimating the optimal weights in two-tier voting systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17073v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miguel Ballesteros, Ivan Naumkin, Gabor Toth</dc:creator>
    </item>
    <item>
      <title>The Joint Asymptotic Distribution of Entropy and Complexity</title>
      <link>https://arxiv.org/abs/2507.17625</link>
      <description>arXiv:2507.17625v1 Announce Type: new 
Abstract: We derive the asymptotic distribution of ordinal-pattern frequencies under weak dependence conditions and investigate the long-run covariance matrix not only analytically for moving-average, Gaussian, and the novel generalized coin-tossing processes, but also approximately by a simulation-based approach. Then, we deduce the asymptotic distribution of the entropy-complexity pair, which emerged as a popular tool for summarizing the time-series dynamics. Here, we make the necessary distinction between a uniform and a non-uniform ordinal pattern distribution and, thus, obtain two different limit theorems. On this basis, we consider a test for serial dependence and check its finite-sample performance. Moreover, we use our asymptotic results to approximate the estimation uncertainty of entropy-complexity pairs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17625v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Angelika Silbernagel, Christian Wei{\ss}</dc:creator>
    </item>
    <item>
      <title>Frequentist Asymptotics of Variational Laplace</title>
      <link>https://arxiv.org/abs/2507.17697</link>
      <description>arXiv:2507.17697v1 Announce Type: new 
Abstract: Variational inference is a general framework to obtain approximations to the posterior distribution in a Bayesian context. In essence, variational inference entails an optimization over a given family of probability distributions to choose the member of this family best approximating the posterior. Variational Laplace, an iterative update scheme motivated by this objective, is widely used in different contexts in the cognitive neuroscience community. However, until now, the theoretical properties of this scheme have not been systematically investigated. Here, we study variational Laplace in the light of frequentist asymptotic statistics. Asymptotical frequentist theory enables one to judge the quality of point estimates by their limit behaviour. We apply this framework to find that point estimates generated by variational Laplace enjoy the desirable properties of asymptotic consistency and efficiency in two toy examples. Furthermore, we derive conditions that are sufficient to establish these properties in a general setting. Besides of point estimates, we also study the frequentist convergence of distributions in the sense of total variation distance, which may be useful to relate variational Laplace both to recent findings regarding variational inference as well as to classical frequentist considerations on the Bayesian posterior. Finally, to illustrate the validity of our theoretical considerations, we conduct simulation experiments in our study examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17697v1</guid>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Janis Keck</dc:creator>
    </item>
    <item>
      <title>Fundamental limits of distributed covariance matrix estimation via a conditional strong data processing inequality</title>
      <link>https://arxiv.org/abs/2507.16953</link>
      <description>arXiv:2507.16953v1 Announce Type: cross 
Abstract: Estimating high-dimensional covariance matrices is a key task across many fields. This paper explores the theoretical limits of distributed covariance estimation in a feature-split setting, where communication between agents is constrained. Specifically, we study a scenario in which multiple agents each observe different components of i.i.d. samples drawn from a sub-Gaussian random vector. A central server seeks to estimate the complete covariance matrix using a limited number of bits communicated by each agent. We obtain a nearly tight minimax lower bound for covariance matrix estimation under operator norm and Frobenius norm. Our main technical tool is a novel generalization of the strong data processing inequality (SDPI), termed the Conditional Strong Data Processing Inequality (C-SDPI) coefficient, introduced in this work. The C-SDPI coefficient shares key properties such as tensorization with the conventional SDPI. Crucially, it quantifies the average contraction in a state-dependent channel and can be significantly lower than the worst-case SDPI coefficient over the state input.
  Utilizing the doubling trick of Geng-Nair and an operator Jensen inequality, we compute this coefficient for Gaussian mixture channels. We then employ it to establish minimax lower bounds on estimation error, capturing the trade-offs among sample size, communication cost, and data dimensionality. Building on this, we present a nearly optimal estimation protocol whose sample and communication requirements match the lower bounds up to logarithmic factors. Unlike much of the existing literature, our framework does not assume infinite samples or Gaussian distributions, making it broadly applicable. Finally, we extend our analysis to interactive protocols, showing interaction can significantly reduce communication requirements compared to non-interactive schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16953v1</guid>
      <category>stat.ML</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Reza Rahmani, Mohammad Hossein Yassaee, Mohammad Reza Aref</dc:creator>
    </item>
    <item>
      <title>Testing Against Tree Ordered Alternatives in One-way ANOVA</title>
      <link>https://arxiv.org/abs/2507.17229</link>
      <description>arXiv:2507.17229v1 Announce Type: cross 
Abstract: The likelihood ratio test against a tree ordered alternative in one-way heteroscedastic ANOVA is considered for the first time. Bootstrap is used to implement this and two multiple comparisons based tests and shown to have very good size and power performance.
  In this paper, the problem of testing the homogeneity of mean effects against the tree ordered alternative is considered in the heteroscedastic one-way ANOVA model. The likelihood ratio test and two multiple comparison-based tests - named Max-D and Min-D are proposed and implemented using the parametric bootstrap method. An extensive simulation study shows that these tests effectively control type-I error rates for various choices of sample sizes and error variances. Further, the likelihood ratio and Max-D tests achieve very good powers in all cases. The test Min-D is seen to perform better than the other two for some specific configurations of parameters. The robustness of these tests is investigated by implementing some non-normal distributions, viz., skew-normal, Laplace, exponential, mixture-normal, and t distributions. `R' packages are developed and shared on "Github" for the ease of users. The proposed tests are illustrated on a dataset of patients undergoing psychological treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17229v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subha Halder, Anjana Mondal, Somesh Kumar</dc:creator>
    </item>
    <item>
      <title>A principled approach for comparing Variable Importance</title>
      <link>https://arxiv.org/abs/2507.17306</link>
      <description>arXiv:2507.17306v1 Announce Type: cross 
Abstract: Variable importance measures (VIMs) aim to quantify the contribution of each input covariate to the predictability of a given output. With the growing interest in explainable AI, numerous VIMs have been proposed, many of which are heuristic in nature. This is often justified by the inherent subjectivity of the notion of importance. This raises important questions regarding usage: What makes a good VIM? How can we compare different VIMs?
  In this paper, we address these questions by: (1) proposing an axiomatic framework that bridges the gap between variable importance and variable selection. This framework formalizes the intuitive principle that features providing no additional information should not be assigned importance. It helps avoid false positives due to spurious correlations, which can arise with popular methods such as Shapley values; and (2) introducing a general pipeline for constructing VIMs, which clarifies the objective of various VIMs and thus facilitates meaningful comparisons. This approach is natural in statistics, but the literature has diverged from it.
  Finally, we provide an extensive set of examples to guide practitioners in selecting and estimating appropriate indices aligned with their specific goals and data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17306v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Angel Reyero-Lobo, Pierre Neuvial, Bertrand Thirion</dc:creator>
    </item>
    <item>
      <title>Polynomial time guarantees for sampling based posterior inference in high-dimensional generalised linear models</title>
      <link>https://arxiv.org/abs/2208.13296</link>
      <description>arXiv:2208.13296v3 Announce Type: replace 
Abstract: The problem of computing posterior functionals in general high-dimensional statistical models with possibly non-log-concave likelihood functions is considered. Based on the proof strategy of Nickl and Wang (2022), but using only local likelihood conditions and without relying on M-estimation theory, nonasymptotic statistical and computational guarantees are provided for a gradient based MCMC algorithm. Given a suitable initialiser, these guarantees scale polynomially in key algorithmic quantities. The abstract results are applied to several concrete statistical models, including density estimation, nonparametric regression with generalised linear models and a canonical statistical non-linear inverse problem from PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.13296v3</guid>
      <category>math.ST</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Randolf Altmeyer</dc:creator>
    </item>
    <item>
      <title>Can we have it all? Non-asymptotically valid and asymptotically exact confidence intervals for expectations and linear regressions</title>
      <link>https://arxiv.org/abs/2507.16776</link>
      <description>arXiv:2507.16776v2 Announce Type: replace 
Abstract: We contribute to bridging the gap between large- and finite-sample inference by studying confidence sets (CSs) that are both non-asymptotically valid and asymptotically exact uniformly (NAVAE) over semi-parametric statistical models. NAVAE CSs are not easily obtained; for instance, we show they do not exist over the set of Bernoulli distributions. We first derive a generic sufficient condition: NAVAE CSs are available as soon as uniform asymptotically exact CSs are. Second, building on that connection, we construct closed-form NAVAE confidence intervals (CIs) in two standard settings -- scalar expectations and linear combinations of OLS coefficients -- under moment conditions only. For expectations, our sole requirement is a bounded kurtosis. In the OLS case, our moment constraints accommodate heteroskedasticity and weak exogeneity of the regressors. Under those conditions, we enlarge the Central Limit Theorem-based CIs, which are asymptotically exact, to ensure non-asymptotic guarantees. Those modifications vanish asymptotically so that our CIs coincide with the classical ones in the limit. We illustrate the potential and limitations of our approach through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16776v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexis Derumigny, Lucas Girard, Yannick Guyonvarch</dc:creator>
    </item>
    <item>
      <title>Rank-adaptive covariance testing with applications to genomics and neuroimaging</title>
      <link>https://arxiv.org/abs/2309.10284</link>
      <description>arXiv:2309.10284v4 Announce Type: replace-cross 
Abstract: In biomedical studies, testing for differences in covariance offers scientific insights beyond mean differences, especially when differences are driven by complex joint behavior between features. However, when differences in joint behavior are weakly dispersed across many dimensions and arise from differences in low-rank structures within the data, as is often the case in genomics and neuroimaging, existing two-sample covariance testing methods may suffer from power loss. The Ky-Fan(k) norm, defined by the sum of the top Ky-Fan(k) singular values, is a simple and intuitive matrix norm able to capture signals caused by differences in low-rank structures between matrices, but its statistical properties in hypothesis testing have not been studied well. In this paper, we investigate the behavior of the Ky-Fan(k) norm in two-sample covariance testing. Ultimately, we propose a novel methodology, Rank-Adaptive Covariance Testing (RACT), which is able to leverage differences in low-rank structures found in the covariance matrices of two groups in order to maximize power. RACT uses permutation for statistical inference, ensuring an exact Type I error control. We validate RACT in simulation studies and evaluate its performance when testing for differences in gene expression networks between two types of lung cancer, as well as testing for covariance heterogeneity in diffusion tensor imaging (DTI) data taken on two different scanner types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.10284v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Veitch, Yinqiu He, Jun Young Park</dc:creator>
    </item>
    <item>
      <title>A Parameter-Efficient Quantum Anomaly Detection Method on a Superconducting Quantum Processor</title>
      <link>https://arxiv.org/abs/2412.16867</link>
      <description>arXiv:2412.16867v4 Announce Type: replace-cross 
Abstract: Quantum machine learning has gained attention for its potential to address computational challenges. However, whether those algorithms can effectively solve practical problems and outperform their classical counterparts, especially on current quantum hardware, remains a critical question. In this work, we propose a novel quantum machine learning method, called Parameter-Efficient Quantum Anomaly Detection (PEQAD), for practical image anomaly detection, which aims to achieve both parameter efficiency and superior accuracy compared to classical models. Emulation results indicate that PEQAD demonstrates favourable recognition capabilities compared to classical baselines, achieving an average accuracy of over 90% on benchmarks with significantly fewer trainable parameters. Theoretical analysis confirms that PEQAD has a comparable expressivity to classical counterparts while requiring only a fraction of the parameters. Furthermore, we demonstrate the first implementation of a quantum anomaly detection method for general image datasets on a superconducting quantum processor. Specifically, we achieve an accuracy of over 80% with only 16 parameters on the device, providing initial evidence of PEQAD's practical viability in the noisy intermediate-scale quantum era and highlighting its significant reduction in parameter requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16867v4</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 24 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maida Wang, Jinyang Jiang, Peter V. Coveney</dc:creator>
    </item>
  </channel>
</rss>
