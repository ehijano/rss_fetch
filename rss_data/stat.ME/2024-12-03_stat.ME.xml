<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 02:52:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>New Axioms for Dependence Measurement and Powerful Tests</title>
      <link>https://arxiv.org/abs/2412.00066</link>
      <description>arXiv:2412.00066v1 Announce Type: new 
Abstract: We build a context-free, comprehensive, flexible, and sound footing for measuring the dependence of two variables based on three new axioms, updating Renyi's (1959) seven postulates. We illustrate the superior footing of axioms by Vinod's (2014) asymmetric matrix of generalized correlation coefficients R*. We list five limitations explaining the poorer footing of axiom-failing Hellinger correlation proposed in 2022. We also describe a new implementation of a one-sided test with Taraldsen's (2021) exact density. This paper provides a new table for more powerful one-sided tests using the exact Taraldsen density and includes a published example where using Taraldsen's method makes a practical difference. The code to implement all our proposals is in R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00066v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hrishikesh D Vinod</dc:creator>
    </item>
    <item>
      <title>Scalable computation of the maximum flow in large brain connectivity networks</title>
      <link>https://arxiv.org/abs/2412.00106</link>
      <description>arXiv:2412.00106v1 Announce Type: new 
Abstract: We are interested in computing an approximation of the maximum flow in large (brain) connectivity networks. The maximum flow in such networks is of interest in order to better understand the routing of information in the human brain. However, the runtime of $O(|V||E|^2)$ for the classic Edmonds-Karp algorithm renders computations of the maximum flow on networks with millions of vertices infeasible, where $V$ is the set of vertices and $E$ is the set of edges. In this contribution, we propose a new Monte Carlo algorithm which is capable of computing an approximation of the maximum flow in networks with millions of vertices via subsampling. Apart from giving a point estimate of the maximum flow, our algorithm also returns valid confidence bounds for the true maximum flow. Importantly, its runtime only scales as $O(B \cdot |\tilde{V}| |\tilde{E}|^2)$, where $B$ is the number of Monte Carlo samples, $\tilde{V}$ is the set of subsampled vertices, and $\tilde{E}$ is the edge set induced by $\tilde{V}$. Choosing $B \in O(|V|)$ and $|\tilde{V}| \in O(\sqrt{|V|})$ (implying $|\tilde{E}| \in O(|V|)$) yields an algorithm with runtime $O(|V|^{3.5})$ while still guaranteeing the usual "root-n" convergence of the confidence interval of the maximum flow estimate. We evaluate our proposed algorithm with respect to both accuracy and runtime on simulated graphs as well as graphs downloaded from the Brain Networks Data Repository (https://networkrepository.com).</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00106v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyun Qian, Georg Hahn</dc:creator>
    </item>
    <item>
      <title>A Doubly Robust Method to Counteract Outcome-Dependent Selection Bias in Multi-Cohort EHR Studies</title>
      <link>https://arxiv.org/abs/2412.00228</link>
      <description>arXiv:2412.00228v1 Announce Type: new 
Abstract: Selection bias can hinder accurate estimation of association parameters in binary disease risk models using non-probability samples like electronic health records (EHRs). The issue is compounded when participants are recruited from multiple clinics or centers with varying selection mechanisms that may depend on the disease or outcome of interest. Traditional inverse-probability-weighted (IPW) methods, based on constructed parametric selection models, often struggle with misspecifications when selection mechanisms vary across cohorts. This paper introduces a new Joint Augmented Inverse Probability Weighted (JAIPW) method, which integrates individual-level data from multiple cohorts collected under potentially outcome-dependent selection mechanisms, with data from an external probability sample. JAIPW offers double robustness by incorporating a flexible auxiliary score model to address potential misspecifications in the selection models. We outline the asymptotic properties of the JAIPW estimator, and our simulations reveal that JAIPW achieves up to five times lower relative bias and three times lower root mean square error (RMSE) compared to the best performing joint IPW methods under scenarios with misspecified selection models. Applying JAIPW to the Michigan Genomics Initiative (MGI), a multi-clinic EHR-linked biobank, combined with external national probability samples, resulted in cancer-sex association estimates more closely aligned with national estimates. We also analyzed the association between cancer and polygenic risk scores (PRS) in MGI to illustrate a situation where the exposure is not available in the external probability sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00228v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ritoban Kundu, Xu Shi, Maxwell Salvatore, Lars G. Fritsche, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>Benchmarking covariates balancing methods, a simulation study</title>
      <link>https://arxiv.org/abs/2412.00280</link>
      <description>arXiv:2412.00280v1 Announce Type: new 
Abstract: Causal inference in observational studies has advanced significantly since Rosenbaum and Rubin introduced propensity score methods. Inverse probability of treatment weighting (IPTW) is widely used to handle confounding bias. However, newer methods, such as energy balancing (EB), kernel optimal matching (KOM), and covariate balancing propensity score by tailored loss function (TLF), offer model-free or non-parametric alternatives. Despite these developments, guidance remains limited in selecting the most suitable method for treatment effect estimation in practical applications. This study compares IPTW with EB, KOM, and TLF, focusing on their ability to estimate treatment effects since this is the primary objective in many applications. Monte Carlo simulations are used to assess the ability of these balancing methods combined with different estimators to evaluate average treatment effect. We compare these methods across a range of scenarios varying sample size, level of confusion, and proportion of treated. In our simulation, we observe no significant advantages in using EB, KOM, or TLF methods over IPTW. Moreover, these recent methods make obtaining confidence intervals with nominal coverage difficult. We also compare the methods on the PROBITsim dataset and get results similar to those of our simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00280v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Etienne Peyrot, Rapha\"el Porcher, Francois Petit</dc:creator>
    </item>
    <item>
      <title>Sparse Bayesian Factor Models with Mass-Nonlocal Factor Scores</title>
      <link>https://arxiv.org/abs/2412.00304</link>
      <description>arXiv:2412.00304v1 Announce Type: new 
Abstract: Bayesian factor models are widely used for dimensionality reduction and pattern discovery in high-dimensional datasets across diverse fields. These models typically focus on imposing priors on factor loading to induce sparsity and improve interpretability. However, factor score, which plays a critical role in individual-level associations with factors, has received less attention and is assumed to have standard multivariate normal distribution. This oversimplification fails to capture the heterogeneity observed in real-world applications. We propose the Sparse Bayesian Factor Model with Mass-Nonlocal Factor Scores (BFMAN), a novel framework that addresses these limitations by introducing a mass-nonlocal prior for factor scores. This prior provides a more flexible posterior distribution that captures individual heterogeneity while assigning positive probability to zero value. The zeros entries in the score matrix, characterize the sparsity, offering a robust and novel approach for determining the optimal number of factors. Model parameters are estimated using a fast and efficient Gibbs sampler. Extensive simulations demonstrate that BFMAN outperforms standard Bayesian sparse factor models in factor recovery, sparsity detection, and score estimation. We apply BFMAN to the Hispanic Community Health Study/Study of Latinos and identify dietary patterns and their associations with cardiovascular outcomes, showcasing the model's ability to uncover meaningful insights in diet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00304v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Yingjie Huang, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>Disentangling The Effects of Air Pollution on Social Mobility: A Bayesian Principal Stratification Approach</title>
      <link>https://arxiv.org/abs/2412.00311</link>
      <description>arXiv:2412.00311v1 Announce Type: new 
Abstract: Principal stratification provides a robust framework for causal inference, enabling the investigation of the causal link between air pollution exposure and social mobility, mediated by the education level. Studying the causal mechanisms through which air pollution affects social mobility is crucial to highlight the role of education as a mediator, and offering evidence that can inform policies aimed at reducing both environmental and educational inequalities for more equitable social outcomes. In this paper, we introduce a novel Bayesian nonparametric model for principal stratification, leveraging the dependent Dirichlet process to flexibly model the distribution of potential outcomes. By incorporating confounders and potential outcomes for the post-treatment variable in the Bayesian mixture model for the final outcome, our approach improves the accuracy of missing data imputation and allows for the characterization of treatment effects. We assess the performance of our method through a simulation study and demonstrate its application in evaluating the principal causal effects of air pollution on social mobility in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00311v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Paolo Dalla Torre, Sonia Petrone, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Risk models from tree-structured Markov random fields following multivariate Poisson distributions</title>
      <link>https://arxiv.org/abs/2412.00607</link>
      <description>arXiv:2412.00607v1 Announce Type: new 
Abstract: We propose risk models for a portfolio of risks, each following a compound Poisson distribution, with dependencies introduced through a family of tree-based Markov random fields with Poisson marginal distributions inspired in C\^ot\'e et al. (2024b, arXiv:2408.13649). The diversity of tree topologies allows for the construction of risk models under several dependence schemes. We study the distribution of the random vector of risks and of the aggregate claim amount of the portfolio. We perform two risk management tasks: the assessment of the global risk of the portfolio and its allocation to each component. Numerical examples illustrate the findings and the efficiency of the computation methods developed throughout. We also show that the discussed family of Markov random fields is a subfamily of the multivariate Poisson distribution constructed through common shocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00607v1</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\'el\`ene Cossette, Benjamin C\^ot\'e, Alexandre Dubeau, Etienne Marceau</dc:creator>
    </item>
    <item>
      <title>Bayesian feature selection in joint models with application to a cardiovascular disease cohort study</title>
      <link>https://arxiv.org/abs/2412.00885</link>
      <description>arXiv:2412.00885v1 Announce Type: new 
Abstract: Cardiovascular disease (CVD) cohorts collect data longitudinally to study the association between CVD risk factors and event times. An important area of scientific research is to better understand what features of CVD risk factor trajectories are associated with the disease. We develop methods for feature selection in joint models where feature selection is viewed as a bi-level variable selection problem with multiple features nested within multiple longitudinal risk factors. We modify a previously proposed Bayesian sparse group selection (BSGS) prior, which has not been implemented in joint models until now, to better represent prior beliefs when selecting features both at the group level (longitudinal risk factor) and within group (features of a longitudinal risk factor). One of the advantages of our method over the BSGS method is the ability to account for correlation among the features within a risk factor. As a result, it selects important features similarly, but excludes the unimportant features within risk factors more efficiently than BSGS. We evaluate our prior via simulations and apply our method to data from the Atherosclerosis Risk in Communities (ARIC) study, a population-based, prospective cohort study consisting of over 15,000 men and women aged 45-64, measured at baseline and at six additional times. We evaluate which CVD risk factors and which characteristics of their trajectories (features) are associated with death from CVD. We find that systolic and diastolic blood pressure, glucose, and total cholesterol are important risk factors with different important features associated with CVD death in both men and women.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00885v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mirajul Islam, Michael J. Daniels, Zeynab Aghabazaz, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>A sensitivity analysis approach to principal stratification with a continuous longitudinal intermediate outcome: Applications to a cohort stepped wedge trial</title>
      <link>https://arxiv.org/abs/2412.00926</link>
      <description>arXiv:2412.00926v1 Announce Type: new 
Abstract: Causal inference in the presence of intermediate variables is a challenging problem in many applications. Principal stratification (PS) provides a framework to estimate principal causal effects (PCE) in such settings. However, existing PS methods primarily focus on settings with binary intermediate variables. We propose a novel approach to estimate PCE with continuous intermediate variables in the context of stepped wedge cluster randomized trials (SW-CRTs). Our method leverages the time-varying treatment assignment in SW-CRTs to calibrate sensitivity parameters and identify the PCE under realistic assumptions. We demonstrate the application of our approach using data from a cohort SW-CRT evaluating the effect of a crowdsourcing intervention on HIV testing uptake among men who have sex with men in China, with social norms as a continuous intermediate variable. The proposed methodology expands the scope of PS to accommodate continuous variables and provides a practical tool for causal inference in SW-CRTs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00926v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Yang, Michael J. Daniels, Fan Li</dc:creator>
    </item>
    <item>
      <title>Generalized spatial autoregressive model</title>
      <link>https://arxiv.org/abs/2412.00945</link>
      <description>arXiv:2412.00945v1 Announce Type: new 
Abstract: This paper presents the generalized spatial autoregression (GSAR) model, a significant advance in spatial econometrics for non-normal response variables belonging to the exponential family. The GSAR model extends the logistic SAR, probit SAR, and Poisson SAR approaches by offering greater flexibility in modeling spatial dependencies while ensuring computational feasibility. Fundamentally, theoretical results are established on the convergence, efficiency, and consistency of the estimates obtained by the model. In addition, it improves the statistical properties of existing methods and extends them to new distributions. Simulation samples show the theoretical results and allow a visual comparison with existing methods. An empirical application is made to Republican voting patterns in the United States. The GSAR model outperforms standard spatial models by capturing nuanced spatial autocorrelation and accommodating regional heterogeneity, leading to more robust inferences. These findings underline the potential of the GSAR model as an analytical tool for researchers working with categorical or count data or skewed distributions with spatial dependence in diverse domains, such as political science, epidemiology, and market research. In addition, the R codes for estimating the model are provided, which allows its adaptability in these scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00945v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>N. A. Cruz, J. D. Toloza-Delgado, O. O. Melo</dc:creator>
    </item>
    <item>
      <title>Multiple Testing in Generalized Universal Inference</title>
      <link>https://arxiv.org/abs/2412.01008</link>
      <description>arXiv:2412.01008v1 Announce Type: new 
Abstract: Compared to p-values, e-values provably guarantee safe, valid inference. If the goal is to test multiple hypotheses simultaneously, one can construct e-values for each individual test and then use the recently developed e-BH procedure to properly correct for multiplicity. Standard e-value constructions, however, require distributional assumptions that may not be justifiable. This paper demonstrates that the generalized universal inference framework can be used along with the e-BH procedure to control frequentist error rates in multiple testing when the quantities of interest are minimizers of risk functions, thereby avoiding the need for distributional assumptions. We demonstrate the validity and power of this approach via a simulation study, testing the significance of a predictor in quantile regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01008v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Dey, Ryan Martin, Jonathan P. Williams</dc:creator>
    </item>
    <item>
      <title>Stochastic Search Variable Selection for Bayesian Generalized Linear Mixed Effect Models</title>
      <link>https://arxiv.org/abs/2412.01084</link>
      <description>arXiv:2412.01084v1 Announce Type: new 
Abstract: Variable selection remains a difficult problem, especially for generalized linear mixed models (GLMMs). While some frequentist approaches to simultaneously select joint fixed and random effects exist, primarily through the use of penalization, existing approaches for Bayesian GLMMs exist only for special cases, like that of logistic regression. In this work, we apply the Stochastic Search Variable Selection (SSVS) approach for the joint selection of fixed and random effects proposed in Yang et al. (2020) for linear mixed models to Bayesian GLMMs. We show that while computational issues remain, SSVS serves as a feasible and effective approach to jointly select fixed and random effects. We demonstrate the effectiveness of the proposed methodology to both simulated and real data. Furthermore, we study the role hyperparameters play in the model selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01084v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ding, Ian Laga</dc:creator>
    </item>
    <item>
      <title>The Deep Latent Position Block Model For The Block Clustering And Latent Representation Of Networks</title>
      <link>https://arxiv.org/abs/2412.01302</link>
      <description>arXiv:2412.01302v1 Announce Type: new 
Abstract: The increased quantity of data has led to a soaring use of networks to model relationships between different objects, represented as nodes. Since the number of nodes can be particularly large, the network information must be summarised through node clustering methods. In order to make the results interpretable, a relevant visualisation of the network is also required. To tackle both issues, we propose a new methodology called deep latent position block model (Deep LPBM) which simultaneously provides a network visualisation coherent with block modelling, allowing a clustering more general than community detection methods, as well as a continuous representation of nodes in a latent space given by partial membership vectors. Our methodology is based on a variational autoencoder strategy, relying on a graph convolutional network, with a specifically designed decoder. The inference is done using both variational and stochastic approximations. In order to efficiently select the number of clusters, we provide a comparison of three model selection criteria. An extensive benchmark as well as an evaluation of the partial memberships are provided. We conclude with an analysis of a French political blogosphere network and a comparison with another methodology to illustrate the insights provided by Deep LPBM results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01302v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R\'emi Boutin (SU, LPSM), Pierre Latouche (UCA, LMBP), Charles Bouveyron (LJAD, MAASAI, CRISAM)</dc:creator>
    </item>
    <item>
      <title>Nonparametric directional variogram estimation in the presence of outlier blocks</title>
      <link>https://arxiv.org/abs/2412.01464</link>
      <description>arXiv:2412.01464v1 Announce Type: new 
Abstract: This paper proposes robust estimators of the variogram, a statistical tool that is commonly used in geostatistics to capture the spatial dependence structure of data. The new estimators are based on the highly robust minimum covariance determinant estimator and estimate the directional variogram for several lags jointly. Simulations and breakdown considerations confirm the good robustness properties of the new estimators. While Genton's estimator based on the robust estimation of the variance of pairwise sums and differences performs well in case of isolated outliers, the new estimators based on robust estimation of multivariate variance and covariance matrices perform superior to the established alternatives in the presence of outlier blocks in the data. The methods are illustrated by an application to satellite data, where outlier blocks may occur because of e.g. clouds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01464v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jana Gierse, Roland Fried</dc:creator>
    </item>
    <item>
      <title>Probabilistic Predictions of Option Prices Using Multiple Sources of Data</title>
      <link>https://arxiv.org/abs/2412.00658</link>
      <description>arXiv:2412.00658v1 Announce Type: cross 
Abstract: A new modular approximate Bayesian inferential framework is proposed that enables fast calculation of probabilistic predictions of future option prices. We exploit multiple information sources, including daily spot returns, high-frequency spot data and option prices. A benefit of this modular Bayesian approach is that it allows us to work with the theoretical option pricing model, without needing to specify an arbitrary statistical model that links the theoretical prices to their observed counterparts. We show that our approach produces accurate probabilistic predictions of option prices in realistic scenarios and, despite not explicitly modelling pricing errors, the method is shown to be robust to their presence. Predictive accuracy based on the Heston stochastic volatility model, with predictions produced via rapid real-time updates, is illustrated empirically for short-maturity options.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00658v1</guid>
      <category>q-fin.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Worapree Maneesoonthorn, David T. Frazier, Gael M. Martin</dc:creator>
    </item>
    <item>
      <title>The ecological forecast horizon revisited: Potential, actual and relative system predictability</title>
      <link>https://arxiv.org/abs/2412.00753</link>
      <description>arXiv:2412.00753v1 Announce Type: cross 
Abstract: Ecological forecasts are model-based statements about currently unknown ecosystem states in time or space. For a model forecast to be useful to inform decision-makers, model validation and verification determine adequateness. The measure of forecast goodness that can be translated into a limit up to which a forecast is acceptable is known as the `forecast horizon'. While verification of meteorological models follows strict criteria with established metrics and forecast horizons, assessments of ecological forecasting models still remain experiment-specific and forecast horizons are rarely reported. As such, users of ecological forecasts remain uninformed of how far into the future statements can be trusted. In this work, we synthesise existing approaches, define empirical forecast horizons in a unified framework for assessing ecological predictability and offer recipes on their computation. We distinguish upper and lower boundary estimates of predictability limits, reflecting the model's potential and actual forecast horizon, and show how a benchmark model can help determine its relative forecast horizon. The approaches are demonstrated with four case studies from population, ecosystem, and earth system research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00753v1</guid>
      <category>stat.AP</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marieke Wesselkamp, Jakob Albrecht, Ewan Pinnington, William J. Castillo, Florian Pappenberger, Carsten F. Dormann</dc:creator>
    </item>
    <item>
      <title>Practical Performative Policy Learning with Strategic Agents</title>
      <link>https://arxiv.org/abs/2412.01344</link>
      <description>arXiv:2412.01344v1 Announce Type: cross 
Abstract: This paper studies the performative policy learning problem, where agents adjust their features in response to a released policy to improve their potential outcomes, inducing an endogenous distribution shift. There has been growing interest in training machine learning models in strategic environments, including strategic classification and performative prediction. However, existing approaches often rely on restrictive parametric assumptions: micro-level utility models in strategic classification and macro-level data distribution maps in performative prediction, severely limiting scalability and generalizability. We approach this problem as a complex causal inference task, relaxing parametric assumptions on both micro-level agent behavior and macro-level data distribution. Leveraging bounded rationality, we uncover a practical low-dimensional structure in distribution shifts and construct an effective mediator in the causal path from the deployed model to the shifted data. We then propose a gradient-based policy optimization algorithm with a differentiable classifier as a substitute for the high-dimensional distribution map. Our algorithm efficiently utilizes batch feedback and limited manipulation patterns. Our approach achieves high sample efficiency compared to methods reliant on bandit feedback or zero-order optimization. We also provide theoretical guarantees for algorithmic convergence. Extensive and challenging experiments on high-dimensional settings demonstrate our method's practical efficacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01344v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Ying Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>Navigating Challenges in Spatio-temporal Modelling of Antarctic Krill Abundance: Addressing Zero-inflated Data and Misaligned Covariates</title>
      <link>https://arxiv.org/abs/2412.01399</link>
      <description>arXiv:2412.01399v1 Announce Type: cross 
Abstract: Antarctic krill (Euphausia superba) are among the most abundant species on our planet and serve as a vital food source for many marine predators in the Southern Ocean. In this paper, we utilise statistical spatio-temporal methods to combine data from various sources and resolutions, aiming to accurately model krill abundance. Our focus lies in fitting the model to a dataset comprising acoustic measurements of krill biomass. To achieve this, we integrate climate covariates obtained from satellite imagery and from drifting surface buoys (also known as drifters). Additionally, we use sparsely collected krill biomass data obtained from net fishing efforts (KRILLBASE) for validation. However, integrating these multiple heterogeneous data sources presents significant modelling challenges, including spatio-temporal misalignment and inflated zeros in the observed data. To address these challenges, we fit a Hurdle-Gamma model to jointly describe the occurrence of zeros and the krill biomass for the non-zero observations, while also accounting for misaligned and heterogeneous data sources, including drifters. Therefore, our work presents a comprehensive framework for analysing and predicting krill abundance in the Southern Ocean, leveraging information from various sources and formats. This is crucial due to the impact of krill fishing, as understanding their distribution is essential for informed management decisions and fishing regulations aimed at protecting the species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01399v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'e Victor Ribeiro Amaral, Adam M. Sykulski, Emma Cavan, Sophie Fielding</dc:creator>
    </item>
    <item>
      <title>A Dimension-Agnostic Bootstrap Anderson-Rubin Test For Instrumental Variable Regressions</title>
      <link>https://arxiv.org/abs/2412.01603</link>
      <description>arXiv:2412.01603v1 Announce Type: cross 
Abstract: Weak-identification-robust Anderson-Rubin (AR) tests for instrumental variable (IV) regressions are typically developed separately depending on whether the number of IVs is treated as fixed or increasing with the sample size. These tests rely on distinct test statistics and critical values. To apply them, researchers are forced to take a stance on the asymptotic behavior of the number of IVs, which can be ambiguous when the number is moderate. In this paper, we propose a bootstrap-based, dimension-agnostic AR test. By deriving strong approximations for the test statistic and its bootstrap counterpart, we show that our new test has a correct asymptotic size regardless of whether the number of IVs is fixed or increasing -- allowing, but not requiring, the number of IVs to exceed the sample size. We also analyze the power properties of the proposed uniformly valid test under both fixed and increasing numbers of IVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01603v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dennis Lim, Wenjie Wang, Yichong Zhang</dc:creator>
    </item>
    <item>
      <title>A time-varying bivariate copula joint model for longitudinal and time-to-event data</title>
      <link>https://arxiv.org/abs/2209.04787</link>
      <description>arXiv:2209.04787v2 Announce Type: replace 
Abstract: A time-varying bivariate copula joint model, which models the repeatedly measured longitudinal outcome at each time point and the survival data jointly by both the random effects and time-varying bivariate copulas, is proposed in this paper. A regular joint model normally supposes there exist subject-specific latent random effects or classes shared by the longitudinal and time-to-event processes and the two processes are conditionally independent given these latent variables. Under this assumption, the joint likelihood of the two processes is straightforward to derive and their association, as well as heterogeneity among the population, are naturally introduced by the unobservable latent variables. However, because of the unobservable nature of these latent variables, the conditional independence assumption is difficult to verify. Therefore, besides the random effects, a time-varying bivariate copula is introduced to account for the extra time-dependent association between the two processes. The proposed model includes a regular joint model as a special case under some copulas. Simulation studies indicates the parameter estimators in the proposed model are robust against copula misspecification and it has superior performance in predicting survival probabilities compared to the regular joint model. A real data application on the Primary biliary cirrhosis (PBC) data is performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04787v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zili Zhang, Christiana Charalambous, Peter Foster</dc:creator>
    </item>
    <item>
      <title>Methods for generating and evaluating synthetic longitudinal patient data: a systematic review</title>
      <link>https://arxiv.org/abs/2309.12380</link>
      <description>arXiv:2309.12380v3 Announce Type: replace 
Abstract: The rapid growth in data availability has facilitated research and development, yet not all industries have benefited equally due to legal and privacy constraints. The healthcare sector faces significant challenges in utilizing patient data because of concerns about data security and confidentiality. To address this, various privacy-preserving methods, including synthetic data generation, have been proposed. Synthetic data replicate existing data as closely as possible, acting as a proxy for sensitive information. While patient data are often longitudinal, this aspect remains underrepresented in existing reviews of synthetic data generation in healthcare. This paper maps and describes methods for generating and evaluating synthetic longitudinal patient data in real-life settings through a systematic literature review, conducted following the PRISMA guidelines and incorporating data from five databases up to May 2024. Thirty-nine methods were identified, with four addressing all challenges of longitudinal data generation, though none included privacy-preserving mechanisms. Resemblance was evaluated in most studies, utility in the majority, and privacy in just over half. Only a small fraction of studies assessed all three aspects. Our findings highlight the need for further research in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.12380v3</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katariina Perkonoja, Kari Auranen, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Causal Panel Analysis under Parallel Trends: Lessons from A Large Reanalysis Study</title>
      <link>https://arxiv.org/abs/2309.15983</link>
      <description>arXiv:2309.15983v4 Announce Type: replace 
Abstract: Two-way fixed effects (TWFE) models are widely used in political science to establish causality, but recent methodological discussions highlight their limitations under heterogeneous treatment effects (HTE) and violations of the parallel trends (PT) assumption. This growing literature has introduced new estimators and diagnostics, causing confusion among researchers about the reliability of existing results and best practices. To address these concerns, we replicated and reanalyzed 49 articles from leading journals using TWFE models for observational panel data with binary treatments. Using six HTE-robust estimators, diagnostic tests, and sensitivity analyses, we find: (i) HTE-robust estimators yield qualitatively similar but highly variable results; (ii) while a few studies show clear signs of PT violations, many lack evidence to support this assumption; and (iii) many studies are underpowered when accounting for HTE and potential PT violations. We emphasize the importance of strong research designs and rigorous validation of key identifying assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15983v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Albert Chiu, Xingchen Lan, Ziyi Liu, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Partially factorized variational inference for high-dimensional mixed models</title>
      <link>https://arxiv.org/abs/2312.13148</link>
      <description>arXiv:2312.13148v2 Announce Type: replace 
Abstract: While generalized linear mixed models are a fundamental tool in applied statistics, many specifications, such as those involving categorical factors with many levels or interaction terms, can be computationally challenging to estimate due to the need to compute or approximate high-dimensional integrals. Variational inference is a popular way to perform such computations, especially in the Bayesian context. However, naive use of such methods can provide unreliable uncertainty quantification. We show that this is indeed the case for mixed models, proving that standard mean-field variational inference dramatically underestimates posterior uncertainty in high-dimensions. We then show how appropriately relaxing the mean-field assumption leads to methods whose uncertainty quantification does not deteriorate in high-dimensions, and whose total computational cost scales linearly with the number of parameters and observations. Our theoretical and numerical results focus on mixed models with Gaussian or binomial likelihoods, and rely on connections to random graph theory to obtain sharp high-dimensional asymptotic analysis. We also provide generic results, which are of independent interest, relating the accuracy of variational inference to the convergence rate of the corresponding coordinate ascent algorithm that is used to find it. Our proposed methodology is implemented in the R package, see https://github.com/mgoplerud/vglmer . Numerical results with simulated and real data examples illustrate the favourable computation cost versus accuracy trade-off of our approach compared to various alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.13148v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomet/asae067</arxiv:DOI>
      <dc:creator>Max Goplerud, Omiros Papaspiliopoulos, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Statistics in Survey Sampling</title>
      <link>https://arxiv.org/abs/2401.07625</link>
      <description>arXiv:2401.07625v4 Announce Type: replace 
Abstract: Survey sampling theory and methods are introduced. Sampling designs and estimation methods are carefully discussed as a textbook for survey sampling. Topics includes Horvitz-Thompson estimation, simple random sampling, stratified sampling, cluster sampling, ratio estimation, regression estimation, variance estimation, two-phase sampling, and nonresponse adjustment methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07625v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jae Kwang Kim</dc:creator>
    </item>
    <item>
      <title>Ranking by Lifts: A Cost-Benefit Approach to Large-Scale A/B Tests</title>
      <link>https://arxiv.org/abs/2407.01036</link>
      <description>arXiv:2407.01036v2 Announce Type: replace 
Abstract: A/B testers that conduct large-scale tests often prioritize lifts as the main outcome metric and want to be able to control costs resulting from false rejections of the null. This work develops a decision-theoretic framework for maximizing profits subject to false discovery rate (FDR) control. We build an empirical Bayes solution for the problem via a greedy knapsack approach. We derive an oracle rule based on ranking the ratio of expected lifts and the cost of wrong rejections using the local false discovery rate (lfdr) statistic. Our oracle decision rule is valid and optimal for large-scale tests. Further, we establish asymptotic validity for the data-driven procedure and demonstrate finite-sample validity in experimental studies. We also demonstrate the merit of the proposed method over other FDR control methods. Finally, we discuss an application to data collected by experiments on the Optimizely platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01036v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pallavi Basu, Ron Berman</dc:creator>
    </item>
    <item>
      <title>Interpretability Indices and Soft Constraints for Factor Models</title>
      <link>https://arxiv.org/abs/2409.11525</link>
      <description>arXiv:2409.11525v4 Announce Type: replace 
Abstract: Factor analysis is a way to characterize the relationships between many observable variables in terms of a smaller number of unobservable random variables. However, the application of factor models and its success can be subjective or difficult to gauge, since the factor model is not identifiable. Thus, there is a need to operationalize a criterion that measures how meaningful or "interpretable" a factor model is. While there are already techniques that address interpretability, new indices and methods are proposed to measure interpretability. The proposed methods can directly incorporate both loadings and semantics, and are generalized to incorporate any "prior information". Moreover, the indices allow for complete or partial specification of relationships at a pairwise level. Two other main benefits of the proposed methods are that they do not require the estimation of factor scores, which avoids the factor score indeterminacy problem, and that no additional explanatory variables are necessary. The implementation of the proposed methods is written in Python 3 and is made available together with several helper functions through the package interpretablefa on the Python Package Index. The methods' application is demonstrated here using data on the Experiences in Close Relationships Scale, obtained from the Open-Source Psychometrics Project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11525v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Philip Tuazon, Gia Mizrane Abubo, Joemari Olea</dc:creator>
    </item>
    <item>
      <title>Interval Estimation of Coefficients in Penalized Regression Models of Insurance Data</title>
      <link>https://arxiv.org/abs/2410.01008</link>
      <description>arXiv:2410.01008v2 Announce Type: replace 
Abstract: The Tweedie exponential dispersion family is a popular choice among many to model insurance losses that consist of zero-inflated semicontinuous data. In such data, it is often important to obtain credibility (inference) of the most important features that describe the endogenous variables. Post-selection inference is the standard procedure in statistics to obtain confidence intervals of model parameters after performing a feature extraction procedure. For a linear model, the lasso estimate often has non-negligible estimation bias for large coefficients corresponding to exogenous variables. To have valid inference on those coefficients, it is necessary to correct the bias of the lasso estimate. Traditional statistical methods, such as hypothesis testing or standard confidence interval construction might lead to incorrect conclusions during post-selection, as they are generally too optimistic. Here we discuss a few methodologies for constructing confidence intervals of the coefficients after feature selection in the Generalized Linear Model (GLM) family with application to insurance data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01008v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alokesh Manna, Zijian Huang, Dipak K. Dey, Yuwen Gu</dc:creator>
    </item>
    <item>
      <title>Inference on Dynamic Spatial Autoregressive Models with Change Point Detection</title>
      <link>https://arxiv.org/abs/2411.18773</link>
      <description>arXiv:2411.18773v2 Announce Type: replace 
Abstract: We analyze a varying-coefficient dynamic spatial autoregressive model with spatial fixed effects. One salient feature of the model is the incorporation of multiple spatial weight matrices through their linear combinations with varying coefficients, which help solve the problem of choosing the most "correct" one for applied econometricians who often face the availability of multiple expert spatial weight matrices. We estimate and make inferences on the model coefficients and coefficients in basis expansions of the varying coefficients through penalized estimations, establishing the oracle properties of the estimators and the consistency of the overall estimated spatial weight matrix, which can be time-dependent. We further consider two applications of our model in change point detections in dynamic spatial autoregressive models, providing theoretical justifications in consistent change point locations estimation and practical implementations. Simulation experiments demonstrate the performance of our proposed methodology, and a real data analysis is also carried out.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18773v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Yudong Chen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Environment Invariant Linear Least Squares</title>
      <link>https://arxiv.org/abs/2303.03092</link>
      <description>arXiv:2303.03092v3 Announce Type: replace-cross 
Abstract: This paper considers a multi-environment linear regression model in which data from multiple experimental settings are collected. The joint distribution of the response variable and covariates may vary across different environments, yet the conditional expectations of $y$ given the unknown set of important variables are invariant. Such a statistical model is related to the problem of endogeneity, causal inference, and transfer learning. The motivation behind it is illustrated by how the goals of prediction and attribution are inherent in estimating the true parameter and the important variable set. We construct a novel environment invariant linear least squares (EILLS) objective function, a multi-environment version of linear least-squares regression that leverages the above conditional expectation invariance structure and heterogeneity among different environments to determine the true parameter. Our proposed method is applicable without any additional structural knowledge and can identify the true parameter under a near-minimal identification condition. We establish non-asymptotic $\ell_2$ error bounds on the estimation error for the EILLS estimator in the presence of spurious variables. Moreover, we further show that the $\ell_0$ penalized EILLS estimator can achieve variable selection consistency in high-dimensional regimes. These non-asymptotic results demonstrate the sample efficiency of the EILLS estimator and its capability to circumvent the curse of endogeneity in an algorithmic manner without any prior structural knowledge. To the best of our knowledge, this paper is the first to realize statistically efficient invariance learning in the general linear model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03092v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/24-AOS2435</arxiv:DOI>
      <arxiv:journal_reference>The Annals of Statistics 52(5) (2024) 2268-2292</arxiv:journal_reference>
      <dc:creator>Jianqing Fan, Cong Fang, Yihong Gu, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>On the existence of powerful p-values and e-values for composite hypotheses</title>
      <link>https://arxiv.org/abs/2305.16539</link>
      <description>arXiv:2305.16539v4 Announce Type: replace-cross 
Abstract: Given a composite null $ \mathcal P$ and composite alternative $ \mathcal Q$, when and how can we construct a p-value whose distribution is exactly uniform under the null, and stochastically smaller than uniform under the alternative? Similarly, when and how can we construct an e-value whose expectation exactly equals one under the null, but its expected logarithm under the alternative is positive? We answer these basic questions, and other related ones, when $ \mathcal P$ and $ \mathcal Q$ are convex polytopes (in the space of probability measures). We prove that such constructions are possible if and only if $ \mathcal Q$ does not intersect the span of $ \mathcal P$. If the p-value is allowed to be stochastically larger than uniform under $P\in \mathcal P$, and the e-value can have expectation at most one under $P\in \mathcal P$, then it is achievable whenever $ \mathcal P$ and $ \mathcal Q$ are disjoint. More generally, even when $ \mathcal P$ and $ \mathcal Q$ are not polytopes, we characterize the existence of a bounded nontrivial e-variable whose expectation exactly equals one under any $P \in \mathcal P$. The proofs utilize recently developed techniques in simultaneous optimal transport. A key role is played by coarsening the filtration: sometimes, no such p-value or e-value exists in the richest data filtration, but it does exist in some reduced filtration, and our work provides the first general characterization of this phenomenon. We also provide an iterative construction that explicitly constructs such processes, and under certain conditions it finds the one that grows fastest under a specific alternative $Q$. We discuss implications for the construction of composite nonnegative (super)martingales, and end with some conjectures and open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.16539v4</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyuan Zhang, Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
    <item>
      <title>Multilevel Monte Carlo for a class of Partially Observed Processes in Neuroscience</title>
      <link>https://arxiv.org/abs/2310.06533</link>
      <description>arXiv:2310.06533v2 Announce Type: replace-cross 
Abstract: In this paper we consider Bayesian parameter inference associated to a class of partially observed stochastic differential equations (SDE) driven by jump processes. Such type of models can be routinely found in applications, of which we focus upon the case of neuroscience. The data are assumed to be observed regularly in time and driven by the SDE model with unknown parameters. In practice the SDE may not have an analytically tractable solution and this leads naturally to a time-discretization. We adapt the multilevel Markov chain Monte Carlo method of [11], which works with a hierarchy of time discretizations and show empirically and theoretically that this is preferable to using one single time discretization. The improvement is in terms of the computational cost needed to obtain a pre-specified numerical error. Our approach is illustrated on models that are found in neuroscience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06533v2</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohamed Maama, Ajay Jasra, Kengo Kamatani</dc:creator>
    </item>
    <item>
      <title>Career Modeling with Missing Data and Traces</title>
      <link>https://arxiv.org/abs/2311.15257</link>
      <description>arXiv:2311.15257v2 Announce Type: replace-cross 
Abstract: Many social scientists study the career trajectories of populations of interest, such as economic and administrative elites. However, data to document such processes are rarely completely available, which motivates the adoption of inference tools that can account for large numbers of missing values. Taking the example of public-private paths of elite civil servants in France, we introduce binary Markov switching models to perform Bayesian data augmentation. Our procedure relies on two data sources: (1) detailed observations of a small number of individual trajectories, and (2) less informative ``traces'' left by all individuals, which we model for imputation of missing data. An advantage of this model class is that it maintains the properties of hidden Markov models and enables a tailored sampler to target the posterior, while allowing for varying parameters across individuals and time. We provide two applied studies which demonstrate this can be used to properly test substantive hypotheses, and expand the social scientific literature in various ways. We notably show that the rate at which ENA graduates exit the French public sector has not increased since 1990, but that the rate at which they come back has increased.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15257v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Voldoire, Robin J. Ryder, Ryan Lahfa</dc:creator>
    </item>
    <item>
      <title>State-Space Modeling of Shape-constrained Functional Time Series</title>
      <link>https://arxiv.org/abs/2404.07586</link>
      <description>arXiv:2404.07586v2 Announce Type: replace-cross 
Abstract: Functional time series data frequently appears in econometric analyses, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. To enable posterior computation by an efficient Markov chain Monte Carlo method, a novel data augmentation technique is devised for the complicated likelihood of this model. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07586v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Dynamic Treatment Effects under Functional Longitudinal Studies</title>
      <link>https://arxiv.org/abs/2406.06868</link>
      <description>arXiv:2406.06868v2 Announce Type: replace-cross 
Abstract: Establishing causality is a fundamental goal in fields like medicine and social sciences. While randomized controlled trials are the gold standard for causal inference, they are not always feasible or ethical. Observational studies can serve as alternatives but introduce confounding biases, particularly in complex longitudinal data, where treatment-confounder feedback complicates analysis. The challenge increases with Dynamic Treatment Regimes (DTRs), where treatment allocation depends on rich historical patient data. The advent of real-time healthcare monitoring technologies, such as MIMIC-IV and Continuous Glucose Monitoring (CGM), has popularized Functional Longitudinal Data (FLD). However, there is yet no investigate of causal inference for FLD with DTRs. In this paper, we address it by developing a population-level framework for functional longitudinal data, accommodating DTRs. To that end, we define the potential outcomes and causal effects of interest. We then develop identification assumptions, and derive g-computation, inverse probability weighting, and doubly robust formulas through novel applications of stochastic process and measure theory. We further show that our framework is nonparametric and compute the efficient influence curve using semiparametric theory. Last, we illustrate our framework's potential through Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06868v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ying</dc:creator>
    </item>
    <item>
      <title>A Note on Doubly Robust Estimator in Regression Continuity Designs</title>
      <link>https://arxiv.org/abs/2411.07978</link>
      <description>arXiv:2411.07978v3 Announce Type: replace-cross 
Abstract: This note introduces a doubly robust (DR) estimator for regression discontinuity (RD) designs. RD designs provide a quasi-experimental framework for estimating treatment effects, where treatment assignment depends on whether a running variable surpasses a predefined cutoff. A common approach in RD estimation is the use of nonparametric regression methods, such as local linear regression. However, the validity of these methods still relies on the consistency of the nonparametric estimators. In this study, we propose the DR-RD estimator, which combines two distinct estimators for the conditional expected outcomes. The primary advantage of the DR-RD estimator lies in its ability to ensure the consistency of the treatment effect estimation as long as at least one of the two estimators is consistent. Consequently, our DR-RD estimator enhances robustness of treatment effect estimators in RD designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07978v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 03 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
