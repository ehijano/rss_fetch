<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multimodal Chip Physical Design Engineer Assistant</title>
      <link>https://arxiv.org/abs/2510.15872</link>
      <description>arXiv:2510.15872v1 Announce Type: new 
Abstract: Modern chip physical design relies heavily on Electronic Design Automation (EDA) tools, which often struggle to provide interpretable feedback or actionable guidance for improving routing congestion. In this work, we introduce a Multimodal Large Language Model Assistant (MLLMA) that bridges this gap by not only predicting congestion but also delivering human-interpretable design suggestions. Our method combines automated feature generation through MLLM-guided genetic prompting with an interpretable preference learning framework that models congestion-relevant tradeoffs across visual, tabular, and textual inputs. We compile these insights into a "Design Suggestion Deck" that surfaces the most influential layout features and proposes targeted optimizations. Experiments on the CircuitNet benchmark demonstrate that our approach outperforms existing models on both accuracy and explainability. Additionally, our design suggestion guidance case study and qualitative analyses confirm that the learned preferences align with real-world design principles and are actionable for engineers. This work highlights the potential of MLLMs as interactive assistants for interpretable and context-aware physical design optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15872v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yun-Da Tsai, Chang-Yu Chao, Liang-Yeh Shen, Tsung-Han Lin, Haoyu Yang, Mark Ho, Yi-Chen Lu, Wen-Hao Liu, Shou-De Lin, Haoxing Ren</dc:creator>
    </item>
    <item>
      <title>Putting the Context back into Memory</title>
      <link>https://arxiv.org/abs/2510.15878</link>
      <description>arXiv:2510.15878v1 Announce Type: new 
Abstract: Requests arriving at main memory are often different from what programmers can observe or estimate by using CPU-based monitoring. Hardware cache prefetching, memory request scheduling and interleaving cause a loss of observability that limits potential data movement and tiering optimizations. In response, memory-side telemetry hardware like page access heat map units (HMU) and page prefetchers were proposed to inform Operating Systems with accurate usage data. However, it is still hard to map memory activity to software program functions and objects because of the decoupled nature of host processors and memory devices. Valuable program context is stripped out from the memory bus, leaving only commands, addresses and data. Programmers have expert knowledge of future data accesses, priorities, and access to processor state, which could be useful hints for runtime memory device optimization. This paper makes context visible at memory devices by encoding any user-visible state as detectable packets in the memory read address stream, in a nondestructive manner without significant capacity overhead, drivers or special access privileges. We prototyped an end-to-end system with metadata injection that can be reliably detected and decoded from a memory address trace, either by a host processor, or a memory module. We illustrate a use case with precise code execution markers and object address range tracking. In the future, real time metadata decoding with near-memory computing (NMC) could provide customized telemetry and statistics to users, or act on application hints to perform functions like prioritizing requests, remapping data and reconfiguring devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15878v1</guid>
      <category>cs.AR</category>
      <category>cs.OS</category>
      <category>cs.PF</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David A. Roberts</dc:creator>
    </item>
    <item>
      <title>Opportunities and Challenges for 3D Systems and Their Design</title>
      <link>https://arxiv.org/abs/2510.15880</link>
      <description>arXiv:2510.15880v1 Announce Type: new 
Abstract: Although it is not a new concept, 3D integration increasingly receives widespread interest and focus as lithographic scaling becomes more challenging, and as the ability to make miniature vias greatly improves. Like Moores law, 3D integration improves density. With improvements in packaging density, however, come the challenges associated with its inherently higher power density. And though it acts somewhat as a scaling accelerator, the vertical integration also poses new challenges to design and manufacturing technologies. The placement of circuits, vias, and macros in the planes of a 3D stack must be co-designed across layers (or must conform to new standards) so that, when assembled, they have correct spatial correspondence. Each layer, although perhaps being a mere functional slice through a system (and we can slice the system in many different ways), must be independently testable so that we can systematically test and diagnose subsystems before and after final assembly. When those layers are assembled, they must come together in a way that enables a sensible yield and facilitates testing the finished product. To make the most of 3D integration, we should articulate the leverages of 3D systems (other researchers offer a more complete treatment elsewhere). Then we can enumerate and elucidate many of the new challenges posed by the design, assembly, and test of 3D systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15880v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip Emma, Eren Kurshan</dc:creator>
    </item>
    <item>
      <title>FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern</title>
      <link>https://arxiv.org/abs/2510.15882</link>
      <description>arXiv:2510.15882v1 Announce Type: new 
Abstract: As large language models (LLMs) continue to scale, multi-node deployment has become a necessity. Consequently, communication has become a critical performance bottleneck. Current intra-node communication libraries, like NCCL, typically make use of a single interconnect such as NVLink. This approach creates performance ceilings, especially on hardware like the H800 GPU where the primary interconnect's bandwidth can become a bottleneck, and leaves other hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable Network Interface Cards (NICs) largely idle during intensive workloads. We propose FlexLink, the first collective communication framework to the best of our knowledge designed to systematically address this by aggregating these heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance communication fabric. FlexLink employs an effective two-stage adaptive load balancing strategy that dynamically partitions communication traffic across all available links, ensuring that faster interconnects are not throttled by slower ones. On an 8-GPU H800 server, our design improves the bandwidth of collective operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL baseline, respectively. This gain is achieved by offloading 2-22% of the total communication traffic to the previously underutilized PCIe and RDMA NICs. FlexLink provides these improvements as a lossless, drop-in replacement compatible with the NCCL API, ensuring easy adoption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15882v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Shen, Rui Zhang, Junping Zhao</dc:creator>
    </item>
    <item>
      <title>Generalized Methodology for Determining Numerical Features of Hardware Floating-Point Matrix Multipliers: Part I</title>
      <link>https://arxiv.org/abs/2510.15884</link>
      <description>arXiv:2510.15884v1 Announce Type: new 
Abstract: Numerical features of matrix multiplier hardware units in NVIDIA and AMD data centre GPUs have recently been studied. Features such as rounding, normalisation, and internal precision of the accumulators are of interest. In this paper, we extend the methodology for analysing those features, to consumer-grade NVIDIA GPUs by implementing an architecture-independent test scheme for various input and output precision formats. Unlike current approaches, the proposed test vector generation method neither performs an exhaustive search nor relies on hard-coded {constants that are device-specific, yet remains applicable to a wide range of mixed-precision formats. We have applied the scheme to the RTX-3060 (Ampere architecture), and Ada RTX-1000 (Ada Lovelace architecture) graphics cards and determined numerical features of matrix multipliers for binary16, TensorFloat32, and bfloat16 input floating point formats and binary16 and binary32 IEEE 754 output formats. Our methodology allowed us to determine that} the numerical features of RTX-3060, a consumer-grade GPU, are identical to those of the A100, a data centre GPU. We do not expect our code to require any changes for performing analysis of matrix multipliers on newer NVIDIA GPUs, Hopper or Blackwell, and their future successors, and any input/output format combination, including the latest 8-bit floating-point formats.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15884v1</guid>
      <category>cs.AR</category>
      <category>cs.MS</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Faizan A Khattak, Mantas Mikaitis</dc:creator>
    </item>
    <item>
      <title>ConZone+: Practical Zoned Flash Storage Emulation for Consumer Devices</title>
      <link>https://arxiv.org/abs/2510.15885</link>
      <description>arXiv:2510.15885v1 Announce Type: new 
Abstract: To facilitate the understanding and efficient enhancement of software and hardware design for consumer-grade zoned flash storage, ConZone is proposed as the first emulator designed to model the resource constraints and architectural features typical of such systems. It incorporates essential components commonly deployed in consumer-grade devices, including limited logical to physical mapping caches, constrained write buffers, and hybrid flash media management. However, ConZone cannot be mounted with the file system due to the lack of in-place update capability, which is required by the metadata area of F2FS. To improve the usability of the emulator, ConZone+ extends ConZone with support for a block interface. We also provide a script to help the deployment and introduces several enhancements over the original version. Users can explore the internal architecture of consumer-grade zoned flash storage and integrate their optimizations with system software using ConZone+. We validate the accuracy of ConZone+ by comparing a hardware architecture representative of consumer-grade zoned flash storage and comparing it with the state-of-the-art. In addition, we conduct several case studies using ConZone+ to investigate the design of zoned storage and explore the inadequacies of the current file system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15885v1</guid>
      <category>cs.AR</category>
      <category>cs.OS</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingcui Yu, Zonghuan Yan, Jialin Liu, Yumiao Zhao, Yanyun Wang, Xinghui Duan, Yina Lv, Liang Shi</dc:creator>
    </item>
    <item>
      <title>basic_RV32s: An Open-Source Microarchitectural Roadmap for RISC-V RV32I</title>
      <link>https://arxiv.org/abs/2510.15887</link>
      <description>arXiv:2510.15887v1 Announce Type: new 
Abstract: This paper introduces BASIC_RV32s, an open-source framework providing a practical microarchitectural roadmap for the RISC-V RV32I architecture, addressing the gap between theoretical knowledge and hardware implementation. Following the classic Patterson and Hennessy methodology, the design evolves from a basic single-cycle core to a 5-stage pipelined core design with full hazard forwarding, dynamic branch prediction, and exception handling. For verification, the final core design is integrated into a System-on-Chip (SoC) with Universal Asynchronous Receiver-Transmitter (UART) communication implemented on a Xilinx Artix-7 Field-Programmable Gate Array (FPGA), achieving 1.09 Dhrystone million instructions per second per megahertz (DMIPS/MHz) at 50 MHz. By releasing all Register-Transfer Level (RTL) source code, signal-level logic block diagrams, and development logs under MIT license on GitHub, BASIC_RV32s offers a reproducible instructional pathway for the open-source hardware ecosystem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15887v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hyun Woo Kang, Ji Woong Choi</dc:creator>
    </item>
    <item>
      <title>Limited Read-Write/Set Hardware Transactional Memory without modifying the ISA or the Coherence Protocol</title>
      <link>https://arxiv.org/abs/2510.15888</link>
      <description>arXiv:2510.15888v1 Announce Type: new 
Abstract: Hardware Transactional Memory (HTM) allows lock-free programming as easy as with traditional coarse-grain locks or similar, while benefiting from the performance advantages of fine-grained locking. Many HTM implementations have been proposed, but they have not received widespread adoption because of their high hardware complexity, their need for additions to the Instruction Set Architecture (ISA), and often for modifications to the cache coherence protocol.
  We show that HTM can be implemented without adding new instructions -- merely by extending the semantics of two existing, Load-Linked and Store-Conditional. Also, our proposed design does not modify or extend standard coherence protocols. We further propose to drastically simplify the implementation of HTM -- confined to modifications in the L1 Data Cache only -- by restricting it to applications where the write set plus the read set of each transaction do not exceed a small number of cache lines. We also propose two alternative mechanisms to guarantee forward progress, both based on detecting retrial attempts.
  We simulated our proposed design in Gem5, and we used it to implement several popular concurrent data structures, showing that a maximum of eight (8) words (cache lines) suffice for the write plus read sets. We provide a detailed explanation of selected implementations, clarifying the intended usage of our HTM from a programmer's perspective. We evaluated our HTM under varying contention levels to explore its scalability limits. The results indicate that our HTM provides good performance in concurrent data structures when contention is spread across multiple nodes: in such cases, the percentage of aborts relative to successful commits is very low. In the atomic fetch-and-increment benchmark for multiple shared counters, the results show that, under low-congestion, our HTM improves performance relative to the TTS lock.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15888v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Konstantinos Kafousis</dc:creator>
    </item>
    <item>
      <title>Accelerating Frontier MoE Training with 3D Integrated Optics</title>
      <link>https://arxiv.org/abs/2510.15893</link>
      <description>arXiv:2510.15893v1 Announce Type: new 
Abstract: The unabated growth in AI workload demands is driving the need for concerted advances in compute, memory, and interconnect performance. As traditional semiconductor scaling slows, high-speed interconnects have emerged as the new scaling engine, enabling the creation of larger logical GPUs by linking many GPUs into a single, low-latency, high-bandwidth compute domain. While initial scale-up fabrics leveraged copper interconnects for their power and cost advantages, the maximum reach of passive electrical interconnects (approximately 1 meter) effectively limits the scale-up domain to within a single rack. The advent of 3D-stacked optics and logic offers a transformative, power-efficient scale-up solution for connecting hundreds of GPU packages (thousands of GPUs) across multiple data center racks. This work explores the design tradeoffs of scale-up technologies and demonstrates how frontier LLMs necessitate novel photonic solutions to achieve aggressive power and performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and switches within the scale-up domain when training Frontier Mixture of Experts (MoE) models exceeding one trillion parameters. Our results show that the substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X increase in scale-up capability. This affords new opportunities for multi-dimensional parallelism within the scale-up domain and results in a 2.7X reduction in time-to-train, unlocking unprecedented model scaling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15893v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/HOTI66940.2025.0002</arxiv:DOI>
      <dc:creator>Mikhail Bernadskiy, Peter Carson, Thomas Graham, Taylor Groves, Ho John Lee, Eric Yeh</dc:creator>
    </item>
    <item>
      <title>DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms</title>
      <link>https://arxiv.org/abs/2510.15897</link>
      <description>arXiv:2510.15897v1 Announce Type: new 
Abstract: Chip placement, the task of determining optimal positions of circuit modules on a chip canvas, is a critical step in the VLSI design flow that directly impacts performance, power consumption, and routability. Traditional methods rely on analytical optimization or reinforcement learning, which struggle with hard placement constraints or require expensive online training for each new circuit design. To address these limitations, we introduce DiffPlace, a framework that formulates chip placement as a conditional denoising diffusion process, enabling transferable placement policies that generalize to unseen circuit netlists without retraining. DiffPlace leverages the generative capabilities of diffusion models to efficiently explore the vast space of placement while conditioning on circuit connectivity and relative quality metrics to identify optimal solutions globally. Our approach combines energy-guided sampling with constrained manifold diffusion to ensure placement legality, achieving extremely low overlap across all experimental scenarios. Our method bridges the gap between optimization-based and learning-based approaches, offering a practical path toward automated, high-quality chip placement for modern VLSI design. Our source code is publicly available at: https://github.com/HySonLab/DiffPlace/</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15897v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kien Le Trung, Truong-Son Hy</dc:creator>
    </item>
    <item>
      <title>LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2510.15899</link>
      <description>arXiv:2510.15899v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are gaining prominence in various fields, thanks to their ability to generate high- quality content from human instructions. This paper delves into the field of chip design using LLMs, specifically in Power- Performance-Area (PPA) optimization and the generation of accurate Verilog codes for circuit designs. We introduce a novel framework VeriPPA designed to optimize PPA and generate Verilog code using LLMs. Our method includes a two-stage process where the first stage focuses on improving the functional and syntactic correctness of the generated Verilog codes, while the second stage focuses on optimizing the Verilog codes to meet PPA constraints of circuit designs, a crucial element of chip design. Our framework achieves an 81.37% success rate in syntactic correctness and 62.06% in functional correctness for code genera- tion, outperforming current state-of-the-art (SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework achieves 99.56% syntactic correctness and 43.79% functional correctness, also surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57% for functional correctness. Furthermore, Our framework able to optimize the PPA of the designs. These results highlight the potential of LLMs in handling complex technical areas and indicate an encouraging development in the automation of chip design processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15899v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kiran Thorat, Jiahui Zhao, Yaotian Liu, Amit Hasan, Hongwu Peng, Xi Xie, Bin Lei, Caiwen Ding</dc:creator>
    </item>
    <item>
      <title>Fully Automated Verification Framework for Configurable IPs: From Requirements to Results</title>
      <link>https://arxiv.org/abs/2510.15902</link>
      <description>arXiv:2510.15902v1 Announce Type: new 
Abstract: The increasing competition in the semiconductor industry has created significant pressure to reduce chip prices while maintaining quality and reliability. Functional verification, particularly for configurable IPs, is a major contributor to development costs due to its complexity and resource-intensive nature. To address this, we propose a fully automated framework for requirements driven functional verification. The framework automates key processes, including vPlan generation, testbench creation, regression execution, and reporting in a requirements management tool, drastically reducing verification effort. This approach accelerates development cycles, minimizes human error, and enhances coverage, offering a scalable and efficient solution to the challenges of verifying configurable IPs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15902v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuhang Zhang, Jelena Radulovic, Thorsten Dworzak</dc:creator>
    </item>
    <item>
      <title>NVM-in-Cache: Repurposing Commodity 6T SRAM Cache into NVM Analog Processing-in-Memory Engine using a Novel Compute-on-Powerline Scheme</title>
      <link>https://arxiv.org/abs/2510.15904</link>
      <description>arXiv:2510.15904v1 Announce Type: new 
Abstract: The rapid growth of deep neural network (DNN) workloads has significantly increased the demand for large-capacity on-chip SRAM in machine learning (ML) applications, with SRAM arrays now occupying a substantial fraction of the total die area. To address the dual challenges of storage density and computation efficiency, this paper proposes an NVM-in-Cache architecture that integrates resistive RAM (RRAM) devices into a conventional 6T-SRAM cell, forming a compact 6T-2R bit-cell. This hybrid cell enables Processing-in-Memory (PIM) mode, which performs massively parallel multiply-and-accumulate (MAC) operations directly on cache power lines while preserving stored cache data. By exploiting the intrinsic properties of the 6T-2R structure, the architecture achieves additional storage capability, high computational throughput without any bit-cell area overhead. Circuit- and array-level simulations in GlobalFoundries 22nm FDSOI technology demonstrate that the proposed design achieves a throughput of 0.4 TOPS and 491.78 TOPS/W. For 128 row-parallel operations, the CIFAR-10 classification is demonstrated by mapping a Resnet-18 neural network, achieving an accuracy of 91.27%. These results highlight the potential of the NVM-in-Cache approach to serve as a scalable, energy-efficient computing method by re-purposing existing 6T SRAM cache architecture for next-generation AI accelerators and general purpose processors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15904v1</guid>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.IV</category>
      <category>eess.SY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhradip Chakraborty, Ankur Singh, Xuming Chen, Gourav Datta, Akhilesh R. Jaiswal</dc:creator>
    </item>
    <item>
      <title>FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures</title>
      <link>https://arxiv.org/abs/2510.15906</link>
      <description>arXiv:2510.15906v1 Announce Type: new 
Abstract: Debugging formal verification (FV) failures represents one of the most time-consuming bottlenecks in modern hardware design workflows. When properties fail, engineers must manually trace through complex counter-examples spanning multiple cycles, analyze waveforms, and cross-reference design specifications to identify root causes - a process that can consume hours or days per bug. Existing solutions are largely limited to manual waveform viewers or simple automated tools that cannot reason about the complex interplay between design intent and implementation logic. We present FVDebug, an intelligent system that automates root-cause analysis by combining multiple data sources - waveforms, RTL code, design specifications - to transform failure traces into actionable insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis that structures failure traces into directed acyclic graphs, (2) Graph Scanner using batched Large Language Model (LLM) analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations. FVDebug further provides concrete RTL fixes through its Fix Generator. Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates. We further report results on two proprietary, production-scale FV counterexamples. These results demonstrate FVDebug's applicability from academic benchmarks to industrial designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15906v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunsheng Bai, Ghaith Bany Hamad, Chia-Tung Ho, Syed Suhaib, Haoxing Ren</dc:creator>
    </item>
    <item>
      <title>Symbolic Timing Analysis of Digital Circuits Using Analytic Delay Functions</title>
      <link>https://arxiv.org/abs/2510.15907</link>
      <description>arXiv:2510.15907v1 Announce Type: new 
Abstract: We propose a novel approach to symbolic timing analysis for digital integrated circuits based on recently developed analytic delay formulas for 2-input NOR, NAND, and Muller-C gates by Ferdowsi et al. (NAHS 2025). Given a fixed order of the transitions of all input and internal signals of a circuit, our framework computes closed-form analytic delay expressions for all the internal signal transition times that depend on (i) the symbolic transition times of the relevant input signals and (ii) the model parameters of the relevant gates. The resulting formulas facilitate per-transition timing analysis without any simulation, by instantiating the symbolic input transition times and the gate parameters. More importantly, however, they also enable an \emph{analytic} study of the dependencies of certain timing properties on input signals and gate parameters. For instance, differentiating a symbolic delay expression with respect to a gate parameter or input transition time enables sensitivity analysis. As a proof of concept, we implement our approach using the computer algebra system SageMath and apply it to the NOR-gate version of the c17 slack benchmark circuit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15907v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Era Thaqi, Dennis Eigner, Arman Ferdowsi, Ulrich Schmid</dc:creator>
    </item>
    <item>
      <title>Belenos: Bottleneck Evaluation to Link Biomechanics to Novel Computing Optimizations</title>
      <link>https://arxiv.org/abs/2510.15908</link>
      <description>arXiv:2510.15908v1 Announce Type: new 
Abstract: Finite element simulations are essential in biomechanics, enabling detailed modeling of tissues and organs. However, architectural inefficiencies in current hardware and software stacks limit performance and scalability, especially for iterative tasks like material parameter identification. As a result, workflows often sacrifice fidelity for tractability. Reconfigurable hardware, such as FPGAs, offers a promising path to domain-specific acceleration without the cost of ASICs, but its potential in biomechanics remains underexplored. This paper presents Belenos, a comprehensive workload characterization of finite element biomechanics using FEBio, a widely adopted simulator, gem5 sensitivity studies, and VTune analysis. VTune results reveal that smaller workloads experience moderate front-end stalls, typically around 13.1%, whereas larger workloads are dominated by significant back-end bottlenecks, with backend-bound cycles ranging from 59.9% to over 82.2%. Complementary gem5 sensitivity studies identify optimal hardware configurations for Domain-Specific Accelerators (DSA), showing that suboptimal pipeline, memory, or branch predictor settings can degrade performance by up to 37.1%. These findings underscore the need for architecture-aware co-design to efficiently support biomechanical simulation workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15908v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hana Chitsaz, Johnson Umeike, Amirmahdi Namjoo, Babak N. Safa, Bahar Asgari</dc:creator>
    </item>
    <item>
      <title>SoCks - Simplifying Firmware and Software Integration for Heterogeneous SoCs</title>
      <link>https://arxiv.org/abs/2510.15910</link>
      <description>arXiv:2510.15910v1 Announce Type: new 
Abstract: Modern heterogeneous System-on-Chip (SoC) devices integrate advanced components into a single package, offering powerful capabilities while also introducing significant complexity. To manage these sophisticated devices, firmware and software developers need powerful development tools. However, as these tools become increasingly complex, they often lack adequate support, resulting in a steep learning curve and challenging troubleshooting. To address this, this work introduces System-on-Chip blocks (SoCks), a flexible and expandable build framework that reduces complexity by partitioning the SoC image into high-level units called blocks. SoCks builds each firmware and software block in an encapsulated way, independently from other components of the image, thereby reducing dependencies to a minimum. While some information exchange between the blocks is unavoidable to ensure seamless runtime integration, this interaction is standardized via interfaces. A small number of dependencies and well-defined interfaces simplify the reuse of existing block implementations and facilitate seamless substitution between versions-for instance, when choosing root file systems for the embedded Linux operating system. Additionally, this approach facilitates the establishment of a decentralized and partially automated development flow through Continuous Integration and Continuous Delivery (CI/CD). Measurement results demonstrate that SoCks can build a complete SoC image up to three times faster than established tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15910v1</guid>
      <category>cs.AR</category>
      <category>hep-ex</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marvin Fuchs, Lukas Scheller, Timo Muscheid, Oliver Sander, Luis E. Ardila-Perez</dc:creator>
    </item>
    <item>
      <title>VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts</title>
      <link>https://arxiv.org/abs/2510.15914</link>
      <description>arXiv:2510.15914v1 Announce Type: new 
Abstract: Large language models (LLMs) have demonstrated strong capabilities in generating Verilog code from natural language descriptions. However, Verilog code inherently encodes structural information of hardware circuits. Effectively leveraging this structural information to enhance the functional and syntactic correctness of LLM-generated Verilog code remains a significant challenge. To address this challenge, we propose VeriGRAG , a novel framework that extracts structural graph embeddings from Verilog code using graph neural networks (GNNs). A multimodal retriever then selects the graph embeddings most relevant to the given generation task, which are aligned with the code modality through the VeriFormer module to generate structure-aware soft prompts. Our experiments demonstrate that VeriGRAG substantially improves the correctness of Verilog code generation, achieving state-of-the-art or superior performance across both VerilogEval and RTLLM benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15914v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.PL</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayu Zhao, Song Chen</dc:creator>
    </item>
    <item>
      <title>Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding</title>
      <link>https://arxiv.org/abs/2510.15917</link>
      <description>arXiv:2510.15917v1 Announce Type: new 
Abstract: Existing storage systems lack visibility into workload intent, limiting their ability to adapt to the semantics of modern, large-scale data-intensive applications. This disconnect leads to brittle heuristics and fragmented, siloed optimizations. To address these limitations, we propose Intent-Driven Storage Systems (IDSS), a vision for a new paradigm where large language models (LLMs) infer workload and system intent from unstructured signals to guide adaptive and cross-layer parameter reconfiguration. IDSS provides holistic reasoning for competing demands, synthesizing safe and efficient decisions within policy guardrails. We present four design principles for integrating LLMs into storage control loops and propose a corresponding system architecture. Initial results on FileBench workloads show that IDSS can improve IOPS by up to 2.45X by interpreting intent and generating actionable configurations for storage components such as caching and prefetching. These findings suggest that, when constrained by guardrails and embedded within structured workflows, LLMs can function as high-level semantic optimizers, bridging the gap between application goals and low-level system control. IDSS points toward a future in which storage systems are increasingly adaptive, autonomous, and aligned with dynamic workload demands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15917v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shai Bergman, Won Wook Song, Lukas Cavigelli, Konstantin Berestizshevsky, Ke Zhou, Ji Zhang</dc:creator>
    </item>
    <item>
      <title>TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs</title>
      <link>https://arxiv.org/abs/2510.15926</link>
      <description>arXiv:2510.15926v1 Announce Type: new 
Abstract: With the emergence of wearable devices and other embedded systems, deploying large language models (LLMs) on edge platforms has become an urgent need. However, this is challenging because of their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as low as 1.58~bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected long latency of the prefill stage. We present \textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for low-power edge FPGAs that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates several novel techniques, including (1) a table-lookup-based ternary matrix multiplication (TLMM) engine utilizing grouped activations and online precomputation for low resource utilization and high throughput; (2) a fine-grained analytic URAM-based weight buffer management scheme for efficient loading and compute engine access; (3) a streaming dataflow architecture that fuses floating-point element-wise operations with linear computations to hide latency; (4) a reversed-reordered prefill stage attention with fused attention operations for high memory efficiency; and (5) a resource-efficient specialized decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to 25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for 64--128 token prompts, marking a significant energy-efficiency advancement in LLM inference on edge FPGAs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15926v1</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Qiao, Zhiheng Chen, Yifan Zhang, Yian Wang, Sitao Huang</dc:creator>
    </item>
    <item>
      <title>UPMEM Unleashed: Software Secrets for Speed</title>
      <link>https://arxiv.org/abs/2510.15927</link>
      <description>arXiv:2510.15927v1 Announce Type: new 
Abstract: Developing kernels for Processing-In-Memory (PIM) platforms poses unique challenges in data management and parallel programming on limited processing units. Although software development kits (SDKs) for PIM, such as the UPMEM SDK, provide essential tools, these emerging platforms still leave significant room for performance optimization. In this paper, we reveal surprising inefficiencies in UPMEM software stack and play with non-standard programming techniques. By making simple modifications to the assembly generated by the UPMEM compiler, we achieve speedups of 1.6-2x in integer addition and 1.4-5.9x in integer multiplication, depending on the data type. We also demonstrate that bit-serial processing of low precision data is a viable option for UPMEM: in INT4 bit-serial dot-product calculation, UPMEM can achieve over 2.7x speedup over the baseline. Minor API extensions for PIM allocation that account for the non-uniform memory access (NUMA) architecture of the server further improve the consistency and throughput of host-PIM data transfers by up to 2.9x. Finally, we show that, when the matrix is preloaded into PIM, our optimized kernels outperform a dual-socket CPU server by over 3x for INT8 generalized matrix-vector multiplication (GEMV) and by 10x for INT4 GEMV. Our optimized INT8 GEMV kernel outperforms the baseline 3.5x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15927v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Krystian Chmielewski, Jaros{\l}aw {\L}awnicki, Uladzislau Lukyanau, Tadeusz Kobus, Maciej Maciejewski</dc:creator>
    </item>
    <item>
      <title>Impl\'ementation Efficiente de Fonctions de Convolution sur FPGA \`a l'Aide de Blocs Param\'etrables et d'Approximations Polynomiales</title>
      <link>https://arxiv.org/abs/2510.15930</link>
      <description>arXiv:2510.15930v1 Announce Type: new 
Abstract: Implementing convolutional neural networks (CNNs) on field-programmable gate arrays (FPGAs) has emerged as a promising alternative to GPUs, offering lower latency, greater power efficiency and greater flexibility. However, this development remains complex due to the hardware knowledge required and the long synthesis, placement and routing stages, which slow down design cycles and prevent rapid exploration of network configurations, making resource optimisation under severe constraints particularly challenging. This paper proposes a library of configurable convolution Blocks designed to optimize FPGA implementation and adapt to available resources. It also presents a methodological framework for developing mathematical models that predict FPGA resources utilization. The approach is validated by analyzing the correlation between the parameters, followed by error metrics. The results show that the designed blocks enable adaptation of convolution layers to hardware constraints, and that the models accurately predict resource consumption, providing a useful tool for FPGA selection and optimized CNN deployment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15930v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philippe Magalh\~aes (LabHC), Virginie Fresse (LabHC), Beno\^it Suffran (LabHC), Olivier Alata (LabHC)</dc:creator>
    </item>
    <item>
      <title>Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing</title>
      <link>https://arxiv.org/abs/2510.16040</link>
      <description>arXiv:2510.16040v1 Announce Type: new 
Abstract: Running Large Language Models (LLMs) on edge devices is crucial for reducing latency, improving real-time processing, and enhancing privacy. By performing inference directly on the device, data does not need to be sent to the cloud, ensuring faster responses and reducing reliance on network connectivity. However, implementing LLMs on edge devices presents challenges, particularly with managing key-value (KV) caches, which plays a pivotal role in LLM serving. As the input text lengthens, the size of the KV cache increases linearly with the sequence length, leading to a significant memory footprint and data access costs. On the other hand, edge devices have limited memory and computational power, making it hard to store and efficiently access the large caches needed for LLM inference.
  To mitigate the substantial overhead caused by KV cache, we propose using embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device, which offers higher storage density compared to SRAM. However, to ensure data integrity, eDRAM needs periodic refresh operations, which are power-intensive. To reduce eDRAM costs and improve overall system performance, we propose~\textit{Kelle}, a software-hardware co-design solution optimized for deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained memory eviction, recomputation, and refresh control algorithms, the \textit{Kelle} accelerator delivers a $3.9\times$ speedup and $4.5\times$ energy savings compared to existing baseline solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16040v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Tianhua Xia, Sai Qian Zhang</dc:creator>
    </item>
    <item>
      <title>Architecture, Simulation and Software Stack to Support Post-CMOS Accelerators: The ARCHYTAS Project</title>
      <link>https://arxiv.org/abs/2510.16487</link>
      <description>arXiv:2510.16487v1 Announce Type: new 
Abstract: ARCHYTAS aims to design and evaluate non-conventional hardware accelerators, in particular, optoelectronic, volatile and non-volatile processing-in-memory, and neuromorphic, to tackle the power, efficiency, and scalability bottlenecks of AI with an emphasis on defense use cases (e.g., autonomous vehicles, surveillance drones, maritime and space platforms). In this paper, we present the system architecture and software stack that ARCHYTAS will develop to integrate and support those accelerators, as well as the simulation software needed for early prototyping of the full system and its components.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16487v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISVLSI65124.2025.11130220</arxiv:DOI>
      <arxiv:journal_reference>2025 IEEE Computer Society Annual Symposium on VLSI (ISVLSI)</arxiv:journal_reference>
      <dc:creator>Giovanni Agosta, Stefano Cherubin, Derek Christ, Francesco Conti, Asbj{\o}rn Djupdal, Matthias Jung, Georgios Keramidas, Roberto Passerone, Paolo Rech, Elisa Ricci, Philippe Velha, Flavio Vella, Kasim Sinan Yildirim, Nils Wilbert</dc:creator>
    </item>
    <item>
      <title>Towards Intelligent Traffic Signaling in Dhaka City Based on Vehicle Detection and Congestion Optimization</title>
      <link>https://arxiv.org/abs/2510.16622</link>
      <description>arXiv:2510.16622v1 Announce Type: new 
Abstract: The vehicular density in urbanizing cities of developing countries such as Dhaka, Bangladesh result in a lot of traffic congestion, causing poor on-road experiences. Traffic signaling is a key component in effective traffic management for such situations, but the advancements in intelligent traffic signaling have been exclusive to developed countries with structured traffic. The non-lane-based, heterogeneous traffic of Dhaka City requires a contextual approach. This study focuses on the development of an intelligent traffic signaling system feasible in the context of developing countries such as Bangladesh. We propose a pipeline leveraging Real Time Streaming Protocol (RTSP) feeds, a low resources system Raspberry Pi 4B processing, and a state of the art YOLO-based object detection model trained on the Non-lane-based and Heterogeneous Traffic (NHT-1071) dataset to detect and classify heterogeneous traffic. A multi-objective optimization algorithm, NSGA-II, then generates optimized signal timings, minimizing waiting time while maximizing vehicle throughput. We test our implementation in a five-road intersection at Palashi, Dhaka, demonstrating the potential to significantly improve traffic management in similar situations. The developed testbed paves the way for more contextual and effective Intelligent Traffic Signaling (ITS) solutions for developing areas with complicated traffic dynamics such as Dhaka City.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16622v1</guid>
      <category>cs.AR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazi Ababil Azam, Hasan Masum, Masfiqur Rahaman, A. B. M. Alim Al Islam</dc:creator>
    </item>
    <item>
      <title>SmaRTLy: RTL Optimization with Logic Inferencing and Structural Rebuilding</title>
      <link>https://arxiv.org/abs/2510.17251</link>
      <description>arXiv:2510.17251v1 Announce Type: new 
Abstract: This paper proposes smaRTLy: a new optimization technique for multiplexers in Register-Transfer Level (RTL) logic synthesis. Multiplexer trees are very common in RTL designs, and traditional tools like Yosys optimize them by traversing the tree and monitoring control port values. However, this method does not fully exploit the intrinsic logical relationships among signals or the potential for structural optimization. To address these limitations, we develop innovative strategies to remove redundant multiplexer trees and restructure the remaining ones, significantly reducing the overall gate count. We evaluate smaRTLy on the IWLS-2005 and RISC-V benchmarks, achieving an additional 8.95% reduction in AIG area compared to Yosys. We also evaluate smaRTLy on an industrial benchmark in the scale of millions of gates, results show that smaRTLy can remove 47.2% more AIG area than Yosys. These results demonstrate the effectiveness of our logic inferencing and structural rebuilding techniques in enhancing the RTL optimization process, leading to more efficient hardware designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17251v1</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengxi Li, Yang Sun, Lei Chen, Yiwen Wang, Mingxuan Yuan, Evangeline F. Y. Young</dc:creator>
    </item>
    <item>
      <title>Direct Simplified Symbolic Analysis (DSSA) Tool</title>
      <link>https://arxiv.org/abs/2510.15901</link>
      <description>arXiv:2510.15901v1 Announce Type: cross 
Abstract: This paper introduces Direct Simplified Symbolic Analysis (DSSA), a new method for simplifying analog circuits. Unlike traditional matrix- or graph-based techniques that are often slow and memory-intensive, DSSA treats the task as a modeling problem and directly extracts the most significant transfer function terms. By combining Monte Carlo simulation with a genetic algorithm, it minimizes error between simplified symbolic and exact numeric expressions. Tests on five circuits in MATLAB show strong performance, with only 0.64 dB average and 1.36 dB maximum variation in dc-gain, along with a 6.8% average pole/zero error. These results highlight DSSA as an efficient and accurate tool for symbolic circuit analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15901v1</guid>
      <category>cs.OH</category>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3934/jdg.2023023</arxiv:DOI>
      <arxiv:journal_reference>Journal of Dynamics and Games, 2024, 11(3), 232-248</arxiv:journal_reference>
      <dc:creator>Mohammad Shokouhifar, Hossein Yazdanjouei, Gerhard-Wilhelm Weber</dc:creator>
    </item>
    <item>
      <title>Latency Based Tiling</title>
      <link>https://arxiv.org/abs/2510.15912</link>
      <description>arXiv:2510.15912v1 Announce Type: cross 
Abstract: Latency Based Tiling provides a systems based approach to deriving approximate tiling solution that maximizes locality while maintaining a fast compile time. The method uses triangular loops to characterize miss ratio scaling of a machine avoiding prefetcher distortion. Miss ratio scaling captures the relationship between data access latency and working set size with sharp increases in latency indicating the data footprint exceeds capacity from a cache level. Through these noticeable increases in latency we can determine an approximate location for L1, L2, and L3 memory sizes. These sizes are expected to be under approximations of a systems true memory sizes which is in line with our expectations given the shared nature of cache in a multi process system as described in defensive loop tiling. Unlike auto tuning, which can be effective but prohibitively slow, Latency Based Tiling achieves negligible compile time overhead. The implementation in Rust enables a hardware agnostic approach which combined with a cache timing based techniques, yields a portable, memory safe system running wherever Rust is supported. The tiling strategy is applied to a subset of the polyhedral model, where loop nestings are tiled based on both the derived memory hierarchy and the observed data footprint per iteration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15912v1</guid>
      <category>cs.PL</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jack Cashman</dc:creator>
    </item>
    <item>
      <title>Self-Attention to Operator Learning-based 3D-IC Thermal Simulation</title>
      <link>https://arxiv.org/abs/2510.15968</link>
      <description>arXiv:2510.15968v1 Announce Type: cross 
Abstract: Thermal management in 3D ICs is increasingly challenging due to higher power densities. Traditional PDE-solving-based methods, while accurate, are too slow for iterative design. Machine learning approaches like FNO provide faster alternatives but suffer from high-frequency information loss and high-fidelity data dependency. We introduce Self-Attention U-Net Fourier Neural Operator (SAU-FNO), a novel framework combining self-attention and U-Net with FNO to capture long-range dependencies and model local high-frequency features effectively. Transfer learning is employed to fine-tune low-fidelity data, minimizing the need for extensive high-fidelity datasets and speeding up training. Experiments demonstrate that SAU-FNO achieves state-of-the-art thermal prediction accuracy and provides an 842x speedup over traditional FEM methods, making it an efficient tool for advanced 3D IC thermal simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15968v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhen Huang, Hong Wang, Wenkai Yang, Muxi Tang, Depeng Xie, Ting-Jung Lin, Yu Zhang, Wei W. Xing, Lei He</dc:creator>
    </item>
    <item>
      <title>SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference</title>
      <link>https://arxiv.org/abs/2510.17189</link>
      <description>arXiv:2510.17189v1 Announce Type: cross 
Abstract: Transformers have shown remarkable performance in both natural language processing (NLP) and computer vision (CV) tasks. However, their real-time inference speed and efficiency are limited due to the inefficiency in Softmax and Layer Normalization (LayerNorm). Previous works based on function approximation suffer from inefficient implementation as they place emphasis on computation while disregarding memory overhead concerns. Moreover, such methods rely on retraining to compensate for approximation error which can be costly and inconvenient.
  In this paper, we present SOLE, a hardware-software co-design for Softmax and LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes log2 quantization of exponent function and log-based division to approximate Softmax while AILayerNorm adopts low-precision statistic calculation. Compared with state-of-the-art designs, we achieve both low-precision calculation and low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE maintains inference accuracy without retraining while offering orders of magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements over prior state-of-the-art custom hardware for Softmax and LayerNorm, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17189v1</guid>
      <category>cs.LG</category>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxun Wang, Shuchang Zhou, Wenyu Sun, Peiqin Sun, Yongpan Liu</dc:creator>
    </item>
    <item>
      <title>ReLACE: A Resource-Efficient Low-Latency Cortical Acceleration Engine</title>
      <link>https://arxiv.org/abs/2510.17392</link>
      <description>arXiv:2510.17392v1 Announce Type: cross 
Abstract: We present a Cortical Neural Pool (CNP) architecture featuring a high-speed, resource-efficient CORDIC-based Hodgkin Huxley (RCHH) neuron model. Unlike shared CORDIC-based DNN approaches, the proposed neuron leverages modular and performance-optimised CORDIC stages with a latency-area trade-off. The FPGA implementation of the RCHH neuron shows 24.5% LUT reduction and 35.2% improved speed, compared to SoTA designs, with 70% better normalised root mean square error (NRMSE). Furthermore, the CNP exhibits 2.85x higher throughput (12.69 GOPS) compared to a functionally equivalent CORDIC-based DNN engine, with only a 0.35% accuracy drop compared to the DNN counterpart on the MNIST dataset. The overall results indicate that the design shows biologically accurate, low-resource spiking neural network implementations for resource-constrained edge AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17392v1</guid>
      <category>cs.NE</category>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sonu Kumar, Arjun S. Nair, Bhawna Chaudhary, Mukul Lokhande, Santosh Kumar Vishvakarma</dc:creator>
    </item>
    <item>
      <title>MTU: The Multifunction Tree Unit for Accelerating Zero-Knowledge Proofs</title>
      <link>https://arxiv.org/abs/2507.16793</link>
      <description>arXiv:2507.16793v2 Announce Type: replace 
Abstract: Zero-Knowledge Proofs (ZKPs) are critical for privacy-preserving techniques and verifiable computation. Many ZKP protocols rely on key kernels such as the SumCheck protocol and Merkle Tree commitments to enable their key security properties. These kernels exhibit balanced binary tree computational patterns, which enable efficient hardware acceleration. Although prior work has investigated accelerating these kernels as part of an overarching ZKP protocol, exploiting this common tree pattern remains relatively underexplored. We conduct a systematic evaluation of these tree-based workloads under different traversal strategies, analyzing performance on multi-threaded CPUs and the Multifunction Tree Unit (MTU) hardware accelerator. We introduce a hardware-friendly Hybrid Traversal for binary tree that improves parallelism and scalability while significantly reducing memory traffic on hardware. Our results show that MTU achieves up to $1478\times$ speedup over CPU at DDR-level bandwidth and that our hybrid traversal outperforms breadth-first search by up to $3\times$. These findings offer practical guidance for designing efficient hardware accelerators for ZKP workloads with binary tree structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.16793v2</guid>
      <category>cs.AR</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3768725.3768737</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the International Workshop on Hardware and Architectural Support for Security and Privacy 2025</arxiv:journal_reference>
      <dc:creator>Jianqiao Mo, Alhad Daftardar, Joey Ah-Kiow, Kaiyue Guo, Benedikt B\"unz, Siddharth Garg, Brandon Reagen</dc:creator>
    </item>
    <item>
      <title>Chiplet-Based RISC-V SoC with Modular AI Acceleration</title>
      <link>https://arxiv.org/abs/2509.18355</link>
      <description>arXiv:2509.18355v3 Announce Type: replace 
Abstract: Achieving high performance, energy efficiency, and cost-effectiveness while maintaining architectural flexibility is a critical challenge in the development and deployment of edge AI devices. Monolithic SoC designs struggle with this complex balance mainly due to low manufacturing yields (below 16%) at advanced 360 mm^2 process nodes. This paper presents a novel chiplet-based RISC-V SoC architecture that addresses these limitations through modular AI acceleration and intelligent system level optimization. Our proposed design integrates 4 different key innovations in a 30mm x 30mm silicon interposer: adaptive cross-chiplet Dynamic Voltage and Frequency Scaling (DVFS); AI-aware Universal Chiplet Interconnect Express (UCIe) protocol extensions featuring streaming flow control units and compression-aware transfers; distributed cryptographic security across heterogeneous chiplets; and intelligent sensor-driven load migration. The proposed architecture integrates a 7nm RISC-V CPU chiplet with dual 5nm AI accelerators (15 TOPS INT8 each), 16GB HBM3 memory stacks, and dedicated power management controllers. Experimental results across industry standard benchmarks like MobileNetV2, ResNet-50 and real-time video processing demonstrate significant performance improvements. The AI-optimized configuration achieves ~14.7% latency reduction, 17.3% throughput improvement, and 16.2% power reduction compared to previous basic chiplet implementations. These improvements collectively translate to a 40.1% efficiency gain corresponding to ~3.5 mJ per MobileNetV2 inference (860 mW/244 images/s), while maintaining sub-5ms real-time capability across all experimented workloads. These performance upgrades demonstrate that modular chiplet designs can achieve near-monolithic computational density while enabling cost efficiency, scalability and upgradeability, crucial for next-generation edge AI device applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18355v3</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suhas Suresh Bharadwaj, Prerana Ramkumar</dc:creator>
    </item>
    <item>
      <title>Cleaning up the Mess</title>
      <link>https://arxiv.org/abs/2510.15744</link>
      <description>arXiv:2510.15744v2 Announce Type: replace 
Abstract: A MICRO 2024 best paper runner-up publication (the Mess paper) with all three artifact badges awarded (including "Reproducible") proposes a new benchmark to evaluate real and simulated memory system performance. In this paper, we demonstrate that the Ramulator 2.0 simulation results reported in the Mess paper are incorrect and, at the time of the publication of the Mess paper, irreproducible. We find that the authors of Mess paper made multiple trivial human errors in both the configuration and usage of the simulators. We show that by correctly configuring Ramulator 2.0, Ramulator 2.0's simulated memory system performance actually resembles real system characteristics well, and thus a key claimed contribution of the Mess paper is factually incorrect. We also identify that the DAMOV simulation results in the Mess paper use wrong simulation statistics that are unrelated to the simulated DRAM performance. Moreover, the Mess paper's artifact repository lacks the necessary sources to fully reproduce all the Mess paper's results.
  Our work corrects the Mess paper's errors regarding Ramulator 2.0 and identifies important issues in the Mess paper's memory simulator evaluation methodology. We emphasize the importance of both carefully and rigorously validating simulation results and contacting simulator authors and developers, in true open source spirit, to ensure these simulators are used with correct configurations and as intended. We encourage the computer architecture community to correct the Mess paper's errors. This is necessary to prevent the propagation of inaccurate and misleading results, and to maintain the reliability of the scientific record. Our investigation also opens up questions about the integrity of the review and artifact evaluation processes. To aid future work, our source code and scripts are openly available at https://github.com/CMU-SAFARI/ramulator2/tree/mess.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15744v2</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haocong Luo, Ataberk Olgun, Maria Makeenkova, F. Nisa Bostanci, Geraldo F. Oliveira, A. Giray Yaglikci, Onur Mutlu</dc:creator>
    </item>
  </channel>
</rss>
