<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 11 Sep 2025 01:21:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Loss Functions for Detecting Outliers in Panel Data</title>
      <link>https://arxiv.org/abs/2509.07014</link>
      <description>arXiv:2509.07014v1 Announce Type: new 
Abstract: The detection of outliers is of critical importance in the assurance of data quality. Outliers may exist in observed data or in data derived from these observed data, such as estimates and forecasts. An outlier may indicate a problem with its data generation process or may simply be a true statement about the world. Without making any distributional assumptions, this paper proposes the use of loss functions to detect these outliers in panel data. An unsigned loss function is derived axiomatically. A signed loss function is developed to account for positive and negative outliers separately. In the case of nominal time an exact parametrization of the loss function is obtained. A time-invariant loss function permits the comparison of data at multiple times on the same basis. Several examples are provided, including an example in which the outliers are classified by another variable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07014v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Charles D. Coleman, Thomas Bryan</dc:creator>
    </item>
    <item>
      <title>Scalable Sample-to-Population Estimation of Hyperbolic Space Models for Hypergraphs</title>
      <link>https://arxiv.org/abs/2509.07031</link>
      <description>arXiv:2509.07031v1 Announce Type: new 
Abstract: Hypergraphs are useful mathematical representations of overlapping and nested subsets of interacting units, including groups of genes or brain regions, economic cartels, political or military coalitions, and groups of products that are purchased together. Despite the vast range of applications, the statistical analysis of hypergraphs is challenging: There are many hyperedges of small and large sizes, and hyperedges can overlap or be nested. We develop a novel statistical approach to hypergraphs with overlapping and nested hyperedges of varying sizes and levels of sparsity, which is amenable to scalable sample-to-population estimation with non-asymptotic theoretical guarantees. First, we introduce a probabilistic framework that embeds the units of a hypergraph in an unobserved hyperbolic space capturing core-periphery structure along with local structure in hypergraphs. Second, we develop scalable manifold optimization algorithms for learning hyperbolic space models based on samples from a hypergraph. Third, we show that the positions of units are identifiable (up to rotations) and provide non-asymptotic theoretical guarantees based on samples from a hypergraph. We use the framework to detect core-periphery structure along with proximity among U.S. politicians based on historical media reports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07031v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cornelius Fritz, Yubai Yuan, Michael Schweinberger</dc:creator>
    </item>
    <item>
      <title>Posterior Summarization for Variable Selection in Bayesian Tree Ensembles</title>
      <link>https://arxiv.org/abs/2509.07121</link>
      <description>arXiv:2509.07121v1 Announce Type: new 
Abstract: Variable selection remains a fundamental challenge in statistics, especially in nonparametric settings where model complexity can obscure interpretability. Bayesian tree ensembles, particularly the popular Bayesian additive regression trees (BART) and their rich variants, offer strong predictive performance with interpretable variable importance measures. We modularize Bayesian tree ensemble methods into model choice (tree prior) and posterior summary, and provide a unified review of existing strategies, including permutation-based inference, sparsity-inducing priors, and clustering-based selection. Though typically framed as a modeling task, we show that variable selection with tree ensembles often hinges on posterior summarization, which remains underexplored. To this end, we introduce the VC-measure (Variable Count and its rank variant) with a clustering-based threshold. This posterior summary is a simple, tuning-free plug-in that requires no additional sampling beyond standard model fitting, integrates with any BART variant, and avoids the instability of the median probability model and the computational cost of permutations. In extensive experiments, it yields uniform $F_1$ gains for both general-purpose and sparsity-inducing priors; when paired with the Dirichlet Additive Regression Tree (DART), it overcomes pitfalls of the original summary and attains the best overall balance of recall, precision, and efficiency. Practical guidance on aligning summaries and downstream goals is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07121v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shengbin Ye, Meng Li</dc:creator>
    </item>
    <item>
      <title>GS-BART: Bayesian Additive Regression Trees with Graph-split Decision Rules</title>
      <link>https://arxiv.org/abs/2509.07166</link>
      <description>arXiv:2509.07166v1 Announce Type: new 
Abstract: Ensemble decision tree methods such as XGBoost, Random Forest, and Bayesian Additive Regression Trees (BART) have gained enormous popularity in data science for their superior performance in machine learning regression and classification tasks. In this paper, we introduce a new Bayesian graph-split additive decision tree method, GS-BART, designed to enhance the performance of axis-parallel split-based BART for dependent data with graph structures. The proposed approach encodes input feature information into candidate graph sets and employs a flexible split rule that respects the graph topology when constructing decision trees. We consider a generalized nonparametric regression model using GS-BART and design a scalable informed MCMC algorithm to sample the decision trees of GS-BART. The algorithm leverages a gradient-based recursive algorithm on root directed spanning trees or chains. The superior performance of the method over conventional ensemble tree models and Gaussian process regression models is illustrated in various regression and classification tasks for spatial and network data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07166v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuren He, Huiyan Sang, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>Z-Curve Plot: A Visual Diagnostic for Publication Bias in Meta-Analysis</title>
      <link>https://arxiv.org/abs/2509.07171</link>
      <description>arXiv:2509.07171v1 Announce Type: new 
Abstract: Publication bias undermines meta-analytic inference, yet visual diagnostics for detecting model misfit due to publication bias are lacking. We propose the z-curve plot, a publication-bias-focused absolute model fit diagnostic. The z-curve plot overlays the model-implied posterior predictive distribution of $z$-statistics on the observed distribution of z-statistics and enables direct comparison across candidate meta-analytic models (e.g., random-effects meta-analysis, selection models, PET-PEESE, and RoBMA). Models that approximate the data well show minimal discrepancy between the observed and predicted distribution of z-statistics, whereas poor-fitting models show pronounced discrepancies. Discontinuities at significance thresholds or at zero provide visual evidence of publication bias; models that account for this bias track these discontinuities in the observed distribution. We further extrapolate the estimated publication-bias-adjusted models to the absence of publication bias to obtain a posterior predictive distribution in the absence of publication bias. From this extrapolation, we derive three summaries: the expected discovery rate (EDR), the false discovery risk (FDR), and the expected number of missing studies (N missing). We demonstrate the visualization and its interpretation using simulated datasets and four meta-analyses spanning different degrees of publication bias. The method is implemented in the RoBMA R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07171v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franti\v{s}ek Barto\v{s}, Ulrich Schimmack</dc:creator>
    </item>
    <item>
      <title>Nonparametric Envelopes for Flexible Response Reduction</title>
      <link>https://arxiv.org/abs/2509.07248</link>
      <description>arXiv:2509.07248v1 Announce Type: new 
Abstract: Envelope methods improve the estimation efficiency in multivariate linear regression by identifying and separating the material and immaterial parts of the responses or the predictors and estimating the regression coefficients using only the material part. Though envelopes have been extended to other models, such as GLMs, these extensions still largely fall under the restrictive parametric modeling framework. In this paper, we introduce a flexible, nonparametric extension of response envelopes for improving efficiency in nonlinear multivariate regressions. We propose the kernel envelope (KENV) estimator for simultaneously estimating the response envelope subspace and the enveloped nonparametric conditional mean function in a reproducing kernel Hilbert space, with a novel penalty that accounts for the envelope structure. We prove that the prediction risk for KENV converges to the optimal risk as the sample size diverges and show that KENV achieves a lower in-sample prediction risk than kernel ridge regression when the response has a non-trivial immaterial component. We compare the prediction performance of KENV with other envelope methods and kernel regression methods in simulations and a real data example, finding that KENV delivers more accurate predictions than both the envelope-based and kernel-based alternatives in both low and high dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07248v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tate Jacobson</dc:creator>
    </item>
    <item>
      <title>Monitoring Adverse Events Through Bayesian Nonparametric Clustering Across Studies</title>
      <link>https://arxiv.org/abs/2509.07267</link>
      <description>arXiv:2509.07267v1 Announce Type: new 
Abstract: We introduce a Bayesian nonparametric inference approach for aggregate adverse event (AE) monitoring across studies. The proposed model seamlessly integrates external data from historical trials to define a relevant background rate and accommodates varying levels of covariate granularity (ranging from patient-level details to study-level aggregated summary data). Inference is based on a covariate-dependent product partition model (PPMx). A central element of the model is the ability to group experimental units with similar profiles. We introduce a pairwise similarity measure, with which we set up a random partition of experimental units with comparable covariate profiles, thereby improving the precision of AE rate estimation. Importantly, the proposed framework supports real-time safety monitoring under blinding with a seamless transition to unblinded analyses when indicated. Using one case study and simulation studies, we demonstrate the model's ability to detect safety signals and assess risk under diverse trial scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07267v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijie Yuan, Kevin Roberts, Noirrit Kiran Chandra, Yuan Ji, Peter M\"uller</dc:creator>
    </item>
    <item>
      <title>An Extension of the d-Variate FGM Copula with Application</title>
      <link>https://arxiv.org/abs/2509.07281</link>
      <description>arXiv:2509.07281v1 Announce Type: new 
Abstract: We introduce an extended d-variate Farlie-Gumbel-Morgenstern (FGM) copula that incorporates additional parameters based on Legendre polynomials to enhance the representation of multivariate dependence structures. Within an i.i.d. framework, we derive closed-form estimators for these parameters and establish their unbiasedness, consistency, and asymptotic normality. A simulation study illustrates the finite-sample performance of the estimators. The model is applied to the Bearing dataset, previously studied by Ota and Kimura (2021) through a d-variate FGM copula and by Longla and Mous-Abou (2025) using an extended bivariate FGM copula. Our analysis shows that the classical d-variate FGM copula does not adequately represent the dependence in this dataset. Based on estimation results and model selection criteria, we propose a reduced version of the extended model as a more appropriate copula specification for the Bearing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07281v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mous-Abou Hamadou, Martial Longla</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning for Estimating Time-Varying Delayed and Instantaneous Effects Using Digital Phenotypes</title>
      <link>https://arxiv.org/abs/2509.07322</link>
      <description>arXiv:2509.07322v1 Announce Type: new 
Abstract: Mobile health (mHealth) leverages digital technologies, such as mobile phones, to capture objective, frequent, and real-world digital phenotypes from individuals, enabling the delivery of tailored interventions to accommodate substantial between-subject and temporal heterogeneity. However, evaluating heterogeneous treatment effects from digital phenotype data is challenging due to the dynamic nature of treatments and the presence of delayed effects that extend beyond immediate responses. Additionally, modeling observational data is complicated by confounding factors. To address these challenges, we propose a double machine learning (DML) method designed to estimate both time-varying instantaneous and delayed treatment effects using digital phenotypes. Our approach uses a sequential procedure to estimate the treatment effects based on a DML estimator to ensure Neyman orthogonality. We establish the asymptotic normality of the proposed estimator. Extensive simulation studies validate the finite-sample performance of our approach, demonstrating the advantages of DML and the decomposition of treatment effects. We apply our method to an mHealth study on Parkinson's disease (PD), where we find that the treatment is significantly more effective for younger PD patients and maintains greater stability over time for individuals with low motor fluctuations. These findings demonstrate the utility of our proposed method in advancing precision medicine in mHealth studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07322v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingche Guo, Zexi Cai, Yuanjia Wang, Donglin Zeng</dc:creator>
    </item>
    <item>
      <title>Bias reduction in g-computation for covariate adjustment in randomized clinical trials</title>
      <link>https://arxiv.org/abs/2509.07369</link>
      <description>arXiv:2509.07369v1 Announce Type: new 
Abstract: G-computation is a powerful method for estimating unconditional treatment effects with covariate adjustment in randomized clinical trials. It typically relies on fitting canonical generalized linear models. However, this could be problematic for small sample sizes or in the presence of rare events. Common issues include underestimation of the variance and the potential nonexistence of maximum likelihood estimators. Bias reduction methods are commonly employed to address these issues, including Firth correction which guarantees the existence of corresponding estimates. Yet, their application within g-computation remains underexplored. In this article, we analyze the asymptotic bias of g-computation estimators and propose a novel bias-reduction method that improves both estimation and inference. Our approach performs a debiasing surgery via generalized Oaxaca-Blinder estimators and thus the resulting estimators are guaranteed to be bounded. The proposed debiased estimators use slightly modified versions of maximum likelihood or Firth correction estimators for nuisance parameters. Inspired by the proposed debiased estimators, we also introduce a simple small-sample bias adjustment for variance estimation, further improving finite-sample inference validity. Through extensive simulations, we demonstrate that our proposed method offers superior finite-sample performance, effectively addressing the bias-efficiency tradeoff. Finally, we illustrate its practical utility by reanalyzing a completed randomized clinical trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07369v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Zhang, Lin Liu, Haitao Chu</dc:creator>
    </item>
    <item>
      <title>Bayesian Pliable Lasso with Horseshoe Prior for Interaction Effects in GLMs with Missing Responses</title>
      <link>https://arxiv.org/abs/2509.07501</link>
      <description>arXiv:2509.07501v1 Announce Type: new 
Abstract: Sparse regression problems, where the goal is to identify a small set of relevant predictors, often require modeling not only main effects but also meaningful interactions through other variables. While the pliable lasso has emerged as a powerful frequentist tool for modeling such interactions under strong heredity constraints, it lacks a natural framework for uncertainty quantification and incorporation of prior knowledge. In this paper, we propose a Bayesian pliable lasso that extends this approach by placing sparsity-inducing priors, such as the horseshoe, on both main and interaction effects. The hierarchical prior structure enforces heredity constraints while adaptively shrinking irrelevant coefficients and allowing important effects to persist. We extend this framework to Generalized Linear Models (GLMs) and develop a tailored approach to handle missing responses. To facilitate posterior inference, we develop an efficient Gibbs sampling algorithm based on a reparameterization of the horseshoe prior. Our Bayesian framework yields sparse, interpretable interaction structures, and principled measures of uncertainty. Through simulations and real-data studies, we demonstrate its advantages over existing methods in recovering complex interaction patterns under both complete and incomplete data.
  Our method is implemented in the package \texttt{hspliable} available on Github.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07501v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Adaptive clinical trial design with delayed treatment effects using elicited prior distributions</title>
      <link>https://arxiv.org/abs/2509.07602</link>
      <description>arXiv:2509.07602v1 Announce Type: new 
Abstract: Clinical trials with time-to-event endpoints, such as overall survival (OS) or progression-free survival (PFS), are fundamental for evaluating new treatments, particularly in immuno-oncology. However, modern therapies, such as immunotherapies and targeted treatments, often exhibit delayed effects that challenge traditional trial designs. These delayed effects violate the proportional hazards assumption, which underpins standard statistical methods like the Cox proportional hazards model and the log-rank test. Careful planning is essential to ensure trials are appropriately designed to account for the timing and magnitude of these effects. Without this planning, interim analyses may lead to premature trial termination if the treatment effect is underestimated early in the study. We present an adaptive trial design framework that incorporates prior distributions, elicited from experts, for delayed treatment effects. By addressing the uncertainty surrounding delayed treatment effects, our approach enhances trial efficiency and robustness, minimizing the risk of premature termination and improving the detection of treatment benefits over time. We present an example illustrating how interim analyses, informed by prior distributions, can guide early stopping decisions. To facilitate the implementation of our framework, we have developed free, open-source software that enables researchers to integrate prior distributions into trial planning and decision-making. This software provides a flexible, accessible tool for designing trials that more accurately evaluate modern therapies through adaptive trial designs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07602v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Salsbury, Jeremy Oakley, Steven Julious, Lisa Hampson</dc:creator>
    </item>
    <item>
      <title>Clustering methods for Categorical Time Series and Sequences : A scoping review</title>
      <link>https://arxiv.org/abs/2509.07885</link>
      <description>arXiv:2509.07885v1 Announce Type: new 
Abstract: Objective: To provide an overview of clustering methods for categorical time series (CTS), a data structure commonly found in epidemiology, sociology, biology, and marketing, and to support method selection in regards to data characteristics.
  Methods: We searched PubMed, Web of Science, and Google Scholar, from inception up to November 2024 to identify articles that propose and evaluate clustering techniques for CTS. Methods were classified according to three major families -- distance-based, feature-based, and model-based -- and assessed on their ability to handle data challenges such as variable sequence length, multivariate data, continuous time, missing data, time-invariant covariates, and large data volumes.
  Results: Out of 14607 studies, we included 124 articles describing 129 methods, spanning domains such as artificial intelligence, social sciences, and epidemiology. Distance-based methods, particularly those using Optimal Matching, were most prevalent, with 56 methods. We identified 28 model-based methods, which demonstrated superior flexibility for handling complex data structures such as multivariate data, continuous time and time-invariant covariates. We also recorded 45 feature-based approaches, which were on average more scalable but less flexible. A searchable Web application was developed to facilitate method selection based on dataset characteristics ( https://cts-clustering-scoping-review-7sxqj3sameqvmwkvnzfynz.streamlit.app/ )
  Discussion: While distance-based methods dominate, model-based approaches offer the richest modeling potential but are less scalable. Feature-based methods favor performance over flexibility, with limited support for complex data structures.
  Conclusion: This review highlights methodological diversity and gaps in CTS clustering. The proposed typology aims to guide researchers in selecting appropriate methods for their specific use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07885v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ottavio Khalifa, Viet-Thi Tran, Alan Balendran, Fran\c{c}ois Petit</dc:creator>
    </item>
    <item>
      <title>Sequential Test for Practical Significance: Truncated Mixture Sequential Probability Ratio Test</title>
      <link>https://arxiv.org/abs/2509.07892</link>
      <description>arXiv:2509.07892v1 Announce Type: new 
Abstract: We present a sequential testing method to identify a practically significant effect. We build on the existing mixture sequential probability ratio test (mSPRT) that can sequentially test for a non-zero treatment effect by using a truncated mixing distribution to differentiate between effects that are large enough to merit a real world action versus that are non-zero but too small to merit a real world action. We verify the Type-I error control of our method theoretically and empirically. We also extend this idea to sequentially test for one-sided practical significance such as non-inferiority testing, and show that we may still effectively control the Type-I error rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07892v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Kyu Min Shim</dc:creator>
    </item>
    <item>
      <title>Machine Generalize Learning in Agent-Based Models: Going Beyond Surrogate Models for Calibration in ABMs</title>
      <link>https://arxiv.org/abs/2509.07013</link>
      <description>arXiv:2509.07013v1 Announce Type: cross 
Abstract: Calibrating agent-based epidemic models is computationally demanding. We present a supervised machine learning calibrator that learns the inverse mapping from epidemic time series to SIR parameters. A three-layer bidirectional LSTM ingests 60-day incidence together with population size and recovery rate, and outputs transmission probability, contact rate, and R0. Training uses a composite loss with an epidemiology-motivated consistency penalty that encourages R0 \* recovery rate to equal transmission probability \* contact rate.
  In a 1000-scenario simulation study, we compare the calibrator with Approximate Bayesian Computation (likelihood-free MCMC). The method achieves lower error across all targets (MAE: R0 0.0616 vs 0.275; transmission 0.0715 vs 0.128; contact 1.02 vs 4.24), produces tighter predictive intervals with near nominal coverage, and reduces wall clock time from 77.4 s to 2.35 s per calibration. Although contact rate and transmission probability are partially nonidentifiable, the approach reproduces epidemic curves more faithfully than ABC, enabling fast and practical calibration. We evaluate it on SIR agent based epidemics generated with epiworldR and provide an implementation in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07013v1</guid>
      <category>cs.LG</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sima Najafzadehkhoei, George Vega Yon, Bernardo Modenesi, Derek S. Meyer</dc:creator>
    </item>
    <item>
      <title>Statistical Methods in Generative AI</title>
      <link>https://arxiv.org/abs/2509.07054</link>
      <description>arXiv:2509.07054v1 Announce Type: cross 
Abstract: Generative Artificial Intelligence is emerging as an important technology, promising to be transformative in many areas. At the same time, generative AI techniques are based on sampling from probabilistic models, and by default, they come with no guarantees about correctness, safety, fairness, or other properties. Statistical methods offer a promising potential approach to improve the reliability of generative AI techniques. In addition, statistical methods are also promising for improving the quality and efficiency of AI evaluation, as well as for designing interventions and experiments in AI.
  In this paper, we review some of the existing work on these topics, explaining both the general statistical techniques used, as well as their applications to generative AI. We also discuss limitations and potential future directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07054v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Sequentially Auditing Differential Privacy</title>
      <link>https://arxiv.org/abs/2509.07055</link>
      <description>arXiv:2509.07055v1 Announce Type: cross 
Abstract: We propose a practical sequential test for auditing differential privacy guarantees of black-box mechanisms. The test processes streams of mechanisms' outputs providing anytime-valid inference while controlling Type I error, overcoming the fixed sample size limitation of previous batch auditing methods. Experiments show this test detects violations with sample sizes that are orders of magnitude smaller than existing methods, reducing this number from 50K to a few hundred examples, across diverse realistic mechanisms. Notably, it identifies DP-SGD privacy violations in \textit{under} one training run, unlike prior methods needing full model training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07055v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom\'as Gonz\'alez, Mateo Dulce-Rubio, Aaditya Ramdas, M\'onica Ribero</dc:creator>
    </item>
    <item>
      <title>Self-Normalization for CUSUM-based Change Detection in Locally Stationary Time Series</title>
      <link>https://arxiv.org/abs/2509.07112</link>
      <description>arXiv:2509.07112v1 Announce Type: cross 
Abstract: A novel self-normalization procedure for CUSUM-based change detection in the mean of a locally stationary time series is introduced. Classical self-normalization relies on the factorization of a constant long-run variance and a stochastic factor. In this case, the CUSUM statistic can be divided by another statistic proportional to the long-run variance, so that the latter cancels. Thereby, a tedious estimation of the long-run variance can be avoided. Under local stationarity, the partial sum process converges to $\int_0^t \sigma(x) d B_x$ and no such factorization is possible. To overcome this obstacle, a self-normalized test statistic is constructed from a carefully designed bivariate partial-sum process. Weak convergence of the process is proven, and it is shown that the resulting self-normalized test attains asymptotic level $\alpha$ under the null hypothesis of no change, while being consistent against a broad class of alternatives. Extensive simulations demonstrate better finite-sample properties compared to existing methods. Applications to real data illustrate the method's practical effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07112v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Heinrichs</dc:creator>
    </item>
    <item>
      <title>Besting Good--Turing: Optimality of Non-Parametric Maximum Likelihood for Distribution Estimation</title>
      <link>https://arxiv.org/abs/2509.07355</link>
      <description>arXiv:2509.07355v1 Announce Type: cross 
Abstract: When faced with a small sample from a large universe of possible outcomes, scientists often turn to the venerable Good--Turing estimator. Despite its pedigree, however, this estimator comes with considerable drawbacks, such as the need to hand-tune smoothing parameters and the lack of a precise optimality guarantee. We introduce a parameter-free estimator that bests Good--Turing in both theory and practice. Our method marries two classic ideas, namely Robbins's empirical Bayes and Kiefer--Wolfowitz non-parametric maximum likelihood estimation (NPMLE), to learn an implicit prior from data and then convert it into probability estimates. We prove that the resulting estimator attains the optimal instance-wise risk up to logarithmic factors in the competitive framework of Orlitsky and Suresh, and that the Good--Turing estimator is strictly suboptimal in the same framework. Our simulations on synthetic data and experiments with English corpora and U.S. Census data show that our estimator consistently outperforms both the Good--Turing estimator and explicit Bayes procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07355v1</guid>
      <category>math.ST</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjun Han, Jonathan Niles-Weed, Yandi Shen, Yihong Wu</dc:creator>
    </item>
    <item>
      <title>Scalable Estimation of Multinomial Response Models with Random Consideration Sets</title>
      <link>https://arxiv.org/abs/2308.12470</link>
      <description>arXiv:2308.12470v5 Announce Type: replace 
Abstract: A common assumption in the fitting of unordered multinomial response models for $J$ mutually exclusive categories is that the responses arise from the same set of $J$ categories across subjects. However, when responses measure a choice made by the subject, it is more appropriate to condition the distribution of multinomial responses on a subject-specific consideration set, drawn from the power set of $\{1,2,\ldots,J\}$. This leads to a mixture of multinomial response models governed by a probability distribution over the $J^{\ast} = 2^J -1$ consideration sets. We introduce a novel method for estimating such generalized multinomial response models based on the fundamental result that any mass distribution over $J^{\ast}$ consideration sets can be represented as a mixture of products of $J$ component-specific inclusion-exclusion probabilities. Moreover, under time-invariant consideration sets, the conditional posterior distribution of consideration sets is sparse. These features enable a scalable MCMC algorithm for sampling the posterior distribution of parameters, random effects, and consideration sets. Under regularity conditions, the posterior distributions of the marginal response probabilities and the model parameters satisfy consistency. The methodology is demonstrated in a longitudinal data set on weekly cereal purchases that cover $J = 101$ brands, a dimension substantially beyond the reach of existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.12470v5</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Chib, Kenichi Shimizu</dc:creator>
    </item>
    <item>
      <title>Sequential Change-point Detection for Compositional Time Series with Exogenous Variables</title>
      <link>https://arxiv.org/abs/2402.18130</link>
      <description>arXiv:2402.18130v2 Announce Type: replace 
Abstract: Sequential change-point detection for time series enables us to sequentially check the hypothesis that the model still holds as more and more data are observed. It is widely used in data monitoring in practice. In this work, we consider sequential change-point detection for compositional time series, time series in which the observations are proportions. For fitting compositional time series, we propose a generalized Beta AR(1) model, which can incorporate exogenous variables upon which the time series observations are dependent. We show the compositional time series are strictly stationary and geometrically ergodic and consider maximum likelihood estimation for model parameters. We show the partial MLEs are consistent and asymptotically normal and propose a parametric sequential change-point detection method for the compositional time series model. The change-point detection method is illustrated using a time series of Covid-19 positivity rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18130v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yajun Liu, Beth Andrews</dc:creator>
    </item>
    <item>
      <title>Counterfactual Cocycles: A Framework for Robust and Coherent Counterfactual Transports</title>
      <link>https://arxiv.org/abs/2405.13844</link>
      <description>arXiv:2405.13844v3 Announce Type: replace 
Abstract: Estimating joint distributions (a.k.a. couplings) over counterfactual outcomes is central to personalized decision-making and treatment risk assessment. Two emergent frameworks with identifiability guarantees are: (i) bijective structural causal models (SCMs), which are flexible but brittle to mis-specified latent noise; and (ii) optimal-transport (OT) methods, which avoid latent noise assumptions but can produce incoherent counterfactual transports which fail to identify higher-order couplings. In this work, we bridge the gap with \emph{counterfactual cocycles}: a framework for counterfactual transports that use algebraic structure to provide coherence and identifiability guarantees. Every counterfactual cocycle corresponds to an equivalence class of SCMs, however the cocycle is invariant to the latent noise distribution, enabling us to sidestep various mis-specification problems. We characterize the structure of all identifiable counterfactual cocycles; propose flexible model parameterizations; introduce a novel cocycle estimator that avoids any distributional assumptions; and derive mis-specification robustness properties of the resulting counterfactual inference method. We demonstrate state-of-the-art performance and noise-robustness of counterfactual cocycles across synthetic benchmarks and a 401(k) eligibility study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13844v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hugh Dance, Benjamin Bloem-Reddy</dc:creator>
    </item>
    <item>
      <title>L-Estimation Approach to Tobit Models with Endogeneity and Weakly Dependent Errors</title>
      <link>https://arxiv.org/abs/2405.19145</link>
      <description>arXiv:2405.19145v3 Announce Type: replace 
Abstract: This article introduces an L-estimator for the semiparametric Tobit model with endogenous regressors. The estimation procedure follows a two-stage approach: the first stage employs least squares, while the second stage utilizes the L-estimation technique. We establish the large-sample properties of the proposed estimators under weakly dependent data. The utility of the proposed methodology is demonstrated for various simulated data and a benchmark real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19145v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Swati Shukla, Subhra Sankar Dhar,  Shalabh</dc:creator>
    </item>
    <item>
      <title>Improving the Estimation of Lifetime Effects in A/B Testing via Treatment Locality</title>
      <link>https://arxiv.org/abs/2407.19618</link>
      <description>arXiv:2407.19618v3 Announce Type: replace 
Abstract: Utilizing randomized experiments to evaluate the effect of short-term treatments on the short-term outcomes has been well understood and become the golden standard in industrial practice. However, as service systems become increasingly dynamical and personalized, much focus is shifting toward maximizing long-term outcomes, such as customer lifetime value, through lifetime exposure to interventions. Our goal is to assess the impact of treatment and control policies on long-term outcomes from relatively short-term observations, such as those generated by A/B testing. A key managerial observation is that many practical treatments are local, affecting only targeted states while leaving other parts of the policy unchanged. This paper rigorously investigates whether and how such locality can be exploited to improve estimation of long-term effects in Markov Decision Processes (MDPs), a fundamental model of dynamic systems. We first develop optimal inference techniques for general A/B testing in MDPs and establish corresponding efficiency bounds. We then propose methods to harness the localized structure by sharing information on the non-targeted states. Our new estimator can achieve a linear reduction with the number of test arms for a major part of the variance without sacrificing unbiasedness. It also matches a tighter variance lower bound that accounts for locality. Furthermore, we extend our framework to a broad class of differentiable estimators, which encompasses many widely used approaches in practice. We show that all such estimators can benefit from variance reduction through information sharing without increasing their bias. Together, these results provide both theoretical foundations and practical tools for conducting efficient experiments in dynamic service systems with local treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19618v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuze Chen, David Simchi-Levi, Chonghuan Wang</dc:creator>
    </item>
    <item>
      <title>Generalized Tensor Completion with Non-Random Missingness</title>
      <link>https://arxiv.org/abs/2509.06225</link>
      <description>arXiv:2509.06225v2 Announce Type: replace 
Abstract: Tensor completion plays a crucial role in applications such as recommender systems and medical imaging, where data are often highly incomplete. While extensive prior work has addressed tensor completion with data missingness, most assume that each entry of the tensor is available independently with probability $p$. However, real-world tensor data often exhibit missing-not-at-random (MNAR) patterns, where the probability of missingness depends on the underlying tensor values. This paper introduces a generalized tensor completion framework for noisy data with MNAR, where the observation probability is modeled as a function of underlying tensor values. Our flexible framework accommodates various tensor data types, such as continuous, binary and count data. For model estimation, we develop an alternating maximization algorithm and derive non-asymptotic error bounds for the estimator at each iteration, under considerably relaxed conditions on the observation probabilities. Additionally, we propose a statistical inference procedure to test whether observation probabilities depend on underlying tensor values, offering a formal assessment of the missingness assumption within our modeling framework. The utility and efficacy of our approach are demonstrated through comparative simulation studies and analyses of two real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06225v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maoyu Zhang, Biao Cai, Will Wei Sun, Jingfei Zhang</dc:creator>
    </item>
    <item>
      <title>The exact non-Gaussian weak lensing likelihood: A framework to calculate analytic likelihoods for correlation functions on masked Gaussian random fields</title>
      <link>https://arxiv.org/abs/2407.08718</link>
      <description>arXiv:2407.08718v2 Announce Type: replace-cross 
Abstract: We present exact non-Gaussian joint likelihoods for auto- and cross-correlation functions on arbitrarily masked spherical Gaussian random fields. Our considerations apply to spin-0 as well as spin-2 fields but are demonstrated here for the spin-2 weak-lensing correlation function.
  We motivate that this likelihood cannot be Gaussian and show how it can nevertheless be calculated exactly for any mask geometry and on a curved sky, as well as jointly for different angular-separation bins and redshift-bin combinations. Splitting our calculation into a large- and small-scale part, we apply a computationally efficient approximation for the small scales that does not alter the overall non-Gaussian likelihood shape.
  To compare our exact likelihoods to correlation-function sampling distributions, we simulated a large number of weak-lensing maps, including shape noise, and find excellent agreement for one-dimensional as well as two-dimensional distributions. Furthermore, we compare the exact likelihood to the widely employed Gaussian likelihood and find significant levels of skewness at angular separations $\gtrsim 1^{\circ}$, such that the mode of the exact distributions is shifted away from the mean towards lower values of the correlation function. We find that the assumption of a Gaussian random field for the weak-lensing field is well valid at these angular separations.
  Considering the skewness of the non-Gaussian likelihood, we evaluate its impact on the posterior constraints on $S_8$. On a simplified weak-lensing-survey setup with an area of $10 \ 000 \ \mathrm{deg}^2$, we find that the posterior mean of $S_8$ is around $2.5\%$ higher when using the non-Gaussian likelihood, a shift comparable to the precision of current stage-III surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08718v2</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 10 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.33232/001c.144235</arxiv:DOI>
      <arxiv:journal_reference>The Open Journal of Astrophysics, Vol. 8, 2025</arxiv:journal_reference>
      <dc:creator>Veronika Oehl, Tilman Tr\"oster</dc:creator>
    </item>
  </channel>
</rss>
