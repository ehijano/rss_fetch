<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Oct 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Lincoln AI Computing Survey (LAICS) and Trends</title>
      <link>https://arxiv.org/abs/2510.20931</link>
      <description>arXiv:2510.20931v1 Announce Type: new 
Abstract: In the past year, generative AI (GenAI) models have received a tremendous amount of attention, which in turn has increased attention to computing systems for training and inference for GenAI. Hence, an update to this survey is due. This paper is an update of the survey of AI accelerators and processors from past seven years, which is called the Lincoln AI Computing Survey -- LAICS (pronounced "lace"). This multi-year survey collects and summarizes the current commercial accelerators that have been publicly announced with peak performance and peak power consumption numbers. In the same tradition of past papers of this survey, the performance and power values are plotted on a scatter graph, and a number of dimensions and observations from the trends on this plot are again discussed and analyzed. Market segments are highlighted on the scatter plot, and zoomed plots of each segment are also included. A brief description of each of the new accelerators that have been added in the survey this year is included, and this update features a new categorization of computing architectures that implement each of the accelerators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20931v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Albert Reuther, Peter Michaleas, Michael Jones, Vijay Gadepally, Jeremy Kepner</dc:creator>
    </item>
    <item>
      <title>Towards Straggler-Resilient Split Federated Learning: An Unbalanced Update Approach</title>
      <link>https://arxiv.org/abs/2510.21155</link>
      <description>arXiv:2510.21155v1 Announce Type: new 
Abstract: Split Federated Learning (SFL) enables scalable training on edge devices by combining the parallelism of Federated Learning (FL) with the computational offloading of Split Learning (SL). Despite its great success, SFL suffers significantly from the well-known straggler issue in distributed learning systems. This problem is exacerbated by the dependency between Split Server and clients: the Split Server side model update relies on receiving activations from clients. Such synchronization requirement introduces significant time latency, making straggler a critical bottleneck to the scalability and efficiency of the system. To mitigate this problem, we propose MU-SplitFed, a straggler-resilient SFL algorithm in zeroth-order optimization that decouples training progress from straggler delays via a simple yet effective unbalanced update mechanism.
  By enabling the server to perform $\tau$ local updates per client round, MU-SplitFed achieves a convergence rate of $O(\sqrt{d/(\tau T)})$ for non-convex objectives, demonstrating a linear speedup of $\tau$ in communication rounds. Experiments demonstrate that MU-SplitFed consistently outperforms baseline methods with the presence of stragglers and effectively mitigates their impact through adaptive tuning of $\tau$. The code for this project is available at https://github.com/Johnny-Zip/MU-SplitFed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21155v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dandan Liang, Jianing Zhang, Evan Chen, Zhe Li, Rui Li, Haibo Yang</dc:creator>
    </item>
    <item>
      <title>From SLA to vendor-neutral metrics: An intelligent knowledge-based approach for multi-cloud SLA-based broker</title>
      <link>https://arxiv.org/abs/2510.21173</link>
      <description>arXiv:2510.21173v1 Announce Type: new 
Abstract: Cloud computing has been consolidated as a support for the vast majority of current and emerging technologies. However, there are some barriers that prevent the exploitation of the full potential of this technology. First, the major cloud providers currently put the onus of implementing the mechanisms that ensure compliance with the desired service levels on cloud consumers. However, consumers do not have the required expertise. Since each cloud provider exports a different set of low-level metrics, the strategies defined to ensure compliance with the established service-level agreement (SLA) are bound to a particular cloud provider. This fosters provider lock-in and prevents consumers from benefiting from the advantages of multi-cloud environments. This paper presents a solution to the problem of automatically translating SLAs into objectives expressed as metrics that can be measured across multiple cloud providers. First, we propose an intelligent knowledge-based system capable of automatically translating high-level SLAs defined by cloud consumers into a set of conditions expressed as vendor-neutral metrics, providing feedback to cloud consumers (intelligent tutoring system). Secondly, we present the set of vendor-neutral metrics and explain how they can be measured for the different cloud providers. Finally, we report a validation based on two use cases (IaaS and PaaS) in a multi-cloud environment formed by leading cloud providers. This evaluation has demonstrated that, thanks to the complementarity of the two solutions, cloud consumers can automatically and transparently exploit the multi-cloud in many application domains, as endorsed by the cloud experts consulted in the course of this study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21173v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1002/int.22638</arxiv:DOI>
      <arxiv:journal_reference>Int J Intell Syst. 2022; 37: 10533-10575</arxiv:journal_reference>
      <dc:creator>V\'ictor Ramp\'erez, Javier Soriano, David Lizcano, Shadi Aljawarneh, Juan A. Lara</dc:creator>
    </item>
    <item>
      <title>Generative Federated Learning for Smart Prediction and Recommendation Applications</title>
      <link>https://arxiv.org/abs/2510.21183</link>
      <description>arXiv:2510.21183v1 Announce Type: new 
Abstract: This paper proposes a generative adversarial network and federated learning-based model to address various challenges of the smart prediction and recommendation applications, such as high response time, compromised data privacy, and data scarcity. The integration of the generative adversarial network and federated learning is referred to as Generative Federated Learning (GFL). As a case study of the proposed model, a heart health monitoring application is considered. The realistic synthetic datasets are generated using the generated adversarial network-based proposed algorithm for improving data diversity, data quality, and data augmentation, and remove the data scarcity and class imbalance issues. In this paper, we implement the centralized and decentralized federated learning approaches in an edge computing paradigm. In centralized federated learning, the edge nodes communicate with the central server to build the global and personalized local models in a collaborative manner. In the decentralized federated learning approach, the edge nodes communicate among themselves to exchange model updates for collaborative training. The comparative study shows that the proposed framework outperforms the existing heart health monitoring applications. The results show that using the proposed framework (i) the prediction accuracy is improved by 12% than the conventional framework, and (ii) the response time is reduced by 73% than the conventional cloud-only system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21183v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anwesha Mukherjee, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Arbitration-Free Consistency is Available (and Vice Versa)</title>
      <link>https://arxiv.org/abs/2510.21304</link>
      <description>arXiv:2510.21304v1 Announce Type: new 
Abstract: The fundamental tension between \emph{availability} and \emph{consistency} shapes the design of distributed storage systems. Classical results capture extreme points of this trade-off: the CAP theorem shows that strong models like linearizability preclude availability under partitions, while weak models like causal consistency remain implementable without coordination. These theorems apply to simple read-write interfaces, leaving open a precise explanation of the combinations of object semantics and consistency models that admit available implementations.
  This paper develops a general semantic framework in which storage specifications combine operation semantics and consistency models. The framework encompasses a broad range of objects (key-value stores, counters, sets, CRDTs, and transactional databases) and consistency models (from causal consistency and sequential consistency to snapshot isolation and transactional and non-transactional SQL).
  Within this framework, we prove the \emph{Arbitration-Free Consistency} (AFC) theorem, showing that an object specification within a consistency model admits an available implementation if and only if it is \emph{arbitration-free}, that is, it does not require a total arbitration order to resolve visibility or read dependencies.
  The AFC theorem unifies and generalizes previous results, revealing arbitration-freedom as the fundamental property that delineates coordination-free consistency from inherently synchronized behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21304v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>POPL 2026 Full Version</arxiv:journal_reference>
      <dc:creator>Hagit Attiya, Constantin Enea, Enrique Rom\'an-Calvo</dc:creator>
    </item>
    <item>
      <title>Parsley's Group Size Study</title>
      <link>https://arxiv.org/abs/2510.21348</link>
      <description>arXiv:2510.21348v1 Announce Type: new 
Abstract: Parsley is a resilient group-based Distributed Hash Table that incorporates a preemptive peer relocation technique and a dynamic data sharding mechanism to enhance robustness and balance. In addition to the hard limits on group size, defined by minimum and maximum thresholds, Parsley introduces two soft limits that define a target interval for maintaining stable group sizes. These soft boundaries allow the overlay to take proactive measures to prevent violations of the hard limits, improving system stability under churn. This work provides an in-depth analysis of the rationale behind the parameter values adopted for Parsley's evaluation. Unlike related systems, which specify group size limits without justification, we conduct a systematic overlay characterization study to understand the effects of these parameters on performance and scalability. The study examines topology operations, the behavior of large groups, and the overall trade-offs observed, offering a grounded explanation for the chosen configuration values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21348v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao A. Silva, Herv\'e Paulino, Jo\~ao M. Louren\c{c}o</dc:creator>
    </item>
    <item>
      <title>LIDC: A Location Independent Multi-Cluster Computing Framework for Data Intensive Science</title>
      <link>https://arxiv.org/abs/2510.21373</link>
      <description>arXiv:2510.21373v1 Announce Type: new 
Abstract: Scientific communities are increasingly using geographically distributed computing platforms. The current methods of compute placement predominantly use logically centralized controllers such as Kubernetes (K8s) to match tasks to available resources. However, this centralized approach is unsuitable in multi-organizational collaborations. Furthermore, workflows often need to use manual configurations tailored for a single platform and cannot adapt to dynamic changes across infrastructure. Our work introduces a decentralized control plane for placing computations on geographically dispersed compute clusters using semantic names. We assign semantic names to computations to match requests with named Kubernetes (K8s) service endpoints. We show that this approach provides multiple benefits. First, it allows placement of computational jobs to be independent of location, enabling any cluster with sufficient resources to execute the computation. Second, it facilitates dynamic compute placement without requiring prior knowledge of cluster locations or predefined configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21373v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/SCW63240.2024.00108</arxiv:DOI>
      <dc:creator>Sankalpa Timilsina, Susmit Shannigrahi</dc:creator>
    </item>
    <item>
      <title>Learning to Schedule: A Supervised Learning Framework for Network-Aware Scheduling of Data-Intensive Workloads</title>
      <link>https://arxiv.org/abs/2510.21419</link>
      <description>arXiv:2510.21419v1 Announce Type: new 
Abstract: Distributed cloud environments hosting data-intensive applications often experience slowdowns due to network congestion, asymmetric bandwidth, and inter-node data shuffling. These factors are typically not captured by traditional host-level metrics like CPU or memory. Scheduling without accounting for these conditions can lead to poor placement decisions, longer data transfers, and suboptimal job performance. We present a network-aware job scheduler that uses supervised learning to predict the completion time of candidate jobs. Our system introduces a prediction-and-ranking mechanism that collects real-time telemetry from all nodes, uses a trained supervised model to estimate job duration per node, and ranks them to select the best placement. We evaluate the scheduler on a geo-distributed Kubernetes cluster deployed on the FABRIC testbed by running network-intensive Spark workloads. Compared to the default Kubernetes scheduler, which makes placement decisions based on current resource availability alone, our proposed supervised scheduler achieved 34-54% higher accuracy in selecting optimal nodes for job placement. The novelty of our work lies in the demonstration of supervised learning for real-time, network-aware job scheduling on a multi-site cluster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21419v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sankalpa Timilsina, Susmit Shannigrahi</dc:creator>
    </item>
    <item>
      <title>On Reduction and Synthesis of Petri's Cycloids</title>
      <link>https://arxiv.org/abs/2510.21493</link>
      <description>arXiv:2510.21493v1 Announce Type: new 
Abstract: Cycloids are particular Petri nets for modelling processes of actions and events, belonging to the fundaments of Petri's general systems theory. Defined by four parameters they provide an algebraic formalism to describe strongly synchronized sequential processes. To further investigate their structure, reduction systems of cycloids are defined in the style of rewriting systems and properties of irreducible cycloids are proved. In particular the synthesis of cycloid parameters from their Petri net structure is derived, leading to an efficient method for a decision procedure for cycloid isomorphism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21493v1</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R\"udiger Valk, Daniel Moldt</dc:creator>
    </item>
    <item>
      <title>Distributed $(\Delta+1)$-Coloring in Graphs of Bounded Neighborhood Independence</title>
      <link>https://arxiv.org/abs/2510.21549</link>
      <description>arXiv:2510.21549v1 Announce Type: new 
Abstract: The distributed coloring problem is arguably one of the key problems studied in the area of distributed graph algorithms. The most standard variant of the problem asks for a proper vertex coloring of a graph with $\Delta+1$ colors, where $\Delta$ is the maximum degree of the graph. Despite an immense amount of work on distributed coloring problems in the distributed setting, determining the deterministic complexity of $(\Delta+1)$-coloring in the standard message passing model remains one of the most important open questions of the area. In this paper, we aim to improve our understanding of the deterministic complexity of $(\Delta+1)$-coloring as a function of $\Delta$ in a special family of graphs for which significantly faster algorithms are already known. The neighborhood independence $\theta$ of a graph is the maximum number of pairwise non-adjacent neighbors of some node of the graph. In general, in graphs of neighborhood independence $\theta=O(1)$ (e.g., line graphs), it is known that $(\Delta+1)$-coloring can be solved in $2^{O(\sqrt{\log\Delta})}+O(\log^* n)$ rounds. In the present paper, we significantly improve this result, and we show that in graphs of neighborhood independence $\theta$, a $(\Delta+1)$-coloring can be computed in $(\theta\cdot\log\Delta)^{O(\log\log\Delta / \log\log\log\Delta)}+O(\log^* n)$ rounds and thus in quasipolylogarithmic time in $\Delta$ as long as $\theta$ is at most polylogarithmic in $\Delta$. We also show that the known approach that leads to a polylogarithmic in $\Delta$ algorithm for $(2\Delta-1)$-edge coloring already fails for edge colorings of hypergraphs of rank at least $3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21549v1</guid>
      <category>cs.DC</category>
      <category>cs.CC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marc Fuchs, Fabian Kuhn</dc:creator>
    </item>
    <item>
      <title>JSTprove: Pioneering Verifiable AI for a Trustless Future</title>
      <link>https://arxiv.org/abs/2510.21024</link>
      <description>arXiv:2510.21024v1 Announce Type: cross 
Abstract: The integration of machine learning (ML) systems into critical industries such as healthcare, finance, and cybersecurity has transformed decision-making processes, but it also brings new challenges around trust, security, and accountability. As AI systems become more ubiquitous, ensuring the transparency and correctness of AI-driven decisions is crucial, especially when they have direct consequences on privacy, security, or fairness. Verifiable AI, powered by Zero-Knowledge Machine Learning (zkML), offers a robust solution to these challenges. zkML enables the verification of AI model inferences without exposing sensitive data, providing an essential layer of trust and privacy. However, traditional zkML systems typically require deep cryptographic expertise, placing them beyond the reach of most ML engineers. In this paper, we introduce JSTprove, a specialized zkML toolkit, built on Polyhedra Network's Expander backend, to enable AI developers and ML engineers to generate and verify proofs of AI inference. JSTprove provides an end-to-end verifiable AI inference pipeline that hides cryptographic complexity behind a simple command-line interface while exposing auditable artifacts for reproducibility. We present the design, innovations, and real-world use cases of JSTprove as well as our blueprints and tooling to encourage community review and extension. JSTprove therefore serves both as a usable zkML product for current engineering needs and as a reproducible foundation for future research and production deployments of verifiable AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21024v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonathan Gold, Tristan Freiberg, Haruna Isah, Shirin Shahabi</dc:creator>
    </item>
    <item>
      <title>xMem: A CPU-Based Approach for Accurate Estimation of GPU Memory in Deep Learning Training Workloads</title>
      <link>https://arxiv.org/abs/2510.21048</link>
      <description>arXiv:2510.21048v1 Announce Type: cross 
Abstract: The global scarcity of GPUs necessitates more sophisticated strategies for Deep Learning jobs in shared cluster environments. Accurate estimation of how much GPU memory a job will require is fundamental to enabling advanced scheduling and GPU sharing, which helps prevent out-of-memory (OOM) errors and resource underutilization. However, existing estimation methods have limitations. Approaches relying on static analysis or historical data with machine learning often fail to accurately capture runtime dynamics. Furthermore, direct GPU analysis consumes scarce resources, and some techniques require intrusive code modifications. Thus, the key challenge lies in precisely estimating dynamic memory requirements, including memory allocator nuances, without consuming GPU resources and non-intrusive code changes. To address this challenge, we propose xMem, a novel framework that leverages CPU-only dynamic analysis to accurately estimate peak GPU memory requirements a priori. We conducted a thorough evaluation of xMem against state-of-the-art solutions using workloads from 25 different models, including architectures like Convolutional Neural Networks and Transformers. The analysis of 5209 runs, which includes ANOVA and Monte Carlo results, highlights xMem's benefits: it decreases the median relative error by 91% and significantly reduces the probability of estimation failure as safe OOM thresholds by 75%, meaning that the estimated value can often be used directly without causing OOM. Ultimately, these improvements lead to a 368% increase in memory conservation potential over current solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21048v1</guid>
      <category>cs.PF</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3721462.3770773</arxiv:DOI>
      <dc:creator>Jiabo Shi, Dimitrios Pezaros, Yehia Elkhatib</dc:creator>
    </item>
    <item>
      <title>Accelerating Mobile Inference through Fine-Grained CPU-GPU Co-Execution</title>
      <link>https://arxiv.org/abs/2510.21081</link>
      <description>arXiv:2510.21081v1 Announce Type: cross 
Abstract: Deploying deep neural networks on mobile devices is increasingly important but remains challenging due to limited computing resources. On the other hand, their unified memory architecture and narrower gap between CPU and GPU performance provide an opportunity to reduce inference latency by assigning tasks to both CPU and GPU. The main obstacles for such collaborative execution are the significant synchronization overhead required to combine partial results, and the difficulty of predicting execution times of tasks assigned to CPU and GPU (due to the dynamic selection of implementations and parallelism level). To overcome these obstacles, we propose both a lightweight synchronization mechanism based on OpenCL fine-grained shared virtual memory (SVM) and machine learning models to accurately predict execution times. Notably, these models capture the performance characteristics of GPU kernels and account for their dispatch times. A comprehensive evaluation on four mobile platforms shows that our approach can quickly select CPU-GPU co-execution strategies achieving up to 1.89x speedup for linear layers and 1.75x speedup for convolutional layers (close to the achievable maximum values of 2.01x and 1.87x, respectively, found by exhaustive grid search on a Pixel~5 smartphone).</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21081v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhuojin Li, Marco Paolieri, Leana Golubchik</dc:creator>
    </item>
    <item>
      <title>Sensing and Storing Less: A MARL-based Solution for Energy Saving in Edge Internet of Things</title>
      <link>https://arxiv.org/abs/2510.21103</link>
      <description>arXiv:2510.21103v1 Announce Type: cross 
Abstract: As the number of Internet of Things (IoT) devices continuously grows and application scenarios constantly enrich, the volume of sensor data experiences an explosive increase. However, substantial data demands considerable energy during computation and transmission. Redundant deployment or mobile assistance is essential to cover the target area reliably with fault-prone sensors. Consequently, the ``butterfly effect" may appear during the IoT operation, since unreasonable data overlap could result in many duplicate data. To this end, we propose Senses, a novel online energy saving solution for edge IoT networks, with the insight of sensing and storing less at the network edge by adopting Muti-Agent Reinforcement Learning (MARL). Senses achieves data de-duplication by dynamically adjusting sensor coverage at the sensor level. For exceptional cases where sensor coverage cannot be altered, Senses conducts data partitioning and eliminates redundant data at the controller level. Furthermore, at the global level, considering the heterogeneity of IoT devices, Senses balances the operational duration among the devices to prolong the overall operational duration of edge IoT networks. We evaluate the performance of Senses through testbed experiments and simulations. The results show that Senses saves 11.37% of energy consumption on control devices and prolongs 20% overall operational duration of the IoT device network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21103v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongyang Yuan, Lailong Luo, Qianzhen Zhang, Bangbang Ren, Deke Guo, Richard T. B. Ma</dc:creator>
    </item>
    <item>
      <title>Benchmarking Catastrophic Forgetting Mitigation Methods in Federated Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2510.21491</link>
      <description>arXiv:2510.21491v1 Announce Type: cross 
Abstract: Catastrophic forgetting (CF) poses a persistent challenge in continual learning (CL), especially within federated learning (FL) environments characterized by non-i.i.d. time series data. While existing research has largely focused on classification tasks in vision domains, the regression-based forecasting setting prevalent in IoT and edge applications remains underexplored. In this paper, we present the first benchmarking framework tailored to investigate CF in federated continual time series forecasting. Using the Beijing Multi-site Air Quality dataset across 12 decentralized clients, we systematically evaluate several CF mitigation strategies, including Replay, Elastic Weight Consolidation, Learning without Forgetting, and Synaptic Intelligence. Key contributions include: (i) introducing a new benchmark for CF in time series FL, (ii) conducting a comprehensive comparative analysis of state-of-the-art methods, and (iii) releasing a reproducible open-source framework. This work provides essential tools and insights for advancing continual learning in federated time-series forecasting systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21491v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Khaled Hallak, Oudom Kem</dc:creator>
    </item>
    <item>
      <title>ProFaaStinate: Delaying Serverless Function Calls to Optimize Platform Performance</title>
      <link>https://arxiv.org/abs/2309.15471</link>
      <description>arXiv:2309.15471v3 Announce Type: replace 
Abstract: Function-as-a-Service (FaaS) enables developers to run serverless applications without managing operational tasks. In current FaaS platforms, both synchronous and asynchronous calls are executed immediately. In this paper, we present ProFaaStinate, which extends serverless platforms to enable delayed execution of asynchronous function calls. This allows platforms to execute calls at convenient times with higher resource availability or lower load. ProFaaStinate is able to optimize performance without requiring deep integration into the rest of the platform, or a complex systems model. In our evaluation, our prototype built on top of Nuclio can reduce request response latency and workflow duration while also preventing the system from being overloaded during load peaks. Using a document preparation use case, we show a 54% reduction in average request response latency. This reduction in resource usage benefits both platforms and users as cost savings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.15471v3</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3631295.3631393</arxiv:DOI>
      <dc:creator>Trever Schirmer, Natalie Carl, Tobias Pfandzelter, David Bermbach</dc:creator>
    </item>
    <item>
      <title>FlexLLM: Token-Level Co-Serving of LLM Inference and Finetuning with SLO Guarantees</title>
      <link>https://arxiv.org/abs/2402.18789</link>
      <description>arXiv:2402.18789v3 Announce Type: replace 
Abstract: Finetuning large language models (LLMs) is essential for task adaptation, yet today's serving stacks isolate inference and finetuning on separate GPU clusters -- wasting resources and under-utilizing hardware. We introduce FlexLLM, the first system to co-serve LLM inference and PEFT-based finetuning on shared GPUs by fusing computation at the token level. FlexLLM's static compilation optimizations -- dependent parallelization and graph pruning significantly shrink activation memory, leading to end-to-end GPU memory savings by up to 80%. At runtime, a novel token-level finetuning mechanism paired with a hybrid token scheduler dynamically interleaves inference and training tokens within each co-serving iteration, meeting strict latency SLOs while maximizing utilization. In end-to-end benchmarks on LLaMA-3.1-8B, Qwen-2.5-14B, and Qwen-2.5-32B, FlexLLM maintains inference SLO compliance at up to 20 req/s, and improves finetuning throughput by $1.9-4.8\times$ under heavy inference workloads and $2.5-6.8\times$ under light loads, preserving over 76% of peak finetuning progress even at peak demand. FlexLLM is publicly available at https://flexllm.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.18789v3</guid>
      <category>cs.DC</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Oliaro, Xupeng Miao, Xinhao Cheng, Vineeth Kada, Mengdi Wu, Ruohan Gao, Yingyi Huang, Remi Delacourt, April Yang, Yingcheng Wang, Colin Unger, Zhihao Jia</dc:creator>
    </item>
    <item>
      <title>Lazarus: Resilient and Elastic Training of Mixture-of-Experts Models</title>
      <link>https://arxiv.org/abs/2407.04656</link>
      <description>arXiv:2407.04656v2 Announce Type: replace 
Abstract: Sparsely-activated Mixture-of-Experts (MoE) architecture has increasingly been adopted to further scale large language models (LLMs). However, frequent failures still pose significant challenges as training scales. The cost of even a single failure is significant, as all GPUs need to idle wait until the failure is resolved, potentially losing considerable training progress as training has to restart from checkpoints. This problem is exacerbated by the growing use of spot instances on public clouds for model training, which despite offering substantial cost savings, introduce frequent preemptions-essentially failures that regularly occur throughout the training process. Existing solutions for efficient fault-tolerant training either lack elasticity or rely on building resiliency into pipeline parallelism, which cannot be applied to MoE models due to the expert parallelism strategy adopted by the MoE architecture.
  We present Lazarus, a system for resilient and elastic training of MoE models. Lazarus adaptively allocates expert replicas to address the inherent imbalance in expert workload and speeds up training, while a provably optimal expert placement algorithm is developed to maximize the probability of recovery upon failures. Through adaptive expert placement and a flexible token dispatcher, Lazarus can also fully utilize all available nodes after failures, leaving no GPU idle. Our evaluation shows that Lazarus outperforms existing MoE training systems by up to 5.7x under frequent node failures and 3.4x on a real spot instance trace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04656v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongji Wu, Wenjie Qu, Xueshen Liu, Tianyang Tao, Yifan Qiao, Zhuang Wang, Wei Bai, Yuan Tian, Jiaheng Zhang, Z. Morley Mao, Matthew Lentz, Danyang Zhuo, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>Domain Adaptation-based Edge Computing for Cross-Conditions Fault Diagnosis</title>
      <link>https://arxiv.org/abs/2411.10340</link>
      <description>arXiv:2411.10340v2 Announce Type: replace 
Abstract: Fault diagnosis of mechanical equipment provides robust support for industrial production. It is worth noting that, the operation of mechanical equipment is accompanied by changes in factors such as speed and load, leading to significant differences in data distribution, which pose challenges for fault diagnosis. Additionally, in terms of application deployment, commonly used cloud-based fault diagnosis methods often encounter issues such as time delays and data security concerns, while common fault diagnosis methods cannot be directly applied to edge computing devices. Therefore, conducting fault diagnosis under cross-operating conditions based on edge computing holds significant research value. This paper proposes a domain-adaptation-based lightweight fault diagnosis framework tailored for edge computing scenarios. Incorporating the local maximum mean discrepancy into knowledge transfer aligns the feature distributions of different domains in a high-dimensional feature space, to discover a common feature space across domains. The acquired fault diagnosis expertise from the cloud-based deep neural network model is transferred to the lightweight edge-based model (edge model) using adaptation knowledge transfer methods. It aims to achieve accurate fault diagnosis under cross-working conditions while ensuring real-time diagnosis capabilities. We utilized the NVIDIA Jetson Xavier NX kit as the edge computing platform and conducted validation experiments on two devices. In terms of diagnostic performance, the proposed method significantly improved diagnostic accuracy, with average increases of 34.44% and 17.33% compared to existing methods, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10340v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanzhi Wang, Jinhong Wu, Chu Wang, Qi Zhou, Tingli Xie</dc:creator>
    </item>
    <item>
      <title>A Survey on Heterogeneous Computing Using SmartNICs and Emerging Data Processing Units</title>
      <link>https://arxiv.org/abs/2504.03653</link>
      <description>arXiv:2504.03653v2 Announce Type: replace 
Abstract: The emergence of new, off-path smart network cards (SmartNICs), known generally as Data Processing Units (DPU), has opened a wide range of research opportunities. Of particular interest is the use of these and related devices in tandem with their host's CPU, creating a heterogeneous computing system with new properties and strengths to be explored, capable of accelerating a wide variety of workloads. This survey begins by providing the motivation and relevant background information for this new field, including its origins, a few current hardware offerings, major programming languages and frameworks for using them, and associated challenges. We then review and categorize a number of recent works in the field, covering a wide variety of studies, benchmarks, and application areas, such as data center infrastructure, commercial uses, and AI and ML acceleration. We conclude with a few observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03653v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nathan Tibbetts, Sifat Ibtisum, Satish Puri</dc:creator>
    </item>
    <item>
      <title>Minos: Exploiting Cloud Performance Variation with Function-as-a-Service Instance Selection</title>
      <link>https://arxiv.org/abs/2505.12928</link>
      <description>arXiv:2505.12928v3 Announce Type: replace 
Abstract: Serverless Function-as-a-Service (FaaS) is a popular cloud paradigm to quickly and cheaply implement complex applications. Because the function instances cloud providers start to execute user code run on shared infrastructure, their performance can vary. From a user perspective, slower instances not only take longer to complete, but also increase cost due to the pay-per-use model of FaaS services where execution duration is billed with microsecond accuracy. In this paper, we present Minos, a system to take advantage of this performance variation by intentionally terminating instances that are slow. Fast instances are not terminated, so that they can be re-used for subsequent invocations. One use case for this are data processing and machine learning workflows, which often download files as a first step, during which Minos can run a short benchmark. Only if the benchmark passes, the main part of the function is actually executed. Otherwise, the request is re-queued and the instance crashes itself, so that the platform has to assign the request to another (potentially faster) instance. In our experiments, this leads to a speedup of up to 13% in the resource intensive part of a data processing workflow, resulting in up to 4% faster overall performance (and consequently 4% cheaper prices). Longer and complex workflows lead to increased savings, as the pool of fast instances is re-used more often. For platforms exhibiting this behavior, users get better performance and save money by wasting more of the platforms resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12928v3</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Trever Schirmer, Natalie Carl, Nils H\"oller, Tobias Pfandzelter, David Bermbach</dc:creator>
    </item>
    <item>
      <title>Distributed Discrete Morse Sandwich: Efficient Computation of Persistence Diagrams for Massive Scalar Data</title>
      <link>https://arxiv.org/abs/2505.21266</link>
      <description>arXiv:2505.21266v2 Announce Type: replace 
Abstract: The persistence diagram, which describes the topological features of a dataset, is a key descriptor in Topological Data Analysis. The "Discrete Morse Sandwich" (DMS) method has been reported to be the most efficient algorithm for computing persistence diagrams of 3D scalar fields on a single node, using shared-memory parallelism. In this work, we extend DMS to distributed-memory parallelism for the efficient and scalable computation of persistence diagrams for massive datasets across multiple compute nodes. On the one hand, we can leverage the embarrassingly parallel procedure of the first and most time-consuming step of DMS (namely the discrete gradient computation). On the other hand, the efficient distributed computations of the subsequent DMS steps are much more challenging. To address this, we have extensively revised the DMS routines by contributing a new self-correcting distributed pairing algorithm, redesigning key data structures and introducing computation tokens to coordinate distributed computations. We have also introduced a dedicated communication thread to overlap communication and computation. Detailed performance analyses show the scalability of our hybrid MPI+thread approach for strong and weak scaling using up to 16 nodes of 32 cores (512 cores total). Our algorithm outperforms DIPHA, a reference method for the distributed computation of persistence diagrams, with an average speedup of x8 on 512 cores. We show the practical capabilities of our approach by computing the persistence diagram of a public 3D scalar field of 6 billion vertices in 174 seconds on 512 cores. Finally, we provide a usage example of our open-source implementation at https://github.com/eve-le-guillou/DDMS-example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21266v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eve Le Guillou, Pierre Fortin, Julien Tierny</dc:creator>
    </item>
    <item>
      <title>On Optimizing Resource Utilization in Distributed Connected Components</title>
      <link>https://arxiv.org/abs/2507.03695</link>
      <description>arXiv:2507.03695v4 Announce Type: replace 
Abstract: Connected Components (CC) is a core graph problem with numerous applications. This paper investigates accelerating distributed CC by optimizing memory and network bandwidth utilization. We present two novel distributed CC algorithms, SiskinCC and RobinCC, which are built upon the Jayanti-Tarjan disjoint set union algorithm. To optimize memory utilization, SiskinCC and RobinCC are designed to facilitate efficient access to a shared array for all cores running in a machine. This allows execution of faster algorithms with larger memory bounds. SiskinCC leverages the continuous inter-machine communication during the computation phase to reduce the final communication overhead and RobinCC leverages the structural properties of real-world graphs to optimize network bandwidth utilization. Our evaluation against a distributed state-of-the-art CC algorithm, using real-world and synthetic graphs with up to 500 billion edges and 11.7 billion vertices, and on up to 2048 CPU cores, demonstrates that SiskinCC and RobinCC achieve geometric mean speedups of 29.1 and 16.8 times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03695v4</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohsen Koohi Esfahani</dc:creator>
    </item>
    <item>
      <title>Prefetching in Deep Memory Hierarchies with NVRAM as Main Memory</title>
      <link>https://arxiv.org/abs/2509.17388</link>
      <description>arXiv:2509.17388v2 Announce Type: replace 
Abstract: Emerging applications, such as big data analytics and machine learning, require increasingly large amounts of main memory, often exceeding the capacity of current commodity processors built on DRAM technology. To address this, recent research has focused on off-chip memory controllers that facilitate access to diverse memory media, each with unique density and latency characteristics. While these solutions improve memory system performance, they also exacerbate the already significant memory latency. As a result, multi-level prefetching techniques are essential to mitigate these extended latencies.
  This paper investigates the advantages of prefetching across both sides of the memory system: the off-chip memory and the on-chip cache hierarchy. Our primary objective is to assess the impact of a multi-level prefetching engine on overall system performance. Additionally, we analyze the individual contribution of each prefetching level to system efficiency. To achieve this, the study evaluates two key prefetching approaches: HMC (Hybrid Memory Controller) and HMC+L1, both of which employ prefetching mechanisms commonly used by processor vendors. The HMC approach integrates a prefetcher within the off-chip hybrid memory controller, while the HMC+L1 approach combines this with additional L1 on-chip prefetchers.
  Experimental results on an out-of-order execution processor show that on-chip cache prefetchers are crucial for maximizing the benefits of off-chip prefetching, which in turn further enhances performance. Specifically, the off-chip HMC prefetcher achieves coverage and accuracy rates exceeding 60% and up to 80%, while the combined HMC+L1 approach boosts off-chip prefetcher coverage to as much as 92%. Consequently, overall performance increases from 9% with the HMC approach to 12% when L1 prefetching is also employed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17388v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manel Lurbe, Miguel Avargues, Salvador Petit, Maria E. Gomez, Rui Yang, Guanhao Wang, Julio Sahuquillo</dc:creator>
    </item>
    <item>
      <title>On the Universality of Round Elimination Fixed Points</title>
      <link>https://arxiv.org/abs/2510.17639</link>
      <description>arXiv:2510.17639v2 Announce Type: replace 
Abstract: Recent work on distributed graph algorithms [e.g. STOC 2022, ITCS 2022, PODC 2020] has drawn attention to the following open question: are round elimination fixed points a universal technique for proving lower bounds? That is, given a locally checkable problem $\Pi$ that requires at least $\Omega(\log n)$ rounds in the deterministic LOCAL model, can we always find a relaxation $\Pi'$ of $\Pi$ that is a nontrivial fixed point for the round elimination technique [see STOC 2016, PODC 2019]? If yes, then a key part of distributed computational complexity would be also decidable.
  The key obstacle so far has been a certain family of homomorphism problems [ITCS 2022], which require $\Omega(\log n)$ rounds, but the only known proof is based on Marks' technique [J. AMS 2016].
  We develop a new technique for constructing round elimination lower bounds systematically. Using so-called tripotent inputs we show that the aforementioned homomorphism problems indeed admit a lower bound proof that is based on round elimination fixed points. Hence we eliminate the only known obstacle for the universality of round elimination.
  Yet we also present a new obstacle: we show that there are some problems with inputs that require $\Omega(\log n)$ rounds, yet there is no proof that is based on relaxations to nontrivial round elimination fixed points. Hence round elimination cannot be a universal technique for problems with inputs (but it might be universal for problems without inputs).
  We also prove the first fully general lower bound theorem that is applicable to any problem, with or without inputs, that is a fixed point in round elimination. Prior results of this form were only able to handle certain very restricted inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17639v2</guid>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Alkida Balliu, Sebastian Brandt, Ole Gabsdil, Dennis Olivetti, Jukka Suomela</dc:creator>
    </item>
    <item>
      <title>RLBoost: Harvesting Preemptible Resources for Cost-Efficient Reinforcement Learning on LLMs</title>
      <link>https://arxiv.org/abs/2510.19225</link>
      <description>arXiv:2510.19225v2 Announce Type: replace 
Abstract: Reinforcement learning (RL) has become essential for unlocking advanced reasoning capabilities in large language models (LLMs). RL workflows involve interleaving rollout and training stages with fundamentally different resource requirements. Rollout typically dominates overall execution time, yet scales efficiently through multiple independent instances. In contrast, training requires tightly-coupled GPUs with full-mesh communication. Existing RL frameworks fall into two categories: co-located and disaggregated architectures. Co-located ones fail to address this resource tension by forcing both stages to share the same GPUs. Disaggregated architectures, without modifications of well-established RL algorithms, suffer from resource under-utilization. Meanwhile, preemptible GPU resources, i.e., spot instances on public clouds and spare capacity in production clusters, present significant cost-saving opportunities for accelerating RL workflows, if efficiently harvested for rollout.
  In this paper, we present RLBoost, a systematic solution for cost-efficient RL training that harvests preemptible GPU resources. Our key insight is that rollout's stateless and embarrassingly parallel nature aligns perfectly with preemptible and often fragmented resources. To efficiently utilize these resources despite frequent and unpredictable availability changes, RLBoost adopts a hybrid architecture with three key techniques: (1) adaptive rollout offload to dynamically adjust workloads on the reserved (on-demand) cluster, (2) pull-based weight transfer that quickly provisions newly available instances, and (3) token-level response collection and migration for efficient preemption handling and continuous load balancing. Extensive experiments show RLBoost increases training throughput by 1.51x-1.97x while improving cost efficiency by 28%-49% compared to using only on-demand GPU resources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19225v2</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Yongji Wu, Xueshen Liu, Haizhong Zheng, Juncheng Gu, Beidi Chen, Z. Morley Mao, Arvind Krishnamurthy, Ion Stoica</dc:creator>
    </item>
    <item>
      <title>Collective Communication for 100k+ GPUs</title>
      <link>https://arxiv.org/abs/2510.20171</link>
      <description>arXiv:2510.20171v2 Announce Type: replace 
Abstract: The increasing scale of large language models (LLMs) necessitates highly efficient collective communication frameworks, particularly as training workloads extend to hundreds of thousands of GPUs. Traditional communication methods face significant throughput and latency limitations at this scale, hindering both the development and deployment of state-of-the-art models. This paper presents the NCCLX collective communication framework, developed at Meta, engineered to optimize performance across the full LLM lifecycle, from the synchronous demands of large-scale training to the low-latency requirements of inference. The framework is designed to support complex workloads on clusters exceeding 100,000 GPUs, ensuring reliable, high-throughput, and low-latency data exchange. Empirical evaluation on the Llama4 model demonstrates substantial improvements in communication efficiency. This research contributes a robust solution for enabling the next generation of LLMs to operate at unprecedented scales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20171v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Min Si, Pavan Balaji, Yongzhou Chen, Ching-Hsiang Chu, Adi Gangidi, Saif Hasan, Subodh Iyengar, Dan Johnson, Bingzhe Liu, Regina Ren, Ashmitha Jeevaraj Shetty, Greg Steinbrecher, Yulun Wang, Bruce Wu, Xinfeng Xie, Jingyi Yang, Mingran Yang, Kenny Yu, Minlan Yu, Cen Zhao, Wes Bland, Denis Boyda, Suman Gumudavelli, Prashanth Kannan, Cristian Lumezanu, Rui Miao, Zhe Qu, Venkat Ramesh, Maxim Samoylov, Jan Seidel, Srikanth Sundaresan, Feng Tian, Qiye Tan, Shuqiang Zhang, Yimeng Zhao, Shengbao Zheng, Art Zhu, Hongyi Zeng</dc:creator>
    </item>
    <item>
      <title>Fair Combinatorial Auction for Blockchain Trade Intents: Being Fair without Knowing What is Fair</title>
      <link>https://arxiv.org/abs/2408.12225</link>
      <description>arXiv:2408.12225v3 Announce Type: replace-cross 
Abstract: We study blockchain trade-intent auctions, which currently intermediate about USD 10 billion in trades each month. These auctions are combinatorial because executing multiple trade intents jointly generates additional efficiencies. However, the auctioneer cannot observe what each trader would have received had its order been auctioned individually and hence cannot determine how these efficiencies should be shared. We compare the two dominant mechanisms - batch auctions and simultaneous individual auctions - and introduce a novel definition of fairness applicable to combinatorial auctions. We then propose a fair combinatorial auction that endogenously constructs a fairness benchmark from individual bids and a counterfactual mechanism. Whether fairness guarantees arise in equilibrium depends on the counterfactual: all traders receive more in the equilibrium of the fair combinatorial auction than in the equilibrium of the counterfactual mechanism when the counterfactual is simultaneous first-price auctions, but that may not be the case if the counterfactual is simultaneous second-price auctions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12225v3</guid>
      <category>econ.TH</category>
      <category>cs.DC</category>
      <category>cs.GT</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Andrea Canidio, Felix Henneke</dc:creator>
    </item>
    <item>
      <title>Intelligent Sampling of Extreme-Scale Turbulence Datasets for Accurate and Efficient Spatiotemporal Model Training</title>
      <link>https://arxiv.org/abs/2508.03872</link>
      <description>arXiv:2508.03872v3 Announce Type: replace-cross 
Abstract: With the end of Moore's law and Dennard scaling, efficient training increasingly requires rethinking data volume. Can we train better models with significantly less data via intelligent subsampling? To explore this, we develop SICKLE, a sparse intelligent curation framework for efficient learning, featuring a novel maximum entropy (MaxEnt) sampling approach, scalable training, and energy benchmarking. We compare MaxEnt with random and phase-space sampling on large direct numerical simulation (DNS) datasets of turbulence. Evaluating SICKLE at scale on Frontier, we show that subsampling as a preprocessing step can, in many cases, improve model accuracy and substantially lower energy consumption, with observed reductions of up to 38x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03872v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3731599.3767340</arxiv:DOI>
      <dc:creator>Wesley Brewer, Murali Meena Gopalakrishnan, Matthias Maiterth, Aditya Kashi, Jong Youl Choi, Pei Zhang, Stephen Nichols, Riccardo Balin, Miles Couchman, Stephen de Bruyn Kops, P. K. Yeung, Daniel Dotson, Rohini Uma-Vaideswaran, Sarp Oral, Feiyi Wang</dc:creator>
    </item>
  </channel>
</rss>
