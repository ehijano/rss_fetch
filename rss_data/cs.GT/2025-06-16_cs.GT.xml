<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Jun 2025 02:28:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Shapley Machine: A Game-Theoretic Framework for N-Agent Ad Hoc Teamwork</title>
      <link>https://arxiv.org/abs/2506.11285</link>
      <description>arXiv:2506.11285v1 Announce Type: cross 
Abstract: Open multi-agent systems are increasingly important in modeling real-world applications, such as smart grids, swarm robotics, etc. In this paper, we aim to investigate a recently proposed problem for open multi-agent systems, referred to as n-agent ad hoc teamwork (NAHT), where only a number of agents are controlled. Existing methods tend to be based on heuristic design and consequently lack theoretical rigor and ambiguous credit assignment among agents. To address these limitations, we model and solve NAHT through the lens of cooperative game theory. More specifically, we first model an open multi-agent system, characterized by its value, as an instance situated in a space of cooperative games, generated by a set of basis games. We then extend this space, along with the state space, to accommodate dynamic scenarios, thereby characterizing NAHT. Exploiting the justifiable assumption that basis game values correspond to a sequence of n-step returns with different horizons, we represent the state values for NAHT in a form similar to $\lambda$-returns. Furthermore, we derive Shapley values to allocate state values to the controlled agents, as credits for their contributions to the ad hoc team. Different from the conventional approach to shaping Shapley values in an explicit form, we shape Shapley values by fulfilling the three axioms uniquely describing them, well defined on the extended game space describing NAHT. To estimate Shapley values in dynamic scenarios, we propose a TD($\lambda$)-like algorithm. The resulting reinforcement learning (RL) algorithm is referred to as Shapley Machine. To our best knowledge, this is the first time that the concepts from cooperative game theory are directly related to RL concepts. In experiments, we demonstrate the effectiveness of Shapley Machine and verify reasonableness of our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11285v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianhong Wang, Yang Li, Samuel Kaski, Jonathan Lawry</dc:creator>
    </item>
    <item>
      <title>Nash equilibria in semidefinite games and Lemke-Howson paths</title>
      <link>https://arxiv.org/abs/2506.11940</link>
      <description>arXiv:2506.11940v1 Announce Type: cross 
Abstract: We consider an algorithmic framework for two-player non-zero-sum semidefinite games, where each player's strategy is a positive semidefinite matrix with trace one. We formulate the computation of Nash equilibria in such games as semidefinite complementarity problems and develop symbolic-numeric techniques to trace generalized Lemke-Howson paths. These paths generalize the piecewise affine-linear trajectories of the classical Lemke-Howson algorithm for bimatrix games, replacing them with nonlinear curve branches governed by eigenvalue complementarity conditions.
  A key feature of our framework is the introduction of event points, which correspond to curve singularities. We analyze the local behavior near these points using Puiseux series expansions. We prove the smoothness of the curve branches under suitable non-degeneracy conditions and establish connections between our approach and both the classical combinatorial and homotopy-theoretic interpretations of the Lemke-Howson algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11940v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Constantin Ickstadt, Thorsten Theobald, Elias Tsigaridas, Antonios Varvitsiotis</dc:creator>
    </item>
    <item>
      <title>Ad Auctions for LLMs via Retrieval Augmented Generation</title>
      <link>https://arxiv.org/abs/2406.09459</link>
      <description>arXiv:2406.09459v2 Announce Type: replace 
Abstract: In the field of computational advertising, the integration of ads into the outputs of large language models (LLMs) presents an opportunity to support these services without compromising content integrity. This paper introduces novel auction mechanisms for ad allocation and pricing within the textual outputs of LLMs, leveraging retrieval-augmented generation (RAG). We propose a segment auction where an ad is probabilistically retrieved for each discourse segment (paragraph, section, or entire output) according to its bid and relevance, following the RAG framework, and priced according to competing bids. We show that our auction maximizes logarithmic social welfare, a new notion of welfare that balances allocation efficiency and fairness, and we characterize the associated incentive-compatible pricing rule. These results are extended to multi-ad allocation per segment. An empirical evaluation validates the feasibility and effectiveness of our approach over several ad auction scenarios, and exhibits inherent tradeoffs in metrics as we allow the LLM more flexibility to allocate ads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09459v2</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>MohammadTaghi Hajiaghayi, S\'ebastien Lahaie, Keivan Rezaei, Suho Shin</dc:creator>
    </item>
    <item>
      <title>Computational Social Choice: Parameterized Complexity and Challenges</title>
      <link>https://arxiv.org/abs/2410.14078</link>
      <description>arXiv:2410.14078v2 Announce Type: replace 
Abstract: We survey two key problems-Multi-Winner Determination and Hedonic Games in Computational Social Choice, with a special focus on their parameterized complexity, and propose some research challenges in the field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14078v2</guid>
      <category>cs.GT</category>
      <category>cs.CC</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiehua Chen, Christian Hatschka, Sofia Simola</dc:creator>
    </item>
    <item>
      <title>Temporal Explorability Games</title>
      <link>https://arxiv.org/abs/2412.16328</link>
      <description>arXiv:2412.16328v3 Announce Type: replace 
Abstract: Temporal graphs extend ordinary graphs with discrete time that affects the availability of edges. We consider solving games played on temporal graphs where one player aims to explore the graph, i.e., visit all vertices. The complexity depends majorly on two factors: the presence of an adversary and how edge availability is specified.
  We demonstrate that on static graphs, where edges are always available, solving explorability games is just as hard as solving reachability games. In contrast, on temporal graphs, the complexity of explorability coincides with generalized reachability (NP-complete for one-player and PSPACE- complete for two player games). We further show that if temporal graphs are given symbolically, even one-player reachability and thus explorability and generalized reachability games are PSPACE-hard. For one player, all these are also solvable in PSPACE and for two players, they are in PSPACE, EXP and EXP, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16328v3</guid>
      <category>cs.GT</category>
      <category>cs.LO</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pete Austin, Nicolas Mazzocchi, Sougata Bose, Patrick Totzke</dc:creator>
    </item>
    <item>
      <title>Noncooperative Equilibrium Selection via a Trading-based Auction</title>
      <link>https://arxiv.org/abs/2502.03616</link>
      <description>arXiv:2502.03616v2 Announce Type: replace 
Abstract: Noncooperative multi-agent systems often face coordination challenges due to conflicting preferences among agents. In particular, agents acting in their own self-interest can settle on different equilibria, leading to suboptimal outcomes or even safety concerns. We propose an algorithm named trading auction for consensus (TACo), a decentralized approach that enables noncooperative agents to reach consensus without communicating directly or disclosing private valuations. TACo facilitates coordination through a structured trading-based auction, where agents iteratively select choices of interest and provably reach an agreement within an a priori bounded number of steps. A series of numerical experiments validate that the termination guarantees of TACo hold in practice, and show that TACo achieves a median performance that minimizes the total cost across all agents, while allocating resources significantly more fairly than baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03616v2</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaehan Im, Filippos Fotiadis, Daniel Delahaye, Ufuk Topcu, David Fridovich-Keil</dc:creator>
    </item>
    <item>
      <title>Delegation with Costly Inspection</title>
      <link>https://arxiv.org/abs/2506.07162</link>
      <description>arXiv:2506.07162v2 Announce Type: replace 
Abstract: We study the problem of delegated choice with inspection cost (DCIC), which is a variant of the delegated choice problem by Kleinberg and Kleinberg (EC'18) as well as an extension of the Pandora's box problem with nonobligatory inspection (PNOI) by Doval (JET'18). In our model, an agent may strategically misreport the proposed element's utility, unlike the standard delegated choice problem which assumes that the agent truthfully reports the utility for the proposed alternative. Thus, the principal needs to inspect the proposed element possibly along with other alternatives to maximize its own utility, given an exogenous cost of inspecting each element. Further, the delegation itself incurs a fixed cost, thus the principal can decide whether to delegate or not and inspect by herself.
  We show that DCIC indeed is a generalization of PNOI where the side information from a strategic agent is available at certain cost, implying its NP-hardness by Fu, Li, and Liu (STOC'23). We first consider a costless delegation setting in which the cost of delegation is free. We prove that the maximal mechanism over the pure delegation with a single inspection and an PNOI policy without delegation achieves a $3$-approximation for DCIC with costless delegation, which is further proven to be tight. These results hold even when the cost comes from an arbitrary monotone set function, and can be improved to a $2$-approximation if the cost of inspection is the same for every element. We extend these techniques by presenting a constant factor approximate mechanism for the general setting for rich class of instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.07162v2</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad T. Hajiaghayi, Piotr Krysta, Mohammad Mahdavi, Suho Shin</dc:creator>
    </item>
    <item>
      <title>Combining Deep Reinforcement Learning and Search with Generative Models for Game-Theoretic Opponent Modeling</title>
      <link>https://arxiv.org/abs/2302.00797</link>
      <description>arXiv:2302.00797v2 Announce Type: replace-cross 
Abstract: Opponent modeling methods typically involve two crucial steps: building a belief distribution over opponents' strategies, and exploiting this opponent model by playing a best response. However, existing approaches typically require domain-specific heurstics to come up with such a model, and algorithms for approximating best responses are hard to scale in large, imperfect information domains.
  In this work, we introduce a scalable and generic multiagent training regime for opponent modeling using deep game-theoretic reinforcement learning. We first propose Generative Best Respoonse (GenBR), a best response algorithm based on Monte-Carlo Tree Search (MCTS) with a learned deep generative model that samples world states during planning. This new method scales to large imperfect information domains and can be plug and play in a variety of multiagent algorithms. We use this new method under the framework of Policy Space Response Oracles (PSRO), to automate the generation of an \emph{offline opponent model} via iterative game-theoretic reasoning and population-based training. We propose using solution concepts based on bargaining theory to build up an opponent mixture, which we find identifying profiles that are near the Pareto frontier. Then GenBR keeps updating an \emph{online opponent model} and reacts against it during gameplay. We conduct behavioral studies where human participants negotiate with our agents in Deal-or-No-Deal, a class of bilateral bargaining games. Search with generative modeling finds stronger policies during both training time and test time, enables online Bayesian co-player prediction, and can produce agents that achieve comparable social welfare and Nash bargaining score negotiating with humans as humans trading among themselves.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.00797v2</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zun Li, Marc Lanctot, Kevin R. McKee, Luke Marris, Ian Gemp, Daniel Hennes, Paul Muller, Kate Larson, Yoram Bachrach, Michael P. Wellman</dc:creator>
    </item>
    <item>
      <title>Spacetime games subsume causal contextuality scenarios</title>
      <link>https://arxiv.org/abs/2405.20143</link>
      <description>arXiv:2405.20143v4 Announce Type: replace-cross 
Abstract: We show that a category of causal contextuality scenarios with no cycles, unique causal bridges, and causally secured covers is equivalent to a category containing a subclass of the formerly published spacetime games, which generalize game theory to decisions arbitrarily located in Minkowski spacetime. This insight leads to certain constructs and proofs being shorter, simpler, and more intuitive when expressed in the spacetime game framework than in the causal contextuality scenario framework. The equivalence of categories and the modular structure of causal contextuality theory also implies that it is possible to build (pure) strategy sheaves, mixed strategy presheaves (equivalent to distribution presheaves) and empirical models on top of spacetime games: the obstruction to a global section in the presence of contextuality corresponds to the non-existence of a mixed strategy in the sense of the Nash game theory framework. This shows that the insights of both frameworks taken together can contribute positively to advancing the field of quantum foundations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20143v4</guid>
      <category>quant-ph</category>
      <category>cs.GT</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ghislain Fourny</dc:creator>
    </item>
    <item>
      <title>Policy Optimization and Multi-agent Reinforcement Learning for Mean-variance Team Stochastic Games</title>
      <link>https://arxiv.org/abs/2503.22779</link>
      <description>arXiv:2503.22779v2 Announce Type: replace-cross 
Abstract: We study a long-run mean-variance team stochastic game (MV-TSG), where each agent shares a common mean-variance objective for the system and takes actions independently to maximize it. MV-TSG has two main challenges. First, the variance metric is neither additive nor Markovian in a dynamic setting. Second, simultaneous policy updates of all agents lead to a non-stationary environment for each individual agent. Both challenges make dynamic programming inapplicable. In this paper, we study MV-TSGs from the perspective of sensitivity-based optimization. The performance difference and performance derivative formulas for joint policies are derived, which provide optimization information for MV-TSGs. We prove the existence of a deterministic Nash policy for this problem. Subsequently, we propose a Mean-Variance Multi-Agent Policy Iteration (MV-MAPI) algorithm with a sequential update scheme, where individual agent policies are updated one by one in a given order. We prove that the MV-MAPI algorithm converges to a first-order stationary point of the objective function. By analyzing the local geometry of stationary points, we derive specific conditions for stationary points to be (local) Nash equilibria, and further, strict local optima. To solve large-scale MV-TSGs in scenarios with unknown environmental parameters, we extend the idea of trust region methods to MV-MAPI and develop a multi-agent reinforcement learning algorithm named Mean-Variance Multi-Agent Trust Region Policy Optimization (MV-MATRPO). We derive a performance lower bound for each update of joint policies. Finally, numerical experiments on energy management in multiple microgrid systems are conducted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22779v2</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 16 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junkai Hu, Li Xia</dc:creator>
    </item>
  </channel>
</rss>
