<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 10 Feb 2025 05:01:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Regulating Reality: Exploring Synthetic Media Through Multistakeholder AI Governance</title>
      <link>https://arxiv.org/abs/2502.04526</link>
      <description>arXiv:2502.04526v1 Announce Type: new 
Abstract: Artificial intelligence's integration into daily life has brought with it a reckoning on the role such technology plays in society and the varied stakeholders who should shape its governance. This is particularly relevant for the governance of AI-generated media, or synthetic media, an emergent visual technology that impacts how people interpret online content and perceive media as records of reality. Studying the stakeholders affecting synthetic media governance is vital to assessing safeguards that help audiences make sense of content in the AI age; yet there is little qualitative research about how key actors from civil society, industry, media, and policy collaborate to conceptualize, develop, and implement such practices. This paper addresses this gap by analyzing 23 in-depth, semi-structured interviews with stakeholders governing synthetic media from across sectors alongside two real-world cases of multistakeholder synthetic media governance. Inductive coding reveals key themes affecting synthetic media governance, including how temporal perspectives-spanning past, present, and future-mediate stakeholder decision-making and rulemaking on synthetic media. Analysis also reveals the critical role of trust, both among stakeholders and between audiences and interventions, as well as the limitations of technical transparency measures like AI labels for supporting effective synthetic media governance. These findings not only inform the evidence-based design of synthetic media policy that serves audiences encountering content, but they also contribute to the literature on multistakeholder AI governance overall through rare insight into real world examples of such processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04526v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Claire R. Leibowicz</dc:creator>
    </item>
    <item>
      <title>Teaching Reform and Exploration on Object-Oriented Programming</title>
      <link>https://arxiv.org/abs/2502.04596</link>
      <description>arXiv:2502.04596v1 Announce Type: new 
Abstract: The problems in our teaching on object-oriented programming are analyzed, and the basic ideas, causes and methods of the reform are discussed on the curriculum, theoretical teaching and practical classes. Our practice shows that these reforms can improve students' understanding of object-oriented to enhance students' practical ability and innovative ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04596v1</guid>
      <category>cs.CY</category>
      <category>physics.ed-ph</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/CSCI.2016.0073</arxiv:DOI>
      <dc:creator>Guowu Yuan, Bing Kong, Haiyan Ding, Jixian Zhang, Yang Zhao</dc:creator>
    </item>
    <item>
      <title>Where does AI come from? A global case study across Europe, Africa, and Latin America</title>
      <link>https://arxiv.org/abs/2502.04860</link>
      <description>arXiv:2502.04860v1 Announce Type: new 
Abstract: This article examines the organisational and geographical forces that shape the supply chains of artificial intelligence (AI) through outsourced and offshored data work. Bridging sociological theories of relational inequalities and embeddedness with critical approaches to Global Value Chains, we conduct a global case study of the digitally enabled organisation of data work in France, Madagascar, and Venezuela. The AI supply chains procure data work via a mix of arm's length contracts through marketplace-like platforms, and of embedded firm-like structures that offer greater stability but less flexibility, with multiple intermediate arrangements. Each solution suits specific types and purposes of data work in AI preparation, verification, and impersonation. While all forms reproduce well-known patterns of exclusion that harm externalised workers especially in the Global South, disadvantage manifests unevenly in different supply chain structures, with repercussions on remunerations, job security and working conditions. Unveiling these processes of contemporary technology development provides insights into possible policy implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04860v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/13563467.2025.2462137</arxiv:DOI>
      <dc:creator>Paola Tubaro (CNRS, ENSAE Paris, CREST, IP Paris), Antonio A Casilli (I3 SES, NOS, LACI, IP Paris), Maxime Cornet (SES, I3 SES, IP Paris, NOS), Cl\'ement Le Ludec (CERSA), Juana Torres Cierpe</dc:creator>
    </item>
    <item>
      <title>WikiReddit: Tracing Information and Attention Flows Between Online Platforms</title>
      <link>https://arxiv.org/abs/2502.04942</link>
      <description>arXiv:2502.04942v1 Announce Type: new 
Abstract: The World Wide Web is a complex interconnected digital ecosystem, where information and attention flow between platforms and communities throughout the globe. These interactions co-construct how we understand the world, reflecting and shaping public discourse. Unfortunately, researchers often struggle to understand how information circulates and evolves across the web because platform-specific data is often siloed and restricted by linguistic barriers. To address this gap, we present a comprehensive, multilingual dataset capturing all Wikipedia links shared in posts and comments on Reddit from 2020 to 2023, excluding those from private and NSFW subreddits. Each linked Wikipedia article is enriched with revision history, page view data, article ID, redirects, and Wikidata identifiers. Through a research agreement with Reddit, our dataset ensures user privacy while providing a query and ID mechanism that integrates with the Reddit and Wikipedia APIs. This enables extended analyses for researchers studying how information flows across platforms. For example, Reddit discussions use Wikipedia for deliberation and fact-checking which subsequently influences Wikipedia content, by driving traffic to articles or inspiring edits. By analyzing the relationship between information shared and discussed on these platforms, our dataset provides a foundation for examining the interplay between social media discourse and collaborative knowledge consumption and production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04942v1</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Gildersleve, Anna Beers, Viviane Ito, Agustin Orozco, Francesca Tripodi</dc:creator>
    </item>
    <item>
      <title>The Role of Science in the Climate Change Discussions on Reddit</title>
      <link>https://arxiv.org/abs/2502.05026</link>
      <description>arXiv:2502.05026v1 Announce Type: new 
Abstract: Collective and individual action necessary to address climate change hinges on the public's understanding of the relevant scientific findings. In this study, we examine the use of scientific sources in the course of 14 years of public deliberation around climate change on one of the largest social media platforms, Reddit. We find that only 4.0% of the links in the Reddit posts, and 6.5% in the comments, point to domains of scientific sources, although these rates have been increasing in the past decades. These links are dwarfed, however, by the citations of mass media, newspapers, and social media, the latter of which peaked especially during 2019-2020. Further, scientific sources are more likely to be posted by users who also post links to sources having central-left political leaning, and less so by those posting more polarized sources. Unfortunately, scientific sources are not often used in response to links to unreliable sources.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05026v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Cornale, Michele Tizzani, Fabio Ciulla, Kyriaki Kalimeri, Elisa Omodei, Daniela Paolotti, Yelena Mejova</dc:creator>
    </item>
    <item>
      <title>Navigating Automated Hiring: Perceptions, Strategy Use, and Outcomes Among Young Job Seekers</title>
      <link>https://arxiv.org/abs/2502.05099</link>
      <description>arXiv:2502.05099v1 Announce Type: new 
Abstract: As the use of automated employment decision tools (AEDTs) has rapidly increased in hiring contexts, especially for computing jobs, there is still limited work on applicants' perceptions of these emerging tools and their experiences navigating them. To investigate, we conducted a survey with 448 computer science students (young, current technology job-seekers) about perceptions of the procedural fairness of AEDTs, their willingness to be evaluated by different AEDTs, the strategies they use relating to automation in the hiring process, and their job seeking success. We find that young job seekers' procedural fairness perceptions of and willingness to be evaluated by AEDTs varied with the level of automation involved in the AEDT, the technical nature of the task being evaluated, and their own use of strategies, such as job referrals. Examining the relationship of their strategies with job outcomes, notably, we find that referrals and family household income have significant and positive impacts on hiring success, while more egalitarian strategies (using free online coding assessment practice or adding keywords to resumes) did not. Overall, our work speaks to young job seekers' distrust of automation in hiring contexts, as well as the continued role of social and socioeconomic privilege in job seeking, despite the use of AEDTs that promise to make hiring "unbiased."</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05099v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lena Armstrong, Dana\'e Metaxa</dc:creator>
    </item>
    <item>
      <title>ApplE: An Applied Ethics Ontology with Event Context</title>
      <link>https://arxiv.org/abs/2502.05110</link>
      <description>arXiv:2502.05110v1 Announce Type: new 
Abstract: Applied ethics is ubiquitous in most domains, requiring much deliberation due to its philosophical nature. Varying views often lead to conflicting courses of action where ethical dilemmas become challenging to resolve. Although many factors contribute to such a decision, the major driving forces can be discretized and thus simplified to provide an indicative answer. Knowledge representation and reasoning offer a way to explicitly translate abstract ethical concepts into applicable principles within the context of an event. To achieve this, we propose ApplE, an Applied Ethics ontology that captures philosophical theory and event context to holistically describe the morality of an action. The development process adheres to a modified version of the Simplified Agile Methodology for Ontology Development (SAMOD) and utilizes standard design and publication practices. Using ApplE, we model a use case from the bioethics domain that demonstrates our ontology's social and scientific value. Apart from the ontological reasoning and quality checks, ApplE is also evaluated using the three-fold testing process of SAMOD. ApplE follows FAIR principles and aims to be a viable resource for applied ethicists and ontology engineers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05110v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aisha Aijaz, Raghava Mutharaju, Manohar Kumar</dc:creator>
    </item>
    <item>
      <title>An Annotated Reading of 'The Singer of Tales' in the LLM Era</title>
      <link>https://arxiv.org/abs/2502.05148</link>
      <description>arXiv:2502.05148v1 Announce Type: new 
Abstract: The Parry-Lord oral-formulaic theory was a breakthrough in understanding how oral narrative poetry is learned, composed, and transmitted by illiterate bards. In this paper, we provide an annotated reading of the mechanism underlying this theory from the lens of large language models (LLMs) and generative artificial intelligence (AI). We point out the the similarities and differences between oral composition and LLM generation, and comment on the implications to society and AI policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05148v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kush R. Varshney</dc:creator>
    </item>
    <item>
      <title>Sparse Autoencoders for Hypothesis Generation</title>
      <link>https://arxiv.org/abs/2502.04382</link>
      <description>arXiv:2502.04382v1 Announce Type: cross 
Abstract: We describe HypotheSAEs, a general method to hypothesize interpretable relationships between text data (e.g., headlines) and a target variable (e.g., clicks). HypotheSAEs has three steps: (1) train a sparse autoencoder on text embeddings to produce interpretable features describing the data distribution, (2) select features that predict the target variable, and (3) generate a natural language interpretation of each feature (e.g., "mentions being surprised or shocked") using an LLM. Each interpretation serves as a hypothesis about what predicts the target variable. Compared to baselines, our method better identifies reference hypotheses on synthetic datasets (at least +0.06 in F1) and produces more predictive hypotheses on real datasets (~twice as many significant findings), despite requiring 1-2 orders of magnitude less compute than recent LLM-based methods. HypotheSAEs also produces novel discoveries on two well-studied tasks: explaining partisan differences in Congressional speeches and identifying drivers of engagement with online headlines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04382v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson</dc:creator>
    </item>
    <item>
      <title>Decoding AI Judgment: How LLMs Assess News Credibility and Bias</title>
      <link>https://arxiv.org/abs/2502.04426</link>
      <description>arXiv:2502.04426v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) are increasingly used to assess news credibility, yet little is known about how they make these judgments. While prior research has examined political bias in LLM outputs or their potential for automated fact-checking, their internal evaluation processes remain largely unexamined. Understanding how LLMs assess credibility provides insights into AI behavior and how credibility is structured and applied in large-scale language models. This study benchmarks the reliability and political classifications of state-of-the-art LLMs - Gemini 1.5 Flash (Google), GPT-4o mini (OpenAI), and LLaMA 3.1 (Meta) - against structured, expert-driven rating systems such as NewsGuard and Media Bias Fact Check. Beyond assessing classification performance, we analyze the linguistic markers that shape LLM decisions, identifying which words and concepts drive their evaluations. We uncover patterns in how LLMs associate credibility with specific linguistic features by examining keyword frequency, contextual determinants, and rank distributions. Beyond static classification, we introduce a framework in which LLMs refine their credibility assessments by retrieving external information, querying other models, and adapting their responses. This allows us to investigate whether their assessments reflect structured reasoning or rely primarily on prior learned associations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04426v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edoardo Loru, Jacopo Nudo, Niccol\`o Di Marco, Matteo Cinelli, Walter Quattrociocchi</dc:creator>
    </item>
    <item>
      <title>"In order that" -- a data driven study of symptoms and causes of obsolescence</title>
      <link>https://arxiv.org/abs/2502.04457</link>
      <description>arXiv:2502.04457v1 Announce Type: cross 
Abstract: The paper is an empirical case study of grammatical obsolescence in progress. The main studied variable is the purpose subordinator in order that, which is shown to be steadily decreasing in the frequency of use starting from the beginning of the twentieth century. This work applies a data-driven approach for the investigation and description of obsolescence, recently developed by the Rudnicka (2019). The methodology combines philological analysis with statistical methods used on data acquired from mega-corpora. Moving from the description of possible symptoms of obsolescence to different causes for it, the paper aims at presenting a comprehensive account of the studied phenomenon. Interestingly, a very significant role in the decline of in order that can be ascribed to the so-called higher-order processes, understood as processes influencing the constructional level from above. Two kinds of higher-order processes are shown to play an important role, namely i) an externally-motivated higher-order process exemplified by the drastic socio-cultural changes of the 19th and 20th centuries; ii) an internally-motivated higher-order processes instantiated by the rise of the to-infinitive (rise of infinite clauses).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04457v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1515/lingvan-2020-0092</arxiv:DOI>
      <arxiv:journal_reference>2021, Linguistics Vanguard 7 (1), 20200092: 1-11</arxiv:journal_reference>
      <dc:creator>Karolina Rudnicka</dc:creator>
    </item>
    <item>
      <title>The "negative end" of change in grammar: terminology, concepts and causes</title>
      <link>https://arxiv.org/abs/2502.04729</link>
      <description>arXiv:2502.04729v1 Announce Type: cross 
Abstract: The topic of "negative end" of change is, contrary to the fields of innovation and emergence, largely under-researched. Yet, it has lately started to gain an increasing attention from language scholars worldwide. The main focus of this article is threefold, namely to discuss the i) terminology; ii) concepts and iii) causes associated with the "negative end" of change in grammar. The article starts with an overview of research conducted on the topic. It then moves to situating phenomena referred to as loss, decline or obsolescence among processes of language change, before elaborating on the terminology and concepts behind it. The last part looks at possible causes for constructions to display a (gradual or rapid, but very consistent) decrease in the frequency of use over time, which continues until the construction disappears or there are only residual or fossilised forms left. Keywords: loss, obsolescence, decline, competition, higher</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04729v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1515/lingvan-2020-0091</arxiv:DOI>
      <arxiv:journal_reference>2021, Linguistics Vanguard 7 (1), 20200091: 1-9</arxiv:journal_reference>
      <dc:creator>Karolina Rudnicka</dc:creator>
    </item>
    <item>
      <title>On the Inference of Sociodemographics on Reddit</title>
      <link>https://arxiv.org/abs/2502.05049</link>
      <description>arXiv:2502.05049v1 Announce Type: cross 
Abstract: Inference of sociodemographic attributes of social media users is an essential step for computational social science (CSS) research to link online and offline behavior. However, there is a lack of a systematic evaluation and clear guidelines for optimal methodologies for this task on Reddit, one of today's largest social media. In this study, we fill this gap by comparing state-of-the-art (SOTA) and probabilistic models.
  To this end, first we collect a novel data set of more than 850k self-declarations on age, gender, and partisan affiliation from Reddit comments. Then, we systematically compare alternatives to the widely used embedding-based model and labeling techniques for the definition of the ground-truth. We do so on two tasks: ($i$) predicting binary labels (classification); and ($ii$)~predicting the prevalence of a demographic class among a set of users (quantification).
  Our findings reveal that Naive Bayes models not only offer transparency and interpretability by design but also consistently outperform the SOTA. Specifically, they achieve an improvement in ROC AUC of up to $19\%$ and maintain a mean absolute error (MAE) below $15\%$ in quantification for large-scale data settings. Finally, we discuss best practices for researchers in CSS, emphasizing coverage, interpretability, reliability, and scalability.
  The code and model weights used for the experiments are publicly available.\footnote{https://anonymous.4open.science/r/SDI-submission-5234}</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05049v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Federico Cinus, Corrado Monti, Paolo Bajardi, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>Mining a Decade of Event Impacts on Contributor Dynamics in Ethereum: A Longitudinal Study</title>
      <link>https://arxiv.org/abs/2502.05054</link>
      <description>arXiv:2502.05054v1 Announce Type: cross 
Abstract: We analyze developer activity across 10 major Ethereum repositories (totaling 129884 commits, 40550 issues) spanning 10 years to examine how events such as technical upgrades, market events, and community decisions impact development. Through statistical, survival, and network analyses, we find that technical events prompt increased activity before the event, followed by reduced commit rates afterwards, whereas market events lead to more reactive development. Core infrastructure repositories like Go-Ethereum exhibit faster issue resolution compared to developer tools, and technical events enhance core team collaboration. Our findings show how different types of events shape development dynamics, offering insights for project managers and developers in maintaining development momentum through major transitions. This work contributes to understanding the resilience of development communities and their adaptation to ecosystem changes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05054v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Vaccargiu, Sabrina Aufiero, Cheick Ba, Silvia Bartolucci, Richard Clegg, Daniel Graziotin, Rumyana Neykova, Roberto Tonelli, Giuseppe Destefanis</dc:creator>
    </item>
    <item>
      <title>Croc: An End-to-End Open-Source Extensible RISC-V MCU Platform to Democratize Silicon</title>
      <link>https://arxiv.org/abs/2502.05090</link>
      <description>arXiv:2502.05090v1 Announce Type: cross 
Abstract: Ensuring a continuous and growing influx of skilled chip designers and a smooth path from education to innovation are key goals for several national and international "Chips Acts". Silicon democratization can greatly benefit from end-to-end (from silicon technology to software) free and open-source (OS) platforms. We present Croc, an extensible RISC-V microcontroller platform explicitly targeted at hands-on teaching and innovation. Croc features a streamlined OS synthesis and an end-to-end OS implementation flow, ensuring full, unconstrained access to the design, the design automation tools, and the implementation technology. Croc uses the industry-proven, open-source CVE2 core, implementing the RV32I(EMC) instruction set architecture (ISA), enabling students to define and implement their own ISA extensions. MLEM, a tapeout of Croc in IHP's open 130 nm node completed in eight weeks by a team of just two students, demonstrates the platform's viability for hands-on teaching in schools, universities, or even on a self-education path. In spring 2025, ETH Zurich will utilize Croc for its curricular VLSI class, involving up to 80 students, producing up to 40 OS application-specific integrated circuit layouts, and completing up to five student-led system-on-chip tapeouts. The lecture notes and exercises are already available under a Creative Commons license.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05090v1</guid>
      <category>cs.AR</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Phillippe Sauter, Thomas Benz, Paul Scheffler, Hannah Pochert, Luisa W\"uthrich, Martin Povi\v{s}er, Beat Muheim, Frank K. G\"urkaynak, Luca Benini</dc:creator>
    </item>
    <item>
      <title>Quantitative Insights into Large Language Model Usage and Trust in Academia: An Empirical Study</title>
      <link>https://arxiv.org/abs/2409.09186</link>
      <description>arXiv:2409.09186v2 Announce Type: replace 
Abstract: Large Language Models (LLMs) are transforming writing, reading, teaching, and knowledge retrieval in many academic fields. However, concerns regarding their misuse and erroneous outputs have led to varying degrees of trust in LLMs within academic communities. In response, various academic organizations have proposed and adopted policies regulating their usage. However, these policies are not based on substantial quantitative evidence because there is no data about use patterns and user opinion. Consequently, there is a pressing need to accurately quantify their usage, user trust in outputs, and concerns about key issues to prioritize in deployment. This study addresses these gaps through a quantitative user study of LLM usage and trust in academic research and education. Specifically, our study surveyed 125 individuals at a private R1 research university regarding their usage of LLMs, their trust in LLM outputs, and key issues to prioritize for robust usage in academia. Our findings reveal: (1) widespread adoption of LLMs, with 75% of respondents actively using them; (2) a significant positive correlation between trust and adoption, as well as between engagement and trust; and (3) that fact-checking is the most critical concern. These findings suggest a need for policies that address pervasive usage, prioritize fact-checking mechanisms, and accurately calibrate user trust levels as they engage with these models. These strategies can help balance innovation with accountability and help integrate LLMs into the academic environment effectively and reliably.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09186v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Minseok Jung, Aurora Zhang, May Fung, Junho Lee, Paul Pu Liang</dc:creator>
    </item>
    <item>
      <title>Global Perspectives of AI Risks and Harms: Analyzing the Negative Impacts of AI Technologies as Prioritized by News Media</title>
      <link>https://arxiv.org/abs/2501.14040</link>
      <description>arXiv:2501.14040v2 Announce Type: replace 
Abstract: Emerging AI technologies have the potential to drive economic growth and innovation but can also pose significant risks to society. To mitigate these risks, governments, companies, and researchers have contributed regulatory frameworks, risk assessment approaches, and safety benchmarks, but these can lack nuance when considered in global deployment contexts. One way to understand these nuances is by looking at how the media reports on AI, as news media has a substantial influence on what negative impacts of AI are discussed in the public sphere and which impacts are deemed important. In this work, we analyze a broad and diverse sample of global news media spanning 27 countries across Asia, Africa, Europe, Middle East, North America, and Oceania to gain valuable insights into the risks and harms of AI technologies as reported and prioritized across media outlets in different countries. This approach reveals a skewed prioritization of Societal Risks followed by Legal &amp; Rights-related Risks, Content Safety Risks, Cognitive Risks, Existential Risks, and Environmental Risks, as reflected in the prevalence of these risk categories in the news coverage of different nations. Furthermore, it highlights how the distribution of such concerns varies based on the political bias of news outlets, underscoring the political nature of AI risk assessment processes and public opinion. By incorporating views from various regions and political orientations for assessing the risks and harms of AI, this work presents stakeholders, such as AI developers and policy makers, with insights into the AI risks categories prioritized in the public sphere. These insights may guide the development of more inclusive, safe, and responsible AI technologies that address the diverse concerns and needs across the world.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14040v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mowafak Allaham, Kimon Kieslich, Nicholas Diakopoulos</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Education: ChemTAsk -- An Open-Source Paradigm for Automated Q&amp;A in the Graduate Classroom</title>
      <link>https://arxiv.org/abs/2502.00016</link>
      <description>arXiv:2502.00016v2 Announce Type: replace 
Abstract: Large language models (LLMs) show promise for aiding graduate level education, but are limited by their training data and potential confabulations. We developed ChemTAsk, an open-source pipeline that combines LLMs with retrieval-augmented generation (RAG) to provide accurate, context-specific assistance. ChemTAsk utilizes course materials, including lecture transcripts and primary publications, to generate accurate responses to student queries. Over nine weeks in an advanced biological chemistry course at the University of Pennsylvania, students could opt in to use ChemTAsk for assistance in any assignment or to understand class material. Comparative analysis showed ChemTAsk performed on par with human teaching assistants (TAs) in understanding student queries and providing accurate information, particularly excelling in creative problem-solving tasks. In contrast, TAs were more precise in their responses and tailored their assistance to the specifics of the class. Student feedback indicated that ChemTAsk was perceived as correct, helpful, and faster than TAs. Open-source and proprietary models from Meta and OpenAI respectively were tested on an original biological chemistry benchmark for future iterations of ChemTAsk. It was found that OpenAI models were more tolerant to deviations in the input prompt and excelled in self-assessment to safeguard for potential confabulations. Taken together, ChemTAsk demonstrates the potential of integrating LLMs with RAG to enhance educational support, offering a scalable tool for students and educators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00016v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryann M. Perez, Marie Shimogawa, Yanan Chang, Hoang Anh T. Phan, Jason G. Marmorstein, Evan S. K. Yanagawa, E. James Petersson</dc:creator>
    </item>
    <item>
      <title>Stop treating `AGI' as the north-star goal of AI research</title>
      <link>https://arxiv.org/abs/2502.03689</link>
      <description>arXiv:2502.03689v2 Announce Type: replace 
Abstract: The AI research community plays a vital role in shaping the scientific, engineering, and societal goals of AI research. In this position paper, we argue that focusing on the highly contested topic of `artificial general intelligence' (`AGI') undermines our ability to choose effective goals. We identify six key traps -- obstacles to productive goal setting -- that are aggravated by AGI discourse: Illusion of Consensus, Supercharging Bad Science, Presuming Value-Neutrality, Goal Lottery, Generality Debt, and Normalized Exclusion. To avoid these traps, we argue that the AI research community needs to (1) prioritize specificity in engineering and societal goals, (2) center pluralism about multiple worthwhile approaches to multiple valuable goals, and (3) foster innovation through greater inclusion of disciplines and communities. Therefore, the AI research community needs to stop treating `AGI' as the north-star goal of AI research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03689v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Borhane Blili-Hamelin, Christopher Graziul, Leif Hancox-Li, Hananel Hazan, El-Mahdi El-Mhamdi, Avijit Ghosh, Katherine Heller, Jacob Metcalf, Fabricio Murai, Eryk Salvaggio, Andrew Smart, Todd Snider, Mariame Tighanimine, Talia Ringer, Margaret Mitchell, Shiri Dori-Hacohen</dc:creator>
    </item>
    <item>
      <title>Digital Gatekeeping: An Audit of Search Engine Results shows tailoring of queries on the Israel-Palestine Conflict</title>
      <link>https://arxiv.org/abs/2502.04266</link>
      <description>arXiv:2502.04266v2 Announce Type: replace 
Abstract: Search engines, often viewed as reliable gateways to information, tailor search results using customization algorithms based on user preferences, location, and more. While this can be useful for routine queries, it raises concerns when the topics are sensitive or contentious, possibly limiting exposure to diverse viewpoints and increasing polarization.
  To examine the extent of this tailoring, we focused on the Israel-Palestine conflict and developed a privacy-protecting tool to audit the behavior of three search engines: DuckDuckGo, Google and Yahoo. Our study focused on two main questions: (1) How do search results for the same query about the conflict vary among different users? and (2) Are these results influenced by the user's location and browsing history?
  Our findings revealed significant customization based on location and browsing preferences, unlike previous studies that found only mild personalization for general topics. Moreover, queries related to the conflict were more customized than unrelated queries, and the results were not neutral concerning the conflict's portrayal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04266v2</guid>
      <category>cs.CY</category>
      <category>cs.IR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\'Iris Dami\~ao, Jos\'e M. Reis, Paulo Almeida, Nuno Santos, Joana Gon\c{c}alves-S\'a</dc:creator>
    </item>
    <item>
      <title>The DSA Transparency Database: Auditing Self-reported Moderation Actions by Social Media</title>
      <link>https://arxiv.org/abs/2312.10269</link>
      <description>arXiv:2312.10269v4 Announce Type: replace-cross 
Abstract: Since September 2023, the Digital Services Act (DSA) obliges large online platforms to submit detailed data on each moderation action they take within the European Union (EU) to the DSA Transparency Database. From its inception, this centralized database has sparked scholarly interest as an unprecedented and potentially unique trove of data on real-world online moderation. Here, we thoroughly analyze all 353.12M records submitted by the eight largest social media platforms in the EU during the first 100 days of the database. Specifically, we conduct a platform-wise comparative study of their: volume of moderation actions, grounds for decision, types of applied restrictions, types of moderated content, timeliness in undertaking and submitting moderation actions, and use of automation. Furthermore, we systematically cross-check the contents of the database with the platforms' own transparency reports. Our analyses reveal that (i) the platforms adhered only in part to the philosophy and structure of the database, (ii) the structure of the database is partially inadequate for the platforms' reporting needs, (iii) the platforms exhibited substantial differences in their moderation actions, (iv) a remarkable fraction of the database data is inconsistent, (v) the platform X (formerly Twitter) presents the most inconsistencies. Our findings have far-reaching implications for policymakers and scholars across diverse disciplines. They offer guidance for future regulations that cater to the reporting needs of online platforms in general, but also highlight opportunities to improve and refine the database itself.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.10269v4</guid>
      <category>cs.SI</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3711085</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of The 28th 2025 ACM Conference on Computer-Supported Cooperative Work and Social Computing (CSCW'25)</arxiv:journal_reference>
      <dc:creator>Amaury Trujillo, Tiziano Fagni, Stefano Cresci</dc:creator>
    </item>
    <item>
      <title>From Data Creator to Data Reuser: Distance Matters</title>
      <link>https://arxiv.org/abs/2402.07926</link>
      <description>arXiv:2402.07926v3 Announce Type: replace-cross 
Abstract: Sharing research data is necessary, but not sufficient, for data reuse. Open science policies focus more heavily on data sharing than on reuse, yet both are complex, labor-intensive, expensive, and require infrastructure investments by multiple stakeholders. The value of data reuse lies in relationships between creators and reusers. By addressing knowledge exchange, rather than mere transactions between stakeholders, investments in data management and knowledge infrastructures can be made more wisely. Drawing upon empirical studies of data sharing and reuse, we develop the metaphor of distance between data creator and data reuser, identifying six dimensions of distance that influence the ability to transfer knowledge effectively: domain, methods, collaboration, curation, purposes, and time and temporality. We explore how social and socio-technical aspects of these dimensions may decrease -- or increase -- distances to be traversed between creators and reusers. Our theoretical framing of the distance between data creators and prospective reusers leads to recommendations to four categories of stakeholders on how to make data sharing and reuse more effective: data creators, data reusers, data archivists, and funding agencies. 'It takes a village' to share research data -- and a village to reuse data. Our aim is to provoke new research questions, new research, and new investments in effective and efficient circulation of research data; and to identify criteria for investments at each stage of data and research life cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07926v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1162/99608f92.35d32cfc</arxiv:DOI>
      <arxiv:journal_reference>Harvard Data Science Review, 7(2) (2025, April)</arxiv:journal_reference>
      <dc:creator>Christine L. Borgman, Paul T. Groth</dc:creator>
    </item>
    <item>
      <title>AI Sandbagging: Language Models can Strategically Underperform on Evaluations</title>
      <link>https://arxiv.org/abs/2406.07358</link>
      <description>arXiv:2406.07358v4 Announce Type: replace-cross 
Abstract: Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of sandbagging, which we define as strategic underperformance on an evaluation. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07358v4</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teun van der Weij, Felix Hofst\"atter, Ollie Jaffe, Samuel F. Brown, Francis Rhys Ward</dc:creator>
    </item>
    <item>
      <title>CollabEdit: Towards Non-destructive Collaborative Knowledge Editing</title>
      <link>https://arxiv.org/abs/2410.09508</link>
      <description>arXiv:2410.09508v3 Announce Type: replace-cross 
Abstract: Collaborative learning of large language models (LLMs) has emerged as a new paradigm for utilizing private data from different parties to guarantee efficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also garnered increased attention due to its ability to manipulate the behaviors of LLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits of multiple parties are aggregated in a privacy-preserving and continual manner) unexamined. To this end, this manuscript dives into the first investigation of collaborative KE, in which we start by carefully identifying the unique three challenges therein, including knowledge overlap, knowledge conflict, and knowledge forgetting. We then propose a non-destructive collaborative KE framework, COLLABEDIT, which employs a novel model merging mechanism to mimic the global KE behavior while preventing the severe performance drop. Extensive experiments on two canonical datasets demonstrate the superiority of COLLABEDIT compared to other destructive baselines, and results shed light on addressing three collaborative KE challenges and future applications. Our code is available at https://github.com/LINs-lab/CollabEdit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09508v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiamu Zheng, Jinghuai Zhang, Tianyu Du, Xuhong Zhang, Jianwei Yin, Tao Lin</dc:creator>
    </item>
    <item>
      <title>Extending and Applying Automated HERMES Software Publication Workflows</title>
      <link>https://arxiv.org/abs/2410.17614</link>
      <description>arXiv:2410.17614v2 Announce Type: replace-cross 
Abstract: Research software is an important output of research and must be published according to the FAIR Principles for Research Software. This can be achieved by publishing software with metadata under a persistent identifier. HERMES is a tool that leverages continuous integration to automate the publication of software with rich metadata. In this work, we describe the HERMES workflow itself, and how to extend it to meet the needs of specific research software metadata or infrastructure. We introduce the HERMES plugin architecture and provide the example of creating a new HERMES plugin that harvests metadata from a metadata source in source code repositories. We show how to use HERMES as an end user, both via the command line interface, and as a step in a continuous integration pipeline. Finally, we report three informal case studies whose results provide a preliminary evaluation of the feasibility and applicability of HERMES workflows, and the extensibility of the hermes software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17614v2</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Mon, 10 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sophie Kernchen, Michael Meinel, Stephan Druskat, Michael Fritzsche, David Pape, Oliver Bertuch</dc:creator>
    </item>
  </channel>
</rss>
