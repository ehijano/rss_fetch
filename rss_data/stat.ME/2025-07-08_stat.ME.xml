<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 09 Jul 2025 01:33:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multiple data-driven missing imputation</title>
      <link>https://arxiv.org/abs/2507.03061</link>
      <description>arXiv:2507.03061v1 Announce Type: new 
Abstract: This paper introduces KZImputer, a novel adaptive imputation method for univariate time series designed for short to medium-sized missed points (gaps) (1-5 points and beyond) with tailored strategies for segments at the start, middle, or end of the series. KZImputer employs a hybrid strategy to handle various missing data scenarios. Its core mechanism differentiates between gaps at the beginning, middle, or end of the series, applying tailored techniques at each position to optimize imputation accuracy. The method leverages linear interpolation and localized statistical measures, adapting to the characteristics of the surrounding data and the gap size. The performance of KZImputer has been systematically evaluated against established imputation techniques, demonstrating its potential to enhance data quality for subsequent time series analysis. This paper describes the KZImputer methodology in detail and discusses its effectiveness in improving the integrity of time series data. Empirical analysis demonstrates that KZImputer achieves particularly strong performance for datasets with high missingness rates (around 50% or more), maintaining stable and competitive results across statistical and signal-reconstruction metrics. The method proves especially effective in high-sparsity regimes, where traditional approaches typically experience accuracy degradation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03061v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.15663430</arxiv:DOI>
      <dc:creator>Sergii Kavun</dc:creator>
    </item>
    <item>
      <title>Causal interpretation of the sibling comparison and its relation to the cross-over design</title>
      <link>https://arxiv.org/abs/2507.03464</link>
      <description>arXiv:2507.03464v1 Announce Type: new 
Abstract: The intuitive motivation for employing a sibling comparison design is to adjust for confounding that is constant within families. Such confounding can be caused by variables that otherwise might prove difficult to measure, for example factors relating to genetics, environment, and upbringing. Recent methodological investigations have shown that despite its intuitive appeal, the conventionally employed analysis does not relate to a well-defined causal target, even in the case of constant confounding. A main challenge is that the analysis will target the subpopulation of exposure discordant pairs. In the presence of an effect of the cosibling's exposure on the sibling's outcome, there is a second challenge, namely that the effect corresponds to an intervention that always exposes the cosibling to the opposite exposure from the sibling. We characterise the sibling comparison in terms of the cross-over design. Estimands of interest are discussed before using this characterisation to establish more natural conditions for targeting an appropriate causal parameter. We cast the above-mentioned challenges of the sibling comparison in terms of those facing the cross-over trialist: in order to target an appropriate estimand one must be able to argue the absence of a certain type of carry-over effect as well as absence of trial-by-treatment interaction, thus establishing that the former study design emulates the latter warts and all. We explore weighting to counter the effects of such interactions and to target other estimands. The weights rely on estimates of the unobserved confounding structure. Through simulations and an example analysis, we illustrate its potential usefulness to assess the validity of the assumptions of the matched analysis. We briefly discuss an extension of the weighting procedure to remove selection bias based on data from a population-level reference sample.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03464v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Bang Kristensen, Christian Bressen Pipper, Jacob von Bornemann Hjelmborg</dc:creator>
    </item>
    <item>
      <title>Small Area Estimation of Fertility in Low- and Middle-Income Countries</title>
      <link>https://arxiv.org/abs/2507.03584</link>
      <description>arXiv:2507.03584v1 Announce Type: new 
Abstract: Accurate fertility estimates at fine spatial resolution are essential for localized public health planning, particularly in low- and middle-income countries (LMICs). While national-level indicators such as age-specific fertility rates (ASFR) and total fertility rate (TFR) are often reported through official statistics, they lack the spatial granularity needed to guide targeted interventions. To address this, we develop a framework for subnational fertility estimation using small-area estimation (SAE) techniques applied to birth history data from household surveys, in particular Demographic and Health Surveys (DHS). Disaggregation by geographic area, time period, and maternal age group leads to significant data sparsity, limiting the reliability of direct estimates at fine scales. To overcome this, we propose a suite of methods, including direct estimators, area-level and unit-level Bayesian hierarchical models, to produce accurate estimates across varying spatial resolutions. The model-based approaches incorporate spatiotemporal smoothing and integrate covariates such as maternal education, contraceptive use and urbanicity. Using data from the 2021 Madagascar DHS, we generate district-level ASFR and TFR estimates and evaluate model performance through cross-validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03584v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yunhan Wu, Jon Wakefield</dc:creator>
    </item>
    <item>
      <title>Multivariate MRP</title>
      <link>https://arxiv.org/abs/2507.03652</link>
      <description>arXiv:2507.03652v1 Announce Type: new 
Abstract: Measuring public opinion at subnational geographies is critical to many theories in political science. Multilevel regression and post-stratification (MRP) is a popular tool for doing so, although existing work is limited to measuring opinion on a single survey question. We provide a framework for estimating the joint distribution of opinion on multiple questions ("Multivariate MRP"). To do so, we derive a novel method for variational inference in multinomial logistic regression with many random effects. This requires performing variational inference with high-dimensional fixed effects, but we show that this can be done at a low computational cost. We validate this procedure by estimating public opinion by party in the United States and show that existing methods can be improved considerably by adding contextual covariates on the prior levels of party identification. Substantively, we show how the output of multivariate MRP can be used to study representation across multiple policy issues simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03652v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Goplerud, Michael Auslen</dc:creator>
    </item>
    <item>
      <title>Differentially private scale testing via rank transformations and percentile modifications</title>
      <link>https://arxiv.org/abs/2507.03725</link>
      <description>arXiv:2507.03725v1 Announce Type: new 
Abstract: We develop a class of differentially private two-sample scale tests, called the rank-transformed percentile-modified Siegel--Tukey tests, or RPST tests. These RPST tests are inspired both by recent differentially private extensions of some common rank tests and some older modifications to non-private rank tests. We present the asymptotic distribution of the RPST test statistic under the null hypothesis, under a very general condition on the rank transformation. We also prove RPST tests are differentially private, and that their type I error does not exceed the given level. We uncover that the growth rate of the rank transformation presents a tradeoff between power and sensitivity. We do extensive simulations to investigate the effects of the tuning parameters and compare to a general private testing framework. Lastly, we show that our techniques can also be used to improve the differentially private signed-rank test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03725v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joshua Levine, Kelly Ramsay</dc:creator>
    </item>
    <item>
      <title>Global p-Values in Multi-Design Studies</title>
      <link>https://arxiv.org/abs/2507.03815</link>
      <description>arXiv:2507.03815v1 Announce Type: new 
Abstract: Replicability issues -- referring to the difficulty or failure of independent researchers to corroborate the results of published studies -- have hindered the meaningful progression of science and eroded public trust in scientific findings. In response to the replicability crisis, one approach is the use of multi-design studies, which incorporate multiple analysis strategies to address a single research question. However, there remains a lack of methods for effectively combining outcomes in multi-design studies. In this paper, we propose a unified framework based on the g-value, for global p-value, which enables meaningful aggregation of outcomes from all the considered analysis strategies in multi-design studies. Our framework mitigates the risk of selective reporting while rigorously controlling type I error rates. At the same time, it maintains statistical power and reduces the likelihood of overlooking true positive effects. Importantly, our method is flexible and broadly applicable across various scientific domains and outcome results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03815v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Coqueret, Yuming Zhang, Christophe P\'erignon, Francesca Chiaromonte, St\'ephane Guerrier</dc:creator>
    </item>
    <item>
      <title>On relation between separable indirect effect, natural indirect effect, and interventional indirect effect</title>
      <link>https://arxiv.org/abs/2507.03879</link>
      <description>arXiv:2507.03879v1 Announce Type: new 
Abstract: Recently, the separable indirect effect (SIE) has gained attention due to its identifiability without requiring the untestable cross-world assumption necessary for the natural indirect effect (NIE). This article systematically compares the causal assumptions underlying the SIE, NIE, and interventional indirect effect (IIE) and evaluates their feasibility for mediational interpretation using the mediation null criterion, with a particular focus on the SIE. We demonstrate that, in the absence of intermediate confounders, the SIE lacks a mediational interpretation unless additional unverifiable assumptions are imposed. When intermediate confounders are present, separable effect methods fail to accurately capture the indirect effect, whereas the NIE still satisfy the mediation null criterion. Additionally, we present a new identification result for the NIE in the presence of intermediate confounders. Finally, we propose an integrated framework for practical analysis. This article emphasizes that the NIE is the most fundamental definition of indirect effect among the three measures and highlights the trade-off between mediational interpretability and assumption falsifiability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03879v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan-Lin Chen, Sheng-Hsuan Lin</dc:creator>
    </item>
    <item>
      <title>A New and Efficient Debiased Estimation of General Treatment Models by Balanced Neural Networks Weighting</title>
      <link>https://arxiv.org/abs/2507.04044</link>
      <description>arXiv:2507.04044v1 Announce Type: new 
Abstract: Estimation and inference of treatment effects under unconfounded treatment assignments often suffer from bias and the `curse of dimensionality' due to the nonparametric estimation of nuisance parameters for high-dimensional confounders. Although debiased state-of-the-art methods have been proposed for binary treatments under particular treatment models, they can be unstable for small sample sizes. Moreover, directly extending them to general treatment models can lead to computational complexity. We propose a balanced neural networks weighting method for general treatment models, which leverages deep neural networks to alleviate the curse of dimensionality while retaining optimal covariate balance through calibration, thereby achieving debiased and robust estimation. Our method accommodates a wide range of treatment models, including average, quantile, distributional, and asymmetric least squares treatment effects, for discrete, continuous, and mixed treatments. Under regularity conditions, we show that our estimator achieves rate double robustness and $\sqrt{N}$-asymptotic normality, and its asymptotic variance achieves the semiparametric efficiency bound. We further develop a statistical inference procedure based on weighted bootstrap, which avoids estimating the efficient influence/score functions. Simulation results reveal that the proposed method consistently outperforms existing alternatives, especially when the sample size is small. Applications to the 401(k) dataset and the Mother's Significant Features dataset further illustrate the practical value of the method for estimating both average and quantile treatment effects under binary and continuous treatments, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04044v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeqi Wu, Meilin Wang, Wei Huang, Zheng Zhang</dc:creator>
    </item>
    <item>
      <title>Adaptive Designs in Fast-Track Registration Processes</title>
      <link>https://arxiv.org/abs/2507.04092</link>
      <description>arXiv:2507.04092v1 Announce Type: new 
Abstract: Fast-track procedures play an important role in the context of conditional registration of health products, such as conditional approval processes and listing processes for digital health applications. Fast-track procedures offer the potential for earlier patient access to innovative products. They involve two registration steps. The applicants can apply first for conditional registration. A successful conditional registration provides a limited funding or approval period and time to prepare the application for permanent registration, which is the second step of the registration process. For conditional registration, typically, products have to fulfil only a part of the requirements necessary for permanent registration. There is interest in valid and efficient study designs for fast-track procedures. This will be addressed in this paper. A motivating example of the paper is the German fast-track registration process of digital health applications (DiGA) for reimbursement by statutory health insurances. The main focus of the paper is the systematic investigation of the utility of adaptive designs in the context of fast-track registration processes. The paper also covers a careful discussion of the different requirements found in the guidelines and their consequences. We demonstrate that the use of adaptive designs in the context of fast-track processes like the DiGA registration process is, in most cases, much more efficient than the current standard of two separate studies. The results presented in this paper are based on numerical calculations supported by mathematical arguments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04092v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liane Kluge, Werner Brannath</dc:creator>
    </item>
    <item>
      <title>Generative Regression with IQ-BART</title>
      <link>https://arxiv.org/abs/2507.04168</link>
      <description>arXiv:2507.04168v1 Announce Type: new 
Abstract: Implicit Quantile BART (IQ-BART) posits a non-parametric Bayesian model on the conditional quantile function, acting as a model over a conditional model for $Y$ given $X$. One of the key ingredients is augmenting the observed data $\{(Y_i,X_i)\}_{i=1}^n$ with uniformly sampled values $\tau_i$ for $1\leq i\leq n$ which serve as training data for quantile function estimation. Using the fact that the location parameter $\mu$ in a $\tau$-tilted asymmetric Laplace distribution corresponds to the $\tau^{th}$ quantile, we build a check-loss likelihood targeting $\mu$ as the parameter of interest. We equip the check-loss likelihood parametrized by $\mu=f(X,\tau)$ with a BART prior on $f(\cdot)$, allowing the conditional quantile function to vary both in $X$ and $\tau$. The posterior distribution over $\mu(\tau,X)$ can be then distilled for estimation of the {\em entire quantile function} as well as for assessing uncertainty through the variation of posterior draws. Simulation-based predictive inference is immediately available through inverse transform sampling using the learned quantile function. The sum-of-trees structure over the conditional quantile function enables flexible distribution-free regression with theoretical guarantees. As a byproduct, we investigate posterior mean quantile estimator as an alternative to the routine sample (posterior mode) quantile estimator. We demonstrate the power of IQ-BART on time series forecasting datasets where IQ-BART can capture multimodality in predictive distributions that might be otherwise missed using traditional parametric approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04168v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean O'Hagan, Veronika Ro\v{c}kov\'a</dc:creator>
    </item>
    <item>
      <title>Normalizing Flow to Augmented Posterior: Conditional Density Estimation with Interpretable Dimension Reduction for High Dimensional Data</title>
      <link>https://arxiv.org/abs/2507.04216</link>
      <description>arXiv:2507.04216v1 Announce Type: new 
Abstract: The conditional density characterizes the distribution of a response variable $y$ given other predictor $x$, and plays a key role in many statistical tasks, including classification and outlier detection. Although there has been abundant work on the problem of Conditional Density Estimation (CDE) for a low-dimensional response in the presence of a high-dimensional predictor, little work has been done for a high-dimensional response such as images. The promising performance of normalizing flow (NF) neural networks in unconditional density estimation acts a motivating starting point. In this work, we extend NF neural networks when external $x$ is present. Specifically, they use the NF to parameterize a one-to-one transform between a high-dimensional $y$ and a latent $z$ that comprises two components \([z_P,z_N]\). The $z_P$ component is a low-dimensional subvector obtained from the posterior distribution of an elementary predictive model for $x$, such as logistic/linear regression. The $z_N$ component is a high-dimensional independent Gaussian vector, which explains the variations in $y$ not or less related to $x$. Unlike existing CDE methods, the proposed approach, coined Augmented Posterior CDE (AP-CDE), only requires a simple modification on the common normalizing flow framework, while significantly improving the interpretation of the latent component, since $z_P$ represents a supervised dimension reduction. In image analytics applications, AP-CDE shows good separation of $x$-related variations due to factors such as lighting condition and subject id, from the other random variations. Further, the experiments show that an unconditional NF neural network, based on an unsupervised model of $z$, such as Gaussian mixture, fails to generate interpretable results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04216v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cheng Zeng, George Michailidis, Hitoshi Iyatomi, Leo L Duan</dc:creator>
    </item>
    <item>
      <title>Structural Classification of Locally Stationary Time Series Based on Second-order Characteristics</title>
      <link>https://arxiv.org/abs/2507.04237</link>
      <description>arXiv:2507.04237v1 Announce Type: new 
Abstract: Time series classification is crucial for numerous scientific and engineering applications. In this article, we present a numerically efficient, practically competitive, and theoretically rigorous classification method for distinguishing between two classes of locally stationary time series based on their time-domain, second-order characteristics. Our approach builds on the autoregressive approximation for locally stationary time series, combined with an ensemble aggregation and a distance-based threshold for classification. It imposes no requirement on the training sample size, and is shown to achieve zero misclassification error rate asymptotically when the underlying time series differ only mildly in their second-order characteristics. The new method is demonstrated to outperform a variety of state-of-the-art solutions, including wavelet-based, tree-based, convolution-based methods, as well as modern deep learning methods, through intensive numerical simulations and a real EEG data analysis for epilepsy classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04237v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chen Qian, Xiucai Ding, Lexin Li</dc:creator>
    </item>
    <item>
      <title>A note on the unique properties of the Kullback--Leibler divergence for sampling via gradient flows</title>
      <link>https://arxiv.org/abs/2507.04330</link>
      <description>arXiv:2507.04330v1 Announce Type: new 
Abstract: We consider the problem of sampling from a probability distribution $\pi$. It is well known that this can be written as an optimisation problem over the space of probability distribution in which we aim to minimise a divergence from $\pi$. and The optimisation problem is normally solved through gradient flows in the space of probability distribution with an appropriate metric. We show that the Kullback--Leibler divergence is the only divergence in the family of Bregman divergences whose gradient flow w.r.t. many popular metrics does not require knowledge of the normalising constant of $\pi$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04330v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca Romana Crucinio</dc:creator>
    </item>
    <item>
      <title>Structural Identifiability of Compartmental Models: Recent Progress and Future Directions</title>
      <link>https://arxiv.org/abs/2507.04496</link>
      <description>arXiv:2507.04496v1 Announce Type: new 
Abstract: We summarize recent progress on the theory and applications of structural identifiability of compartmental models. On the applications side, we review identifiability analyses undertaken recently for models arising in epidemiology, oncology, and other areas; and we summarize common approaches for handling models that are unidentifiable. We also highlight recent theoretical and algorithmic results on how to reparametrize unidentifiable models and, in the context of linear compartmental models, how to predict identifiability properties directly from the model structure. Finally, we highlight future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04496v1</guid>
      <category>stat.ME</category>
      <category>math.DS</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolette Meshkat, Anne Shiu</dc:creator>
    </item>
    <item>
      <title>AL-SPCE -- Reliability analysis for nondeterministic models using stochastic polynomial chaos expansions and active learning</title>
      <link>https://arxiv.org/abs/2507.04553</link>
      <description>arXiv:2507.04553v1 Announce Type: new 
Abstract: Reliability analysis typically relies on deterministic simulators, which yield repeatable outputs for identical inputs. However, many real-world systems display intrinsic randomness, requiring stochastic simulators whose outputs are random variables. This inherent variability must be accounted for in reliability analysis. While Monte Carlo methods can handle this, their high computational cost is often prohibitive. To address this, stochastic emulators have emerged as efficient surrogate models capable of capturing the random response of simulators at reduced cost. Although promising, current methods still require large training sets to produce accurate reliability estimates, which limits their practicality for expensive simulations. This work introduces an active learning framework to further reduce the computational burden of reliability analysis using stochastic emulators. We focus on stochastic polynomial chaos expansions (SPCE) and propose a novel learning function that targets regions of high predictive uncertainty relevant to failure probability estimation. To quantify this uncertainty, we exploit the asymptotic normality of the maximum likelihood estimator. The resulting method, named active learning stochastic polynomial chaos expansions (AL-SPCE), is applied to three test cases. Results demonstrate that AL-SPCE maintains high accuracy in reliability estimates while significantly improving efficiency compared to conventional surrogate-based methods and direct Monte Carlo simulation. This confirms the potential of active learning in enhancing the practicality of stochastic reliability analysis for complex, computationally expensive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04553v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>A. Pires, M. Moustapha, S. Marelli, B. Sudret</dc:creator>
    </item>
    <item>
      <title>A Test for Jumps in Metric-Space Conditional Means</title>
      <link>https://arxiv.org/abs/2507.04560</link>
      <description>arXiv:2507.04560v1 Announce Type: new 
Abstract: Standard methods for detecting discontinuities in conditional means are not applicable to outcomes that are complex, non-Euclidean objects like distributions, networks, or covariance matrices. This article develops a nonparametric test for jumps in conditional means when outcomes lie in a non-Euclidean metric space. Using local Fr\'echet regression$\unicode{x2014}$which generalizes standard regression to metric-space valued data$\unicode{x2014}$the method estimates a mean path on either side of a candidate cutoff, extending existing k-sample tests to a flexible regression setting. Key theoretical contributions include a central limit theorem for the local estimator of the conditional Fr\'echet variance and the asymptotic validity and consistency of the proposed test. Simulations confirm nominal size control and robust power in finite samples. Two applications demonstrate the method's value by revealing effects invisible to scalar-based tests. First, I detect a sharp change in work-from-home compositions at Washington State's income threshold for non-compete enforceability during COVID-19, highlighting remote work's role as a bargaining margin. Second, I find that countries restructure their input-output networks after losing preferential US trade access. These findings underscore that analyzing regression functions within their native metric spaces can reveal structural discontinuities that scalar summaries would miss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04560v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Van Dijcke</dc:creator>
    </item>
    <item>
      <title>Inverse Probability Weighting for Recurrent Event Models</title>
      <link>https://arxiv.org/abs/2507.04567</link>
      <description>arXiv:2507.04567v1 Announce Type: new 
Abstract: Recurrent events are common and important clinical trial endpoints in many disease areas, e.g., cardiovascular hospitalizations in heart failure, relapses in multiple sclerosis, or exacerbations in asthma. During a trial, patients may experience intercurrent events, that is, events after treatment assignment which affect the interpretation or existence of the outcome of interest. In many settings, a treatment effect in the scenario in which the intercurrent event would not occur is of clinical interest. A proper estimation method of such a hypothetical treatment effect has to account for all confounders of the recurrent event process and the intercurrent event. In this paper, we propose estimators targeting hypothetical estimands in recurrent events with proper adjustments of baseline and internal time-varying covariates. Specifically, we apply inverse probability weighting (IPW) to the commonly used Lin-Wei-Yang-Ying (LWYY) and negative binomial (NB) models in recurrent event analysis. Simulation studies demonstrate that our approach outperforms alternative analytical methods in terms of bias and power.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04567v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tobias Mutze, Richard Cook, Tianmeng Lyu</dc:creator>
    </item>
    <item>
      <title>Bias Corrected Variance Stabilizing Transformation for Small Area Estimation</title>
      <link>https://arxiv.org/abs/2507.04583</link>
      <description>arXiv:2507.04583v1 Announce Type: new 
Abstract: Small area estimation models are typically based on the normality assumption of response variables. More recently, attention has been drawn to the transformation of the original variables to justify the assumption of normality. Variance stabilizing transformation of observation serves the dual purpose of reaching closer to normality, as well as known variance of the transformed variables in contrast to the assumption of known variances of the original variables, the latter needed to avoid non-identifiability. However, the existing literature on the topic ignores a certain bias introduced in the seemingly correct back transformation. The present paper rectifies this deficiency by introducing asymptotically unbiased empirical Bayes (EB) estimators of small area means. Mean squared errors (MSEs) and estimated MSEs of such estimators are provided. The theoretical results were accompanied with simulations and data analysis. A somewhat surprising phenomenon is a finding which connects one of our results to the natural exponential family quadratic variance function (NEF-QVF) family of distributions introduced by Morris (1982,1983).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04583v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masayo Y. Hirose, Malay Ghosh, Mayumi Oka</dc:creator>
    </item>
    <item>
      <title>Forward Variable Selection in Ultra-High Dimensional Linear Regression Using Gram-Schmidt Orthogonalization</title>
      <link>https://arxiv.org/abs/2507.04668</link>
      <description>arXiv:2507.04668v1 Announce Type: new 
Abstract: We investigate forward variable selection for ultra-high dimensional linear regression using a Gram-Schmidt orthogonalization procedure. Unlike the commonly used Forward Regression (FR) method, which computes regression residuals using an increasing number of selected features, or the Orthogonal Greedy Algorithm (OGA), which selects variables based on their marginal correlations with the residuals, our proposed Gram-Schmidt Forward Regression (GSFR) simplifies the selection process by evaluating marginal correlations between the residuals and the orthogonalized new variables. Moreover, we introduce a new model size selection criterion that determines the number of selected variables by detecting the most significant change in their unique contributions, effectively filtering out redundant predictors along the selection path. While GSFR is theoretically equivalent to FR except for the stopping rule, our refinement and the newly proposed stopping rule significantly improve computational efficiency. In ultra-high dimensional settings, where the dimensionality far exceeds the sample size and predictors exhibit strong correlations, we establish that GSFR achieves a convergence rate comparable to OGA and ensures variable selection consistency under mild conditions. We demonstrate the proposed method {using} simulations and real data examples. Extensive numerical studies show that GSFR outperforms commonly used methods in ultra-high dimensional variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04668v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jialuo Chen, Zhaoxing Gao, Ruey S. Tsay</dc:creator>
    </item>
    <item>
      <title>Graph Estimation Based on Neighborhood Selection for Matrix-variate Data</title>
      <link>https://arxiv.org/abs/2507.04711</link>
      <description>arXiv:2507.04711v1 Announce Type: new 
Abstract: Undirected graphical models are powerful tools for uncovering complex relationships among high-dimensional variables. This paper aims to fully recover the structure of an undirected graphical model when the data naturally take matrix form, such as temporal multivariate data.
  As conventional vector-variate analyses have clear limitations in handling such matrix-structured data, several approaches have been proposed, mostly relying on the likelihood of the Gaussian distribution with a separable covariance structure. Although some of these methods provide theoretical guarantees against false inclusions (i.e. all identified edges exist in the true graph), they may suffer from crucial limitations: (1) failure to detect important true edges, or (2) dependency on conditions for the estimators that have not been verified.
  We propose a novel regression-based method for estimating matrix graphical models, based on the relationship between partial correlations and regression coefficients. Adopting the primal-dual witness technique from the regression framework, we derive a non-asymptotic inequality for exact recovery of an edge set. Under suitable regularity conditions, our method consistently identifies the true edge set with high probability.
  Through simulation studies, we compare the support recovery performance of the proposed method against existing alternatives. We also apply our method to an electroencephalography (EEG) dataset to estimate both the spatial brain network among 64 electrodes and the temporal network across 256 time points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04711v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsub Shin, Johan Lim, Seongoh Park</dc:creator>
    </item>
    <item>
      <title>Optimal Exact Designs of Multiresponse Experiments under Linear and Sparsity Constraints</title>
      <link>https://arxiv.org/abs/2507.04713</link>
      <description>arXiv:2507.04713v1 Announce Type: new 
Abstract: We propose a computational approach to constructing exact designs on finite design spaces that are optimal for multiresponse regression experiments under a combination of the standard linear and specific 'sparsity' constraints. The linear constraints address, for example, limits on multiple resource consumption and the problem of optimal design augmentation, while the sparsity constraints control the set of distinct trial conditions utilized by the design. The key idea is to construct an artificial optimal design problem that can be solved using any existing mathematical programming technique for univariate-response optimal designs under pure linear constraints. The solution to this artificial problem can then be directly converted into an optimal design for the primary multivariate-response setting with combined linear and sparsity constraints. We demonstrate the utility and flexibility of the approach through dose-response experiments with constraints on safety, efficacy, and cost, where cost also depends on the number of distinct doses used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04713v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lenka Filov\'a, P\'al Somogyi, Radoslav Harman</dc:creator>
    </item>
    <item>
      <title>The connection of the stability of the binary choice model with its discriminatory power</title>
      <link>https://arxiv.org/abs/2507.04866</link>
      <description>arXiv:2507.04866v1 Announce Type: new 
Abstract: The key indicators of model stability are the population stability index (PSI), which uses the difference in population distribution, and the Kolmogorov-Smirnov statistic (KS) between two distributions. When deriving a binary choice model, the question arises about the real Gini index for any new model. The paper shows that when the Gini changes, the real Gini index should be less than the obtained Gini index. This type is included in the equation using a formula, and the PSI formula in KS is also included based on the scoring indicator. The error in calculating the Gini index of the equation is unavoidable, so it is necessary to always rely on the calculation formula. This type of research is suitable for a wide range of tasks where it is necessary to consider the error in scoring the indicator at any length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04866v1</guid>
      <category>stat.ME</category>
      <category>q-fin.RM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Pomazanov</dc:creator>
    </item>
    <item>
      <title>A cautionary tale of model misspecification and identifiability</title>
      <link>https://arxiv.org/abs/2507.04894</link>
      <description>arXiv:2507.04894v1 Announce Type: new 
Abstract: Mathematical models are routinely applied to interpret biological data, with common goals that include both prediction and parameter estimation. A challenge in mathematical biology, in particular, is that models are often complex and non-identifiable, while data are limited. Rectifying identifiability through simplification can seemingly yield more precise parameter estimates, albeit, as we explore in this perspective, at the potentially catastrophic cost of introducing model misspecification and poor accuracy. We demonstrate how uncertainty in model structure can be propagated through to uncertainty in parameter estimates using a semi-parametric Gaussian process approach that delineates parameters of interest from uncertainty in model terms. Specifically, we study generalised logistic growth with an unknown crowding function, and a spatially resolved process described by a partial differential equation with a time-dependent diffusivity parameter. Allowing for structural model uncertainty yields more robust and accurate parameter estimates, and a better quantification of remaining uncertainty. We conclude our perspective by discussing the connections between identifiability and model misspecification, and alternative approaches to dealing with model misspecification in mathematical biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04894v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander P Browning, Jennifer A Flegg, Ryan J Murphy</dc:creator>
    </item>
    <item>
      <title>Covariance test for discretely observed functional data: when and how it works?</title>
      <link>https://arxiv.org/abs/2507.04962</link>
      <description>arXiv:2507.04962v1 Announce Type: new 
Abstract: For covariance test in functional data analysis, existing methods are developed only for fully observed curves, whereas in practice, trajectories are typically observed discretely and with noise. To bridge this gap, we employ a pool-smoothing strategy to construct an FPC-based test statistic, allowing the number of estimated eigenfunctions to grow with the sample size. This yields a consistently nonparametric test, while the challenge arises from the concurrence of diverging truncation and discretized observations. Facilitated by advancing perturbation bounds of estimated eigenfunctions, we establish that the asymptotic null distribution remains valid across permissable truncation levels. Moreover, when the sampling frequency (i.e., the number of measurements per subject) reaches certain magnitude of sample size, the test behaves as if the functions were fully observed. This phase transition phenomenon differs from the well-known result of the pooling mean/covariance estimation, reflecting the elevated difficulty in covariance test due to eigen-decomposition. The numerical studies, including simulations and real data examples, yield favorable performance compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04962v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Zhou, Jin Yang, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Sequential multiple importance sampling for high-dimensional Bayesian inference</title>
      <link>https://arxiv.org/abs/2507.05114</link>
      <description>arXiv:2507.05114v1 Announce Type: new 
Abstract: This paper introduces a sequential multiple importance sampling (SeMIS) algorithm for high-dimensional Bayesian inference. The method estimates Bayesian evidence using all generated samples from each proposal distribution while obtaining posterior samples through an importance-resampling scheme. A key innovation of SeMIS is the use of a softly truncated prior distribution as the intermediate proposal, providing a new way bridging prior and posterior distributions. By enabling samples from high-likelihood regions to traverse low-probability zones, SeMIS enhances mode mixing in challenging inference problems. Comparative evaluations against subset simulation (SuS) and adaptive Bayesian updating with structural reliability methods (aBUS) demonstrate that SeMIS achieves superior performance in evidence estimation (lower bias and variance) and posterior sampling (higher effective sample sizes and closer approximation to the true posterior), particularly for multimodal distributions. The efficacy of SeMIS is further validated in a high-dimensional finite element model updating application, where it successfully localizes structural damages by quantifying stiffness loss. The proposed algorithm not only advances Bayesian computation for complex posterior distributions but also provides a robust tool for uncertainty quantification in civil engineering systems, offering new possibilities for probabilistic structural health monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05114v1</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Li Binbin, He Xiao, Liao Zihan</dc:creator>
    </item>
    <item>
      <title>Practical considerations for Gaussian Process modeling for causal inference quasi-experimental studies with panel data</title>
      <link>https://arxiv.org/abs/2507.05128</link>
      <description>arXiv:2507.05128v1 Announce Type: new 
Abstract: Estimating causal effects in quasi-experiments with spatio-temporal panel data often requires adjusting for unmeasured confounding that varies across space and time. Gaussian Processes (GPs) offer a flexible, nonparametric modeling approach that can account for such complex dependencies through carefully chosen covariance kernels. In this paper, we provide a practical and interpretable framework for applying GPs to causal inference in panel data settings. We demonstrate how GPs generalize popular methods such as synthetic control and vertical regression, and we show that the GP posterior mean can be represented as a weighted average of observed outcomes, where the weights reflect spatial and temporal similarity. To support applied use, we explore how different kernel choices impact both estimation performance and interpretability, offering guidance for selecting between separable and nonseparable kernels. Through simulations and application to Hurricane Katrina mortality data, we illustrate how GP models can be used to estimate counterfactual outcomes and quantify treatment effects. All code and materials are made publicly available to support reproducibility and encourage adoption. Our results suggest that GPs are a promising and interpretable tool for addressing unmeasured spatio-temporal confounding in quasi-experimental studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05128v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sofia L. Vega, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Predictive posteriors under hidden confounding</title>
      <link>https://arxiv.org/abs/2507.05170</link>
      <description>arXiv:2507.05170v1 Announce Type: new 
Abstract: Predicting outcomes in external domains is challenging due to hidden confounders that influence both predictors and outcomes, complicating generalization under distribution shifts. Traditional methods often rely on stringent assumptions or overly conservative regularization, compromising estimation and predictive accuracy. Generative Invariance (GI) is a novel framework that facilitates predictions in unseen domains without requiring hyperparameter tuning or knowledge of specific distribution shifts. However, the available frequentist version of GI does not always enable identification and lacks uncertainty quantification for its predictions. This paper develops a Bayesian formulation that extends GI with well-calibrated external predictions and facilitates causal discovery. We present theoretical guarantees showing that prior distributions assign asymptotic meaning to the number of distinct datasets that could be observed. Simulations and an application case highlight the remarkable empirical coverage behavior of our approach, nearly unchanged when transitioning from low- to moderate-dimensional settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05170v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Garc\'ia Meixide, David R\'ios Insua</dc:creator>
    </item>
    <item>
      <title>Blind Targeting: Personalization under Third-Party Privacy Constraints</title>
      <link>https://arxiv.org/abs/2507.05175</link>
      <description>arXiv:2507.05175v1 Announce Type: new 
Abstract: Major advertising platforms recently increased privacy protections by limiting advertisers' access to individual-level data. Instead of providing access to granular raw data, the platforms only allow a limited number of aggregate queries to a dataset, which is further protected by adding differentially private noise. This paper studies whether and how advertisers can design effective targeting policies within these restrictive privacy preserving data environments. To achieve this, I develop a probabilistic machine learning method based on Bayesian optimization, which facilitates dynamic data exploration. Since Bayesian optimization was designed to sample points from a function to find its maximum, it is not applicable to aggregate queries and to targeting. Therefore, I introduce two innovations: (i) integral updating of posteriors which allows to select the best regions of the data to query rather than individual points and (ii) a targeting-aware acquisition function that dynamically selects the most informative regions for the targeting task. I identify the conditions of the dataset and privacy environment that necessitate the use of such a "smart" querying strategy. I apply the strategic querying method to the Criteo AI Labs dataset for uplift modeling (Diemert et al., 2018) that contains visit and conversion data from 14M users. I show that an intuitive benchmark strategy only achieves 33% of the non-privacy-preserving targeting potential in some cases, while my strategic querying method achieves 97-101% of that potential, and is statistically indistinguishable from Causal Forest (Athey et al., 2019): a state-of-the-art non-privacy-preserving machine learning targeting method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05175v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anya Shchetkina</dc:creator>
    </item>
    <item>
      <title>On the Posterior Computation Under the Dirichlet-Laplace Prior</title>
      <link>https://arxiv.org/abs/2507.05214</link>
      <description>arXiv:2507.05214v1 Announce Type: new 
Abstract: Modern applications routinely collect high-dimensional data, leading to statistical models having more parameters than there are samples available. A common solution is to impose sparsity in parameter estimation, often using penalized optimization methods. Bayesian approaches provide a probabilistic framework to formally quantify uncertainty through shrinkage priors. Among these, the Dirichlet-Laplace prior has attained recognition for its theoretical guarantees and wide applicability. This article identifies a critical yet overlooked issue in the implementation of Gibbs sampling algorithms for such priors. We demonstrate that ambiguities in the presentation of key algorithmic steps, while mathematically coherent, have led to widespread implementation inaccuracies that fail to target the intended posterior distribution -- a target endowed with rigorous asymptotic guarantees. Using the normal-means problem and high-dimensional linear regressions as canonical examples, we clarify these implementation pitfalls and their practical consequences and propose corrected and more efficient sampling procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05214v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paolo Onorati, David B. Dunson, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis of epidemic forecasting and spreading on networks with probability generating functions</title>
      <link>https://arxiv.org/abs/2506.24103</link>
      <description>arXiv:2506.24103v1 Announce Type: cross 
Abstract: Epidemic forecasting tools embrace the stochasticity and heterogeneity of disease spread to predict the growth and size of outbreaks. Conceptually, stochasticity and heterogeneity are often modeled as branching processes or as percolation on contact networks. Mathematically, probability generating functions provide a flexible and efficient tool to describe these models and quickly produce forecasts. While their predictions are probabilistic-i.e., distributions of outcome-they depend deterministically on the input distribution of transmission statistics and/or contact structure. Since these inputs can be noisy data or models of high dimension, traditional sensitivity analyses are computationally prohibitive and are therefore rarely used. Here, we use statistical condition estimation to measure the sensitivity of stochastic polynomials representing noisy generating functions. In doing so, we can separate the stochasticity of their forecasts from potential noise in their input. For standard epidemic models, we find that predictions are most sensitive at the critical epidemic threshold (basic reproduction number $R_0 = 1$) only if the transmission is sufficiently homogeneous (dispersion parameter $k &gt; 0.3$). Surprisingly, in heterogeneous systems ($k \leq 0.3$), the sensitivity is highest for values of $R_{0} &gt; 1$. We expect our methods will improve the transparency and applicability of the growing utility of probability generating functions as epidemic forecasting tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24103v1</guid>
      <category>q-bio.PE</category>
      <category>math.DS</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mariah C. Boudreau, William H. W. Thompson, Christopher M. Danforth, Jean-Gabriel Young, Laurent H\'ebert-Dufresne</dc:creator>
    </item>
    <item>
      <title>Testing Hypotheses regarding Covariance and Correlation matrices with the R package CovCorTest</title>
      <link>https://arxiv.org/abs/2507.03406</link>
      <description>arXiv:2507.03406v1 Announce Type: cross 
Abstract: In addition to the commonly analyzed measures of location, dispersion measurements such as variance and correlation provide many valuable information. Consequently, they play a crucial role in multivariate statistics, which leads to tests regarding covariance and correlation matrices. Furthermore, also the structure of these matrices leads to important hypotheses of interest, since it contains substantial information about the underlying model. In fact, assumptions regarding the structures of covariance and correlation matrices are often fundamental in statistical modelling and testing.
  In this context, semi-parametric settings with minimal distributional assumptions and very general hypotheses are essential for enabling manifold usage. The free available package CovCorTest provides suitable tests addressing all aforementioned issues, using bootstrap and similar techniques to achieve good performance, particularly in small samples. Additionally, the package offers flexible specification options for the hypotheses under investigation in two central tests, accommodating users with varying levels of expertise, which results in high flexibility and user-friendliness at the same time. This paper also presents the application of \textbf{CovCorTest} for various issues, illustrated by multiple examples, where the tests are applied to a real-world dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03406v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Svenja Jedhoff</dc:creator>
    </item>
    <item>
      <title>TaxaPLN: a taxonomy-aware augmentation strategy for microbiome-trait classification including metadata</title>
      <link>https://arxiv.org/abs/2507.03588</link>
      <description>arXiv:2507.03588v1 Announce Type: cross 
Abstract: The gut microbiome plays a crucial role in human health, making it a corner stone of modern biomedical research. To study its structure and dynamics, machine learning models are increasingly used to identify key microbial patterns associated with disease and environmental factors. However, microbiome data present unique challenges due to their compositionality, high-dimensionality, sparsity, and high variability, which can obscure meaningful signals. Besides, the effectiveness of machine learning models is often constrained by limited sample sizes, as microbiome data collection remains costly and time consuming. In this context, data augmentation has emerged as a promising strategy to enhance model robustness and predictive performance by generating artificial microbiome data. The aim of this study is to improve predictive modeling from microbiome data by introducing a model-based data augmentation approach that incorporates both taxonomic relationships and covariate information. To that end, we propose TaxaPLN, a data augmentation method built on PLN-Tree generative models, which leverages the taxonomy and a data-driven sampler to generate realistic synthetic microbiome compositions. We further introduce a conditional extension based on feature-wise linear modulation, enabling covariate-aware generation. Experiments on high-quality curated microbiome datasets show that TaxaPLN preserves ecological properties and generally improves or maintains predictive performances, particularly with non-linear classifiers, outperforming state-of-the-art baselines. Besides, TaxaPLN conditional augmentation establishes a novel benchmark for covariate-aware microbiome augmentation. The MIT-licensed source code is available at https://github.com/ AlexandreChaussard/PLNTree-package along with the datasets used in our experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03588v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexandre Chaussard, Anna Bonnet, Sylvain Le Corff, Harry Sokol</dc:creator>
    </item>
    <item>
      <title>The Monge optimal transport barycenter problem</title>
      <link>https://arxiv.org/abs/2507.03669</link>
      <description>arXiv:2507.03669v1 Announce Type: cross 
Abstract: A novel methodology is developed for the solution of the data-driven Monge optimal transport barycenter problem, where the pushforward condition is formulated in terms of the statistical independence between two sets of random variables: the factors $z$ and a transformed outcome $y$. Relaxing independence to the uncorrelation between all functions of $z$ and $y$ within suitable finite-dimensional spaces leads to an adversarial formulation, for which the adversarial strategy can be found in closed form through the first principal components of a small-dimensional matrix. The resulting pure minimization problem can be solved very efficiently through gradient descent driven flows in phase space. The methodology extends beyond scenarios where only discrete factors affect the outcome, to multivariate sets of both discrete and continuous factors, for which the corresponding barycenter problems have infinitely many marginals. Corollaries include a new framework for the solution of the Monge optimal transport problem, a procedure for the data-based simulation and estimation of conditional probability densities, and a nonparametric methodology for Bayesian inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03669v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew D. Lipnick, Esteban G. Tabak, Giulio Trigila, Yating Wang, Xuancheng Ye, Wenjun Zhao</dc:creator>
    </item>
    <item>
      <title>Robust estimation of heterogeneous treatment effects in randomized trials leveraging external data</title>
      <link>https://arxiv.org/abs/2507.03681</link>
      <description>arXiv:2507.03681v1 Announce Type: cross 
Abstract: Randomized trials are typically designed to detect average treatment effects but often lack the statistical power to uncover effect heterogeneity over patient characteristics, limiting their value for personalized decision-making. To address this, we propose the QR-learner, a model-agnostic learner that estimates conditional average treatment effects (CATE) within the trial population by leveraging external data from other trials or observational studies. The proposed method is robust: it has the potential to reduce the CATE prediction mean squared error while maintaining consistency, even when the external data is not aligned with the trial. Moreover, we introduce a procedure that combines the QR-learner with a trial-only CATE learner and show that it asymptotically matches or exceeds the trial-only learner in terms of mean squared error. We examine the performance of our approach in simulation studies and apply the methods to a real-world dataset, demonstrating improvements in both CATE estimation and statistical power for detecting heterogeneous effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03681v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rickard Karlsson, Piersilvio De Bartolomeis, Issa J. Dahabreh, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>On the Estimation of Anisotropic Covariance Functions on Compact Two-Point Homogeneous Spaces</title>
      <link>https://arxiv.org/abs/2507.03723</link>
      <description>arXiv:2507.03723v1 Announce Type: cross 
Abstract: In this paper, the asymptotic theory presented in (Caponera et al., 2022) for spline-type anysotropic covariance estimator on the 2-dimensional sphere is generalized to the case of connected and compact two-point homogeneous spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03723v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessia Caponera</dc:creator>
    </item>
    <item>
      <title>Determination of Particle-Size Distributions from Light-Scattering Measurement Using Constrained Gaussian Process Regression</title>
      <link>https://arxiv.org/abs/2507.03736</link>
      <description>arXiv:2507.03736v1 Announce Type: cross 
Abstract: In this work, we propose a novel methodology for robustly estimating particle size distributions from optical scattering measurements using constrained Gaussian process regression. The estimation of particle size distributions is commonly formulated as a Fredholm integral equation of the first kind, an ill-posed inverse problem characterized by instability due to measurement noise and limited data. To address this, we use a Gaussian process prior to regularize the solution and integrate a normalization constraint into the Gaussian process via two approaches: by constraining the Gaussian process using a pseudo-measurement and by using Lagrange multipliers in the equivalent optimization problem. To improve computational efficiency, we employ a spectral expansion of the covariance kernel using eigenfunctions of the Laplace operator, resulting in a computationally tractable low-rank representation without sacrificing accuracy. Additionally, we investigate two complementary strategies for hyperparameter estimation: a data-driven approach based on maximizing the unconstrained log marginal likelihood, and an alternative approach where the physical constraints are taken into account. Numerical experiments demonstrate that the proposed constrained Gaussian process regression framework accurately reconstructs particle size distributions, producing numerically stable, smooth, and physically interpretable results. This methodology provides a principled and efficient solution for addressing inverse scattering problems and related ill-posed integral equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03736v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>physics.optics</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fahime Seyedheydari, Mahdi Nasiri, Marcin Mi\'nkowski, Simo S\"arkk\"a</dc:creator>
    </item>
    <item>
      <title>Tied Pools and Drawn Games</title>
      <link>https://arxiv.org/abs/2507.03894</link>
      <description>arXiv:2507.03894v1 Announce Type: cross 
Abstract: We consider the problem of estimating `preference' or `strength' parameters in three-way comparison experiments, each composed of a series of paired comparisons, but where only the single `preferred' or `strongest' candidate is known in each trial. Such experiments arise in psychology and market research, but here we use chess competitions as the prototypical context, in particular a series of `pools' between three players that occurred in 1821. The possibilities of tied pools, redundant and therefore unplayed games, and drawn games must all be considered. This leads us to reconsider previous models for estimating strength parameters when drawn games are a possible result. In particular, Davidson's method for ties has been questioned, and we propose an alternative. We argue that the most correct use of this method is to estimate strength parameters first, and then fix these to estimate a draw-propensity parameter, rather than estimating all parameters simultaneously, as Davidson does. This results in a model that is consistent with, and provides more context for, a simple method for handling draws proposed by Glickman. Finally, in pools with incomplete information, the number of drawn games can be estimated by adopting a draw-propensity parameter from related data with more complete information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03894v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roderick Edwards</dc:creator>
    </item>
    <item>
      <title>GenAI-Powered Inference</title>
      <link>https://arxiv.org/abs/2507.03897</link>
      <description>arXiv:2507.03897v1 Announce Type: cross 
Abstract: We introduce GenAI-Powered Inference (GPI), a statistical framework for both causal and predictive inference using unstructured data, including text and images. GPI leverages open-source Generative Artificial Intelligence (GenAI) models - such as large language models and diffusion models - not only to generate unstructured data at scale but also to extract low-dimensional representations that capture their underlying structure. Applying machine learning to these representations, GPI enables estimation of causal and predictive effects while quantifying associated estimation uncertainty. Unlike existing approaches to representation learning, GPI does not require fine-tuning of generative models, making it computationally efficient and broadly accessible. We illustrate the versatility of the GPI framework through three applications: (1) analyzing Chinese social media censorship, (2) estimating predictive effects of candidates' facial appearance on electoral outcomes, and (3) assessing the persuasiveness of political rhetoric. An open-source software package is available for implementing GPI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03897v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kosuke Imai, Kentaro Nakamura</dc:creator>
    </item>
    <item>
      <title>Real-TabPFN: Improving Tabular Foundation Models via Continued Pre-training With Real-World Data</title>
      <link>https://arxiv.org/abs/2507.03971</link>
      <description>arXiv:2507.03971v1 Announce Type: cross 
Abstract: Foundation models for tabular data, like TabPFN, achieve strong performance on small datasets when pre-trained solely on synthetic data. We show that this performance can be significantly boosted by a targeted continued pre-training phase. Specifically, we demonstrate that leveraging a small, curated collection of large, real-world datasets for continued pre-training yields superior downstream predictive accuracy compared to using broader, potentially noisier corpora like CommonCrawl or GitTables. Our resulting model, Real-TabPFN, achieves substantial performance gains on 29 datasets from the OpenML AutoML Benchmark.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03971v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anurag Garg, Muhammad Ali, Noah Hollmann, Lennart Purucker, Samuel M\"uller, Frank Hutter</dc:creator>
    </item>
    <item>
      <title>Integrated Gaussian Processes for Robust and Adaptive Multi-Object Tracking</title>
      <link>https://arxiv.org/abs/2507.04116</link>
      <description>arXiv:2507.04116v1 Announce Type: cross 
Abstract: This paper presents a computationally efficient multi-object tracking approach that can minimise track breaks (e.g., in challenging environments and against agile targets), learn the measurement model parameters on-line (e.g., in dynamically changing scenes) and infer the class of the tracked objects, if joint tracking and kinematic behaviour classification is sought. It capitalises on the flexibilities offered by the integrated Gaussian process as a motion model and the convenient statistical properties of non-homogeneous Poisson processes as a suitable observation model. This can be combined with the proposed effective track revival / stitching mechanism. We accordingly introduce the two robust and adaptive trackers, Gaussian and Poisson Process with Classification (GaPP-Class) and GaPP with Revival and Classification (GaPP-ReaCtion). They employ an appropriate particle filtering inference scheme that efficiently integrates track management and hyperparameter learning (including the object class, if relevant). GaPP-ReaCtion extends GaPP-Class with the addition of a Markov Chain Monte Carlo kernel applied to each particle permitting track revival and stitching (e.g., within a few time steps after deleting a trajectory). Performance evaluation and benchmarking using synthetic and real data show that GaPP-Class and GaPP-ReaCtion outperform other state-of-the-art tracking algorithms. For example, GaPP-ReaCtion significantly reduces track breaks (e.g., by around 30% from real radar data and markedly more from simulated data).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04116v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fred Lydeard, Bashar I. Ahmad, Simon Godsill</dc:creator>
    </item>
    <item>
      <title>Consistent Labeling Across Group Assignments: Variance Reduction in Conditional Average Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2507.04332</link>
      <description>arXiv:2507.04332v1 Announce Type: cross 
Abstract: Numerous algorithms have been developed for Conditional Average Treatment Effect (CATE) estimation. In this paper, we first highlight a common issue where many algorithms exhibit inconsistent learning behavior for the same instance across different group assignments. We introduce a metric to quantify and visualize this inconsistency. Next, we present a theoretical analysis showing that this inconsistency indeed contributes to higher test errors and cannot be resolved through conventional machine learning techniques. To address this problem, we propose a general method called \textbf{Consistent Labeling Across Group Assignments} (CLAGA), which eliminates the inconsistency and is applicable to any existing CATE estimation algorithm. Experiments on both synthetic and real-world datasets demonstrate significant performance improvements with CLAGA.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04332v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi-Fu Fu, Keng-Te Liao, Shou-De Lin</dc:creator>
    </item>
    <item>
      <title>Optimal Model Selection for Conformalized Robust Optimization</title>
      <link>https://arxiv.org/abs/2507.04716</link>
      <description>arXiv:2507.04716v1 Announce Type: cross 
Abstract: In decision-making under uncertainty, Contextual Robust Optimization (CRO) provides reliability by minimizing the worst-case decision loss over a prediction set, hedging against label variability. While recent advances use conformal prediction to construct prediction sets for machine learning models, the downstream decisions critically depend on model selection. This paper introduces novel model selection frameworks for CRO that unify robustness control with decision risk minimization. We first propose Conformalized Robust Optimization with Model Selection (CROMS), which automatically selects models to approximately minimize the average decision risk in CRO solutions. We develop two algorithms: E-CROMS, which is computationally efficient, and F-CROMS, which enjoys a marginal robustness guarantee in finite samples. Further, we introduce Conformalized Robust Optimization with Individualized Model Selection (CROiMS), which performs individualized model selection by minimizing the conditional decision risk given the covariate of test data. This framework advances conformal prediction methodology by enabling covariate-aware model selection. Theoretically, CROiMS achieves asymptotic conditional robustness and decision efficiency under mild assumptions. Numerical results demonstrate significant improvements in decision efficiency and robustness across diverse synthetic and real-world applications, outperforming baseline approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04716v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajie Bao, Yang Hu, Haojie Ren, Peng Zhao, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Sure Convergence and Constructive Universal Approximation for Multi-Layer Neural Networks</title>
      <link>https://arxiv.org/abs/2507.04779</link>
      <description>arXiv:2507.04779v1 Announce Type: cross 
Abstract: We propose a new neural network model, 01Neuro, built on indicator activation neurons. Its boosted variant possesses two key statistical properties: (1) Sure Convergence, where model optimization can be achieved with high probability given sufficient computational resources; and (2) Constructive Universal Approximation: In the infinite sample setting, the model can approximate any finite sum of measurable functions, each depending on only k out of p input features, provided the architecture is properly tuned. Unlike most universal approximation results that are agnostic to training procedures, our guarantees are directly tied to the model's explicit construction and optimization algorithm. To improve prediction stability, we integrate stochastic training and bagging into the boosted 01Neuro framework. Empirical evaluations on simulated and real-world tabular datasets with small to medium sample sizes highlight its strengths: effective approximation of interaction components (multiplicative terms), stable prediction performance (comparable to Random Forests), robustness to many noisy features, and insensitivity to feature scaling. A major limitation of the current implementation of boosted 01Neuro is its higher computational cost, which is approximately 5 to 30 times that of Random Forests and XGBoost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04779v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chien-Ming Chi</dc:creator>
    </item>
    <item>
      <title>Variance-based variable selection in sensor calibration with strong interferents -- application to air pollution monitoring with a carbon nanotube sensor array</title>
      <link>https://arxiv.org/abs/2507.05001</link>
      <description>arXiv:2507.05001v1 Announce Type: cross 
Abstract: Air and water pollution are major threats to public health, highlighting the need for reliable environmental monitoring. Low-cost multisensor systems are promising but suffer from limited selectivity, because their responses are influenced by non-target variables (interferents) such as temperature and humidity. This complicates pollutant detection, especially in data-driven models with noisy, correlated inputs. We propose a method for selecting the most relevant interferents for sensor calibration, balancing performance and cost. Including too many variables can lead to overfitting, while omitting key variables reduces accuracy. Our approach evaluates numerous models using a bias-variance trade-off and variance analysis. The method is first validated on simulated data to assess strengths and limitations, then applied to a carbon nanotube-based sensor array deployed outdoors to characterize its sensitivity to air pollutants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05001v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marine Dumon, Berengere Lebental, Guillaume Perrin</dc:creator>
    </item>
    <item>
      <title>Vecchia-Inducing-Points Full-Scale Approximations for Gaussian Processes</title>
      <link>https://arxiv.org/abs/2507.05064</link>
      <description>arXiv:2507.05064v1 Announce Type: cross 
Abstract: Gaussian processes are flexible, probabilistic, non-parametric models widely used in machine learning and statistics. However, their scalability to large data sets is limited by computational constraints. To overcome these challenges, we propose Vecchia-inducing-points full-scale (VIF) approximations combining the strengths of global inducing points and local Vecchia approximations. Vecchia approximations excel in settings with low-dimensional inputs and moderately smooth covariance functions, while inducing point methods are better suited to high-dimensional inputs and smoother covariance functions. Our VIF approach bridges these two regimes by using an efficient correlation-based neighbor-finding strategy for the Vecchia approximation of the residual process, implemented via a modified cover tree algorithm. We further extend our framework to non-Gaussian likelihoods by introducing iterative methods that substantially reduce computational costs for training and prediction by several orders of magnitudes compared to Cholesky-based computations when using a Laplace approximation. In particular, we propose and compare novel preconditioners and provide theoretical convergence results. Extensive numerical experiments on simulated and real-world data sets show that VIF approximations are both computationally efficient as well as more accurate and numerically stable than state-of-the-art alternatives. All methods are implemented in the open source C++ library GPBoost with high-level Python and R interfaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.05064v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Gyger, Reinhard Furrer, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>On synthetic interval data with predetermined subject partitioning, and partial control of the variables' marginal correlation structure</title>
      <link>https://arxiv.org/abs/1709.00872</link>
      <description>arXiv:1709.00872v3 Announce Type: replace 
Abstract: A standard approach for assessing the performance of partition models is to create synthetic data sets with a prespecified clustering structure, and assess how well the model reveals this structure. A common format is that subjects are assigned to different clusters, with observations simulated so that subjects within the same cluster have similar profiles, allowing for some variability. In this manuscript, we consider observations from interval variables, taking a finite number of values. Interval data are commonly observed in cohort and Genome Wide Association studies, and our focus is on Single Nucleotide Polymorphisms. Theoretical and empirical results are utilized to explore the dependence structure between the variables, in relation with the clustering structure for the subjects. A novel algorithm is proposed that allows to control the marginal stratified correlation structure of the variables, specifying exact correlation values within groups of variables. Practical examples are shown, and a synthetic dataset is compared to a real one, to demonstrate similarities and differences.</description>
      <guid isPermaLink="false">oai:arXiv.org:1709.00872v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Papathomas</dc:creator>
    </item>
    <item>
      <title>QuTE: decentralized multiple testing on sensor networks with false discovery rate control</title>
      <link>https://arxiv.org/abs/2210.04334</link>
      <description>arXiv:2210.04334v2 Announce Type: replace 
Abstract: This paper designs methods for decentralized multiple hypothesis testing on graphs that are equipped with provable guarantees on the false discovery rate (FDR). We consider the setting where distinct agents reside on the nodes of an undirected graph, and each agent possesses p-values corresponding to one or more hypotheses local to its node. Each agent must individually decide whether to reject one or more of its local hypotheses by only communicating with its neighbors, with the joint aim that the global FDR over the entire graph must be controlled at a predefined level. We propose a simple decentralized family of Query-Test-Exchange (QuTE) algorithms and prove that they can control FDR under independence or positive dependence of the p-values. Our algorithm reduces to the Benjamini-Hochberg (BH) algorithm when after graph-diameter rounds of communication, and to the Bonferroni procedure when no communication has occurred or the graph is empty. To avoid communicating real-valued p-values, we develop a quantized BH procedure, and extend it to a quantized QuTE procedure. QuTE works seamlessly in streaming data settings, where anytime-valid p-values may be continually updated at each node. Last, QuTE is robust to arbitrary dropping of packets, or a graph that changes at every step, making it particularly suitable to mobile sensor networks involving drones or other multi-agent systems. We study the power of our procedure using a simulation suite of different levels of connectivity and communication on a variety of graph structures, and also provide an illustrative real-world example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.04334v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Jianbo Chen, Martin J. Wainwright, Michael I. Jordan</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Linear Models and Regression Adjusted Causal Inference in Randomized Experiments</title>
      <link>https://arxiv.org/abs/2210.08589</link>
      <description>arXiv:2210.08589v5 Announce Type: replace 
Abstract: Linear models are foundational tools in statistics and ubiquitous across the applied sciences. However, conventional statistical inference -- such as $t$-tests and $F$-tests -- are only valid at fixed sample sizes, making them unsuitable for sequential settings such as online A/B testing. We develop an anytime-valid theory of inference for the linear model, introducing sequential analogues of classical tests and confidence sets that provide Type-I error control and coverage guarantees uniformly over all sample sizes. Our construction is based on likelihood ratios of invariantly sufficient statistics, yielding simple closed-form expressions of ordinary least squares estimators and standard errors. The resulting tests are optimal in the GROW/REGROW sense for both frequentist and Bayesian alternative hypotheses. We then relax the linear model assumptions to provide heteroskedasticity-robust asymptotic sequential tests and confidence sequences, which enable sequential regression-adjusted inference for causal estimands in randomized controlled experiments. This formally allows experiments to be continuously monitored for significance, stopped early, and safeguards against statistical malpractices in data collection. We demonstrate the practical utility of our approach through simulations and applications to real A/B test data from Netflix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.08589v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Lindon, Dae Woong Ham, Martin Tingley, Iavor Bojinov</dc:creator>
    </item>
    <item>
      <title>REACT to NHST: Sensible conclusions to meaningful hypotheses</title>
      <link>https://arxiv.org/abs/2308.09112</link>
      <description>arXiv:2308.09112v4 Announce Type: replace 
Abstract: While Null Hypothesis Significance Testing (NHST) remains a widely used statistical tool, it suffers from several shortcomings in its common usage, such as conflating statistical and practical significance, the formulation of inappropriate null hypotheses, and the inability to distinguish between accepting the null hypothesis and failing to reject it. Recent efforts have focused on developing alternatives that address these issues. Despite these efforts, conventional NHST remains dominant in scientific research due to its procedural simplicity and mistakenly presumed ease of interpretation. Our work presents an intuitive alternative to conventional NHST designed to bridge the gap between the expectations of researchers and the actual outcomes of hypothesis tests: $\texttt{REACT}$. $\texttt{REACT}$ not only tackles shortcomings of conventional NHST but also offers additional advantages over existing alternatives. For instance, $\texttt{REACT}$ applies to multiparametric hypotheses and does not require stringent significance-level corrections when conducting multiple tests. We illustrate the practical utility of $\texttt{REACT}$ through real-world data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09112v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rafael Izbicki, Luben M. C. Cabezas, Fernando A. B. Colugnatti, Rodrigo F. L. Lassance, Altay A. L. de Souza, Rafael B. Stern</dc:creator>
    </item>
    <item>
      <title>Robust Penalized Estimators for High--Dimensional Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2312.04661</link>
      <description>arXiv:2312.04661v2 Announce Type: replace 
Abstract: Robust estimators for generalized linear models (GLMs) are not easy to develop due to the nature of the distributions involved. Recently, there has been growing interest in robust estimation methods, particularly in contexts involving a potentially large number of explanatory variables. Transformed M-estimators (MT-estimators) provide a natural extension of M-estimation techniques to the GLM framework, offering robust methodologies. We propose a penalized variant of MT-estimators to address high-dimensional data scenarios. Under suitable assumptions, we demonstrate the consistency and asymptotic normality of this novel class of estimators. Our theoretical development focuses on redescending rho-functions and penalization functions that satisfy specific regularity conditions. We present an Iterative Re-Weighted Least Squares algorithm, together with a deterministic initialization procedure, which is crucial since the estimating equations may have multiple solutions. We evaluate the finite sample performance of this method for Poisson distribution and well known penalization functions through Monte Carlo simulations that consider various types of contamination, as well as an empirical application using a real dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.04661v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marina Valdora, Claudio Agostinelli</dc:creator>
    </item>
    <item>
      <title>Transformation Discriminant Analysis for Constructing Optimal Biomarker Combinations</title>
      <link>https://arxiv.org/abs/2402.03004</link>
      <description>arXiv:2402.03004v2 Announce Type: replace 
Abstract: Accurate diagnostic tests are essential for effective screening and treatment. However, individual biomarkers often fail to provide sufficient diagnostic accuracy, as they typically capture only one aspect of the complex disease process. Combining multiple biomarkers, each capturing a distinct mechanism, can help constructing more informative diagnostic tests. In practice, logistic regression is used as the default to combine biomarkers, but it can perform poorly when biomarker distributions exhibit skewness or differ across disease groups. Nonparametric methods provide more flexibility but generally require large sample sizes that are infrequently available in biomedical research.
  We propose a novel framework called transformation discriminant analysis which combines biomarkers through the likelihood ratio function to construct theoretically optimal diagnostic scores. Transformation discriminant analysis balances between flexibility and efficiency. It can accommodate a wide range of distributional shapes and disease-specific dependence structures while remaining fully parametric. This allows for likelihood inference and strong performance even in small-sample settings.
  We evaluate TDA through simulations and benchmark its performance against commonly used methods. Finally, we illustrate its utility in constructing an optimal diagnostic test for hepatocellular carcinoma, a disease with no single ideal biomarker. An open-source R implementation is provided for reproducibility and broader application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03004v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ainesh Sewak, Sandra Siegfried, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Infinite joint species distribution models</title>
      <link>https://arxiv.org/abs/2402.13384</link>
      <description>arXiv:2402.13384v3 Announce Type: replace 
Abstract: Joint species distribution models are popular in ecology for modeling covariate effects on species occurrence, while characterizing cross-species dependence. Data consist of multivariate binary indicators of the occurrences of different species in each sample, along with sample-specific covariates. A key problem is that current models implicitly assume that the list of species under consideration is predefined and finite, while for highly diverse groups of organisms, it is impossible to anticipate which species will be observed in a study and discovery of unknown species is common. This article proposes a new modeling paradigm for statistical ecology, which generalizes traditional multivariate probit models to accommodate large numbers of rare species and new species discovery. We discuss theoretical properties of the proposed modeling paradigm and implement efficient algorithms for posterior computation. Simulation studies and applications to fungal biodiversity data provide compelling support for the new modeling class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13384v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Stolf, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Supervised Bayesian joint graphical model for simultaneous network estimation and subgroup identification</title>
      <link>https://arxiv.org/abs/2403.19994</link>
      <description>arXiv:2403.19994v2 Announce Type: replace 
Abstract: Heterogeneity is a fundamental characteristic of cancer. To accommodate heterogeneity, subgroup identification has been extensively studied and broadly categorized into unsupervised and supervised analysis. Compared to unsupervised analysis, supervised approaches potentially hold greater clinical implications. Under the unsupervised analysis framework, several methods focusing on network-based subgroup identification have been developed, offering more comprehensive insights than those restricted to mean, variance, and other simplistic distributions by incorporating the interconnections among variables. However, research on supervised network-based subgroup identification remains limited. In this study, we develop a novel supervised Bayesian graphical model for jointly identifying multiple heterogeneous networks and subgroups. In the proposed model, heterogeneity is not only reflected in molecular data but also associated with a clinical outcome, and a novel similarity prior is introduced to effectively accommodate similarities among the networks of different subgroups, significantly facilitating clinically meaningful biological network construction and subgroup identification. The consistency properties of the estimates are rigorously established, and an efficient algorithm is developed. Extensive simulation studies and a real-world application to TCGA data are conducted, which demonstrate the advantages of the proposed approach in terms of both subgroup and network identification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19994v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xing Qin, Xu Liu, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>Exploring Spatial Context: A Comprehensive Bibliography of GWR and MGWR</title>
      <link>https://arxiv.org/abs/2404.16209</link>
      <description>arXiv:2404.16209v4 Announce Type: replace 
Abstract: Local spatial models such as Geographically Weighted Regression (GWR) and Multiscale Geographically Weighted Regression (MGWR) serve as instrumental tools to capture intrinsic contextual effects through the estimates of the local intercepts and behavioral contextual effects through estimates of the local slope parameters. GWR and MGWR provide simple implementation yet powerful frameworks that could be extended to various disciplines that handle spatial data. This bibliography aims to serve as a comprehensive compilation of peer-reviewed papers that have utilized GWR or MGWR as a primary analytical method to conduct spatial analyses and acts as a useful guide to anyone searching the literature for previous examples of local statistical modeling in a wide variety of application fields.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.16209v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Stewart Fotheringham, Chen-Lun Kao, Hanchen Yu, Sarah Bardin, Taylor Oshan, Ziqi Li, Mehak Sachdeva, Wei Luo</dc:creator>
    </item>
    <item>
      <title>Multicalibration for Modeling Censored Survival Data with Universal Adaptability</title>
      <link>https://arxiv.org/abs/2405.15948</link>
      <description>arXiv:2405.15948v4 Announce Type: replace 
Abstract: Traditional statistical and machine learning methods typically assume that the training and test data follow the same distribution. However, this assumption is frequently violated in real-world applications, where the training data in the source domain may under-represent specific subpopulations in the test data of the target domain. This paper addresses target-independent learning under covariate shift, focusing on multicalibration for survival probability and restricted mean survival time. A black-box post-processing boosting algorithm specifically designed for censored survival data is introduced. By leveraging pseudo-observations, our method produces a multicalibrated predictor that is competitive with inverse propensity score weighting in predicting the survival outcome in an unlabeled target domain, ensuring not only overall accuracy but also fairness across diverse subpopulations. Our theoretical analysis of pseudo-observations builds upon the functional delta method and the $p$-variational norm. The algorithm's sample complexity, convergence properties, and multicalibration guarantees for post-processed predictors are provided. Our results establish a fundamental connection between multicalibration and universal adaptability, demonstrating that our calibrated function is comparable to, or outperforms, the inverse propensity score weighting estimator. Extensive numerical simulations and a real-world case study on cardiovascular disease risk prediction using two large prospective cohort studies validate the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15948v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanxuan Ye, Hongzhe Li</dc:creator>
    </item>
    <item>
      <title>Causal Inference with Outcomes Truncated by Death and Missing Not at Random</title>
      <link>https://arxiv.org/abs/2406.10554</link>
      <description>arXiv:2406.10554v5 Announce Type: replace 
Abstract: In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10554v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Li, Yuan Liu, Shanshan Luo, Zhi Geng</dc:creator>
    </item>
    <item>
      <title>Rethinking the handling of method failure in comparison studies</title>
      <link>https://arxiv.org/abs/2408.11594</link>
      <description>arXiv:2408.11594v2 Announce Type: replace 
Abstract: Comparison studies in methodological research are intended to compare methods in an evidence-based manner to help data analysts select a suitable method for their application. To provide trustworthy evidence, they must be carefully designed, implemented, and reported, especially given the many decisions made in planning and running. A common challenge in comparison studies is to handle the "failure" of one or more methods to produce a result for some (real or simulated) data sets, such that their performances cannot be measured in those instances. Despite an increasing emphasis on this topic in recent literature (focusing on non-convergence as a common manifestation), there is little guidance on proper handling and interpretation, and reporting of the chosen approach is often neglected. This paper aims to fill this gap and offers practical guidance on handling method failure in comparison studies. After exploring common handlings across various published comparison studies from classical statistics and predictive modeling, we show that the popular approaches of discarding data sets yielding failure (either for all or the failing methods only) and imputing are inappropriate in most cases. We then recommend a different perspective on method failure - viewing it as the result of a complex interplay of several factors rather than just its manifestation. Building on this, we provide recommendations on more adequate handlings of method failure derived from realistic considerations. In particular, we propose considering fallback strategies that directly reflect the behavior of real-world users. Finally, we illustrate our recommendations and the dangers of inadequate handling of method failure through two exemplary comparison studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11594v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Milena W\"unsch, Moritz Herrmann, Elisa Noltenius, Mattia Mohr, Tim P. Morris, Anne-Laure Boulesteix</dc:creator>
    </item>
    <item>
      <title>Information criteria for the number of directions of extremes in high-dimensional data</title>
      <link>https://arxiv.org/abs/2409.10174</link>
      <description>arXiv:2409.10174v2 Announce Type: replace 
Abstract: In multivariate extreme value analysis, the estimation of the dependence structure in extremes is demanding, especially in the context of high-dimensional data. Therefore, a common approach is to reduce the model dimension by considering only the directions in which extreme values occur. In this paper, we use the concept of sparse regular variation recently introduced by Meyer and Wintenberger (2021) to derive information criteria for the number of directions in which extreme events occur, such as a Bayesian information criterion (BIC), a mean-squared error-based information criterion (MSEIC), and a quasi-Akaike information criterion (QAIC) based on the Gaussian likelihood function. As is typical in extreme value analysis, a challenging task is the choice of the number $k_n$ of observations used for the estimation. Therefore, for all information criteria, we present a two-step procedure to estimate both the number of directions of extremes and an optimal choice of $k_n$. We prove that the AIC of Meyer and Wintenberger (2023) and the MSEIC are inconsistent information criteria for the number of extreme directions whereas the BIC and the QAIC are consistent information criteria. Finally, the performance of the different information criteria is compared in a simulation study and applied on wind speed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10174v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Butsch, Vicky Fasen-Hartmann</dc:creator>
    </item>
    <item>
      <title>Spatial Sign based Principal Component Analysis for High Dimensional Data</title>
      <link>https://arxiv.org/abs/2409.13267</link>
      <description>arXiv:2409.13267v2 Announce Type: replace 
Abstract: This article focuses on the robust principal component analysis (PCA) of high-dimensional data with elliptical distributions. We investigate the PCA of the sample spatial-sign covariance matrix in both nonsparse and sparse contexts, referring to them as SPCA and SSPCA, respectively. We present both nonasymptotic and asymptotic analyses to quantify the theoretical performance of SPCA and SSPCA. In sparse settings, we demonstrate that SSPCA, implemented through a combinatoric program, achieves the optimal rate of convergence. Our proposed SSPCA method is computationally efficient and exhibits robustness against heavy-tailed distributions compared to existing methods. Simulation studies and real-world data applications further validate the superiority of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13267v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ping Zhao, Hongfei Wang, Long Feng</dc:creator>
    </item>
    <item>
      <title>Two-stage Design for Failure Probability Estimation with Gaussian Process Surrogates</title>
      <link>https://arxiv.org/abs/2410.04496</link>
      <description>arXiv:2410.04496v3 Announce Type: replace 
Abstract: We tackle the problem of quantifying failure probabilities for expensive deterministic computer experiments with stochastic inputs under a fixed budget. The computational cost of the computer simulation prohibits direct Monte Carlo (MC) and necessitates a surrogate model, which may facilitate either a "surrogate MC" estimator or a surrogate-informed importance sampling estimator. We embrace the former, finding importance sampling too variable when budgets are limited, and propose a novel design strategy to effectively train a surrogate for the purpose of failure probability estimation. Existing works exhaust the entire evaluation budget on active learning through sequential contour location (CL), attempting to balance exploration with exploitation of the failure contour throughout the design, but we find exhaustive CL to be suboptimal. Instead we propose a novel two-stage surrogate design. In Stage 1, we conduct sequential CL to locate the failure contour. In Stage 2, once surrogate learning has saturated, we use a solely exploitative strategy -- allocating the remaining evaluation budget to MC samples with the highest classification entropy to ensure they are classified correctly. We propose a stopping criterion to determine the transition between stages without any tuning. Our two-stage design outperforms alternatives, including exhaustive CL and surrogate-informed importance sampling, on a variety of benchmark exercises. With these tools, we are able to effectively estimate small failure probabilities with only hundreds of simulator evaluations, showcasing functionality with both shallow and deep Gaussian process surrogates, and deploying our method on a simulation of fluid flow around an airfoil.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04496v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annie S. Booth, S. Ashwin Renganathan</dc:creator>
    </item>
    <item>
      <title>Surrogate modeling with functional nonlinear autoregressive models (F-NARX)</title>
      <link>https://arxiv.org/abs/2410.07293</link>
      <description>arXiv:2410.07293v2 Announce Type: replace 
Abstract: We propose a novel functional approach to surrogate modeling of dynamical systems with exogenous inputs. This approach, named Functional Nonlinear AutoRegressive with eXogenous inputs (F-NARX), approximates the system response based on temporal features of the exogenous inputs and the system response. This marks a major step away from the discrete-time-centric approach of classical NARX models, which determines the relationship between selected time steps of the input/output time series. By modeling the system in a time-feature space, F-NARX takes advantage of the temporal smoothness of the process being modeled, providing more stable predictions and reducing the dependence of model performance on the discretization of the time axis. In this work, we introduce an F-NARX implementation based on principal component analysis and polynomial regression. To further improve prediction accuracy, we also introduce a modified hybrid least angle regression approach to identify a sparse model structure and minimize the expected forecast error, rather than the one-step-ahead prediction error. We investigate the behavior and capabilities of our F-NARX implementation on two case studies: an eight-story building under wind loading and a three-story steel frame under seismic loading. Our results demonstrate that F-NARX has several favorable properties that make it well-suited to surrogate modeling applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07293v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.ress.2025.111276</arxiv:DOI>
      <arxiv:journal_reference>Reliability Engineering &amp; System Safety, Volume 264, Part A, December 2025, 111276</arxiv:journal_reference>
      <dc:creator>Styfen Sch\"ar, Stefano Marelli, Bruno Sudret</dc:creator>
    </item>
    <item>
      <title>Partial identification of principal causal effects under violations of principal ignorability</title>
      <link>https://arxiv.org/abs/2412.06628</link>
      <description>arXiv:2412.06628v2 Announce Type: replace 
Abstract: Principal stratification is a general framework for studying causal mechanisms involving post-treatment variables. When estimating principal causal effects, the principal ignorability assumption is commonly invoked, which we study in detail in this manuscript. Our first key contribution is studying a commonly used strategy of using parametric models to jointly model the outcome and principal strata without requiring the principal ignorability assumption. We show that even if the joint distribution of principal strata is known, this strategy necessarily leads to only partial identification of causal effects, even under very simple and correctly specified outcome models. While principal ignorability leads to point identification in this setting, we discuss alternative, weaker assumptions and show how they can lead to informative partial identification regions. An additional contribution is that we provide theoretical support to strategies used in the literature for identifying association parameters that govern the joint distribution of principal strata. We prove that this is possible, but only if the principal ignorability assumption is violated. Additionally, due to partial identifiability of causal effects even when these association parameters are known, we show that these association parameters are only identifiable under strong parametric constraints. Lastly, we extend these results to more flexible semiparametric and nonparametric Bayesian models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06628v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minxuan Wu, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Assessing treatment efficacy for interval-censored endpoints using multistate semi-Markov models fit to multiple data streams</title>
      <link>https://arxiv.org/abs/2501.14097</link>
      <description>arXiv:2501.14097v2 Announce Type: replace 
Abstract: We introduce a computationally efficient and general approach for utilizing multiple, possibly interval-censored, data streams to study complex biomedical endpoints using multistate semi-Markov models. Our motivating application is the REGEN-2069 trial, which investigated the protective efficacy (PE) of the monoclonal antibody combination REGEN-COV against SARS-CoV-2 when administered prophylactically to individuals in households at high risk of secondary transmission. Using data on symptom onset, episodic RT-qPCR sampling, and serological testing, we estimate the PE of REGEN-COV for asymptomatic infection, its effect on seroconversion following infection, and the duration of viral shedding. We find that REGEN-COV reduced the risk of asymptomatic infection and the duration of viral shedding, and led to lower rates of seroconversion among asymptomatically infected participants. Our algorithm for fitting semi-Markov models to interval-censored data employs a Monte Carlo expectation maximization (MCEM) algorithm combined with importance sampling to efficiently address the intractability of the marginal likelihood when data are intermittently observed. Our algorithm provide substantial computational improvements over existing methods and allows us to fit semi-parametric models despite complex coarsening of the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14097v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Raphael Morsomme, C. Jason Liang, Allyson Mateja, Dean A. Follmann, Meagan P. O'Brien, Chenguang Wang, Jonathan Fintzi</dc:creator>
    </item>
    <item>
      <title>Combined P-value Functions for Compatible Effect Estimation and Hypothesis Testing in Drug Regulation</title>
      <link>https://arxiv.org/abs/2503.10246</link>
      <description>arXiv:2503.10246v2 Announce Type: replace 
Abstract: The two-trials rule in drug regulation requires statistically significant results from two pivotal trials to demonstrate efficacy. However, it is unclear how the effect estimates from both trials should be combined to quantify the drug effect. Fixed-effect meta-analysis is commonly used but may yield confidence intervals that exclude the value of no effect even when the two-trials rule is not fulfilled. We systematically address this by recasting the two-trials rule and meta-analysis in a unified framework of combined p-value functions, where they are variants of Wilkinson's and Stouffer's combination methods, respectively. This allows us to obtain compatible combined p-values, effect estimates, and confidence intervals, which we derive in closed-form. Additionally, we provide new results for Edgington's, Fisher's, Pearson's, and Tippett's p-value combination methods. When both trials have the same true effect, all methods can consistently estimate it, although some show bias. When true effects differ, the two-trials rule and Pearson's method are conservative (converging to the less extreme effect), Fisher's and Tippett's methods are anti-conservative (converging to the more extreme effect), and Edgington's method and meta-analysis are balanced (converging to a weighted average). Notably, Edgington's confidence intervals asymptotically always include the individual trial effects, while meta-analytic confidence intervals shrink to a point at the weighted average effect. We conclude that all of these methods may be appropriate depending on the estimand of interest. We implement combined p-value function inference for two trials in the R package twotrials, allowing researchers to easily perform compatible hypothesis testing and effect estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10246v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel, Ma{\l}gorzata Roos, Leonhard Held</dc:creator>
    </item>
    <item>
      <title>Easily Computed Marginal Likelihoods for Multivariate Mixture Models Using the THAMES Estimator</title>
      <link>https://arxiv.org/abs/2504.21812</link>
      <description>arXiv:2504.21812v3 Announce Type: replace 
Abstract: We present a new version of the truncated harmonic mean estimator (THAMES) for univariate or multivariate mixture models. The estimator computes the marginal likelihood from Markov chain Monte Carlo (MCMC) samples, is consistent, asymptotically normal and of finite variance. In addition, it is invariant to label switching, does not require posterior samples from hidden allocation vectors, and is easily approximated, even for an arbitrarily high number of components. Its computational efficiency is based on an asymptotically optimal ordering of the parameter space, which can in turn be used to provide useful visualisations. We test it in simulation settings where the true marginal likelihood is available analytically. It performs well against state-of-the-art competitors, even in multivariate settings with a high number of components. We demonstrate its utility for inference and model selection on univariate and multivariate data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.21812v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Metodiev, Nicholas J. Irons, Marie Perrot-Dock\`es, Pierre Latouche, Adrian E. Raftery</dc:creator>
    </item>
    <item>
      <title>A practical identifiability criterion leveraging weak-form parameter estimation</title>
      <link>https://arxiv.org/abs/2506.17373</link>
      <description>arXiv:2506.17373v2 Announce Type: replace 
Abstract: In this work, we define a practical identifiability criterion, (e, q)-identifiability, based on a parameter e, reflecting the noise in observed variables, and a parameter q, reflecting the mean-square error of the parameter estimator. This criterion is better able to encompass changes in the quality of the parameter estimate due to increased noise in the data (compared to existing criteria based solely on average relative errors). Furthermore, we leverage a weak-form equation error-based method of parameter estimation for systems with unobserved variables to assess practical identifiability far more quickly in comparison to output error-based parameter estimation. We do so by generating weak-form input-output equations using differential algebra techniques, as previously proposed by Boulier et al [1], and then applying Weak form Estimation of Nonlinear Dynamics (WENDy) to obtain parameter estimates. This method is computationally efficient and robust to noise, as demonstrated through two classical biological modelling examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.17373v2</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nora Heitzman-Breen, Vanja Dukic, David M. Bortz</dc:creator>
    </item>
    <item>
      <title>Stop Lying to Me: New Visual Tools to Choose the Most Honest Nonlinear Dimension Reduction</title>
      <link>https://arxiv.org/abs/2506.22051</link>
      <description>arXiv:2506.22051v2 Announce Type: replace 
Abstract: Nonlinear dimension reduction (NLDR) techniques such as tSNE, and UMAP provide a low-dimensional representation of high-dimensional data ($p\text{-}D$) by applying a nonlinear transformation. NLDR often exaggerates random patterns. But NLDR views have an important role in data analysis because, if done well, they provide a concise visual (and conceptual) summary of $p\text{-}D$ distributions. The NLDR methods and hyper-parameter choices can create wildly different representations, making it difficult to decide which is best, or whether any or all are accurate or misleading. To help assess the NLDR and decide on which, if any, is the most reasonable representation of the structure(s) present in the $p\text{-}D$ data, we have developed an algorithm to show the $2\text{-}D$ NLDR model in the $p\text{-}D$ space, viewed with a tour, a movie of linear projections. From this, one can see if the model fits everywhere, or better in some subspaces, or completely mismatches the data. Also, we can see how different methods may have similar summaries or quirks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22051v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jayani P. Gamage, Dianne Cook, Paul Harrison, Michael Lydeamore, Thiyanga S. Talagala</dc:creator>
    </item>
    <item>
      <title>The Optimality of Blocking Designs in Equally and Unequally Allocated Randomized Experiments with General Response</title>
      <link>https://arxiv.org/abs/2212.01887</link>
      <description>arXiv:2212.01887v4 Announce Type: replace-cross 
Abstract: We consider the performance of the difference-in-means estimator in a two-arm randomized experiment under common experimental endpoints such as continuous (regression), incidence, proportion and survival. We examine performance under both equal and unequal allocation to treatment groups and we consider both the Neyman randomization model and the population model. We show that in the Neyman model, where the only source of randomness is the treatment manipulation, there is no free lunch: complete randomization is minimax for the estimator's mean squared error. In the population model, where each subject experiences response noise with zero mean, the optimal design is the deterministic perfect-balance allocation. However, this allocation is generally NP-hard to compute and moreover, depends on unknown response parameters. When considering the tail criterion of Kapelner et al. (2021), we show the optimal design is less random than complete randomization and more random than the deterministic perfect-balance allocation. We prove that Fisher's blocking design provides the asymptotically optimal degree of experimental randomness. Theoretical results are supported by simulations in all considered experimental settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01887v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Azriel, Abba M. Krieger, Adam Kapelner</dc:creator>
    </item>
    <item>
      <title>Online Estimation with Rolling Validation: Adaptive Nonparametric Estimation with Streaming Data</title>
      <link>https://arxiv.org/abs/2310.12140</link>
      <description>arXiv:2310.12140v4 Announce Type: replace-cross 
Abstract: Online nonparametric estimators are gaining popularity due to their efficient computation and competitive generalization abilities. An important example includes variants of stochastic gradient descent. These algorithms often take one sample point at a time and incrementally update the parameter estimate of interest. In this work, we consider model selection/hyperparameter tuning for such online algorithms. We propose a weighted rolling validation procedure, an online variant of leave-one-out cross-validation, that costs minimal extra computation for many typical stochastic gradient descent estimators and maintains their online nature. Similar to batch cross-validation, it can boost base estimators to achieve better heuristic performance and adaptive convergence rate. Our analysis is straightforward, relying mainly on some general statistical stability assumptions. The simulation study underscores the significance of diverging weights in practice and demonstrates its favorable sensitivity even when there is only a slim difference between candidate estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12140v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianyu Zhang, Jing Lei</dc:creator>
    </item>
    <item>
      <title>Structured Difference-of-Q via Orthogonal Learning</title>
      <link>https://arxiv.org/abs/2406.08697</link>
      <description>arXiv:2406.08697v3 Announce Type: replace-cross 
Abstract: Offline reinforcement learning is important in many settings with available observational data but the inability to deploy new policies online due to safety, cost, and other concerns. Many recent advances in causal inference and machine learning target estimation of causal contrast functions such as CATE, which is sufficient for optimizing decisions and can adapt to potentially smoother structure. We develop a dynamic generalization of the R-learner (Nie and Wager 2021, Lewis and Syrgkanis 2021) for estimating and optimizing the difference of $Q^\pi$-functions, $Q^\pi(s,1)-Q^\pi(s,0)$ (which can be used to optimize multiple-valued actions). We leverage orthogonal estimation to improve convergence rates in the presence of slower nuisance estimation rates and prove consistency of policy optimization under a margin condition. The method can leverage black-box nuisance estimators of the $Q$-function and behavior policy to target estimation of a more structured $Q$-function contrast.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08697v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Defu Cao, Angela Zhou</dc:creator>
    </item>
    <item>
      <title>Implementing Response-Adaptive Randomisation in Stratified Rare-disease Trials: Design Challenges and Practical Solutions</title>
      <link>https://arxiv.org/abs/2410.03346</link>
      <description>arXiv:2410.03346v2 Announce Type: replace-cross 
Abstract: Although response-adaptive randomisation (RAR) has gained substantial attention in the literature, it still has limited use in clinical trials. Amongst other reasons, the implementation of RAR in real world trials raises important practical questions, often neglected in the technical literature. Motivated by an innovative phase-II stratified RAR rare-disease trial, this paper addresses two challenges: (1) How to ensure that RAR allocations are desirable i.e. both acceptable and faithful to the intended probabilities, particularly in small samples? and (2) What adaptations to trigger after interim analyses in the presence of missing data? To answer (1), we propose a Mapping strategy that discretises the randomisation probabilities into a vector of allocation ratios, resulting in improved frequentist errors. Under the implementation of Mapping, we answer (2) by analysing the impact of missing data on operating characteristics in selected scenarios. Finally, we discuss additional concerns including: pooling data across trial strata, analysing the level of blinding in the trial, and reporting safety results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.03346v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajenki Das, Nina Deliu, Mark Toshner, Sof\'ia S Villar</dc:creator>
    </item>
    <item>
      <title>Inductive randomness predictors: beyond conformal</title>
      <link>https://arxiv.org/abs/2503.02803</link>
      <description>arXiv:2503.02803v2 Announce Type: replace-cross 
Abstract: This paper introduces inductive randomness predictors, which form a proper superset of inductive conformal predictors but have the same principal property of validity under the assumption of randomness (i.e., of IID data). It turns out that every non-trivial inductive conformal predictor is strictly dominated by an inductive randomness predictor, although the improvement is not great, at most a factor of $\mathrm{e}\approx2.72$ in the case of e-prediction. The dominating inductive randomness predictors are more complicated and more difficult to compute; besides, an improvement by a factor of $\mathrm{e}$ is rare. Therefore, this paper does not suggest replacing inductive conformal predictors by inductive randomness predictors and only calls for a more detailed study of the latter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02803v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Vovk</dc:creator>
    </item>
    <item>
      <title>Treatment Effect Heterogeneity in Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2503.13696</link>
      <description>arXiv:2503.13696v3 Announce Type: replace-cross 
Abstract: Empirical studies using Regression Discontinuity (RD) designs often explore heterogeneous treatment effects based on pretreatment covariates, even though no formal statistical methods exist for such analyses. This has led to the widespread use of ad hoc approaches in applications. Motivated by common empirical practice, we develop a unified, theoretically grounded framework for RD heterogeneity analysis. We show that a fully interacted local linear (in functional parameters) model effectively captures heterogeneity while still being tractable and interpretable in applications. The model structure holds without loss of generality for discrete covariates. Although our proposed model is potentially restrictive for continuous covariates, it naturally aligns with standard empirical practice and offers a causal interpretation for RD applications. We establish principled bandwidth selection and robust bias-corrected inference methods to analyze heterogeneous treatment effects and test group differences. We provide companion software to facilitate implementation of our results. An empirical application illustrates the practical relevance of our methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13696v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sebastian Calonico, Matias D. Cattaneo, Max H. Farrell, Filippo Palomba, Rocio Titiunik</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v2 Announce Type: replace-cross 
Abstract: This study investigates adaptive experimental design for treatment choice, also known as fixed-budget best-arm identification. We consider an adaptive procedure consisting of a treatment-allocation phase followed by a treatment-choice phase, and we design an adaptive experiment for this setup to efficiently identify the best treatment arm, defined as the one with the highest expected outcome. In our designed experiment, the treatment-allocation phase consists of two stages. The first stage is a pilot phase, where we allocate each treatment arm uniformly with equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, we allocate treatment arms in proportion to the variances estimated in the first stage. After the treatment-allocation phase, the procedure enters the treatment-choice phase, where we choose the treatment arm with the highest sample mean as our estimate of the best treatment arm. We prove that this single design is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that match our lower bounds up to exact constants. Therefore, our designed experiment achieves the sharp efficiency limits without requiring separate tuning for minimax and Bayesian objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v2</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 08 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
