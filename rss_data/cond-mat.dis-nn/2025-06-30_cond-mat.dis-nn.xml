<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 02:22:28 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Ecosystems as adaptive living circuits</title>
      <link>https://arxiv.org/abs/2506.22017</link>
      <description>arXiv:2506.22017v1 Announce Type: cross 
Abstract: Unlike many physical nonequilibrium systems, in biological systems, the coupling to external energy sources is not a fixed parameter but adaptively controlled by the system itself. We do not have theoretical frameworks that allow for such adaptability. As a result, we cannot understand emergent behavior in living systems where structure formation and non-equilibrium drive coevolve. Here, using ecosystems as a model of adaptive systems, we develop a framework of living circuits whose architecture changes adaptively with the energy dissipated in each circuit edge. We find that unlike traditional nonequilibrium systems, living circuits exhibit a phase transition from equilibrium death to a nonequilibrium dissipative state beyond a critical driving potential. This transition emerges through a feedback mechanism that saves the weakest edges by routing dissipation through them, even though the adaptive rule locally rewards the strongest dissipating edges. Despite lacking any global optimization principle, living circuits achieve near-maximal dissipation, with higher drive promoting more complex circuits. Our work establishes ecosystems as paradigmatic examples of living circuits whose structure and dissipation are tuned through local adaptive rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22017v1</guid>
      <category>q-bio.PE</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ankit Dhanuka, Avi I. Flamholz, Arvind Murugan, Akshit Goyal</dc:creator>
    </item>
    <item>
      <title>Boosting Classification with Quantum-Inspired Augmentations</title>
      <link>https://arxiv.org/abs/2506.22241</link>
      <description>arXiv:2506.22241v1 Announce Type: cross 
Abstract: Understanding the impact of small quantum gate perturbations, which are common in quantum digital devices but absent in classical computers, is crucial for identifying potential advantages in quantum machine learning. While these perturbations are typically seen as detrimental to quantum computation, they can actually enhance performance by serving as a natural source of data augmentation. Additionally, they can often be efficiently simulated on classical hardware, enabling quantum-inspired approaches to improve classical machine learning methods. In this paper, we investigate random Bloch sphere rotations, which are fundamental SU(2) transformations, as a simple yet effective quantum-inspired data augmentation technique. Unlike conventional augmentations such as flipping, rotating, or cropping, quantum transformations lack intuitive spatial interpretations, making their application to tasks like image classification less straightforward. While common quantum augmentation methods rely on applying quantum models or trainable quanvolutional layers to classical datasets, we focus on the direct application of small-angle Bloch rotations and their effect on classical data. Using the large-scale ImageNet dataset, we demonstrate that our quantum-inspired augmentation method improves image classification performance, increasing Top-1 accuracy by 3%, Top-5 accuracy by 2.5%, and the F$_1$ score from 8% to 12% compared to standard classical augmentation methods. Finally, we examine the use of stronger unitary augmentations. Although these transformations preserve information in principle, they result in visually unrecognizable images with potential applications for privacy computations. However, we show that our augmentation approach and simple SU(2) transformations do not enhance differential privacy and discuss the implications of this limitation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22241v1</guid>
      <category>cs.CV</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias Tsch\"ope, Vitor Fortes Rey, Sogo Pierre Sanon, Paul Lukowicz, Nikolaos Palaiodimopoulos, Maximilian Kiefer-Emmanouilidis</dc:creator>
    </item>
    <item>
      <title>Towards Distributed Neural Architectures</title>
      <link>https://arxiv.org/abs/2506.22389</link>
      <description>arXiv:2506.22389v1 Announce Type: cross 
Abstract: We introduce and train distributed neural architectures (DNA) in vision and language domains. DNAs are initialized with a proto-architecture that consists of (transformer, MLP, attention, etc.) modules and routers. Any token (or patch) can traverse any series of modules in any order. DNAs are a natural generalization of the sparse methods such as Mixture-of-Experts, Mixture-of-Depths, parameter sharing, etc. Computation and communication patterns of DNA modules are learnt end-to-end during training and depend on the content and context of each token (or patch). These patterns can be shaped by further requirements added to the optimization objective such as compute/memory efficiency or load balancing. We empirically show that (i) trained DNAs are competitive with the dense baselines in both domains and (ii) compute efficiency/parameter sharing can be learnt from data. Next, we analyze the emergent connectivity and computation patterns in the trained DNAs. We find that the paths that tokens take through the models are themselves distributed according to a power-law. We show that some paths (or, equivalently, groups of modules) show emergent specialization. Finally, we demonstrate that models learn to allocate compute and active parameters in an interpretable way.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22389v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aditya Cowsik, Tianyu He, Andrey Gromov</dc:creator>
    </item>
    <item>
      <title>Spring-block theory of feature learning in deep neural networks</title>
      <link>https://arxiv.org/abs/2407.19353</link>
      <description>arXiv:2407.19353v4 Announce Type: replace 
Abstract: Feature-learning deep nets progressively collapse data to a regular low-dimensional geometry. How this emerges from the collective action of nonlinearity, noise, learning rate, and other factors, has eluded first-principles theories built from microscopic neuronal dynamics. We exhibit a noise-nonlinearity phase diagram that identifies regimes where shallow or deep layers learn more effectively and propose a macroscopic mechanical theory that reproduces the diagram and links feature learning across layers to generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19353v4</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Shi, Liming Pan, Ivan Dokmani\'c</dc:creator>
    </item>
    <item>
      <title>Equations of state and stability condition of mixed p-spin glass model</title>
      <link>https://arxiv.org/abs/2506.10579</link>
      <description>arXiv:2506.10579v2 Announce Type: replace 
Abstract: The Sherrington-Kirkpatrick (SK) is a foundational model for understanding spin glass systems. It is based on the pairwise interaction between each two spins in a fully connected lattice with quenched disordered interactions. The nature of long-range interaction among spins in the (SK) model simplifies the study of this system by eliminating fluctuations. An advanced (SK) model, known as the p-spin model, introduces higher-order interactions that involve the interaction of P spins. This research focuses on the general Hamiltonian of the spin glass model with long-range interaction, referred to as the mixed p-spin glass model, which consists of adding all p-spin interaction terms. This research aims to derive the equation of states for this Hamiltonian, formulate the equation of state within the framework of the first replica symmetry breaking, and determine both the stability condition of the replica symmetric solution and the stability of the replicas belonging to the same group in the first step of replica symmetry breaking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10579v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Talebi</dc:creator>
    </item>
    <item>
      <title>Collective excitations in Hydrogen across the pressure-induced transition from molecular to atomic fluid</title>
      <link>https://arxiv.org/abs/2506.20791</link>
      <description>arXiv:2506.20791v2 Announce Type: replace-cross 
Abstract: Dispersion of collective excitations in fluid Hydrogen along the isothermal line T=2500~K, including the region of molecular-to-atomis fluid transition, is studied by ab initio molecular dynamics (AIMD) simulations. The obtained density dependence of the adiabatic and high-frequency speed of sound contains a plateau in the region of the molecular-to-atomic fluid transition. We show, that the five-variable thermo-viscoelastic model of generalized hydrodynamics for pure molecular H$_2$ and pure atomic (H) fluids is able to recover perfectly the AIMD-derived time correlation functions and sound eigenvalues nicely agree with the numerically estimated sound dispersion. In the region of the molecular-to-atomic fluid transition a dynamic model of chemical reacting mixture should be applied. We discuss the calculations of time correlation functions from molecular/atomic units in the reacting mixture from AIMD trajectories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20791v2</guid>
      <category>physics.chem-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 30 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>I. -M. Ilenkov, T. Bryk</dc:creator>
    </item>
  </channel>
</rss>
