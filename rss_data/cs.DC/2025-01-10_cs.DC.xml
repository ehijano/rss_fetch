<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2025 05:00:25 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Can vehicular cloud replace edge computing?</title>
      <link>https://arxiv.org/abs/2501.04702</link>
      <description>arXiv:2501.04702v1 Announce Type: new 
Abstract: Edge computing (EC) consists of deploying computation resources close to the users, thus enabling low-latency applications, such as augmented reality and online gaming. However, large-scale deployment of edge nodes can be highly impractical and expensive. Besides EC, there is a rising concept known as Vehicular Cloud Computing (VCC). VCC is a computing paradigm that amplifies the capabilities of vehicles by exploiting part of their computational resources, enabling them to participate in services similar to those provided by the EC. The advantage of VCC is that it can opportunistically exploit part of the computation resources already present on vehicles, thus relieving a network operator from the deployment and maintenance cost of EC nodes. However, it is still unknown under which circumstances VCC can enable low-latency applications without EC. In this work, we show that VCC has the potential to effectively supplant EC in urban areas, especially given the higher density of vehicles in such environments. The goal of this paper is to analyze, via simulation, the key parameters determining the conditions under which this substitution of EC by VCC is feasible. In addition, we provide a high level cost analysis to show that VCC is much less costly for a network operator than adopting EC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04702v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rosario Patan\`e, Nadjib Achir, Andrea Araldo, Lila Boukhatem</dc:creator>
    </item>
    <item>
      <title>Distributed Graph Algorithms with Predictions</title>
      <link>https://arxiv.org/abs/2501.05267</link>
      <description>arXiv:2501.05267v1 Announce Type: new 
Abstract: We initiate the study of deterministic distributed graph algorithms with predictions in synchronous message passing systems. The process at each node in the graph is given a prediction, which is some extra information about the problem instance that may be incorrect. The processes may use the predictions to help them solve the problem. The overall goal is to develop algorithms that both work faster when predictions are good and do not work much worse than algorithms without predictions when predictions are bad. Concepts from the more general area of algorithms with predictions, such as error measures, consistency, robustness, and smoothness, are adapted to distributed graph algorithms with predictions.
  We consider algorithms with predictions for four distributed graph problems, Maximal Independent Set, Maximal Matching, $(\Delta+1)$-Vertex Coloring, and $(2\Delta-1)$-Edge Coloring, where $\Delta$ denotes the degree of the graph. For each, we define an appropriate error measure. We present generic templates that can be used to design deterministic distributed graph algorithms with predictions from existing algorithms without predictions. Using these templates, we develop algorithms with predictions for Maximal Independent Set. Alternative error measures for the Maximal Independent Set problem are also considered. We obtain algorithms with predictions for general graphs and for rooted trees and analyze them using two of these error measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05267v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joan Boyar, Faith Ellen, Kim S. Larsen</dc:creator>
    </item>
    <item>
      <title>Optimizing Distributed Deployment of Mixture-of-Experts Model Inference in Serverless Computing</title>
      <link>https://arxiv.org/abs/2501.05313</link>
      <description>arXiv:2501.05313v1 Announce Type: new 
Abstract: With the advancement of serverless computing, running machine learning (ML) inference services over a serverless platform has been advocated, given its labor-free scalability and cost effectiveness. Mixture-of-Experts (MoE) models have been a dominant type of model architectures to enable large models nowadays, with parallel expert networks. Serving large MoE models on serverless computing is potentially beneficial, but has been underexplored due to substantial challenges in handling the skewed expert popularity and scatter-gather communication bottleneck in MoE model execution, for cost-efficient serverless MoE deployment and performance guarantee. We study optimized MoE model deployment and distributed inference serving on a serverless platform, that effectively predict expert selection, pipeline communication with model execution, and minimize the overall billed cost of serving MoE models. Especially, we propose a Bayesian optimization framework with multi-dimensional epsilon-greedy search to learn expert selections and optimal MoE deployment achieving optimal billed cost, including: 1) a Bayesian decision-making method for predicting expert popularity; 2) flexibly pipelined scatter-gather communication; and 3) an optimal model deployment algorithm for distributed MoE serving. Extensive experiments on AWS Lambda show that our designs reduce the billed cost of all MoE layers by at least 75.67% compared to CPU clusters while maintaining satisfactory inference throughput. As compared to LambdaML in serverless computing, our designs achieves 43.41% lower cost with a throughput decrease of at most 18.76%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05313v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengfan Liu, Wei Wang, Chuan Wu</dc:creator>
    </item>
    <item>
      <title>Byzantine Fault Tolerant Protocols with Near-Constant Work per Node without Signatures</title>
      <link>https://arxiv.org/abs/2501.05377</link>
      <description>arXiv:2501.05377v1 Announce Type: new 
Abstract: Numerous distributed tasks have to be handled in a setting where a fraction of nodes behaves Byzantine, that is, deviates arbitrarily from the intended protocol. Resilient, deterministic protocols rely on the detection of majorities to avoid inconsistencies if there is a Byzantine minority, which requires individual nodes to handle a communication load that is proportional to the size of the network -- an intolerable disadvantage in large networks.
  Randomized protocols circumvent this by probing only small parts of the network, thus allowing for consistent decisions quickly and with a high level of confidence with communication that is near-constant in the network size. However, such protocols usually come with the drawback of limiting the fault tolerance of the protocol. For instance, by severely restricting the number or type of failures that the protocol can tolerate.
  We present randomized protocols to reliably aggregate and broadcast information, form consensus and compute common coins that tolerate a constant fraction of Byzantine failures, do not require cryptographic methods and have a near-constant time and message complexity per node. Our main technique is to compute a system of witness committees as a pre-computation step almost optimally. This pre-computation step allows to solve the aforementioned distributed tasks repeatedly and efficiently, but may have far reaching further applications, e.g., for sharding of distributed data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05377v1</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Schneider</dc:creator>
    </item>
    <item>
      <title>TimeRL: Efficient Deep Reinforcement Learning with Polyhedral Dependence Graphs</title>
      <link>https://arxiv.org/abs/2501.05408</link>
      <description>arXiv:2501.05408v1 Announce Type: new 
Abstract: Modern deep learning (DL) workloads increasingly use complex deep reinforcement learning (DRL) algorithms that generate training data within the learning loop. This results in programs with several nested loops and dynamic data dependencies between tensors. While DL systems with eager execution support such dynamism, they lack the optimizations and smart scheduling of graph-based execution. Graph-based execution, however, cannot express dynamic tensor shapes, instead requiring the use of multiple static subgraphs. Either execution model for DRL thus leads to redundant computation, reduced parallelism, and less efficient memory management.
  We describe TimeRL, a system for executing dynamic DRL programs that combines the dynamism of eager execution with the whole-program optimizations and scheduling of graph-based execution. TimeRL achieves this by introducing the declarative programming model of recurrent tensors, which allows users to define dynamic dependencies as intuitive recurrence equations. TimeRL translates recurrent tensors into a polyhedral dependence graph (PDG) with dynamic dependencies as symbolic expressions. Through simple PDG transformations, TimeRL applies whole-program optimizations, such as automatic vectorization, incrementalization, and operator fusion. The PDG also allows for the computation of an efficient program-wide execution schedule, which decides on buffer deallocations, buffer donations, and GPU/CPU memory swapping. We show that TimeRL executes current DRL algorithms up to 47$\times$ faster than existing DRL systems, while using 16$\times$ less GPU peak memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05408v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro F. Silvestre, Peter Pietzuch</dc:creator>
    </item>
    <item>
      <title>One Node One Model: Featuring the Missing-Half for Graph Clustering</title>
      <link>https://arxiv.org/abs/2412.09902</link>
      <description>arXiv:2412.09902v2 Announce Type: cross 
Abstract: Most existing graph clustering methods primarily focus on exploiting topological structure, often neglecting the ``missing-half" node feature information, especially how these features can enhance clustering performance. This issue is further compounded by the challenges associated with high-dimensional features. Feature selection in graph clustering is particularly difficult because it requires simultaneously discovering clusters and identifying the relevant features for these clusters. To address this gap, we introduce a novel paradigm called ``one node one model", which builds an exclusive model for each node and defines the node label as a combination of predictions for node groups. Specifically, the proposed ``Feature Personalized Graph Clustering (FPGC)" method identifies cluster-relevant features for each node using a squeeze-and-excitation block, integrating these features into each model to form the final representations. Additionally, the concept of feature cross is developed as a data augmentation technique to learn low-order feature interactions. Extensive experimental results demonstrate that FPGC outperforms state-of-the-art clustering methods. Moreover, the plug-and-play nature of our method provides a versatile solution to enhance GNN-based models from a feature perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09902v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanting Xie, Bingheng Li, Erlin Pan, Zhaochen Guo, Zhao Kang, Wenyu Chen</dc:creator>
    </item>
    <item>
      <title>Developing a Modular Compiler for a Subset of a C-like Language</title>
      <link>https://arxiv.org/abs/2501.04503</link>
      <description>arXiv:2501.04503v1 Announce Type: cross 
Abstract: The paper introduces the development of a modular compiler for a subset of a C-like language, which addresses the challenges in constructing a compiler for high-level languages. This modular approach will allow developers to modify a language by adding or removing subsets as required, resulting in a minimal and memory-efficient compiler. The development process is divided into small, incremental steps, where each step yields a fully functioning compiler for an expanding subset of the language. The paper outlines the iterative developmental phase of the compiler, emphasizing progressive enhancements in capabilities and functionality. Adherence to industry best practices of modular design, code reusability, and documentation has enabled the resulting compiler's functional efficiency, maintainability, and extensibility. The compiler proved to be effective not only in managing the language structure but also in developing optimized code, which demonstrates its practical usability. This was also further assessed using the compiler on a tiny memory-deficient single-board computer, again showing the compiler's efficiency and suitability for resource-constrained devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04503v1</guid>
      <category>cs.PL</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Debasish Dutta, Neeharika Sonowal, Irani Hazarika</dc:creator>
    </item>
    <item>
      <title>Topology-aware Microservice Architecture in Edge Networks: Deployment Optimization and Implementation</title>
      <link>https://arxiv.org/abs/2501.04956</link>
      <description>arXiv:2501.04956v1 Announce Type: cross 
Abstract: As a ubiquitous deployment paradigm, integrating microservice architecture (MSA) into edge networks promises to enhance the flexibility and scalability of services. However, it also presents significant challenges stemming from dispersed node locations and intricate network topologies. In this paper, we have proposed a topology-aware MSA characterized by a three-tier network traffic model encompassing the service, microservices, and edge node layers. This model meticulously characterizes the complex dependencies between edge network topologies and microservices, mapping microservice deployment onto link traffic to accurately estimate communication delay. Building upon this model, we have formulated a weighted sum communication delay optimization problem considering different types of services. Then, a novel topology-aware and individual-adaptive microservices deployment (TAIA-MD) scheme is proposed to solve the problem efficiently, which accurately senses the network topology and incorporates an individual-adaptive mechanism in a genetic algorithm to accelerate the convergence and avoid local optima. Extensive simulations show that, compared to the existing deployment schemes, TAIA-MD improves the communication delay performance by approximately 30% to 60% and effectively enhances the overall network performance. Furthermore, we implement the TAIA-MD scheme on a practical microservice physical platform. The experimental results demonstrate that TAIA-MD achieves superior robustness in withstanding link failures and network fluctuations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.04956v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuang Chen, Chang Wu, Fangyu Zhang, Chengdi Lu, Yongsheng Huang, Hancheng Lu</dc:creator>
    </item>
    <item>
      <title>A Scalable System for Visual Analysis of Ocean Data</title>
      <link>https://arxiv.org/abs/2501.05009</link>
      <description>arXiv:2501.05009v1 Announce Type: cross 
Abstract: Oceanographers rely on visual analysis to interpret model simulations, identify events and phenomena, and track dynamic ocean processes. The ever increasing resolution and complexity of ocean data due to its dynamic nature and multivariate relationships demands a scalable and adaptable visualization tool for interactive exploration. We introduce pyParaOcean, a scalable and interactive visualization system designed specifically for ocean data analysis. pyParaOcean offers specialized modules for common oceanographic analysis tasks, including eddy identification and salinity movement tracking. These modules seamlessly integrate with ParaView as filters, ensuring a user-friendly and easy-to-use system while leveraging the parallelization capabilities of ParaView and a plethora of inbuilt general-purpose visualization functionalities. The creation of an auxiliary dataset stored as a Cinema database helps address I/O and network bandwidth bottlenecks while supporting the generation of quick overview visualizations. We present a case study on the Bay of Bengal (BoB) to demonstrate the utility of the system and scaling studies to evaluate the efficiency of the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05009v1</guid>
      <category>cs.GR</category>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/cgf.15279</arxiv:DOI>
      <arxiv:journal_reference>Computer Graphics Forum, 2025</arxiv:journal_reference>
      <dc:creator>Toshit Jain, Upkar Singh, Varun Singh, Vijay Kumar Boda, Ingrid Hotz, Sathish S. Vadhiyar, P. N. Vinayachandran, Vijay Natarajan</dc:creator>
    </item>
    <item>
      <title>Validation of GPU Computation in Decentralized, Trustless Networks</title>
      <link>https://arxiv.org/abs/2501.05374</link>
      <description>arXiv:2501.05374v1 Announce Type: cross 
Abstract: Verifying computational processes in decentralized networks poses a fundamental challenge, particularly for Graphics Processing Unit (GPU) computations. Our investigation reveals significant limitations in existing approaches: exact recomputation fails due to computational non-determinism across GPU nodes, Trusted Execution Environments (TEEs) require specialized hardware, and Fully Homomorphic Encryption (FHE) faces prohibitive computational costs. To address these challenges, we explore three verification methodologies adapted from adjacent technical domains: model fingerprinting techniques, semantic similarity analysis, and GPU profiling. Through systematic exploration of these approaches, we develop novel probabilistic verification frameworks, including a binary reference model with trusted node verification and a ternary consensus framework that eliminates trust requirements. These methodologies establish a foundation for ensuring computational integrity across untrusted networks while addressing the inherent challenges of non-deterministic execution in GPU-accelerated workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05374v1</guid>
      <category>cs.ET</category>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Boniardi, Stanley Bishop, Alison Haire</dc:creator>
    </item>
    <item>
      <title>Decentralized Diffusion Models</title>
      <link>https://arxiv.org/abs/2501.05450</link>
      <description>arXiv:2501.05450v1 Announce Type: cross 
Abstract: Large-scale AI model training divides work across thousands of GPUs, then synchronizes gradients across them at each step. This incurs a significant network burden that only centralized, monolithic clusters can support, driving up infrastructure costs and straining power systems. We propose Decentralized Diffusion Models, a scalable framework for distributing diffusion model training across independent clusters or datacenters by eliminating the dependence on a centralized, high-bandwidth networking fabric. Our method trains a set of expert diffusion models over partitions of the dataset, each in full isolation from one another. At inference time, the experts ensemble through a lightweight router. We show that the ensemble collectively optimizes the same objective as a single model trained over the whole dataset. This means we can divide the training burden among a number of "compute islands," lowering infrastructure costs and improving resilience to localized GPU failures. Decentralized diffusion models empower researchers to take advantage of smaller, more cost-effective and more readily available compute like on-demand GPU nodes rather than central integrated systems. We conduct extensive experiments on ImageNet and LAION Aesthetics, showing that decentralized diffusion models FLOP-for-FLOP outperform standard diffusion models. We finally scale our approach to 24 billion parameters, demonstrating that high-quality diffusion models can now be trained with just eight individual GPU nodes in less than a week.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05450v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David McAllister, Matthew Tancik, Jiaming Song, Angjoo Kanazawa</dc:creator>
    </item>
    <item>
      <title>Convergence Analysis of Split Federated Learning on Heterogeneous Data</title>
      <link>https://arxiv.org/abs/2402.15166</link>
      <description>arXiv:2402.15166v3 Announce Type: replace 
Abstract: Split federated learning (SFL) is a recent distributed approach for collaborative model training among multiple clients. In SFL, a global model is typically split into two parts, where clients train one part in a parallel federated manner, and a main server trains the other. Despite the recent research on SFL algorithm development, the convergence analysis of SFL is missing in the literature, and this paper aims to fill this gap. The analysis of SFL can be more challenging than that of federated learning (FL), due to the potential dual-paced updates at the clients and the main server. We provide convergence analysis of SFL for strongly convex and general convex objectives on heterogeneous data. The convergence rates are $O(1/T)$ and $O(1/\sqrt[3]{T})$, respectively, where $T$ denotes the total number of rounds for SFL training. We further extend the analysis to non-convex objectives and the scenario where some clients may be unavailable during training. Experimental experiments validate our theoretical results and show that SFL outperforms FL and split learning (SL) when data is highly heterogeneous across a large number of clients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15166v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pengchao Han, Chao Huang, Geng Tian, Ming Tang, Xin Liu</dc:creator>
    </item>
    <item>
      <title>Deterministic and Reliable Software-Defined Vehicles: key building blocks, challenges, and vision</title>
      <link>https://arxiv.org/abs/2407.17287</link>
      <description>arXiv:2407.17287v2 Announce Type: replace 
Abstract: As vehicle systems become increasingly complex, with more features, services, sensors, actuators, and processing units, it is important to view vehicles not just as modes of transportation moving toward full autonomy, but also as adaptive systems that respond to the needs of their occupants. Vehicular services can be developed to support these adaptations. However, the increasing complexity of vehicular service development, even with current standardizations, best practices and guidelines, are insufficient to tackle the high complexity of development, with expectations of up to 1 (U.S.) billion lines of code for a fully (level 5) autonomous vehicle. Within this survey, the paradigm of Deterministic Software Defined Vehicles is explored, aiming to enhance the quality and ease of developing automotive services by focusing on service-oriented architectures, virtualization techniques, and the necessary deterministic intra- and inter-vehicular communications. Considering the main open challenges for such verticals, a vision architecture towards improved services development and orchestration is presented, focusing on: a) a deterministic network configurator; b) a data layer configurator; c) a hypervisor configurator; d) the vehicle abstraction layer; and e) a software orchestrator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17287v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro Veloso Teixeira, Duarte Raposo, Rui Lopes, Susana Sargento</dc:creator>
    </item>
    <item>
      <title>CloudSim 7G: An Integrated Toolkit for Modeling and Simulation of Future Generation Cloud Computing Environments</title>
      <link>https://arxiv.org/abs/2408.13386</link>
      <description>arXiv:2408.13386v3 Announce Type: replace 
Abstract: Cloud Computing has established itself as an efficient and cost-effective paradigm for the execution of web-based applications, and scientific workloads, that need elasticity and on-demand scalability capabilities. However, the evaluation of novel resource provisioning and management techniques is a major challenge due to the complexity of large-scale data centers. Therefore, Cloud simulators are an essential tool for academic and industrial researchers, to investigate the effectiveness of novel algorithms and mechanisms in large-scale scenarios. This paper proposes CloudSim 7G, the seventh generation of CloudSim, which features a re-engineered and generalized internal architecture to facilitate the integration of multiple CloudSim extensions within the same simulated environment. As part of the new design, we introduced a set of standardized interfaces to abstract common functionalities and carried out extensive refactoring and refinement of the codebase. The result is a substantial reduction in lines of code with no loss in functionality, significant improvements in run-time performance and memory efficiency (up to 25% less heap memory allocated), as well as increased flexibility, ease-of-use, and extensibility of the framework. These improvements benefit not only CloudSim developers but also researchers and practitioners using the framework for modeling and simulating next-generation Cloud Computing environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.13386v3</guid>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Remo Andreoli, Jie Zhao, Tommaso Cucinotta, Rajkumar Buyya</dc:creator>
    </item>
    <item>
      <title>Differentially Private Online Federated Learning with Correlated Noise</title>
      <link>https://arxiv.org/abs/2403.16542</link>
      <description>arXiv:2403.16542v3 Announce Type: replace-cross 
Abstract: We introduce a novel differentially private algorithm for online federated learning that employs temporally correlated noise to enhance utility while ensuring privacy of continuously released models. To address challenges posed by DP noise and local updates with streaming non-iid data, we develop a perturbed iterate analysis to control the impact of the DP noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed under a quasi-strong convexity condition. Subject to an $(\epsilon, \delta)$-DP budget, we establish a dynamic regret bound over the entire time horizon, quantifying the impact of key parameters and the intensity of changes in dynamic environments. Numerical experiments confirm the efficacy of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16542v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaojiao Zhang, Linglingzhi Zhu, Mikael Johansson</dc:creator>
    </item>
    <item>
      <title>Decentralized Federated Anomaly Detection in Smart Grids: A P2P Gossip Approach</title>
      <link>https://arxiv.org/abs/2407.15879</link>
      <description>arXiv:2407.15879v2 Announce Type: replace-cross 
Abstract: The increasing security and privacy concerns in the Smart Grid sector have led to a significant demand for robust intrusion detection systems within critical smart grid infrastructure. To address the challenges posed by privacy preservation and decentralized power system zones with distinct data ownership, Federated Learning (FL) has emerged as a promising privacy-preserving solution which facilitates collaborative training of attack detection models without necessitating the sharing of raw data. However, FL presents several implementation limitations in the power system domain due to its heavy reliance on a centralized aggregator and the risks of privacy leakage during model update transmission. To overcome these technical bottlenecks, this paper introduces a novel decentralized federated anomaly detection scheme based on two main gossip protocols namely Random Walk and Epidemic. Our findings indicate that the Random Walk protocol exhibits superior performance compared to the Epidemic protocol, highlighting its efficacy in decentralized federated learning environments. Experimental validation of the proposed framework utilizing publicly available industrial control systems datasets demonstrates superior attack detection accuracy while safeguarding data confidentiality and mitigating the impact of communication latency and stragglers. Furthermore, our approach yields a notable 35% improvement in training time compared to conventional FL, underscoring the efficacy and robustness of our decentralized learning method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15879v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Akbar Husnoo, Adnan Anwar, Md Enamul Haque, A. N. Mahmood</dc:creator>
    </item>
    <item>
      <title>Locally Differentially Private Online Federated Learning With Correlated Noise</title>
      <link>https://arxiv.org/abs/2411.18752</link>
      <description>arXiv:2411.18752v2 Announce Type: replace-cross 
Abstract: We introduce a locally differentially private (LDP) algorithm for online federated learning that employs temporally correlated noise to improve utility while preserving privacy. To address challenges posed by the correlated noise and local updates with streaming non-IID data, we develop a perturbed iterate analysis that controls the impact of the noise on the utility. Moreover, we demonstrate how the drift errors from local updates can be effectively managed for several classes of nonconvex loss functions. Subject to an $(\epsilon,\delta)$-LDP budget, we establish a dynamic regret bound that quantifies the impact of key parameters and the intensity of changes in the dynamic environment on the learning performance. Numerical experiments confirm the efficacy of the proposed algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.18752v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.ML</category>
      <pubDate>Fri, 10 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaojiao Zhang, Linglingzhi Zhu, Dominik Fay, Mikael Johansson</dc:creator>
    </item>
  </channel>
</rss>
