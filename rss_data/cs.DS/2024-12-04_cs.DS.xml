<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Dec 2024 05:00:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>You (Almost) Can't Beat Brute Force for 3-Matroid Intersection</title>
      <link>https://arxiv.org/abs/2412.02217</link>
      <description>arXiv:2412.02217v1 Announce Type: new 
Abstract: The $\ell$-matroid intersection ($\ell$-MI) problem asks if $\ell$ given matroids share a common basis. Already for $\ell = 3$, notable canonical NP-complete special cases are $3$-Dimensional Matching and Hamiltonian Path on directed graphs. However, while these problems admit exponential-time algorithms that improve the simple brute force, the fastest known algorithm for $3$-MI is exactly brute force with runtime $2^{n}/poly(n)$, where $n$ is the number of elements. Our first result shows that in fact, brute force cannot be significantly improved, by ruling out an algorithm for $\ell$-MI with runtime $o\left(2^{n-5 \cdot n^{\frac{1}{\ell-1}} \cdot \log (n)}\right)$, for any fixed $\ell\geq 3$.
  The complexity gap between $3$-MI and the polynomially solvable $2$-matroid intersection calls for a better understanding of the complexity of intermediate problems. One such prominent problem is exact matroid intersection (EMI). Given two matroids whose elements are either red or blue and a number $k$, decide if there is a common basis which contains exactly $k$ red elements. We show that EMI does not admit a randomized polynomial time algorithm. This bound implies that the parameterized algorithm of Eisenbrand et al. (FOCS'24) for exact weight matroid cannot be generalized to matroid intersection.
  We further obtain: (i) an algorithm that solves $\ell$-MI faster than brute force in time $2^{n-\Omega\left(\log^2 (n)\right)} $ (ii) a parameterized running time lower bound of $2^{(\ell-2) \cdot k \cdot \log k} \cdot poly(n)$ for $\ell$-MI, where the parameter $k$ is the rank of the matroids. We obtain these two results by generalizing the Monotone Local Search technique of Fomin et al. (J. ACM'19). Broadly speaking, our generalization converts any parameterized algorithm for a subset problem into an exponential-time algorithm which is faster than brute-force.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02217v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilan Doron-Arad, Ariel Kulik, Hadas Shachnai</dc:creator>
    </item>
    <item>
      <title>Testing vs Estimation for Index-Invariant Properties in the Huge Object Model</title>
      <link>https://arxiv.org/abs/2412.02235</link>
      <description>arXiv:2412.02235v1 Announce Type: new 
Abstract: The Huge Object model of property testing [Goldreich and Ron, TheoretiCS 23] concerns properties of distributions supported on $\{0,1\}^n$, where $n$ is so large that even reading a single sampled string is unrealistic. Instead, query access is provided to the samples, and the efficiency of the algorithm is measured by the total number of queries that were made to them.
  Index-invariant properties under this model were defined in [Chakraborty et al., COLT 23], as a compromise between enduring the full intricacies of string testing when considering unconstrained properties, and giving up completely on the string structure when considering label-invariant properties. Index-invariant properties are those that are invariant through a consistent reordering of the bits of the involved strings.
  Here we provide an adaptation of Szemer\'edi's regularity method for this setting, and in particular show that if an index-invariant property admits an $\epsilon$-test with a number of queries depending only on the proximity parameter $\epsilon$, then it also admits a distance estimation algorithm whose number of queries depends only on the approximation parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02235v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sourav Chakraborty, Eldar Fischer, Arijit Ghosh, Amit Levi, Gopinath Mishra, Sayantan Sen</dc:creator>
    </item>
    <item>
      <title>UNIFY: Unified Index for Range Filtered Approximate Nearest Neighbors Search</title>
      <link>https://arxiv.org/abs/2412.02448</link>
      <description>arXiv:2412.02448v1 Announce Type: new 
Abstract: This paper presents an efficient and scalable framework for Range Filtered Approximate Nearest Neighbors Search (RF-ANNS) over high-dimensional vectors associated with attribute values. Given a query vector $q$ and a range $[l, h]$, RF-ANNS aims to find the approximate $k$ nearest neighbors of $q$ among data whose attribute values fall within $[l, h]$. Existing methods including pre-, post-, and hybrid filtering strategies that perform attribute range filtering before, after, or during the ANNS process, all suffer from significant performance degradation when query ranges shift. Though building dedicated indexes for each strategy and selecting the best one based on the query range can address this problem, it leads to index consistency and maintenance issues.
  Our framework, called UNIFY, constructs a unified Proximity Graph-based (PG-based) index that seamlessly supports all three strategies. In UNIFY, we introduce SIG, a novel Segmented Inclusive Graph, which segments the dataset by attribute values. It ensures the PG of objects from any segment combinations is a sub-graph of SIG, thereby enabling efficient hybrid filtering by reconstructing and searching a PG from relevant segments. Moreover, we present Hierarchical Segmented Inclusive Graph (HSIG), a variant of SIG which incorporates a hierarchical structure inspired by HNSW to achieve logarithmic hybrid filtering complexity. We also implement pre- and post-filtering for HSIG by fusing skip list connections and compressed HNSW edges into the hierarchical graph. Experimental results show that UNIFY delivers state-of-the-art RF-ANNS performance across small, mid, and large query ranges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02448v1</guid>
      <category>cs.DS</category>
      <category>cs.DB</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anqi Liang, Pengcheng Zhang, Bin Yao, Zhongpu Chen, Yitong Song, Guangxu Cheng</dc:creator>
    </item>
    <item>
      <title>The Cost of Consistency: Submodular Maximization with Constant Recourse</title>
      <link>https://arxiv.org/abs/2412.02492</link>
      <description>arXiv:2412.02492v1 Announce Type: new 
Abstract: In this work, we study online submodular maximization, and how the requirement of maintaining a stable solution impacts the approximation. In particular, we seek bounds on the best-possible approximation ratio that is attainable when the algorithm is allowed to make at most a constant number of updates per step. We show a tight information-theoretic bound of $\tfrac{2}{3}$ for general monotone submodular functions, and an improved (also tight) bound of $\tfrac{3}{4}$ for coverage functions. Since both these bounds are attained by non poly-time algorithms, we also give a poly-time randomized algorithm that achieves a $0.51$-approximation. Combined with an information-theoretic hardness of $\tfrac{1}{2}$ for deterministic algorithms from prior work, our work thus shows a separation between deterministic and randomized algorithms, both information theoretically and for poly-time algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02492v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul D\"utting, Federico Fusco, Silvio Lattanzi, Ashkan Norouzi-Fard, Ola Svensson, Morteza Zadimoghaddam</dc:creator>
    </item>
    <item>
      <title>The Two-Center Problem of Uncertain Points on Cactus Graphs</title>
      <link>https://arxiv.org/abs/2412.02559</link>
      <description>arXiv:2412.02559v1 Announce Type: new 
Abstract: We study the two-center problem on cactus graphs in facility locations, which aims to place two facilities on the graph network to serve customers in order to minimize the maximum transportation cost. In our problem, the location of each customer is uncertain and may appear at $O(m)$ points on the network with probabilities. More specifically, given are a cactus graph $G$ and a set $\calP$ of $n$ (weighted) uncertain points where every uncertain point has $O(m)$ possible locations on $G$ each associated with a probability and is of a non-negative weight. The problem aims to compute two centers (points) on $G$ so that the maximum (weighted) expected distance of the $n$ uncertain points to their own expected closest center is minimized. No previous algorithms are known for this problem. In this paper, we present the first algorithm for this problem and it solves the problem in $O(|G|+ m^{2}n^{2}\log mn)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02559v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haitao Xu, Jingru Zhang</dc:creator>
    </item>
    <item>
      <title>The Two-Center Problem of Uncertain Points on Trees</title>
      <link>https://arxiv.org/abs/2412.02580</link>
      <description>arXiv:2412.02580v1 Announce Type: new 
Abstract: In this paper, we consider the (weighted) two-center problem of uncertain points on a tree. Given are a tree $T$ and a set $\calP$ of $n$ (weighted) uncertain points each of which has $m$ possible locations on $T$ associated with probabilities. The goal is to compute two points on $T$, i.e., two centers with respect to $\calP$, so that the maximum (weighted) expected distance of $n$ uncertain points to their own expected closest center is minimized. This problem can be solved in $O(|T|+ n^{2}\log n\log mn + mn\log^2 mn \log n)$ time by the algorithm for the general $k$-center problem. In this paper, we give a more efficient and simple algorithm that solves this problem in $O(|T| + mn\log mn)$ time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02580v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haitao Xu, Jingru Zhang</dc:creator>
    </item>
    <item>
      <title>The Space Complexity of Approximating Logistic Loss</title>
      <link>https://arxiv.org/abs/2412.02639</link>
      <description>arXiv:2412.02639v1 Announce Type: new 
Abstract: We provide space complexity lower bounds for data structures that approximate logistic loss up to $\epsilon$-relative error on a logistic regression problem with data $\mathbf{X} \in \mathbb{R}^{n \times d}$ and labels $\mathbf{y} \in \{-1,1\}^d$. The space complexity of existing coreset constructions depend on a natural complexity measure $\mu_\mathbf{y}(\mathbf{X})$, first defined in (Munteanu, 2018). We give an $\tilde{\Omega}(\frac{d}{\epsilon^2})$ space complexity lower bound in the regime $\mu_\mathbf{y}(\mathbf{X}) = O(1)$ that shows existing coresets are optimal in this regime up to lower order factors. We also prove a general $\tilde{\Omega}(d\cdot \mu_\mathbf{y}(\mathbf{X}))$ space lower bound when $\epsilon$ is constant, showing that the dependency on $\mu_\mathbf{y}(\mathbf{X})$ is not an artifact of mergeable coresets. Finally, we refute a prior conjecture that $\mu_\mathbf{y}(\mathbf{X})$ is hard to compute by providing an efficient linear programming formulation, and we empirically compare our algorithm to prior approximate methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02639v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gregory Dexter, Petros Drineas, Rajiv Khanna</dc:creator>
    </item>
    <item>
      <title>Efficient Graph Matching for Correlated Stochastic Block Models</title>
      <link>https://arxiv.org/abs/2412.02661</link>
      <description>arXiv:2412.02661v1 Announce Type: new 
Abstract: We study learning problems on correlated stochastic block models with two balanced communities. Our main result gives the first efficient algorithm for graph matching in this setting. In the most interesting regime where the average degree is logarithmic in the number of vertices, this algorithm correctly matches all but a vanishing fraction of vertices with high probability, whenever the edge correlation parameter $s$ satisfies $s^2 &gt; \alpha \approx 0.338$, where $\alpha$ is Otter's tree-counting constant. Moreover, we extend this to an efficient algorithm for exact graph matching whenever this is information-theoretically possible, positively resolving an open problem of R\'acz and Sridhar (NeurIPS 2021). Our algorithm generalizes the recent breakthrough work of Mao, Wu, Xu, and Yu (STOC 2023), which is based on centered subgraph counts of a large family of trees termed chandeliers. A major technical challenge that we overcome is dealing with the additional estimation errors that are necessarily present due to the fact that, in relevant parameter regimes, the latent community partition cannot be exactly recovered from a single graph. As an application of our results, we give an efficient algorithm for exact community recovery using multiple correlated graphs in parameter regimes where it is information-theoretically impossible to do so using just a single graph.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02661v1</guid>
      <category>cs.DS</category>
      <category>cs.SI</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuwen Chai, Mikl\'os Z. R\'acz</dc:creator>
    </item>
    <item>
      <title>Communication efficient application of sequences of planar rotations to a matrix</title>
      <link>https://arxiv.org/abs/2412.01852</link>
      <description>arXiv:2412.01852v1 Announce Type: cross 
Abstract: We present an efficient algorithm for the application of sequences of planar rotations to a matrix. Applying such sequences efficiently is important in many numerical linear algebra algorithms for eigenvalues. Our algorithm is novel in three main ways. First, we introduce a new kernel that is optimized for register reuse in a novel way. Second, we introduce a blocking and packing scheme that improves the cache efficiency of the algorithm. Finally, we thoroughly analyze the memory operations of the algorithm which leads to important theoretical insights and makes it easier to select good parameters. Numerical experiments show that our algorithm outperforms the state-of-the-art and achieves a flop rate close to the theoretical peak on modern hardware.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01852v1</guid>
      <category>cs.PF</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thijs Steel, Julien Langou</dc:creator>
    </item>
    <item>
      <title>Simple Construction of Greedy Trees and Greedy Permutations</title>
      <link>https://arxiv.org/abs/2412.02554</link>
      <description>arXiv:2412.02554v1 Announce Type: cross 
Abstract: \begin{abstract}
  Greedy permutations, also known as Gonzalez Orderings or Farthest Point Traversals are a standard way to approximate $k$-center clustering and have many applications in sampling and approximating metric spaces.
  A greedy tree is an added structure on a greedy permutation that tracks the (approximate) nearest predecessor.
  Greedy trees have applications in proximity search as well as in topological data analysis.
  For metrics of doubling dimension $d$, a $2^{O(d)}n\log n$ time algorithm is known, but it is randomized and also, quite complicated.
  Its construction involves a series of intermediate structures and $O(n \log n)$ space.
  In this paper, we show how to construct greedy permutations and greedy trees using a simple variation of an algorithm of Clarkson that was shown to run in $2^{O(d)}n\log \Delta$ time, where the spread $\spread$ is the ratio of largest to smallest pairwise distances.
  The improvement comes from the observation that the greedy tree can be constructed more easily than the greedy permutation.
  This leads to a linear time algorithm for merging two approximate greedy trees and thus, an $2^{O(d)}n \log n$ time algorithm for computing the tree.
  Then, we show how to extract a $(1+\frac{1}{n})$-approximate greedy permutation from the approximate greedy tree in the same asymptotic running time. \end{abstract}</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02554v1</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oliver Chubet, Don Sheehy, Siddharth Sheth</dc:creator>
    </item>
    <item>
      <title>The Broader Landscape of Robustness in Algorithmic Statistics</title>
      <link>https://arxiv.org/abs/2412.02670</link>
      <description>arXiv:2412.02670v1 Announce Type: cross 
Abstract: The last decade has seen a number of advances in computationally efficient algorithms for statistical methods subject to robustness constraints. An estimator may be robust in a number of different ways: to contamination of the dataset, to heavy-tailed data, or in the sense that it preserves privacy of the dataset. We survey recent results in these areas with a focus on the problem of mean estimation, drawing technical and conceptual connections between the various forms of robustness, showing that the same underlying algorithmic ideas lead to computationally efficient estimators in all these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02670v1</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Kamath</dc:creator>
    </item>
    <item>
      <title>Restless reachability problems in temporal graphs</title>
      <link>https://arxiv.org/abs/2010.08423</link>
      <description>arXiv:2010.08423v5 Announce Type: replace 
Abstract: We study a family of reachability problems under waiting-time restrictions in temporal and vertex-colored temporal graphs. Given a temporal graph and a set of source vertices, we find the set of vertices that are reachable from a source via a time-respecting path, where the difference in timestamps between consecutive edges is at most a resting time. Given a vertex-colored temporal graph and a multiset query of colors, we find the set of vertices reachable from a source via a time-respecting path such that the vertex colors of the path agree with the multiset query and the difference in timestamps between consecutive edges is at most a resting time. These kind of problems have applications in understanding the spread of a disease in a network, tracing contacts in epidemic outbreaks, finding signaling pathways in the brain network, and recommending tours for tourists, among other.
  We present an algebraic algorithmic framework based on constrained multi\-linear sieving for solving the restless reachability problems we propose. In particular, parameterized by the length $k$ of a path sought, we show that the proposed problems can be solved in $O(2^k k m \Delta)$ time and $O(n \Delta)$ space, where $n$ is the number of vertices, $m$ the number of edges, and $\Delta$ the maximum resting time of an input temporal graph. In addition, we prove that our algorithms for the restless reachability problems in vertex-colored temporal graphs are optimal under plausible complexity-theoretic assumptions. Finally, with an open-source implementation, we demonstrate that our algorithm scales to large graphs with up to one billion temporal edges, despite the problems being NP-hard. Specifically, we present extensive experiments to evaluate our scalability claims both on synthetic and real-world graphs. Our implementation is efficiently engineered and highly optimized.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.08423v5</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Suhas Thejaswi, Juho Lauri, Aristides Gionis</dc:creator>
    </item>
    <item>
      <title>A Polynomial-Time Deterministic Algorithm for An NP-Complete Problem</title>
      <link>https://arxiv.org/abs/2108.03877</link>
      <description>arXiv:2108.03877v3 Announce Type: replace 
Abstract: An NP-complete graph decision problem, the "Multi-stage graph Simple Path" (abbr. MSP) problem, is introduced. The problem is about the decision of existence of specific "global paths" in a graph $G$. We show that the MSP problem can be solved in polynomial ($O(|E|^9)$) time, by proposing a poly-time graph algorithm and the proof of its correctness. Our result implies NP$=$P, and hence no chance is left for NP$\neq$P any more. The algorithm exploits the data structure of reachable-path edge-set $R(e)$. By establishing the inter-play between preceding decisions and subsequent decisions, the computed (in a monotonically decreasing manner) information for $R(e)$ carries all necessary contextual information, and can be utilized to summarize the "history" and to detect the "future" for searching "global paths". Paths are always regarded as a collection of edge sets, for the avoidance of exponential complexity. Our proof of the algorithm builds upon a mathematical inductive proving framework, which relies on a crucial structural property of the MSP problem: all MSP instances are arranged into the sequence {$G_0,G_1,G_2,...$}, and each $G_{j}(j&gt;0)$ in the sequence must have some $G_{i}(0\leq i&lt;j)$ that keeps completely accordant with $G_{j}$ on the existence of "global paths".</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.03877v3</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwen Jiang, Holden Wool</dc:creator>
    </item>
    <item>
      <title>Optimal Clustering with Dependent Costs in Bayesian Networks</title>
      <link>https://arxiv.org/abs/2308.03970</link>
      <description>arXiv:2308.03970v4 Announce Type: replace 
Abstract: Clustering of nodes in Bayesian Networks (BNs) and related graphical models such as Dynamic BNs (DBNs) has been demonstrated to enhance computational efficiency and improve model learning. Typically, it involves the partitioning of the underlying Directed Acyclic Graph (DAG) into cliques, or optimising for some cost or criteria. Computational cost is important since BN and DBN inference, such as estimating marginal distributions given evidence or updating model parameters, is NP-hard. The challenge is exacerbated by cost dependency, where inference outcomes and hence clustering cost depends on both nodes within a cluster and the mapping of clusters that are connected by at least one arc. We propose an algorithm called Dependent Cluster MAPping (DCMAP) which is shown analytically, given an arbitrarily defined, positive cost function, to find all optimal cluster mappings, and do so with no more iterations than an equally informed algorithm. DCMAP is demonstrated on a complex systems seagrass DBN, which has 25 nodes per time-slice, and captures biological, ecological and environmental dynamics and their interactions to predict the impact of dredging stressors on resilience and their cumulative effects over time. The algorithm is employed to find clusters to optimise the computational efficiency of inferring marginal distributions given evidence. For the 25 (one time-slice) and 50-node (two time-slices) DBN, the search space size was $9.91\times10^9$ and $1.51\times10^{21}$ possible cluster mappings, respectively, but the first optimal solution was found at iteration number 856 (95\% CI 852,866), and 1569 (1566,1581) with a cost that was 4\% and 0.2\% of the naive heuristic cost, respectively. Through optimal clustering, DCMAP opens up opportunities for further research beyond improving computational efficiency, such as using clustering to minimise entropy in BN learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.03970v4</guid>
      <category>cs.DS</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Pao-Yen Wu, Fabrizio Ruggeri, Kerrie Mengersen</dc:creator>
    </item>
    <item>
      <title>Practical Parallel Algorithms for Non-Monotone Submodular Maximization</title>
      <link>https://arxiv.org/abs/2308.10656</link>
      <description>arXiv:2308.10656v2 Announce Type: replace 
Abstract: Submodular maximization has found extensive applications in various domains within the field of artificial intelligence, including but not limited to machine learning, computer vision, and natural language processing. With the increasing size of datasets in these domains, there is a pressing need to develop efficient and parallelizable algorithms for submodular maximization. One measure of the parallelizability of a submodular maximization algorithm is its adaptive complexity, which indicates the number of sequential rounds where a polynomial number of queries to the objective function can be executed in parallel. In this paper, we study the problem of non-monotone submodular maximization subject to a knapsack constraint, and propose the first combinatorial algorithm achieving an $(8+\epsilon)$-approximation under $\mathcal{O}(\log n)$ adaptive complexity, which is \textit{optimal} up to a factor of $\mathcal{O}(\log\log n)$. Moreover, we also propose the first algorithm with both provable approximation ratio and sublinear adaptive complexity for the problem of non-monotone submodular maximization subject to a $k$-system constraint. As a by-product, we show that our two algorithms can also be applied to the special case of submodular maximization subject to a cardinality constraint, and achieve performance bounds comparable with those of state-of-the-art algorithms. Finally, the effectiveness of our approach is demonstrated by extensive experiments on real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10656v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuang Cui, Kai Han, Jing Tang, Xueying Li, Aakas Zhiyuli, Hanxiao Li</dc:creator>
    </item>
    <item>
      <title>Graph Reconstruction via MIS Queries</title>
      <link>https://arxiv.org/abs/2401.05845</link>
      <description>arXiv:2401.05845v4 Announce Type: replace 
Abstract: In the Graph Reconstruction (GR) problem, a player initially only knows the vertex set $V$ of an input graph $G=(V, E)$ and is required to learn its set of edges $E$. To this end, the player submits queries to an oracle and must deduce $E$ from the oracle's answers.
  In this paper, we initiate the study of GR via Maximal Independent Set (MIS) queries, a more powerful variant of Independent Set (IS) queries. Given a query $U \subseteq V$, the oracle responds with any, potentially adversarially chosen, maximal independent set $I \subseteq U$ in the induced subgraph $G[U]$.
  We show that, for GR, MIS queries are strictly more powerful than IS queries when parametrized by the maximum degree $\Delta$ of the input graph. We give tight (up to poly-logarithmic factors) upper and lower bounds for this problem:
  1. We observe that the simple strategy of taking uniform independent random samples of $V$ and submitting those to the oracle yields a non-adaptive randomized algorithm that executes $O(\Delta^2 \cdot \log n)$ queries and succeeds with high probability. Furthermore, combining the strategy of taking uniform random samples of $V$ with the probabilistic method, we show the existence of a deterministic non-adaptive algorithm that executes $O(\Delta^3 \cdot \log(\frac{n}{\Delta}))$ queries.
  2. Regarding lower bounds, we prove that the additional $\Delta$ factor when going from randomized non-adaptive algorithms to deterministic non-adaptive algorithms is necessary. We show that every non-adaptive deterministic algorithm requires $\Omega(\Delta^3 / \log^2 \Delta)$ queries. For arbitrary randomized adaptive algorithms, we show that $\Omega(\Delta^2)$ queries are necessary in graphs of maximum degree $\Delta$, and that $\Omega(\log n)$ queries are necessary, even when the input graph is an $n$-vertex cycle.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05845v4</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Konrad, Conor O'Sullivan, Victor Traistaru</dc:creator>
    </item>
    <item>
      <title>Generalized compression and compressive search of large datasets</title>
      <link>https://arxiv.org/abs/2409.12161</link>
      <description>arXiv:2409.12161v2 Announce Type: replace 
Abstract: The Big Data explosion has necessitated the development of search algorithms that scale sub-linearly in time and memory.
  While compression algorithms and search algorithms do exist independently, few algorithms offer both, and those which do are domain-specific.
  We present panCAKES, a novel approach to compressive search, i.e., a way to perform $k$-NN and $\rho$-NN search on compressed data while only decompressing a small, relevant, portion of the data.
  panCAKES assumes the manifold hypothesis and leverages the low-dimensional structure of the data to compress and search it efficiently.
  panCAKES is generic over any distance function for which the distance between two points is proportional to the memory cost of storing an encoding of one in terms of the other.
  This property holds for many widely-used distance functions, e.g. string edit distances (Levenshtein, Needleman-Wunsch, etc.) and set dissimilarity measures (Jaccard, Dice, etc.).
  We benchmark panCAKES on a variety of datasets, including genomic, proteomic, and set data.
  We compare compression ratios to gzip, and search performance between the compressed and uncompressed versions of the same dataset.
  panCAKES achieves compression ratios close to those of gzip, while offering sub-linear time performance for $k$-NN and $\rho$-NN search.
  We conclude that panCAKES is an efficient, general-purpose algorithm for exact compressive search on large datasets that obey the manifold hypothesis.
  We provide an open-source implementation of panCAKES in the Rust programming language.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12161v2</guid>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proc. IEEE Big Data 2024</arxiv:journal_reference>
      <dc:creator>Morgan E. Prior, Thomas Howard III, Emily Light, Najib Ishaq, Noah M. Daniels</dc:creator>
    </item>
    <item>
      <title>Paired-domination Problem on Circle and $k$-polygon Graphs</title>
      <link>https://arxiv.org/abs/2411.19473</link>
      <description>arXiv:2411.19473v2 Announce Type: replace 
Abstract: A vertex set $D \subseteq V$ is considered a dominating set of $G$ if every vertex in $V - D$ is adjacent to at least one vertex in $D$. We called a dominating set $D$ as a paired-dominating set if the subgraph of $G$ induced by $D$ contains a perfect matching. In this paper, we show that determining the minimum paired-dominating set on circle graphs is NP-complete. We further propose an $O(n(\frac{n}{k^2-k})^{2k^2-2k})$-time algorithm for $k$-polygon graphs, a subclass of circle graphs, for finding the minimum paired-dominating set. Moreover, we extend our method to improve the algorithm for finding the minimum dominating set on $k$-polygon graphs in~[\emph{E.S.~Elmallah and L.K.~Stewart, Independence and domination in polygon graphs, Discrete Appl. Math., 1993}] and reduce their time-complexity from $O(n^{4k^2+3})$ to $O(n(\frac{n}{k^2-k})^{2k^2-4k})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19473v2</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ta-Yu Mu, Ching-Chi Lin</dc:creator>
    </item>
    <item>
      <title>Towards a Unified Theory of Light Spanners I: Fast (Yet Optimal) Constructions</title>
      <link>https://arxiv.org/abs/2106.15596</link>
      <description>arXiv:2106.15596v5 Announce Type: replace-cross 
Abstract: Seminal works on light spanners over the years provide spanners with optimal lightness in various graph classes, such as in general graphs, Euclidean spanners, and minor-free graphs. Three shortcomings of previous works on light spanners are: (1) The techniques are ad hoc per graph class, and thus can't be applied broadly. (2) The runtimes of these constructions are almost always sub-optimal, and usually far from optimal. (3) These constructions are optimal in the standard and crude sense, but not in a refined sense that takes into account a wider range of involved parameters.
  This work aims at addressing these shortcomings by presenting a unified framework of light spanners in a variety of graph classes. Informally, the framework boils down to a transformation from sparse spanners to light spanners; since the state-of-the-art for sparse spanners is much more advanced than that for light spanners, such a transformation is powerful. Our framework is developed in two papers. The current paper is the first of the two -- it lays the basis of the unified framework and then applies it to design fast constructions with optimal lightness for several graph classes.
  Among various applications and implications of our framework, we highlight here the following:
  _ In low-dimensional Euclidean spaces, we present an $O(n\log n)$-time construction of $(1+\epsilon)$-spanners with lightness and degree both bounded by constants in the algebraic computation tree (ACT). Our construction resolves a major problem in the area of geometric spanners, which was open for three decades.
  _ In general graphs, for any $k \geq 2$, we construct a $(2k-1)(1+\epsilon)$-spanner with lightness $O(n^{1/k})$ in $O(m \alpha(m,n))$ time. This result for light spanners in general weighted graphs is surprising, as it outperforms the analog one for sparse spanners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.15596v5</guid>
      <category>cs.CG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Le, Shay Solomon</dc:creator>
    </item>
    <item>
      <title>C*: A New Bounding Approach for the Moving-Target Traveling Salesman Problem</title>
      <link>https://arxiv.org/abs/2312.05499</link>
      <description>arXiv:2312.05499v2 Announce Type: replace-cross 
Abstract: We introduce a new bounding approach called Continuity* C*, which provides optimality guarantees for the Moving-Target Traveling Salesman Problem (MT-TSP). Our approach relaxes the continuity constraints on the agent's tour by partitioning the targets' trajectories into smaller segments. This allows the agent to arrive at any point within a segment and depart from any point in the same segment when visiting each target. This formulation enables us to pose the bounding problem as a Generalized Traveling Salesman Problem (GTSP) on a graph, where the cost of traveling along an edge requires solving a new problem called the Shortest Feasible Travel (SFT). We present various methods for computing bounds for the SFT problem, leading to several variants of C*. We first prove that the proposed algorithms provide valid lower-bounds for the MT-TSP. Additionally, we provide computational results to validate the performance of all C* variants on instances with up to 15 targets. For the special case where targets move along straight lines, we compare our C* variants with a mixed-integer Second Order Conic Program (SOCP) based method, the current state-of-the-art solver for the MT-TSP. While the SOCP-based method performs well on instances with 5 and 10 targets, C* outperforms it on instances with 15 targets. For the general case, on average, our approaches find feasible solutions within approximately 4.5% of the lower-bounds for the tested instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.05499v2</guid>
      <category>cs.RO</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allen George Philip, Zhongqiang Ren, Sivakumar Rathinam, Howie Choset</dc:creator>
    </item>
    <item>
      <title>CuckooGraph: A Scalable and Space-Time Efficient Data Structure for Large-Scale Dynamic Graphs</title>
      <link>https://arxiv.org/abs/2405.15193</link>
      <description>arXiv:2405.15193v3 Announce Type: replace-cross 
Abstract: Graphs play an increasingly important role in various big data applications. However, existing graph data structures cannot simultaneously address the performance bottlenecks caused by the dynamic updates, large scale, and high query complexity of current graphs. This paper proposes a novel data structure for large-scale dynamic graphs called CuckooGraph. It does not require any prior knowledge of the upcoming graphs, and can adaptively resize to the most memory-efficient form while requiring few memory accesses for very fast graph data processing. The key techniques of CuckooGraph include TRANSFORMATION and DENYLIST. TRANSFORMATION fully utilizes the limited memory by designing related data structures that allow flexible space transformations to smoothly expand/tighten the required space depending on the number of incoming items. DENYLIST efficiently handles item insertion failures and further improves processing speed. Our experimental results show that compared with the most competitive solution Spruce, CuckooGraph achieves about $33\times$ higher insertion throughput while requiring only about $68\%$ of the memory space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15193v3</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuochen Fan, Yalun Cai, Zirui Liu, Jiarui Guo, Xin Fan, Tong Yang, Bin Cui</dc:creator>
    </item>
    <item>
      <title>Differentially Private Algorithms for Graph Cuts: A Shifting Mechanism Approach and More</title>
      <link>https://arxiv.org/abs/2407.06911</link>
      <description>arXiv:2407.06911v5 Announce Type: replace-cross 
Abstract: In this paper, we address the challenge of differential privacy in the context of graph cuts, specifically focusing on the multiway cut and the minimum $k$-cut. We introduce edge-differentially private algorithms that achieve nearly optimal performance for these problems. Motivated by multiway cut, we propose the shifting mechanism, a general framework for private combinatorial optimization problems. This framework allows us to develop an efficient private algorithm with a multiplicative approximation ratio that matches the state-of-the-art non-private algorithm, improving over previous private algorithms that have provably worse multiplicative loss. We then provide a tight information-theoretic lower bound on the additive error, demonstrating that for constant $k$, our algorithm is optimal in terms of the privacy cost. The shifting mechanism also allows us to design private algorithm for the multicut and max-cut problems, with runtimes determined by the best non-private algorithms for these tasks. For the minimum $k$-cut problem we use a different approach, combining the exponential mechanism with bounds on the number of approximate $k$-cuts to get the first private algorithm with optimal additive error of $O(k\log n)$ (for a fixed privacy parameter). We also establish an information-theoretic lower bound that matches this additive error. Furthermore, we provide an efficient private algorithm even for non-constant $k$, including a polynomial-time 2-approximation with an additive error of $\tilde{O}(k^{1.5})$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06911v5</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rishi Chandra, Michael Dinitz, Chenglin Fan, Zongrui Zou</dc:creator>
    </item>
  </channel>
</rss>
