<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:43:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Augmenting Limited and Biased RCTs through Pseudo-Sample Matching-Based Observational Data Fusion Method</title>
      <link>https://arxiv.org/abs/2509.18148</link>
      <description>arXiv:2509.18148v1 Announce Type: new 
Abstract: In the online ride-hailing pricing context, companies often conduct randomized controlled trials (RCTs) and utilize uplift models to assess the effect of discounts on customer orders, which substantially influences competitive market outcomes. However, due to the high cost of RCTs, the proportion of trial data relative to observational data is small, which only accounts for 0.65\% of total traffic in our context, resulting in significant bias when generalizing to the broader user base. Additionally, the complexity of industrial processes reduces the quality of RCT data, which is often subject to heterogeneity from potential interference and selection bias, making it difficult to correct. Moreover, existing data fusion methods are challenging to implement effectively in complex industrial settings due to the high dimensionality of features and the strict assumptions that are hard to verify with real-world data. To address these issues, we propose an empirical data fusion method called pseudo-sample matching. By generating pseudo-samples from biased, low-quality RCT data and matching them with the most similar samples from large-scale observational data, the method expands the RCT dataset while mitigating its heterogeneity. We validated the method through simulation experiments, conducted offline and online tests using real-world data. In a week-long online experiment, we achieved a 0.41\% improvement in profit, which is a considerable gain when scaled to industrial scenarios with hundreds of millions in revenue. In addition, we discuss the harm to model training, offline evaluation, and online economic benefits when the RCT data quality is not high, and emphasize the importance of improving RCT data quality in industrial scenarios. Further details of the simulation experiments can be found in the GitHub repository https://github.com/Kairong-Han/Pseudo-Matching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18148v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kairong Han, Weidong Huang, Taiyang Zhou, Peng Zhen, Kun Kuang</dc:creator>
    </item>
    <item>
      <title>Estimation and inference in generalised linear models with constrained iteratively-reweighted least squares</title>
      <link>https://arxiv.org/abs/2509.18406</link>
      <description>arXiv:2509.18406v1 Announce Type: new 
Abstract: We propose a simple and flexible framework for generalised linear models (GLM) with linear constraints on the coefficients. Linear constraints are useful in a wide range of applications, allowing the fitting of model with high-dimensional or highly collinear predictors, as well as encoding assumptions on the association between some or all predictors and the response. We propose the constrained iteratively-reweighted least squares (CIRLS) to fit the model, iterating quadratic programs to ensure the coefficient vector remains feasible according to the constraints. Inference for constrained coefficients can be obtained by simulating from a truncated multivariate normal distribution and computing empirical confidence intervals or variance-covariance matrix from the simulated coefficient vectors. We additionally discuss the complexity of a constrained GLM, proposing a measure of expected degrees of freedom which accounts for the stringency of constraints in the reduction of the model degrees of freedom. An extensive simulations study shows that constraining the coefficients introduces some bias to the estimation, but also decreases the estimator variance. This trade-off results in an improved estimator when constraints are chosen appropriately. The simulations also show that our proposed inference results in error in variance estimation and coverage. The proposed framework is illustrated on two case studies, showing its usefulness as well as some of its weaknesses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18406v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Masselot, Devon Nenon, Jacopo Vanoli, Zaid Chalabi, Antonio Gasparrini</dc:creator>
    </item>
    <item>
      <title>Latent class multivariate probit and latent trait models for evaluating test accuracy without a gold standard: A simulation study</title>
      <link>https://arxiv.org/abs/2509.18489</link>
      <description>arXiv:2509.18489v1 Announce Type: new 
Abstract: In the context of an imperfect gold standard, latent class modelling can be used to estimate accuracy of multiple medical tests. However, the conditional independence (CI) assumption is rarely thought to be clinically valid. Two models accommodating conditional dependence are the latent class multivariate probit (LC-MVP) and latent trait models. Despite LC-MVP's greater flexibility - modelling full correlation matrices versus the latent trait's restricted structure - the latent trait has been more widely used. No simulation studies have directly compared these two models.
  We conducted a comprehensive simulation study comparing both models across five data generating mechanisms: CI, low-heterogeneity (latent trait-generated), and high-heterogeneity (LC-MVP-generated) correlation structures. We evaluated multiple priors, including novel constrained correlation priors using Pinkney's method that preserves prior interpretability. Models were fit using our BayesMVP R package, which achieves GPU-like speed-ups on these inherently serial models.
  The LC-MVP model demonstrated superior overall performance. Whilst the latent trait model performed acceptably on its own generated data, it failed for high-heterogeneity structures, sometimes performing worse than the CI model. The CI model did badly for most dependent structures. We also found ceiling effects: high sensitivities reduced the importance of correlation recovery, explaining paradoxes where models achieved good performance despite poor correlation recovery.
  Our results strongly favour LC-MVP for practical applications. The latent trait model's severe consequences under realistic correlation structures make it a more risky choice. However, LC-MVP with custom correlation constraints and priors provides a safer, more flexible framework for test accuracy evaluation without a perfect gold standard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18489v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enzo Cerullo, Sean Pinkney, Alex J. Sutton, Tim Lucas, Nicola J. Cooper, Hayley E. Jones</dc:creator>
    </item>
    <item>
      <title>Functional Mixed effects Model for Joint Analysis of Longitudinal and Cross-Sectional Growth Data</title>
      <link>https://arxiv.org/abs/2509.18491</link>
      <description>arXiv:2509.18491v1 Announce Type: new 
Abstract: A new method is proposed to perform joint analysis of longitudinal and cross-sectional growth data. Clustering is first performed to group similar subjects in cross-sectional data to form a pseudo longitudinal data set, then the pseudo longitudinal data and real longitudinal data are combined and analyzed by using a functional mixed effects model. To account for the variational difference between pseudo and real longitudinal growth data, it is assumed that the covariance functions of the random effects and the variance functions of the measurement errors for pseudo and real longitudinal data can be different. Various simulation studies and real data analysis demonstrate the good performance of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18491v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Long Chen, Ji Chen, Yingchun Zhou</dc:creator>
    </item>
    <item>
      <title>Enhanced Survival Trees</title>
      <link>https://arxiv.org/abs/2509.18494</link>
      <description>arXiv:2509.18494v1 Announce Type: new 
Abstract: We introduce a new survival tree method for censored failure time data that incorporates three key advancements over traditional approaches. First, we develop a more computationally efficient splitting procedure that effectively mitigates the end-cut preference problem, and we propose an intersected validation strategy to reduce the variable selection bias inherent in greedy searches. Second, we present a novel framework for determining tree structures through fused regularization. In combination with conventional pruning, this approach enables the merging of non-adjacent terminal nodes, producing more parsimonious and interpretable models. Third, we address inference by constructing valid confidence intervals for median survival times within the subgroups identified by the final tree. To achieve this, we apply bootstrap-based bias correction to standard errors. The proposed method is assessed through extensive simulation studies and illustrated with data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18494v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiwen Zhou, Ke Xie, Lei Liu, Zhichen Xu, Jimin Ding, Xiaogang Su</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayesian Joint Latent Space Modeling via Cumulative Shrinkage</title>
      <link>https://arxiv.org/abs/2509.18580</link>
      <description>arXiv:2509.18580v1 Announce Type: new 
Abstract: Network models are increasingly vital in psychometrics for analyzing relational data, which are often accompanied by high-dimensional node attributes. Joint latent space models (JLSM) provide an elegant framework for integrating these data sources by assuming a shared underlying latent representation; however, a persistent methodological challenge is determining the dimension of the latent space, as existing methods typically require pre-specification or rely on computationally intensive post-hoc procedures. We develop a novel Bayesian joint latent space model that incorporates a cumulative ordered spike-and-slab (COSS) prior. This approach enables the latent dimension to be inferred automatically and simultaneously with all model parameters. We develop an efficient Markov Chain Monte Carlo (MCMC) algorithm for posterior computation. Theoretically, we establish that the posterior distribution concentrates on the true latent dimension and that parameter estimates achieve Hellinger consistency at a near-optimal rate that adapts to the unknown dimensionality. Through extensive simulations and two real-data applications, we demonstrate the method's superior performance in both dimension recovery and parameter estimation. Our work offers a principled, computationally efficient, and theoretically grounded solution for adaptive dimension selection in psychometric network models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18580v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bin Lv, Yincai Tang, Siliang Zhang</dc:creator>
    </item>
    <item>
      <title>Optimization-centric cutting feedback for semiparametric models</title>
      <link>https://arxiv.org/abs/2509.18708</link>
      <description>arXiv:2509.18708v1 Announce Type: new 
Abstract: Modern statistics deals with complex models from which the joint model used for inference is built by coupling submodels, called modules. We consider modular inference where the modules may depend on parametric and nonparametric components. In such cases, a joint Bayesian inference is highly susceptible to misspecification across any module, and inappropriate priors for nonparametric components may deliver subpar inferences for parametric components, and vice versa. We propose a novel ``optimization-centric'' approach to cutting feedback for semiparametric modular inference, which can address misspecification and prior-data conflicts. The proposed generalized cut posteriors are defined through a variational optimization problem for generalized posteriors where regularization is based on R\'{e}nyi divergence, rather than Kullback-Leibler divergence (KLD), and variational computational methods are developed. We show empirically that using R\'{e}nyi divergence to define the cut posterior delivers more robust inferences than KLD. We derive novel posterior concentration results that accommodate the R\'{e}nyi divergence and allow for semiparametric components, greatly extending existing results for cut posteriors that were derived for parametric models and KLD. We demonstrate these new methods in a benchmark toy example and two real examples: Gaussian process adjustments for confounding in causal inference and misspecified copula models with nonparametric marginals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18708v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Linda S. L. Tan, David J. Nott, David T. Frazier</dc:creator>
    </item>
    <item>
      <title>Tractable Approximation of Labeled Multi-Object Posterior Densities</title>
      <link>https://arxiv.org/abs/2509.18780</link>
      <description>arXiv:2509.18780v1 Announce Type: new 
Abstract: Multi-object estimation in state-space models (SSMs) wherein the system state is represented as a finite set has attracted significant interest in recent years. In Bayesian inference, the posterior density captures all information on the system trajectory since it considers the past history of states. In most multi-object SSM applications, closed-form multi-object posteriors are not available for non-standard multi-object models. Thus, functional approximation is necessary because these posteriors are very high-dimensional. This work provides a tractable multi-scan Generalized Labeled Multi-Bernoulli (GLMB) approximation that matches the trajectory cardinality distribution of the labeled multi-object posterior density. The proposed approximation is also proven to minimize the Kullback-Leibler divergence over a special class of multi-scan GLMB model. Additionally, we develop a tractable algorithm for computing the approximate multi-object posteriors over finite windows. Numerical experiments, including simulation results on a multi-object SSM with social force model and uninformative observations, are presented to validate the applicability of the approximation method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18780v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thi Hong Thai Nguyen, Ba-Ngu Vo, Ba-Tuong Vo</dc:creator>
    </item>
    <item>
      <title>Dynamic Prediction in Mixture Cure Models: A Model-Based Landmarking Approach</title>
      <link>https://arxiv.org/abs/2509.18875</link>
      <description>arXiv:2509.18875v1 Announce Type: new 
Abstract: Mixture cure models are widely used in survival analysis when a portion of patients is considered cured and is no longer at risk for the event of interest. In clinical settings, dynamic survival prediction is particularly important to refine prognosis by incorporating updated patient information over time. Landmarking methods have emerged as a flexible approach for this purpose, as they allow to summarize longitudinal covariates up to a given landmark time and to use these summaries in subsequent prediction. For mixture cure models, the only landmarking strategy available in the literature relies on the last observation carried forward (LOCF) method to summarize longitudinal dynamics up to the landmark time. However, LOCF discards most of the longitudinal information, does not correct for measurement error, and may rely on outdated values if observation times are far apart. To overcome these limitations, we propose a sequential approach that integrates model-based landmarking within a mixture cure model. Initially, longitudinal covariates are modeled using (generalized) linear mixed models, from which individual-specific random effects are predicted. The predicted random effects are then incorporated as covariates into a Cox proportional hazards cure model. We investigated the performance of the proposed approach under different cure fractions, sample sizes, and longitudinal data structures through an extensive simulation study. The results show that the model-based strategy provides more refined predictions compared to LOCF, even when the model is misspecified in favour of the LOCF approach. Finally, we illustrate our method using a real-world dataset on renal transplant patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18875v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Cipriani, Marco Alf\`o, Mirko Signorelli</dc:creator>
    </item>
    <item>
      <title>Causal tail coefficient for compound extremes in multivariate time series</title>
      <link>https://arxiv.org/abs/2509.19007</link>
      <description>arXiv:2509.19007v1 Announce Type: new 
Abstract: Extreme events are often multivariate in nature. A compound extreme occurs when a combination of variables jointly produces a significant impact, even if individual components are not necessarily marginally extreme. Compound extremes have been observed across a wide range of domains, including space weather, climate, and environmental science. For example, heavy rainfall sustained over consecutive days can impose cumulative stress on urban drainage systems, potentially resulting in flooding. However, most existing methods for detecting extremal causality focus primarily on individual extreme values and lack the flexibility to capture causal relationships between compound extremes. This work introduces a novel framework for detecting causal dependencies between extreme events, including compound extremes. We introduce the compound causal tail coefficient that captures the extremal dependance of compound events between pairs of stationary time series. Based on a consistent estimator of this coefficient, we develop a bootstrap hypothesis test to evaluate the presence and direction of causal relationships. Our method can accommodate nonlinearity and latent confounding variables. We demonstrate the effectiveness of our method by establishing theoretic properties and through simulation studies and an application to space-weather data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19007v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cathy Yin, Adam M. Sykulski, Almut E. D. Veraart</dc:creator>
    </item>
    <item>
      <title>Nonparametric efficient estimation of the longitudinal front-door functional</title>
      <link>https://arxiv.org/abs/2509.19040</link>
      <description>arXiv:2509.19040v1 Announce Type: new 
Abstract: The front-door criterion is an identification strategy for the intervention-specific mean outcome in settings where the standard back-door criterion fails due to unmeasured exposure-outcome confounders, but an intermediate variable exists that completely mediates the effect of exposure on the outcome and is not affected by unmeasured confounding. The front-door criterion has been extended to the longitudinal setting, where exposure and mediator are measured repeatedly over time. However, to the best of our knowledge, applications of the longitudinal front-door criterion remain unexplored. This may reflect both limited awareness of the method and the absence of suitable estimation techniques. In this report, we propose nonparametric efficient estimators of the longitudinal front-door functional. The estimators are multiply robust and allow for the use of data-adaptive (machine learning) methods for nuisance estimation while providing valid inference. The theoretical properties of the estimators are showcased in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19040v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marie S. Breum, Helene C. W. Rytgaard, Torben Martinussen, Erin E. Gabriel</dc:creator>
    </item>
    <item>
      <title>Modified Lepage-type test statistics for the weak null hypothesis</title>
      <link>https://arxiv.org/abs/2509.19126</link>
      <description>arXiv:2509.19126v1 Announce Type: new 
Abstract: Detecting simultaneous shifts in the location and scale of two populations is a challenging problem in statistical research. A common way to address this issue is by combining location and scale test statistics. A well-known example is the Lepage test, which combines the Wilcoxon-Mann-Whitney test for location with the Ansari-Bradley test for scale. However, the Wilcoxon-Mann-Whitney test assumes that the population variances are equal, while the Ansari-Bradley test assumes the population medians are equal. This study introduces new approaches that combine recent methodological advances to relax these assumptions. We incorporate the Fligner-Policello test, a distribution-free alternative to the Wilcoxon-Mann-Whitney test that does not require the assumption of equal variances. The Fligner-Policello test is further enhanced by the Fong-Huang method, which provides an improved variance estimation. Additionally, we propose a new variance estimator for the Ansari-Bradley test, thereby eliminating the need for the equal medians assumption. These methodological modifications are integrated into the Lepage framework to operate under a weak null hypothesis. Simulation results suggest that these new tests are promising candidates for location-scale testing. The practical utility of the proposed tests is then demonstrated through an analysis of four real-world biomedical datasets. These empirical applications confirm the robustness and reliability of the modified tests for the two-sample independent location-scale problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19126v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abid Hussain, Michail Tsagris</dc:creator>
    </item>
    <item>
      <title>A survey of Monte Carlo methods for noisy and costly densities with application to reinforcement learning and ABC</title>
      <link>https://arxiv.org/abs/2108.00490</link>
      <description>arXiv:2108.00490v3 Announce Type: cross 
Abstract: This survey gives an overview of Monte Carlo methodologies using surrogate models, for dealing with densities which are intractable, costly, and/or noisy. This type of problem can be found in numerous real-world scenarios, including stochastic optimization and reinforcement learning, where each evaluation of a density function may incur some computationally-expensive or even physical (real-world activity) cost, likely to give different results each time. The surrogate model does not incur this cost, but there are important trade-offs and considerations involved in the choice and design of such methodologies. We classify the different methodologies into three main classes and describe specific instances of algorithms under a unified notation. A modular scheme which encompasses the considered methods is also presented. A range of application scenarios is discussed, with special attention to the likelihood-free setting and reinforcement learning. Several numerical comparisons are also provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.00490v3</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1111/insr.12573</arxiv:DOI>
      <arxiv:journal_reference>International Statistical Review. 2024</arxiv:journal_reference>
      <dc:creator>F. Llorente, L. Martino, J. Read, D. Delgado</dc:creator>
    </item>
    <item>
      <title>On Multi-entity, Multivariate Quickest Change Point Detection</title>
      <link>https://arxiv.org/abs/2509.18310</link>
      <description>arXiv:2509.18310v1 Announce Type: cross 
Abstract: We propose a framework for online Change Point Detection (CPD) from multi-entity, multivariate time series data, motivated by applications in crowd monitoring where traditional sensing methods (e.g., video surveillance) may be infeasible. Our approach addresses the challenge of detecting system-wide behavioral shifts in complex, dynamic environments where the number and behavior of individual entities may be uncertain or evolve. We introduce the concept of Individual Deviation from Normality (IDfN), computed via a reconstruction-error-based autoencoder trained on normal behavior. We aggregate these individual deviations using mean, variance, and Kernel Density Estimates (KDE) to yield a System-Wide Anomaly Score (SWAS). To detect persistent or abrupt changes, we apply statistical deviation metrics and the Cumulative Sum (CUSUM) technique to these scores. Our unsupervised approach eliminates the need for labeled data or feature extraction, enabling real-time operation on streaming input. Evaluations on both synthetic datasets and crowd simulations, explicitly designed for anomaly detection in group behaviors, demonstrate that our method accurately detects significant system-level changes, offering a scalable and privacy-preserving solution for monitoring complex multi-agent systems. In addition to this methodological contribution, we introduce new, challenging multi-entity multivariate time series datasets generated from crowd simulations in Unity and coupled nonlinear oscillators. To the best of our knowledge, there is currently no publicly available dataset of this type designed explicitly to evaluate CPD in complex collective and interactive systems, highlighting an essential gap that our work addresses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18310v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bahar Kor, Bipin Gaikwad, Abani Patra, Eric L. Miller</dc:creator>
    </item>
    <item>
      <title>Evaluating Bias Reduction Methods in Binary Emax Model for Reliable Dose-Response Estimation</title>
      <link>https://arxiv.org/abs/2509.18459</link>
      <description>arXiv:2509.18459v1 Announce Type: cross 
Abstract: The Binary Emax model is widely employed in dose-response analysis during Phase II clinical studies to identify the optimal dose for subsequence confirmatory trials. The parameter estimation and inference heavily rely on the asymptotic properties of Maximum Likelihood (ML) estimators; however, this approach may be questionable under small or moderate sample sizes and is not robust to violation of model assumptions. To provide a reliable solution, this paper examines three bias-reduction methods: the Cox-Snell bias correction, Firth-score modification, and a maximum penalized likelihood estimator (MPLE) using Jeffreys prior. Through comprehensive simulation studies, we evaluate the performance of these methods in reducing bias and controlling variance, especially when model assumptions are violated. The results demonstrate that both Firth and MPLE methods provide robust estimates, with MPLE outperforming in terms of stability and lower variance. We further illustrate the practical application of these methods using data from the TURANDOT study, a Phase II clinical trial. Our findings suggest that MPLE with Jeffreys prior offers an effective and reliable alternative to the Firth method, particularly for dose-response relationships that deviate from monotonicity, making it valuable for robust parameter estimation in dose-ranging studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18459v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiangshan Zhang, Vivek Pradhan, Yuxi Zhao</dc:creator>
    </item>
    <item>
      <title>Optimal estimation for regression discontinuity design with binary outcomes</title>
      <link>https://arxiv.org/abs/2509.18857</link>
      <description>arXiv:2509.18857v1 Announce Type: cross 
Abstract: We develop a finite-sample optimal estimator for regression discontinuity designs when the outcomes are bounded, including binary outcomes as the leading case. Our finite-sample optimal estimator achieves the exact minimax mean squared error among linear shrinkage estimators with nonnegative weights when the regression function of a bounded outcome lies in a Lipschitz class. Although the original minimax problem involves an iterating (n+1)-dimensional non-convex optimization problem where n is the sample size, we show that our estimator is obtained by solving a convex optimization problem. A key advantage of our estimator is that the Lipschitz constant is the only tuning parameter. We also propose a uniformly valid inference procedure without a large-sample approximation. In a simulation exercise for small samples, our estimator exhibits smaller mean squared errors and shorter confidence intervals than conventional large-sample techniques which may be unreliable when the effective sample size is small. We apply our method to an empirical multi-cutoff design where the sample size for each cutoff is small. In the application, our method yields informative confidence intervals, in contrast to the leading large-sample approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18857v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Takuya Ishihara, Masayuki Sawada, Kohei Yata</dc:creator>
    </item>
    <item>
      <title>Markov Combinations of Discrete Statistical Models</title>
      <link>https://arxiv.org/abs/2509.18983</link>
      <description>arXiv:2509.18983v1 Announce Type: cross 
Abstract: Markov combination is an operation that takes two statistical models and produces a third whose marginal distributions include those of the original models. Building upon and extending existing work in the Gaussian case, we develop Markov combinations for categorical variables and their statistical models. We present several variants of this operation, both algorithmically and from a sampling perspective, and discuss relevant examples and theoretical properties. We describe Markov combinations for special models such as regular exponential families, discrete copulas, and staged trees. Finally, we offer results about model invariance and the maximum likelihood estimation of Markov combinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18983v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Orlando Marigliano, Eva Riccomagno</dc:creator>
    </item>
    <item>
      <title>Quasi-randomization tests for network interference</title>
      <link>https://arxiv.org/abs/2403.16673</link>
      <description>arXiv:2403.16673v3 Announce Type: replace 
Abstract: Network interference amounts to the treatment status of one unit affecting the potential outcome of other units in the population. Testing for spillover effects in this setting makes the null hypothesis non-sharp. An interesting approach to tackling the non-sharp nature of the null hypothesis in this setup is constructing conditional randomization tests such that the null is sharp on the restricted population. In randomized experiments, conditional randomized tests hold finite sample validity and are assumption-lean. In this paper, we incorporate the network amongst the population as a random variable instead of being fixed. We propose a new approach that builds a conditional quasi-randomization test. To build the (non-sharp) null distribution of no spillover effects, we use random graph null models. We show that our method is exactly valid in finite samples under mild assumptions. Our method displays enhanced power over state-of-the-art methods, with a substantial improvement in cluster randomized trials. We illustrate our methodology to test for interference in a weather insurance adoption experiment run in rural China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16673v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Supriya Tiwari, Pallavi Basu</dc:creator>
    </item>
    <item>
      <title>YEAST: Yet Another Sequential Test</title>
      <link>https://arxiv.org/abs/2406.16523</link>
      <description>arXiv:2406.16523v3 Announce Type: replace 
Abstract: Online evaluation of machine learning models is typically conducted through A/B experiments. Sequential statistical tests are valuable tools for analysing these experiments, as they enable researchers to stop data collection early without increasing the risk of false discoveries. However, existing sequential tests either limit the number of interim analyses or suffer from low statistical power. In this paper, we introduce a novel sequential test designed for continuous monitoring of A/B experiments. We validate our method using semi-synthetic simulations and demonstrate that it outperforms current state-of-the-art sequential testing approaches. Our method is derived using a new technique that inverts a bound on the probability of threshold crossing, based on a classical maximal inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16523v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexey Kurennoy, Majed Dodin, Tural Gurbanov, Ana Peleteiro Ramallo</dc:creator>
    </item>
    <item>
      <title>"6 choose 4": A framework to understand and facilitate discussion of strategies for overall survival safety monitoring</title>
      <link>https://arxiv.org/abs/2410.04020</link>
      <description>arXiv:2410.04020v2 Announce Type: replace 
Abstract: Advances in anticancer therapies have significantly contributed to declining death rates in certain disease and clinical settings. However, they have also made it difficult to power a clinical trial in these settings with overall survival (OS) as the primary efficacy endpoint. Therefore, two approaches have been recently proposed for the pre-specified analysis of OS as a safety endpoint (Fleming et al., 2024; Rodriguez et al., 2024). In this paper, we provide a simple, unifying framework that includes the aforementioned approaches (and a couple others) as special cases. By highlighting each approach's focus, priority, tolerance for risk, and strengths or challenges for practical implementation, this framework can help to facilitate discussions between stakeholders on "fit-for-purpose OS data collection and assessment of harm" (American Association for Cancer Research, 2024). We apply this framework to a real clinical trial in large B-cell lymphoma to illustrate its application and value. Several recommendations and open questions are also raised.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04020v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Godwin Yung, Kaspar Rufibach, Marcel Wolbers, Mark Yan, Jue Wang</dc:creator>
    </item>
    <item>
      <title>Semiparametric Spatial Point Processes</title>
      <link>https://arxiv.org/abs/2410.04359</link>
      <description>arXiv:2410.04359v3 Announce Type: replace 
Abstract: We introduce a broad class of models called semiparametric spatial point process for making inference between spatial point patterns and spatial covariates. These models feature an intensity function with both parametric and nonparametric components. For the parametric component, we derive the semiparametric efficiency lower bound under Poisson point patterns and propose a point process double machine learning estimator that can achieve this lower bound. The proposed estimator for the parametric component is also shown to be consistent and asymptotically normal for non-Poisson point patterns. For the nonparametric component, we propose a kernel-based estimator and characterize its rates of convergence. Computationally, we introduce a fast, numerical approximation that transforms the proposed estimator into an estimator derived from weighted generalized partial linear models. We conclude with a simulation study and two real data analyses from ecology and hydrogeology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04359v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xindi Lin, Bumjun Park, Christopher Zahasky, Hyunseung Kang</dc:creator>
    </item>
    <item>
      <title>Bayesian Multivariate Density-Density Regression</title>
      <link>https://arxiv.org/abs/2504.12617</link>
      <description>arXiv:2504.12617v2 Announce Type: replace 
Abstract: We introduce a novel and scalable Bayesian framework for multivariate-density-density regression (DDR), designed to model relationships between multivariate distributions. Our approach addresses the critical issue of distributions residing in spaces of differing dimensions. We utilize a generalized Bayes framework, circumventing the need for a fully specified likelihood by employing the sliced Wasserstein distance to measure the discrepancy between fitted and observed distributions. This choice not only handles high-dimensional data and varying sample sizes efficiently but also facilitates a Metropolis-adjusted Langevin algorithm (MALA) for posterior inference. Furthermore, we establish the posterior consistency of our generalized Bayesian approach, ensuring that the posterior distribution concentrates around the true parameters as the sample size increases. Through simulations and application to a population-scale single-cell dataset, we show that Bayesian DDR provides robust fits, superior predictive performance compared to traditional methods, and valuable insights into complex biological interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12617v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Yang Ni, Peter Mueller</dc:creator>
    </item>
    <item>
      <title>Probabilistic patient risk profiling with pair-copula constructions</title>
      <link>https://arxiv.org/abs/2506.13731</link>
      <description>arXiv:2506.13731v2 Announce Type: replace 
Abstract: We propose vine copula-based classifiers for probabilistic risk prediction in perioperative settings. We obtain full joint probability models for mixed continuous-ordinal variables by fitting a separate vine copula to each outcome class, capturing nonlinear and tail-asymmetric dependence. In a cohort of 767 elective bowel surgeries (81 serious vs. 686 non-serious complications), posterior probabilities from the fitted vine classification models are used to allocate patients into low-, moderate-, and high-risk groups. Compared to weighted logistic regression and random forests with stratified sampling, the vine copula-based classifiers achieve up to 10% lower class-specific Brier scores and negative log-likelihoods on the out-of-sample. The vine copula-based classifier identifies a large cohort of true low-risk patients potentially eligible for early discharge. Scenario analyses based on the fitted vine copula models provide interpretable risk profiles, including nonlinear relationships between body mass index, surgery duration, and blood loss, which might remain undetected under linear models. These results demonstrate that vine copula-based classifiers offer a reliable and interpretable framework for individualized, probability-based patient risk profiling. As such, they represent a new, promising tool for data-driven decision-making in perioperative care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13731v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge \c{S}ahin</dc:creator>
    </item>
    <item>
      <title>Survival Analysis with Discrete Biomarkers Under a Semiparametric Bayesian Conditional Poisson Model</title>
      <link>https://arxiv.org/abs/2509.08162</link>
      <description>arXiv:2509.08162v2 Announce Type: replace 
Abstract: Discrete biomarkers derived as cell densities or counts from tissue microarrays and immunostaining are widely used to study immune signatures in relation to survival outcomes in cancer. Although routinely collected, these signatures are not measured with exact precision because the sampling mechanism involves examination of small tissue cores from a larger section of interest. We model these error-prone biomarkers as Poisson processes with latent rates, inducing heteroscedasticity in their conditional variance. While critical for tumor histology, such measurement error frameworks remain understudied for conditionally Poisson-distributed covariates. To address this, we propose a Bayesian joint model that incorporates a Dirichlet process (DP) mixture to flexibly characterize the latent covariate distribution. The proposed approach is evaluated using simulation studies which demonstrate a superior bias reduction and robustness to the underlying model in realistic settings when compared to existing methods. We further incorporate Bayes factors for hypothesis testing in the Bayesian semiparametric joint model. The methodology is applied to a survival study of high-grade serous carcinoma where comparisons are made between the proposed and existing approaches. Accompanying R software is available at the GitHub repository listed in the Web Appendices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08162v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Aijun Yang, Phineas T. Hamilton, Brad H. Nelson, Julian J. Lum, Mary Lesperance, Farouk S. Nathoo</dc:creator>
    </item>
    <item>
      <title>Language Models as Causal Effect Generators</title>
      <link>https://arxiv.org/abs/2411.08019</link>
      <description>arXiv:2411.08019v2 Announce Type: replace-cross 
Abstract: In this work, we present sequence-driven structural causal models (SD-SCMs), a framework for specifying causal models with user-defined structure and language-model-defined mechanisms. We characterize how an SD-SCM enables sampling from observational, interventional, and counterfactual distributions according to the desired causal structure. We then leverage this procedure to propose a new type of benchmark for causal inference methods, generating individual-level counterfactual data to test treatment effect estimation. We create an example benchmark consisting of thousands of datasets, and test a suite of popular estimation methods for average, conditional average, and individual treatment effect estimation. We find under this benchmark that (1) causal methods outperform non-causal methods and that (2) even state-of-the-art methods struggle with individualized effect estimation, suggesting this benchmark captures some inherent difficulties in causal estimation. Apart from generating data, this same technique can underpin the auditing of language models for (un)desirable causal effects, such as misinformation or discrimination. We believe SD-SCMs can serve as a useful tool in any application that would benefit from sequential data with controllable causal structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08019v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucius E. J. Bynum, Kyunghyun Cho</dc:creator>
    </item>
    <item>
      <title>Bayes Error Rate Estimation in Difficult Situations</title>
      <link>https://arxiv.org/abs/2506.03159</link>
      <description>arXiv:2506.03159v3 Announce Type: replace-cross 
Abstract: The Bayes Error Rate (BER) is the fundamental limit on the achievable generalizable classification accuracy of any machine learning model due to inherent uncertainty within the data. BER estimators offer insight into the difficulty of any classification problem and set expectations for optimal classification performance. In order to be useful, the estimators must also be accurate with a limited number of samples on multivariate problems with unknown class distributions. To determine which estimators meet the minimum requirements for "usefulness", an in-depth examination of their accuracy is conducted using Monte Carlo simulations with synthetic data in order to obtain their confidence bounds for binary classification. To examine the usability of the estimators for real-world applications, new non-linear multi-modal test scenarios are introduced. In each scenario, 2500 Monte Carlo simulations per scenario are run over a wide range of BER values. In a comparison of k-Nearest Neighbor (kNN), Generalized Henze-Penrose (GHP) divergence and Kernel Density Estimation (KDE) techniques, results show that kNN is overwhelmingly the more accurate non-parametric estimator. In order to reach the target of an under 5% range for the 95% confidence bounds, the minimum number of required samples per class is 1000. As more features are added, more samples are needed, so that 2500 samples per class are required at only 4 features. Other estimators do become more accurate than kNN as more features are added, but continuously fail to meet the target range.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03159v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lesley Wheat, Martin v. Mohrenschildt, Saeid Habibi</dc:creator>
    </item>
    <item>
      <title>Demystifying Spectral Feature Learning for Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2506.10899</link>
      <description>arXiv:2506.10899v2 Announce Type: replace-cross 
Abstract: We address the problem of causal effect estimation in the presence of hidden confounders, using nonparametric instrumental variable (IV) regression. A leading strategy employs spectral features - that is, learned features spanning the top eigensubspaces of the operator linking treatments to instruments. We derive a generalization error bound for a two-stage least squares estimator based on spectral features, and gain insights into the method's performance and failure modes. We show that performance depends on two key factors, leading to a clear taxonomy of outcomes. In a good scenario, the approach is optimal. This occurs with strong spectral alignment, meaning the structural function is well-represented by the top eigenfunctions of the conditional operator, coupled with this operator's slow eigenvalue decay, indicating a strong instrument. Performance degrades in a bad scenario: spectral alignment remains strong, but rapid eigenvalue decay (indicating a weaker instrument) demands significantly more samples for effective feature learning. Finally, in the ugly scenario, weak spectral alignment causes the method to fail, regardless of the eigenvalues' characteristics. Our synthetic experiments empirically validate this taxonomy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.10899v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitri Meunier, Antoine Moulin, Jakub Wornbard, Vladimir R. Kostic, Arthur Gretton</dc:creator>
    </item>
    <item>
      <title>On admissibility in post-hoc hypothesis testing</title>
      <link>https://arxiv.org/abs/2508.00770</link>
      <description>arXiv:2508.00770v2 Announce Type: replace-cross 
Abstract: The validity of classical hypothesis testing requires the significance level $\alpha$ be fixed before any statistical analysis takes place. This is a stringent requirement. For instance, it prohibits updating $\alpha$ during (or after) an experiment due to changing concern about the cost of false positives, or to reflect unexpectedly strong evidence against the null. Perhaps most disturbingly, witnessing a p-value $p\ll\alpha$ vs $p= \alpha- \epsilon$ for tiny $\epsilon &gt; 0$ has no (statistical) relevance for any downstream decision-making. Following recent work of Gr\"unwald (2024), we develop a theory of post-hoc hypothesis testing, enabling $\alpha$ to be chosen after seeing and analyzing the data. To study "good" post-hoc tests we introduce $\Gamma$-admissibility, where $\Gamma$ is a set of adversaries which map the data to a significance level. We classify the set of $\Gamma$-admissible rules for various sets $\Gamma$, showing they must be based on e-values, and recover the Neyman-Pearson lemma when $\Gamma$ is the constant map. We also give a Rao-Blackwellization result, proving that the expected utility of an e-value can be improved (for any concave utility) by conditioning on a sufficient statistic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00770v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Tyron Lardy, Aaditya Ramdas, Peter Gr\"unwald</dc:creator>
    </item>
    <item>
      <title>Bilateral Distribution Compression: Reducing Both Data Size and Dimensionality</title>
      <link>https://arxiv.org/abs/2509.17543</link>
      <description>arXiv:2509.17543v2 Announce Type: replace-cross 
Abstract: Existing distribution compression methods reduce dataset size by minimising the Maximum Mean Discrepancy (MMD) between original and compressed sets, but modern datasets are often large in both sample size and dimensionality. We propose Bilateral Distribution Compression (BDC), a two-stage framework that compresses along both axes while preserving the underlying distribution, with overall linear time and memory complexity in dataset size and dimension. Central to BDC is the Decoded MMD (DMMD), which quantifies the discrepancy between the original data and a compressed set decoded from a low-dimensional latent space. BDC proceeds by (i) learning a low-dimensional projection using the Reconstruction MMD (RMMD), and (ii) optimising a latent compressed set with the Encoded MMD (EMMD). We show that this procedure minimises the DMMD, guaranteeing that the compressed set faithfully represents the original distribution. Experiments show that across a variety of scenarios BDC can achieve comparable or superior performance to ambient-space compression at substantially lower cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.17543v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dominic Broadbent, Nick Whiteley, Robert Allison, Tom Lovett</dc:creator>
    </item>
  </channel>
</rss>
