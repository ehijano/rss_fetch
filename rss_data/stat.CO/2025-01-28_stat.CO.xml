<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Modernizing full posterior inference for surrogate modeling of categorical-output simulation experiments</title>
      <link>https://arxiv.org/abs/2501.14946</link>
      <description>arXiv:2501.14946v1 Announce Type: new 
Abstract: Gaussian processes (GPs) are powerful tools for nonlinear classification in which latent GPs are combined with link functions. But GPs do not scale well to large training data. This is compounded for classification where the latent GPs require Markov chain Monte Carlo integration. Consequently, fully Bayesian, sampling-based approaches had been largely abandoned. Instead, maximization-based alternatives, such as Laplace/variational inference (VI) combined with low rank approximations, are preferred. Though feasible for large training data sets, such schemes sacrifice uncertainty quantification and modeling fidelity, two aspects that are important to our work on surrogate modeling of computer simulation experiments. Here we are motivated by a large scale simulation of binary black hole (BBH) formation. We propose an alternative GP classification framework which uses elliptical slice sampling for Bayesian posterior integration and Vecchia approximation for computational thrift. We demonstrate superiority over VI-based alternatives for BBH simulations and other benchmark classification problems. We then extend our setup to warped inputs for "deep" nonstationary classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14946v1</guid>
      <category>stat.CO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Cooper, Annie S. Booth, Robert B. Gramacy</dc:creator>
    </item>
    <item>
      <title>Sampling with time-changed Markov processes</title>
      <link>https://arxiv.org/abs/2501.15155</link>
      <description>arXiv:2501.15155v1 Announce Type: new 
Abstract: We study time-changed Markov processes to speed up the convergence of Markov chain Monte Carlo (MCMC) algorithms in the context of multimodal distributions and rare event simulation. The time-changed process is defined by adjusting the speed of time of a base process via a user-chosen, state-dependent function. We apply this framework to several Markov processes from the MCMC literature, such as Langevin diffusions and piecewise deterministic Markov processes, obtaining novel modifications of classical algorithms and also re-discovering known MCMC algorithms. We prove theoretical properties of the time-changed process under suitable conditions on the base process, focusing on connecting the stationary distributions and qualitative convergence properties such as geometric and uniform ergodicity, as well as a functional central limit theorem. A comparison with the framework of space transformations is provided, clarifying the similarities between the approaches. Throughout the paper we give various visualisations and numerical simulations on simple tasks to gain intuition on the method and its performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15155v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea Bertazzi, Giorgos Vasdekis</dc:creator>
    </item>
    <item>
      <title>A mirror descent approach to maximum likelihood estimation in latent variable models</title>
      <link>https://arxiv.org/abs/2501.15896</link>
      <description>arXiv:2501.15896v1 Announce Type: new 
Abstract: We introduce an approach based on mirror descent and sequential Monte Carlo (SMC) to perform joint parameter inference and posterior estimation in latent variable models. This approach is based on minimisation of a functional over the parameter space and the space of probability distributions and, contrary to other popular approaches, can be implemented when the latent variable takes values in discrete spaces. We provide a detailed theoretical analysis of both the mirror descent algorithm and its approximation via SMC. We experimentally show that the proposed algorithm outperforms standard expectation maximisation algorithms and is competitive with other popular methods for real-valued latent variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15896v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesca R. Crucinio</dc:creator>
    </item>
    <item>
      <title>A characterization of uniform distribution using varextropy with application in testing uniformity</title>
      <link>https://arxiv.org/abs/2501.14797</link>
      <description>arXiv:2501.14797v1 Announce Type: cross 
Abstract: In statistical analysis, quantifying uncertainties through measures such as entropy, extropy, varentropy, and varextropy is of fundamental importance for understanding distribution functions. This paper investigates several properties of varextropy and give a new characterization of uniform distribution using varextropy. The alredy proposed estimators are used as a test statistics. Building on the characterization of the uniform distribution using varextropy, we give a uniformity test. The critical value and power of the test statistics are derived. The proposed test procedure is applied to a real-world dataset to assess its performance and effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14797v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santosh Kumar Chaudhary, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>A Semiparametric Bayesian Method for Instrumental Variable Analysis with Partly Interval-Censored Time-to-Event Outcome</title>
      <link>https://arxiv.org/abs/2501.14837</link>
      <description>arXiv:2501.14837v1 Announce Type: cross 
Abstract: This paper develops a semiparametric Bayesian instrumental variable analysis method for estimating the causal effect of an endogenous variable when dealing with unobserved confounders and measurement errors with partly interval-censored time-to-event data, where event times are observed exactly for some subjects but left-censored, right-censored, or interval-censored for others. Our method is based on a two-stage Dirichlet process mixture instrumental variable (DPMIV) model which simultaneously models the first-stage random error term for the exposure variable and the second-stage random error term for the time-to-event outcome using a bivariate Gaussian mixture of the Dirichlet process (DPM) model. The DPM model can be broadly understood as a mixture model with an unspecified number of Gaussian components, which relaxes the normal error assumptions and allows the number of mixture components to be determined by the data. We develop an MCMC algorithm for the DPMIV model tailored for partly interval-censored data and conduct extensive simulations to assess the performance of our DPMIV method in comparison with some competing methods. Our simulations revealed that our proposed method is robust under different error distributions and can have superior performance over its parametric counterpart under various scenarios. We further demonstrate the effectiveness of our approach on an UK Biobank data to investigate the causal effect of systolic blood pressure on time-to-development of cardiovascular disease from the onset of diabetes mellitus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.14837v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Han Cui, Xuyang Lu, Jin Zhou, Hua Zhou, Gang Li</dc:creator>
    </item>
    <item>
      <title>Reliable Pseudo-labeling via Optimal Transport with Attention for Short Text Clustering</title>
      <link>https://arxiv.org/abs/2501.15194</link>
      <description>arXiv:2501.15194v1 Announce Type: cross 
Abstract: Short text clustering has gained significant attention in the data mining community. However, the limited valuable information contained in short texts often leads to low-discriminative representations, increasing the difficulty of clustering. This paper proposes a novel short text clustering framework, called Reliable \textbf{P}seudo-labeling via \textbf{O}ptimal \textbf{T}ransport with \textbf{A}ttention for Short Text Clustering (\textbf{POTA}), that generate reliable pseudo-labels to aid discriminative representation learning for clustering. Specially, \textbf{POTA} first implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information. Additionally, the proposed OT can adaptively estimate cluster distributions, making \textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering. Extensive experiments demonstrate \textbf{POTA} outperforms state-of-the-art methods. The code is available at: \href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15194v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihao Yao, Jixuan Yin, Bo Li</dc:creator>
    </item>
    <item>
      <title>rcpptimer: Rcpp Tic-Toc Timer with OpenMP Support</title>
      <link>https://arxiv.org/abs/2501.15856</link>
      <description>arXiv:2501.15856v1 Announce Type: cross 
Abstract: Efficient code writing is both a critical and challenging task, especially with the growing demand for computationally intensive algorithms in statistical and machine-learning applications. Despite the availability of significant computational power today, the need for optimized algorithm implementations remains crucial. Many R users rely on Rcpp to write performant code in C++, but writing and benchmarking C++ code presents its own difficulties. While R's benchmarking tools are insufficient for measuring the execution times of C++ code segments, C++'s native profiling tools often come with a steep learning curve. The rcpptimer package bridges this gap by offering a simple and efficient solution for timing C++ code within the Rcpp ecosystem. This novel package introduces a user-friendly tic-toc class that supports overlapping and nested timers and OpenMP parallelism, providing nanosecond-level time resolution. Results, including summary statistics, are seamlessly passed back to R without requiring users to write any C++ code. This paper contextualizes the rcpptimer package within the broader ecosystem of R and C++ profiling tools, explains the motivation behind its development, and offers a comprehensive overview of its implementation. Supplementary to this paper, we provide multiple vignettes that thoroughly explain this package's usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15856v1</guid>
      <category>cs.MS</category>
      <category>cs.PF</category>
      <category>stat.CO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Berrisch</dc:creator>
    </item>
    <item>
      <title>Properties of the generalized inverse Gaussian with applications to Monte Carlo simulation and distribution function evaluation</title>
      <link>https://arxiv.org/abs/2401.00749</link>
      <description>arXiv:2401.00749v3 Announce Type: replace 
Abstract: The generalized inverse Gaussian, denoted $\mathrm{GIG}(p, a, b)$, is a flexible family of distributions that includes the gamma, inverse gamma, and inverse Gaussian distributions as special cases. In addition to its applications in statistical modeling and its theoretical interest, the GIG often arises in computational statistics, especially in Markov chain Monte Carlo (MCMC) algorithms for posterior inference. This article introduces two mixture representations for the GIG: one that expresses the distribution as a continuous mixture of inverse Gaussians and another that reveals a recursive relationship between GIGs with different values of $p$. The former representation forms the basis for a data augmentation scheme that leads to a geometrically ergodic Gibbs sampler for the GIG. This simple Gibbs sampler, which alternates between gamma and inverse Gaussian conditional distributions, can be incorporated within an encompassing MCMC algorithm when simulation from a GIG is required. The latter representation leads to algorithms for exact, rejection-free sampling as well as CDF evaluation for the GIG with half-integer $p.$ We highlight computational examples from the literature where these new algorithms could be applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00749v3</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Pe\~na, Michael Jauch</dc:creator>
    </item>
    <item>
      <title>Polynomial time sampling from log-smooth distributions in fixed dimension under semi-log-concavity of the forward diffusion with application to strongly dissipative distributions</title>
      <link>https://arxiv.org/abs/2501.00565</link>
      <description>arXiv:2501.00565v2 Announce Type: replace 
Abstract: In this article, we provide a stochastic sampling algorithm with polynomial complexity in fixed dimension that leverages the recent advances on diffusion models where it is shown that under mild conditions, sampling can be achieved via an accurate estimation of intermediate scores across the marginals $(p_t)_{t\ge 0}$ of the standard Ornstein-Uhlenbeck process started at $\mu$, the density we wish to sample from. The heart of our method consists into approaching these scores via a computationally cheap estimator and relating the variance of this estimator to the smoothness properties of the forward process. Under the assumption that the density to sample from is $L$-log-smooth and that the forward process is semi-log-concave: $-\nabla^2 \log(p_t) \succeq -\beta I_d$ for some $\beta \geq 0$, we prove that our algorithm achieves an expected $\epsilon$ error in $\text{KL}$ divergence in $O(d^7(L+\beta)^2L^{d+2}\epsilon^{-2(d+3)}(d+m_2(\mu))^{2(d+1)})$ time with $m_2(\mu)$ the second order moment of $\mu$. In particular, our result allows to fully transfer the problem of sampling from a log-smooth distribution into a regularity estimate problem. As an application, we derive an exponential complexity improvement for the problem of sampling from an $L$-log-smooth distribution that is $\alpha$-strongly log-concave outside some ball of radius $R$: after proving that such distributions verify the semi-log-concavity assumption, a result which might be of independent interest, we recover a $poly(R, L, \alpha^{-1}, \epsilon^{-1})$ complexity in fixed dimension which exponentially improves upon the previously known $poly(e^{LR^2}, L,\alpha^{-1}, \log(\epsilon^{-1}))$ complexity in the low precision regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.00565v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Adrien Vacher, Omar Chehab, Anna Korba</dc:creator>
    </item>
    <item>
      <title>Gaussian entropic optimal transport: Schr\"odinger bridges and the Sinkhorn algorithm</title>
      <link>https://arxiv.org/abs/2412.18432</link>
      <description>arXiv:2412.18432v2 Announce Type: replace-cross 
Abstract: Entropic optimal transport problems are regularized versions of optimal transport problems. These models play an increasingly important role in machine learning and generative modelling. For finite spaces, these problems are commonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting procedure). However, in more general settings the Sinkhorn iterations are based on nonlinear conditional/conjugate transformations and exact finite-dimensional solutions cannot be computed. This article presents a finite-dimensional recursive formulation of the iterative proportional fitting procedure for general Gaussian multivariate models. As expected, this recursive formulation is closely related to the celebrated Kalman filter and related Riccati matrix difference equations, and it yields algorithms that can be implemented in practical settings without further approximations. We extend this filtering methodology to develop a refined and self-contained convergence analysis of Gaussian Sinkhorn algorithms, including closed form expressions of entropic transport maps and Schr\"odinger bridges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18432v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.CO</category>
      <pubDate>Tue, 28 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>O. Deniz Akyildiz, Pierre Del Moral, Joaqu\'in Miguez</dc:creator>
    </item>
  </channel>
</rss>
