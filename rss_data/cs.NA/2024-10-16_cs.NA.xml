<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 04:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Generic MATLAB Toolbox to Approximate PDEs Using Computational Geometry</title>
      <link>https://arxiv.org/abs/2410.12026</link>
      <description>arXiv:2410.12026v1 Announce Type: new 
Abstract: This article introduces a general purpose framework and software to approximate partial differential equations (PDEs). The sparsity patterns of finite element discretized operators is identified automatically using the tools from computational geometry. They may enable experimentation with novel mesh generation techniques and could simplify the implementation of methods such as multigrid. We also implement quadrature methods following the work of Grundmann and Moller. These methods have been overlooked in the past but are more efficient than traditional tensor product methods. The proposed framework is applied to several standard examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12026v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kiefer Green, Harbir Antil</dc:creator>
    </item>
    <item>
      <title>Global random walk for one-dimensional one-phase Stefan-type moving-boundary problems: Simulation results</title>
      <link>https://arxiv.org/abs/2410.12378</link>
      <description>arXiv:2410.12378v1 Announce Type: new 
Abstract: This work presents global random walk approximations of solutions to one-dimensional Stefan-type moving-boundary problems. We are particularly interested in the case when the moving boundary is driven by an explicit representation of its speed. This situation is usually referred to in the literature as moving-boundary problem with kinetic condition. As a direct application, we propose a numerical scheme to forecast the penetration of small diffusants into a rubber-based material. To check the quality of our results, we compare the numerical results obtained by global random walks either using the analytical solution to selected benchmark cases or relying on finite element approximations with a priori known convergence rates. It turns out that the global random walk concept can be used to produce good quality approximations of the weak solutions to the target class of problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12378v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolae Suciu, Surendra Nepal, Yosief Wondmagegne, Magnus \"Ogren, Adrian Muntean</dc:creator>
    </item>
    <item>
      <title>The eigenvalue decomposition of normal matrices by the decomposition of the skew-symmetric part with applications to orthogonal matrices</title>
      <link>https://arxiv.org/abs/2410.12421</link>
      <description>arXiv:2410.12421v1 Announce Type: new 
Abstract: We propose a fast method for computing the eigenvalue decomposition of a dense real normal matrix $A$. The method leverages algorithms that are known to be efficiently implemented, such as the bidiagonal singular value decomposition and the symmetric eigenvalue decomposition. For symmetric and skew-symmetric matrices, the method reduces to calling the latter, so that its advantages are for orthogonal matrices mostly and, potentially, any other normal matrix. The method relies on the real Schur decomposition of the skew-symmetric part of $A$. To obtain the eigenvalue decomposition of the normal matrix $A$, additional steps depending on the distribution of the eigenvalues are required. We provide a complexity analysis of the method and compare its numerical performance with existing algorithms. In most cases, the method is as fast as obtaining the Hessenberg factorization of a dense matrix. Finally, we evaluate the method's accuracy and provide experiments for the application of a Karcher mean on the special orthogonal group.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12421v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Mataigne, Kyle gallivan</dc:creator>
    </item>
    <item>
      <title>An Efficient Explicit-Implicit Adaptive Method for Peridynamic Modelling of Quasi-Static Fracture Formation and Evolution</title>
      <link>https://arxiv.org/abs/2410.12552</link>
      <description>arXiv:2410.12552v1 Announce Type: new 
Abstract: Understanding the quasi-static fracture formation and evolution is essential for assessing the mechanical properties and structural load-bearing capacity of materials. Peridynamics (PD) provides an effective computational method to depict fracture mechanics. The explicit adaptive dynamic relaxation (ADR) method and the implicit methods are two mainstream PD approaches to simulate evolution of quasi-static fractures. However, no comprehensive and quantitative studies have been reported to compare their accuracy and efficiency. In this work, we first develop an implicit method for bond-based peridynamics (BBPD) based on the full nonlinear equilibrium equation and the degenerate form of the bond failure function, where the Jacobian matrices are derived using the Newton-Raphson (NR) scheme. Subsequently, we analyze the solvability of the implicit BBPD scheme. Second, a consistent and comprehensive comparison of accuracy and efficiency of the explicit ADR and implicit methods is conducted, which reveals computational efficiency of the implicit methods and their limitations in accurately describing crack formation. Finally, by utilizing the unique advantage of both methods, we develop an adaptive explicit-implicit method and propose a switching criterion to deploy appropriate scheme accordingly. Four typical quasi-static problems are employed as the numerical experiments, which show the acceleration ratios of the current method range from 6.4 to 141.7 when compared to the explicit ADR. Therefore, the explicit-implicit adaptive method provides a powerful method to simulate quasi-static fracture formation and evolution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12552v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shiwei Hu, Tianbai Xiao, Mingshuo Han, Zuoxu Li, Erkan Oterkus, Selda Oterkus, Yonghao Zhang</dc:creator>
    </item>
    <item>
      <title>A finite difference method with symmetry properties for the high-dimensional Bratu equation</title>
      <link>https://arxiv.org/abs/2410.12553</link>
      <description>arXiv:2410.12553v1 Announce Type: new 
Abstract: Solving the three-dimensional (3D) Bratu equation is highly challenging due to the presence of multiple and sharp solutions. Research on this equation began in the late 1990s, but there are no satisfactory results to date. To address this issue, we introduce a symmetric finite difference method (SFDM) which embeds the symmetry properties of the solutions into a finite difference method (FDM). This SFDM is primarily used to obtain more accurate solutions and bifurcation diagrams for the 3D Bratu equation. Additionally, we propose modifying the Bratu equation by incorporating a new constraint that facilitates the construction of bifurcation diagrams and simplifies handling the turning points. The proposed method, combined with the use of sparse matrix representation, successfully solves the 3D Bratu equation on grids of up to $301^3$ points. The results demonstrate that SFDM outperforms all previously employed methods for the 3D Bratu equation. Furthermore, we provide bifurcation diagrams for the 1D, 2D, 4D, and 5D cases, and accurately identify the first turning points in all dimensions. All simulations indicate that the bifurcation diagrams of the Bratu equation on the cube domains closely resemble the well-established behavior on the ball domains described by Joseph and Lundgren [1]. Furthermore, when SFDM is applied to linear stability analysis, it yields the same largest real eigenvalue as the standard FDM despite having fewer equations and variables in the nonlinear system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12553v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.OC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muhammad Luthfi Shahab, Hadi Susanto, Haralampos Hatzikirou</dc:creator>
    </item>
    <item>
      <title>Mixed-precision finite element kernels and assembly: Rounding error analysis and hardware acceleration</title>
      <link>https://arxiv.org/abs/2410.12614</link>
      <description>arXiv:2410.12614v1 Announce Type: new 
Abstract: In this paper we develop the first fine-grained rounding error analysis of finite element (FE) cell kernels and assembly. The theory includes mixed-precision implementations and accounts for hardware-acceleration via matrix multiplication units, thus providing theoretical guidance for designing reduced- and mixed-precision FE algorithms on CPUs and GPUs. Guided by this analysis, we introduce hardware-accelerated mixed-precision implementation strategies which are provably robust to low-precision computations. Indeed, these algorithms are accurate to the lower-precision unit roundoff with an error constant that is independent from: the conditioning of FE basis function evaluations, the ill-posedness of the cell, the polynomial degree, and the number of quadrature nodes. Consequently, we present the first AMX-accelerated FE kernel implementations on Intel Sapphire Rapids CPUs. Numerical experiments demonstrate that the proposed mixed- (single/half-) precision algorithms are up to 60 times faster than their double precision equivalent while being orders of magnitude more accurate than their fully half-precision counterparts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12614v1</guid>
      <category>math.NA</category>
      <category>cs.AR</category>
      <category>cs.MS</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>M. Croci, G. N. Wells</dc:creator>
    </item>
    <item>
      <title>Activation functions enabling the addition of neurons and layers without altering outcomes</title>
      <link>https://arxiv.org/abs/2410.12625</link>
      <description>arXiv:2410.12625v1 Announce Type: new 
Abstract: In this work, we propose activation functions for neuronal networks that are refinable and sum the identity. This new class of activation function allows the insertion of new layers between existing ones and/or the increase of neurons in a layer, both without altering the network outputs.
  Our approach is grounded in subdivision theory. The proposed activation functions are constructed from basic limit functions of convergent subdivision schemes. As a showcase of our results, we introduce a family of spline activation functions and provide comprehensive details for their practical implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12625v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio L\'opez-Ure\~na</dc:creator>
    </item>
    <item>
      <title>A comparative analysis of metamodels for lumped cardiovascular models, and pipeline for sensitivity analysis, parameter estimation, and uncertainty quantification</title>
      <link>https://arxiv.org/abs/2410.12654</link>
      <description>arXiv:2410.12654v1 Announce Type: new 
Abstract: Zero-dimensional (0D) cardiovascular models are reduced-order models used to study global circulation dynamics and transport. They provide estimates of biomarkers (such as pressure, flow rates, and concentrations) for surgery planning and boundary conditions for high-fidelity 3D models. Although their computational cost is low, tasks like parameter estimation and uncertainty quantification require many model evaluations, making them computationally expensive. This motivates building metamodels. In this work, we propose a pipeline from 0D models to metamodel building for tasks such as sensitivity analysis, parameter estimation, and uncertainty quantification. Three strategies are explored: Neural Networks, Polynomial Chaos Expansion, and Gaussian Processes, applied to three different 0D models. The first model predicts portal vein pressure after surgery, considering liver hemodynamics and global circulation. The second simulates whole-body circulation under pulmonary arterial hypertension before and after shunt insertion. The third assesses organ blood perfusion after revascularization surgery, focusing on contrast agent transport, requiring specific metamodel treatment. Metamodels are trained and tested on synthetic data. Neural networks proved the most efficient in terms of result quality, computational time, and ease for parameter estimation, sensitivity analysis, and uncertainty quantification. Finally, we demonstrate the full pipeline with a neural network as the emulator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12654v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>q-bio.TO</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>John M. Hanna, Pavlos Varsos, J\'er\^ome Kowalski, Lorenzo Sala, Roel Meiburg, Irene E. Vignon-Clementel</dc:creator>
    </item>
    <item>
      <title>Should exponential integrators be used for advection-dominated problems?</title>
      <link>https://arxiv.org/abs/2410.12765</link>
      <description>arXiv:2410.12765v1 Announce Type: new 
Abstract: In this paper, we consider the application of exponential integrators to problems that are advection dominated, either on the entire or on a subset of the domain. In this context, we compare Leja and Krylov based methods to compute the action of exponential and related matrix functions. We set up a performance model by counting the different operations needed to implement the considered algorithms. This model assumes that the evaluation of the right-hand side is memory bound and allows us to evaluate performance in a hardware independent way. We find that exponential integrators perform comparably to explicit Runge-Kutta schemes for problems that are advection dominated in the entire domain. Moreover, they are able to outperform explicit methods in situations where small parts of the domain are diffusion dominated. We generally observe that Leja based methods outperform Krylov iterations in the problems considered. This is in particular true if computing inner products is expensive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12765v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lukas Einkemmer, Trung-Hau Hoang, Alexander Ostermann</dc:creator>
    </item>
    <item>
      <title>On the time complexity analysis of numerical percolation threshold estimation</title>
      <link>https://arxiv.org/abs/2410.11874</link>
      <description>arXiv:2410.11874v1 Announce Type: cross 
Abstract: The main purpose of percolation theory is to model phase transitions in a variety of random systems, which is highly valuable in fields related to materials physics, biology, or otherwise unrelated areas like oil extraction or even quantum computing. Thus, one of the problems encountered is the calculation of the threshold at which such transition occurs, known as percolation threshold. Since there are no known closed forms to determine the threshold in an exact manner in systems with particular properties, it is decided to rely on numerical methods as the Monte Carlo approach, which provides a sufficiently accurate approximation to serve as a valid estimate in the projects or research where it is involved. However, in order to achieve an exact characterization of the threshold in two-dimensional systems with site percolation, in this work it is performed an analysis of the complexity, both temporal and spatial, of an algorithm that implements its computation from the aforementioned numerical method. Specifically, the conduction of an accurate analysis of the cost of such algorithm implies a deep enough knowledge about certain metrics regarding its duration, or work completed per iteration, which along with its formalization may contribute to the determination of the threshold based on these metrics. Nevertheless, as a result, various bounds are achieved for the best, average and worst cases of the execution on systems spanning several dimensions, revealing that in 1 and 2 the complexity is directly conditioned by the duration, although from 3 onwards no proof for this point has been found, notwithstanding the evidence suggesting its compliance. Furthermore, based on the average case, several methods are proposed that could be applied to characterize the threshold, although they have not been thoroughly explored beyond what is necessary for the complexity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11874v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.CC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Garc\'ia Solla</dc:creator>
    </item>
    <item>
      <title>Parametric model reduction of mean-field and stochastic systems via higher-order action matching</title>
      <link>https://arxiv.org/abs/2410.12000</link>
      <description>arXiv:2410.12000v1 Announce Type: cross 
Abstract: The aim of this work is to learn models of population dynamics of physical systems that feature stochastic and mean-field effects and that depend on physics parameters. The learned models can act as surrogates of classical numerical models to efficiently predict the system behavior over the physics parameters. Building on the Benamou-Brenier formula from optimal transport and action matching, we use a variational problem to infer parameter- and time-dependent gradient fields that represent approximations of the population dynamics. The inferred gradient fields can then be used to rapidly generate sample trajectories that mimic the dynamics of the physical system on a population level over varying physics parameters. We show that combining Monte Carlo sampling with higher-order quadrature rules is critical for accurately estimating the training objective from sample data and for stabilizing the training process. We demonstrate on Vlasov-Poisson instabilities as well as on high-dimensional particle and chaotic systems that our approach accurately predicts population dynamics over a wide range of parameters and outperforms state-of-the-art diffusion-based and flow-based modeling that simply condition on time and physics parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12000v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jules Berman, Tobias Blickhan, Benjamin Peherstorfer</dc:creator>
    </item>
    <item>
      <title>Transfer Learning on Multi-Dimensional Data: A Novel Approach to Neural Network-Based Surrogate Modeling</title>
      <link>https://arxiv.org/abs/2410.12241</link>
      <description>arXiv:2410.12241v1 Announce Type: cross 
Abstract: The development of efficient surrogates of partial differential equations (PDEs) is a critical step towards scalable modeling of complex, multiscale systems-of-systems. Convolutional neural networks (CNNs) have gained popularity as the basis for such surrogate models due to their success in capturing high-dimensional input-output mappings and the negligible cost of a forward pass. However, the high cost of generating training data -- typically via classical numerical solvers -- raises the question of whether these models are worth pursuing over more straightforward alternatives with well-established theoretical foundations, such as Monte Carlo methods. To reduce the cost of data generation, we propose training a CNN surrogate model on a mixture of numerical solutions to both the $d$-dimensional problem and its ($d-1$)-dimensional approximation, taking advantage of the efficiency savings guaranteed by the curse of dimensionality. We demonstrate our approach on a multiphase flow test problem, using transfer learning to train a dense fully-convolutional encoder-decoder CNN on the two classes of data. Numerical results from a sample uncertainty quantification task demonstrate that our surrogate model outperforms Monte Carlo with several times the data generation budget.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12241v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrienne M. Propp, Daniel M. Tartakovsky</dc:creator>
    </item>
    <item>
      <title>Ellipsoidal Density-Equalizing Map for Genus-0 Closed Surfaces</title>
      <link>https://arxiv.org/abs/2410.12331</link>
      <description>arXiv:2410.12331v1 Announce Type: cross 
Abstract: Surface parameterization is a fundamental task in geometry processing and plays an important role in many science and engineering applications. In recent years, the density-equalizing map, a shape deformation technique based on the physical principle of density diffusion, has been utilized for the parameterization of simply connected and multiply connected open surfaces. More recently, a spherical density-equalizing mapping method has been developed for the parameterization of genus-0 closed surfaces. However, for genus-0 closed surfaces with extreme geometry, using a spherical domain for the parameterization may induce large geometric distortion. In this work, we develop a novel method for computing density-equalizing maps of genus-0 closed surfaces onto an ellipsoidal domain. This allows us to achieve ellipsoidal area-preserving parameterizations and ellipsoidal parameterizations with controlled area change. We further propose an energy minimization approach that combines density-equalizing maps and quasi-conformal maps, which allows us to produce ellipsoidal density-equalizing quasi-conformal maps for achieving a balance between density-equalization and quasi-conformality. Using our proposed methods, we can significantly improve the performance of surface remeshing for genus-0 closed surfaces. Experimental results on a large variety of genus-0 closed surfaces are presented to demonstrate the effectiveness of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12331v1</guid>
      <category>cs.GR</category>
      <category>cs.CG</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyuan Lyu, Lok Ming Lui, Gary P. T. Choi</dc:creator>
    </item>
    <item>
      <title>A Primal-dual algorithm for image reconstruction with ICNNs</title>
      <link>https://arxiv.org/abs/2410.12441</link>
      <description>arXiv:2410.12441v1 Announce Type: cross 
Abstract: We address the optimization problem in a data-driven variational reconstruction framework, where the regularizer is parameterized by an input-convex neural network (ICNN). While gradient-based methods are commonly used to solve such problems, they struggle to effectively handle non-smoothness which often leads to slow convergence. Moreover, the nested structure of the neural network complicates the application of standard non-smooth optimization techniques, such as proximal algorithms. To overcome these challenges, we reformulate the problem and eliminate the network's nested structure. By relating this reformulation to epigraphical projections of the activation functions, we transform the problem into a convex optimization problem that can be efficiently solved using a primal-dual algorithm. We also prove that this reformulation is equivalent to the original variational problem. Through experiments on several imaging tasks, we demonstrate that the proposed approach outperforms subgradient methods in terms of both speed and stability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12441v1</guid>
      <category>math.OC</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hok Shing Wong, Matthias J. Ehrhardt, Subhadip Mukherjee</dc:creator>
    </item>
    <item>
      <title>Low-Rank Adversarial PGD Attack</title>
      <link>https://arxiv.org/abs/2410.12607</link>
      <description>arXiv:2410.12607v1 Announce Type: cross 
Abstract: Adversarial attacks on deep neural network models have seen rapid development and are extensively used to study the stability of these networks. Among various adversarial strategies, Projected Gradient Descent (PGD) is a widely adopted method in computer vision due to its effectiveness and quick implementation, making it suitable for adversarial training. In this work, we observe that in many cases, the perturbations computed using PGD predominantly affect only a portion of the singular value spectrum of the original image, suggesting that these perturbations are approximately low-rank. Motivated by this observation, we propose a variation of PGD that efficiently computes a low-rank attack. We extensively validate our method on a range of standard models as well as robust models that have undergone adversarial training. Our analysis indicates that the proposed low-rank PGD can be effectively used in adversarial training due to its straightforward and fast implementation coupled with competitive performance. Notably, we find that low-rank PGD often performs comparably to, and sometimes even outperforms, the traditional full-rank PGD attack, while using significantly less memory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12607v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dayana Savostianova, Emanuele Zangrando, Francesco Tudisco</dc:creator>
    </item>
    <item>
      <title>Generative Neural Reparameterization for Differentiable PDE-constrained Optimization</title>
      <link>https://arxiv.org/abs/2410.12683</link>
      <description>arXiv:2410.12683v1 Announce Type: cross 
Abstract: Partial-differential-equation (PDE)-constrained optimization is a well-worn technique for acquiring optimal parameters of systems governed by PDEs. However, this approach is limited to providing a single set of optimal parameters per optimization. Given a differentiable PDE solver, if the free parameters are reparameterized as the output of a neural network, that neural network can be trained to learn a map from a probability distribution to the distribution of optimal parameters. This proves useful in the case where there are many well performing local minima for the PDE. We apply this technique to train a neural network that generates optimal parameters that minimize laser-plasma instabilities relevant to laser fusion and show that the neural network generates many well performing and diverse minima.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12683v1</guid>
      <category>physics.comp-ph</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.plasm-ph</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Archis S. Joglekar</dc:creator>
    </item>
    <item>
      <title>Phase retrieval via media diversity</title>
      <link>https://arxiv.org/abs/2410.12767</link>
      <description>arXiv:2410.12767v1 Announce Type: cross 
Abstract: This work studies phase retrieval for wave fields, aiming to recover the phase of an incoming wave from multi-plane intensity measurements behind different types of linear and nonlinear media. We show that unique phase retrieval can be achieved by utilizing intensity data produced by multiple media. This uniqueness does not require prescribed boundary conditions for the phase in the incidence plane, in contrast to existing phase retrieval methods based on the transport of intensity equation. Moreover, the uniqueness proofs lead to explicit phase reconstruction algorithms. Numerical simulations are presented to validate the theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12767v1</guid>
      <category>physics.optics</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>math.NA</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Cheng, Kui Ren, Nathan Soedjak</dc:creator>
    </item>
    <item>
      <title>Generative modeling with low-rank Wasserstein polynomial chaos expansions</title>
      <link>https://arxiv.org/abs/2203.09358</link>
      <description>arXiv:2203.09358v2 Announce Type: replace 
Abstract: A new Wasserstein multi-element polynomial chaos expansion (WPCE) is proposed, which is inspired by recent advances in computational optimal transport for estimating Wasserstein distances. The developed method combines unsupervised learning with the explicit functional representation of a random vector $Y$. Its training only relies on a finite set of samples from an unknown distribution, which is used to minimize a regularized empirical Wasserstein metric known as debiased Sinkhorn divergence. An interesting application that motivates the approach comes from the numerical upscaling of non-periodic random fields defined on a micro-scale. The WPCE can encode higher order stochastic information about the effective material behavior in contrast to the constant characterization with stochastic homogenization. A striking feature of the new method is the generalization of common diffeomorphic transport maps to the case of discontinuous and non-injective model classes $\mathcal{M}$ with possibly different input and output dimension. It computes a (functional) relation $Y=\mathcal{M}(X)$ in distribution with input random variables $X$ and target $Y$. The exponential growth of the PCE is alleviated by a new stacked tensor train (STT) format. By the choice of the model class $\mathcal{M}$ and the smooth loss function, higher-order optimization schemes and in particular Riemannian descent methods become possible. The proposed approach is illustrated numerically with a high-dimensional upscaling problem, which considers a microscopic random non-periodic composite material. It results in a computationally tractable effective macroscopic random field in adapted stochastic coordinates. By using a relaxation to a discontinuous model class, multimodal distributions also become tractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.09358v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Gruhlke, Martin Eigel</dc:creator>
    </item>
    <item>
      <title>ResQPASS: an algorithm for bounded variable linear least squares with asymptotic Krylov convergence</title>
      <link>https://arxiv.org/abs/2302.13616</link>
      <description>arXiv:2302.13616v4 Announce Type: replace 
Abstract: We present the Residual Quadratic Programming Active-Set Subspace (ResQPASS) method that solves large-scale linear least-squares problems with bound constraints on the variables. The problem is solved by creating a series of small problems of increasing size by projecting onto the basis of residuals. Each projected problem is solved by the active-set method for convex quadratic programming, warm-started with a working set and solution from the previous problem. The method coincides with conjugate gradients (CG) or, equivalently, LSQR when none of the constraints is active. When only a few constraints are active the method converges, after a few initial iterations, like CG and LSQR. An analysis links the convergence to an asymptotic Krylov subspace. We also present an efficient implementation where QR factorizations of the projected problems are updated over the inner iterations and Cholesky the outer iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.13616v4</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bas Symoens, Wim Vanroose</dc:creator>
    </item>
    <item>
      <title>Bounds on non-linear errors for variance computation with stochastic rounding</title>
      <link>https://arxiv.org/abs/2304.05177</link>
      <description>arXiv:2304.05177v2 Announce Type: replace 
Abstract: The main objective of this work is to investigate non-linear errors and pairwise summation using stochastic rounding (SR) in variance computation algorithms. We estimate the forward error of computations under SR through two methods: the first is based on a bound of the variance and Bienaym{\'e}-Chebyshev inequality, while the second is based on martingales and Azuma-Hoeffding inequality. The study shows that for pairwise summation, using SR results in a probabilistic bound of the forward error proportional to log(n)u rather than the deterministic bound in O(log(n)u) when using the default rounding mode. We examine two algorithms that compute the variance, called ''textbook'' and ''two-pass'', which both exhibit non-linear errors. Using the two methods mentioned above, we show that these algorithms' forward errors have probabilistic bounds under SR in O($\sqrt$ nu) instead of nu for the deterministic bounds. We show that this advantage holds using pairwise summation for both textbook and two-pass, with probabilistic bounds of the forward error proportional to log(n)u.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.05177v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1137/23M1563001</arxiv:DOI>
      <dc:creator>El-Mehdi El Arar (LI-PaRAD, UVSQ), Devan Sohier (LI-PaRAD, UVSQ), Pablo de Oliveira Castro (LI-PaRAD, UVSQ), Eric Petit</dc:creator>
    </item>
    <item>
      <title>On full linear convergence and optimal complexity of adaptive FEM with inexact solver</title>
      <link>https://arxiv.org/abs/2311.15738</link>
      <description>arXiv:2311.15738v5 Announce Type: replace 
Abstract: The ultimate goal of any numerical scheme for partial differential equations (PDEs) is to compute an approximation of user-prescribed accuracy at quasi-minimal computational time. To this end, algorithmically, the standard adaptive finite element method (AFEM) integrates an inexact solver and nested iterations with discerning stopping criteria balancing the different error components. The analysis ensuring optimal convergence order of AFEM with respect to the overall computational cost critically hinges on the concept of R-linear convergence of a suitable quasi-error quantity. This work tackles several shortcomings of previous approaches by introducing a new proof strategy. First, the algorithm requires several fine-tuned parameters in order to make the underlying analysis work. A redesign of the standard line of reasoning and the introduction of a summability criterion for R-linear convergence allows us to remove restrictions on those parameters. Second, the usual assumption of a (quasi-)Pythagorean identity is replaced by the generalized notion of quasi-orthogonality from [Feischl, Math. Comp., 91 (2022)]. Importantly, this paves the way towards extending the analysis to general inf-sup stable problems beyond the energy minimization setting. Numerical experiments investigate the choice of the adaptivity parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15738v5</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Bringmann, Michael Feischl, Ani Miraci, Dirk Praetorius, Julian Streitberger</dc:creator>
    </item>
    <item>
      <title>Energy Bounds for Discontinuous Galerkin Spectral Element Approximations of Well-Posed Overset Grid Problems for Hyperbolic Systems</title>
      <link>https://arxiv.org/abs/2405.04668</link>
      <description>arXiv:2405.04668v2 Announce Type: replace 
Abstract: We show that even though the Discontinuous Galerkin Spectral Element Method is stable for hyperbolic boundary-value problems, and the overset domain problem is well-posed in an appropriate norm, the energy of the approximation of the latter is bounded by data only for fixed polynomial order, mesh, and time. In the absence of dissipation, coupling of the overlapping domains is destabilizing by allowing positive eigenvalues in the system to be integrated in time. This coupling can be stabilized in one space dimension by using the upwind numerical flux. To help provide additional dissipation, we introduce a novel penalty method that applies dissipation at arbitrary points within the overlap region and depends only on the difference between the solutions. We present numerical experiments in one space dimension to illustrate the implementation of the well-posed penalty formulation, and show spectral convergence of the approximations when sufficient dissipation is applied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04668v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David A. Kopriva, Andrew R. Winters, Jan Nordstr\"om</dc:creator>
    </item>
    <item>
      <title>Which Spaces can be Embedded in $L_p$-type Reproducing Kernel Banach Space? A Characterization via Metric Entropy</title>
      <link>https://arxiv.org/abs/2410.11116</link>
      <description>arXiv:2410.11116v2 Announce Type: replace 
Abstract: In this paper, we establish a novel connection between the metric entropy growth and the embeddability of function spaces into reproducing kernel Hilbert/Banach spaces. Metric entropy characterizes the information complexity of function spaces and has implications for their approximability and learnability. Classical results show that embedding a function space into a reproducing kernel Hilbert space (RKHS) implies a bound on its metric entropy growth. Surprisingly, we prove a \textbf{converse}: a bound on the metric entropy growth of a function space allows its embedding to a $L_p-$type Reproducing Kernel Banach Space (RKBS). This shows that the ${L}_p-$type RKBS provides a broad modeling framework for learnable function classes with controlled metric entropies. Our results shed new light on the power and limitations of kernel methods for learning complex function spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11116v2</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.FA</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiping Lu, Daozhe Lin, Qiang Du</dc:creator>
    </item>
    <item>
      <title>Control of Probability Flow in Markov Chain Monte Carlo -- Nonreversibility and Lifting</title>
      <link>https://arxiv.org/abs/1207.0258</link>
      <description>arXiv:1207.0258v2 Announce Type: replace-cross 
Abstract: The Markov Chain Monte Carlo (MCMC) method is widely used in various fields as a powerful numerical integration technique for systems with many degrees of freedom. In MCMC methods, probabilistic state transitions can be considered as a random walk in state space, and random walks allow for sampling from complex distributions. However, paradoxically, it is necessary to carefully suppress the randomness of the random walk to improve computational efficiency. By breaking detailed balance, we can create a probability flow in the state space and perform more efficient sampling along this flow. Motivated by this idea, practical and efficient nonreversible MCMC methods have been developed over the past ten years. In particular, the lifting technique, which introduces probability flows in an extended state space, has been applied to various systems and has proven more efficient than conventional reversible updates. We review and discuss several practical approaches to implementing nonreversible MCMC methods, including the shift method in the cumulative distribution and the directed-worm algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:1207.0258v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hidemaro Suwa, Synge Todo</dc:creator>
    </item>
    <item>
      <title>Collocation-based Robust Variational Physics-Informed Neural Networks (CRVPINN)</title>
      <link>https://arxiv.org/abs/2401.02300</link>
      <description>arXiv:2401.02300v3 Announce Type: replace-cross 
Abstract: Physics-Informed Neural Networks (PINNs) have been successfully applied to solve Partial Differential Equations (PDEs). Their loss function is founded on a strong residual minimization scheme. Variational Physics-Informed Neural Networks (VPINNs) are their natural extension to weak variational settings. In this context, the recent work of Robust Variational Physics-Informed Neural Networks (RVPINNs) highlights the importance of conveniently translating the norms of the underlying continuum-level spaces to the discrete level. Otherwise, VPINNs might become unrobust, implying that residual minimization might be highly uncorrelated with a desired minimization of the error in the energy norm. However, applying this robustness to VPINNs typically entails dealing with the inverse of a Gram matrix, usually producing slow convergence speeds during training. In this work, we accelerate the implementation of RVPINN, establishing a LU factorization of sparse Gram matrix in a kind of point-collocation scheme with the same spirit as original PINNs. We call out method the Collocation-based Robust Variational Physics Informed Neural Networks (CRVPINN). We test our efficient CRVPINN algorithm on Laplace, advection-diffusion, and Stokes problems in two spatial dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02300v3</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcin {\L}o\'s, Tomasz S{\l}u\.zalec, Pawe{\l} Maczuga, Askold Vilkha, Carlos Uriarte, Maciej Paszy\'nski</dc:creator>
    </item>
    <item>
      <title>Mechanistic interpretability of large language models with applications to the financial services industry</title>
      <link>https://arxiv.org/abs/2407.11215</link>
      <description>arXiv:2407.11215v2 Announce Type: replace-cross 
Abstract: Large Language Models such as GPTs (Generative Pre-trained Transformers) exhibit remarkable capabilities across a broad spectrum of applications. Nevertheless, due to their intrinsic complexity, these models present substantial challenges in interpreting their internal decision-making processes. This lack of transparency poses critical challenges when it comes to their adaptation by financial institutions, where concerns and accountability regarding bias, fairness, and reliability are of paramount importance. Mechanistic interpretability aims at reverse engineering complex AI models such as transformers. In this paper, we are pioneering the use of mechanistic interpretability to shed some light on the inner workings of large language models for use in financial services applications. We offer several examples of how algorithmic tasks can be designed for compliance monitoring purposes. In particular, we investigate GPT-2 Small's attention pattern when prompted to identify potential violation of Fair Lending laws. Using direct logit attribution, we study the contributions of each layer and its corresponding attention heads to the logit difference in the residual stream. Finally, we design clean and corrupted prompts and use activation patching as a causal intervention method to localize our task completion components further. We observe that the (positive) heads $10.2$ (head $2$, layer $10$), $10.7$, and $11.3$, as well as the (negative) heads $9.6$ and $10.6$ play a significant role in the task completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11215v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3677052.3698612</arxiv:DOI>
      <arxiv:journal_reference>5th ACM International Conference on AI in Finance (ICAIF 2024)</arxiv:journal_reference>
      <dc:creator>Ashkan Golgoon, Khashayar Filom, Arjun Ravi Kannan</dc:creator>
    </item>
  </channel>
</rss>
