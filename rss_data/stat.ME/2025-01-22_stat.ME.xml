<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 22 Jan 2025 05:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Median of Means Sampling for the Keister Function</title>
      <link>https://arxiv.org/abs/2501.10440</link>
      <description>arXiv:2501.10440v1 Announce Type: new 
Abstract: This study investigates the performance of median-of-means sampling compared to traditional mean-of-means sampling for computing the Keister function integral using Randomized Quasi-Monte Carlo (RQMC) methods. The research tests both lattice points and digital nets as point distributions across dimensions 2, 3, 5, and 8, with sample sizes ranging from 2^8 to 2^19 points. Results demonstrate that median-of-means sampling consistently outperforms mean-of-means for sample sizes larger than 10^3 points, while mean-of-means shows better accuracy with smaller sample sizes, particularly for digital nets. The study also confirms previous theoretical predictions about median-of-means' superior performance with larger sample sizes and reflects the known challenges of maintaining accuracy in higher-dimensional integration. These findings support recent research suggesting median-of-means as a promising alternative to traditional sampling methods in numerical integration, though limitations in sample size and dimensionality warrant further investigation with different test functions and larger parameter spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10440v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bocheng Zhang</dc:creator>
    </item>
    <item>
      <title>Softplus and Neural Architectures for Enhanced Negative Binomial INGARCH Modeling</title>
      <link>https://arxiv.org/abs/2501.10655</link>
      <description>arXiv:2501.10655v1 Announce Type: new 
Abstract: The study addresses a significant gap in the literature by introducing the Softplus negative binomial Integer-valued Generalized Autoregressive Conditional Heteroskedasticity (sp NB- INGARCH) model and establishing its stationarity properties, alongside methodology for parameter estimation. Building upon this foundation, the Neural negative binomial INGARCH (neu - NB-INGARCH) model is proposed, designed to enhance predictive accuracy while accommodating moderate non-stationarity in count time series data. A simulation study and data analysis demonstrate the efficacy of the sp NB-INGARCH model, while the practical utility of the neu - NB - INGARCH model is showcased through a comprehensive analysis of a healthcare data. Additionally, a thorough literature review is presented, focusing on the application of neural networks in time series modeling, with particular emphasis on count time series. In short, this work contributes to advancing the theoretical understanding and practical application of neural network-based models in count time series forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10655v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Divya Kuttenchalil Andrews, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>Clustering the Nearest Neighbor Gaussian Process</title>
      <link>https://arxiv.org/abs/2501.10656</link>
      <description>arXiv:2501.10656v1 Announce Type: new 
Abstract: Gaussian processes are ubiquitous as the primary tool for modeling spatial data. However, the Gaussian process is limited by its $\mathcal{O}(n^3)$ cost, making direct parameter fitting algorithms infeasible for the scale of modern data collection initiatives. The Nearest Neighbor Gaussian Process (NNGP) was introduced as a scalable approximation to dense Gaussian processes which has been successful for $n\sim 10^6$ observations. This project introduces the $\textit{clustered Nearest Neighbor Gaussian Process}$ (cNNGP) which reduces the computational and storage cost of the NNGP. The accuracy of parameter estimation and reduction in computational and memory storage requirements are demonstrated with simulated data, where the cNNGP provided comparable inference to that obtained with the NNGP, in a fraction of the sampling time. To showcase the method's performance, we modeled biomass over the state of Maine using data collected by the Global Ecosystem Dynamics Investigation (GEDI) to generate wall-to-wall predictions over the state. In 16% of the time, the cNNGP produced nearly indistinguishable inference and biomass prediction maps to those obtained with the NNGP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10656v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ashlynn Crisp, Daniel Taylor-Rodriguez, Andrew O. Finley</dc:creator>
    </item>
    <item>
      <title>Robust Local Polynomial Regression with Similarity Kernels</title>
      <link>https://arxiv.org/abs/2501.10729</link>
      <description>arXiv:2501.10729v1 Announce Type: new 
Abstract: Local Polynomial Regression (LPR) is a widely used nonparametric method for modeling complex relationships due to its flexibility and simplicity. It estimates a regression function by fitting low-degree polynomials to localized subsets of the data, weighted by proximity. However, traditional LPR is sensitive to outliers and high-leverage points, which can significantly affect estimation accuracy. This paper revisits the kernel function used to compute regression weights and proposes a novel framework that incorporates both predictor and response variables in the weighting mechanism. By introducing two positive definite kernels, the proposed method robustly estimates weights, mitigating the influence of outliers through localized density estimation. The method is implemented in Python and is publicly available at https://github.com/yaniv-shulman/rsklpr, demonstrating competitive performance in synthetic benchmark experiments. Compared to standard LPR, the proposed approach consistently improves robustness and accuracy, especially in heteroscedastic and noisy environments, without requiring multiple iterations. This advancement provides a promising extension to traditional LPR, opening new possibilities for robust regression applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10729v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yaniv Shulman</dc:creator>
    </item>
    <item>
      <title>The Building Blocks of Classical Nonparametric Two-Sample Testing Procedures: Statistically Equivalent Blocks</title>
      <link>https://arxiv.org/abs/2501.10844</link>
      <description>arXiv:2501.10844v1 Announce Type: new 
Abstract: Statistically equivalent blocks are not frequently considered in the context of nonparametric two-sample hypothesis testing. Despite the limited exposure, this paper shows that a number of classical nonparametric hypothesis tests can be derived on the basis of statistically equivalent blocks and their frequencies. Far from a moot historical point, this allows for a more unified approach in considering the many two-sample nonparametric tests based on ranks, signs, placements, order statistics, and runs. Perhaps more importantly, this approach also allows for the easy extension of many univariate nonparametric tests into arbitrarily high dimensions that retain all null properties regardless of dimensionality and are invariant to the scaling of the observations. These generalizations do not require depth functions or the explicit use of spatial signs or ranks and may be of use in various areas such as life-testing and quality control. In the manuscript, an overview of statistically equivalent blocks and tests based on these blocks are provided. This is followed by reformulations of some popular univariate tests and generalizations to higher dimensions. Comments comparing proposed methods to those based on spatial signs and ranks are offered along with some conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10844v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chase Holcombe</dc:creator>
    </item>
    <item>
      <title>An Online Algorithm for Bayesian Variable Selection in Logistic Regression Models With Streaming Data</title>
      <link>https://arxiv.org/abs/2501.10930</link>
      <description>arXiv:2501.10930v1 Announce Type: new 
Abstract: In several modern applications, data are generated continuously over time, such as data generated from smartwatches. We assume data are collected and analyzed sequentially, in batches. Since traditional or offline methods can be extremely slow, Ghosh et al. (2025) proposed an online method for Bayesian model averaging (BMA). Inspired by the literature on renewable estimation, they developed an online Bayesian method for generalized linear models (GLMs) that reduces storage and computational demands dramatically compared to traditional methods for BMA. The method of Ghosh et al. (2025) works very well when the number of models is small. It can also work reasonably well in moderately large model spaces. For the latter case, the method relies on a screening stage to identify important models in the first several batches via offline methods. Thereafter, the model space remains fixed in all subsequent batches. In the post-screening stage, online updates are made to the model specific parameters, for models selected in the screening stage. For high-dimensional model spaces, the chance of missing important models in the screening stage is more likely. This necessitates the development of a method, which permits the model space to be updated as new batches of data arrive. In this article, we develop an online Bayesian model selection method for logistic regression, where the selected model can potentially change throughout the data collection process. We use simulation studies to show that our new method can outperform the method of Ghosh et al. (2025). Furthermore, we describe scenarios under which the gain from our new method is expected to be small. We revisit the traffic crash data analyzed by Ghosh et al. (2025) and illustrate that our new model selection method can have better performance for variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10930v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Payel Ghosal, Shamriddha De, Joyee Ghosh</dc:creator>
    </item>
    <item>
      <title>Large covariance matrix estimation with factor-assisted variable clustering</title>
      <link>https://arxiv.org/abs/2501.10942</link>
      <description>arXiv:2501.10942v1 Announce Type: new 
Abstract: This paper studies the covariance matrix estimation for high-dimensional time series within a new framework that combines low-rank factor and latent variable-specific cluster structures. The popular methods based on assuming the sparse error covariance matrix after taking out common factors may be invalid for many financial applications. Our formulation postulates a latent model-based error cluster structure after removing observable factors, which not only leads to more interpretable cluster patterns but also accounts for non-sparse cross-sectional correlations among the variable-specific residuals. Our method begins with using least-squares to estimate the factor loadings, followed by identifying the latent cluster structure by thresholding the scaled covariance difference measures of residuals. A novel ratio-based criterion is introduced to determine the threshold parameter when performing the developed clustering algorithm. We then establish the cluster recovery consistency of our method and derive the convergence rates of our proposed covariance matrix estimators under different norms. Finally, we demonstrate the superior finite sample performance of our proposal over the competing methods through both extensive simulations and a real data application on minimum variance portfolio.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10942v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dong Li, Xinghao Qiao, Cheng Yu</dc:creator>
    </item>
    <item>
      <title>Robust Functional Ward's Linkages with Applications in EEG data Clustering</title>
      <link>https://arxiv.org/abs/2501.11081</link>
      <description>arXiv:2501.11081v1 Announce Type: new 
Abstract: This paper proposes two new distance measures, called functional Ward's linkages, for functional data clustering that are robust against outliers. Conventional Ward's linkage defines the distance between two clusters as the increase in sum of squared errors (SSE) upon merging, which can be interpreted graphically as an increase in the diameter. Analogously, functional Ward's linkage defines the distance of two clusters as the increased width of the band delimited by the merged clusters. To address the limitations of conventional Ward's linkage in handling outliers and contamination, the proposed linkages focus exclusively on the most central curves by leveraging magnitude-shape outlyingness measures and modified band depth, respectively. Simulations and real-world electroencephalogram (EEG) data analysis demonstrate that the proposed methods outperform other competitive approaches, particularly in the presence of various types of outliers and contamination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11081v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianbo Chen</dc:creator>
    </item>
    <item>
      <title>Penalized generalized linear mixed models for longitudinal outcomes in genetic association studies</title>
      <link>https://arxiv.org/abs/2501.11083</link>
      <description>arXiv:2501.11083v1 Announce Type: new 
Abstract: This work is motivated by analyses of longitudinal data collected from participants in the Quebec Longitudinal Study of Child Development (QLSCD) and the Quebec Newborn Twin Study (QNTS) to identify important genetic predictors for emotional and behavioral difficulties in childhood and adolescence. We propose a lasso penalized mixed model for continuous and binary longitudinal traits that allows the inclusion of multiple random effects to account for random individual effects not attributable to the genetic similarity between individuals. Through simulation studies, we show that replacing the estimated genetic relatedness matrix (GRM) by a sparse matrix introduces bias in the variance components estimates, but that the obtained computational gain is major while the impact on the performance of the penalized model to retrieve important predictors is negligible. We compare the performance of the proposed penalized mixed model to a standard lasso and to a univariate mixed model association test and show that the proposed model always identifies causal predictors with greater precision. Finally, we show an application of the proposed methodology to predict three externalizing behavorial scores in the combined QLSCD and QNTS longitudinal cohorts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11083v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien St-Pierre, Sahir Rai Bhatnagar, Massimiliano Orri, Michel Boivin, Jos\'ee Dupuis, Karim Oualkacha</dc:creator>
    </item>
    <item>
      <title>Sample size and power calculation for propensity score analysis of observational studies</title>
      <link>https://arxiv.org/abs/2501.11181</link>
      <description>arXiv:2501.11181v1 Announce Type: new 
Abstract: This paper develops theoretically justified analytical formulas for sample size and power calculation in the propensity score analysis of causal inference using observational data. By analyzing the variance of the inverse probability weighting estimator of the average treatment effect (ATE), we clarify the three key components for sample size calculations: propensity score distribution, potential outcome distribution, and their correlation. We devise analytical procedures to identify these components based on commonly available and interpretable summary statistics. We elucidate the critical role of covariate overlap between treatment groups in determining the sample size. In particular, we propose to use the Bhattacharyya coefficient as a measure of covariate overlap, which, together with the treatment proportion, leads to a uniquely identifiable and easily computable propensity score distribution. The proposed method is applicable to both continuous and binary outcomes. We show that the standard two-sample $z$-test and variance inflation factor methods often lead to, sometimes vastly, inaccurate sample size estimates, especially with limited overlap. We also derive formulas for the average treatment effects for the treated (ATT) and overlapped population (ATO) estimands. We provide simulated and real examples to illustrate the proposed method. We develop an associated R package PSpower.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11181v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bo Liu, Xiaoxiao Zhou, Fan Li</dc:creator>
    </item>
    <item>
      <title>The Dynamical Behavior of Detected vs. Undetected Targets</title>
      <link>https://arxiv.org/abs/2501.11189</link>
      <description>arXiv:2501.11189v1 Announce Type: new 
Abstract: This paper is a sequel of the 2019 paper [5]. It demonstrates the following: a) the Poisson multi-Bernoulli mixture (PMBM) approach to detected vs. undetected (U/D) targets cannot be rigorously formulated using either the two-step or single-step multitarget recursive Bayes filter (MRBF); b) it can, however, be partially salvaged using a novel single-step MRBF; c) probability hypothesis density (PHD) filters can be derived for both the original "S-U/D" approach in [5] and the novel "D-U/D" approach; d) important U/D formulas in [5] can be verified using purely algebraic methods rather than the intricate statistical analysis employed in that paper; and e) the claim, that PMBM filters can propagate detected and undetected targets separately in parallel, is doubtful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11189v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Mahler</dc:creator>
    </item>
    <item>
      <title>Irregular measurement times in estimating time-varying treatment effects: Categorizing biases and comparing adjustment methods</title>
      <link>https://arxiv.org/abs/2501.11449</link>
      <description>arXiv:2501.11449v1 Announce Type: new 
Abstract: To estimate the causal effect of treatments that vary over time from observational data, one must adjust for time-varying confounding. A common procedure to address confounding is the use of inverse probability of treatment weighting methods. However, the timing of covariate measurements is often irregular, which may introduce additional confounding bias as well as selection bias into the causal effect estimate. Two reweighting methods have been proposed to adjust for these biases: time-as-confounder and reweighting by measurement time. However, it is currently not well understood in which situations these irregularly timed measurements induce bias, and how the available reweighting methods compare to each other in different situations. In this work, we provide a complete inventarization of all possible backdoor paths through which bias is induced. Based on these paths, we distinguish three categories of confounding bias by measurement time: direct confounding (DC), confounding through measured variables (CMV), and confounding through unmeasured variables (CUV). These categories differ in the assumptions and reweighting methods necessary to adjust for bias and may occur simultaneously with selection bias. Through simulation studies, we illustrate: 1. Reweighting by measurement time may be used to adjust for selection bias and confounding through measured variables; 2. Time-as-confounder may be used to adjust for all categories of confounding bias, but not selection bias; 3. In some cases, the use of a combination of both techniques may be used to adjust for both confounding and selection bias. We finally apply the categorization and reweighting methods on the pre-DIVA data set. Adjusting for measurement times is crucial in order to avoid bias, and the categorization of biases and techniques that we introduce may help researchers to choose the appropriate analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11449v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wouter M. R. Kant, Jesse H. Krijthe</dc:creator>
    </item>
    <item>
      <title>Precision of Treatment Hierarchy: A Metric for Quantifying Certainty in Treatment Hierarchies from Network Meta-Analysis</title>
      <link>https://arxiv.org/abs/2501.11596</link>
      <description>arXiv:2501.11596v1 Announce Type: new 
Abstract: Network meta-analysis (NMA) is an extension of pairwise meta-analysis which facilitates the estimation of relative effects for multiple competing treatments. A hierarchy of treatments is a useful output of an NMA. Treatment hierarchies are produced using ranking metrics. Common ranking metrics include the Surface Under the Cumulative RAnking curve (SUCRA) and P-scores, which are the frequentist analogue to SUCRAs. Both metrics consider the size and uncertainty of the estimated treatment effects, with larger values indicating a more preferred treatment. Although SUCRAs and P-scores themselves consider uncertainty, treatment hierarchies produced by these ranking metrics are typically reported without a measure of certainty, which might be misleading to practitioners. We propose a new metric, Precision of Treatment Hierarchy (POTH), which quantifies the certainty in producing a treatment hierarchy from SUCRAs or P-scores. The metric connects three statistical quantities: The variance of the SUCRA values, the variance of the mean rank of each treatment, and the average variance of the distribution of individual ranks for each treatment. POTH provides a single, interpretable value which quantifies the degree of certainty in producing a treatment hierarchy. We show how the metric can be adapted to apply to subsets of treatments in a network, for example, to quantify the certainty in the hierarchy of the top three treatments. We calculate POTH for a database of NMAs to investigate its empirical properties, and we demonstrate its use on three published networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11596v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Augustine Wigle (University of Waterloo), Audrey B\'eliveau (University of Waterloo), Georgia Salanti (University of Bern), Gerta R\"ucker (University of Freiburg), Guido Schwarzer (University of Freiburg), Dimitris Mavridis (University of Ioannina), Adriani Nikolakopoulou (Aristotle University of Thessaloniki, University of Freiburg)</dc:creator>
    </item>
    <item>
      <title>Risk-Adjusted learning curve assessment using comparative probability metrics</title>
      <link>https://arxiv.org/abs/2501.11637</link>
      <description>arXiv:2501.11637v1 Announce Type: new 
Abstract: Surgical learning curves are graphical tools used to evaluate a trainee's progress in the early stages of their career and determine whether they have achieved proficiency after completing a specified number of surgeries. Cumulative sum (CUSUM) techniques are commonly used to assess learning curves due to their simplicity, but they face criticism for relying on fixed performance thresholds and lacking interpretability. This paper introduces a risk-adjusted surgical learning curve assessment (SLCA) method that focuses on estimation rather than hypothesis testing, as seen in CUSUM methods. The method is designed to accommodate right-skewed outcomes, such as surgery durations, characterized by the Weibull distribution. To evaluate the learning process, the SLCA approach estimates comparative probability metrics that assess the likelihood of a clinically important difference between the trainee's performance and a standard. Expecting improvement over time, we use weighted estimating equations to give greater weight to recent outcomes. Compared to CUSUM methods, SLCA offers enhanced interpretability, avoids reliance on externally defined performance levels, and emphasizes assessing clinical equivalence or noninferiority. We demonstrate the method's effectiveness through a colorectal surgery dataset case study and a numerical study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11637v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Ahmadi Nadi, Stefan Steiner, Nathaniel Stevens</dc:creator>
    </item>
    <item>
      <title>Investigating the performance of the Phase II Hotelling T2 chart when monitoring multivariate time series observations</title>
      <link>https://arxiv.org/abs/2501.11649</link>
      <description>arXiv:2501.11649v1 Announce Type: new 
Abstract: Thanks to high-tech measurement systems like sensors, data are often collected with high frequency in modern industrial processes. This phenomenon could potentially produce autocorrelated and cross-correlated measurements. It has been shown that if this issue is not properly accounted for while designing the control charts, many false alarms may be observed, disrupting the monitoring process efficiency. There are generally two recommended ways to monitor autocorrelated data: fitting a time series model and then monitoring the residuals or directly monitoring the original observations. Although residual charts are popular in the literature because they offer advantages such as ease of implementation, questions have been raised about their efficiency due to the loss of information. This paper develops the methodology for applying Hotelling's T2 chart directly to monitoring multivariate autocorrelated and cross-correlated observations. To model such data, we use the multivariate vector autoregressive time series model of order p &gt;= 1, denoted as VAR(p). We compare the performance of the T2 chart based on the original observations and the residual-based T2 chart using the well-known metric average run length and a newly introduced criterion called first-to-signal. The results indicate that the proposed method consistently outperforms the alternative chart. We also illustrate the proposed method using two examples: one from a steel sheet rolling process (VAR(1) observations) and another from a chemical process (VAR(3) observations).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11649v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adel Ahmadi Nadi, Giovanni Celano, Stefan Steiner</dc:creator>
    </item>
    <item>
      <title>Exact Bounds of Spearman's footrule in the Presence of Missing Data with Applications to Independence Testing</title>
      <link>https://arxiv.org/abs/2501.11696</link>
      <description>arXiv:2501.11696v1 Announce Type: new 
Abstract: This work studies exact bounds of Spearman's footrule between two partially observed $n$-dimensional distinct real-valued vectors $X$ and $Y$. The lower bound is obtained by sequentially constructing imputations of the partially observed vectors, each with a non-increasing value of Spearman's footrule. The upper bound is found by first considering the set of all possible values of Spearman's footrule for imputations of $X$ and $Y$, and then the size of this set is gradually reduced using several constraints. Algorithms with computational complexities $O(n^2)$ and $O(n^3)$ are provided for computing the lower and upper bound of Spearman's footrule for $X$ and $Y$, respectively. As an application of the bounds, we propose a novel two-sample independence testing method for data with missing values. Improving on all existing approaches, our method controls the Type I error under arbitrary missingness. Simulation results demonstrate our method has good power, typically when the proportion of pairs containing missing data is below $15\%$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11696v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yijin Zeng, Niall M. Adams, Dean A. Bodenham</dc:creator>
    </item>
    <item>
      <title>A new class of non-stationary Gaussian fields with general smoothness on metric graphs</title>
      <link>https://arxiv.org/abs/2501.11738</link>
      <description>arXiv:2501.11738v1 Announce Type: new 
Abstract: The increasing availability of network data has driven the development of advanced statistical models specifically designed for metric graphs, where Gaussian processes play a pivotal role. While models such as Whittle-Mat\'ern fields have been introduced, there remains a lack of practically applicable options that accommodate flexible non-stationary covariance structures or general smoothness. To address this gap, we propose a novel class of generalized Whittle-Mat\'ern fields, which are rigorously defined on general compact metric graphs and permit both non-stationarity and arbitrary smoothness. We establish new regularity results for these fields, which extend even to the standard Whittle-Mat\'ern case. Furthermore, we introduce a method to approximate the covariance operator of these processes by combining the finite element method with a rational approximation of the operator's fractional power, enabling computationally efficient Bayesian inference for large datasets. Theoretical guarantees are provided by deriving explicit convergence rates for the covariance approximation error, and the practical utility of our approach is demonstrated through simulation studies and an application to traffic speed data, highlighting the flexibility and effectiveness of the proposed model class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11738v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Bolin, Lenin Riera-Segura, Alexandre B. Simas</dc:creator>
    </item>
    <item>
      <title>Beyond fixed thresholds: optimizing summaries of wearable device data via piecewise linearization of quantile functions</title>
      <link>https://arxiv.org/abs/2501.11777</link>
      <description>arXiv:2501.11777v1 Announce Type: new 
Abstract: Wearable devices, such as actigraphy monitors and continuous glucose monitors (CGMs), capture high-frequency data, which are often summarized by the percentages of time spent within fixed thresholds. For example, actigraphy data are categorized into sedentary, light, and moderate-to-vigorous activity, while CGM data are divided into hypoglycemia, normoglycemia, and hyperglycemia based on a standard glucose range of $70\unicode{x2013}180$ mg/dL. Although scientific and clinical guidelines inform the choice of thresholds, it remains unclear whether this choice is optimal and whether the same thresholds should be applied across different populations. In this work, we define threshold optimality with loss functions that quantify discrepancies between the full empirical distributions of wearable device measurements and their discretizations based on specific thresholds. We introduce two loss functions: one that aims to accurately reconstruct the original distributions and another that preserves the pairwise sample distances. Using the Wasserstein distance as the base measure, we reformulate the loss minimization as optimal piecewise linearization of quantile functions. We solve this optimization via stepwise algorithms and differential evolution. We also formulate semi-supervised approaches where some thresholds are predefined based on scientific rationale. Applications to CGM data from two distinct populations$\unicode{x2014}$individuals with type 1 diabetes and those with normal glycemic control$\unicode{x2014}$demonstrate that data-driven thresholds vary by population and improve discriminative power over fixed thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11777v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junyoung Park, Neo Kok, Irina Gaynanova</dc:creator>
    </item>
    <item>
      <title>Envelope-Guided Regularization for Improved Prediction in High-Dimensional Multivariate Regression</title>
      <link>https://arxiv.org/abs/2501.11791</link>
      <description>arXiv:2501.11791v1 Announce Type: new 
Abstract: Envelope methods perform dimension reduction of predictors or responses in multivariate regression, exploiting the relationship between them to improve estimation efficiency. While most research on envelopes has focused on their estimation properties, certain envelope estimators have been shown to excel at prediction in both low and high dimensions. In this paper, we propose to further improve prediction through envelope-guided regularization (EgReg), a novel method which uses envelope-derived information to guide shrinkage along the principal components (PCs) of the predictor matrix. We situate EgReg among other PC-based regression methods and envelope methods to motivate its development. We show that EgReg delivers lower prediction risk than a closely related non-shrinkage envelope estimator when the number of predictors $p$ and observations $n$ are fixed and in any alignment. In an asymptotic regime where the true intrinsic dimension of the predictors and $n$ diverge proportionally, we find that the limiting prediction risk of the non-shrinkage envelope estimator exhibits a double descent phenomenon and is consistently larger than the limiting risk for EgReg. We compare the prediction performance of EgReg with envelope methods and other PC-based prediction methods in simulations and a real data example, observing improved prediction performance over these alternative approaches in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11791v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tate Jacobson, Oh-Ran Kwon</dc:creator>
    </item>
    <item>
      <title>Paradise of Forking Paths: Revisiting the Adaptive Data Analysis Problem</title>
      <link>https://arxiv.org/abs/2501.11804</link>
      <description>arXiv:2501.11804v1 Announce Type: new 
Abstract: The Adaptive Data Analysis (ADA) problem, where an analyst interacts with a dataset through statistical queries, is often studied under the assumption of adversarial analyst behavior. To decrease this gap, we propose a revised model of ADA that accounts for more constructive interactions between the analysts and the data, where the goal is to enhance inference accuracy. Specifically, we focus on distribution estimation as a central objective guiding analyst's queries. The problem is addressed within a non-parametric Bayesian framework, capturing the flexibility and dynamic evolution of analyst's beliefs. Our hierarchical approach leverages P\'olya trees (PTs) as priors over the distribution space, facilitating the adaptive selection of counting queries to efficiently reduce the estimation error without increasing the number of queries. Furthermore, with its interpretability and conjugacy, the proposed framework allows for intuitive conversion of subjective beliefs into objective priors and their effortless updates to posteriors. Using theoretical derivations, we formalize the PT-based solution as a computational algorithm. Simulations further demonstrate its effectiveness in distribution estimation tasks compared to the non-adaptive approach. By aligning with real-world applications, this structured ADA framework fosters opportunities for collaborative research in related areas, such as human-in-the-loop systems and cognitive studies of belief updating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11804v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Amir Hossein Hadavi, Mohammad M. Mojahedian, Mohammad Reza Aref</dc:creator>
    </item>
    <item>
      <title>Multiple change point detection based on Hodrick-Prescott and $l_1$ filtering method for random walk time series data</title>
      <link>https://arxiv.org/abs/2501.11805</link>
      <description>arXiv:2501.11805v1 Announce Type: new 
Abstract: We propose new methods for detecting multiple change points in time series, specifically designed for random walk processes, where stationarity and variance changes present challenges. Our approach combines two trend estimation methods: the Hodrick Prescott (HP) filter and the l1 filter. A major challenge in these methods is selecting the tuning parameter lambda, which we address by introducing two selection techniques. For the HP based change point detection, we propose a probability-based threshold to select lambda under the assumption of an exponential distribution. For the l1 based method, we suggest a selection strategy assuming normality. Additionally, we introduce a technique to estimate the maximum number of change points in time segments using the l1 based method. We validate our methods by comparing them to similar techniques, such as PELT, using simulated data. We also demonstrate the practical application of our approach to real-world SNP stock data, showcasing its effectiveness in detecting change points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11805v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiyuan Liu</dc:creator>
    </item>
    <item>
      <title>Dynamic Risk-Adjusted Monitoring of Time Between Events: Applications of NHPP in Pipeline Accident Surveillance</title>
      <link>https://arxiv.org/abs/2501.11809</link>
      <description>arXiv:2501.11809v1 Announce Type: new 
Abstract: Monitoring time between events (TBE) is a critical task in industrial settings. Traditional Statistical Process Monitoring (SPM) methods often assume that TBE variables follow an exponential distribution, which implies a constant failure intensity. While this assumption may hold for products with homogeneous quality, it is less appropriate for complex systems, such as repairable systems, where failure mechanisms evolve over time due to degradation or aging. In such cases, the Non-Homogeneous Poisson Process (NHPP), which accommodates time-varying failure intensity, is a more suitable model. Furthermore, failure patterns in complex systems are frequently influenced by risk factors, including environmental conditions and human interventions, and system failures often incur restoration costs. This work introduces a novel approach: a risk-adjusted control chart based on the NHPP model, specifically designed to monitor the ratio of cost to TBE, referred to as the average cost per time unit (AC). The proposed method is evaluated through extensive simulations, demonstrating its superior performance. Additionally, the chart is applied to monitor pipeline accidents over time, accounting for the impact of various risk factors. These results highlight the effectiveness of the developed chart in enhancing monitoring capabilities for complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11809v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hussam Ahmad, Adel Ahmadi Nadi, Mohammad Amini, Subhabrata Chakraborti</dc:creator>
    </item>
    <item>
      <title>Automatic Debiased Machine Learning for Smooth Functionals of Nonparametric M-Estimands</title>
      <link>https://arxiv.org/abs/2501.11868</link>
      <description>arXiv:2501.11868v1 Announce Type: new 
Abstract: We propose a unified framework for automatic debiased machine learning (autoDML) to perform inference on smooth functionals of infinite-dimensional M-estimands, defined as population risk minimizers over Hilbert spaces. By automating debiased estimation and inference procedures in causal inference and semiparametric statistics, our framework enables practitioners to construct valid estimators for complex parameters without requiring specialized expertise. The framework supports Neyman-orthogonal loss functions with unknown nuisance parameters requiring data-driven estimation, as well as vector-valued M-estimands involving simultaneous loss minimization across multiple Hilbert space models. We formalize the class of parameters efficiently estimable by autoDML as a novel class of nonparametric projection parameters, defined via orthogonal minimum loss objectives. We introduce three autoDML estimators based on one-step estimation, targeted minimum loss-based estimation, and the method of sieves. For data-driven model selection, we derive a novel decomposition of model approximation error for smooth functionals of M-estimands and propose adaptive debiased machine learning estimators that are superefficient and adaptive to the functional form of the M-estimand. Finally, we illustrate the flexibility of our framework by constructing autoDML estimators for the long-term survival under a beta-geometric model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11868v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Aurelien Bibaut, Nathan Kallus, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Bias Analysis of Experiments for Multi-Item Multi-Period Inventory Control Policies</title>
      <link>https://arxiv.org/abs/2501.11996</link>
      <description>arXiv:2501.11996v1 Announce Type: new 
Abstract: Randomized experiments, or A/B testing, are the gold standard for evaluating interventions but are underutilized in the area of inventory management. This study addresses this gap by analyzing A/B testing strategies in multi-item, multi-period inventory systems with lost sales and capacity constraints. We examine switchback experiments, item-level randomization, pairwise randomization, and staggered rollouts, analyzing their biases theoretically and comparing them through numerical experiments. Our findings provide actionable guidance for selecting experimental designs across various contexts in inventory management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11996v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinqi Chen, Xingyu Bai, Zeyu Zheng, Nian Si</dc:creator>
    </item>
    <item>
      <title>Enough?</title>
      <link>https://arxiv.org/abs/2501.12161</link>
      <description>arXiv:2501.12161v1 Announce Type: new 
Abstract: We respond to Aronow et al. (2025)'s paper arguing that randomized controlled trials (RCTs) are "enough," while nonparametric identification in observational studies is not. We agree with their position with respect to experimental versus observational research, but question what it would mean to extend this logic to the scientific enterprise more broadly. We first investigate what is meant by "enough," arguing that this is fundamentally a sociological claim about the relationship between statistical work and larger social and institutional processes, rather than something that can be decided from within the logic of statistics. For a more complete conception of "enough," we outline all that would need to be known -- not just knowledge of propensity scores, but knowledge of many other spatial and temporal characteristics of the social world. Even granting the logic of the critique in Aronow et al. (2025), its practical importance is a question of the contexts under study. We argue that we should not be satisfied by appeals to intuition about the complexity of "naturally occurring" propensity score functions. Instead, we call for more empirical metascience to begin to characterize this complexity. We apply this logic to the example of recommender systems developed by Aronow et al. (2025) as a demonstration of the weakness of allowing statisticians' intuitions to serve in place of metascientific data. Rather than implicitly deciding what is "enough" based on statistical applications the social world has determined to be most profitable, we argue that practicing statisticians should explicitly engage with questions like "for what?" and "for whom?" in order to adequately answer the question of "enough?"</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12161v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Drew Dimmery, Kevin Munger</dc:creator>
    </item>
    <item>
      <title>Optimizing a multi-state cold-standby system with multiple vacations in the repair and loss of units</title>
      <link>https://arxiv.org/abs/2501.10446</link>
      <description>arXiv:2501.10446v1 Announce Type: cross 
Abstract: A complex multi-state redundant system with preventive maintenance subject to multiple events is considered. The online unit can undergo several types of failures: internal and those provoked by external shocks. Multiple degradation levels are assumed so as internal and external. Degradation levels are observed by random inspections and if they are major, the unit goes to repair facility where preventive maintenance is carried out. This repair facility is composed of a single repairperson governed by a multiple vacation policy. This policy is set up according to the operational number of units. Two types of task can be performed by the repairperson, corrective repair and preventive maintenance. The times embedded in the system are phase type distributed and the model is built by using Markovian Arrival Processes with marked arrivals. Multiple performance measures besides of the transient and stationary distribution are worked out through matrix-analytic methods. This methodology enables us to express the main results and the global development in a matrix-algorithmic form. To optimize the model costs and rewards are included. A numerical example shows the versatility of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10446v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3390/math9080913</arxiv:DOI>
      <arxiv:journal_reference>Mathematics 2021, 9(8), 913</arxiv:journal_reference>
      <dc:creator>Juan Eloy Ruiz-Castro</dc:creator>
    </item>
    <item>
      <title>Crossing penalised CAViaR</title>
      <link>https://arxiv.org/abs/2501.10564</link>
      <description>arXiv:2501.10564v1 Announce Type: cross 
Abstract: Dynamic quantiles, or Conditional Autoregressive Value at Risk (CAViaR) models, have been extensively studied at the individual level. However, efforts to estimate multiple dynamic quantiles jointly have been limited. Existing approaches either sequentially estimate fitted quantiles or impose restrictive assumptions on the data generating process. This paper fills this gap by proposing an objective function for the joint estimation of all quantiles, introducing a crossing penalty to guide the process. Monte Carlo experiments and an empirical application on the FTSE100 validate the effectiveness of the method, offering a flexible and robust approach to modelling multiple dynamic quantiles in time-series data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10564v1</guid>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tibor Szendrei</dc:creator>
    </item>
    <item>
      <title>A new Monte Carlo method for valid prior-free possibilistic statistical inference</title>
      <link>https://arxiv.org/abs/2501.10585</link>
      <description>arXiv:2501.10585v1 Announce Type: cross 
Abstract: Inferential models (IMs) offer prior-free, Bayesian-like, posterior degrees of belief designed for statistical inference, which feature a frequentist-like calibration property that ensures reliability of said inferences. The catch is that IMs' degrees of belief are possibilistic rather than probabilistic and, since the familiar Monte Carlo methods approximate probabilistic quantities, there are computational challenges associated with putting the IM framework into practice. The present paper addresses this shortcoming by developing a new Monte Carlo-based tool designed specifically to approximate the IM's possibilistic output. The proposal is based on a characterization of the possibilistic IM's credal set, which identifies the "best probabilistic approximation" of the IM as a mixture distribution that can be readily approximated and sampled from; these samples can then be transformed into a possibilistic approximation of the IM. Numerical results are presented highlighting the proposed approximation's accuracy and computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10585v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Assessing Markov Property in Driving Behaviors: Insights from Statistical Tests</title>
      <link>https://arxiv.org/abs/2501.10625</link>
      <description>arXiv:2501.10625v1 Announce Type: cross 
Abstract: The Markov property serves as a foundational assumption in most existing work on vehicle driving behavior, positing that future states depend solely on the current state, not the series of preceding states. This study validates the Markov properties of vehicle trajectories for both Autonomous Vehicles (AVs) and Human-driven Vehicles (HVs). A statistical method used to test whether time series data exhibits Markov properties is applied to examine whether the trajectory data possesses Markov characteristics. t test and F test are additionally introduced to characterize the differences in Markov properties between AVs and HVs. Based on two public trajectory datasets, we investigate the presence and order of the Markov property of different types of vehicles through rigorous statistical tests. Our findings reveal that AV trajectories generally exhibit stronger Markov properties compared to HV trajectories, with a higher percentage conforming to the Markov property and lower Markov orders. In contrast, HV trajectories display greater variability and heterogeneity in decision-making processes, reflecting the complex perception and information processing involved in human driving. These results have significant implications for the development of driving behavior models, AV controllers, and traffic simulation systems. Our study also demonstrates the feasibility of using statistical methods to test the presence of Markov properties in driving trajectory data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10625v1</guid>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zheng Li, Haoming Meng, Chengyuan Ma, Ke Ma, Xiaopeng Li</dc:creator>
    </item>
    <item>
      <title>Recovering Unobserved Network Links from Aggregated Relational Data: Discussions on Bayesian Latent Surface Modeling and Penalized Regression</title>
      <link>https://arxiv.org/abs/2501.10675</link>
      <description>arXiv:2501.10675v1 Announce Type: cross 
Abstract: Accurate network data are essential in fields such as economics, sociology, and computer science. Aggregated Relational Data (ARD) provides a way to capture network structures using partial data. This article compares two main frameworks for recovering network links from ARD: Bayesian Latent Surface Modeling (BLSM) and Frequentist Penalized Regression (FPR). Using simulation studies and real-world applications, we evaluate their theoretical properties, computational efficiency, and practical utility in domains like financial risk assessment and epidemiology. Key findings emphasize the importance of trait design, privacy considerations, and hybrid modeling approaches to improve scalability and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10675v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yen-hsuan Tseng</dc:creator>
    </item>
    <item>
      <title>Estimation of Linear models from Coarsened Observations Estimation of Linear models Estimation from Coarsened Observations A Method of Moments Approach</title>
      <link>https://arxiv.org/abs/2501.10726</link>
      <description>arXiv:2501.10726v1 Announce Type: cross 
Abstract: In the last few decades, the study of ordinal data in which the variable of interest is not exactly observed but only known to be in a specific ordinal category has become important. In Psychometrics such variables are analysed under the heading of item response models (IRM). In Econometrics, subjective well-being (SWB) and self-assessed health (SAH) studies, and in marketing research, Ordered Probit, Ordered Logit, and Interval Regression models are common research platforms. To emphasize that the problem is not specific to a specific discipline we will use the neutral term coarsened observation. For single-equation models estimation of the latent linear model by Maximum Likelihood (ML) is routine. But, for higher -dimensional multivariate models it is computationally cumbersome as estimation requires the evaluation of multivariate normal distribution functions on a large scale. Our proposed alternative estimation method, based on the Generalized Method of Moments (GMM), circumvents this multivariate integration problem. The method is based on the assumed zero correlations between explanatory variables and generalized residuals. This is more general than ML but coincides with ML if the error distribution is multivariate normal. It can be implemented by repeated application of standard techniques. GMM provides a simpler and faster approach than the usual ML approach. It is applicable to multiple -equation models with -dimensional error correlation matrices and response categories for the equation. It also yields a simple method to estimate polyserial and polychoric correlations. Comparison of our method with the outcomes of the Stata ML procedure cmp yields estimates that are not statistically different, while estimation by our method requires only a fraction of the computing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10726v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bernard M. S. van Praag, J. Peter Hop, William H. Greene</dc:creator>
    </item>
    <item>
      <title>A Multi-fidelity Estimator of the Expected Information Gain for Bayesian Optimal Experimental Design</title>
      <link>https://arxiv.org/abs/2501.10845</link>
      <description>arXiv:2501.10845v1 Announce Type: cross 
Abstract: Optimal experimental design (OED) is a framework that leverages a mathematical model of the experiment to identify optimal conditions for conducting the experiment. Under a Bayesian approach, the design objective function is typically chosen to be the expected information gain (EIG). However, EIG is intractable for nonlinear models and must be estimated numerically. Estimating the EIG generally entails some variant of Monte Carlo sampling, requiring repeated data model and likelihood evaluations $\unicode{x2013}$ each involving solving the governing equations of the experimental physics $\unicode{x2013}$ under different sample realizations. This computation becomes impractical for high-fidelity models.
  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the approximate control variate (ACV) framework. This estimator is unbiased with respect to the high-fidelity mean, and minimizes variance under a given computational budget. We achieve this by first reparameterizing the EIG so that its expectations are independent of the data models, a requirement for compatibility with ACV. We then provide specific examples under different data model forms, as well as practical enhancements of sample size optimization and sample reuse techniques. We demonstrate the MF-EIG estimator in two numerical examples: a nonlinear benchmark and a turbulent flow problem involving the calibration of shear-stress transport turbulence closure model parameters within the Reynolds-averaged Navier-Stokes model. We validate the estimator's unbiasedness and observe one- to two-orders-of-magnitude variance reduction compared to existing single-fidelity EIG estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10845v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas E. Coons, Xun Huan</dc:creator>
    </item>
    <item>
      <title>Linear scaling causal discovery from high-dimensional time series by dynamical community detection</title>
      <link>https://arxiv.org/abs/2501.10886</link>
      <description>arXiv:2501.10886v1 Announce Type: cross 
Abstract: Understanding which parts of a dynamical system cause each other is extremely relevant in fundamental and applied sciences. However, inferring causal links from observational data, namely without direct manipulations of the system, is still computationally challenging, especially if the data are high-dimensional. In this study we introduce a framework for constructing causal graphs from high-dimensional time series, whose computational cost scales linearly with the number of variables. The approach is based on the automatic identification of dynamical communities, groups of variables which mutually influence each other and can therefore be described as a single node in a causal graph. These communities are efficiently identified by optimizing the Information Imbalance, a statistical quantity that assigns a weight to each putative causal variable based on its information content relative to a target variable. The communities are then ordered starting from the fully autonomous ones, whose evolution is independent from all the others, to those that are progressively dependent on other communities, building in this manner a community causal graph. We demonstrate the computational efficiency and the accuracy of our approach on time-discrete and time-continuous dynamical systems including up to 80 variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10886v1</guid>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matteo Allione, Vittorio Del Tatto, Alessandro Laio</dc:creator>
    </item>
    <item>
      <title>High-dimensional Sobolev tests on hyperspheres</title>
      <link>https://arxiv.org/abs/2501.10898</link>
      <description>arXiv:2501.10898v1 Announce Type: cross 
Abstract: We derive the limit null distribution of the class of Sobolev tests of uniformity on the hypersphere when the dimension and the sample size diverge to infinity at arbitrary rates. The limiting non-null behavior of these tests is obtained for a sequence of integrated von Mises-Fisher local alternatives. The asymptotic results are applied to test for high-dimensional rotational symmetry and spherical symmetry. Numerical experiments illustrate the derived behavior of the uniformity and spherically symmetry tests under the null and under local and fixed alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10898v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Bruno Ebner, Eduardo Garc\'ia-Portugu\'es, Thomas Verdebout</dc:creator>
    </item>
    <item>
      <title>A Note on the Conversion of Nonnegative Integers to the Canonical Signed-digit Representation</title>
      <link>https://arxiv.org/abs/2501.10908</link>
      <description>arXiv:2501.10908v1 Announce Type: cross 
Abstract: This note addresses the signed-digit representation of non-negative integer binary numbers. We review and revisit popular literature methods for canonical signed-digit representation. A method based on string substitution is discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10908v1</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>R. J. Cintra</dc:creator>
    </item>
    <item>
      <title>On Testing Kronecker Product Structure in Tensor Factor Models</title>
      <link>https://arxiv.org/abs/2501.11208</link>
      <description>arXiv:2501.11208v1 Announce Type: cross 
Abstract: We propose a test for testing the Kronecker product structure of a factor loading matrix implied by a tensor factor model with Tucker decomposition in the common component. Through defining a Kronecker product structure set, we define if a tensor time series response $\{\mathcal{Y}_t\}$ has a Kronecker product structure, equivalent to the ability to decompose $\{\mathcal{Y}_t\}$ according to a tensor factor model. Our test is built on analysing and comparing the residuals from fitting a full tensor factor model, and the residuals from fitting a (tensor) factor model on a reshaped version of the data. In the most extreme case, the reshaping is the vectorisation of the tensor data, and the factor loading matrix in such a case can be general if there is no Kronecker product structure present. Theoretical results are developed through asymptotic normality results on estimated residuals. Numerical experiments suggest that the size of the tests gets closer to the pre-set nominal value as the sample size or the order of the tensor gets larger, while the power increases with mode dimensions and the number of combined modes. We demonstrate out tests through a NYC taxi traffic data and a Fama-French matrix portfolio of returns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11208v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zetai Cen, Clifford Lam</dc:creator>
    </item>
    <item>
      <title>Transductive Conformal Inference for Ranking</title>
      <link>https://arxiv.org/abs/2501.11384</link>
      <description>arXiv:2501.11384v1 Announce Type: cross 
Abstract: We introduce a method based on Conformal Prediction (CP) to quantify the uncertainty of full ranking algorithms. We focus on a specific scenario where $n + m$ items are to be ranked by some ''black box'' algorithm. It is assumed that the relative (ground truth) ranking of n of them is known. The objective is then to quantify the error made by the algorithm on the ranks of the m new items among the total $(n + m)$. In such a setting, the true ranks of the n original items in the total $(n + m)$ depend on the (unknown) true ranks of the m new ones. Consequently, we have no direct access to a calibration set to apply a classical CP method. To address this challenge, we propose to construct distribution-free bounds of the unknown conformity scores using recent results on the distribution of conformal p-values. Using these scores upper bounds, we provide valid prediction sets for the rank of any item. We also control the false coverage proportion, a crucial quantity when dealing with multiple prediction sets. Finally, we empirically show on both synthetic and real data the efficiency of our CP method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11384v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jean-Baptiste Fermanian (UM, Inria, IMAG), Pierre Humbert (SU, LPSM), Gilles Blanchard (LMO, DATASHAPE)</dc:creator>
    </item>
    <item>
      <title>Diffusion-aware Censored Gaussian Processes for Demand Modelling</title>
      <link>https://arxiv.org/abs/2501.12354</link>
      <description>arXiv:2501.12354v1 Announce Type: cross 
Abstract: Inferring the true demand for a product or a service from aggregate data is often challenging due to the limited available supply, thus resulting in observations that are censored and correspond to the realized demand, thereby not accounting for the unsatisfied demand. Censored regression models are able to account for the effect of censoring due to the limited supply, but they don't consider the effect of substitutions, which may cause the demand for similar alternative products or services to increase. This paper proposes Diffusion-aware Censored Demand Models, which combine a Tobit likelihood with a graph diffusion process in order to model the latent process of transfer of unsatisfied demand between similar products or services. We instantiate this new class of models under the framework of GPs and, based on both simulated and real-world data for modeling sales, bike-sharing demand, and EV charging demand, demonstrate its ability to better recover the true demand and produce more accurate out-of-sample predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12354v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filipe Rodrigues</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Causal Effects of High-Dimensional Treatments: Application to Conjoint Analysis</title>
      <link>https://arxiv.org/abs/2201.01357</link>
      <description>arXiv:2201.01357v5 Announce Type: replace 
Abstract: Estimation of heterogeneous treatment effects is an active area of research. Most of the existing methods, however, focus on estimating the conditional average treatment effects of a single, binary treatment given a set of pre-treatment covariates. In this paper, we propose a method to estimate the heterogeneous causal effects of high-dimensional treatments, which poses unique challenges in terms of estimation and interpretation. The proposed approach finds maximally heterogeneous groups and uses a Bayesian mixture of regularized logistic regressions to identify groups of units who exhibit similar patterns of treatment effects. By directly modeling group membership with covariates, the proposed methodology allows one to explore the unit characteristics that are associated with different patterns of treatment effects. Our motivating application is conjoint analysis, which is a popular type of survey experiment in social science and marketing research and is based on a high-dimensional factorial design. We apply the proposed methodology to the conjoint data, where survey respondents are asked to select one of two immigrant profiles with randomly selected attributes. We find that a group of respondents with a relatively high degree of prejudice appears to discriminate against immigrants from non-European countries like Iraq. An open-source software package is available for implementing the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.01357v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Goplerud, Kosuke Imai, Nicole E. Pashley</dc:creator>
    </item>
    <item>
      <title>Asymptotics of numerical integration for two-level mixed models</title>
      <link>https://arxiv.org/abs/2202.07864</link>
      <description>arXiv:2202.07864v2 Announce Type: replace 
Abstract: We study mixed models with a single grouping factor, where inference about unknown parameters requires optimizing a marginal likelihood defined by an intractable integral. Low-dimensional numerical integration techniques are regularly used to approximate these integrals, with inferences about parameters based on the resulting approximate marginal likelihood. For a generic class of mixed models that satisfy explicit regularity conditions, we derive the stochastic relative error rate incurred for both the likelihood and maximum likelihood estimator when adaptive numerical integration is used to approximate the marginal likelihood. We then specialize the analysis to well-specified generalized linear mixed models having exponential family response and multivariate Gaussian random effects, verifying that the regularity conditions hold, and hence that the convergence rates apply. We also prove that for models with likelihoods satisfying very weak concentration conditions that the maximum likelihood estimators from non-adaptive numerical integration approximations of the marginal likelihood are not consistent, further motivating adaptive numerical integration as the preferred tool for inference in mixed models. Code to reproduce the simulations in this paper is provided at https://github.com/awstringer1/aq-theory-paper-code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.07864v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Stringer, Blair Bilodeau, Yanbo Tang</dc:creator>
    </item>
    <item>
      <title>A Gaussian Sliding Windows Regression Model for Hydrological Inference</title>
      <link>https://arxiv.org/abs/2306.00453</link>
      <description>arXiv:2306.00453v4 Announce Type: replace 
Abstract: Statistical models are an essential tool to model, forecast and understand the hydrological processes in watersheds. In particular, the understanding of time lags associated with the delay between rainfall occurrence and subsequent changes in streamflow is of high practical importance. Since water can take a variety of flow paths to generate streamflow, a series of distinct runoff pulses may combine to create the observed streamflow time series. Current state-of-the-art models are not able to sufficiently confront the problem complexity with interpretable parametrization, thus preventing novel insights about the dynamics of distinct flow paths from being formed. The proposed Gaussian Sliding Windows Regression Model targets this problem by combining the concept of multiple windows sliding along the time axis with multiple linear regression. The window kernels, which indicate the weights applied to different time lags, are implemented via Gaussian-shaped kernels. As a result, straightforward process inference can be achieved since each window can represent one flow path. Experiments on simulated and real-world scenarios underline that the proposed model achieves accurate parameter estimates and competitive predictive performance, while fostering explainable and interpretable hydrological modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00453v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Schrunner, Parham Pishrobat, Joseph Janssen, Anna Jenul, Jiguo Cao, Ali A. Ameli, William J. Welch</dc:creator>
    </item>
    <item>
      <title>Evaluating the impact of outcome delay on the efficiency of two-arm group-sequential trials</title>
      <link>https://arxiv.org/abs/2306.04430</link>
      <description>arXiv:2306.04430v2 Announce Type: replace 
Abstract: Adaptive designs(AD) are a broad class of trial designs that allow preplanned modifications based on patient data providing improved efficiency and flexibility. However, a delay in observing the primary outcome variable can harm this added efficiency. In this paper, we aim to ascertain the size of such outcome delay that results in the realised efficiency gains of ADs becoming negligible compared to classical fixed sample RCTs. We measure the impact of delay by developing formulae for the no. of overruns in 2 arm GSDs with normal data, assuming different recruitment models. The efficiency of a GSD is usually measured in terms of the expected sample size (ESS), with GSDs generally reducing the ESS compared to a standard RCT. Our formulae measures the efficiency gain from a GSD in terms of ESS reduction that is lost due to delay. We assess whether careful choice of design (e.g., altering the spacing of the IAs) can help recover the benefits of GSDs in presence of delay. We also analyse the efficiency of GSDs with respect to time to complete the trial. Comparing the expected efficiency gains, with and without consideration of delay, it is evident GSDs suffer considerable losses due to delay. Even a small delay can have a significant impact on the trial's efficiency. In contrast, even in the presence of substantial delay, a GSD will have a smaller expected time to trial completion in comparison to a simple RCT. Greater efficiency is lost with increase in the no. of stages. The timing of IAs also can impact the efficiency of a GSDs with delay. Particularly, for unequally spaced IAs, conducting IAs too early in the trial can be harmful for the design with delay.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04430v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aritra Mukherjee, Michael J. Grayling, James M. S. Wason</dc:creator>
    </item>
    <item>
      <title>Modified treatment policy effect estimation with weighted energy distance</title>
      <link>https://arxiv.org/abs/2310.11620</link>
      <description>arXiv:2310.11620v2 Announce Type: replace 
Abstract: The causal effects of continuous treatments are often characterized through the average dose response function, which is challenging to estimate from observational data due to confounding and positivity violations. Modified treatment policies (MTPs) are an alternative approach that aim to assess the effect of a modification to observed treatment values and work under relaxed assumptions. Estimators for MTPs generally focus on estimating the conditional density of treatment given covariates and using it to construct weights. However, weighting using conditional density models has well-documented challenges. Further, MTPs with larger treatment modifications have stronger confounding and no tools exist to help choose an appropriate modification magnitude. This paper investigates the role of weights for MTPs showing that to control confounding, weights should balance the weighted data to an unobserved hypothetical target population that can be characterized with observed data. Leveraging this insight, we present a versatile set of tools to enhance estimation for MTPs. We introduce a distance that measures imbalance of covariate distributions under the MTP and use it to develop new weighting methods and tools to aid in the estimation of MTPs. Using our methods we study the effect of mechanical power of ventilation on in-hospital mortality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11620v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziren Jiang, Jared D. Huling</dc:creator>
    </item>
    <item>
      <title>Penalized G-estimation for effect modifier selection in a structural nested mean model for repeated outcomes</title>
      <link>https://arxiv.org/abs/2402.00154</link>
      <description>arXiv:2402.00154v4 Announce Type: replace 
Abstract: Effect modification occurs when the impact of the treatment on an outcome varies based on the levels of other covariates known as effect modifiers. Modeling these effect differences is important for etiological goals and for purposes of optimizing treatment. Structural nested mean models (SNMMs) are useful causal models for estimating the potentially heterogeneous effect of a time-varying exposure on the mean of an outcome in the presence of time-varying confounding. A data-adaptive selection approach is necessary if the effect modifiers are unknown a priori and need to be identified. Although variable selection techniques are available for estimating the conditional average treatment effects using marginal structural models or for developing optimal dynamic treatment regimens, all of these methods consider a single end-of-follow-up outcome. In the context of an SNMM for repeated outcomes, we propose a doubly robust penalized G-estimator for the causal effect of a time-varying exposure with a simultaneous selection of effect modifiers and prove the oracle property of our estimator. We conduct a simulation study for the evaluation of its performance in finite samples and verification of its double-robustness property. Our work is motivated by the study of hemodiafiltration for treating patients with end-stage renal disease at the Centre Hospitalier de l'Universit\'e de Montr\'eal. We apply the proposed method to investigate the effect heterogeneity of dialysis facility on the repeated session-specific hemodiafiltration outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.00154v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujae165</arxiv:DOI>
      <dc:creator>Ajmery Jaman, Guanbo Wang, Ashkan Ertefaie, Mich\`ele Bally, Ren\'ee L\'evesque, Robert W. Platt, Mireille E. Schnitzer</dc:creator>
    </item>
    <item>
      <title>A Bayes Factor Framework for Unified Parameter Estimation and Hypothesis Testing</title>
      <link>https://arxiv.org/abs/2403.09350</link>
      <description>arXiv:2403.09350v3 Announce Type: replace 
Abstract: The Bayes factor, the data-based updating factor of the prior to posterior odds of two hypotheses, is a natural measure of statistical evidence for one hypothesis over the other. We show how Bayes factors can also be used for parameter estimation. The key idea is to consider the Bayes factor as a function of the parameter value under the null hypothesis. This "support curve" is inverted to obtain point estimates ("maximum evidence estimates") and interval estimates ("support intervals"), similar to how P-value functions are inverted to obtain point estimates and confidence intervals. This provides data analysts with a unified inference framework as Bayes factors (for any tested parameter value), support intervals (at any level), and point estimates can be easily read off from a plot of the support curve. This approach shares similarities but is also distinct from conventional Bayesian and frequentist approaches: It uses the Bayesian evidence calculus, but without synthesizing data and prior, and it defines statistical evidence in terms of (integrated) likelihood ratios, but also includes a natural way for dealing with nuisance parameters. Applications to meta-analysis, replication studies, and logistic regression illustrate how our framework is of practical value for making quantitative inferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09350v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Samuel Pawel</dc:creator>
    </item>
    <item>
      <title>Finite mixture copulas for modeling dependence in longitudinal count data</title>
      <link>https://arxiv.org/abs/2403.10165</link>
      <description>arXiv:2403.10165v2 Announce Type: replace 
Abstract: Dependence modeling of multivariate count data has garnered significant attention in recent years. Multivariate elliptical copulas are typically preferred in statistical literature to analyze dependence between repeated measurements of longitudinal data since they allow for different choices of the correlation structure. But these copulas lack in flexibility to model dependence and inference is only feasible under parametric restrictions. In this article, we propose employing finite mixtures of elliptical copulas to better capture the intricate and hidden temporal dependencies present in discrete longitudinal data. Our approach allows for the utilization of different correlation matrices within each component of the mixture copula. We theoretically explore the dependence properties of finite mixtures of copulas before employing them to construct regression models for count longitudinal data. Inference for this proposed class of models is based on a composite likelihood approach, and we evaluate the finite sample performance of parameter estimates through extensive simulation studies. To validate our models, we extend traditional techniques and introduce the t-plot method to accommodate finite mixtures of elliptical copulas. Finally, we apply our models to analyze the temporal dependence within two real-world longitudinal datasets and demonstrate their superiority over standard elliptical copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10165v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhajit Chattopadhyay</dc:creator>
    </item>
    <item>
      <title>Augmented Doubly Robust Post-Imputation Inference for Proteomic Data</title>
      <link>https://arxiv.org/abs/2403.15802</link>
      <description>arXiv:2403.15802v2 Announce Type: replace 
Abstract: Quantitative measurements produced by mass spectrometry proteomics experiments offer a direct way to explore the role of proteins in molecular mechanisms. However, analysis of such data is challenging due to the large proportion of missing values. A common strategy to address this issue is to utilize an imputed dataset, which often introduces systematic bias into downstream analyses if the imputation errors are ignored. In this paper, we propose a statistical framework inspired by doubly robust estimators that offers valid and efficient inference for proteomic data. Our framework combines powerful machine learning tools, such as variational autoencoders, to augment the imputation quality with high-dimensional peptide data, and a parametric model to estimate the propensity score for debiasing imputed outcomes. Our estimator is compatible with the double machine learning framework and has provable properties. In application to both single-cell and bulk-cell proteomic data our method utilizes the imputed data to gain additional, meaningful discoveries and yet maintains good control of false positives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15802v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Haeun Moon, Jin-Hong Du, Jing Lei, Kathryn Roeder</dc:creator>
    </item>
    <item>
      <title>Adaptive-TMLE for the Average Treatment Effect based on Randomized Controlled Trial Augmented with Real-World Data</title>
      <link>https://arxiv.org/abs/2405.07186</link>
      <description>arXiv:2405.07186v2 Announce Type: replace 
Abstract: We consider the problem of estimating the average treatment effect (ATE) when both randomized control trial (RCT) data and external real-world data (RWD) are available. We decompose the ATE estimand as the difference between a pooled-ATE estimand that integrates RCT and RWD and a bias estimand that captures the conditional effect of RCT enrollment on the outcome. We introduce an adaptive targeted maximum likelihood estimation (A-TMLE) framework to estimate them. We prove that the A-TMLE estimator is root-n-consistent and asymptotically normal. Moreover, in finite sample, it achieves the super-efficiency one would obtain had one known the oracle model for the conditional effect of the RCT enrollment on the outcome. Consequently, the smaller and more parsimonious the working model of the bias induced by the RWD is, the greater our estimator's efficiency, while our estimator will always be at least as efficient as an efficient estimator that uses the RCT data only. A-TMLE outperforms existing methods in simulations by having smaller mean-squared-error and 95% confidence intervals. We apply A-TMLE to augment the DEVOTE trial with external data from the Optum Clinformatics Data Mart, demonstrating its potential to establish treatment superiority in noninferiority trials. A-TMLE could utilize external RWD to help improve the power of randomized trials without biasing the estimates of intervention effects. This approach could allow for smaller, faster trials, decreasing the time until patients can receive effective treatments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07186v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mark van der Laan, Sky Qiu, Jens Magelund Tarp, Lars van der Laan</dc:creator>
    </item>
    <item>
      <title>Nonparametric Inference on Dose-Response Curves Without the Positivity Condition</title>
      <link>https://arxiv.org/abs/2405.09003</link>
      <description>arXiv:2405.09003v2 Announce Type: replace 
Abstract: Existing statistical methods in causal inference often assume the positivity condition, where every individual has some chance of receiving any treatment level regardless of covariates. This assumption could be violated in observational studies with continuous treatments. In this paper, we develop identification and estimation theories for causal effects with continuous treatments (i.e., dose-response curves) without relying on the positivity condition. Our approach identifies and estimates the derivative of the treatment effect for each observed sample, integrating it to the treatment level of interest to mitigate bias from the lack of positivity. The method is grounded in a weaker assumption, satisfied by additive confounding models. We propose a fast and reliable numerical recipe for computing our integral estimator in practice and derive its asymptotic properties. To enable valid inference on the dose-response curve and its derivative, we use the nonparametric bootstrap and establish its consistency. The performances of our proposed estimators are validated through simulation studies and an analysis of the effect of air pollution exposure (PM$_{2.5}$) on cardiovascular mortality rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.09003v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen, Alexander Giessing</dc:creator>
    </item>
    <item>
      <title>Gradient Boosting for Hierarchical Data in Small Area Estimation</title>
      <link>https://arxiv.org/abs/2406.04256</link>
      <description>arXiv:2406.04256v2 Announce Type: replace 
Abstract: This paper introduces Mixed Effect Gradient Boosting (MEGB), which combines the strengths of Gradient Boosting with Mixed Effects models to address complex, hierarchical data structures often encountered in statistical analysis. The methodological foundations, including a review of the Mixed Effects model and the Extreme Gradient Boosting method, leading to the introduction of MEGB are shown in detail. It highlights how MEGB can derive area-level mean estimations from unit-level data and calculate Mean Squared Error (MSE) estimates using a nonparametric bootstrap approach. The paper evaluates MEGB's performance through model-based and design-based simulation studies, comparing it against established estimators. The findings indicate that MEGB provides promising area mean estimations and may outperform existing small area estimators in various scenarios. The paper concludes with a discussion on future research directions, highlighting the possibility of extending MEGB's framework to accommodate different types of outcome variables or non-linear area level indicators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.04256v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Messer, Timo Schmid</dc:creator>
    </item>
    <item>
      <title>Polytomous Explanatory Item Response Models for Item Discrimination: Assessing Negative-Framing Effects in Social-Emotional Learning Surveys</title>
      <link>https://arxiv.org/abs/2406.05304</link>
      <description>arXiv:2406.05304v2 Announce Type: replace 
Abstract: Modeling item parameters as a function of item characteristics has a long history but has generally focused on models for item location. Explanatory item response models for item discrimination are available but rarely used. In this study, we extend existing approaches for modeling item discrimination from dichotomous to polytomous item responses. We illustrate our proposed approach with an application to four social-emotional learning surveys of preschool children to investigate how item discrimination depends on whether an item is positively or negatively framed. Negative framing predicts significantly lower item discrimination on two of the four surveys, and a plausibly causal estimate from a regression discontinuity analysis shows that negative framing reduces discrimination by about 30\% on one survey. We conclude with a discussion of potential applications of explanatory models for item discrimination.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05304v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joshua B. Gilbert, Lijin Zhang, Esther Ulitzsch, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>A Calibrated Sensitivity Analysis for Weighted Causal Decompositions</title>
      <link>https://arxiv.org/abs/2407.00139</link>
      <description>arXiv:2407.00139v2 Announce Type: replace 
Abstract: Disparities in health or well-being experienced by minority groups can be difficult to study using the traditional exposure-outcome paradigm in causal inference, since potential outcomes in variables such as race or sexual minority status are challenging to interpret. Causal decomposition analysis addresses this gap by positing causal effects on disparities under interventions to other, intervenable exposures that may play a mediating role in the disparity. While invoking weaker assumptions than causal mediation approaches, decomposition analyses are often conducted in observational settings and require uncheckable assumptions that eliminate unmeasured confounders. Leveraging the marginal sensitivity model, we develop a sensitivity analysis for weighted causal decomposition estimators and use the percentile bootstrap to construct valid confidence intervals for causal effects on disparities. We also propose a two-parameter reformulation that enhances interpretability and facilitates an intuitive understanding of the plausibility of unmeasured confounders and their effects. We illustrate our framework on a study examining the effect of parental acceptance on disparities in suicidal ideation among sexual minority youth. We find that the effect is small and sensitive to unmeasured confounding, suggesting that further screening studies are needed to identify mitigating interventions in this vulnerable population.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.00139v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy A. Shen, Elina Visoki, Ran Barzilay, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Sparse Asymptotic PCA: Identifying Sparse Latent Factors Across Time Horizon</title>
      <link>https://arxiv.org/abs/2407.09738</link>
      <description>arXiv:2407.09738v2 Announce Type: replace 
Abstract: This paper introduces a novel sparse latent factor modeling framework using sparse asymptotic Principal Component Analysis (APCA) to analyze the co-movements of high-dimensional panel data over time. Unlike existing methods based on sparse PCA, which assume sparsity in the loading matrices, our approach posits sparsity in the factor processes while allowing non-sparse loadings. This is motivated by the fact that financial returns typically exhibit universal and non-sparse exposure to market factors. Unlike the commonly used $\ell_1$-relaxation in sparse PCA, the proposed sparse APCA employs a truncated power method to estimate the leading sparse factor and a sequential deflation method for multi-factor cases under $\ell_0$-constraints. Furthermore, we develop a data-driven approach to identify the sparsity of risk factors over the time horizon using a novel cross-sectional cross-validation method. We establish the consistency of our estimators under mild conditions as both the dimension $N$ and the sample size $T$ grow. Monte Carlo simulations demonstrate that the proposed method performs well in finite samples. Empirically, we apply our method to daily S&amp;P 500 stock returns (2004--2016) and identify nine risk factors influencing the stock market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09738v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao</dc:creator>
    </item>
    <item>
      <title>Two-way Matrix Autoregressive Model with Thresholds</title>
      <link>https://arxiv.org/abs/2407.10272</link>
      <description>arXiv:2407.10272v2 Announce Type: replace 
Abstract: Recently, matrix-valued time series data have attracted significant attention in the literature with the recognition of threshold nonlinearity representing a significant advance. However, given the fact that a matrix is a two-array structure, it is unfortunate, perhaps even unusual, for the threshold literature to focus on using the same threshold variable for the rows and the columns. In fact, evidence in economic, financial, environmental and other data shows advantages of allowing the possibilities of two different threshold variables (with possibly different threshold parameters for rows and columns), hence the need for a Two-way Matrix AutoRegressive model with Thresholds (2-MART). Naturally, two threshold variables pose new and perhaps even fierce challenges, which might be the reason behind the adoption of only one threshold variable in the literature up to now. In this paper, we develop a comprehensive methodology for the 2-MART model, by overcoming various challenges. Compared with existing models in the literature, the new model can achieve greater dimension reduction, much better model fitting, more accurate predictions, and more plausible interpretations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10272v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Yu, Dong Li, Xinyu Zhang, Howell Tong</dc:creator>
    </item>
    <item>
      <title>Block-Additive Gaussian Processes under Monotonicity Constraints</title>
      <link>https://arxiv.org/abs/2407.13402</link>
      <description>arXiv:2407.13402v2 Announce Type: replace 
Abstract: We generalize the additive constrained Gaussian process framework to handle interactions between input variables while enforcing monotonicity constraints everywhere on the input space. The block-additive structure of the model is particularly suitable in the presence of interactions, while maintaining tractable computations. In addition, we develop a sequential algorithm, MaxMod, for model selection (i.e., the choice of the active input variables and of the blocks). We speed up our implementations through efficient matrix computations and thanks to explicit expressions of criteria involved in MaxMod. The performance and scalability of our methodology are showcased with several numerical examples in dimensions up to 120, as well as in a 5D real-world coastal flooding application, where interpretability is enhanced by the selection of the blocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13402v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Deronzier, A. F. L\'opez-Lopera, F. Bachoc, O. Roustant, J. Rohmer</dc:creator>
    </item>
    <item>
      <title>An online generalization of the (e-)Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2407.20683</link>
      <description>arXiv:2407.20683v3 Announce Type: replace 
Abstract: In online multiple testing, the hypotheses arrive one by one, and at each time we must immediately reject or accept the current hypothesis solely based on the data and hypotheses observed so far. Many online procedures have been proposed, but none of them are generalizations of the Benjamini-Hochberg (BH) procedure based on p-values, or of the e-BH procedure that uses e-values. In this paper, we consider a relaxed problem setup that allows the current hypothesis to be rejected at any later step. We show that this relaxation allows us to define -- what we justify extensively to be -- the natural and appropriate online extension of the BH and e-BH procedures. We show that the FDR guarantees for BH (resp. e-BH) and online BH (resp. online e-BH) are identical under positive, negative or arbitrary dependence, at fixed and stopping times. Further, the online BH (resp. online e-BH) rule recovers the BH (resp. e-BH) rule as a special case when the number of hypotheses is known to be fixed. Of independent interest, our proof techniques also allow us to prove that numerous existing online procedures, which were known to control the FDR at fixed times, also control the FDR at stopping times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20683v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lasse Fischer, Ziyu Xu, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>A Capture-Recapture Approach to Facilitate Causal Inference for a Trial-eligible Observational Cohort</title>
      <link>https://arxiv.org/abs/2409.18358</link>
      <description>arXiv:2409.18358v2 Announce Type: replace 
Abstract: Background: We extend recently proposed design-based capture-recapture methods for prevalence estimation among registry participants, in order to support causal inference among a trial-eligible target population. The proposed design for CRC analysis integrates an observational study cohort with a randomized trial involving a small representative study sample, and enhances the generalizability and transportability of the findings. Methods: We develop a novel CRC-type estimator derived via multinomial distribution-based maximum-likelihood that exploits the design to deliver benefits in terms of validity and efficiency for comparing the effects of two treatments on a binary outcome. Additionally, the design enables a direct standardization-type estimator for efficient estimation of general means (e.g., of biomarker levels) under a specific treatment, and for their comparison across treatments. For inference, we propose a tailored Bayesian credible interval approach to improve coverage properties in conjunction with the proposed CRC estimator for binary outcomes, along with a bootstrap percentile interval approach for use in the case of continuous outcomes. Results: Simulations demonstrate the proposed estimators derived from the CRC design. The multinomial-based maximum-likelihood estimator shows benefits in terms of validity and efficiency in treatment effect comparisons, while the direct standardization-type estimator allows comprehensive comparison of treatment effects within the target population. Conclusion: The extended CRC methods provide a useful framework for causal inference in a trial-eligible target population by integrating observational and randomized trial data. The novel estimators enhance the generalizability and transportability of findings, offering efficient and valid tools for treatment effect comparisons on both binary and continuous outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18358v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Ge, Yuzi Zhang, Lance A. Waller, Robert H. Lyles</dc:creator>
    </item>
    <item>
      <title>Perturbation-Robust Predictive Modeling of Social Effects by Network Subspace Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2410.01163</link>
      <description>arXiv:2410.01163v4 Announce Type: replace 
Abstract: Network-linked data, where multivariate observations are interconnected by a network, are becoming increasingly prevalent in fields such as sociology and biology. These data often exhibit inherent noise and complex relational structures, complicating conventional modeling and statistical inference. Motivated by empirical challenges in analyzing such data sets, this paper introduces a family of network subspace generalized linear models designed for analyzing noisy, network-linked data. We propose a model inference method based on subspace-constrained maximum likelihood, which emphasizes flexibility in capturing network effects and provides a robust inference framework against network perturbations. We establish the asymptotic distributions of the estimators under network perturbations, demonstrating the method's accuracy through extensive simulations involving random network models and deep-learning-based embedding algorithms. The proposed methodology is applied to a comprehensive analysis of a large-scale study on school conflicts, where it identifies significant social effects, offering meaningful and interpretable insights into student behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01163v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianxiang Wang, Can M. Le, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Exploring Learning Rate Selection in Generalised Bayesian Inference using Posterior Predictive Checks</title>
      <link>https://arxiv.org/abs/2410.01475</link>
      <description>arXiv:2410.01475v2 Announce Type: replace 
Abstract: Generalised Bayesian Inference (GBI) attempts to address model misspecification in a standard Bayesian setup by tempering the likelihood. The likelihood is raised to a fractional power, called the learning rate, which reduces its importance in the posterior and has been established as a method to address certain kinds of model misspecification. Posterior Predictive Checks (PPC) attempt to detect model misspecification by locating a diagnostic, computed on the observed data, within the posterior predictive distribution of the diagnostic. This can be used to construct a hypothesis test where a small $p$-value indicates potential misfit. The recent Embedded Diachronic Sense Change (EDiSC) model suffers from misspecification and benefits from likelihood tempering. Using EDiSC as a case study, this exploratory work examines whether PPC could be used in a novel way to set the learning rate in a GBI setup. Specifically, the learning rate selected is the lowest value for which a hypothesis test using the log likelihood diagnostic is not rejected at the 10% level. The experimental results are promising, though not definitive, and indicate the need for further research along the lines suggested here.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01475v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Schyan Zafar, Geoff K. Nicholls</dc:creator>
    </item>
    <item>
      <title>Improve the Precision of Area Under the Curve Estimation for Recurrent Events Through Covariate Adjustment</title>
      <link>https://arxiv.org/abs/2410.24163</link>
      <description>arXiv:2410.24163v2 Announce Type: replace 
Abstract: The area under the curve (AUC) of the mean cumulative function (MCF) has recently been introduced as a novel estimand for evaluating treatment effects in recurrent event settings, capturing a totality of evidence in relation to disease progression. While the Lin-Wei-Yang-Ying (LWYY) model is commonly used for analyzing recurrent events, it relies on the proportional rate assumption between treatment arms, which might be violated in practice. In contrast, the AUC under MCFs does not depend on such proportionality assumptions and offers a clinically interpretable measure of treatment effect. To improve the precision of the AUC estimation while preserving its unconditional interpretability, we propose a nonparametric covariate adjustment approach. This approach guarantees efficiency gain compared to unadjusted analysis, as demonstrated by theoretical asymptotic distributions, and is universally applicable to various randomization schemes, including both simple and covariate-adaptive designs. Extensive simulations across different scenarios further support its advantage in increasing statistical power. Our findings highlight the importance of covariate adjustment for the analysis of AUC in recurrent event settings, offering practical guidance for its application in randomized clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.24163v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Tuo Wang, Yanyao Yi, Ting Ye, Jun Shao, Yu Du</dc:creator>
    </item>
    <item>
      <title>New Additive OCBA Procedures for Robust Ranking and Selection</title>
      <link>https://arxiv.org/abs/2412.06020</link>
      <description>arXiv:2412.06020v2 Announce Type: replace 
Abstract: Robust ranking and selection (R&amp;S) is an important and challenging variation of conventional R&amp;S that seeks to select the best alternative among a finite set of alternatives. It captures the common input uncertainty in the simulation model by using an ambiguity set to include multiple possible input distributions and shifts to select the best alternative with the smallest worst-case mean performance over the ambiguity set. In this paper, we aim at developing new fixed-budget robust R&amp;S procedures to minimize the probability of incorrect selection (PICS) under a limited sampling budget. Inspired by an additive upper bound of the PICS, we derive a new asymptotically optimal solution to the budget allocation problem. Accordingly, we design a new sequential optimal computing budget allocation (OCBA) procedure to solve robust R&amp;S problems efficiently. We then conduct a comprehensive numerical study to verify the superiority of our robust OCBA procedure over existing ones. The numerical study also provides insights on the budget allocation behaviors that lead to enhanced efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06020v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchen Wan, Zaile Li, L. Jeff Hong</dc:creator>
    </item>
    <item>
      <title>Ergodic Network Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2412.17779</link>
      <description>arXiv:2412.17779v2 Announce Type: replace 
Abstract: We propose a novel framework for Network Stochastic Differential Equations (N-SDE), where each node in a network is governed by an SDE influenced by interactions with its neighbors. The evolution of each node is driven by the interplay of three key components: the node's intrinsic dynamics (\emph{momentum effect}), feedback from neighboring nodes (\emph{network effect}), and a \emph{stochastic volatility} term modeled by Brownian motion. Our primary objective is to estimate the parameters of the N-SDE system from high-frequency discrete-time observations. The motivation behind this model lies in its ability to analyze very high-dimensional time series by leveraging the inherent sparsity of the underlying network graph. We consider two distinct scenarios: \textit{i) known network structure}: the graph is fully specified, and we establish conditions under which the parameters can be identified, considering the linear growth of the parameter space with the number of edges. \textit{ii) unknown network structure}: the graph must be inferred from the data. For this, we develop an iterative procedure using adaptive Lasso, tailored to a specific subclass of N-SDE models. In this work, we assume the network graph is oriented, paving the way for novel applications of SDEs in causal inference, enabling the study of cause-effect relationships in dynamic systems. Through extensive simulation studies, we demonstrate the performance of our estimators across various graph topologies in high-dimensional settings. We also showcase the framework's applicability to real-world datasets, highlighting its potential for advancing the analysis of complex networked systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17779v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Francesco Iafrate, Stefano Iacus</dc:creator>
    </item>
    <item>
      <title>A note on local parameter orthogonality for multivariate data and the Whittle algorithm for multivariate autoregressive models</title>
      <link>https://arxiv.org/abs/2501.08093</link>
      <description>arXiv:2501.08093v2 Announce Type: replace 
Abstract: This article extends the Cox--Reid local parameter orthogonality to a multivariate setting, gives an affirmative reply to one of Cox and Reid's questions, and shows that the extension can lead to efficient computational algorithms with the celebrated Whittle algorithm for multivariate autoregressive modeling as a showcase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08093v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changle Shen, Dong Li, Howell Tong</dc:creator>
    </item>
    <item>
      <title>A note on the construction of augmented designs in square arrays</title>
      <link>https://arxiv.org/abs/2501.08448</link>
      <description>arXiv:2501.08448v2 Announce Type: replace 
Abstract: An augmented design in a square array can be derived from a smaller row-column design (the contraction). Such a contraction has also previously been used to generate a two-replicate resolvable incomplete block design. We demonstrate a parallel between these two uses of the contraction and thereby establish a recently proposed conjecture by linking the average efficiency factor of the augmented design with that of its contraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08448v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. R. Williams, H-P. Piepho</dc:creator>
    </item>
    <item>
      <title>A Primer on the Signature Method in Machine Learning</title>
      <link>https://arxiv.org/abs/1603.03788</link>
      <description>arXiv:1603.03788v2 Announce Type: replace-cross 
Abstract: We provide an introduction to the signature method, focusing on its theoretical properties and machine learning applications. Our presentation is divided into two parts. In the first part, we present the definition and fundamental properties of the signature of a path. The signature is a sequence of numbers associated with a path that captures many of its important analytic and geometric properties. As a sequence of numbers, the signature serves as a compact description (dimension reduction) of a path. In presenting its theoretical properties, we assume only familiarity with classical real analysis and integration, and supplement theory with straightforward examples. We also mention several advanced topics, including the role of the signature in rough path theory. In the second part, we present practical applications of the signature to the area of machine learning. The signature method is a non-parametric way of transforming data into a set of features that can be used in machine learning tasks. In this method, data are converted into multi-dimensional paths, by means of embedding algorithms, of which the signature is then computed. We describe this pipeline in detail, making a link with the properties of the signature presented in the first part. We furthermore review some of the developments of the signature method in machine learning and, as an illustrative example, present a detailed application of the method to handwritten digit classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:1603.03788v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilya Chevyrev, Andrey Kormilitzin</dc:creator>
    </item>
    <item>
      <title>Dynamic CoVaR Modeling and Estimation</title>
      <link>https://arxiv.org/abs/2206.14275</link>
      <description>arXiv:2206.14275v4 Announce Type: replace-cross 
Abstract: The popular systemic risk measure CoVaR (conditional Value-at-Risk) and its variants are widely used in economics and finance. In this article, we propose joint dynamic forecasting models for the Value-at-Risk (VaR) and CoVaR. The CoVaR version we consider is defined as a large quantile of one variable (e.g., losses in the financial system) conditional on some other variable (e.g., losses in a bank's shares) being in distress. We introduce a two-step M-estimator for the model parameters drawing on recently proposed bivariate scoring functions for the pair (VaR, CoVaR). We prove consistency and asymptotic normality of our parameter estimator and analyze its finite-sample properties in simulations. Finally, we apply a specific subclass of our dynamic forecasting models, which we call CoCAViaR models, to log-returns of large US banks. A formal forecast comparison shows that our CoCAViaR models generate CoVaR predictions which are superior to forecasts issued from current benchmark models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14275v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.RM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timo Dimitriadis, Yannick Hoga</dc:creator>
    </item>
    <item>
      <title>Some Notes of Inequalities Under $\mathcal{C}$-mixing Conditions and Their Applications to Variance Estimation</title>
      <link>https://arxiv.org/abs/2208.11481</link>
      <description>arXiv:2208.11481v3 Announce Type: replace-cross 
Abstract: As a mixing condition including many interesting dynamic systems as special cases, $\mathcal{C}$-mixing condition has drawn significant attention in recent years. This paper aims to do some contributions on the following points. First, we show a Bernstein-type inequality under $\mathcal{C}$-mixing conditions. Compared with the pioneering work on this point, \citeA{hang2017bernstein}, our inequality is sharper under more general assumptions. Second, since the general definition of $\mathcal{C}$-mixing condition is based on a covariance inequality whose upper bound relies on some given $\mathcal{C}$-norm (see Definition \ref{def 1}), a natural difficulty arises when the $\mathcal{C}$-norm is infinitely large. Under this circumstances, we show some inequalities bounding the variance of partial sums without requiring finite $\mathcal{C}$-norms. Finally, up to our knowledge, there is few literature discussing central limit theorem under $C$-mixing conditions as general as that of \citeA{hang2017bernstein}. Thus, under \citeauthor{hang2017bernstein}'s $\mathcal{C}$-mixing conditions, we take one step forward on this point by deriving a central limit theorem with mild moment conditions. As for the applications, we apply the previously mentioned results to show Bahadur representation and asymptotic normality of weighted $M$-estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11481v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zihao Yuan</dc:creator>
    </item>
    <item>
      <title>Measuring the Driving Forces of Predictive Performance: Application to Credit Scoring</title>
      <link>https://arxiv.org/abs/2212.05866</link>
      <description>arXiv:2212.05866v4 Announce Type: replace-cross 
Abstract: As they play an increasingly important role in determining access to credit, credit scoring models are under growing scrutiny from banking supervisors and internal model validators. These authorities need to monitor the model performance and identify its key drivers. To facilitate this, we introduce the XPER methodology to decompose a performance metric (e.g., AUC, $R^2$) into specific contributions associated with the various features of a forecasting model. XPER is theoretically grounded on Shapley values and is both model-agnostic and performance metric-agnostic. Furthermore, it can be implemented either at the model level or at the individual level. Using a novel dataset of car loans, we decompose the AUC of a machine-learning model trained to forecast the default probability of loan applicants. We show that a small number of features can explain a surprisingly large part of the model performance. Notably, the features that contribute the most to the predictive performance of the model may not be the ones that contribute the most to individual forecasts (SHAP). Finally, we show how XPER can be used to deal with heterogeneity issues and improve performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.05866v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hu\'e Sullivan, Hurlin Christophe, P\'erignon Christophe, Saurin S\'ebastien</dc:creator>
    </item>
    <item>
      <title>Functional Data-Driven Quantile Model Averaging with Application to Cryptocurrencies</title>
      <link>https://arxiv.org/abs/2310.01970</link>
      <description>arXiv:2310.01970v2 Announce Type: replace-cross 
Abstract: Given the high volatility and susceptibility to extreme events in the cryptocurrency market, forecasting tail risk is of paramount importance. Value-at-Risk (VaR), a quantile-based risk measure, is widely used for assessing tail risk and is central to monitoring financial market stability. In data-rich environments, functional data from various domains are employed to forecast conditional quantiles. However, the infinite-dimensional nature of functional data introduces uncertainty. This paper addresses this uncertainty problem by proposing a novel data-driven conditional quantile model averaging (MA) approach. With a set of candidate models varying by the number of components, MA assigns weights to each model determined by a K-fold cross-validation criterion. We prove the asymptotic optimality of the selected weights in terms of minimizing the excess final prediction error when all candidate models are misspecified. Additionally, when the true regression relationship belongs to the set of candidate models, we provide consistency results for the averaged estimators. Numerical studies indicate that, in most cases, the proposed method outperforms other model selection and averaging methods, particularly for extreme quantiles in cryptocurrency markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.01970v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenchao Xu, Xinyu Zhang, Jeng-Min Chiou, Yuying Sun</dc:creator>
    </item>
    <item>
      <title>Estimation for multistate models subject to reporting delays and incomplete event adjudication</title>
      <link>https://arxiv.org/abs/2311.04318</link>
      <description>arXiv:2311.04318v3 Announce Type: replace-cross 
Abstract: Complete observation of event histories is often impossible due to sampling effects such as right-censoring and left-truncation, but also due to reporting delays and incomplete event adjudication. This is for example the case for health insurance claims and during interim stages of clinical trials. In this paper, we develop a parametric method that takes the aforementioned effects into account, treating the latter two as partially exogenous. The method, which takes the form of a two-step M-estimation procedure, is applicable to multistate models in general, including competing risks and recurrent event models. The effect of reporting delays is derived via thinning, offering an alternative to existing results for Poisson models. To address incomplete event adjudication, we propose an imputed likelihood approach which, compared to existing methods, has the advantage of allowing for dependencies between the event history and adjudication processes as well as allowing for unreported events and multiple event types. We establish consistency and asymptotic normality under standard identifiability, integrability, and smoothness conditions, and we demonstrate the validity of the percentile bootstrap. Finally, a simulation study shows favorable finite sample performance of our method compared to other alternatives, while an application to disability insurance data illustrates its practical potential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04318v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Buchardt, C. Furrer, O. L. Sandqvist</dc:creator>
    </item>
    <item>
      <title>Bayesian variable selection in sample selection models using spike-and-slab priors</title>
      <link>https://arxiv.org/abs/2312.03538</link>
      <description>arXiv:2312.03538v3 Announce Type: replace-cross 
Abstract: Sample selection models represent a common methodology for correcting bias induced by data missing not at random. These models are not empirically identifiable without exclusion restrictions. In other words, some variables predictive of missingness do not affect the outcome model of interest. The drive to establish this requirement often leads to the inclusion of irrelevant variables in the model. A recent proposal uses adaptive LASSO to circumvent this problem, but its performance depends on the so-called covariance assumption, which can be violated in small to moderate samples. Additionally, there are no tools yet for post-selection inference for this model. To address these challenges, we propose two families of spike-and-slab priors to conduct Bayesian variable selection in sample selection models. These prior structures allow for constructing a Gibbs sampler with tractable conditionals, which is scalable to the dimensions of practical interest. We illustrate the performance of the proposed methodology through a simulation study and present a comparison against adaptive LASSO and stepwise selection. We also provide two applications using publicly available real data. An implementation and code to reproduce the results in this paper can be found at https://github.com/adam-iqbal/selection-spike-slab.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03538v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Iqbal, Emmanuel O. Ogundimu, F. Javier Rubio</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Treatment Effects with Item-Level Outcome Data: Insights from Item Response Theory</title>
      <link>https://arxiv.org/abs/2405.00161</link>
      <description>arXiv:2405.00161v4 Announce Type: replace-cross 
Abstract: Analyses of heterogeneous treatment effects (HTE) are common in applied causal inference research. However, when outcomes are latent variables assessed via psychometric instruments such as educational tests, standard methods ignore the potential HTE that may exist among the individual items of the outcome measure. Failing to account for "item-level" HTE (IL-HTE) can lead to both underestimated standard errors and identification challenges in the estimation of treatment-by-covariate interaction effects. We demonstrate how Item Response Theory (IRT) models that estimate a treatment effect for each assessment item can both address these challenges and provide new insights into HTE generally. This study articulates the theoretical rationale for the IL-HTE model and demonstrates its practical value using 75 datasets from 48 randomized controlled trials containing 5.8 million item responses in economics, education, and health research. Our results show that the IL-HTE model reveals item-level variation masked by single-number scores, provides more meaningful standard errors in many settings, allows for estimates of the generalizability of causal effects to untested items, resolves identification problems in the estimation of interaction effects, and provides estimates of standardized treatment effect sizes corrected for attenuation due to measurement error.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00161v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joshua B. Gilbert, Zachary Himmelsbach, James Soland, Mridul Joshi, Benjamin W. Domingue</dc:creator>
    </item>
    <item>
      <title>Reconstructing East Asian Temperatures from 1368 to 1911 Using Historical Documents, Climate Models, and Data Assimilation</title>
      <link>https://arxiv.org/abs/2410.21790</link>
      <description>arXiv:2410.21790v2 Announce Type: replace-cross 
Abstract: We propose a novel approach for reconstructing annual temperatures in East Asia from 1368 to 1911, leveraging the Reconstructed East Asian Climate Historical Encoded Series (REACHES). The lack of instrumental data during this period poses significant challenges to understanding past climate conditions. REACHES digitizes historical documents from the Ming and Qing dynasties of China, converting qualitative descriptions into a four-level ordinal temperature scale. However, these index-based data are biased toward abnormal or extreme weather phenomena, leading to data gaps that likely correspond to normal conditions. To address this bias and reconstruct historical temperatures at any point within East Asia, including locations without direct historical data, we employ a three-tiered statistical framework. First, we perform kriging to interpolate temperature data across East Asia, adopting a zero-mean assumption to handle missing information. Next, we utilize the Last Millennium Ensemble (LME) reanalysis data and apply quantile mapping to calibrate the kriged REACHES data to Celsius temperature scales. Finally, we introduce a novel Bayesian data assimilation method that integrates the kriged Celsius data with LME simulations to enhance reconstruction accuracy. We model the LME data at each geographic location using a flexible nonstationary autoregressive time series model and employ regularized maximum likelihood estimation with a fused lasso penalty. The resulting dynamic distribution serves as a prior, which is refined via Kalman filtering by incorporating the kriged Celsius REACHES data to yield posterior temperature estimates. This comprehensive integration of historical documentation, contemporary climate models, and advanced statistical methods improves the accuracy of historical temperature reconstructions and provides a crucial resource for future environmental and climate studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21790v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Sun, Kuan-hui Elaine Lin, Wan-Ling Tseng, Pao K. Wang, Hsin-Cheng Huang</dc:creator>
    </item>
    <item>
      <title>On the Unknowable Limits to Prediction</title>
      <link>https://arxiv.org/abs/2411.19223</link>
      <description>arXiv:2411.19223v4 Announce Type: replace-cross 
Abstract: We propose a rigorous decomposition of predictive error, highlighting that not all 'irreducible' error is genuinely immutable. Many domains stand to benefit from iterative enhancements in measurement, construct validity, and modeling. Our approach demonstrates how apparently 'unpredictable' outcomes can become more tractable with improved data (across both target and features) and refined algorithms. By distinguishing aleatoric from epistemic error, we delineate how accuracy may asymptotically improve--though inherent stochasticity may remain--and offer a robust framework for advancing computational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19223v4</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiani Yan, Charles Rahal</dc:creator>
    </item>
    <item>
      <title>The Choice of Normalization Influences Shrinkage in Regularized Regression</title>
      <link>https://arxiv.org/abs/2501.03821</link>
      <description>arXiv:2501.03821v2 Announce Type: replace-cross 
Abstract: Regularized models are often sensitive to the scales of the features in the data and it has therefore become standard practice to normalize (center and scale) the features before fitting the model. But there are many different ways to normalize the features and the choice may have dramatic effects on the resulting model. In spite of this, there has so far been no research on this topic. In this paper, we begin to bridge this knowledge gap by studying normalization in the context of lasso, ridge, and elastic net regression. We focus on normal and binary features and show that the class balances of binary features directly influences the regression coefficients and that this effect depends on the combination of normalization and regularization methods used. We demonstrate that this effect can be mitigated by scaling binary features with their variance in the case of the lasso and standard deviation in the case of ridge regression, but that this comes at the cost of increased variance. For the elastic net, we show that scaling the penalty weights, rather than the features, can achieve the same effect. Finally, we also tackle mixes of binary and normal features as well as interactions and provide some initial results on how to normalize features in these cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03821v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 22 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johan Larsson, Jonas Wallin</dc:creator>
    </item>
  </channel>
</rss>
