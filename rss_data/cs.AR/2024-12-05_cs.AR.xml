<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 06 Dec 2024 02:43:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>BinSparX: Sparsified Binary Neural Networks for Reduced Hardware Non-Idealities in Xbar Arrays</title>
      <link>https://arxiv.org/abs/2412.03553</link>
      <description>arXiv:2412.03553v1 Announce Type: new 
Abstract: Compute-in-memory (CiM)-based binary neural network (CiM-BNN) accelerators marry the benefits of CiM and ultra-low precision quantization, making them highly suitable for edge computing. However, CiM-enabled crossbar (Xbar) arrays are plagued with hardware non-idealities like parasitic resistances and device non-linearities that impair inference accuracy, especially in scaled technologies. In this work, we first analyze the impact of Xbar non-idealities on the inference accuracy of various CiM-BNNs, establishing that the unique properties of CiM-BNNs make them more prone to hardware non-idealities compared to higher precision deep neural networks (DNNs). To address this issue, we propose BinSparX, a training-free technique that mitigates non-idealities in CiM-BNNs. BinSparX utilizes the distinct attributes of BNNs to reduce the average current generated during the CiM operations in Xbar arrays. This is achieved by statically and dynamically sparsifying the BNN weights and activations, respectively (which, in the context of BNNs, is defined as reducing the number of +1 weights and activations). This minimizes the IR drops across the parasitic resistances, drastically mitigating their impact on inference accuracy. To evaluate our technique, we conduct experiments on ResNet-18 and VGG-small CiM-BNNs designed at the 7nm technology node using 8T-SRAM and 1T-1ReRAM. Our results show that BinSparX is highly effective in alleviating the impact of non-idealities, recouping the inference accuracy to near-ideal (software) levels in some cases and providing accuracy boost of up to 77.25%. These benefits are accompanied by energy reduction, albeit at the cost of mild latency/area increase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03553v1</guid>
      <category>cs.AR</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Akul Malhotra, Sumeet Kumar Gupta</dc:creator>
    </item>
    <item>
      <title>Massimult: A Novel Parallel CPU Architecture Based on Combinator Reduction</title>
      <link>https://arxiv.org/abs/2412.02765</link>
      <description>arXiv:2412.02765v1 Announce Type: cross 
Abstract: The Massimult project aims to design and implement an innovative CPU architecture based on combinator reduction with a novel combinator base and a new abstract machine. The evaluation of programs within this architecture is inherently highly parallel and localized, allowing for faster computation, reduced energy consumption, improved scalability, enhanced reliability, and increased resistance to attacks. In this paper, we introduce the machine language LambdaM, detail its compilation into KVY assembler code, and describe the abstract machine Matrima. The best part of Matrima is its ability to exploit inherent parallelism and locality in combinator reduction, leading to significantly faster computations with lower energy consumption, scalability across multiple processors, and enhanced security against various types of attacks. Matrima can be simulated as a software virtual machine and is intended for future hardware implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02765v1</guid>
      <category>cs.DC</category>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jurgen Nicklisch-Franken, Ruslan Feizerakhmanov</dc:creator>
    </item>
    <item>
      <title>Online Soft Error Tolerance in ReRAM Crossbars for Deep Learning Accelerators</title>
      <link>https://arxiv.org/abs/2412.03089</link>
      <description>arXiv:2412.03089v1 Announce Type: cross 
Abstract: Resistive Random-Access Memory (ReRAM) crossbar arrays are promising candidates for in-situ matrix-vector multiplication (MVM), a frequent operation in Deep Learning algorithms. Despite their advantages, these emerging non-volatile memories are susceptible to errors due to non-idealities such as immature fabrication processes and runtime errors, which lead to accuracy degradation in Processing-in-Memory (PIM) accelerators. This paper proposes an online soft error detection and correction method in ReRAM crossbar arrays. We utilize a test input vector and Error Correcting Codes (ECCs) to detect and correct faulty columns. The proposed approach demonstrates near fault-free accuracy for Neural Networks (NNs) on MNIST and CIFAR-10 datasets, with low area overhead and power consumption compared to recent methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03089v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benyamin Khezeli, Hamid Reza Zarandi, Elham Cheshmikhani</dc:creator>
    </item>
    <item>
      <title>BOSS: Blocking algorithm for optimizing shuttling scheduling in Ion Trap</title>
      <link>https://arxiv.org/abs/2412.03443</link>
      <description>arXiv:2412.03443v1 Announce Type: cross 
Abstract: Ion traps stand at the forefront of quantum hardware technology, presenting unparalleled benefits for quantum computing, such as high-fidelity gates, extensive connectivity, and prolonged coherence times. In this context, we explore the critical role of shuttling operations within these systems, especially their influence on the fidelity loss and elongated execution times. To address these challenges, we have developed BOSS, an efficient blocking algorithm tailored to enhance shuttling efficiency. This optimization not only bolsters the shuttling process but also elevates the overall efficacy of ion trap devices. We experimented on multiple applications using two qubit gates up to 4000+ and qubits ranging from 64 to 78. Our method significantly reduces the number of shuttles on most applications, with a maximum reduction of 96.1%. Additionally, our investigation includes simulations of realistic experimental parameters that incorporate sympathetic cooling, offering a higher fidelity and a refined estimate of execution times that align more closely with practical scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03443v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xian Wu, Chenghong Zhu, Jingbo Wang, Xin Wang</dc:creator>
    </item>
    <item>
      <title>HLSFactory: A Framework Empowering High-Level Synthesis Datasets for Machine Learning and Beyond</title>
      <link>https://arxiv.org/abs/2405.00820</link>
      <description>arXiv:2405.00820v3 Announce Type: replace 
Abstract: Machine learning (ML) techniques have been applied to high-level synthesis (HLS) flows for quality-of-result (QoR) prediction and design space exploration (DSE). Nevertheless, the scarcity of accessible high-quality HLS datasets and the complexity of building such datasets present challenges. Existing datasets have limitations in terms of benchmark coverage, design space enumeration, vendor extensibility, or lack of reproducible and extensible software for dataset construction. Many works also lack user-friendly ways to add more designs, limiting wider adoption of such datasets. In response to these challenges, we introduce HLSFactory, a comprehensive framework designed to facilitate the curation and generation of high-quality HLS design datasets. HLSFactory has three main stages: 1) a design space expansion stage to elaborate single HLS designs into large design spaces using various optimization directives across multiple vendor tools, 2) a design synthesis stage to execute HLS and FPGA tool flows concurrently across designs, and 3) a data aggregation stage for extracting standardized data into packaged datasets for ML usage. This tripartite architecture ensures broad design space coverage via design space expansion and supports multiple vendor tools. Users can contribute to each stage with their own HLS designs and synthesis results and extend the framework itself with custom frontends and tool flows. We also include an initial set of built-in designs from common HLS benchmarks curated open-source HLS designs. We showcase the versatility and multi-functionality of our framework through seven case studies: I) ML model for QoR prediction; II) Design space sampling; III) Fine-grained parallelism backend speedup; IV) Targeting Intel's HLS flow; V) Adding new auxiliary designs; VI) Integrating published HLS data; VII) HLS tool version regression benchmarking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00820v3</guid>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3670474.3685961</arxiv:DOI>
      <dc:creator>Stefan Abi-Karam, Rishov Sarkar, Allison Seigler, Sean Lowe, Zhigang Wei, Hanqiu Chen, Nanditha Rao, Lizy John, Aman Arora, Cong Hao</dc:creator>
    </item>
    <item>
      <title>ML-QLS: Multilevel Quantum Layout Synthesis</title>
      <link>https://arxiv.org/abs/2405.18371</link>
      <description>arXiv:2405.18371v3 Announce Type: replace-cross 
Abstract: Quantum Layout Synthesis (QLS) plays a crucial role in optimizing quantum circuit execution on physical quantum devices. As we enter the era where quantum computers have hundreds of qubits, we are faced with scalability issues using optimal approaches and degrading heuristic methods' performance due to the lack of global optimization. To this end, we introduce a hybrid design that obtains the much improved solution for the heuristic method utilizing the multilevel framework, which is an effective methodology to solve large-scale problems in VLSI design. In this paper, we present ML-QLS, the first multilevel quantum layout tool with a scalable refinement operation integrated with novel cost functions and clustering strategies. Our clustering provides valuable insights into generating a proper problem approximation for quantum circuits and devices. Our experimental results demonstrate that ML-QLS can scale up to problems involving hundreds of qubits and achieve a remarkable 52% performance improvement over leading heuristic QLS tools for large circuits, which underscores the effectiveness of multilevel frameworks in quantum applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18371v3</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wan-Hsuan Lin, Jason Cong</dc:creator>
    </item>
    <item>
      <title>Control-Flow Attestation: Concepts, Solutions, and Open Challenges</title>
      <link>https://arxiv.org/abs/2408.06304</link>
      <description>arXiv:2408.06304v4 Announce Type: replace-cross 
Abstract: Control-flow attestation unifies the worlds of control-flow integrity and platform attestation by measuring and reporting a target's run-time behaviour to a verifier. Trust assurances in the target are provided by testing whether its execution follows an authorised control-flow path. The problem has been explored in various settings, such as assessing the trustworthiness of cloud platforms, cyber-physical systems, and Internet of Things devices. Despite a significant number of proposals being made in recent years, the area remains fragmented, with different adversarial behaviours, verification paradigms, and deployment challenges being addressed. In this paper, we present the first survey of control-flow attestation, examining the core ideas and solutions in state-of-the-art schemes. In total, we survey over 30 papers published between 2016--2024, consolidate and compare their key features, and pose several challenges and recommendations for future research in the area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06304v4</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhanyu Sha, Carlton Shepherd, Amir Rafi, Konstantinos Markantonakis</dc:creator>
    </item>
    <item>
      <title>Distributed Binary Optimization with In-Memory Computing: An Application for the SAT Problem</title>
      <link>https://arxiv.org/abs/2409.09152</link>
      <description>arXiv:2409.09152v3 Announce Type: replace-cross 
Abstract: In-memory computing (IMC) has been shown to be a promising approach for solving binary optimization problems while significantly reducing energy and latency. Building on the advantages of parallel computation, we propose an IMC-compatible parallelism framework inspired by parallel tempering (PT), enabling cross-replica communication to improve the performance of IMC solvers. This framework enables an IMC solver not only to improve performance beyond what can be achieved through parallelization, but also affords greater flexibility for the search process with low hardware overhead. We justify that the framework can be applied to almost any IMC solver. We demonstrate the effectiveness of the framework for the Boolean satisfiability (SAT) problem, using the WalkSAT heuristic as a proxy for existing IMC solvers. The resulting PT-inspired cooperative WalkSAT (PTIC-WalkSAT) algorithm outperforms the traditional WalkSAT heuristic in terms of the iterations-to-solution in 76.3% of the tested problem instances and its na\"ive parallel variant (PA-WalkSAT) does so in 68.4% of the instances. An estimate of the energy overhead of the PTIC framework for two hardware accelerator architectures indicates that in both cases the overhead of running the PTIC framework would be less than 1% of the total energy required to run each accelerator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.09152v3</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <category>math.OC</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiangyi Zhang, Fabian B\"ohm, Elisabetta Valiante, Moslem Noori, Thomas Van Vaerenbergh, Chan-Woo Yang, Giacomo Pedretti, Masoud Mohseni, Raymond Beausoleil, Ignacio Rozada</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Simulation Framework for CXL Disaggregated Memory</title>
      <link>https://arxiv.org/abs/2411.02282</link>
      <description>arXiv:2411.02282v4 Announce Type: replace-cross 
Abstract: Compute eXpress Link (CXL) is a pivotal technology for memory disaggregation in future heterogeneous computing systems, enabling on-demand memory expansion and improved resource utilization. Despite its potential, CXL is in its early stages with limited market products, highlighting the need for a reliable system-level simulation tool. This paper introduces CXL-DMSim, an open-source, high-fidelity full-system simulator for CXL disaggregated memory systems, comparable in speed to gem5. CXL-DMSim includes a flexible CXL memory expander model, device driver, and support for CXL\.io and CXL\.mem protocols. It supports both app-managed and kernel-managed modes, with the latter featuring a NUMA-compatible mechanism. Rigorous verification against real hardware testbeds with FPGA-based and ASIC-based CXL memory prototypes confirms CXL-DMSim's accuracy, with an average simulation error of 4.1%. Benchmark results using LMbench and STREAM indicate that CXL-FPGA memory has approximately ~2.88x higher latency than local DDR, while CXL-ASIC latency is about ~2.18x. CXL-FPGA achieves 45-69% of local DDR's memory bandwidth, and CXL-ASIC reaches 82-83%. The performance of CXL memory is significantly more sensitive to Rd/Wr patterns than local DDR, with optimal bandwidth at a 74%:26% ratio rather than 50%:50% due to the current CXL+DDR controller design. The study also shows that CXL memory can markedly enhance the performance of memory-intensive applications, with the most improvement seen in Viper (~23x) and in bandwidth-sensitive scenarios like MERCI (16%). CXL-DMSim's observability and expandability are demonstrated through detailed case studies, showcasing its potential for research on future CXL-interconnected hybrid memory pools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02282v4</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Thu, 05 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanjing Wang, Lizhou Wu, Wentao Hong, Yang Ou, Zicong Wang, Sunfeng Gao, Jie Zhang, Sheng Ma, Dezun Dong, Xingyun Qi, Mingche Lai, Nong Xiao</dc:creator>
    </item>
  </channel>
</rss>
