<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Feb 2025 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Non-Negotiated Implicit ETSI VAM Clustering</title>
      <link>https://arxiv.org/abs/2502.10617</link>
      <description>arXiv:2502.10617v1 Announce Type: new 
Abstract: Including Vulnerable Road User (VRU) in Cooperative Intelligent Transport Systems (C-ITS) framework aims to increase road safety. However, this approach implies a massive increase of network nodes and thus is vulnerable to medium capacity issues, e.g., contention, congestion, resource scheduling. Implementing cluster schemes -to reduce the number of nodes but represent the same number of VRUs- is a direct way to address the issue. One of them is suggested by European Telecommunications Standards Institute (ETSI) and consists of nodes (connected pedestrians and cyclists) sending vicarious messages to enable a leader node to cover for a cluster of VRUs. However, the proposed scheme includes negotiation to establish a cluster, and in-cluster communication to maintain it, requiring extra messages of variable sizes and thus does not fully resolve the original medium capacity issues. Furthermore, these exchanges assume network reliability (i.e. a lossless channel and low latency to meet time constraints). We propose a method for VRU Awareness Message (VAM) clustering where 1) all cluster operations are performed without negotiation, 2) cluster leaders do not require sending additional messages or meet deadlines, and 3) assumes a lossy communication channel and offers a mechanism for cluster resilience. Our results show the feasibility of the concept by halving message generations compared to individual messages while keeping the awareness levels (i.e., that VRUs are accounted for).</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10617v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Felipe Valle, Daniel Bleckert, Linus Frisk, Oscar Amador Molina, Elena Haller, Alexey Vinel</dc:creator>
    </item>
    <item>
      <title>Multi-objective Aerial IRS-assisted ISAC Optimization via Generative AI-enhanced Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2502.10687</link>
      <description>arXiv:2502.10687v1 Announce Type: new 
Abstract: Integrated sensing and communication (ISAC) has garnered substantial research interest owing to its pivotal role in advancing the development of next-generation (6G) wireless networks. However, achieving a performance balance between communication and sensing in the dual-function radar communication (DFRC)-based ISAC system remains a significant challenge. In this paper, an aerial intelligent reflecting surface (IRS)-assisted ISAC system is explored, where a base station (BS) supports dual-functional operations, enabling both data transmission for multiple users and sensing for a blocked target, with the channel quality enhanced by an IRS mounted on the unmanned aerial vehicle (UAV). Moreover, we formulate an integrated communication, sensing, and energy efficiency multi-objective optimization problem (CSEMOP), which aims to maximize the communication rate of the users and the echo rate of the target, while minimizing UAV propulsion energy consumption by jointly optimizing the BS beamforming matrix, IRS phase shifts, the flight velocity and angle of the UAV. Considering the non-convexity, trade-off, and dynamic nature of the formulated CSEMOP, we propose a generative diffusion model-based deep deterministic policy gradient (GDMDDPG) method to solve the problem. Specifically, the diffusion model is incorporated into the actor network of DDPG to improve the action quality, with noise perturbation mechanism for better exploration and recent prioritized experience replay (RPER) sampling mechanism for enhanced training efficiency. Simulation results indicate that the GDMDDPG method delivers superior performance compared to the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10687v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwen Xie, Geng Sun, Jiacheng Wang, Hongyang Du, Jiawen Kang, Kaibin Huang, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>Service Function Chain Dynamic Scheduling in Space-Air-Ground Integrated Networks</title>
      <link>https://arxiv.org/abs/2502.10731</link>
      <description>arXiv:2502.10731v1 Announce Type: new 
Abstract: As an important component of the sixth generation communication technologies, the space-air-ground integrated network (SAGIN) attracts increasing attentions in recent years. However, due to the mobility and heterogeneity of the components such as satellites and unmanned aerial vehicles in multi-layer SAGIN, the challenges of inefficient resource allocation and management complexity are aggregated. To this end, the network function virtualization technology is introduced and can be implemented via service function chains (SFCs) deployment. However, urgent unexpected tasks may bring conflicts and resource competition during SFC deployment, and how to schedule the SFCs of multiple tasks in SAGIN is a key issue. In this paper, we address the dynamic and complexity of SAGIN by presenting a reconfigurable time extension graph and further propose the dynamic SFC scheduling model. Then, we formulate the SFC scheduling problem to maximize the number of successful deployed SFCs within limited resources and time horizons. Since the problem is in the form of integer linear programming and intractable to solve, we propose the algorithm by incorporating deep reinforcement learning. Finally, simulation results show that the proposed algorithm has better convergence and performance compared to other benchmark algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10731v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziye Jia, Yilu Cao, Lijun He, Qihui Wu, Qiuming Zhu, Dusit Niyato, Zhu Han</dc:creator>
    </item>
    <item>
      <title>Towards Cloud-Native Agentic Protocol Learning for Conflict-Free 6G: A Case Study on Inter-Slice Resource Allocation</title>
      <link>https://arxiv.org/abs/2502.10775</link>
      <description>arXiv:2502.10775v1 Announce Type: new 
Abstract: In this paper, we propose a novel cloud-native architecture for collaborative agentic network slicing. Our approach addresses the challenge of managing shared infrastructure, particularly CPU resources, across multiple network slices with heterogeneous requirements. Each network slice is controlled by a dedicated agent operating within a Dockerized environment, ensuring isolation and scalability. The agents dynamically adjust CPU allocations based on real-time traffic demands, optimizing the performance of the overall system. A key innovation of this work is the development of emergent communication among the agents. Through their interactions, the agents autonomously establish a communication protocol that enables them to coordinate more effectively, optimizing resource allocations in response to dynamic traffic demands. Based on synthetic traffic modeled on real-world conditions, accounting for varying load patterns, tests demonstrated the effectiveness of the proposed architecture in handling diverse traffic types, including eMBB, URLLC, and mMTC, by adjusting resource allocations to meet the strict requirements of each slice. Additionally, the cloud-native design enables real-time monitoring and analysis through Prometheus and Grafana, ensuring the system's adaptability and efficiency in dynamic network environments. The agents managed to learn how to maximize the shared infrastructure with a conflict rate of less than 3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10775v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sebasti\'an Camargo, Farhad Rezazadeh, Hatim Chergui, Shuaib Siddiqui, Lingjia Liu</dc:creator>
    </item>
    <item>
      <title>Towards End-to-End Application Slicing in Multi-access Edge Computing systems: Architecture Discussion and Proof-of-Concept</title>
      <link>https://arxiv.org/abs/2502.10860</link>
      <description>arXiv:2502.10860v1 Announce Type: new 
Abstract: Network slicing is one of the most critical 5G pillars. It allows for sharing a 5G infrastructure among different tenants leading to improved service customisation and increased operators' revenues. Concurrently, introducing the Multi-access Edge Computing (MEC) into 5G to support time-critical applications raises the need to integrate this distributed computing infrastructure to the 5G network slicing framework. Indeed, end-to-end latency guarantees require the end-to-end management of slice resources. For this purpose, after discussing the main gaps in the state-of-the-art with regards to such an objective, we propose a novel slicing architecture that enables the management and orchestration of slice segments that span over all the domains of an end-to-end application service, including the MEC. We also show how this general management architecture can be instantiated into a multi-tenant MEC infrastructure. A preliminary implementation of the proposed architecture focusing on the MEC domain is also provided, together with performance tests to validate the feasibility and efficacy of our design approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10860v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.future.2022.05.027</arxiv:DOI>
      <arxiv:journal_reference>Future Generation Computer Systems, Volume 136, November 2022, Pages 110-127</arxiv:journal_reference>
      <dc:creator>Simone Bolettieri, Dinh Thai Bui, Raffaele Bruno</dc:creator>
    </item>
    <item>
      <title>AquaScope: Reliable Underwater Image Transmission on Mobile Devices</title>
      <link>https://arxiv.org/abs/2502.10891</link>
      <description>arXiv:2502.10891v1 Announce Type: new 
Abstract: Underwater communication is essential for both recreational and scientific activities, such as scuba diving. However, existing methods remain highly constrained by environmental challenges and often require specialized hardware, driving research into more accessible underwater communication solutions. While recent acoustic-based communication systems support text messaging on mobile devices, their low data rates severely limit broader applications.
  We present AquaScope, the first acoustic communication system capable of underwater image transmission on commodity mobile devices. To address the key challenges of underwater environments -- limited bandwidth and high transmission errors -- AquaScope employs and enhances generative image compression to improve compression efficiency, and integrates it with reliability-enhancement techniques at the physical layer to strengthen error resilience. We implemented AquaScope on the Android platform and demonstrated its feasibility for underwater image transmission. Experimental results show that AquaScope enables reliable, low-latency image transmission while preserving perceptual image quality, across various bandwidth-constrained and error-prone underwater conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10891v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beitong Tian, Lingzhi Zhao, Bo Chen, Mingyuan Wu, Haozhen Zheng, Deepak Vasisht, Francis Y. Yan, Klara Nahrstedt</dc:creator>
    </item>
    <item>
      <title>Leveraging Uncertainty Estimation for Efficient LLM Routing</title>
      <link>https://arxiv.org/abs/2502.11021</link>
      <description>arXiv:2502.11021v1 Announce Type: new 
Abstract: Deploying large language models (LLMs) in edge-cloud environments requires an efficient routing strategy to balance cost and response quality. Traditional approaches prioritize either human-preference data or accuracy metrics from benchmark datasets as routing criteria, but these methods suffer from rigidity and subjectivity. Moreover, existing routing frameworks primarily focus on accuracy and cost, neglecting response quality from a human preference perspective. In this work, we propose the Confidence-Driven LLM Router, a novel framework that leverages uncertainty estimation to optimize routing decisions. To comprehensively assess routing performance, we evaluate both system cost efficiency and response quality. In particular, we introduce the novel use of LLM-as-a-Judge to simulate human rating preferences, providing the first systematic assessment of response quality across different routing strategies. Extensive experiments on MT-Bench, GSM8K, and MMLU demonstrate that our approach outperforms state-of-the-art routing methods, achieving superior response quality while maintaining cost efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11021v1</guid>
      <category>cs.NI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuo Zhang, Asal Mehradfar, Dimitrios Dimitriadis, Salman Avestimehr</dc:creator>
    </item>
    <item>
      <title>Integrating Language Models for Enhanced Network State Monitoring in DRL-Based SFC Provisioning</title>
      <link>https://arxiv.org/abs/2502.11298</link>
      <description>arXiv:2502.11298v1 Announce Type: new 
Abstract: Efficient Service Function Chain (SFC) provisioning and Virtual Network Function (VNF) placement are critical for enhancing network performance in modern architectures such as Software-Defined Networking (SDN) and Network Function Virtualization (NFV). While Deep Reinforcement Learning (DRL) aids decision-making in dynamic network environments, its reliance on structured inputs and predefined rules limits adaptability in unforeseen scenarios. Additionally, incorrect actions by a DRL agent may require numerous training iterations to correct, potentially reinforcing suboptimal policies and degrading performance. This paper integrates DRL with Language Models (LMs), specifically Bidirectional Encoder Representations from Transformers (BERT) and DistilBERT, to enhance network management. By feeding final VNF allocations from DRL into the LM, the system can process and respond to queries related to SFCs, DCs, and VNFs, enabling real-time insights into resource utilization, bottleneck detection, and future demand planning. The LMs are fine-tuned to our domain-specific dataset using Low-Rank Adaptation (LoRA). Results show that BERT outperforms DistilBERT with a lower test loss (0.28 compared to 0.36) and higher confidence (0.83 compared to 0.74), though BERT requires approximately 46% more processing time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11298v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Parisa Fard Moshiri, Murat Arda Onsu, Poonam Lohan, Burak Kantarci, Emil Janulewicz</dc:creator>
    </item>
    <item>
      <title>Intelligent Mobile AI-Generated Content Services via Interactive Prompt Engineering and Dynamic Service Provisioning</title>
      <link>https://arxiv.org/abs/2502.11386</link>
      <description>arXiv:2502.11386v1 Announce Type: new 
Abstract: Due to massive computational demands of large generative models, AI-Generated Content (AIGC) can organize collaborative Mobile AIGC Service Providers (MASPs) at network edges to provide ubiquitous and customized content generation for resource-constrained users. However, such a paradigm faces two significant challenges: 1) raw prompts (i.e., the task description from users) often lead to poor generation quality due to users' lack of experience with specific AIGC models, and 2) static service provisioning fails to efficiently utilize computational and communication resources given the heterogeneity of AIGC tasks. To address these challenges, we propose an intelligent mobile AIGC service scheme. Firstly, we develop an interactive prompt engineering mechanism that leverages a Large Language Model (LLM) to generate customized prompt corpora and employs Inverse Reinforcement Learning (IRL) for policy imitation through small-scale expert demonstrations. Secondly, we formulate a dynamic mobile AIGC service provisioning problem that jointly optimizes the number of inference trials and transmission power allocation. Then, we propose the Diffusion-Enhanced Deep Deterministic Policy Gradient (D3PG) algorithm to solve the problem. By incorporating the diffusion process into Deep Reinforcement Learning (DRL) architecture, the environment exploration capability can be improved, thus adapting to varying mobile AIGC scenarios. Extensive experimental results demonstrate that our prompt engineering approach improves single-round generation success probability by 6.3 times, while D3PG increases the user service experience by 67.8% compared to baseline DRL approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11386v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinqiu Liu, Ruichen Zhang, Jiacheng Wang, Dusit Niyato, Xianbin Wang, Dong In Kim, Hongyang Du</dc:creator>
    </item>
    <item>
      <title>End-to-End Reliability in Wireless IEEE 802.1Qbv Time-Sensitive Networks</title>
      <link>https://arxiv.org/abs/2502.11595</link>
      <description>arXiv:2502.11595v1 Announce Type: new 
Abstract: Industrial cyber-physical systems require dependable network communication with formal end-to-end reliability guarantees. Striving towards this goal, recent efforts aim to advance the integration of 5G into Time-Sensitive Networking (TSN). However, we show that IEEE 802.1Qbv TSN schedulers that are unattuned to 5G packet delay variations may jeopardize any reliability guarantees provided by the 5G system. We demonstrate this on a case where a 99.99% reliability in the inner 5G network diminishes to below 10% when looking at end-to-end communication in TSN. In this paper, we overcome this shortcoming by introducing Full Interleaving Packet Scheduling (FIPS) as a wireless-friendly IEEE 802.1Qbv scheduler. To the best of our knowledge, FIPS is the first to provide formal end-to-end QoS guarantees in wireless TSN. FIPS allows a controlled batching of TSN streams, which improves schedulability in terms of the number of wireless TSN streams by a factor of up to x45. Even in failure cases, FIPS isolates the otherwise cascading QoS violations to the affected streams and protects all other streams. With formal end-to-end reliability, improved schedulability, and fault isolation, FIPS makes a substantial advance towards dependability in wireless TSN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11595v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S. Egger (University of Stuttgart), J. Gross (KTH Royal Institute of Technology), J. Sachs (Ericsson Research), G. P. Sharma (KTH Royal Institute of Technology), C. Becker (University of Stuttgart), F. D\"urr (University of Stuttgart)</dc:creator>
    </item>
    <item>
      <title>Design Considerations Based on Stability for a Class of TCP Algorithms</title>
      <link>https://arxiv.org/abs/2502.11983</link>
      <description>arXiv:2502.11983v1 Announce Type: new 
Abstract: Transmission Control Protocol (TCP) continues to be the dominant transport protocol on the Internet. The stability of fluid models has been a key consideration in the design of TCP and the performance evaluation of TCP algorithms. Based on local stability analysis, we formulate some design considerations for a class of TCP algorithms. We begin with deriving sufficient conditions for the local stability of a generalized TCP algorithm in the presence of heterogeneous round-trip delays. Within this generalized model, we consider three specific variants of TCP: TCP Reno, Compound TCP, and Scalable TCP. The sufficient conditions we derive are scalable across network topologies with one, two, and many bottleneck links. We are interested in networks with intermediate and small drop-tail buffers as they offer smaller queuing delays. The small buffer regime is more attractive as the conditions for stability are decentralized. TCP algorithms that follow our design considerations can provide stable operation on any network topology, irrespective of the number of bottleneck links or delays in the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11983v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sreekanth Prabhakar, Gaurav Raina</dc:creator>
    </item>
    <item>
      <title>Evolutionary Power-Aware Routing in VANETs using Monte-Carlo Simulation</title>
      <link>https://arxiv.org/abs/2502.10417</link>
      <description>arXiv:2502.10417v1 Announce Type: cross 
Abstract: This work addresses the reduction of power consumption of the AODV routing protocol in vehicular networks as an optimization problem. Nowadays, network designers focus on energy-aware communication protocols, specially to deploy wireless networks. Here, we introduce an automatic method to search for energy-efficient AODV configurations by using an evolutionary algorithm and parallel Monte-Carlo simulations to improve the accuracy of the evaluation of tentative solutions. The experimental results demonstrate that significant power consumption improvements over the standard configuration can be attained, with no noteworthy loss in the quality of service.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10417v1</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/HPCSim.2012.6266900</arxiv:DOI>
      <dc:creator>J. Toutouh, S. Nesmachnow, E. Alba</dc:creator>
    </item>
    <item>
      <title>From Layers to States: A State Space Model Perspective to Deep Neural Network Layer Dynamics</title>
      <link>https://arxiv.org/abs/2502.10463</link>
      <description>arXiv:2502.10463v1 Announce Type: cross 
Abstract: The depth of neural networks is a critical factor for their capability, with deeper models often demonstrating superior performance. Motivated by this, significant efforts have been made to enhance layer aggregation - reusing information from previous layers to better extract features at the current layer, to improve the representational power of deep neural networks. However, previous works have primarily addressed this problem from a discrete-state perspective which is not suitable as the number of network layers grows. This paper novelly treats the outputs from layers as states of a continuous process and considers leveraging the state space model (SSM) to design the aggregation of layers in very deep neural networks. Moreover, inspired by its advancements in modeling long sequences, the Selective State Space Models (S6) is employed to design a new module called Selective State Space Model Layer Aggregation (S6LA). This module aims to combine traditional CNN or transformer architectures within a sequential framework, enhancing the representational capabilities of state-of-the-art vision networks. Extensive experiments show that S6LA delivers substantial improvements in both image classification and detection tasks, highlighting the potential of integrating SSMs with contemporary deep learning techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10463v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinshuo Liu, Weiqin Zhao, Wei Huang, Yanwen Fang, Lequan Yu, Guodong Li</dc:creator>
    </item>
    <item>
      <title>Federated Learning-Driven Cybersecurity Framework for IoT Networks with Privacy-Preserving and Real-Time Threat Detection Capabilities</title>
      <link>https://arxiv.org/abs/2502.10599</link>
      <description>arXiv:2502.10599v1 Announce Type: cross 
Abstract: The rapid expansion of the Internet of Things (IoT) ecosystem has transformed various sectors but has also introduced significant cybersecurity challenges. Traditional centralized security methods often struggle to balance privacy preservation and real-time threat detection in IoT networks. To address these issues, this study proposes a Federated Learning-Driven Cybersecurity Framework designed specifically for IoT environments. The framework enables decentralized data processing by training models locally on edge devices, ensuring data privacy. Secure aggregation of these locally trained models is achieved using homomorphic encryption, allowing collaborative learning without exposing sensitive information.
  The proposed framework utilizes recurrent neural networks (RNNs) for anomaly detection, optimized for resource-constrained IoT networks. Experimental results demonstrate that the system effectively detects complex cyber threats, including distributed denial-of-service (DDoS) attacks, with over 98% accuracy. Additionally, it improves energy efficiency by reducing resource consumption by 20% compared to centralized approaches.
  This research addresses critical gaps in IoT cybersecurity by integrating federated learning with advanced threat detection techniques. The framework offers a scalable and privacy-preserving solution adaptable to various IoT applications. Future work will explore the integration of blockchain for transparent model aggregation and quantum-resistant cryptographic methods to further enhance security in evolving technological landscapes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10599v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Milad Rahmati</dc:creator>
    </item>
    <item>
      <title>Evaluating the Potential of Quantum Machine Learning in Cybersecurity: A Case-Study on PCA-based Intrusion Detection Systems</title>
      <link>https://arxiv.org/abs/2502.11173</link>
      <description>arXiv:2502.11173v1 Announce Type: cross 
Abstract: Quantum computing promises to revolutionize our understanding of the limits of computation, and its implications in cryptography have long been evident. Today, cryptographers are actively devising post-quantum solutions to counter the threats posed by quantum-enabled adversaries. Meanwhile, quantum scientists are innovating quantum protocols to empower defenders. However, the broader impact of quantum computing and quantum machine learning (QML) on other cybersecurity domains still needs to be explored. In this work, we investigate the potential impact of QML on cybersecurity applications of traditional ML. First, we explore the potential advantages of quantum computing in machine learning problems specifically related to cybersecurity. Then, we describe a methodology to quantify the future impact of fault-tolerant QML algorithms on real-world problems. As a case study, we apply our approach to standard methods and datasets in network intrusion detection, one of the most studied applications of machine learning in cybersecurity. Our results provide insight into the conditions for obtaining a quantum advantage and the need for future quantum hardware and software advancements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11173v1</guid>
      <category>quant-ph</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.cose.2025.104341</arxiv:DOI>
      <dc:creator>Armando Bellante, Tommaso Fioravanti, Michele Carminati, Stefano Zanero, Alessandro Luongo</dc:creator>
    </item>
    <item>
      <title>Grassroots Platforms with Atomic Transactions: Social Networks, Cryptocurrencies, and Democratic Federations</title>
      <link>https://arxiv.org/abs/2502.11299</link>
      <description>arXiv:2502.11299v1 Announce Type: cross 
Abstract: Grassroots platforms aim to offer an egalitarian alternative to global platforms -- centralized/autocratic (Facebook etc.) and decentralized/plutocratic (Bitcoin etc.) alike. Key grassroots platforms include grassroots social networks, grassroots cryptocurrencies, and grassroots democratic federations. Previously, grassroots platforms were defined formally and proven grassroots using unary distributed transition systems, in which each transition is carried out by a single agent. However, grassroots platforms cater for a more abstract specification using transactions carried out atomically by multiple agents, something that cannot be expressed by unary transition systems. As a result, their original specifications and proofs were unnecessarily cumbersome and opaque.
  Here, we aim to provide a more suitable formal foundation for grassroots platforms. To do so, we enhance the notion of a distributed transition system to include atomic transactions and revisit the notion of grassroots platforms within this new foundation. We present crisp specifications of key grassroots platforms using atomic transactions: befriending and defriending for grassroots social networks, coin swaps for grassroots cryptocurrencies, and communities forming, joining, and leaving a federation for grassroots democratic federations. We prove a general theorem that a platform specified by atomic transactions that are so-called interactive is grassroots; show that the atomic transactions used to specify all three platforms are interactive; and conclude that the platforms thus specified are indeed grassroots. We thus provide a better mathematical foundation for grassroots platforms and a solid and clear starting point from which their implementation can commence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11299v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>A GNN-based Spectral Filtering Mechanism for Imbalance Classification in Network Digital Twin</title>
      <link>https://arxiv.org/abs/2502.11505</link>
      <description>arXiv:2502.11505v1 Announce Type: cross 
Abstract: Graph Neural Networks are gaining attention in Fifth-Generation (5G) core network digital twins, which are data-driven complex systems with numerous components. Analyzing these data can be challenging due to rare failure types, leading to imbalanced classification in multiclass settings. Digital twins of 5G networks increasingly employ graph classification as the main method for identifying failure types. However, the skewed distribution of failure occurrences is a major class imbalance issue that prevents effective graph data mining. Previous studies have not sufficiently tackled this complex problem. In this paper, we propose Class-Fourier Graph Neural Network (CF-GNN) introduces a class-oriented spectral filtering mechanism that ensures precise classification by estimating a unique spectral filter for each class. We employ eigenvalue and eigenvector spectral filtering to capture and adapt to variations in the minority classes, ensuring accurate class-specific feature discrimination, and adept at graph representation learning for complex local structures among neighbors in an end-to-end setting. Extensive experiments have demonstrated that the proposed CF-GNN could help with both the creation of new techniques for enhancing classifiers and the investigation of the characteristics of the multi-class imbalanced data in a network digital twin system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11505v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abubakar Isah, Ibrahim Aliyu, Sulaiman Muhammad Rashid, Jaehyung Park, Minsoo Hahn, Jinsul Kim</dc:creator>
    </item>
    <item>
      <title>A Unified Modeling Framework for Automated Penetration Testing</title>
      <link>https://arxiv.org/abs/2502.11588</link>
      <description>arXiv:2502.11588v1 Announce Type: cross 
Abstract: The integration of artificial intelligence into automated penetration testing (AutoPT) has highlighted the necessity of simulation modeling for the training of intelligent agents, due to its cost-efficiency and swift feedback capabilities. Despite the proliferation of AutoPT research, there is a recognized gap in the availability of a unified framework for simulation modeling methods. This paper presents a systematic review and synthesis of existing techniques, introducing MDCPM to categorize studies based on literature objectives, network simulation complexity, dependency of technical and tactical operations, and scenario feedback and variation. To bridge the gap in unified method for multi-dimensional and multi-level simulation modeling, dynamic environment modeling, and the scarcity of public datasets, we introduce AutoPT-Sim, a novel modeling framework that based on policy automation and encompasses the combination of all sub dimensions. AutoPT-Sim offers a comprehensive approach to modeling network environments, attackers, and defenders, transcending the constraints of static modeling and accommodating networks of diverse scales. We publicly release a generated standard network environment dataset and the code of Network Generator. By integrating publicly available datasets flexibly, support is offered for various simulation modeling levels focused on policy automation in MDCPM and the network generator help researchers output customized target network data by adjusting parameters or fine-tuning the network generator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11588v1</guid>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunfei Wang, Shixuan Liu, Wenhao Wang, Changling Zhou, Chao Zhang, Jiandong Jin, Cheng Zhu</dc:creator>
    </item>
    <item>
      <title>Blank Space: Adaptive Causal Coding for Streaming Communications Over Multi-Hop Networks</title>
      <link>https://arxiv.org/abs/2502.11984</link>
      <description>arXiv:2502.11984v1 Announce Type: cross 
Abstract: In this work, we introduce Blank Space AC-RLNC (BS), a novel Adaptive and Causal Network Coding (AC-RLNC) solution designed to mitigate the triplet trade-off between throughput-delay-efficiency in multi-hop networks. BS leverages the network's physical limitations considering the bottleneck from each node to the destination. In particular, BS introduces a light-computational re-encoding algorithm, called Network AC-RLNC (NET), implemented independently at intermediate nodes. NET adaptively adjusts the Forward Error Correction (FEC) rates and schedules idle periods. It incorporates two distinct suspension mechanisms: 1) Blank Space Period, accounting for the forward-channels bottleneck, and 2) No-New No-FEC approach, based on data availability. The experimental results achieve significant improvements in resource efficiency, demonstrating a 20% reduction in channel usage compared to baseline RLNC solutions. Notably, these efficiency gains are achieved while maintaining competitive throughput and delay performance, ensuring improved resource utilization does not compromise network performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11984v1</guid>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adina Waxman, Shai Ginzach, Aviel Glam, Alejandro Cohen</dc:creator>
    </item>
    <item>
      <title>Reconfigurable Intelligent Surfaces-Assisted Integrated Access and Backhaul</title>
      <link>https://arxiv.org/abs/2502.12011</link>
      <description>arXiv:2502.12011v1 Announce Type: cross 
Abstract: In this paper, we study the impact of reconfigurable intelligent surfaces (RISs) on the coverage extension of integrated access and backhaul (IAB) networks. Particularly, using a finite stochastic geometry model, with random distributions of user equipments (UEs) in a finite region, and planned hierachical architecture for IAB, we study the service coverage probability defined as the probability of the event that the UEs' minimum rate requirements are satisfied. We present comparisons between different cases including IAB-only, IAB assisted with RIS for backhaul as well as IAB assisted by network controlled repeaters (NCRs). Our investigations focus on wide-area IAB assisted with RIS through the lens of different design architectures and deployments, revealing both conflicts and synergies for minimizing the effect of tree foliage over seasonal changes. Our simulation results reveal both opportunities and challenges towards the implementation of RIS in IAB.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12011v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Charitha Madapatha, Behrooz Makki, Hao Guo, Tommy Svensson</dc:creator>
    </item>
    <item>
      <title>Design and Simulation of the Adaptive Continuous Entanglement Generation Protocol</title>
      <link>https://arxiv.org/abs/2502.01964</link>
      <description>arXiv:2502.01964v2 Announce Type: replace 
Abstract: Generating and distributing remote entangled pairs (EPs) is a primary function of quantum networks, as entanglement is the fundamental resource for key quantum network applications. A critical performance metric for quantum networks is the time-to-serve (TTS) for users' EP requests, which is the time to distribute EPs between the requested nodes. Minimizing the TTS is essential given the limited qubit coherence time. In this paper, we study the Adaptive Continuous entanglement generation Protocol (ACP), which enables quantum network nodes to continuously generate EPs with their neighbors, while adaptively selecting the neighbors to optimize TTS. Meanwhile, entanglement purification is used to mitigate decoherence in pre-generated EPs prior to the arrival of user requests. We extend the SeQUeNCe simulator to fully implement ACP and conduct extensive simulations across various network scales. Our results show that ACP reduces TTS by up to 94% and increases entanglement fidelity by up to 0.05.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.01964v2</guid>
      <category>cs.NI</category>
      <category>quant-ph</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Caitao Zhan, Joaquin Chung, Allen Zang, Alexander Kolar, Rajkumar Kettimuthu</dc:creator>
    </item>
    <item>
      <title>SkyOctopus: Enabling Low-Latency Mobile Satellite Network through Multiple Anchors</title>
      <link>https://arxiv.org/abs/2502.03250</link>
      <description>arXiv:2502.03250v2 Announce Type: replace 
Abstract: The rapid deployment of low earth orbit (LEO) satellite constellations has drawn attention to the potential of nonterrestrial networks (NTN) in providing global communication services. Telecom operators are attempting to collaborate with satellite network providers to develop mobile satellite networks, which serve as an effective supplement to terrestrial networks. However, current mobile satellite network architectures still employ the single-anchor design of terrestrial mobile networks, leading to severely circuitous routing for users and significantly impacting their service experience. To reduce unnecessary latency caused by circuitous routing and provide users with low-latency global internet services, this paper presents SkyOctopus, an advanced multi-anchor mobile satellite network architecture. SkyOctopus innovatively deploys traffic classifiers on satellites to enable connections between users and multiple anchor points distributed globally. It guarantees optimal anchor point selection for each user's target server by monitoring multiple end-to-end paths. We build a prototype of SkyOctopus using enhanced Open5GS and UERANSIM, which is driven by actual LEO satellite constellations such as Starlink, Kuiper, and OneWeb. We conducted extensive experiments, and the results demonstrate that, compared to standard 5G NTN and two other existing schemes, SkyOctopus can reduce end-to-end latency by up to 53\%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03250v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaojie Su, Jiasheng Wu, Zijie Ying, Zhiyuan Zhao, Xiangyu Jia, Wenjun Zhu, Yue Gao</dc:creator>
    </item>
    <item>
      <title>A Fair Federated Learning Framework for Collaborative Network Traffic Prediction and Resource Allocation</title>
      <link>https://arxiv.org/abs/2502.06743</link>
      <description>arXiv:2502.06743v2 Announce Type: replace 
Abstract: In the beyond 5G era, AI/ML empowered realworld digital twins (DTs) will enable diverse network operators to collaboratively optimize their networks, ultimately improving end-user experience. Although centralized AI-based learning techniques have been shown to achieve significant network traffic accuracy, resulting in efficient network operations, they require sharing of sensitive data among operators, leading to privacy and security concerns. Distributed learning, and specifically federated learning (FL), that keeps data isolated at local clients, has emerged as an effective and promising solution for mitigating such concerns. Federated learning poses, however, new challenges in ensuring fairness both in terms of collaborative training contributions from heterogeneous data and in mitigating bias in model predictions with respect to sensitive attributes. To address these challenges, a fair FL framework is proposed for collaborative network traffic prediction and resource allocation. To demonstrate the effectiveness of the proposed approach, noniid and imbalanced federated datasets based on real-word traffic traces are utilized for an elastic optical network. The assumption is that different optical nodes may be managed by different operators. Fairness is evaluated according to the coefficient of variations measure in terms of accuracy across the operators and in terms of quality-of-service across the connections (i.e., reflecting end-user experience). It is shown that fair traffic prediction across the operators result in fairer resource allocations across the connections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06743v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saroj Kumar Panda, Tania Panayiotou, Georgios Ellinas, Sadananda Behera</dc:creator>
    </item>
    <item>
      <title>A Comparative Study of ASR Implementations in Resource-Constrained Wireless Sensor Networks for Real-Time Voice Communication</title>
      <link>https://arxiv.org/abs/2502.06969</link>
      <description>arXiv:2502.06969v2 Announce Type: replace 
Abstract: This paper investigates the challenges and trade-offs associated with implementing Automatic Speech Recognition (ASR) in resource-limited Wireless Sensor Networks (WSNs) for real-time voice communication. We analyze three main architectural approaches: Network Speech Recognition (NSR), Distributed Speech Recognition (DSR), and Embedded Speech Recognition (ESR). Each approach is evaluated based on factors such as bandwidth consumption, processing power requirements, latency, accuracy (Word Error Rate - WER), and adaptability to offline operation. We discuss the advantages and disadvantages of each method, considering the computational and communication limitations of WSN nodes. This comparative study provides insights for selecting the most appropriate ASR implementation strategy based on specific application requirements and resource constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06969v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Inaam F. Qutaiba I. Ali</dc:creator>
    </item>
    <item>
      <title>Generative AI for Deep Reinforcement Learning: Framework, Analysis, and Use Cases</title>
      <link>https://arxiv.org/abs/2405.20568</link>
      <description>arXiv:2405.20568v2 Announce Type: replace-cross 
Abstract: As a form of artificial intelligence (AI) technology based on interactive learning, deep reinforcement learning (DRL) has been widely applied across various fields and has achieved remarkable accomplishments. However, DRL faces certain limitations, including low sample efficiency and poor generalization. Therefore, we present how to leverage generative AI (GAI) to address these issues above and enhance the performance of DRL algorithms in this paper. We first introduce several classic GAI and DRL algorithms and demonstrate the applications of GAI-enhanced DRL algorithms. Then, we discuss how to use GAI to improve DRL algorithms from the data and policy perspectives. Subsequently, we introduce a framework that demonstrates an actual and novel integration of GAI with DRL, i.e., GAI-enhanced DRL. Additionally, we provide a case study of the framework on UAV-assisted integrated near-field/far-field communication to validate the performance of the proposed framework. Moreover, we present several future directions. Finally, the related code is available at: https://xiewenwen22.github.io/GAI-enhanced-DRL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20568v2</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geng Sun, Wenwen Xie, Dusit Niyato, Fang Mei, Jiawen Kang, Hongyang Du, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Utilizing Transaction Prioritization to Enhance Confirmation Speed in the IOTA Network</title>
      <link>https://arxiv.org/abs/2501.16763</link>
      <description>arXiv:2501.16763v2 Announce Type: replace-cross 
Abstract: With the rapid advancement of blockchain technology, a significant trend is the adoption of Directed Acyclic Graphs (DAGs) as an alternative to traditional chain-based architectures for organizing ledger records. Systems like IOTA, which are specially designed for the Internet of Things (IoT), leverage DAG-based architectures to achieve greater scalability by enabling multiple attachment points in the ledger for new transactions while allowing these transactions to be added to the network without incurring any fees. To determine these attachment points, many tip selection algorithms commonly employ specific strategies on the DAG ledger. Transaction prioritization is not considered in the IOTA network, which becomes especially important when network bandwidth is limited. In this paper, we propose an optimization framework designed to integrate a priority level for critical or high-priority IoT transactions within the IOTA network. We evaluate our system using fully based on the official IOTA GitHub repository, which employs the currently operational IOTA node software (Hornet version), as part of the Chrysalis update (1.5). The experimental results show that higher-priority transactions in the proposed algorithm reach final confirmation in less time compared to the original IOTA system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.16763v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seyyed Ali Aghamiri, Reza Sharifnia, Ahmad Khonsari</dc:creator>
    </item>
    <item>
      <title>Future Resource Bank for ISAC: Achieving Fast and Stable Win-Win Matching for Both Individuals and Coalitions</title>
      <link>https://arxiv.org/abs/2502.08118</link>
      <description>arXiv:2502.08118v2 Announce Type: replace-cross 
Abstract: Future wireless networks must support emerging applications where environmental awareness is as critical as data transmission. Integrated Sensing and Communication (ISAC) enables this vision by allowing base stations (BSs) to allocate bandwidth and power to mobile users (MUs) for communications and cooperative sensing. However, this resource allocation is highly challenging due to: (i) dynamic resource demands from MUs and resource supply from BSs, and (ii) the selfishness of MUs and BSs. To address these challenges, existing solutions rely on either real-time (online) resource trading, which incurs high overhead and failures, or static long-term (offline) resource contracts, which lack flexibility. To overcome these limitations, we propose the Future Resource Bank for ISAC, a hybrid trading framework that integrates offline and online resource allocation through a level-wise client model, where MUs and their coalitions negotiate with BSs. We introduce two mechanisms: (i) Role-Friendly Win-Win Matching (offRFW$^2$M), leveraging overbooking to establish risk-aware, stable contracts, and (ii) Effective Backup Win-Win Matching (onEBW$^2$M), which dynamically reallocates unmet demand and surplus supply. We theoretically prove stability, individual rationality, and weak Pareto optimality of these mechanisms. Through simulations, we show that our framework improves social welfare, latency, and energy efficiency compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08118v2</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Houyi Qi, Minghui Liwang, Seyyedali Hosseinalipour, Liqun Fu, Sai Zou, Wei Ni</dc:creator>
    </item>
    <item>
      <title>TrustZero - open, verifiable and scalable zero-trust</title>
      <link>https://arxiv.org/abs/2502.10281</link>
      <description>arXiv:2502.10281v2 Announce Type: replace-cross 
Abstract: We present a passport-level trust token for Europe. In an era of escalating cyber threats fueled by global competition in economic, military, and technological domains, traditional security models are proving inadequate. The rise of advanced attacks exploiting zero-day vulnerabilities, supply chain infiltration, and system interdependencies underscores the need for a paradigm shift in cybersecurity. Zero Trust Architecture (ZTA) emerges as a transformative framework that replaces implicit trust with continuous verification of identity and granular access control. This thesis introduces TrustZero, a scalable layer of zero-trust security built around a universal "trust token" - a non-revocable self-sovereign identity with cryptographic signatures to enable robust, mathematically grounded trust attestations. By integrating ZTA principles with cryptography, TrustZero establishes a secure web-of-trust framework adaptable to legacy systems and inter-organisational communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10281v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 18 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrian-Tudor Dumitrescu, Johan Pouwelse</dc:creator>
    </item>
  </channel>
</rss>
