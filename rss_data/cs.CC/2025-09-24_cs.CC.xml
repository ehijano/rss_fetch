<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Sep 2025 01:44:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Circuit Complexity From Physical Constraints: Scaling Limitations of Attention</title>
      <link>https://arxiv.org/abs/2509.19161</link>
      <description>arXiv:2509.19161v1 Announce Type: new 
Abstract: We argue that the standard circuit complexity measures derived from $NC, AC, TC$ provide limited practical information and are now insufficient to further differentiate model expressivity. To address these new limitations, we define a novel notion of local uniformity and a family of circuit complexity classes $RC(\cdot)$ that capture the fundamental constraints of scaling physical circuits. Through the lens of $RC(\cdot)$, we show that attention mechanisms with $\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy of increasingly complex datasets. Our results simultaneously provide a methodology for defining meaningful bounds on transformer expressivity and naturally expose the restricted viability of attention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.19161v1</guid>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Prada, Ankur Mali</dc:creator>
    </item>
    <item>
      <title>Singleton algorithms for the Constraint Satisfaction Problem</title>
      <link>https://arxiv.org/abs/2509.18434</link>
      <description>arXiv:2509.18434v1 Announce Type: cross 
Abstract: A natural strengthening of an algorithm for the (promise) constraint satisfaction problem is its singleton version: we first fix a constraint to some tuple from the constraint relation, then run the algorithm, and remove the tuple from the constraint if the answer is negative. We characterize the power of the singleton versions of standard universal algorithms for the (promise) CSP over a fixed template in terms of the existence of a minion homomorphism. Using the Hales-Jewett theorem, we show that for finite relational structures this minion condition is equivalent to the existence of polymorphisms with certain symmetries, called palette block symmetric polymorphisms. By proving the existence of such polymorphisms we establish that the singleton version of the BLP+AIP algorithm solves all tractable CSPs over domains of size at most 7. Finally, by providing concrete CSP templates, we illustrate the limitations of linear programming, the power of the singleton versions, and the elegance of the palette block symmetric polymorphisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18434v1</guid>
      <category>cs.LO</category>
      <category>cs.CC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dmitriy Zhuk</dc:creator>
    </item>
    <item>
      <title>Precoloring extension with demands on paths</title>
      <link>https://arxiv.org/abs/2509.18936</link>
      <description>arXiv:2509.18936v1 Announce Type: cross 
Abstract: Let $G$ be a graph with a set of precolored vertices, and let us be given an integer distance parameter $d$ and a set of integer demands $d_1,\dots,d_c$. The Distance Precoloring Extension with Demands (DPED) problem is to compute a vertex $c$-coloring of $G$ such that the following three conditions hold: (i) the resulting coloring respects the colors of the precolored vertices, (ii) the distance of two vertices of the same color is at least $d$, and (iii) the number of vertices colored by color $i$ is exactly $d_i$. This problem is motivated by a program scheduling in commercial broadcast channels with constraints on content repetition and placement, which leads precisely to the DPED problem for paths.
  In this paper, we study DPED on paths and present a polynomial time exact algorithm when precolored vertices are restricted to the two ends of the path and devise an approximation algorithm for DPED with an additive approximation factor polynomially bounded by $d$ and the number of precolored vertices. Then, we prove that the Distance Precoloring Extension problem on paths, a less restrictive version of DPED without the demand constraints, and then DPED itself, is NP-complete. Motivated by this result, we further study the parameterized complexity of DPED on paths. We establish that the DPED problem on paths is $W[1]$-hard when parameterized by the number of colors and the distance. On the positive side, we devise a fixed parameter tractable (FPT) algorithm for DPED on paths when the number of colors, the distance, and the number of precolored vertices are considered as the parameters. Moreover, we prove that Distance Precoloring Extension is FPT parameterized by the distance. As a byproduct, we also obtain several results for the Distance List Coloring problem on paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18936v1</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4230/LIPIcs.ISAAC.2025.43</arxiv:DOI>
      <dc:creator>Arun Kumar Das, Michal Opler, Tom\'a\v{s} Valla</dc:creator>
    </item>
    <item>
      <title>On Inapproximability of Reconfiguration Problems: PSPACE-Hardness and some Tight NP-Hardness Results</title>
      <link>https://arxiv.org/abs/2312.17140</link>
      <description>arXiv:2312.17140v3 Announce Type: replace 
Abstract: Recently, Ohsaka [STACS'23] put forth the Reconfiguration Inapproximability Hypothesis (RIH), which roughly asserts that there is some $\epsilon&gt;0$ such that given as input a $k$-CSP instance (for some constant $k$) over some constant sized alphabet, and two satisfying assignments $\psi_s$ and $\psi_t$, it is PSPACE-hard to find a sequence of assignments starting from $\psi_s$ and ending at $\psi_t$ such that every assignment in the sequence satisfies at least $(1-\epsilon)$ fraction of the constraints and also that every assignment in the sequence is obtained by changing its immediately preceding assignment (in the sequence) on exactly one variable. Assuming RIH, many important reconfiguration problems have been shown to be PSPACE-hard to approximate by Ohsaka [STACS'23; SODA'24].
  In this paper, we provide a proof of RIH. Our proof uses known constructions of PCP of Proximity to create the gap, and further leverages a parallelization framework from recent parameterized inapproximability results to analyze the quantitative trade-off between $\epsilon$ and $k$ in RIH. We note that Hirahara and Ohsaka [STOC'24] have also independently proved RIH.
  We also prove that the aforementioned $k$-CSP Reconfiguration problem is NP-hard to approximate to within a factor of $1/2 + \epsilon$ (for any $\epsilon&gt;0$) when $k=2$. We complement this with a polynomial time $(1/2 - \epsilon)$-approximation algorithm, which improves upon a $(1/4 - \epsilon)$-approximation algorithm of Ohsaka [2023] (again for any $\epsilon&gt;0$). Finally, we show that Set Cover Reconfiguration is NP-hard to approximate to within a factor of $2 - \epsilon$ for any constant $\epsilon &gt; 0$, which matches the simple linear-time 2-approximation algorithm by Ito et al. [TCS'11].</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.17140v3</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Venkatesan Guruswami, Karthik C. S., Pasin Manurangsi, Xuandi Ren, Kewen Wu</dc:creator>
    </item>
    <item>
      <title>Near Optimal Alphabet-Soundness Tradeoff PCPs</title>
      <link>https://arxiv.org/abs/2404.07441</link>
      <description>arXiv:2404.07441v2 Announce Type: replace 
Abstract: We show that for all $\varepsilon&gt;0$, for sufficiently large $q\in\mathbb{N}$ power of $2$, for all $\delta&gt;0$, it is NP-hard to distinguish whether a given $2$-Prover-$1$-Round projection game with alphabet size $q$ has value at least $1-\delta$, or value at most $1/q^{1-\varepsilon}$. This establishes a nearly optimal alphabet-to-soundness tradeoff for $2$-query PCPs with alphabet size $q$, improving upon a result of [Chan, J.ACM 2016]. Our result has the following implications:
  1) Near optimal hardness for Quadratic Programming: it is NP-hard to approximate the value of a given Boolean Quadratic Program within factor $(\log n)^{1 - o(1)}$ under quasi-polynomial time reductions. This improves upon a result of [Khot, Safra, ToC 2013] and nearly matches the performance of the best known algorithms due to [Megretski, IWOTA 2000], [Nemirovski, Roos, Terlaky, Mathematical programming 1999] and [Charikar, Wirth, FOCS 2004] that achieve $O(\log n)$ approximation ratio.
  2) Bounded degree $2$-CSPs: under randomized reductions, for sufficiently large $d&gt;0$, it is NP-hard to approximate the value of $2$-CSPs in which each variable appears in at most $d$ constraints within factor $(1-o(1))\frac{d}{2}$, improving upon a result of [Lee, Manurangsi, ITCS 2024].
  3) Improved hardness results for connectivity problems: using results of [Laekhanukit, SODA 2014] and [Manurangsi, Inf. Process. Lett., 2019], we deduce improved hardness results for the Rooted $k$-Connectivity Problem, the Vertex-Connectivity Survivable Network Design Problem and the Vertex-Connectivity $k$-Route Cut Problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07441v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dor Minzer, Kai Zhe Zheng</dc:creator>
    </item>
    <item>
      <title>Constant-Depth Arithmetic Circuits for Linear Algebra Problems</title>
      <link>https://arxiv.org/abs/2404.10839</link>
      <description>arXiv:2404.10839v2 Announce Type: replace 
Abstract: We design polynomial size, constant depth (namely, $\mathsf{AC}^0$) arithmetic formulae for the greatest common divisor (GCD) of two polynomials, as well as the related problems of the discriminant, resultant, B\'ezout coefficients, squarefree decomposition, and the inversion of structured matrices like Sylvester and B\'ezout matrices. Our GCD algorithm extends to any number of polynomials. Previously, the best known arithmetic formulae for these problems required super-polynomial size, regardless of depth.
  These results are based on new algorithmic techniques to compute various symmetric functions in the roots of polynomials, as well as manipulate the multiplicities of these roots, without having access to them. These techniques allow $\mathsf{AC}^0$ computation of a large class of linear and polynomial algebra problems, which include the above as special cases.
  We extend these techniques to problems whose inputs are multivariate polynomials, which are represented by $\mathsf{AC}^0$ arithmetic circuits. Here too we solve problems such as computing the GCD and squarefree decomposition in $\mathsf{AC}^0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10839v2</guid>
      <category>cs.CC</category>
      <category>cs.SC</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/FOCS61266.2024.00138</arxiv:DOI>
      <dc:creator>Robert Andrews, Avi Wigderson</dc:creator>
    </item>
    <item>
      <title>Maximum Reachability Orientation of Mixed Graphs</title>
      <link>https://arxiv.org/abs/2506.16171</link>
      <description>arXiv:2506.16171v2 Announce Type: replace 
Abstract: We aim to find orientations of mixed graphs optimizing the total reachability, a problem that has applications in causality and biology. For given a digraph $D$, we use $P(D)$ for the set of ordered pairs of distinct vertices in $V(D)$ and we define $\kappa_D:P(D)\rightarrow \{0,1\}$ by $\kappa_D(u,v)=1$ if $v$ is reachable from $u$ in $D$, and $\kappa_D(u,v)=0$, otherwise. We use $R(D)=\sum_{(u,v)\in P(D)}\kappa_D(u,v)$.
  Now, given a mixed graph $G$, we aim to find an orientation $\vec{G}$ of $G$ that maximizes $R(\vec{G})$. Hakimi, Schmeichel, and Young proved that the problem can be solved in polynomial time when restricted to undirected inputs. They inquired about the complexity in mixed graphs.
  We answer this question by showing that this problem is NP-hard, and, moreover, APX-hard.
  We then develop a finer understanding of how quickly the problem becomes difficult when going from undirected to mixed graphs. To this end, we consider the parameterized complexity of the problem with respect to the number $k$ of preoriented arcs of $G$, a poorly understood form of parameterization.
  We show that the problem can be solved in time $n^{O(k)}$ and that a $(1-\epsilon)$-approximation can be computed in time $f(k,\epsilon)n^{O(1)}$ for any $\epsilon &gt; 0$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16171v2</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian H\"orsch</dc:creator>
    </item>
    <item>
      <title>Rice-like complexity lower bounds for Boolean and uniform automata networks</title>
      <link>https://arxiv.org/abs/2409.08762</link>
      <description>arXiv:2409.08762v2 Announce Type: replace-cross 
Abstract: Automata networks are a versatile model of finite discrete dynamical systems composed of interacting entities (the automata), able to embed any directed graph as a dynamics on its space of configurations (the set of vertices, representing all the assignments of a state to each entity). In this world, virtually any question is decidable by a simple exhaustive search. We lever the Rice-like complexity lower bound, stating that any non-trivial monadic second order logic question on the graph of its dynamics is NP-hard or coNP-hard (given the automata network description), to bounded alphabets (including the Boolean case). This restriction is particularly meaningful for applications to "complex systems", where each entity has a restricted set of possible states (its alphabet). For the deterministic case, trivial questions are solvable in constant time, hence there is a sharp gap in complexity for the algorithmic solving of concrete problems on them. For the non-deterministic case, non-triviality is defined at bounded cliquewidth, which offers a structure to establish metatheorems of complexity lower bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08762v2</guid>
      <category>cs.DM</category>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ali\'enor Goubault-Larrecq, K\'evin Perrot</dc:creator>
    </item>
    <item>
      <title>Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory</title>
      <link>https://arxiv.org/abs/2509.18057</link>
      <description>arXiv:2509.18057v2 Announce Type: replace-cross 
Abstract: We explore whether techniques from AI can help discover new combinatorial structures that improve on known limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings:
  a) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place.
  b) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from "standard" H{\aa}stad-style PCPs.
  A key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18057v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CC</category>
      <category>math.CO</category>
      <pubDate>Wed, 24 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ansh Nagda, Prabhakar Raghavan, Abhradeep Thakurta</dc:creator>
    </item>
  </channel>
</rss>
