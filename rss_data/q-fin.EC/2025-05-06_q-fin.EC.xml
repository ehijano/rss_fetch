<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 May 2025 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Examining gender and cultural influences on customer emotions</title>
      <link>https://arxiv.org/abs/2505.02852</link>
      <description>arXiv:2505.02852v1 Announce Type: new 
Abstract: Understanding consumer emotional experiences on e-commerce platforms is essential for businesses striving to enhance customer engagement and personalisation. Recent research has demonstrated that these experiences are more intricate and diverse than previously examined, encompassing a wider range of discrete emotions and spanning multiple-dimensional scales. This study examines how gender and cultural differences shape these complex emotional responses, revealing significant variations between male and female consumers across all sentiment, valence, arousal, and dominance scores. Additionally, clear cultural distinctions emerge, with Western and Eastern consumers displaying markedly different emotional behaviours across the larger spectrum of emotions, including admiration, amusement, approval, caring, curiosity, desire, disappointment, optimism, and pride. Furthermore, the study uncovers a critical interaction between gender and culture in shaping consumer emotions. Notably, gender-based emotional disparities are more pronounced in Western cultures than in Eastern ones, an aspect that has been largely overlooked in previous research. From a theoretical perspective, this study advances the understanding of gender and cultural variations in online consumer behaviour by integrating insights from neuroscience theories and Hofstede cultural dimension model. Practically, it offers valuable guidance for businesses, equipping them with the tools to more accurately interpret customer feedback, refine sentiment and emotional analysis models, and develop personalised marketing strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02852v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vinh Truong (RMIT University)</dc:creator>
    </item>
    <item>
      <title>The quest for explosive bubbles in the Indonesian Rupiah/US exchange rate: Does the uncertainty trinity matter?</title>
      <link>https://arxiv.org/abs/2505.02869</link>
      <description>arXiv:2505.02869v1 Announce Type: new 
Abstract: The Generalized Supremum Augmented Dickey-Fuller (GSADF) technique is performed to resolve whether the Indonesian Rupiah/US exchange rate has experienced multiple explosive bubbles. The GSADF uncovers that the Indonesian Rupiah/US exchange rate deviates from the fundamental values by six times from January 1985 to September 2023, periodically indicating the presence of numerous explosive behaviors. Once the full-sample period separates into the managed-floating regime and the free-floating regime, the GSADF still detects multiple bubbles. Of particular curiosity on uncertainty trinity, this study underlines that global geopolitical risk negatively drives explosive actions in the ratio of exchange rates for non-traded and traded goods. The global economic policy uncertainty negatively affects speculative bubbles in the exchange rate and the ratio of exchange rates for non-traded. The country's geopolitical risks negatively strike only speculative bubbles in the exchange rate. Further, we find heterogeneity in our results by examining different exchange rate systems. The robustness checks further firmly ascertain across baseline empirical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02869v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5267/j.dsl.2024.1.005</arxiv:DOI>
      <dc:creator>Abdul Khaliq, Syafruddin Karimi, Werry Darta Taifur, Endrizal Ridwan</dc:creator>
    </item>
    <item>
      <title>Strategic Effort and Bandwagon Effects in Finite Multi-Stage Games with Non-Linear Externalities: Evidence from Triathlon</title>
      <link>https://arxiv.org/abs/2505.03247</link>
      <description>arXiv:2505.03247v1 Announce Type: new 
Abstract: This paper examines strategic effort and positioning choices resulting in bandwagon effects under externalities in finite multi-stage games using causal evidence from triathlon (Reichel, 2025). Focusing on open-water swim draftingwhere athletes reduce drag most effectively by swimming directly behind peerswe estimate its performance effects through a structural contest framework with endogenous, deterministic effort and drafting position. Leveraging exogenous variation from COVID-19 drafting bans in Austrian triathlons, we apply a panel leave-one-out (LOO/LOTO) peer ability instrumental variables (IV) strategy to isolate the causal non-linear effect of drafting. Results from restricted sample analysis and pooled estimated bandwagon IV effects show substantial and nonlinear gains: in small (group size below 10) drafting swim groups/clusters, each deeper position improves finishing rank on average by over 30%, with rapidly diminishing returns in larger groups. Leading however is consistently more costly than optimal positioning, aligning with theoretical predictions of energy expenditure (metabolic costs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03247v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Learning by exporting with a dose-response function</title>
      <link>https://arxiv.org/abs/2505.03328</link>
      <description>arXiv:2505.03328v1 Announce Type: new 
Abstract: This paper investigates the causal effect of export intensity on productivity and other firm-level outcomes with a dose-response function. After positing that export intensity acts as a continuous treatment, we investigate counterfactual productivity levels in a quasi-experimental setting. For our purpose, we exploit a control group of non-temporary exporters that have already sustained the fixed costs of reaching foreign markets, thus controlling for self-selection into exporting. Our findings reveal a non-linear relationship between export intensity and productivity, with small albeit statistically significant benefits ranging from 0.1% to 0.6% per year only after exports reach 60% of total revenues. After we look at sales, variable costs, capital intensity, and the propensity to filing patents, we show that, before the 60% threshold, economies of scale and capital adjustment offset each other and induce, on average, a minimal albeit statistically significant loss in productivity of about 0.01% per year. Crucially, we find that heterogeneous export intensity is associated with the firm's position on the technological frontier, as the propensity to file a patent increases when export intensity ranges in 8%-60% with a peak at 40%. The latest finding further highlights that learning-by-exporting is linked to the building of absorptive capacity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03328v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giovanni Cerulli, Francesca Micocci, Armando Rungi</dc:creator>
    </item>
    <item>
      <title>Who Flees Conflict?</title>
      <link>https://arxiv.org/abs/2505.03405</link>
      <description>arXiv:2505.03405v1 Announce Type: new 
Abstract: Despite the growing numbers of forcibly displaced persons worldwide, many people living under conflict choose not to flee. Individuals face two lotteries - staying or leaving - characterized by two distributions of potential outcomes. This paper proposes to model the choice between these two lotteries using quantile maximization as opposed to expected utility theory. The paper posits that risk-averse individuals aim at minimizing losses by choosing the lottery with the best outcome at the lower end of the distribution, whereas risk-tolerant individuals aim at maximizing gains by choosing the lottery with the best outcome at the higher end of the distribution. Using a rich set of household and conflict panel data from Nigeria, the paper finds that risk-tolerant individuals have a significant preference for staying and risk-averse individuals have a significant preference for fleeing, in line with the predictions of the quantile maximization model. These findings are in contrast to findings on economic migrants, and call for separate policies toward economic and forced migrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03405v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lidia Ceriani, Paolo Verme</dc:creator>
    </item>
    <item>
      <title>The Precautionary Principle and the Innovation Principle: Incompatible Guides for AI Innovation Governance?</title>
      <link>https://arxiv.org/abs/2505.02846</link>
      <description>arXiv:2505.02846v1 Announce Type: cross 
Abstract: In policy debates concerning the governance and regulation of Artificial Intelligence (AI), both the Precautionary Principle (PP) and the Innovation Principle (IP) are advocated by their respective interest groups. Do these principles offer wholly incompatible and contradictory guidance? Does one necessarily negate the other? I argue here that provided attention is restricted to weak-form PP and IP, the answer to both of these questions is "No." The essence of these weak formulations is the requirement to fully account for type-I error costs arising from erroneously preventing the innovation's diffusion through society (i.e. mistaken regulatory red-lighting) as well as the type-II error costs arising from erroneously allowing the innovation to diffuse through society (i.e. mistaken regulatory green-lighting). Within the Signal Detection Theory (SDT) model developed here, weak-PP red-light (weak-IP green-light) determinations are optimal for sufficiently small (large) ratios of expected type-I to type-II error costs. For intermediate expected cost ratios, an amber-light 'wait-and-monitor' policy is optimal. Regulatory sandbox instruments allow AI testing and experimentation to take place within a structured environment of limited duration and societal scale, whereby the expected cost ratio falls within the 'wait-and-monitor' range. Through sandboxing regulators and innovating firms learn more about the expected cost ratio, and what respective adaptations -- of regulation, of technical solution, of business model, or combination thereof, if any -- are needed to keep the ratio out of the weak-PP red-light zone.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02846v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Kim Kaivanto</dc:creator>
    </item>
    <item>
      <title>Deep Learning in Renewable Energy Forecasting: A Cross-Dataset Evaluation of Temporal and Spatial Models</title>
      <link>https://arxiv.org/abs/2505.03109</link>
      <description>arXiv:2505.03109v1 Announce Type: cross 
Abstract: Unpredictability of renewable energy sources coupled with the complexity of those methods used for various purposes in this area calls for the development of robust methods such as DL models within the renewable energy domain. Given the nonlinear relationships among variables in renewable energy datasets, DL models are preferred over traditional machine learning (ML) models because they can effectively capture and model complex interactions between variables. This research aims to identify the factors responsible for the accuracy of DL techniques, such as sampling, stationarity, linearity, and hyperparameter optimization for different algorithms. The proposed DL framework compares various methods and alternative training/test ratios. Seven ML methods, such as Long-Short Term Memory (LSTM), Stacked LSTM, Convolutional Neural Network (CNN), CNN-LSTM, Deep Neural Network (DNN), Multilayer Perceptron (MLP), and Encoder-Decoder (ED), were evaluated on two different datasets. The first dataset contains the weather and power generation data. It encompasses two distinct datasets, hourly energy demand data and hourly weather data in Spain, while the second dataset includes power output generated by the photovoltaic panels at 12 locations. This study deploys regularization approaches, including early stopping, neuron dropping, and L2 regularization, to reduce the overfitting problem associated with DL models. The LSTM and MLP models show superior performance. Their validation data exhibit exceptionally low root mean square error values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03109v1</guid>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lutfu Sua, Haibo Wang, Jun Huang</dc:creator>
    </item>
    <item>
      <title>Multi-Agent Deep Reinforcement Learning for Zonal Ancillary Market Coupling</title>
      <link>https://arxiv.org/abs/2505.03288</link>
      <description>arXiv:2505.03288v1 Announce Type: cross 
Abstract: We characterize zonal ancillary market coupling relying on noncooperative game theory. To that purpose, we formulate the ancillary market as a multi-leader single follower bilevel problem, that we subsequently cast as a generalized Nash game with side constraints and nonconvex feasibility sets. We determine conditions for equilibrium existence and show that the game has a generalized potential game structure. To compute market equilibrium, we rely on two exact approaches: an integrated optimization approach and Gauss-Seidel best-response, that we compare against multi-agent deep reinforcement learning. On real data from Germany and Austria, simulations indicate that multi-agent deep reinforcement learning achieves the smallest convergence rate but requires pretraining, while best-response is the slowest. On the economics side, multi-agent deep reinforcement learning results in smaller market costs compared to the exact methods, but at the cost of higher variability in the profit allocation among stakeholders. Further, stronger coupling between zones tends to reduce costs for larger zones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.03288v1</guid>
      <category>cs.MA</category>
      <category>cs.GT</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francesco Morri, H\'el\`ene Le Cadre, Pierre Gruet, Luce Brotcorne</dc:creator>
    </item>
    <item>
      <title>Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal Choice</title>
      <link>https://arxiv.org/abs/2405.19313</link>
      <description>arXiv:2405.19313v2 Announce Type: replace-cross 
Abstract: The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice -- where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19313v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths</dc:creator>
    </item>
    <item>
      <title>Why do financial prices exhibit Brownian motion despite predictable order flow?</title>
      <link>https://arxiv.org/abs/2502.17906</link>
      <description>arXiv:2502.17906v3 Announce Type: replace-cross 
Abstract: In financial market microstructure, there are two enigmatic empirical laws: (i) the market-order flow has predictable persistence due to metaorder splitters by institutional investors, well formulated as the Lillo-Mike-Farmer model. However, this phenomenon seems paradoxical given the diffusive and unpredictable price dynamics; (ii) the price impact $I(Q)$ of a large metaorder $Q$ follows the square-root law, $I(Q)\propto \sqrt{Q}$. Here we theoretically reveal why price dynamics follows Brownian motion despite predictable order flow by unifying these enigmas. We generalize the Lillo-Mike-Farmer model to nonlinear price-impact dynamics, which is mapped to an exactly solvable L\'evy-walk model. Our exact solution shows that the price dynamics remains diffusive under the square-root law, even under persistent order flow. This work illustrates the crucial role of the square-root law in mitigating large price movements by large metaorders, thereby leading to the Brownian price dynamics, consistently with the efficient market hypothesis over long timescales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17906v3</guid>
      <category>q-fin.TR</category>
      <category>cond-mat.stat-mech</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>q-fin.MF</category>
      <category>q-fin.PR</category>
      <pubDate>Wed, 07 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuki Sato, Kiyoshi Kanazawa</dc:creator>
    </item>
  </channel>
</rss>
