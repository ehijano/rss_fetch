<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 08 Aug 2024 01:31:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 07 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On The Stability of Moral Preferences: A Problem with Computational Elicitation Methods</title>
      <link>https://arxiv.org/abs/2408.02862</link>
      <description>arXiv:2408.02862v1 Announce Type: new 
Abstract: Preference elicitation frameworks feature heavily in the research on participatory ethical AI tools and provide a viable mechanism to enquire and incorporate the moral values of various stakeholders. As part of the elicitation process, surveys about moral preferences, opinions, and judgments are typically administered only once to each participant. This methodological practice is reasonable if participants' responses are stable over time such that, all other relevant factors being held constant, their responses today will be the same as their responses to the same questions at a later time. However, we do not know how often that is the case. It is possible that participants' true moral preferences change, are subject to temporary moods or whims, or are influenced by environmental factors we don't track. If participants' moral responses are unstable in such ways, it would raise important methodological and theoretical issues for how participants' true moral preferences, opinions, and judgments can be ascertained. We address this possibility here by asking the same survey participants the same moral questions about which patient should receive a kidney when only one is available ten times in ten different sessions over two weeks, varying only presentation order across sessions. We measured how often participants gave different responses to simple (Study One) and more complicated (Study Two) repeated scenarios. On average, the fraction of times participants changed their responses to controversial scenarios was around 10-18% across studies, and this instability is observed to have positive associations with response time and decision-making difficulty. We discuss the implications of these results for the efficacy of moral preference elicitation, highlighting the role of response instability in causing value misalignment between stakeholders and AI tools trained on their moral judgments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02862v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyle Boerstler, Vijay Keswani, Lok Chan, Jana Schaich Borg, Vincent Conitzer, Hoda Heidari, Walter Sinnott-Armstrong</dc:creator>
    </item>
    <item>
      <title>Fake News Detection via Wisdom of Synthetic &amp; Representative Crowds</title>
      <link>https://arxiv.org/abs/2408.03154</link>
      <description>arXiv:2408.03154v1 Announce Type: new 
Abstract: Social media companies have struggled to provide a democratically legitimate definition of "Fake News". Reliance on expert judgment has attracted criticism due to a general trust deficit and political polarisation. Approaches reliant on the ``wisdom of the crowds'' are a cost-effective, transparent and inclusive alternative. This paper provides a novel end-to-end methodology to detect fake news on X via "wisdom of the synthetic &amp; representative crowds". We deploy an online survey on the Lucid platform to gather veracity assessments for a number of pandemic-related tweets from crowd-workers. Borrowing from the MrP literature, we train a Hierarchical Bayesian model to predict the veracity of each tweet from the perspective of different personae from the population of interest.
  We then weight the predicted veracity assessments according to a representative stratification frame, such that decisions about ``fake'' tweets are representative of the overall polity of interest. Based on these aggregated scores, we analyse a corpus of tweets and perform a second MrP to generate state-level estimates of the number of people who share fake news. We find small but statistically meaningful heterogeneity in fake news sharing across US states. At the individual-level: i. sharing fake news is generally rare, with an average sharing probability interval [0.07,0.14]; ii. strong evidence that Democrats share less fake news, accounting for a reduction in the sharing odds of [57.3%,3.9%] relative to the average user; iii. when Republican definitions of fake news are used, it is the latter who show a decrease in the propensity to share fake news worth [50.8%, 2.0%]; iv. some evidence that women share less fake news than men, an effect worth a [29.5%,4.9%] decrease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03154v1</guid>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois t'Serstevens, Roberto Cerina, Giulia Piccillo</dc:creator>
    </item>
    <item>
      <title>Narrowband-IoT (NB-IoT) and IoT Use Cases in Universities, Campuses, and Educational Institutions: A Research Analysis</title>
      <link>https://arxiv.org/abs/2408.03157</link>
      <description>arXiv:2408.03157v1 Announce Type: new 
Abstract: The main objective of this research paper is to analyze the available use cases of Narrowband-IoT and IoT in universities, campuses, and educational institutions. A literature review was conducted using multiple databases such as IEEE Xplore, ACM Digital Library, and Scopus. The study explores the benefits of IoT adoption in higher education. Various use cases of NB-IoT in educational institutions were analyzed, including smart campus management, asset tracking, monitoring, and safety and security systems. Of the six use cases assessed, three focused on the deployment of IoT Things, while three focused on NB-IoT Connectivity. The research paper concludes that NB-IoT technology has significant potential to enhance various aspects of educational institutions, from smart campus management to improving safety and security systems. The study recommends further exploration and implementation of NB-IoT technology in educational settings to improve efficiency, security, and overall campus management. The research highlights the potential applications of NB-IoT in universities and educational institutions, paving the way for future studies in this area. The social implications of this research could involve enhancing the overall learning experience for students, improving campus safety, and promoting technological advancements in educational settings.
  Keywords: narrowband-IoT, Internet-of-Things, smart campus, smart institutions</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03157v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.25147/ijcsr.2017.001.1.202</arxiv:DOI>
      <dc:creator>Lyberius Ennio F. Taruc, Arvin R. De La Cruz</dc:creator>
    </item>
    <item>
      <title>On Biases in a UK Biobank-based Retinal Image Classification Model</title>
      <link>https://arxiv.org/abs/2408.02676</link>
      <description>arXiv:2408.02676v1 Announce Type: cross 
Abstract: Recent work has uncovered alarming disparities in the performance of machine learning models in healthcare. In this study, we explore whether such disparities are present in the UK Biobank fundus retinal images by training and evaluating a disease classification model on these images. We assess possible disparities across various population groups and find substantial differences despite strong overall performance of the model. In particular, we discover unfair performance for certain assessment centres, which is surprising given the rigorous data standardisation protocol. We compare how these differences emerge and apply a range of existing bias mitigation methods to each one. A key insight is that each disparity has unique properties and responds differently to the mitigation methods. We also find that these methods are largely unable to enhance fairness, highlighting the need for better bias mitigation methods tailored to the specific type of bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02676v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>eess.IV</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anissa Alloula, Rima Mustafa, Daniel R McGowan, Bart{\l}omiej W. Papie\.z</dc:creator>
    </item>
    <item>
      <title>Patient-centered data science: an integrative framework for evaluating and predicting clinical outcomes in the digital health era</title>
      <link>https://arxiv.org/abs/2408.02677</link>
      <description>arXiv:2408.02677v1 Announce Type: cross 
Abstract: This study proposes a novel, integrative framework for patient-centered data science in the digital health era. We developed a multidimensional model that combines traditional clinical data with patient-reported outcomes, social determinants of health, and multi-omic data to create comprehensive digital patient representations. Our framework employs a multi-agent artificial intelligence approach, utilizing various machine learning techniques including large language models, to analyze complex, longitudinal datasets. The model aims to optimize multiple patient outcomes simultaneously while addressing biases and ensuring generalizability. We demonstrate how this framework can be implemented to create a learning healthcare system that continuously refines strategies for optimal patient care. This approach has the potential to significantly improve the translation of digital health innovations into real-world clinical benefits, addressing current limitations in AI-driven healthcare models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02677v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohsen Amoei, Dan Poenaru</dc:creator>
    </item>
    <item>
      <title>Assessing the Effects of Container Handling Strategies on Enhancing Freight Throughput</title>
      <link>https://arxiv.org/abs/2408.02768</link>
      <description>arXiv:2408.02768v1 Announce Type: cross 
Abstract: As global supply chains and freight volumes grow, the U.S. faces escalating transportation demands. The heavy reliance on road transport, coupled with the underutilization of the railway system, results in congested highways, prolonged transportation times, higher costs, and increased carbon emissions. California's San Pedro Port Complex (SPPC), the nation's busiest, incurs a significant share of these challenges. We utilize an agent-based simulation to replicate real-world scenarios, focusing on the intricacies of interactions in a modified intermodal inbound freight system for the SPPC. This involves relocating container classification to potential warehouses in California, Utah, Arizona, and Nevada, rather than exclusively at port areas. Our primary aim is to evaluate the proposed system's efficiency, considering cost and freight throughput, while also examining the effects of workforce shortages. Computational analysis suggests that strategically installing intermodal capabilities in select warehouses can reduce transportation costs, boost throughput, and foster resour</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02768v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sarita Rattanakunuprakarn, Mingzhou Jin, Mustafa Can Camur, Xueping Li</dc:creator>
    </item>
    <item>
      <title>Mixing Individual and Collective Behaviours to Predict Out-of-Routine Mobility</title>
      <link>https://arxiv.org/abs/2404.02740</link>
      <description>arXiv:2404.02740v2 Announce Type: replace 
Abstract: Predicting human displacements is crucial for addressing various societal challenges, including urban design, traffic congestion, epidemic management, and migration dynamics. While predictive models like deep learning and Markov models offer insights into individual mobility, they often struggle with out-of-routine behaviours. Our study introduces an approach that dynamically integrates individual and collective mobility behaviours, leveraging collective intelligence to enhance prediction accuracy. Evaluating the model on millions of privacy-preserving trajectories across three US cities, we demonstrate its superior performance in predicting out-of-routine mobility, surpassing even advanced deep learning methods. Spatial analysis highlights the model's effectiveness near urban areas with a high density of points of interest, where collective behaviours strongly influence mobility. During disruptive events like the COVID-19 pandemic, our model retains predictive capabilities, unlike individual-based models. By bridging the gap between individual and collective behaviours, our approach offers transparent and accurate predictions, crucial for addressing contemporary mobility challenges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.02740v2</guid>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sebastiano Bontorin, Simone Centellegher, Riccardo Gallotti, Luca Pappalardo, Bruno Lepri, Massimiliano Luca</dc:creator>
    </item>
    <item>
      <title>The Sociotechnical Stack: Opportunities for Social Computing Research in Non-consensual Intimate Media</title>
      <link>https://arxiv.org/abs/2405.03585</link>
      <description>arXiv:2405.03585v3 Announce Type: replace 
Abstract: Non-consensual intimate media (NCIM) involves sharing intimate content without the depicted person's consent, including "revenge porn" and sexually explicit deepfakes. While NCIM has received attention in legal, psychological, and communication fields over the past decade, it is not sufficiently addressed in computing scholarship. This paper addresses this gap by linking NCIM harms to the specific technological components that facilitate them. We introduce the sociotechnical stack, a conceptual framework designed to map the technical stack to its corresponding social impacts. The sociotechnical stack allows us to analyze sociotechnical problems like NCIM, and points toward opportunities for computing research. We propose a research roadmap for computing and social computing communities to deter NCIM perpetration and support victim-survivors through building and rebuilding technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03585v3</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Li Qiwei, Allison McDonald, Oliver L. Haimson, Sarita Schoenebeck, Eric Gilbert</dc:creator>
    </item>
    <item>
      <title>Training Compute Thresholds: Features and Functions in AI Regulation</title>
      <link>https://arxiv.org/abs/2405.10799</link>
      <description>arXiv:2405.10799v2 Announce Type: replace 
Abstract: Regulators in the US and EU are using thresholds based on training compute--the number of computational operations used in training--to identify general-purpose artificial intelligence (GPAI) models that may pose risks of large-scale societal harm. We argue that training compute currently is the most suitable metric to identify GPAI models that deserve regulatory oversight and further scrutiny. Training compute correlates with model capabilities and risks, is quantifiable, can be measured early in the AI lifecycle, and can be verified by external actors, among other advantageous features. These features make compute thresholds considerably more suitable than other proposed metrics to serve as an initial filter to trigger additional regulatory requirements and scrutiny. However, training compute is an imperfect proxy for risk. As such, compute thresholds should not be used in isolation to determine appropriate mitigation measures. Instead, they should be used to detect potentially risky GPAI models that warrant regulatory oversight, such as through notification requirements, and further scrutiny, such as via model evaluations and risk assessments, the results of which may inform which mitigation measures are appropriate. In fact, this appears largely consistent with how compute thresholds are used today. As GPAI technology and market structures evolve, regulators should update compute thresholds and complement them with other metrics into regulatory review processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10799v2</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lennart Heim, Leonie Koessler</dc:creator>
    </item>
    <item>
      <title>AIR-Bench 2024: A Safety Benchmark Based on Risk Categories from Regulations and Policies</title>
      <link>https://arxiv.org/abs/2407.17436</link>
      <description>arXiv:2407.17436v2 Announce Type: replace 
Abstract: Foundation models (FMs) provide societal benefits but also amplify risks. Governments, companies, and researchers have proposed regulatory frameworks, acceptable use policies, and safety benchmarks in response. However, existing public benchmarks often define safety categories based on previous literature, intuitions, or common sense, leading to disjointed sets of categories for risks specified in recent regulations and policies, which makes it challenging to evaluate and compare FMs across these benchmarks. To bridge this gap, we introduce AIR-Bench 2024, the first AI safety benchmark aligned with emerging government regulations and company policies, following the regulation-based safety categories grounded in our AI risks study, AIR 2024. AIR 2024 decomposes 8 government regulations and 16 company policies into a four-tiered safety taxonomy with 314 granular risk categories in the lowest tier. AIR-Bench 2024 contains 5,694 diverse prompts spanning these categories, with manual curation and human auditing to ensure quality. We evaluate leading language models on AIR-Bench 2024, uncovering insights into their alignment with specified safety concerns. By bridging the gap between public benchmarks and practical AI risks, AIR-Bench 2024 provides a foundation for assessing model safety across jurisdictions, fostering the development of safer and more responsible AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17436v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zeng, Yu Yang, Andy Zhou, Jeffrey Ziwei Tan, Yuheng Tu, Yifan Mai, Kevin Klyman, Minzhou Pan, Ruoxi Jia, Dawn Song, Percy Liang, Bo Li</dc:creator>
    </item>
    <item>
      <title>The Traveling Mailman: Topological Optimization Methods for User-Centric Redistricting</title>
      <link>https://arxiv.org/abs/2407.19535</link>
      <description>arXiv:2407.19535v2 Announce Type: replace 
Abstract: This study introduces a new districting approach using the US Postal Service network to measure community connectivity. We combine Topological Data Analysis with Markov Chain Monte Carlo methods to assess district boundaries' impact on community integrity. Using Iowa as a case study, we generate and refine districting plans using KMeans clustering and stochastic rebalancing. Our method produces plans with fewer cut edges and more compact shapes than the official Iowa plan under relaxed conditions. The low likelihood of finding plans as disruptive as the official one suggests potential inefficiencies in existing boundaries. Gaussian Mixture Model analysis reveals three distinct distributions in the districting landscape. This framework offers a more accurate reflection of community interactions for fairer political representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19535v2</guid>
      <category>cs.CY</category>
      <category>math.SG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nelson A. Col\'on Vargas</dc:creator>
    </item>
    <item>
      <title>Television Discourse Decoded: Comprehensive Multimodal Analytics at Scale</title>
      <link>https://arxiv.org/abs/2402.12629</link>
      <description>arXiv:2402.12629v2 Announce Type: replace-cross 
Abstract: In this paper, we tackle the complex task of analyzing televised debates, with a focus on a prime time news debate show from India. Previous methods, which often relied solely on text, fall short in capturing the multimodal essence of these debates. To address this gap, we introduce a comprehensive automated toolkit that employs advanced computer vision and speech-to-text techniques for large-scale multimedia analysis. Utilizing state-of-the-art computer vision algorithms and speech-to-text methods, we transcribe, diarize, and analyze thousands of YouTube videos of a prime-time television debate show in India. These debates are a central part of Indian media but have been criticized for compromised journalistic integrity and excessive dramatization. Our toolkit provides concrete metrics to assess bias and incivility, capturing a comprehensive multimedia perspective that includes text, audio utterances, and video frames. Our findings reveal significant biases in topic selection and panelist representation, along with alarming levels of incivility. This work offers a scalable, automated approach for future research in multimedia analysis, with profound implications for the quality of public discourse and democratic debate. To catalyze further research in this area, we also release the code, dataset collected and supplemental pdf.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12629v2</guid>
      <category>cs.MM</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anmol Agarwal, Pratyush Priyadarshi, Shiven Sinha, Shrey Gupta, Hitkul Jangra, Ponnurangam Kumaraguru, Kiran Garimella</dc:creator>
    </item>
    <item>
      <title>Unveiling LLMs: The Evolution of Latent Representations in a Dynamic Knowledge Graph</title>
      <link>https://arxiv.org/abs/2404.03623</link>
      <description>arXiv:2404.03623v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) demonstrate an impressive capacity to recall a vast range of factual knowledge. However, understanding their underlying reasoning and internal mechanisms in exploiting this knowledge remains a key research area. This work unveils the factual information an LLM represents internally for sentence-level claim verification. We propose an end-to-end framework to decode factual knowledge embedded in token representations from a vector space to a set of ground predicates, showing its layer-wise evolution using a dynamic knowledge graph. Our framework employs activation patching, a vector-level technique that alters a token representation during inference, to extract encoded knowledge. Accordingly, we neither rely on training nor external models. Using factual and common-sense claims from two claim verification datasets, we showcase interpretability analyses at local and global levels. The local analysis highlights entity centrality in LLM reasoning, from claim-related information and multi-hop reasoning to representation errors causing erroneous evaluation. On the other hand, the global reveals trends in the underlying evolution, such as word-based knowledge evolving into claim-related facts. By interpreting semantics from LLM latent representations and enabling graph-related analyses, this work enhances the understanding of the factual knowledge resolution process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03623v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bronzini, Carlo Nicolini, Bruno Lepri, Jacopo Staiano, Andrea Passerini</dc:creator>
    </item>
    <item>
      <title>AI-Generated Faces in the Real World: A Large-Scale Case Study of Twitter Profile Images</title>
      <link>https://arxiv.org/abs/2404.14244</link>
      <description>arXiv:2404.14244v2 Announce Type: replace-cross 
Abstract: Recent advances in the field of generative artificial intelligence (AI) have blurred the lines between authentic and machine-generated content, making it almost impossible for humans to distinguish between such media. One notable consequence is the use of AI-generated images for fake profiles on social media. While several types of disinformation campaigns and similar incidents have been reported in the past, a systematic analysis has been lacking. In this work, we conduct the first large-scale investigation of the prevalence of AI-generated profile pictures on Twitter. We tackle the challenges of a real-world measurement study by carefully integrating various data sources and designing a multi-stage detection pipeline. Our analysis of nearly 15 million Twitter profile pictures shows that 0.052% were artificially generated, confirming their notable presence on the platform. We comprehensively examine the characteristics of these accounts and their tweet content, and uncover patterns of coordinated inauthentic behavior. The results also reveal several motives, including spamming and political amplification campaigns. Our research reaffirms the need for effective detection and mitigation strategies to cope with the potential negative effects of generative AI in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14244v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.SI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonas Ricker, Dennis Assenmacher, Thorsten Holz, Asja Fischer, Erwin Quiring</dc:creator>
    </item>
    <item>
      <title>STAR: SocioTechnical Approach to Red Teaming Language Models</title>
      <link>https://arxiv.org/abs/2406.11757</link>
      <description>arXiv:2406.11757v3 Announce Type: replace-cross 
Abstract: This research introduces STAR, a sociotechnical framework that improves on current best practices for red teaming safety of large language models. STAR makes two key contributions: it enhances steerability by generating parameterised instructions for human red teamers, leading to improved coverage of the risk surface. Parameterised instructions also provide more detailed insights into model failures at no increased cost. Second, STAR improves signal quality by matching demographics to assess harms for specific groups, resulting in more sensitive annotations. STAR further employs a novel step of arbitration to leverage diverse viewpoints and improve label reliability, treating disagreement not as noise but as a valuable contribution to signal quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11757v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Weidinger, John Mellor, Bernat Guillen Pegueroles, Nahema Marchal, Ravin Kumar, Kristian Lum, Canfer Akbulut, Mark Diaz, Stevie Bergman, Mikel Rodriguez, Verena Rieser, William Isaac</dc:creator>
    </item>
  </channel>
</rss>
