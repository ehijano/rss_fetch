<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 10 Apr 2025 01:40:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Generalized Tangent Approximation Framework for Strongly Super-Gaussian Likelihoods</title>
      <link>https://arxiv.org/abs/2504.05431</link>
      <description>arXiv:2504.05431v1 Announce Type: new 
Abstract: Tangent approximation form a popular class of variational inference (VI) techniques for Bayesian analysis in intractable non-conjugate models. It is based on the principle of convex duality to construct a minorant of the marginal likelihood, making the problem tractable. Despite its extensive applications, a general methodology for tangent approximation encompassing a large class of likelihoods beyond logit models with provable optimality guarantees is still elusive. In this article, we propose a general Tangent Approximation based Variational InferencE (TAVIE) framework for strongly super-Gaussian (SSG) likelihood functions which includes a broad class of flexible probability models. Specifically, TAVIE obtains a quadratic lower bound of the corresponding log-likelihood, thus inducing conjugacy with Gaussian priors over the model parameters. Under mild assumptions on the data-generating process, we demonstrate the optimality of our proposed methodology in the fractional likelihood setup. Furthermore, we illustrate the empirical performance of TAVIE through extensive simulations and an application on the U.S. 2000 Census real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05431v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Somjit Roy, Pritam Dey, Debdeep Pati, Bani K. Mallick</dc:creator>
    </item>
    <item>
      <title>Microbial correlation: a semi-parametric model for investigating microbial co-metabolism</title>
      <link>https://arxiv.org/abs/2504.05450</link>
      <description>arXiv:2504.05450v1 Announce Type: new 
Abstract: The gut microbiome plays a crucial role in human health, yet the mechanisms underlying host-microbiome interactions remain unclear, limiting its translational potential. Recent microbiome multiomics studies, particularly paired microbiome-metabolome studies (PM2S), provide valuable insights into gut metabolism as a key mediator of these interactions. Our preliminary data reveal strong correlations among certain gut metabolites, suggesting shared metabolic pathways and microbial co-metabolism. However, these findings are confounded by various factors, underscoring the need for a more rigorous statistical approach. Thus, we introduce microbial correlation, a novel metric that quantifies how two metabolites are co-regulated by the same gut microbes while accounting for confounders. Statistically, it is based on a partially linear model that isolates microbial-driven associations, and a consistent estimator is established based on semi-parametric theory. To improve efficiency, we develop a calibrated estimator with a parametric rate, maximizing the use of large external metagenomic datasets without paired metabolomic profiles. This calibrated estimator also enables efficient p-value calculation for identifying significant microbial co-metabolism signals. Through extensive numerical analysis, our method identifies important microbial co-metabolism patterns for healthy individuals, serving as a benchmark for future studies in diseased populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05450v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Shi, Yue Wang, Dan Cheng</dc:creator>
    </item>
    <item>
      <title>Modeling Multivariate Degradation Data with Dynamic Covariates Under a Bayesian Framework</title>
      <link>https://arxiv.org/abs/2504.05484</link>
      <description>arXiv:2504.05484v1 Announce Type: new 
Abstract: Degradation data are essential for determining the reliability of high-end products and systems, especially when covering multiple degradation characteristics (DCs). Modern degradation studies not only measure these characteristics but also record dynamic system usage and environmental factors, such as temperature, humidity, and ultraviolet exposures, referred to as the dynamic covariates. Most current research either focuses on a single DC with dynamic covariates or multiple DCs with fixed covariates. This paper presents a Bayesian framework to analyze data with multiple DCs, which incorporates dynamic covariates. We develop a Bayesian framework for mixed effect nonlinear general path models to describe the degradation path and use Bayesian shape-constrained P-splines to model the effects of dynamic covariates. We also detail algorithms for estimating the failure time distribution induced by our degradation model, validate the developed methods through simulation, and illustrate their use in predicting the lifespan of organic coatings in dynamic environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05484v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhengzhi Lin, Xiao Liu, Yisha Xiang, Yili Hong</dc:creator>
    </item>
    <item>
      <title>Bayesian Shrinkage in High-Dimensional VAR Models: A Comparative Study</title>
      <link>https://arxiv.org/abs/2504.05489</link>
      <description>arXiv:2504.05489v1 Announce Type: new 
Abstract: High-dimensional vector autoregressive (VAR) models offer a versatile framework for multivariate time series analysis, yet face critical challenges from over-parameterization and uncertain lag order. In this paper, we systematically compare three Bayesian shrinkage priors (horseshoe, lasso, and normal) and two frequentist regularization approaches (ridge and nonparametric shrinkage) under three carefully crafted simulation scenarios. These scenarios encompass (i) overfitting in a low-dimensional setting, (ii) sparse high-dimensional processes, and (iii) a combined scenario where both large dimension and overfitting complicate inference.
  We evaluate each method in quality of parameter estimation (root mean squared error, coverage, and interval length) and out-of-sample forecasting (one-step-ahead forecast RMSE). Our findings show that local-global Bayesian methods, particularly the horseshoe, dominate in maintaining accurate coverage and minimizing parameter error, even when the model is heavily over-parameterized. Frequentist ridge often yields competitive point forecasts but underestimates uncertainty, leading to sub-nominal coverage. A real-data application using macroeconomic variables from Canada illustrates how these methods perform in practice, reinforcing the advantages of local-global priors in stabilizing inference when dimension or lag order is inflated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05489v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Harrison Katz, Robert E. Weiss</dc:creator>
    </item>
    <item>
      <title>Adaptive Design for Contour Estimation from Computer Experiments with Quantitative and Qualitative Inputs</title>
      <link>https://arxiv.org/abs/2504.05498</link>
      <description>arXiv:2504.05498v1 Announce Type: new 
Abstract: Computer experiments with quantitative and qualitative inputs are widely used to study many scientific and engineering processes. Much of the existing work has focused on design and modeling or process optimization for such experiments. This paper proposes an adaptive design approach for estimating a contour from computer experiments with quantitative and qualitative inputs. A new criterion is introduced to search for the follow-up inputs. The key features of the proposed criterion are (a) the criterion yields adaptive search regions; and (b) it is region-based cooperative in that for each stage of the sequential procedure, the candidate points in the design space is divided into two disjoint groups using confidence bounds, and within each group, an acquisition function is used to select a candidate point. Among the two selected points, a point that is closer to the contour level with the higher uncertainty or that has higher uncertainty when the distance between its prediction and the contour level is within a threshold is chosen. The proposed approach provides empirically more accurate contour estimation than existing approaches as illustrated in numerical examples and a real application. Theoretical justification of the proposed adaptive search region is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05498v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Shahrokhian, X. Deng, C. D. Lin, P. Ranjan, L. Xu</dc:creator>
    </item>
    <item>
      <title>A new discrimination measure for assessing predictive performance of non-linear survival models</title>
      <link>https://arxiv.org/abs/2504.05630</link>
      <description>arXiv:2504.05630v1 Announce Type: new 
Abstract: Non-linear survival models are flexible models in which the proportional hazard assumption is not required. This poses difficulties in their evaluation. We introduce a new discrimination measure, time-dependent Uno's C-index, to assess the discrimination performance of non-linear survival models. This is an unbiased version of Antolini's time-dependent concordance. We prove convergence of both measures employing Nolan and Pollard's results on U-statistics. We explore the relationship between these measures and, in particular, the bias of Antolini's concordance in the presence of censoring using simulated data. We demonstrate the value of time-dependent Uno's C-index for the evaluation of models trained on censored real data and for model tuning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05630v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alfensi Faruk, Jan Palczewski, Georgios Aivaliotis</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of causal peer effects using instrumental variables</title>
      <link>https://arxiv.org/abs/2504.05658</link>
      <description>arXiv:2504.05658v1 Announce Type: new 
Abstract: In social science researches, causal inference regarding peer effects often faces significant challenges due to homophily bias and contextual confounding. For example, unmeasured health conditions (e.g., influenza) and psychological states (e.g., happiness, loneliness) can spread among closely connected individuals, such as couples or siblings. To address these issues, we define four effect estimands for dyadic data to characterize direct effects and spillover effects. We employ dual instrumental variables to achieve nonparametric identification of these causal estimands in the presence of unobserved confounding. We then derive the efficient influence functions for these estimands under the nonparametric model. Additionally, we develop a triply robust and locally efficient estimator that remains consistent even under partial misspecification of the observed data model. The proposed robust estimators can be easily adapted to flexible approaches such as machine learning estimation methods, provided that certain rate conditions are satisfied. Finally, we illustrate our approach through simulations and an empirical application evaluating the peer effects of retirement on fluid cognitive perception among couples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05658v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shanshan Luo, Kang Shuai, Yechi Zhang, Wei Li, Yangbo He</dc:creator>
    </item>
    <item>
      <title>Interpreting the Win Ratio in Hierarchical Composite Endpoints: Challenges, Limitations, and Perspectives with Examples from Chronic Kidney Disease Trials</title>
      <link>https://arxiv.org/abs/2504.05909</link>
      <description>arXiv:2504.05909v1 Announce Type: new 
Abstract: Win statistics based methods have gained traction as a method for analyzing Hierarchical Composite Endpoints (HCEs) in randomized clinical trials, particularly in cardiovascular and kidney disease research. HCEs offer several key advantages, including increased statistical power, the mitigation of competing risks, and hierarchical weighting of clinical importance for different outcome components. While, as summary measures, the win ratio (WR) along with the Net Benefit (NB) and the Win Odds (WO) provide a structured approach to analyzing HCEs, several concerns regarding their interpretability remain. In this paper, we explore critical aspects of WR interpretation that have received limited attention. Specifically, we discuss the challenge of defining an appropriate estimand in the context of HCEs using the WR, the difficulties in formulating a relevant causal question underlying the WR, and the dependency of the WR on the variance of its components, which complicates its role as an effect measure. Additionally, we highlight the non-collapsibility of the WR, akin to hazard and odds ratios, further complicating its interpretation. While the WR remains a valuable tool in clinical trials, its inherent limitations must be acknowledged to ensure its appropriate application and interpretation in regulatory and clinical decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05909v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Henrik F. Thomsen, Samvel B. Gasparyan, Julie F. Furberg, Christoph Tasto, Nicole Rethemeier, Patrick Schloemer, Tuo Wang, Niels Jongs, Yu Du, Tom Greene</dc:creator>
    </item>
    <item>
      <title>Unified Beta Regression Model with Random Effects for the Analysis of Sensory Attributes</title>
      <link>https://arxiv.org/abs/2504.05996</link>
      <description>arXiv:2504.05996v1 Announce Type: new 
Abstract: Studies involving sensory analysis are essential for evaluating and measuring the characteristics of food and beverages, including consumer acceptance of samples. For various products, the experimental designs are generally incomplete block designs, with sensory attributes assessed using hedonic scales, ratings, or scores. Statistical methods such as generalized logits are commonly used to analyze these data but face limitations, including convergence issues due to superparameterization. Furthermore, sensory attributes are traditionally analyzed separately, increasing the complexity of the process and complicating the interpretation of results. This study proposes a unified beta regression model with random effects for simultaneously analyzing multiple sensory attributes, whose scores were converted to the (0,1) interval. Simulation studies demonstrated overall agreement rates greater than 82% for the unified model compared to models fitted separately for each attribute. As a motivational example, the unified model was applied to a real dataset in which 98 potential consumers evaluated eight grape juice formulations for each sensory attribute: colour, flavour, aroma, acidity, and sweetness. The unified model identified the same top-rated formulations as the separately fitted models, characterized by a higher proportion of juice relative to sugar. The results underscore the ability of the unified model to simplify the analytical process without compromising accuracy, offering an efficient and insightful approach to sensory studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05996v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao C\'esar Reis Alves, Gabriel Rodrigues Palma, Idemauro Antonio Rodrigues de Lara</dc:creator>
    </item>
    <item>
      <title>Characterizing direct and indirect causal effects when outcomes are dependent due to treatment spillover and outcome spillover</title>
      <link>https://arxiv.org/abs/2504.06108</link>
      <description>arXiv:2504.06108v1 Announce Type: new 
Abstract: We provide novel insight into causal inference when both treatment spillover and outcome spillover occur in connected populations, by taking advantage of recent advances in statistical network analysis. Scenarios with treatment spillover and outcome spillover are challenging, because both forms of spillover affect outcomes and therefore treatment spillover and outcome spillover are intertwined, and outcomes are dependent conditional on treatments by virtue of outcome spillover. As a result, the direct and indirect causal effects arising from spillover have remained black boxes: While the direct and indirect causal effects can be identified, it is unknown how these causal effects explicitly depend on the effects of treatment, treatment spillover, and outcome spillover. We make three contributions, facilitated by low-rank random interference graphs. First, we provide novel insight into direct and indirect causal effects by disentangling the contributions of treatment, treatment spillover, and outcome spillover. Second, we provide scalable estimators of direct and indirect causal effects. Third, we establish rates of convergence for estimators of direct and indirect causal effects. These are the first convergence rates in scenarios in which treatment spillover and outcome spillover are intertwined and outcomes are dependent conditional on treatments, and the interference graph is sparse or dense.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06108v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subhankar Bhadra, Michael Schweinberger</dc:creator>
    </item>
    <item>
      <title>Generalized Ridge Regression: Applications to Nonorthogonal Linear Regression Models</title>
      <link>https://arxiv.org/abs/2504.06171</link>
      <description>arXiv:2504.06171v1 Announce Type: new 
Abstract: This paper analyzes the possibilities of using the generalized ridge regression to mitigate multicollinearity in a multiple linear regression model. For this purpose, we obtain the expressions for the estimated variance, the coefficient of variation, the coefficient of correlation, the variance inflation factor and the condition number. The results obtained are illustrated with two numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06171v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rom\'an Salmer\'on G\'omez, Catalina Garc\'ia Garc\'ia, Guillermo Hortal Reina</dc:creator>
    </item>
    <item>
      <title>Randomization Inference in Two-Sided Market Experiments</title>
      <link>https://arxiv.org/abs/2504.06215</link>
      <description>arXiv:2504.06215v1 Announce Type: new 
Abstract: Randomized experiments are increasingly employed in two-sided markets, such as buyer-seller platforms, to evaluate treatment effects from marketplace interventions. These experiments must reflect the underlying two-sided market structure in their design (e.g., sellers and buyers), making them particularly challenging to analyze. In this paper, we propose a randomization inference framework to analyze outcomes from such two-sided experiments. Our approach is finite-sample valid under sharp null hypotheses for any test statistic and maintains asymptotic validity under weak null hypotheses through studentization. Moreover, we provide heuristic guidance for choosing among multiple valid randomization tests to enhance statistical power, which we demonstrate empirically. Finally, we demonstrate the performance of our methodology through a series of simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.06215v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jizhou Liu, Azeem M. Shaikh, Panos Toulis</dc:creator>
    </item>
    <item>
      <title>Survey on Algorithms for multi-index models</title>
      <link>https://arxiv.org/abs/2504.05426</link>
      <description>arXiv:2504.05426v1 Announce Type: cross 
Abstract: We review the literature on algorithms for estimating the index space in a multi-index model. The primary focus is on computationally efficient (polynomial-time) algorithms in Gaussian space, the assumptions under which consistency is guaranteed by these methods, and their sample complexity. In many cases, a gap is observed between the sample complexity of the best known computationally efficient methods and the information-theoretical minimum. We also review algorithms based on estimating the span of gradients using nonparametric methods, and algorithms based on fitting neural networks using gradient descent</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05426v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joan Bruna, Daniel Hsu</dc:creator>
    </item>
    <item>
      <title>Why do zeroes happen? A model-based approach for demand classification</title>
      <link>https://arxiv.org/abs/2504.05894</link>
      <description>arXiv:2504.05894v1 Announce Type: cross 
Abstract: Effective demand forecasting is critical for inventory management, production planning, and decision making across industries. Selecting the appropriate model and suitable features to efficiently capture patterns in the data is one of the main challenges in demand forecasting. In reality, this becomes even more complicated when the recorded sales have zeroes, which can happen naturally or due to some anomalies, such as stockouts and recording errors. Mistreating the zeroes can lead to the application of inappropriate forecasting methods, and thus leading to poor decision making. Furthermore, the demand itself can have different fundamental characteristics, and being able to distinguish one type from another might bring substantial benefits in terms of accuracy and thus decision making. We propose a two-stage model-based classification framework that in the first step, identifies artificially occurring zeroes, and then classifies demand to one of the possible types: regular/intermittent, intermittent smooth/lumpy, fractional/count. The framework utilises statistical modelling and information criteria to detect anomalous zeroes and then classify demand into those categories. We then argue that different types of demand need different features, and show empirically that they tend to increase the accuracy of the forecasting methods compared to those applied directly to the dataset without the generated features and the two-stage framework. Our general practical recommendation based on that is to use the mixture approach for intermittent demand, capturing the demand sizes and demand probability separately, as it seems to improve the accuracy of different forecasting approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.05894v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ivan Svetunkov, Anna Sroginis</dc:creator>
    </item>
    <item>
      <title>Flexible Modeling of Demographic Transition Processes with a Bayesian Hierarchical B-splines Model</title>
      <link>https://arxiv.org/abs/2301.09694</link>
      <description>arXiv:2301.09694v3 Announce Type: replace 
Abstract: Several demographic and health indicators, including the total fertility rate (TFR) and modern contraceptive use rate (mCPR), evolve similarly over time, characterized by a transition between stable states. Existing approaches for estimation or projection of transitions in multiple populations have successfully used parametric functions to capture the relation between the rate of change of an indicator and its level. However, incorrect parametric forms may result in bias or incorrect coverage in long-term projections. We propose a new class of models to capture demographic transitions in multiple populations. Our proposal, the B-spline Transition Model (BTM), models the relationship between the rate of change of an indicator and its level using B-splines, allowing for data-adaptive estimation of transition functions. Bayesian hierarchical models are used to share information on the transition function between populations. We apply the BTM to estimate and project country-level TFR and mCPR and compare the results against those from extant parametric models. For TFR, BTM projections have generally lower error than the comparison model. For mCPR, while results are comparable between BTM and a parametric approach, the B-spline model generally improves out-of-sample predictions. The case studies suggest that the BTM may be considered for demographic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.09694v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Herbert Susmann, Leontine Alkema</dc:creator>
    </item>
    <item>
      <title>Penalty-Induced Basis Exploration for Bayesian Splines</title>
      <link>https://arxiv.org/abs/2311.13481</link>
      <description>arXiv:2311.13481v3 Announce Type: replace 
Abstract: Spline basis exploration via Bayesian model selection is a widely employed strategy for determining the optimal set of basis terms in nonparametric regression. However, despite its widespread use, this approach often encounters performance limitations owing to the finite approximation of infinite-dimensional parameters. This limitation arises because Bayesian model selection tends to favor simpler models over more complex ones when the true model is not among the candidates. Drawing inspiration from penalized splines, one potential remedy is to incorporate an additional roughness penalty that directly regulates the smoothness of functions. This strategy mitigates underfitting by allowing the inclusion of more basis terms while preventing overfitting through explicit smoothness control. Motivated by this insight, we propose a novel penalty-induced prior distribution for Bayesian basis exploration. The proposed prior evaluates the complexity of spline functions based on a convex combination of a roughness penalty and a ridge-type penalty for model selection. Our method adapts to the unknown level of smoothness and achieves the minimax-optimal posterior contraction rate up to a logarithmic factor. We also provide an efficient Markov chain Monte Carlo algorithm for its implementation. Extensive simulation studies demonstrate that our method outperforms competing approaches in terms of performance metrics and model complexity. An application to real datasets further substantiates the validity of our proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13481v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sunwoo Lim, Sihyeon Pyeon, Seonghyun Jeong</dc:creator>
    </item>
    <item>
      <title>Highest Probability Density Conformal Regions</title>
      <link>https://arxiv.org/abs/2406.08366</link>
      <description>arXiv:2406.08366v2 Announce Type: replace 
Abstract: This paper proposes a new method for finding the highest predictive density set or region, within the heteroscedastic regression framework. This framework enjoys the property that any highest predictive density set is a translation of some scalar multiple of a highest density set for the standardized regression error, with the same prediction accuracy. The proposed method leverages this property to efficiently compute conformal prediction regions, using signed conformal inference, kernel density estimation, in conjunction with any conditional mean, and scale estimators. While most conformal prediction methods output prediction intervals, this method adapts to the target. When the target is multi-modal, the proposed method outputs an approximation of the smallest multi-modal set. When the target is uni-modal, the proposed method outputs an approximation of the smallest interval. Under mild regularity conditions, we show that these conformal prediction sets are asymptotically close to the true smallest prediction sets. Because of the conformal guarantee, even in finite sample sizes the method has guaranteed coverage. With simulations and a real data analysis we demonstrate that the proposed method is better than existing methods when the target is multi-modal, and gives similar results when the target is uni-modal. Supplementary materials, including proofs and additional images, are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08366v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Flexible Conformal Highest Predictive Conditional Density Sets</title>
      <link>https://arxiv.org/abs/2406.18052</link>
      <description>arXiv:2406.18052v3 Announce Type: replace 
Abstract: We introduce our method, conformal highest conditional density sets (CHCDS), that forms conformal prediction sets using existing estimated conditional highest density predictive regions. We prove the validity of the method, and that conformal adjustment is negligible under some regularity conditions. In particular, if we correctly specify the underlying conditional density estimator, the conformal adjustment will be negligible. The conformal adjustment, however, always provides guaranteed nominal unconditional coverage, even when the underlying model is incorrectly specified. We compare the proposed method via simulation and a real data analysis to other existing methods. Our numerical results show that CHCDS is better than existing methods in scenarios where the error term is multi-modal, and just as good as existing methods when the error terms are unimodal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18052v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Joint Learning from Heterogeneous Rank Data</title>
      <link>https://arxiv.org/abs/2407.10846</link>
      <description>arXiv:2407.10846v2 Announce Type: replace 
Abstract: The statistical modelling of ranking data has a long history and encompasses various perspectives on how observed rankings arise. One of the most common models, the Plackett-Luce model, is frequently used to aggregate rankings from multiple rankers into a single ranking that corresponds to the underlying quality of the ranked objects. Given that rankers frequently exhibit heterogeneous preferences, mixture-type models have been developed to group rankers with more or less homogeneous preferences together to reduce bias. However, occasionally, these preference groups are known a-priori. Under these circumstances, current practice consists of fitting Plackett-Luce models separately for each group. Nevertheless, there might be some commonalities between different groups of rankers, such that separate estimation implies a loss of information. We propose an extension of the Plackett-Luce model, the Sparse Fused Plackett-Luce model, that allows for joint learning of such heterogeneous rank data, whereby information from different groups is utilised to achieve better model performance. The observed rankings can be considered a function of variables pertaining to the ranked objects. As such, we allow for these types of variables, where information on the coefficients is shared across groups. Moreover, as not all variables might be relevant for the ranking of an object, we impose sparsity on the coefficients to improve interpretability, estimation and prediction of the model. Simulations studies indicate superior performance of the proposed method compared to existing approaches. To illustrate the usage and interpretation of the method, an application on data consisting of consumer preferences regarding various sweet potato varieties is provided. An R package containing the proposed methodology can be found on https://CRAN.R-project.org/package=SFPL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10846v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sjoerd Hermes, Joost van Heerwaarden, Pariya Behrouzi</dc:creator>
    </item>
    <item>
      <title>Isotropy testing in spatial point patterns: nonparametric versus parametric replication under misspecification</title>
      <link>https://arxiv.org/abs/2411.19633</link>
      <description>arXiv:2411.19633v2 Announce Type: replace 
Abstract: Several hypothesis testing methods have been proposed to validate the assumption of isotropy in spatial point patterns. A majority of these methods are characterised by an unknown distribution of the test statistic under the null hypothesis of isotropy. Parametric approaches to approximating the distribution involve simulation of patterns from a user-specified isotropic model. Alternatively, nonparametric replicates of the test statistic under isotropy can be used to waive the need for specifying a model. In this paper, we first present a general framework which allows for the integration of a selected nonparametric replication method into isotropy testing. We then conduct a large simulation study comprising application-like scenarios to assess the performance of tests with different parametric and nonparametric replication methods. In particular, we explore distortions in test size and power caused by model misspecification, and demonstrate the advantages of nonparametric replication in such scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19633v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2025.100898</arxiv:DOI>
      <dc:creator>Jakub J. Pypkowski, Adam M. Sykulski, James S. Martin</dc:creator>
    </item>
    <item>
      <title>A partial likelihood approach to tree-based density modeling and its application in Bayesian inference</title>
      <link>https://arxiv.org/abs/2412.11692</link>
      <description>arXiv:2412.11692v4 Announce Type: replace 
Abstract: Tree-based priors for probability distributions are usually specified using a predetermined, data-independent collection of candidate recursive partitions of the sample space. To characterize an unknown target density in detail over the entire sample space, candidate partitions must have the capacity to expand deeply into all areas of the sample space with potential non-zero sampling probability. Such an expansive system of partitions often incurs prohibitive computational costs and makes inference prone to overfitting, especially in regions with little probability mass. Thus, existing models typically make a compromise and rely on relatively shallow trees. This hampers one of the most desirable features of trees, their ability to characterize local features, and results in reduced statistical efficiency. Traditional wisdom suggests that this compromise is inevitable to ensure coherent likelihood-based reasoning in Bayesian inference, as a data-dependent partition system that allows deeper expansion only in regions with more observations would induce double dipping of the data. We propose a simple strategy to restore coherency while allowing the candidate partitions to be data-dependent, using Cox's partial likelihood. Our partial likelihood approach is broadly applicable to existing likelihood-based methods and, in particular, to Bayesian inference on tree-based models. We give examples in density estimation in which the partial likelihood is endowed with existing priors on tree-based models and compare with the standard, full-likelihood approach. The results show substantial gains in estimation accuracy and computational efficiency from adopting the partial likelihood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11692v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Ma, Benedetta Bruni</dc:creator>
    </item>
    <item>
      <title>Family-wise Error Rate Control with E-values</title>
      <link>https://arxiv.org/abs/2501.09015</link>
      <description>arXiv:2501.09015v2 Announce Type: replace 
Abstract: The closure principle is a standard tool for achieving family-wise error rate (FWER) control in multiple testing problems. In general, the computational cost for closed testing can be exponential in the number of hypotheses. The celebrated graphical approach of FWER control overcomes the computational hurdle by using weighted Bonferroni local tests on p-values with appropriately chosen weights. In this study, we extend the graphical approach to e-values. With valid e-values -- common in settings of sequential hypothesis testing or universal inference for irregular parametric models -- we can derive strictly more powerful local tests based on weighted averages of e-values. Consequently, this e-value-based closed test is more powerful than the corresponding graphical approach with inverse e-values as p-values. Although the computational shortcuts for the p-value-based graphical approach are not applicable, we develop efficient polynomial-time algorithms using dynamic programming for e-value-based graphical approaches with any directed acyclic graph. For special graphs, such as those used in the Holm's procedure and fallback procedure, we develop tailored algorithms with computation cost linear in the number of hypotheses, up to logarithmic factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09015v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Will Hartog, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>A New Approach to Radiocarbon Summarisation: Rigorous Identification of Variations/Changepoints in the Occurrence Rate of Radiocarbon Samples using a Poisson Process</title>
      <link>https://arxiv.org/abs/2501.15980</link>
      <description>arXiv:2501.15980v2 Announce Type: replace 
Abstract: A commonly-used paradigm to estimate changes in the frequency of past events or the size of populations is to consider the occurrence rate of archaeological/environmental samples found at a site over time. The reliability of such a "dates-as-data" approach is highly dependent upon how the occurrence rates are estimated from the underlying samples, particularly when calendar age information for the samples is obtained from radiocarbon (14C). The most frequently used "14C-dates-as-data" approach of creating Summed Probability Distributions (SPDs) is not statistically valid, or coherent, and can provide highly misleading inference. Here, we provide an alternative method with a rigorous statistical underpinning that also provides valuable additional information on potential changepoints in the rate of events. Furthermore, unlike current SPD alternatives, our summarisation approach does not restrict users to pre-specified, rigid, summary formats (e.g., exponential or logistic growth) but instead flexibly adapts to the dates themselves. Our methodology ensures more reliable "14C-dates-as-data" analyses, allowing us to better assess and identify potential signals present. We model the occurrence of events, each assumed to leave a radiocarbon sample in the archaeological/environmental record, as an inhomogeneous Poisson process. The varying rate of samples over time is then estimated within a fully-Bayesian framework using reversible-jump Markov Chain Monte Carlo (RJ-MCMC). Given a set of radiocarbon samples, we reconstruct how their occurrence rate varies over calendar time and identify if that rate contains statistically-significant changes, i.e., specific times at which the rate of events abruptly changes. We illustrate our method with both a simulation study and a practical example concerning late-Pleistocene megafaunal population changes in Alaska and Yukon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15980v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Timothy J Heaton, Sara Al-assam, Edouard Bard</dc:creator>
    </item>
    <item>
      <title>A New Approach to the Nonparametric Behrens-Fisher Problem with Compatible Confidence Intervals</title>
      <link>https://arxiv.org/abs/2504.01796</link>
      <description>arXiv:2504.01796v2 Announce Type: replace 
Abstract: We propose a new test to address the nonparametric Behrens-Fisher problem involving different distribution functions in the two samples. Our procedure tests the null hypothesis $\mathcal{H}_0: \theta = \frac{1}{2}$, where $\theta = P(X&lt;Y) + \frac{1}{2}P(X=Y)$ denotes the Mann-Whitney effect. No restrictions on the underlying distributions of the data are imposed with the trivial exception of one-point distributions. The method is based on evaluating the ratio of the variance $\sigma_N^2$ of the Mann-Whitney effect estimator $\widehat{\theta}$ to its theoretical maximum, as derived from the Birnbaum-Klose inequality. Through simulations, we demonstrate that the proposed test effectively controls the type-I error rate under various conditions, including small sample sizes, unbalanced designs, and different data-generating mechanisms. Notably, it provides better control of the type-1 error rate compared to the widely used Brunner-Munzel test, particularly at small significance levels such as $\alpha \in \{0.01, 0.005\}$. Additionally, we derive range-preserving compatible confidence intervals, showing that they offer improved coverage over those compatible to the Brunner-Munzel test. Finally, we illustrate the application of our method in a clinical trial example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01796v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stephen Sch\"u\"urhuis, Frank Konietschke, Edgar Brunner</dc:creator>
    </item>
    <item>
      <title>Generalized Random Forests using Fixed-Point Trees</title>
      <link>https://arxiv.org/abs/2306.11908</link>
      <description>arXiv:2306.11908v3 Announce Type: replace-cross 
Abstract: We propose a computationally efficient alternative to generalized random forests arXiv:1610.01271 (GRFs) for estimating heterogeneous effects in large dimensions. While GRFs rely on a gradient-based splitting criterion, which in large dimensions is computationally expensive and unstable, our method introduces a fixed-point approximation that eliminates the need for Jacobian estimation. This gradient-free approach preserves GRFs theoretical guarantees of consistency and asymptotic normality while significantly improving computational efficiency. We demonstrate that our method achieves multiple times the speed over standard GRFs without compromising statistical accuracy. Experiments on both simulated and real-world data, validate our approach. Our findings suggest that the proposed method is a scalable alternative for localized effect estimation in machine learning and causal inference applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11908v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Fleischer, David A. Stephens, Archer Yang</dc:creator>
    </item>
    <item>
      <title>Inference with Mondrian Random Forests</title>
      <link>https://arxiv.org/abs/2310.09702</link>
      <description>arXiv:2310.09702v3 Announce Type: replace-cross 
Abstract: Random forests are popular methods for regression and classification analysis, and many different variants have been proposed in recent years. One interesting example is the Mondrian random forest, in which the underlying constituent trees are constructed via a Mondrian process. We give precise bias and variance characterizations, along with a Berry-Esseen-type central limit theorem, for the Mondrian random forest regression estimator. By combining these results with a carefully crafted debiasing approach and an accurate variance estimator, we present valid statistical inference methods for the unknown regression function. These methods come with explicitly characterized error bounds in terms of the sample size, tree complexity parameter, and number of trees in the forest, and include coverage error rates for feasible confidence interval estimators. Our novel debiasing procedure for the Mondrian random forest also allows it to achieve the minimax-optimal point estimation convergence rate in mean squared error for multivariate $\beta$-H\"older regression functions, for all $\beta &gt; 0$, provided that the underlying tuning parameters are chosen appropriately. Efficient and implementable algorithms are devised for both batch and online learning settings, and we study the computational complexity of different Mondrian random forest implementations. Finally, simulations with synthetic data validate our theory and methodology, demonstrating their excellent finite-sample properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.09702v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Jason M. Klusowski, William G. Underwood</dc:creator>
    </item>
    <item>
      <title>Constructing the Truth: Text Mining and Linguistic Networks in Public Hearings of Case 03 of the Special Jurisdiction for Peace (JEP)</title>
      <link>https://arxiv.org/abs/2504.04325</link>
      <description>arXiv:2504.04325v2 Announce Type: replace-cross 
Abstract: Case 03 of the Special Jurisdiction for Peace (JEP), focused on the so-called false positives in Colombia, represents one of the most harrowing episodes of the Colombian armed conflict. This article proposes an innovative methodology based on natural language analysis and semantic co-occurrence models to explore, systematize, and visualize narrative patterns present in the public hearings of victims and appearing parties. By constructing skipgram networks and analyzing their modularity, the study identifies thematic clusters that reveal regional and procedural status differences, providing empirical evidence on dynamics of victimization, responsibility, and acknowledgment in this case. This computational approach contributes to the collective construction of both judicial and extrajudicial truth, offering replicable tools for other transitional justice cases. The work is grounded in the pillars of truth, justice, reparation, and non-repetition, proposing a critical and in-depth reading of contested memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04325v2</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 09 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Alejandro Urrego-L\'opez, Cesar Prieto, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
  </channel>
</rss>
