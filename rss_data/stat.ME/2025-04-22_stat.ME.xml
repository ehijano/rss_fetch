<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Apr 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Revisiting John Snow's Cholera Map: A Data Visualisation Case Study for Statistical Education</title>
      <link>https://arxiv.org/abs/2504.13970</link>
      <description>arXiv:2504.13970v1 Announce Type: new 
Abstract: Data visualisation is a fundamental tool in statistical analysis, enabling the identification of patterns and relationships that might otherwise remain hidden in raw data. One of the most famous historical examples is John Snow's 1854 cholera map, which demonstrated the spatial clustering of cholera cases around a contaminated water pump in London. This study explores how Snow's visualisation can be effectively incorporated into statistics education as an interactive case study. Using R, we outline the steps involved in reproducing Snow's cholera map, demonstrating geospatial data manipulation, visualisation techniques, and spatial analysis. We discuss the pedagogical benefits of historical case studies in statistics courses, emphasising their role in fostering curiosity, critical thinking, and technical proficiency. Additionally, we explore how these methods can be extended beyond epidemiology to applications in public health, urban analytics and environmental science. By integrating historical datasets with modern computational tools, educators can create engaging, hands-on learning experiences that reinforce core statistical principles while illustrating the real-world impact of data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13970v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niamh Mimnagh</dc:creator>
    </item>
    <item>
      <title>An Introduction to Topological Data Analysis Ball Mapper in R</title>
      <link>https://arxiv.org/abs/2504.14081</link>
      <description>arXiv:2504.14081v1 Announce Type: new 
Abstract: The Topological Data Analysis Ball Mapper (TDABM) algorithm of Dlotko (2019) provides a model free means to visualize multi-dimensional data. The visualizations are abstract two-dimensional representations of covers of the dataset. To construct a TDABM plot, each variable in the dataset should be ordinal and suitable for representing as an axis of a scatter plot. The graphs produced by TDABM provide a map of the dataset on which outcomes may be charted, models assessed and new models formed. The benefits of TDABM are powering a growing literature. This document provides a step-by-step introduction to the algorithm with code in R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14081v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Rudkin</dc:creator>
    </item>
    <item>
      <title>Time-varying treatment effect models in stepped-wedge cluster-randomized trials with multiple interventions</title>
      <link>https://arxiv.org/abs/2504.14109</link>
      <description>arXiv:2504.14109v1 Announce Type: new 
Abstract: The traditional model specification of stepped-wedge cluster-randomized trials assumes a homogeneous treatment effect across time while adjusting for fixed-time effects. However, when treatment effects vary over time, the constant effect estimator may be biased. In the general setting of stepped-wedge cluster-randomized trials with multiple interventions, we derive the expected value of the constant effect estimator when the true treatment effects depend on exposure time periods. Applying this result to concurrent and factorial stepped wedge designs, we show that the estimator represents a weighted average of exposure-time-specific treatment effects, with weights that are not necessarily uniform across exposure periods. Extensive simulation studies reveal that ignoring time heterogeneity can result in biased estimates and poor coverage of the average treatment effect. In this study, we examine two models designed to accommodate multiple interventions with time-varying treatment effects: (1) a time-varying fixed treatment effect model, which allows treatment effects to vary by exposure time but remain fixed for each time point, and (2) a random treatment effect model, where the time-varying treatment effects are modeled as random deviations from an overall mean. In the simulations considered in this study, concurrent designs generally achieve higher power than factorial designs under a time-varying fixed treatment effect model, though the differences are modest. Finally, we apply the constant effect model and both time-varying treatment effect models to data from the Prognosticating Outcomes and Nudging Decisions in the Electronic Health Record (PONDER) trial. All three models indicate a lack of treatment effect for either intervention, though they differ in the precision of their estimates, likely due to variations in modeling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14109v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Chen, Wei Wang, Yingying Lu, Scott D. Halpern, Katherine R. Courtright, Fan Li, Michael O. Harhay</dc:creator>
    </item>
    <item>
      <title>Correction for nonignorable nonresponse bias in the estimation of turnout using callback data</title>
      <link>https://arxiv.org/abs/2504.14169</link>
      <description>arXiv:2504.14169v1 Announce Type: new 
Abstract: Overestimation of turnout has long been an issue in election surveys, with nonresponse bias or voter overrepresentation regarded as one of the major sources of bias. However, the adjustment for nonignorable nonresponse bias is substantially challenging. Based on the ANES Non-Response Follow-Up Study concerning the 2020 U.S. presidential election, we investigate the role of callback data in adjusting for nonresponse bias in the estimation of turnout. Callback data are the records of contact attempts in the survey course, available in many modern large-scale surveys. We propose a stableness of resistance assumption to account for the nonignorable missingness in the outcome, which states that the impact of the missing outcome on the response propensity is stable in the first two call attempts. Under this assumption and by leveraging covariates information from the census data, we establish the identifiability and develop estimation methods for turnout, including a doubly robust estimator. Our methods produce estimates very close to the official turnout and successfully capture the trend of declining willingness to vote as response reluctance or contact difficulty increases. This work hints at the importance of adjusting for nonignorable nonresponse bias and exhibits the promise of callback data for political surveys.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14169v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Li, Naiwen Ying, Kendrick Qijun Li, Xu Shi, Wang Miao</dc:creator>
    </item>
    <item>
      <title>A generalized tetrad constraint for testing conditional independence given a latent variable</title>
      <link>https://arxiv.org/abs/2504.14173</link>
      <description>arXiv:2504.14173v1 Announce Type: new 
Abstract: The tetrad constraint is widely used to test whether four observed variables are conditionally independent given a latent variable, based on the fact that if four observed variables following a linear model are mutually independent after conditioning on an unobserved variable, then products of covariances of any two different pairs of these four variables are equal. It is an important tool for discovering a latent common cause or distinguishing between alternative linear causal structures. However, the classical tetrad constraint fails in nonlinear models because the covariance of observed variables cannot capture nonlinear association. In this paper, we propose a generalized tetrad constraint, which establishes a testable implication for conditional independence given a latent variable in nonlinear and nonparametric models. In commonly-used linear models, this constraint implies the classical tetrad constraint; in nonlinear models, it remains a necessary condition for conditional independence but the classical tetrad constraint no longer is. Based on this constraint, we further propose a formal test, which can control type I error for significance level below 0.215 and has power approaching unity under certain conditions. We illustrate the proposed approach via simulations and two real data applications on moral attitudes towards dishonesty and on mental ability tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14173v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naiwen Ying, Shanshan Luo, Wang Miao</dc:creator>
    </item>
    <item>
      <title>A New Generalized Fisk distribution: Its Properties, Characterizations and Applications</title>
      <link>https://arxiv.org/abs/2504.14179</link>
      <description>arXiv:2504.14179v1 Announce Type: new 
Abstract: The shortcomings of the traditional univariate distributions in the past greatly encouraged mathematical statisticians to develop new generalizations of distributions. The New Generalized Fisk distribution, a unique distribution presented in this study, is thoroughly examined. There is a thorough discussion of a few fundamental statistical traits and attributes, such as the quantile function, order statistics, skewness and kurtosis, moments, and moment-generating functions. The new distribution incorporates additional parameters, enhancing its ability to capture a wide range of skewness and kurtosis behaviors, making it applicable to diverse fields such as economics, reliability engineering, and environmental sciences. Both numerical and graphical evaluations are used to evaluate the performance of the recently proposed model. Additionally, the performance of the maximum likelihood estimators is assessed by a simulation study. Real-world applications are analyzed, and the model parameters are estimated using the maximum likelihood estimation technique. It is contrasted with the popular models of computing. According to model adequacy and discrimination approaches, the suggested model performs the best. The models are compared to choose the model that best fits the data with the essential characteristics. The graphical and model comparison approaches suggested an outstanding improvement in the combined distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14179v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Veeranna Banoth</dc:creator>
    </item>
    <item>
      <title>A robust mixed-effects quantile regression model using generalized Laplace mixtures to handle outliers and skewness</title>
      <link>https://arxiv.org/abs/2504.14515</link>
      <description>arXiv:2504.14515v1 Announce Type: new 
Abstract: Mixed-effects quantile regression models are widely used to capture heterogeneous responses in hierarchically structured data. The asymmetric Laplace (AL) distribution has traditionally served as the basis for quantile regression; however, its fixed skewness limits flexibility and renders it sensitive to outliers. In contrast, the generalized asymmetric Laplace (GAL) distribution enables more flexible modeling of skewness and heavy-tailed behavior, yet it remains vulnerable to extreme observations. In this paper, we extend the GAL distribution by introducing a contaminated GAL (cGAL) mixture model that incorporates a scale-inflated component to mitigate the impact of outliers without requiring explicit outlier identification or deletion. We apply this model within a Bayesian mixed-effects quantile regression framework to model HIV viral load decay over time. Our results demonstrate that the cGAL-based model more reliably captures the dynamics of HIV viral load decay, yielding more accurate parameter estimates compared to both AL and GAL approaches. Model diagnostics and comparison statistics confirm the cGAL model as the preferred choice. A simulation study further shows that the cGAL model is more robust to outliers than the GAL and exhibits favorable frequentist properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14515v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Divan A. Burger, Sean van der Merwe, Emmanuel Lesaffre</dc:creator>
    </item>
    <item>
      <title>Evaluating the Cauchy Combination Test for Count Data</title>
      <link>https://arxiv.org/abs/2504.14552</link>
      <description>arXiv:2504.14552v1 Announce Type: new 
Abstract: The Cauchy combination test (CCT) is a $p$-value combination method used in multiple-hypothesis testing and is robust under dependence structures. This study aims to evaluate the CCT for independent and correlated count data where the individual $p$-values are derived from tests based on Normal approximation to the negative binomial distribution. The correlated count data are modelled via copula methods. The CCT performance is evaluated in a simulation study to assess the type 1 error rate and the statistical power and compare it with existing methods. In addition, we consider the influence of factors such as the success parameter of the negative binomial distribution, the number of individual $p$-values, and the correlation strength. Our results indicate that the negative binomial success parameter and the number of combined individual $p$-values may influence the type 1 error rate for the CCT under independence or weak dependence. The choice of copula method for modelling the correlation between count data has a significant influence on type 1 error rates for both the CCT and MinP tests. The CCT has more control over managing the type 1 error rate as the strength increases in the Gumbel-Hougaard copula. This knowledge may have significant implications for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14552v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Huda Alsulami, Silvia Liverani</dc:creator>
    </item>
    <item>
      <title>Statistical Design and Planning of an Adaptive Trial using Hierarchical Composite Outcomes: A Practical example</title>
      <link>https://arxiv.org/abs/2504.14748</link>
      <description>arXiv:2504.14748v1 Announce Type: new 
Abstract: Hierarchical composite endpoints, such as those analyzed using the Finkelstein-Schoenfeld (FS) statistic, are increasingly used in clinical trials for their ability to incorporate clinically prioritized outcomes. However, adaptive design methods for these endpoints remain underdeveloped. This paper presents a practical framework for implementing sample size re-estimation (SSR) in trials using hierarchical composites, motivated by a cardiovascular trial with mortality, hospitalization, and a functional response as prioritized endpoints. We use a two-stage adaptive design with a single interim analysis for illustration. The interim analysis incorporates predictive probabilities to determine whether the trial should stop for futility, continue as planned, or increase the sample size to maintain power. The decision framework is based on predefined zones for predictive probability, with corresponding adjustments to the stage 2 sample size. Simulation studies across various treatment scenarios demonstrate strong type I error control and increased power compared to a fixed design, particularly for treatment effects that are clinically relevant but lower than the alternative hypothesis. We also explore an alternative conditional power approach for SSR, offering further sample size optimization. Our results support the use of SSR with hierarchical composite outcomes using an FS statistic, enhancing trial efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14748v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Krishna Padmanabhan, Cyrus Mehta</dc:creator>
    </item>
    <item>
      <title>Monotone Ecological Inference</title>
      <link>https://arxiv.org/abs/2504.14752</link>
      <description>arXiv:2504.14752v1 Announce Type: new 
Abstract: We study monotone ecological inference, a partial identification approach to ecological inference. The approach exploits information about one or both of the following conditional associations: (1) outcome differences between groups within the same neighborhood, and (2) outcomes differences within the same group across neighborhoods with different group compositions. We show how assumptions about the sign of these conditional associations, whether individually or in relation to one another, can yield informative sharp bounds in ecological inference settings. We illustrate our proposed approach using county-level data to study differences in Covid-19 vaccination rates among Republicans and Democrats in the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14752v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hadi Elzayn, Jacob Goldin, Cameron Guage, Daniel E. Ho, Claire Morton</dc:creator>
    </item>
    <item>
      <title>Prevalence estimation in infectious diseases with imperfect tests: A comparison of Frequentist and Bayesian Logistic Regression methods with misclassification correction</title>
      <link>https://arxiv.org/abs/2504.15150</link>
      <description>arXiv:2504.15150v1 Announce Type: new 
Abstract: Accurate estimation of disease prevalence is essential for guiding public health strategies. Imperfect diagnostic tests can cause misclassification errors-false positives (FP) and false negatives (FN)-that may skew estimates if unaddressed. This study compared four statistical methods for estimating the prevalence of sexually transmitted infections (STIs) and associated factors, while correcting for misclassification. The methods were: (1) Standard Logistic Regression with external correction using known sensitivity and specificity; (2) the Liu et al. model, which jointly estimates FP and FN rates; (3) Bayesian Logistic Regression with external correction; and (4) a Bayesian model with internal correction using informative priors on diagnostic accuracy. Data came from 11,452 participants in a voluntary screening campaign for HIV, syphilis, and hepatitis B (2020-2024). Prevalence estimates and regression coefficients were compared across models using relative changes from crude estimates, confidence interval (CI) width, and coefficient variability. The Liu model produced higher prevalence estimates but had wider CIs and convergence issues in low-prevalence settings. The Bayesian model with internal correction gave intermediate estimates with the narrowest CIs and more stable intercepts, suggesting improved baseline prevalence estimation. Informative or weakly informative priors helped regularize estimates, especially in small-sample or rare-event contexts. Accounting for misclassification influenced both prevalence and covariate associations. While the Liu model offers theoretical strengths, its practical limitations in sparse data settings reduce its utility. Bayesian models with misclassification correction emerge as robust and flexible tools, particularly valuable in low-prevalence contexts where diagnostic uncertainty is high.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15150v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorge Mario Estrada Alvarez, Henan F. Garcia, Miguel \'Angel Montero-Alonso, Juan de Dios Luna del Castillo</dc:creator>
    </item>
    <item>
      <title>Scalable and robust regression models for continuous proportional data</title>
      <link>https://arxiv.org/abs/2504.15269</link>
      <description>arXiv:2504.15269v1 Announce Type: new 
Abstract: Beta regression is used routinely for continuous proportional data, but it often encounters practical issues such as a lack of robustness of regression parameter estimates to misspecification of the beta distribution. We develop an improved class of generalized linear models starting with the continuous binomial (cobin) distribution and further extending to dispersion mixtures of cobin distributions (micobin). The proposed cobin regression and micobin regression models have attractive robustness, computation, and flexibility properties. A key innovation is the Kolmogorov-Gamma data augmentation scheme, which facilitates Gibbs sampling for Bayesian computation, including in hierarchical cases involving nested, longitudinal, or spatial data. We demonstrate robustness, ability to handle responses exactly at the boundary (0 or 1), and computational efficiency relative to beta regression in simulation experiments and through analysis of the benthic macroinvertebrate multimetric index of US lakes using lake watershed covariates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15269v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changwoo J. Lee, Benjamin K. Dahl, Otso Ovaskainen, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Efficient Testing Using Surrogate Information</title>
      <link>https://arxiv.org/abs/2504.15273</link>
      <description>arXiv:2504.15273v1 Announce Type: new 
Abstract: In modern clinical trials, there is immense pressure to use surrogate markers in place of an expensive or long-term primary outcome to make more timely decisions about treatment effectiveness. However, using a surrogate marker to test for a treatment effect can be difficult and controversial. Existing methods tend to either rely on fully parametric methods where strict assumptions are made about the relationship between the surrogate and the outcome, or assume the surrogate marker is valid for the entire study population. In this paper, we develop a fully nonparametric method for efficient testing using surrogate information (ETSI). Our approach is specifically designed for settings where there is heterogeneity in the utility of the surrogate marker, i.e., the surrogate is valid for certain patient subgroups and not others. ETSI enables treatment effect estimation and hypothesis testing via kernel-based estimation for a setting where the surrogate is used in place of the primary outcome for individuals for whom the surrogate is valid, and the primary outcome is purposefully only measured in the remaining patients. In addition, we provide a framework for future study design with power and sample size estimates based on our proposed testing procedure. We demonstrate the performance of our methods via a simulation study and application to two distinct HIV clinical trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15273v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca Knowlton, Layla Parast</dc:creator>
    </item>
    <item>
      <title>Testing Random Effects for Binomial Data</title>
      <link>https://arxiv.org/abs/2504.13977</link>
      <description>arXiv:2504.13977v1 Announce Type: cross 
Abstract: In modern scientific research, small-scale studies with limited participants are increasingly common. However, interpreting individual outcomes can be challenging, making it standard practice to combine data across studies using random effects to draw broader scientific conclusions. In this work, we introduce an optimal methodology for assessing the goodness of fit between a given reference distribution and the distribution of random effects arising from binomial counts.
  Using the minimax framework, we characterize the smallest separation between the null and alternative hypotheses, called the critical separation, under the 1-Wasserstein distance that ensures the existence of a valid and powerful test. The optimal test combines a plug-in estimator of the Wasserstein distance with a debiased version of Pearson's chi-squared test.
  We focus on meta-analyses, where a key question is whether multiple studies agree on a treatment's effectiveness before pooling data. That is, researchers must determine whether treatment effects are homogeneous across studies. We begin by analyzing scenarios with a specified reference effect, such as testing whether all studies show the treatment is effective 80% of the time, and describe how the critical separation depends on the reference effect. We then extend the analysis to homogeneity testing without a reference effect and construct an optimal test by debiasing Cochran's chi-squared test.
  Finally, we illustrate how our proposed methodologies improve the construction of p-values and confidence intervals, with applications to assessing drug safety in the context of rare adverse outcomes and modeling political outcomes at the county level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13977v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Larry Wasserman, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Asymptotic well-calibration of the posterior predictive $p$-value under the modified Kolmogorov-Smirnov test</title>
      <link>https://arxiv.org/abs/2504.14077</link>
      <description>arXiv:2504.14077v1 Announce Type: cross 
Abstract: The posterior predictive $p$-value is a widely used tool for Bayesian model checking. However, under most test statistics, its asymptotic null distribution is more concentrated around 1/2 than uniform. Consequently, its finite-sample behavior is difficult to interpret and tends to lack power, which is a well-known issue among practitioners. A common choice of test statistic is the Kolmogorov-Smirnov test with plug-in estimators. It provides a global measure of model-data discrepancy for real-valued observations and is sensitive to model misspecification. In this work, we establish that under this test statistic, the posterior predictive $p$-value converges in distribution to uniform under the null. We further use numerical experiments to demonstrate that this $p$-value is well-behaved in finite samples and can effectively detect a wide range of alternative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14077v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yueming Shen</dc:creator>
    </item>
    <item>
      <title>Finite Population Identification and Design-Based Sensitivity Analysis</title>
      <link>https://arxiv.org/abs/2504.14127</link>
      <description>arXiv:2504.14127v1 Announce Type: cross 
Abstract: We develop an approach to sensitivity analysis that uses design distributions to calibrate sensitivity parameters in a finite population model. We use this approach to (1) give a new formal analysis of the role of randomization, (2) provide a new motivation for examining covariate balance, and (3) show how to construct design-based confidence intervals for the average treatment effect, which allow for heterogeneous treatment effects but do not rely on asymptotics. This approach to confidence interval construction relies on partial identification analysis rather than hypothesis test inversion. Moreover, these intervals also have a non-frequentist, identification-based interpretation. We illustrate our approach in three empirical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14127v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brendan Kline, Matthew A. Masten</dc:creator>
    </item>
    <item>
      <title>Learning over von Mises-Fisher Distributions via a Wasserstein-like Geometry</title>
      <link>https://arxiv.org/abs/2504.14164</link>
      <description>arXiv:2504.14164v1 Announce Type: cross 
Abstract: We introduce a novel, geometry-aware distance metric for the family of von Mises-Fisher (vMF) distributions, which are fundamental models for directional data on the unit hypersphere. Although the vMF distribution is widely employed in a variety of probabilistic learning tasks involving spherical data, principled tools for comparing vMF distributions remain limited, primarily due to the intractability of normalization constants and the absence of suitable geometric metrics. Motivated by the theory of optimal transport, we propose a Wasserstein-like distance that decomposes the discrepancy between two vMF distributions into two interpretable components: a geodesic term capturing the angular separation between mean directions, and a variance-like term quantifying differences in concentration parameters. The derivation leverages a Gaussian approximation in the high-concentration regime to yield a tractable, closed-form expression that respects the intrinsic spherical geometry. We show that the proposed distance exhibits desirable theoretical properties and induces a latent geometric structure on the space of non-degenerate vMF distributions. As a primary application, we develop the efficient algorithms for vMF mixture reduction, enabling structure-preserving compression of mixture models in high-dimensional settings. Empirical results on synthetic datasets and real-world high-dimensional embeddings, including biomedical sentence representations and deep visual features, demonstrate the effectiveness of the proposed geometry in distinguishing distributions and supporting interpretable inference. This work expands the statistical toolbox for directional data analysis by introducing a tractable, transport-inspired distance tailored to the geometry of the hypersphere.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14164v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kisung You, Dennis Shung, Mauro Giuffr\`e</dc:creator>
    </item>
    <item>
      <title>Seeing Through Risk: A Symbolic Approximation of Prospect Theory</title>
      <link>https://arxiv.org/abs/2504.14448</link>
      <description>arXiv:2504.14448v1 Announce Type: cross 
Abstract: We propose a novel symbolic modeling framework for decision-making under risk that merges interpretability with the core insights of Prospect Theory. Our approach replaces opaque utility curves and probability weighting functions with transparent, effect-size-guided features. We mathematically formalize the method, demonstrate its ability to replicate well-known framing and loss-aversion phenomena, and provide an end-to-end empirical validation on synthetic datasets. The resulting model achieves competitive predictive performance while yielding clear coefficients mapped onto psychological constructs, making it suitable for applications ranging from AI safety to economic policy analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14448v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Arslan Yousaf, Umair Rehman, Muhammad Umair Danish</dc:creator>
    </item>
    <item>
      <title>Dose optimization design accounting for unknown patient heterogeneity in cancer clinical trials</title>
      <link>https://arxiv.org/abs/2504.14622</link>
      <description>arXiv:2504.14622v1 Announce Type: cross 
Abstract: Project Optimus, an initiative by the FDA's Oncology Center of Excellence, seeks to reform the dose-optimization and dose-selection paradigm in oncology. We propose a dose-optimization design that considers plateau efficacy profiles, integrates pharmacokinetic data to inform the exposure-toxicity curve, and accounts for patient characteristics that may contribute to heterogeneity in response. The dose-optimization design is carried out in two stages. First, a toxicity-driven stage estimates a safe set of doses. Then, a dose-ranging efficacy-driven stage explores the set using response and patient characteristic data, employing Bayesian Sparse Group Selection to understand patient heterogeneity. Between stages, the design integrates pharmacokinetic data and uses futility assessments to identify the target population among the general phase I patient population. An optimal dose is recommended for each identified subpopulation within the target population. The simulation study demonstrates that a model-based approach to identifying the target population can be effective; patient characteristics relating to heterogeneity were identified and different optimal doses were recommended for each identified target subpopulation. Most designs that account for patient heterogeneity are intended for trials where heterogeneity is known and pre-defined subpopulations are specified. However, given the limited information at such an early stage, subpopulations should be learned through the design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14622v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rebecca B. Silva, Bin Cheng, Shing M. Lee</dc:creator>
    </item>
    <item>
      <title>AltGDmin: Alternating GD and Minimization for Partly-Decoupled (Federated) Optimization</title>
      <link>https://arxiv.org/abs/2504.14741</link>
      <description>arXiv:2504.14741v1 Announce Type: cross 
Abstract: This article describes a novel optimization solution framework, called alternating gradient descent (GD) and minimization (AltGDmin), that is useful for many problems for which alternating minimization (AltMin) is a popular solution. AltMin is a special case of the block coordinate descent algorithm that is useful for problems in which minimization w.r.t one subset of variables keeping the other fixed is closed form or otherwise reliably solved. Denote the two blocks/subsets of the optimization variables Z by Za, Zb, i.e., Z = {Za, Zb}. AltGDmin is often a faster solution than AltMin for any problem for which (i) the minimization over one set of variables, Zb, is much quicker than that over the other set, Za; and (ii) the cost function is differentiable w.r.t. Za. Often, the reason for one minimization to be quicker is that the problem is ``decoupled" for Zb and each of the decoupled problems is quick to solve. This decoupling is also what makes AltGDmin communication-efficient for federated settings.
  Important examples where this assumption holds include (a) low rank column-wise compressive sensing (LRCS), low rank matrix completion (LRMC), (b) their outlier-corrupted extensions such as robust PCA, robust LRCS and robust LRMC; (c) phase retrieval and its sparse and low-rank model based extensions; (d) tensor extensions of many of these problems such as tensor LRCS and tensor completion; and (e) many partly discrete problems where GD does not apply -- such as clustering, unlabeled sensing, and mixed linear regression. LRCS finds important applications in multi-task representation learning and few shot learning, federated sketching, and accelerated dynamic MRI. LRMC and robust PCA find important applications in recommender systems, computer vision and video analytics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14741v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Namrata Vaswani</dc:creator>
    </item>
    <item>
      <title>Causal DAG Summarization (Full Version)</title>
      <link>https://arxiv.org/abs/2504.14937</link>
      <description>arXiv:2504.14937v1 Announce Type: cross 
Abstract: Causal inference aids researchers in discovering cause-and-effect relationships, leading to scientific insights. Accurate causal estimation requires identifying confounding variables to avoid false discoveries. Pearl's causal model uses causal DAGs to identify confounding variables, but incorrect DAGs can lead to unreliable causal conclusions. However, for high dimensional data, the causal DAGs are often complex beyond human verifiability. Graph summarization is a logical next step, but current methods for general-purpose graph summarization are inadequate for causal DAG summarization. This paper addresses these challenges by proposing a causal graph summarization objective that balances graph simplification for better understanding while retaining essential causal information for reliable inference. We develop an efficient greedy algorithm and show that summary causal DAGs can be directly used for inference and are more robust to misspecification of assumptions, enhancing robustness for causal inference. Experimenting with six real-life datasets, we compared our algorithm to three existing solutions, showing its effectiveness in handling high-dimensional data and its ability to generate summary DAGs that ensure both reliable causal inference and robustness against misspecifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.14937v1</guid>
      <category>cs.LG</category>
      <category>cs.DB</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Zeng, Michael Cafarella, Batya Kenig, Markos Markakis, Brit Youngmann, Babak Salimi</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing under uniform-block covariance structures</title>
      <link>https://arxiv.org/abs/2304.08553</link>
      <description>arXiv:2304.08553v2 Announce Type: replace 
Abstract: A block covariance structure is widely observed across large-scale and high-dimensional datasets in diverse fields such as biology, medicine, engineering, economics, and finance. This pattern entails partitioning a covariance matrix into uniform blocks, where each block exhibits equal variances and covariances. The importance of uniform-block structures lies in their ubiquity, interpretability, and ability to accommodate high dimensionality and data missingness. Despite their prevalence, statistical hypothesis testing under uniform-block covariance structures remains largely unexplored, and unknown statistical properties limit their application in research. To address this gap, we develop a comprehensive framework for joint hypothesis tests of both covariance and mean structures, leveraging a novel block Hadamard product representation of uniform-block matrices. Specifically, we derive closed-form likelihood ratio test statistics and information statistics, explicitly establishing their null distributions. Additionally, we perform simultaneous marginal mean tests under a procedure that controls the false discovery proportion (FDP). Extensive simulations validate the consistency between theoretical and empirical distributions of the joint test statistics, assess the performance of the proposed FDP control procedure, and evaluate the robustness of the joint test statistics against structural disruptions and missing data. Lastly, we apply our methodology to hypothesis testing in a high-dimensional imaging dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.08553v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Shuo Chen, Ming Wang</dc:creator>
    </item>
    <item>
      <title>The Decaying Missing-at-Random Framework: Model Doubly Robust Causal Inference with Partially Labeled Data</title>
      <link>https://arxiv.org/abs/2305.12789</link>
      <description>arXiv:2305.12789v3 Announce Type: replace 
Abstract: In modern large-scale observational studies, data collection constraints often result in partially labeled datasets, posing challenges for reliable causal inference, especially due to potential labeling bias and relatively small size of the labeled data. This paper introduces a decaying missing-at-random (decaying MAR) framework and associated approaches for doubly robust causal inference on treatment effects in such semi-supervised (SS) settings. This simultaneously addresses selection bias in the labeling mechanism and the extreme imbalance between labeled and unlabeled groups, bridging the gap between the standard SS and missing data literatures, while throughout allowing for confounded treatment assignment and high-dimensional confounders under appropriate sparsity conditions. To ensure robust causal conclusions, we propose a bias-reduced SS (BRSS) estimator for the average treatment effect, a type of 'model doubly robust' estimator appropriate for such settings, establishing asymptotic normality at the appropriate rate under decaying labeling propensity scores, provided that at least one nuisance model is correctly specified. Our approach also relaxes sparsity conditions beyond those required in existing methods, including standard supervised approaches. Recognizing the asymmetry between labeling and treatment mechanisms, we further introduce a de-coupled BRSS (DC-BRSS) estimator, which integrates inverse probability weighting (IPW) with bias-reducing techniques in nuisance estimation. This refinement further weakens model specification and sparsity requirements. Numerical experiments confirm the effectiveness and adaptability of our estimators in addressing labeling bias and model misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12789v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuqian Zhang, Abhishek Chakrabortty, Jelena Bradic</dc:creator>
    </item>
    <item>
      <title>Robust Universal Inference For Misspecified Models</title>
      <link>https://arxiv.org/abs/2307.04034</link>
      <description>arXiv:2307.04034v3 Announce Type: replace 
Abstract: In statistical inference, it is rarely realistic that the hypothesized statistical model is well-specified, and consequently it is important to understand the effects of misspecification on inferential procedures. When the hypothesized statistical model is misspecified, the natural target of inference is a projection of the data generating distribution onto the model. We present a general method for constructing valid confidence sets for such projections, under weak regularity conditions, despite possible model misspecification. Our method builds upon the universal inference method of Wasserman et al. (2020) and is based on inverting a family of split-sample tests of relative fit. We study settings in which our methods yield either exact or approximate, finite-sample valid confidence sets for various projection distributions. We study rates at which the resulting confidence sets shrink around the target of inference and complement these results with a simulation study and a causal discovery using linear causal model on CausalEffectPairs dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.04034v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beomjo Park, Sivaraman Balakrishnan, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Aggregating Dependent Signals with Heavy-Tailed Combination Tests</title>
      <link>https://arxiv.org/abs/2310.20460</link>
      <description>arXiv:2310.20460v3 Announce Type: replace 
Abstract: Combining dependent p-values poses a long-standing challenge in statistical inference, particularly when aggregating findings from multiple methods to enhance signal detection. Recently, p-value combination tests based on regularly varying-tailed distributions, such as the Cauchy combination test and harmonic mean p-value, have attracted attention for their robustness to unknown dependence. This paper provides a theoretical and empirical evaluation of these methods under an asymptotic regime where the number of p-values is fixed and the global test significance level approaches zero. We examine two types of dependence among the p-values. First, when p-values are pairwise asymptotically independent, such as with bivariate normal test statistics with no perfect correlation, we prove that these combination tests are asymptotically valid. However, they become equivalent to the Bonferroni test as the significance level tends to zero for both one-sided and two-sided p-values. Empirical investigations suggest that this equivalence can emerge at moderately small significance levels. Second, under pairwise quasi-asymptotic dependence, such as with bivariate t-distributed test statistics, our simulations suggest that these combination tests can remain valid and exhibit notable power gains over Bonferroni, even as the significance level diminishes. These findings highlight the potential advantages of these combination tests in scenarios where p-values exhibit substantial dependence. Our simulations also examine how test performance depends on the support and tail heaviness of the underlying distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20460v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Gui, Yuchao Jiang, Jingshu Wang</dc:creator>
    </item>
    <item>
      <title>The general linear hypothesis testing problem for multivariate functional data with applications</title>
      <link>https://arxiv.org/abs/2312.02518</link>
      <description>arXiv:2312.02518v3 Announce Type: replace 
Abstract: As technology continues to advance at a rapid pace, the prevalence of multivariate functional data (MFD) has expanded across diverse disciplines, spanning biology, climatology, finance, and numerous other fields of study. Although MFD are encountered in various fields, the development of methods for hypotheses on mean functions, especially the general linear hypothesis testing (GLHT) problem for such data has been limited. In this study, we propose and study a new global test for the GLHT problem for MFD, which includes the one-way FMANOVA, post hoc, and contrast analysis as special cases. The asymptotic null distribution of the test statistic is shown to be a chi-squared-type mixture dependent of eigenvalues of the heteroscedastic covariance functions. The distribution of the chi-squared-type mixture can be well approximated by a three-cumulant matched chi-squared-approximation with its approximation parameters estimated from the data. By incorporating an adjustment coefficient, the proposed test performs effectively irrespective of the correlation structure in the functional data, even when dealing with a relatively small sample size. Additionally, the proposed test is shown to be root-n consistent, that is, it has a nontrivial power against a local alternative. Simulation studies and a real data example demonstrate finite-sample performance and broad applicability of the proposed test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02518v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianming Zhu</dc:creator>
    </item>
    <item>
      <title>Constructing a T-test for Value Function Comparison of Individualized Treatment Regimes in the Presence of Multiple Imputation for Missing Data</title>
      <link>https://arxiv.org/abs/2312.15217</link>
      <description>arXiv:2312.15217v3 Announce Type: replace 
Abstract: Optimal individualized treatment decision-making has improved health outcomes in recent years. The value function is commonly used to evaluate the goodness of an individualized treatment decision rule. Despite recent advances, comparing value functions between different treatment decision rules or constructing confidence intervals around value functions remains difficult. We propose a t-test based method applied to a test set that generates valid p-values to compare value functions between a given pair of treatment decision rules when some of the data are missing. We demonstrate the ease in use of this method and evaluate its performance via simulation studies and apply it to the China Health and Nutrition Survey data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15217v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minxin Lu, Annie Green Howard, Penny Gordon-Larsen, Katie A. Meyer, Hsiao-Chuan Tien, Shufa Du, Huijun Wang, Bing Zhang, Michael R. Kosorok</dc:creator>
    </item>
    <item>
      <title>Inference for Synthetic Controls via Refined Placebo Tests</title>
      <link>https://arxiv.org/abs/2401.07152</link>
      <description>arXiv:2401.07152v3 Announce Type: replace 
Abstract: The synthetic control method is often applied to problems with one treated unit and a small number of control units. A common inferential task in this setting is to test null hypotheses regarding the average treatment effect on the treated. Inference procedures that are justified asymptotically are often unsatisfactory due to (1) small sample sizes that render large-sample approximation fragile and (2) simplification of the estimation procedure that is implemented in practice. An alternative is permutation inference, which is related to a common diagnostic called the placebo test. It has provable Type-I error guarantees in finite samples without simplification of the method, when the treatment is uniformly assigned. Despite this robustness, the placebo test suffers from low resolution since the null distribution is constructed from only $N$ reference estimates, where $N$ is the sample size. This creates a barrier for statistical inference at a common level like $\alpha = 0.05$, especially when $N$ is small. We propose a novel leave-two-out procedure that bypasses this issue, while still maintaining the same finite-sample Type-I error guarantee under uniform assignment for a wide range of $N$. Unlike the placebo test whose Type-I error always equals the theoretical upper bound, our procedure often achieves a lower unconditional Type-I error than theory suggests; this enables useful inference in the challenging regime when $\alpha &lt; 1/N$. Empirically, our procedure achieves a higher power when the effect size is reasonably large and a comparable power otherwise. We generalize our procedure to non-uniform assignments and show how to conduct sensitivity analysis. From a methodological perspective, our procedure can be viewed as a new type of randomization inference different from permutation or rank-based inference, which is particularly effective in small samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07152v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihua Lei, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>On Bootstrapping Lasso in Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2403.19515</link>
      <description>arXiv:2403.19515v2 Announce Type: replace 
Abstract: Generalized linear models or GLM constitute plethora of sub-models which extends the ordinary linear regression by connecting the mean of response variable with the covariates through appropriate link functions. On the other hand, Lasso is a popular and easy-to-implement penalization method in regression when not all covariates are relevant. However, Lasso does not generally have a tractable asymptotic distribution (Knight and Fu (2000)). In this paper, we develop a Bootstrap method which works as an alternative to the asymptotic distribution of Lasso for all the submodels of GLM. We support our theoretical findings by showing good finite-sample properties of the proposed Bootstrap method through a moderately large simulation study. We also implement our method on a real data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.19515v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mayukh Choudhury, Debraj Das</dc:creator>
    </item>
    <item>
      <title>Efficient estimation and data fusion under general semiparametric restrictions on outcome mean functions</title>
      <link>https://arxiv.org/abs/2406.06941</link>
      <description>arXiv:2406.06941v3 Announce Type: replace 
Abstract: We provide a novel characterization of semiparametric efficiency in a generic supervised learning setting where the outcome mean function -- defined as the conditional expectation of the outcome of interest given the other observed variables -- is restricted to lie in some known semiparametric function class. The primary motivation is causal inference where a researcher running a randomized controlled trial often has access to an auxiliary observational dataset that is confounded or otherwise biased for estimating causal effects. Prior work has imposed various bespoke assumptions on this bias in an attempt to improve precision via data fusion. We show how many of these assumptions can be formulated as restrictions on the outcome mean function in the concatenation of the experimental and observational datasets. Then our theory provides a unified framework to maximally leverage such restrictions for precision gain by constructing efficient estimators in all of these settings as well as in a wide range of others that future investigators might be interested in. For example, when the observational dataset is subject to outcome-mediated selection bias, we show our novel efficient estimator dominates an existing control variate approach both asymptotically and in numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06941v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harrison H. Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Mapping of Mortality Clusters</title>
      <link>https://arxiv.org/abs/2407.19135</link>
      <description>arXiv:2407.19135v2 Announce Type: replace 
Abstract: Disease mapping analyses the distribution of several disease outcomes within a territory. Primary goals include identifying areas with unexpected changes in mortality rates, studying the relation among multiple diseases, and dividing the analysed territory into clusters based on the observed levels of disease incidence or mortality. In this work, we focus on detecting spatial mortality clusters, that occur when neighbouring areas within a territory exhibit similar mortality levels due to one or more diseases. When multiple causes of death are examined together, it is relevant to identify not only the spatial boundaries of the clusters but also the diseases that lead to their formation. However, existing methods in literature struggle to address this dual problem effectively and simultaneously. To overcome these limitations, we introduce Perla, a multivariate Bayesian model that clusters areas in a territory according to the observed mortality rates of multiple causes of death, also exploiting the information of external covariates. Our model incorporates the spatial structure of data directly into the clustering probabilities by leveraging the stick-breaking formulation of the multinomial distribution. Additionally, it exploits suitable global-local shrinkage priors to ensure that the detection of clusters depends on diseases showing concrete increases or decreases in mortality levels, while excluding uninformative diseases. We propose an MCMC algorithm for posterior inference that consists of closed-form Gibbs sampling moves for nearly every model parameter. To demonstrate the flexibility and effectiveness of our methodology, we validate Perla with a series of simulation experiments and two extensive case studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19135v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrea Sottosanti, Pietro Belloni, Enrico Bovo, Giovanna Boccuzzo</dc:creator>
    </item>
    <item>
      <title>Regression-based proximal causal inference for right-censored time-to-event data</title>
      <link>https://arxiv.org/abs/2409.08924</link>
      <description>arXiv:2409.08924v3 Announce Type: replace 
Abstract: Unmeasured confounding is one of the major concerns in causal inference from observational data. Proximal causal inference (PCI) is an emerging methodological framework to detect and potentially account for confounding bias by carefully leveraging a pair of negative control exposure (NCE) and outcome (NCO) variables, also known as treatment and outcome confounding proxies. Although regression-based PCI is well developed for binary and continuous outcomes, analogous PCI regression methods for right-censored time-to-event outcomes are currently lacking. In this paper, we propose a novel two-stage regression PCI approach for right-censored survival data under an additive hazard structural model. We provide theoretical justification for the proposed approach tailored to different types of NCOs, including continuous, count, and right-censored time-to-event variables. We illustrate the approach with an evaluation of the effectiveness of right heart catheterization among critically ill patients using data from the SUPPORT study. Our method is implemented in the open-access R package 'pci2s'.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08924v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kendrick Li, George C. Linderman, Xu Shi, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Higher-criticism for sparse multi-stream change-point detection</title>
      <link>https://arxiv.org/abs/2409.15597</link>
      <description>arXiv:2409.15597v2 Announce Type: replace 
Abstract: We study a statistical procedure based on higher criticism (HC) to address the sparse multi-stream quickest change-point detection problem. Namely, we aim to detect a potential change in the distribution of multiple data streams at some unknown time. If a change occurs, only a few streams are affected, whereas the identity of the affected streams is unknown. The HC-based procedure involves testing for a change point in individual streams and combining multiple tests using higher criticism. Relying on HC thresholding, the procedure also indicates a set of streams suspected to be affected by the change. We provide a theoretical analysis under a sparse heteroscedastic normal change-point model. We establish an information-theoretic detection delay lower bound when individual tests are based on the likelihood ratio or the generalized likelihood ratio statistics and show that the delay of the HC-based method converges in distribution to this bound. In the special case of constant variance, our bound coincides with known results in (Chan, 2017). We demonstrate the effectiveness of the HC-based method compared to other methods in detecting sparse changes through extensive numerical evaluations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15597v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tingnan Gong, Alon Kipnis, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Expert-elicitation method for non-parametric joint priors using normalizing flows</title>
      <link>https://arxiv.org/abs/2411.15826</link>
      <description>arXiv:2411.15826v2 Announce Type: replace 
Abstract: We propose an expert-elicitation method for learning non-parametric joint prior distributions using normalizing flows. Normalizing flows are a class of generative models that enable exact, single-step density evaluation and can capture complex density functions through specialized deep neural networks. Building on our previously introduced simulation-based framework, we adapt and extend the methodology to accommodate non-parametric joint priors. Our framework thus supports the development of elicitation methods for learning both parametric and non-parametric priors, as well as independent or joint priors for model parameters. To evaluate the performance of the proposed method, we perform four simulation studies and present an evaluation pipeline that incorporates diagnostics and additional evaluation tools to support decision-making at each stage of the elicitation process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15826v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florence Bockting, Stefan T. Radev, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Addressing Positivity Violations in Extending Inference to a Target Population</title>
      <link>https://arxiv.org/abs/2412.09845</link>
      <description>arXiv:2412.09845v2 Announce Type: replace 
Abstract: Enhancing the external validity of trial results is essential for their applicability to real-world populations. However, violations of the positivity assumption can limit both the generalizability and transportability of findings. To address positivity violations in estimating the average treatment effect for a target population, we propose a framework that integrates characterizing the underrepresented group and performing sensitivity analysis for inference in the original target population. Our approach helps identify limitations in trial sampling and improves the robustness of trial findings for real-world populations. We apply this approach to extend findings from phase IV trials of treatments for opioid use disorder to a real-world population based on the 2021 Treatment Episode Data Set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.09845v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jun Lu, Sanjib Basu</dc:creator>
    </item>
    <item>
      <title>Multiple change point detection based on Hodrick-Prescott and $l_1$ filtering method for random walk time series data</title>
      <link>https://arxiv.org/abs/2501.11805</link>
      <description>arXiv:2501.11805v3 Announce Type: replace 
Abstract: We propose new methods for detecting multiple change points in time series, specifically designed for random walk processes, where stationarity and variance changes present challenges. Our approach combines two trend estimation methods: the Hodrick Prescott (HP) filter and the l1 filter. A major challenge in these methods is selecting the tuning parameter lambda, which we address by introducing two selection techniques. For the HP based change point detection, we propose a probability-based threshold to select lambda under the assumption of an exponential distribution. For the l1 based method, we suggest a selection strategy assuming normality. Additionally, we introduce a technique to estimate the maximum number of change points in time segments using the l1 based method. We validate our methods by comparing them to similar techniques, such as PELT, using simulated data. We also demonstrate the practical application of our approach to real-world SNP stock data, showcasing its effectiveness in detecting change points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11805v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiyuan Liu</dc:creator>
    </item>
    <item>
      <title>Multilevel Monte Carlo Metamodeling for Variance Function Estimation</title>
      <link>https://arxiv.org/abs/2503.19294</link>
      <description>arXiv:2503.19294v2 Announce Type: replace 
Abstract: This work introduces a novel multilevel Monte Carlo (MLMC) metamodeling approach for variance function estimation. Although devising an efficient experimental design for simulation metamodeling can be elusive, the MLMC-based approach addresses this challenge by dynamically adjusting the number of design points and budget allocation at each level, thereby automatically creating an efficient design. Theoretical analyses show that, under mild conditions, the proposed MLMC metamodeling approach for variance function estimation can achieve superior computational efficiency compared to standard Monte Carlo metamodeling while achieving the desired level of accuracy. Additionally, this work establishes the asymptotic normality of the MLMC metamodeling estimator under certain sufficient conditions, providing valuable insights for uncertainty quantification. Finally, two MLMC metamodeling procedures are proposed for variance function estimation: one to achieve a target accuracy level and another to efficiently utilize a fixed computational budget. Numerical evaluations support the theoretical results and demonstrate the potential of the proposed approach in facilitating global sensitivity analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.19294v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jingtao Zhang, Xi Chen</dc:creator>
    </item>
    <item>
      <title>An Improved Satterthwaite Effective Degrees of Freedom Correction for Weighted Syntheses of Variance</title>
      <link>https://arxiv.org/abs/2503.22080</link>
      <description>arXiv:2503.22080v2 Announce Type: replace 
Abstract: This article presents an improved approximation for the effective degrees of freedom in the Satterthwaite (1941, 1946) method which estimates the distribution of a weighted combination of variance components The standard Satterthwaite approximation assumes a scaled chisquare distribution for the composite variance estimator but is known to be biased downward when component degrees of freedom are small. Building on recent work by von Davier (2025), we propose an adjusted estimator that corrects this bias by modifying both the numerator and denominator of the traditional formula. The new approximation incorporates a weighted average of component degrees of freedom and a scaling factor that ensures consistency as the number of components or their degrees of freedom increases. We demonstrate the utility of this adjustment in practical settings, including Rubin's (1987) total variance estimation in multiple imputations, where weighted variance combinations are common. The proposed estimator generalizes and further improves von Davier's (2025) unweighted case and more accurately approximates synthetic variance estimators with arbitrary weights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22080v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Matthias von Davier</dc:creator>
    </item>
    <item>
      <title>Graph-Based Prediction Models for Data Debiasing</title>
      <link>https://arxiv.org/abs/2504.09348</link>
      <description>arXiv:2504.09348v2 Announce Type: replace 
Abstract: Bias in data collection, arising from both under-reporting and over-reporting, poses significant challenges in critical applications such as healthcare and public safety. In this work, we introduce Graph-based Over- and Under-reporting Debiasing (GROUD), a novel graph-based optimization framework that debiases reported data by jointly estimating the true incident counts and the associated reporting bias probabilities. By modeling the bias as a smooth signal over a graph constructed from geophysical or feature-based similarities, our convex formulation not only ensures a unique solution but also comes with theoretical recovery guarantees under certain assumptions. We validate GROUD on both challenging simulated experiments and real-world datasets -- including Atlanta emergency calls and COVID-19 vaccine adverse event reports -- demonstrating its robustness and superior performance in accurately recovering debiased counts. This approach paves the way for more reliable downstream decision-making in systems affected by reporting irregularities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09348v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dongze Wu, Hanyang Jiang, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Gaussian Transforms Modeling and the Estimation of Distributional Regression Functions</title>
      <link>https://arxiv.org/abs/2011.06416</link>
      <description>arXiv:2011.06416v2 Announce Type: replace-cross 
Abstract: We propose flexible Gaussian representations for conditional cumulative distribution functions and give a concave likelihood criterion for their estimation. Optimal representations satisfy the monotonicity property of conditional cumulative distribution functions, including in finite samples and under general misspecification. We use these representations to provide a unified framework for the flexible Maximum Likelihood estimation of conditional density, cumulative distribution, and quantile functions at parametric rate. Our formulation yields substantial simplifications and finite sample improvements over related methods. An empirical application to the gender wage gap in the United States illustrates our framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2011.06416v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard Spady, Sami Stouli</dc:creator>
    </item>
    <item>
      <title>Composite Goodness-of-fit Tests with Kernels</title>
      <link>https://arxiv.org/abs/2111.10275</link>
      <description>arXiv:2111.10275v5 Announce Type: replace-cross 
Abstract: Model misspecification can create significant challenges for the implementation of probabilistic models, and this has led to development of a range of robust methods which directly account for this issue. However, whether these more involved methods are required will depend on whether the model is really misspecified, and there is a lack of generally applicable methods to answer this question. In this paper, we propose one such method. More precisely, we propose kernel-based hypothesis tests for the challenging composite testing problem, where we are interested in whether the data comes from any distribution in some parametric family. Our tests make use of minimum distance estimators based on the maximum mean discrepancy and the kernel Stein discrepancy. They are widely applicable, including whenever the density of the parametric model is known up to normalisation constant, or if the model takes the form of a simulator. As our main result, we show that we are able to estimate the parameter and conduct our test on the same data (without data splitting), while maintaining a correct test level. Our approach is illustrated on a range of problems, including testing for goodness-of-fit of an unnormalised non-parametric density model, and an intractable generative model of a biological cellular network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.10275v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research 26(51):1-60 2025</arxiv:journal_reference>
      <dc:creator>Oscar Key, Arthur Gretton, Fran\c{c}ois-Xavier Briol, Tamara Fernandez</dc:creator>
    </item>
    <item>
      <title>Early Detection of Treatments Side Effect: A Sequential Approach</title>
      <link>https://arxiv.org/abs/2401.13760</link>
      <description>arXiv:2401.13760v2 Announce Type: replace-cross 
Abstract: With the emergence and spread of infectious diseases with pandemic potential, such as COVID- 19, the urgency for vaccine development have led to unprecedented compressed and accelerated schedules that shortened the standard development timeline. In a relatively short time, the leading pharmaceutical companies1, received an Emergency Use Authorization (EUA) for vaccine\prime s en-mass deployment To monitor the potential side effect(s) of the vaccine during the (initial) vaccination campaign, we developed an optimal sequential test that allows for the early detection of potential side effect(s). This test employs a rule to stop the vaccination process once the observed number of side effect incidents exceeds a certain (pre-determined) threshold. The optimality of the proposed sequential test is justified when compared with the ({\alpha}, {\beta}) optimality of the non-randomized fixed-sample Uniformly Most Powerful (UMP) test. In the case of a single side effect, we study the properties of the sequential test and derive the exact expressions of the Average Sample Number (ASN) curve of the stopping time (and its variance) via the regularized incomplete beta function. Additionally, we derive the asymptotic distribution of the relative savings in ASN as compared to maximal sample size. Moreover, we construct the post-test parameter estimate and studied its sampling properties, including its asymptotic behavior under local-type alternatives. These limiting behavior results are the consistency and asymptotic normality of the post-test parameter estimator. We conclude the paper with a small simulation study illustrating the asymptotic performance of the point and interval estimation and provide a detailed example, based on COVID-19 side effect data (see Beatty et al. (2021)) of our suggested testing procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13760v2</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Wang, Ben Boukai</dc:creator>
    </item>
    <item>
      <title>CAP: A General Algorithm for Online Selective Conformal Prediction with FCR Control</title>
      <link>https://arxiv.org/abs/2403.07728</link>
      <description>arXiv:2403.07728v4 Announce Type: replace-cross 
Abstract: We study the problem of post-selection predictive inference in an online fashion. To avoid devoting resources to unimportant units, a preliminary selection of the current individual before reporting its prediction interval is common and meaningful in online predictive tasks. Since the online selection causes a temporal multiplicity in the selected prediction intervals, it is important to control the real-time false coverage-statement rate (FCR) which measures the overall miscoverage level. We develop a general framework named CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on historical data to construct a calibration set if the current individual is selected and then outputs a conformal prediction interval for the unobserved label. We provide tractable procedures for constructing the calibration set for popular online selection rules. We proved that CAP can achieve an exact selection-conditional coverage guarantee in the finite-sample and distribution-free regimes. To account for the distribution shift in online data, we also embed CAP into some recent dynamic conformal prediction algorithms and show that the proposed method can deliver long-run FCR control. Numerical results on both synthetic and real data corroborate that CAP can effectively control FCR around the target level and yield more narrowed prediction intervals over existing baselines across various settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07728v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Federated Transfer Learning with Differential Privacy</title>
      <link>https://arxiv.org/abs/2403.11343</link>
      <description>arXiv:2403.11343v3 Announce Type: replace-cross 
Abstract: Federated learning has emerged as a powerful framework for analysing distributed data, yet two challenges remain pivotal: heterogeneity across sites and privacy of local data. In this paper, we address both challenges within a federated transfer learning framework, aiming to enhance learning on a target data set by leveraging information from multiple heterogeneous source data sets while adhering to privacy constraints. We rigorously formulate the notion of federated differential privacy, which offers privacy guarantees for each data set without assuming a trusted central server. Under this privacy model, we study three classical statistical problems: univariate mean estimation, low-dimensional linear regression, and high-dimensional linear regression. By investigating the minimax rates and quantifying the cost of privacy in each problem, we show that federated differential privacy is an intermediate privacy model between the well-established local and central models of differential privacy. Our analyses account for data heterogeneity and privacy, highlighting the fundamental costs associated with each factor and the benefits of knowledge transfer in federated learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11343v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengchu Li, Ye Tian, Yang Feng, Yi Yu</dc:creator>
    </item>
    <item>
      <title>A Skewness-Based Criterion for Addressing Heteroscedastic Noise in Causal Discovery</title>
      <link>https://arxiv.org/abs/2410.06407</link>
      <description>arXiv:2410.06407v2 Announce Type: replace-cross 
Abstract: Real-world data often violates the equal-variance assumption (homoscedasticity), making it essential to account for heteroscedastic noise in causal discovery. In this work, we explore heteroscedastic symmetric noise models (HSNMs), where the effect $Y$ is modeled as $Y = f(X) + \sigma(X)N$, with $X$ as the cause and $N$ as independent noise following a symmetric distribution. We introduce a novel criterion for identifying HSNMs based on the skewness of the score (i.e., the gradient of the log density) of the data distribution. This criterion establishes a computationally tractable measurement that is zero in the causal direction but nonzero in the anticausal direction, enabling the causal direction discovery. We extend this skewness-based criterion to the multivariate setting and propose SkewScore, an algorithm that handles heteroscedastic noise without requiring the extraction of exogenous noise. We also conduct a case study on the robustness of SkewScore in a bivariate model with a latent confounder, providing theoretical insights into its performance. Empirical studies further validate the effectiveness of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06407v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingyu Lin, Yuxing Huang, Wenqin Liu, Haoran Deng, Ignavier Ng, Kun Zhang, Mingming Gong, Yi-An Ma, Biwei Huang</dc:creator>
    </item>
    <item>
      <title>Constrained Multi-objective Bayesian Optimization through Optimistic Constraints Estimation</title>
      <link>https://arxiv.org/abs/2411.03641</link>
      <description>arXiv:2411.03641v2 Announce Type: replace-cross 
Abstract: Multi-objective Bayesian optimization has been widely adopted in scientific experiment design, including drug discovery and hyperparameter optimization. In practice, regulatory or safety concerns often impose additional thresholds on certain attributes of the experimental outcomes. Previous work has primarily focused on constrained single-objective optimization tasks or active search under constraints. The existing constrained multi-objective algorithms address the issue with heuristics and approximations, posing challenges to the analysis of the sample efficiency. We propose a novel constrained multi-objective Bayesian optimization algorithm COMBOO that balances active learning of the level-set defined on multiple unknowns with multi-objective optimization within the feasible region. We provide both theoretical analysis and empirical evidence, demonstrating the efficacy of our approach on various synthetic benchmarks and real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.03641v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diantong Li, Fengxue Zhang, Chong Liu, Yuxin Chen</dc:creator>
    </item>
    <item>
      <title>A Spatiotemporal, Quasi-experimental Causal Inference Approach to Characterize the Effects of Global Plastic Waste Export and Burning on Air Quality Using Remotely Sensed Data</title>
      <link>https://arxiv.org/abs/2503.04491</link>
      <description>arXiv:2503.04491v2 Announce Type: replace-cross 
Abstract: Open burning of plastic waste may pose a significant threat to global health by degrading air quality, but quantitative research on this problem -- crucial for policy making -- has been stunted by lack of data. Critically, many low- and middle-income countries, where open burning is of greatest concern, have little to no air quality monitoring. Here, we propose an approach to leverage remotely sensed data products combined with spatiotemporal causal analytic techniques to evaluate the impact of large-scale plastic waste policies on air quality. Throughout, we use the case study of Indonesia before and after 2018, when China halted its import of plastic waste, resulting in diversion of this massive waste stream to other countries. We tailor cutting-edge statistical methods to this setting, estimating effects of the increase in plastic waste imports on fine particulate matter (PM$_{2.5}$) near waste dump sites in Indonesia as a function of proximity to ports, which serves as an induced continuous exposure. We observe that dump sites above the 20th quantile of port proximity experienced a statistically significant increase in monthly PM$_{2.5}$ concentrations after China's ban took effect (2018-2019) compared to concentrations expected under business-as-usual (2012-2017), with increases ranging from 0.76--1.72$\mu$g/m$^3$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04491v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ellen M. Considine, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Relaxing the Markov Requirements on Reinforcement Learning Under Weak Relative Ignorability</title>
      <link>https://arxiv.org/abs/2504.07722</link>
      <description>arXiv:2504.07722v3 Announce Type: replace-cross 
Abstract: Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``relative ignorabilty" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07722v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 22 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MaryLena Bleile</dc:creator>
    </item>
  </channel>
</rss>
