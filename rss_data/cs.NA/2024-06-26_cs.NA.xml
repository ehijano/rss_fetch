<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Jun 2024 04:01:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Operator splitting for coupled linear port-Hamiltonian systems</title>
      <link>https://arxiv.org/abs/2406.17311</link>
      <description>arXiv:2406.17311v1 Announce Type: new 
Abstract: Operator splitting methods tailored to coupled linear port-Hamiltonian systems are developed. We present algorithms that are able to exploit scalar coupling, as well as multirate potential of these coupled systems. The obtained algorithms preserve the dissipative structure of the overall system and are convergent of second order. Numerical results for coupled mass-spring-damper chains illustrate the computational efficiency of the splitting methods compared to a straight-forward application of the implicit midpoint rule to the overall system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17311v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jan Lorenz, Tom Zwerschke, Michael G\"unther, Kevin Sch\"afers</dc:creator>
    </item>
    <item>
      <title>Annealing-based approach to solving partial differential equations</title>
      <link>https://arxiv.org/abs/2406.17364</link>
      <description>arXiv:2406.17364v1 Announce Type: new 
Abstract: Solving partial differential equations using an annealing-based approach is based on solving generalized eigenvalue problems. When a partial differential equation is discretized, it leads to a system of linear equations (SLE). Solving an SLE can be expressed as a general eigenvalue problem, which can be converted into an optimization problem with the objective function being a generalized Rayleigh quotient. The proposed algorithm allows the computation of eigenvectors at arbitrary precision without increasing the number of variables using an Ising machine. Simple examples solved using this method and theoretical analysis provide a guideline for appropriate parameter settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17364v1</guid>
      <category>math.NA</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kazue Kudo</dc:creator>
    </item>
    <item>
      <title>Error Estimates for Data-driven Weakly Convex Frame-based Image Regularization</title>
      <link>https://arxiv.org/abs/2406.17461</link>
      <description>arXiv:2406.17461v1 Announce Type: new 
Abstract: Inverse problems are fundamental in fields like medical imaging, geophysics, and computerized tomography, aiming to recover unknown quantities from observed data. However, these problems often lack stability due to noise and ill-conditioning, leading to inaccurate reconstructions. To mitigate these issues, regularization methods are employed, introducing constraints to stabilize the inversion process and achieve a meaningful solution. Recent research has shown that the application of regularizing filters to diagonal frame decompositions (DFD) yields regularization methods. These filters dampen some frame coefficients to prevent noise amplification. This paper introduces a non-linear filtered DFD method combined with a learning strategy for determining optimal non-linear filters from training data pairs. In our experiments, we applied this approach to the inversion of the Radon transform using 500 image-sinogram pairs from real CT scans. Although the learned filters were found to be strictly increasing, they did not satisfy the non-expansiveness condition required to link them with convex regularizers and prove stability and convergence in the sense of regularization methods in previous works. Inspired by this, the paper relaxes the non-expansiveness condition, resulting in weakly convex regularization. Despite this relaxation, we managed to derive stability, convergence, and convergence rates with respect to the absolute symmetric Bregman distance for the learned non-linear regularizing filters. Extensive numerical results demonstrate the effectiveness of the proposed method in achieving stable and accurate reconstructions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17461v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Ebner, Matthias Schwab, Markus Haltmeier</dc:creator>
    </item>
    <item>
      <title>Constructing structured tensor priors for Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2406.17597</link>
      <description>arXiv:2406.17597v1 Announce Type: new 
Abstract: Specifying a prior distribution is an essential part of solving Bayesian inverse problems. The prior encodes a belief on the nature of the solution and this regularizes the problem. In this article we completely characterize a Gaussian prior that encodes the belief that the solution is a structured tensor. We first define the notion of (A,b)-constrained tensors and show that they describe a large variety of different structures such as Hankel, circulant, triangular, symmetric, and so on. Then we completely characterize the Gaussian probability distribution of such tensors by specifying its mean vector and covariance matrix. Furthermore, explicit expressions are proved for the covariance matrix of tensors whose entries are invariant under a permutation. These results unlock a whole new class of priors for Bayesian inverse problems. We illustrate how new kernel functions can be designed and efficiently computed and apply our results on two particular Bayesian inverse problems: completing a Hankel matrix from a few noisy measurements and learning an image classifier of handwritten digits. The effectiveness of the proposed priors is demonstrated for both problems. All applications have been implemented as reactive Pluto notebooks in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17597v1</guid>
      <category>math.NA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kim Batselier</dc:creator>
    </item>
    <item>
      <title>Optimization of Approximate Maps for Linear Systems Arising in Discretized PDEs</title>
      <link>https://arxiv.org/abs/2406.17656</link>
      <description>arXiv:2406.17656v1 Announce Type: new 
Abstract: Generally, discretization of partial differential equations (PDEs) creates a sequence of linear systems $A_k x_k = b_k, k = 0, 1, 2, ..., N$ with well-known and structured sparsity patterns. Preconditioners are often necessary to achieve fast convergence When solving these linear systems using iterative solvers. We can use preconditioner updates for closely related systems instead of computing a preconditioner for each system from scratch. One such preconditioner update is the sparse approximate map (SAM), which is based on the sparse approximate inverse preconditioner using a least squares approximation. A SAM then acts as a map from one matrix in the sequence to another nearby one for which we have an effective preconditioner. To efficiently compute an effective SAM update (i.e., one that facilitates fast convergence of the iterative solver), we seek to compute an optimal sparsity pattern. In this paper, we examine several sparsity patterns for computing the SAM update to characterize optimal or near-optimal sparsity patterns for linear systems arising from discretized PDEs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17656v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rishad Islam, Arielle Carr, Colin Jacobs</dc:creator>
    </item>
    <item>
      <title>Accelerating Phase Field Simulations Through a Hybrid Adaptive Fourier Neural Operator with U-Net Backbone</title>
      <link>https://arxiv.org/abs/2406.17119</link>
      <description>arXiv:2406.17119v1 Announce Type: cross 
Abstract: Prolonged contact between a corrosive liquid and metal alloys can cause progressive dealloying. For such liquid-metal dealloying (LMD) process, phase field models have been developed. However, the governing equations often involve coupled non-linear partial differential equations (PDE), which are challenging to solve numerically. In particular, stiffness in the PDEs requires an extremely small time steps (e.g. $10^{-12}$ or smaller). This computational bottleneck is especially problematic when running LMD simulation until a late time horizon is required. This motivates the development of surrogate models capable of leaping forward in time, by skipping several consecutive time steps at-once. In this paper, we propose U-Shaped Adaptive Fourier Neural Operators (U-AFNO), a machine learning (ML) model inspired by recent advances in neural operator learning. U-AFNO employs U-Nets for extracting and reconstructing local features within the physical fields, and passes the latent space through a vision transformer (ViT) implemented in the Fourier space (AFNO). We use U-AFNOs to learn the dynamics mapping the field at a current time step into a later time step. We also identify global quantities of interest (QoI) describing the corrosion process (e.g. the deformation of the liquid-metal interface) and show that our proposed U-AFNO model is able to accurately predict the field dynamics, in-spite of the chaotic nature of LMD. Our model reproduces the key micro-structure statistics and QoIs with a level of accuracy on-par with the high-fidelity numerical solver. We also investigate the opportunity of using hybrid simulations, in which we alternate forward leap in time using the U-AFNO with high-fidelity time stepping. We demonstrate that while advantageous for some surrogate model design choices, our proposed U-AFNO model in fully auto-regressive settings consistently outperforms hybrid schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17119v1</guid>
      <category>cs.CE</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Bonneville, Nathan Bieberdorf, Arun Hegde, Mark Asta, Habib N. Najm, Laurent Capolungo, Cosmin Safta</dc:creator>
    </item>
    <item>
      <title>Efficient, Multimodal, and Derivative-Free Bayesian Inference With Fisher-Rao Gradient Flows</title>
      <link>https://arxiv.org/abs/2406.17263</link>
      <description>arXiv:2406.17263v1 Announce Type: cross 
Abstract: In this paper, we study efficient approximate sampling for probability distributions known up to normalization constants. We specifically focus on a problem class arising in Bayesian inference for large-scale inverse problems in science and engineering applications. The computational challenges we address with the proposed methodology are: (i) the need for repeated evaluations of expensive forward models; (ii) the potential existence of multiple modes; and (iii) the fact that gradient of, or adjoint solver for, the forward model might not be feasible.
  While existing Bayesian inference methods meet some of these challenges individually, we propose a framework that tackles all three systematically. Our approach builds upon the Fisher-Rao gradient flow in probability space, yielding a dynamical system for probability densities that converges towards the target distribution at a uniform exponential rate. This rapid convergence is advantageous for the computational burden outlined in (i). We apply Gaussian mixture approximations with operator splitting techniques to simulate the flow numerically; the resulting approximation can capture multiple modes thus addressing (ii). Furthermore, we employ the Kalman methodology to facilitate a derivative-free update of these Gaussian components and their respective weights, addressing the issue in (iii).
  The proposed methodology results in an efficient derivative-free sampler flexible enough to handle multi-modal distributions: Gaussian Mixture Kalman Inversion (GMKI). The effectiveness of GMKI is demonstrated both theoretically and numerically in several experiments with multimodal target distributions, including proof-of-concept and two-dimensional examples, as well as a large-scale application: recovering the Navier-Stokes initial condition from solution data at positive times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17263v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Chen, Daniel Zhengyu Huang, Jiaoyang Huang, Sebastian Reich, Andrew M. Stuart</dc:creator>
    </item>
    <item>
      <title>DiffusionPDE: Generative PDE-Solving Under Partial Observation</title>
      <link>https://arxiv.org/abs/2406.17763</link>
      <description>arXiv:2406.17763v1 Announce Type: cross 
Abstract: We introduce a general framework for solving partial differential equations (PDEs) using generative diffusion models. In particular, we focus on the scenarios where we do not have the full knowledge of the scene necessary to apply classical solvers. Most existing forward or inverse PDE approaches perform poorly when the observations on the data or the underlying coefficients are incomplete, which is a common assumption for real-world measurements. In this work, we propose DiffusionPDE that can simultaneously fill in the missing information and solve a PDE by modeling the joint distribution of the solution and coefficient spaces. We show that the learned generative priors lead to a versatile framework for accurately solving a wide range of PDEs under partial observation, significantly outperforming the state-of-the-art methods for both forward and inverse directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17763v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahe Huang, Guandao Yang, Zichen Wang, Jeong Joon Park</dc:creator>
    </item>
    <item>
      <title>A new variable shape parameter strategy for RBF approximation using neural networks</title>
      <link>https://arxiv.org/abs/2210.16945</link>
      <description>arXiv:2210.16945v2 Announce Type: replace 
Abstract: The choice of the shape parameter highly effects the behaviour of radial basis function (RBF) approximations, as it needs to be selected to balance between ill-condition of the interpolation matrix and high accuracy. In this paper, we demonstrate how to use neural networks to determine the shape parameters in RBFs. In particular, we construct a multilayer perceptron trained using an unsupervised learning strategy, and use it to predict shape parameters for inverse multiquadric and Gaussian kernels. We test the neural network approach in RBF interpolation tasks and in a RBF-finite difference method in one and two-space dimensions, demonstrating promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.16945v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.camwa.2023.05.005</arxiv:DOI>
      <dc:creator>Fatemeh Nassajian Mojarrad, Maria Han Veiga, Jan S. Hesthaven, Philipp \"Offner</dc:creator>
    </item>
    <item>
      <title>Interpolatory $\mathcal{H}_2$-optimality Conditions for Structured Linear Time-invariant Systems</title>
      <link>https://arxiv.org/abs/2310.10618</link>
      <description>arXiv:2310.10618v3 Announce Type: replace 
Abstract: Interpolatory necessary optimality conditions for $\mathcal{H}_2$-optimal reduced-order modeling of unstructured linear time-invariant (LTI) systems are well-known. Based on previous work on $\mathcal{L}_2$-optimal reduced-order modeling of stationary parametric problems, in this paper we develop and investigate optimality conditions for $\mathcal{H}_2$-optimal reduced-order modeling of structured LTI systems, in particular, for second-order, port-Hamiltonian, and time-delay systems. Under certain diagonalizability assumptions, we show that across all these different structured settings, bitangential Hermite interpolation is the common form for optimality, thus proving a unifying optimality framework for structured reduced-order modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.10618v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petar Mlinari\'c, Peter Benner, Serkan Gugercin</dc:creator>
    </item>
    <item>
      <title>Generalized Jacobi Method for Computing Eigenvalues of Dual Quaternion Hermitian Matrices</title>
      <link>https://arxiv.org/abs/2405.13649</link>
      <description>arXiv:2405.13649v3 Announce Type: replace 
Abstract: Dual quaternion matrices have various applications in robotic research and its spectral theory has been extensively studied in recent years. In this paper, we extend Jacobi method to compute all eigenpairs of dual quaternion Hermitian matrices and establish its convergence. The improved version with elimination strategy is proposed to reduce the computational time. Especially, we present a novel three-step Jacobi method to compute such eigenvalues which have identical standard parts but different dual parts. We prove that the proposed three-step Jacobi method terminates after at most finite iterations and can provide $\epsilon$-approximation of eigenvalue. To the best of our knowledge, both the power method and the Rayleigh quotient iteration method can not handle such eigenvalue problem in this scenario. Numerical experiments illustrate the proposed Jacobi-type algorithms are effective and stable, and also outperform the power method and the Rayleigh quotient iteration method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13649v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yongjun Chen, Liping Zhang</dc:creator>
    </item>
    <item>
      <title>A note on best n-term approximation for generalized Wiener classes</title>
      <link>https://arxiv.org/abs/2406.10761</link>
      <description>arXiv:2406.10761v2 Announce Type: replace 
Abstract: We determine the best n-term approximation of generalized Wiener model classes in a Hilbert space $H $. This theory is then applied to several special cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.10761v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ronald DeVore, Guergana Petrova, Przemyslaw Wojtaszczyk</dc:creator>
    </item>
    <item>
      <title>Approximation Theory of Tree Tensor Networks: Tensorized Multivariate Functions</title>
      <link>https://arxiv.org/abs/2101.11932</link>
      <description>arXiv:2101.11932v5 Announce Type: replace-cross 
Abstract: We study the approximation of multivariate functions with tensor networks (TNs). The main conclusion of this work is an answer to the following two questions: ``What are the approximation capabilities of TNs?" and "What is an appropriate model class of functions that can be approximated with TNs?"
  To answer the former, we show that TNs can (near to) optimally replicate $h$-uniform and $h$-adaptive approximation, for any smoothness order of the target function. Tensor networks thus exhibit universal expressivity w.r.t. isotropic, anisotropic and mixed smoothness spaces that is comparable with more general neural networks families such as deep rectified linear unit (ReLU) networks. Put differently, TNs have the capacity to (near to) optimally approximate many function classes -- without being adapted to the particular class in question.
  To answer the latter, as a candidate model class we consider approximation classes of TNs and show that these are (quasi-)Banach spaces, that many types of classical smoothness spaces are continuously embedded into said approximation classes and that TN approximation classes are themselves not embedded in any classical smoothness space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.11932v5</guid>
      <category>math.FA</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mazen Ali, Anthony Nouy</dc:creator>
    </item>
    <item>
      <title>MgNO: Efficient Parameterization of Linear Operators via Multigrid</title>
      <link>https://arxiv.org/abs/2310.19809</link>
      <description>arXiv:2310.19809v2 Announce Type: replace-cross 
Abstract: In this work, we propose a concise neural operator architecture for operator learning. Drawing an analogy with a conventional fully connected neural network, we define the neural operator as follows: the output of the $i$-th neuron in a nonlinear operator layer is defined by $\mathcal O_i(u) = \sigma\left( \sum_j \mathcal W_{ij} u + \mathcal B_{ij}\right)$. Here, $\mathcal W_{ij}$ denotes the bounded linear operator connecting $j$-th input neuron to $i$-th output neuron, and the bias $\mathcal B_{ij}$ takes the form of a function rather than a scalar. Given its new universal approximation property, the efficient parameterization of the bounded linear operators between two neurons (Banach spaces) plays a critical role. As a result, we introduce MgNO, utilizing multigrid structures to parameterize these linear operators between neurons. This approach offers both mathematical rigor and practical expressivity. Additionally, MgNO obviates the need for conventional lifting and projecting operators typically required in previous neural operators. Moreover, it seamlessly accommodates diverse boundary conditions. Our empirical observations reveal that MgNO exhibits superior ease of training compared to other CNN-based models, while also displaying a reduced susceptibility to overfitting when contrasted with spectral-type neural operators. We demonstrate the efficiency and accuracy of our method with consistently state-of-the-art performance on different types of partial differential equations (PDEs).</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19809v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juncai He, Xinliang Liu, Jinchao Xu</dc:creator>
    </item>
    <item>
      <title>On the numerical reliability of nonsmooth autodiff: a MaxPool case study</title>
      <link>https://arxiv.org/abs/2401.02736</link>
      <description>arXiv:2401.02736v2 Announce Type: replace-cross 
Abstract: This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers  (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool function on the precision of 16 and 32 bits. These findings suggest that nonsmooth MaxPool Jacobians with lower norms help maintain stable and efficient test accuracy, whereas those with higher norms can result in instability and decreased performance. We also observe that the influence of MaxPool's nonsmooth Jacobians on learning can be reduced by using batch normalization, Adam-like optimizers, or increasing the precision level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02736v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Transactions on Machine Learning Research Journal, 2024, 23 p</arxiv:journal_reference>
      <dc:creator>Ryan Boustany (TSE-R)</dc:creator>
    </item>
    <item>
      <title>Classification with neural networks with quadratic decision functions</title>
      <link>https://arxiv.org/abs/2401.10710</link>
      <description>arXiv:2401.10710v2 Announce Type: replace-cross 
Abstract: Neural networks with quadratic decision functions have been introduced as alternatives to standard neural networks with affine linear ones. They are advantageous when the objects or classes to be identified are compact and of basic geometries like circles, ellipses etc. In this paper we investigate the use of such ansatz functions for classification. In particular we test and compare the algorithm on the MNIST dataset for classification of handwritten digits and for classification of subspecies. We also show, that the implementation can be based on the neural network structure in the software Tensorflow and Keras, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10710v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leon Frischauf, Otmar Scherzer, Cong Shi</dc:creator>
    </item>
    <item>
      <title>A macroscopic pedestrian model with variable maximal density</title>
      <link>https://arxiv.org/abs/2406.14649</link>
      <description>arXiv:2406.14649v2 Announce Type: replace-cross 
Abstract: In this paper we propose a novel macroscopic (fluid dynamics) model for describing pedestrian flow in low and high density regimes. The model is characterized by the fact that the maximal density reachable by the crowd - usually a fixed model parameter - is instead a state variable. To do that, the model couples a conservation law, devised as usual for tracking the evolution of the crowd density, with a Burgers-like PDE with a nonlocal term describing the evolution of the maximal density. The variable maximal density is used here to describe the effects of the psychological/physical pushing forces which are observed in crowds during competitive or emergency situations. Specific attention is also dedicated to the fundamental diagram, i.e., the function which expresses the relationship between crowd density and flux. Although the model needs a well defined fundamental diagram as known input parameter, it is not evident a priori which relationship between density and flux will be actually observed, due to the time-varying maximal density. An a posteriori analysis shows that the observed fundamental diagram has an elongated "tail" in the congested region, thus resulting similar to the concave/concave fundamental diagram with a "double hump" observed in real crowds. The main features of the model are investigated through 1D and 2D numerical simulations. The numerical code for the 1D simulation is freely available at https://gitlab.com/cristiani77/code_arxiv_2406.14649</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14649v2</guid>
      <category>math.DS</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Bartoli, Simone Cacace, Emiliano Cristiani, Roberto Ferretti</dc:creator>
    </item>
  </channel>
</rss>
