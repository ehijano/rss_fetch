<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 18 Mar 2025 03:27:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayes factor functions for testing partial correlation coefficients</title>
      <link>https://arxiv.org/abs/2503.10787</link>
      <description>arXiv:2503.10787v2 Announce Type: new 
Abstract: Partial correlation coefficients are widely applied in the social sciences to evaluate the relationship between two variables after accounting for the influence of others. In this article, we present Bayes Factor Functions (BFFs) for assessing the presence of partial correlation. BFFs represent Bayes factors derived from test statistics and are expressed as functions of a standardized effect size. While traditional frequentist methods based on $p$-values have been criticized for their inability to provide cumulative evidence in favor of the true hypothesis, Bayesian approaches are often challenged due to their computational demands and sensitivity to prior distributions. BFFs overcome these limitations and offer summaries of hypothesis tests as alternative hypotheses are varied over a range of prior distributions on standardized effects. They also enable the integration of evidence across multiple studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10787v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saptati Datta, Valen E. Johnson</dc:creator>
    </item>
    <item>
      <title>Surviving the frailty of time to event analysis in massive datasets with Generalized Additive Models (and the help of Simon Laplace)</title>
      <link>https://arxiv.org/abs/2503.10823</link>
      <description>arXiv:2503.10823v1 Announce Type: new 
Abstract: Analyses of time to event datasets have been invariably based on the Cox proportional hazards model (PHM). Reformulations of the PHM as a Poisson Generalized Additive Model (GAM) or as a Generalized Linear Mixed Model (GLMM) have been proposed in the literature, aiming to increase the flexibility of the PHM and allow its use in situations in which complex spatiotemporal relationships have to be taken into account when modeling survival. In this report, we provide a unified framework for considering these previous attempts and consider the implementation in software for GAM and GLMM in the R programming language. The connection between GAM/GLMM and the PHM is leveraged to provide computationally efficient implementations for a subclass of survival models that incorporate individual random effects ('frailty models'). Frailty models provide a unified method to address repeated events, correlated outcomes and also time varying visitation schedules when analyzing Electronic Health Record data. However the current implementation of frailty models in software facilities for the Cox model does not scale because of long computation times; conversely the direct implementation of individual random effects in GAM/GLMM software does not scale well with memory usage. We propose a two stage method for survival models with frailty based on the Laplace approximation. Using a D-optimal experimental design to simulate the performance of the proposed method across simulated datasets we illustrate that the proposed method can circumvent the limitations of existing implementations, opening up the possibility to model datasets of hundred of thousands to million individuals using high end workstations from within R.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.10823v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christos Argyropoulos (Division of Nephrology Department of Internal Medicine University of New Mexico, Clinical and Translational Sciences Center Health Sciences Center University of New Mexico), Hamza Mir (Division of Nephrology Department of Internal Medicine University of New Mexico), Maria-Eleni Roumelioti (Division of Nephrology Department of Internal Medicine University of New Mexico), Pablo Garcia (Division of Nephrology Department of Internal Medicine University of New Mexico)</dc:creator>
    </item>
    <item>
      <title>Bounds for the regression parameters in dependently censored survival models</title>
      <link>https://arxiv.org/abs/2503.11210</link>
      <description>arXiv:2503.11210v1 Announce Type: new 
Abstract: We propose a semiparametric model to study the effect of covariates on the distribution of a censored event time while making minimal assumptions about the censoring mechanism. The result is a partially identified model, in the sense that we obtain bounds on the covariate effects, which are allowed to be time-dependent. Moreover, these bounds can be interpreted as classical confidence intervals and are obtained by aggregating information in the conditional Peterson bounds over the entire covariate space. As a special case, our approach can be used to study the popular Cox proportional hazards model while leaving the censoring distribution as well as its dependence with the time of interest completely unspecified. A simulation study illustrates good finite sample performance of the method, and several data applications in both economics and medicine demonstrate its practicability on real data. All developed methodology is implemented in R and made available in the package depCensoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11210v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilias Willems, Jad Beyhum, Ingrid Van Keilegom</dc:creator>
    </item>
    <item>
      <title>Rank estimation for the accelerated failure time model with partially interval-censored data</title>
      <link>https://arxiv.org/abs/2503.11268</link>
      <description>arXiv:2503.11268v1 Announce Type: new 
Abstract: This paper presents a unified rank-based inferential procedure for fitting the accelerated failure time model to partially interval-censored data. A Gehan-type monotone estimating function is constructed based on the idea of the familiar weighted log-rank test, and an extension to a general class of rank-based estimating functions is suggested. The proposed estimators can be obtained via linear programming and are shown to be consistent and asymptotically normal via standard empirical process theory. Unlike common maximum likelihood-based estimators for partially interval-censored regression models, our approach can directly provide a regression coefficient estimator without involving a complex nonparametric estimation of the underlying residual distribution function. An efficient variance estimation procedure for the regression coefficient estimator is considered. Moreover, we extend the proposed rank-based procedure to the linear regression analysis of multivariate clustered partially interval-censored data. The finite-sample operating characteristics of our approach are examined via simulation studies. Data example from a colorectal cancer study illustrates the practical usefulness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11268v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202024.0003</arxiv:DOI>
      <dc:creator>Taehwa Choi, Sangbum Choi, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Factors affecting power in stepped wedge trials when the treatment effect varies with time</title>
      <link>https://arxiv.org/abs/2503.11472</link>
      <description>arXiv:2503.11472v1 Announce Type: new 
Abstract: Stepped wedge cluster randomized trials (SW-CRTs) have historically been analyzed using immediate treatment (IT) models, which assume the effect of the treatment is immediate after treatment initiation and subsequently remains constant over time. However, recent research has shown that this assumption can lead to severely misleading results if treatment effects vary with exposure time, i.e. time since the intervention started. Models that account for time-varying treatment effects, such as the exposure time indicator (ETI) model, allow researchers to target estimands such as the time-averaged treatment effect (TATE) over an interval of exposure time, or the point treatment effect (PTE) representing a treatment contrast at one time point. However, this increased flexibility results in reduced power. In this paper, we use public power calculation software and simulation to characterize factors affecting SW-CRT power. Key elements include choice of estimand, study design considerations, and analysis model selection. For common SW-CRT designs, the sample size (individuals per cluster-period) must be increased by a factor of roughly 2.5 to 3 to maintain 90\% power when switching from an IT model to an ETI model (targeting the TATE over the entire study). However, the inflation factor is lower when considering TATE estimands over shorter periods that exclude longer exposure times for which there is limited information. In general, SW-CRT designs (including the ``staircase'' variant) have much greater power for estimating ``short-term effects'' relative to ``long-term effects''. For an ETI model targeting a TATE estimand, substantial power can be gained by adding time points to the start of the study or increasing baseline sample size, but surprisingly little power is gained from adding time points to the end of the study. More restrictive choices for modeling the exposure time... [truncated]</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11472v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Avi Kenny, Emily C. Voldal, Fan Xia, Kwun Chuen Gary Chan, Patrick J. Heagerty, James P. Hughes</dc:creator>
    </item>
    <item>
      <title>Gradient-bridged Posterior: Bayesian Inference for Models with Implicit Functions</title>
      <link>https://arxiv.org/abs/2503.11637</link>
      <description>arXiv:2503.11637v1 Announce Type: new 
Abstract: Many statistical problems include model parameters that are defined as the solutions to optimization sub-problems. These include classical approaches such as profile likelihood as well as modern applications involving flow networks or Procrustes distances. In such cases, the likelihood of the data involves an implicit function, often complicating inferential procedures and entailing prohibitive computational cost. In this article, we propose an intuitive and tractable posterior inference approach for this setting. We introduce a class of continuous models that handle implicit function values using the first-order optimality of the sub-problems. Specifically, we apply a shrinkage kernel to the gradient norm, which retains a probabilistic interpretation within a generative model. This can be understood as a generalization of the Gibbs posterior framework to newly enable concentration around partial minimizers in a subset of the parameters. We show that this method, termed the gradient-bridged posterior, is amenable to efficient posterior computation, and enjoys theoretical guarantees, establishing a Bernstein--von Mises theorem for asymptotic normality. The advantages of our approach are highlighted on a synthetic flow network experiment and an application to data integration using Procrustes distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11637v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cheng Zeng, Yaozhi Yang, Jason Xu, Leo L Duan</dc:creator>
    </item>
    <item>
      <title>Towards practical PDMP sampling: Metropolis adjustments, locally adaptive step-sizes, and NUTS-based time lengths</title>
      <link>https://arxiv.org/abs/2503.11479</link>
      <description>arXiv:2503.11479v1 Announce Type: cross 
Abstract: Piecewise-Deterministic Markov Processes (PDMPs) hold significant promise for sampling from complex probability distributions. However, their practical implementation is hindered by the need to compute model-specific bounds. Conversely, while Hamiltonian Monte Carlo (HMC) offers a generally efficient approach to sampling, its inability to adaptively tune step sizes impedes its performance when sampling complex distributions like funnels.
  To address these limitations, we introduce three innovative concepts: (a) a Metropolis-adjusted approximation for PDMP simulation that eliminates the need for explicit bounds without compromising the invariant measure, (b) an adaptive step size mechanism compatible with the Metropolis correction, and (c) a No U-Turn Sampler (NUTS)-inspired scheme for dynamically selecting path lengths in PDMPs. These three ideas can be seamlessly integrated into a single, `doubly-adaptive' PDMP sampler with favourable robustness and efficiency properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11479v1</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Augustin Chevallier, Sam Power, Matthew Sutton</dc:creator>
    </item>
    <item>
      <title>Modeling complex measurement error in microbiome experiments to estimate relative abundances and detection effects</title>
      <link>https://arxiv.org/abs/2204.12733</link>
      <description>arXiv:2204.12733v2 Announce Type: replace 
Abstract: Accurate estimates of microbial species abundances are needed to advance our understanding of the role that microbiomes play in human and environmental health. However, artificially constructed microbiomes demonstrate that intuitive estimators of microbial relative abundances are biased. To address this, we propose a semiparametric method to estimate relative abundances, species detection effects, and/or cross-sample contamination in microbiome experiments. We show that certain experimental designs result in identifiable model parameters, and we present consistent estimators and asymptotically valid inference procedures. Notably, our procedure can estimate relative abundances on the boundary of the simplex. We demonstrate the utility of the method for comparing experimental protocols, removing cross-sample contamination, and estimating species' detectability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.12733v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David S Clausen, Amy D Willis</dc:creator>
    </item>
    <item>
      <title>Nested stochastic block model for simultaneously clustering networks and nodes</title>
      <link>https://arxiv.org/abs/2307.09210</link>
      <description>arXiv:2307.09210v2 Announce Type: replace 
Abstract: We introduce the nested stochastic block model (NSBM) to cluster a collection of networks while simultaneously detecting communities within each network. NSBM has several appealing features including the ability to work on unlabeled networks with potentially different node sets, the flexibility to model heterogeneous communities, and the means to automatically select the number of classes for the networks and the number of communities within each network. This is accomplished via a Bayesian model, with a novel application of the nested Dirichlet process (NDP) as a prior to jointly model the between-network and within-network clusters. The dependency introduced by the network data creates nontrivial challenges for the NDP, especially in the development of efficient samplers. For posterior inference, we propose several Markov chain Monte Carlo algorithms including a standard Gibbs sampler, a collapsed Gibbs sampler, and two blocked Gibbs samplers that ultimately return two levels of clustering labels from both within and across the networks. Extensive simulation studies are carried out which demonstrate that the model provides very accurate estimates of both levels of the clustering structure. We also apply our model to two social network datasets that cannot be analyzed using any previous method in the literature due to the anonymity of the nodes and the varying number of nodes in each network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09210v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>stat.ML</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nathaniel Josephs, Arash A. Amini, Marina Paez, Lizhen Lin</dc:creator>
    </item>
    <item>
      <title>Estimating Fold Changes from Partially Observed Outcomes with Applications in Microbial Metagenomics</title>
      <link>https://arxiv.org/abs/2402.05231</link>
      <description>arXiv:2402.05231v2 Announce Type: replace 
Abstract: We consider the problem of estimating fold-changes in the expected value of a multivariate outcome observed with unknown sample-specific and category-specific perturbations. This challenge arises in high-throughput sequencing studies of the abundance of microbial taxa because microbes are systematically over- and under-detected relative to their true abundances. Our model admits a partially identifiable estimand, and we establish full identifiability by imposing interpretable parameter constraints. To reduce bias and guarantee the existence of estimators in the presence of sparse observations, we apply an asymptotically negligible and constraint-invariant penalty to our estimating function. We develop a fast coordinate descent algorithm for estimation, and an augmented Lagrangian algorithm for estimation under null hypotheses. We construct a model-robust score test and demonstrate valid inference even for small sample sizes and violated distributional assumptions. The flexibility of the approach and comparisons to related methods are illustrated through a meta-analysis of microbial associations with colorectal cancer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05231v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David S Clausen, Sarah Teichman, Amy D Willis</dc:creator>
    </item>
    <item>
      <title>Parameter estimation for partially observed second-order diffusion processes</title>
      <link>https://arxiv.org/abs/2406.14738</link>
      <description>arXiv:2406.14738v2 Announce Type: replace 
Abstract: Estimating parameters of a diffusion process given continuous-time observations of the process via maximum likelihood approaches or, online, via stochastic gradient descent or Kalman filter formulations constitutes a well-established research area. It has also been established previously that these techniques are, in general, not robust to perturbations in the data in the form of temporal correlations of the driving noise. While the subject is relatively well understood and appropriate modifications have been suggested in the context of multi-scale diffusion processes and their reduced model equations, we consider here an alternative but related setting where a diffusion process in positions and velocities is only observed via its positions. In this note, we propose a simple modification to standard stochastic gradient descent and Kalman filter formulations, which eliminates the arising systematic estimation biases. The modification can be extended to standard maximum likelihood approaches and avoids computation of previously proposed correction terms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14738v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Albrecht, Sebastian Reich</dc:creator>
    </item>
    <item>
      <title>An Economical Approach to Design Posterior Analyses</title>
      <link>https://arxiv.org/abs/2411.13748</link>
      <description>arXiv:2411.13748v2 Announce Type: replace 
Abstract: To design Bayesian studies, criteria for the operating characteristics of posterior analyses - such as power and the type I error rate - are often assessed by estimating sampling distributions of posterior probabilities via simulation. In this paper, we propose an economical method to determine optimal sample sizes and decision criteria for such studies. Using our theoretical results that model posterior probabilities as a function of the sample size, we assess operating characteristics throughout the sample size space given simulations conducted at only two sample sizes. These theoretical results are used to construct bootstrap confidence intervals for the optimal sample sizes and decision criteria that reflect the stochastic nature of simulation-based design. We also repurpose the simulations conducted in our approach to efficiently investigate various sample sizes and decision criteria using contour plots. The broad applicability and wide impact of our methodology is illustrated using two clinical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13748v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2025.2476221</arxiv:DOI>
      <dc:creator>Luke Hagar, Nathaniel T. Stevens</dc:creator>
    </item>
    <item>
      <title>Randomized interventional effects in semicompeting risks</title>
      <link>https://arxiv.org/abs/2412.06114</link>
      <description>arXiv:2412.06114v2 Announce Type: replace 
Abstract: In clinical studies, the risk of the primary (terminal) event may be modified by intermediate events, resulting in semicompeting risks. To study the treatment effect on the terminal event mediated by the intermediate event, researchers wish to decompose the total effect into direct and indirect effects. In this article, we extend the randomized interventional approach to time-to-event data, where both the intermediate and terminal events are subject to right censoring. We envision a random draw for the intermediate event process according to some reference distribution, either marginally over time-varying confounders or conditionally given observed history. We present the identification formula for interventional effects and discuss some variants of the identification assumptions. The target estimands can be estimated using likelihood-based methods. As an illustration, we study the effect of transplant modalities on death mediated by relapse in an allogeneic stem cell transplantation study to treat leukemia with a time-varying confounder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06114v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhao Deng, Rui Wang</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing with e-values</title>
      <link>https://arxiv.org/abs/2410.23614</link>
      <description>arXiv:2410.23614v3 Announce Type: replace-cross 
Abstract: This book is written to offer a humble, but unified, treatment of e-values in hypothesis testing. The book is organized into three parts: Fundamental Concepts, Core Ideas, and Advanced Topics. The first part includes three chapters that introduce the basic concepts. The second part includes five chapters of core ideas such as universal inference, log-optimality, e-processes, operations on e-values, and e-values in multiple testing. The third part contains five chapters of advanced topics. We hope that, by putting the materials together in this book, the concept of e-values becomes more accessible for educational, research, and practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23614v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 17 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaditya Ramdas, Ruodu Wang</dc:creator>
    </item>
  </channel>
</rss>
