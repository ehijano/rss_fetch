<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 14 Jan 2025 03:26:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Jungle of Generative Drug Discovery: Traps, Treasures, and Ways Out</title>
      <link>https://arxiv.org/abs/2501.05457</link>
      <description>arXiv:2501.05457v1 Announce Type: new 
Abstract: "How to evaluate de novo designs proposed by a generative model?" Despite the transformative potential of generative deep learning in drug discovery, this seemingly simple question has no clear answer. The absence of standardized guidelines challenges both the benchmarking of generative approaches and the selection of molecules for prospective studies. In this work, we take a fresh $- \textit{critical}$ and $\textit{constructive} -$ perspective on de novo design evaluation. We systematically investigate widely used evaluation metrics and expose key pitfalls ('traps') that were previously overlooked. In addition, we identify tools ('treasures') and strategies ('ways out') to navigate the complex 'jungle' of generative drug discovery, and strengthen the connections between the molecular and deep learning fields along the way. Our systematic and large-scale results are expected to provide a new lens for evaluating the de novo designs proposed by generative deep learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05457v1</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>R{\i}za \"Oz\c{c}elik, Francesca Grisoni</dc:creator>
    </item>
    <item>
      <title>Prediction of Binding Affinity for ErbB Inhibitors Using Deep Neural Network Model with Morgan Fingerprints as Features</title>
      <link>https://arxiv.org/abs/2501.05607</link>
      <description>arXiv:2501.05607v1 Announce Type: new 
Abstract: The ErbB receptor family, including EGFR and HER2, plays a crucial role in cell growth and survival and is associated with the progression of various cancers such as breast and lung cancer. In this study, we developed a deep learning model to predict the binding affinity of ErbB inhibitors using molecular fingerprints derived from SMILES representations. The SMILES representations for each ErbB inhibitor were obtained from the ChEMBL database. We first generated Morgan fingerprints from the SMILES strings and applied AutoDock Vina docking to calculate the binding affinity values. After filtering the dataset based on binding affinity, we trained a deep neural network (DNN) model to predict binding affinity values from the molecular fingerprints. The model achieved significant performance, with a Mean Squared Error (MSE) of 0.2591, Mean Absolute Error (MAE) of 0.3658, and an R-squared value of 0.9389 on the training set. Although performance decreased slightly on the test set (R squared = 0.7731), the model still demonstrated robust generalization capabilities. These results indicate that the deep learning approach is highly effective for predicting the binding affinity of ErbB inhibitors, offering a valuable tool for virtual screening and drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05607v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>La Ode Aman</dc:creator>
    </item>
    <item>
      <title>Interpretable Enzyme Function Prediction via Residue-Level Detection</title>
      <link>https://arxiv.org/abs/2501.05644</link>
      <description>arXiv:2501.05644v1 Announce Type: new 
Abstract: Predicting multiple functions labeled with Enzyme Commission (EC) numbers from the enzyme sequence is of great significance but remains a challenge due to its sparse multi-label classification nature, i.e., each enzyme is typically associated with only a few labels out of more than 6000 possible EC numbers. However, existing machine learning algorithms generally learn a fixed global representation for each enzyme to classify all functions, thereby they lack interpretability and the fine-grained information of some function-specific local residue fragments may be overwhelmed. Here we present an attention-based framework, namely ProtDETR (Protein Detection Transformer), by casting enzyme function prediction as a detection problem. It uses a set of learnable functional queries to adaptatively extract different local representations from the sequence of residue-level features for predicting different EC numbers. ProtDETR not only significantly outperforms existing deep learning-based enzyme function prediction methods, but also provides a new interpretable perspective on automatically detecting different local regions for identifying different functions through cross-attentions between queries and residue-level features. Code is available at https://github.com/yangzhao1230/ProtDETR.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.05644v1</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhao Yang, Bing Su, Jiahao Chen, Ji-Rong Wen</dc:creator>
    </item>
    <item>
      <title>A unified cross-attention model for predicting antigen binding specificity to both HLA and TCR molecules</title>
      <link>https://arxiv.org/abs/2405.06653</link>
      <description>arXiv:2405.06653v2 Announce Type: replace 
Abstract: The immune checkpoint inhibitors have demonstrated promising clinical efficacy across various tumor types, yet the percentage of patients who benefit from them remains low. The bindings between tumor antigens and HLA-I/TCR molecules determine the antigen presentation and T-cell activation, thereby playing an important role in the immunotherapy response. In this paper, we propose UnifyImmun, a unified cross-attention transformer model designed to simultaneously predict the bindings of peptides to both receptors, providing more comprehensive evaluation of antigen immunogenicity. We devise a two-phase strategy using virtual adversarial training that enables these two tasks to reinforce each other mutually, by compelling the encoders to extract more expressive features. Our method demonstrates superior performance in predicting both pHLA and pTCR binding on multiple independent and external test sets. Notably, on a large-scale COVID-19 pTCR binding test set without any seen peptide in training set, our method outperforms the current state-of-the-art methods by more than 10\%. The predicted binding scores significantly correlate with the immunotherapy response and clinical outcomes on two clinical cohorts. Furthermore, the cross-attention scores and integrated gradients reveal the amino-acid sites critical for peptide binding to receptors. In essence, our approach marks a significant step toward comprehensive evaluation of antigen immunogenicity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06653v2</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <pubDate>Mon, 13 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chenpeng Yu, Xing Fang, Hui Liu</dc:creator>
    </item>
  </channel>
</rss>
