<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 04 Jun 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Brain Morphology Normative modelling platform for abnormality and Centile estimation: Brain MoNoCle</title>
      <link>https://arxiv.org/abs/2406.01107</link>
      <description>arXiv:2406.01107v1 Announce Type: new 
Abstract: Normative models of brain structure estimate the effects of covariates such as age and sex using large samples of healthy controls. These models can then be applied to smaller clinical cohorts to distinguish disease effects from other covariates. However, these advanced statistical modelling approaches can be difficult to access, and processing large healthy cohorts is computationally demanding. Thus, accessible platforms with pre-trained normative models are needed.
  We present such a platform for brain morphology analysis as an open-source web application https://cnnplab.shinyapps.io/normativemodelshiny/, with six key features: (i) user-friendly web interface, (ii) individual and group outputs, (iii) multi-site analysis, (iv) regional and whole-brain analysis, (v) integration with existing tools, and (vi) featuring multiple morphology metrics.
  Using a diverse sample of 3,276 healthy controls across 13 sites, we pre-trained normative models on various metrics. We validated the models with a small clinical sample of bipolar disorder, showing outputs that aligned closely with existing literature only after applying our normative modelling. Further validation with a temporal lobe epilepsy dataset also showed agreement with previous group-level findings and individual-level seizure lateralisation. Finally, with the ability to investigate multiple morphology measures in the same framework, we found that biological covariates are better explained in specific morphology measures, and for clinical applications, only some measures are sensitive to the disease process.
  Our platform offers a comprehensive framework to analyze brain morphology in clinical and research settings. Validations confirm the superiority of normative models and the advantage of investigating a range of brain morphology metrics together.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01107v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bethany Little, Nida Alyas, Alexander Surtees, Peter Taylor, Karoline Leiberg, Yujiang Wang</dc:creator>
    </item>
    <item>
      <title>Pattern Formation in a Spiking Neural-Field of Renewal Neurons</title>
      <link>https://arxiv.org/abs/2406.01167</link>
      <description>arXiv:2406.01167v1 Announce Type: new 
Abstract: Elucidating the neurophysiological mechanisms underlying neural pattern formation remains an outstanding challenge in Computational Neuroscience. In this paper, we address the issue of understanding the emergence of neural patterns by considering a network of renewal neurons, a well-established class of spiking cells. Taking the thermodynamics limit, the network's dynamics can be accurately represented by a partial differential equation coupled with a nonlocal differential equation. The stationary state of the nonlocal system is determined, and a perturbation analysis is performed to analytically characterize the conditions for the occurrence of Turing instabilities. Considering neural network parameters such as the synaptic coupling and the external drive, we numerically obtain the bifurcation line that separates the asynchronous regime from the emergence of patterns. Our theoretical findings provide a new and insightful perspective on the emergence of Turing patterns in spiking neural networks. In the long term, our formalism will enable the study of neural patterns while maintaining the connections between microscopic cellular properties, network coupling, and the emergence of Turing instabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01167v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Dumont, Carmen Oana Tarniceriu</dc:creator>
    </item>
    <item>
      <title>Uncovering dynamical equations of stochastic decision models using data-driven SINDy algorithm</title>
      <link>https://arxiv.org/abs/2406.01370</link>
      <description>arXiv:2406.01370v1 Announce Type: new 
Abstract: Decision formation in perceptual decision-making involves sensory evidence accumulation instantiated by the temporal integration of an internal decision variable towards some decision criterion or threshold, as described by sequential sampling theoretical models. The decision variable can be represented in the form of experimentally observable neural activities. Hence, elucidating the appropriate theoretical model becomes crucial to understanding the mechanisms underlying perceptual decision formation. Existing computational methods are limited to either fitting of choice behavioural data or linear model estimation from neural activity data. In this work, we made use of sparse identification of nonlinear dynamics (SINDy), a data-driven approach, to elucidate the deterministic linear and nonlinear components of often-used stochastic decision models within reaction time task paradigms. Based on the simulated decision variable activities of the models, SINDy, enhanced with a trial-averaging approach, could readily uncover the dynamical equations of the models while predicting the models' choice accuracy and decision time across a range of signal-to-noise ratio values. In particular, SINDy performed relatively better for decision models which have an accelerating dynamical component during decision formation, as expressed by a metastable linear competing accumulator model and a nonlinear bistable model. Taken together, our work suggests that SINDy can be a useful tool for uncovering the dynamics in perceptual decision-making, and more generally, for first-passage time problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01370v1</guid>
      <category>q-bio.NC</category>
      <category>physics.data-an</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Lenfesty, Saugat Bhattacharyya, KongFatt Wong-Lin</dc:creator>
    </item>
    <item>
      <title>Evidence for five types of fixation during a random saccade eye tracking task: Implications for the study of oculomotor fatigue</title>
      <link>https://arxiv.org/abs/2406.01496</link>
      <description>arXiv:2406.01496v1 Announce Type: new 
Abstract: Our interest was to evaluate changes in fixation duration as a function of time-on-task (TOT) during a random saccade task. We employed a large, publicly available dataset. The frequency histogram of fixation durations was multimodal and modelled as a Gaussian mixture. We found five fixation types. The ``ideal'' response would be a single accurate saccade after each target movement, with a typical saccade latency of 200-250 msec, followed by a long fixation (&gt; 800 msec) until the next target jump. We found fixations like this, but they comprised only 10% of all fixations and were the first fixation after target movement only 23.4% of the time. More frequently (57.4% of the time), the first fixation after target movement was short (117.7 msec mean) and was commonly followed by a corrective saccade. Across the entire 100 sec of the task, median total fixation duration decreased. This decrease was approximated with a power law fit with R^2=0.94. A detailed examination of the frequency of each of our five fixation types over time on task (TOT) revealed that the three shortest duration fixation types became more and more frequent with TOT whereas the two longest fixations became less and less frequent. In all cases, the changes over TOT followed power law relationships, with R^2 values between 0.73 and 0.93. We concluded that, over the 100 second duration of our task, long fixations are common in the first 15 to 22 seconds but become less common after that. Short fixations are relatively uncommon in the first 15 to 22 seconds but become more and more common as the task progressed. Apparently. the ability to produce an ideal response, although somewhat likely in the first 22 seconds, rapidly declines. This might be related to a noted decline in saccade accuracy over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01496v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lee Friedman, Oleg V. Komogortsev</dc:creator>
    </item>
    <item>
      <title>Recent Trends in Insect and Robot Navigation through the Lens of Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2406.01501</link>
      <description>arXiv:2406.01501v1 Announce Type: new 
Abstract: Bees are among the master navigators of the insect world. Despite impressive advances in robot navigation research, the performance of these insects is still unrivaled by any artificial system in terms of training efficiency and generalization capabilities, particularly considering the limited computational capacity. On the other hand, computational principles underlying these extraordinary feats are still only partially understood. The theoretical framework of reinforcement learning (RL) provides an ideal focal point to bring the two fields together for mutual benefit. While RL has long been at the core of robot navigation research, current computational theories of insect navigation are not commonly formulated within this framework, but largely as an associative learning process implemented in the insect brain, especially in a region called the mushroom body. Here we argue that this neural substrate can be understood as implementing a certain class of relatively simple RL algorithms, capable of integrating distinct components of a navigation task, reminiscent of hierarchical RL models used in robot navigation. The efficiency of insect navigation is likely rooted in an efficient and robust internal representation of space, linking retinotopic (egocentric) visual input with the geometry of the environment. We discuss how current models of insect and robot navigation are exploring representations beyond classical, complete map-like representations, with spatial information being embedded in the respective latent representations to varying degrees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01501v1</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephan Lochner, Daniel Honerkamp, Abhinav Valada, Andrew D. Straw</dc:creator>
    </item>
    <item>
      <title>Tilting the Odds at the Lottery: the Interplay of Overparameterisation and Curricula in Neural Networks</title>
      <link>https://arxiv.org/abs/2406.01589</link>
      <description>arXiv:2406.01589v1 Announce Type: cross 
Abstract: A wide range of empirical and theoretical works have shown that overparameterisation can amplify the performance of neural networks. According to the lottery ticket hypothesis, overparameterised networks have an increased chance of containing a sub-network that is well-initialised to solve the task at hand. A more parsimonious approach, inspired by animal learning, consists in guiding the learner towards solving the task by curating the order of the examples, i.e. providing a curriculum. However, this learning strategy seems to be hardly beneficial in deep learning applications. In this work, we undertake an analytical study that connects curriculum learning and overparameterisation. In particular, we investigate their interplay in the online learning setting for a 2-layer network in the XOR-like Gaussian Mixture problem. Our results show that a high degree of overparameterisation -- while simplifying the problem -- can limit the benefit from curricula, providing a theoretical account of the ineffectiveness of curricula in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01589v1</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Sarao Mannelli, Yaraslau Ivashinka, Andrew Saxe, Luca Saglietti</dc:creator>
    </item>
    <item>
      <title>Bridging Cognitive Maps: a Hierarchical Active Inference Model of Spatial Alternation Tasks and the Hippocampal-Prefrontal Circuit</title>
      <link>https://arxiv.org/abs/2308.11463</link>
      <description>arXiv:2308.11463v2 Announce Type: replace 
Abstract: Cognitive problem-solving benefits from cognitive maps aiding navigation and planning. Previous studies revealed that cognitive maps for physical space navigation involve hippocampal (HC) allocentric codes, while cognitive maps for abstract task space engage medial prefrontal cortex (mPFC) task-specific codes. Solving challenging cognitive tasks requires integrating these two types of maps. This is exemplified by spatial alternation tasks in multi-corridor settings, where animals like rodents are rewarded upon executing an alternation pattern in maze corridors. Existing studies demonstrated the HC - mPFC circuit's engagement in spatial alternation tasks and that its disruption impairs task performance. Yet, a comprehensive theory explaining how this circuit integrates task-related and spatial information is lacking. We advance a novel hierarchical active inference model clarifying how the HC - mPFC circuit enables the resolution of spatial alternation tasks, by merging physical and task-space cognitive maps. Through a series of simulations, we demonstrate that the model's dual layers acquire effective cognitive maps for navigation within physical (HC map) and task (mPFC map) spaces, using a biologically-inspired approach: a clone-structured cognitive graph. The model solves spatial alternation tasks through reciprocal interactions between the two layers. Importantly, disrupting inter-layer communication impairs difficult decisions, consistent with empirical findings. The same model showcases the ability to switch between multiple alternation rules. However, inhibiting message transmission between the two layers results in perseverative behavior, consistent with empirical findings. In summary, our model provides a mechanistic account of how the HC - mPFC circuit supports spatial alternation tasks and how its disruption impairs task performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.11463v2</guid>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toon Van de Maele, Bart Dhoedt, Tim Verbelen, Giovanni Pezzulo</dc:creator>
    </item>
    <item>
      <title>The Topology and Geometry of Neural Representations</title>
      <link>https://arxiv.org/abs/2309.11028</link>
      <description>arXiv:2309.11028v3 Announce Type: replace 
Abstract: A central question for neuroscience is how to characterize brain representations of perceptual and cognitive content. An ideal characterization should distinguish different functional regions with robustness to noise and idiosyncrasies of individual brains that do not correspond to computational differences. Previous studies have characterized brain representations by their representational geometry, which is defined by the representational dissimilarity matrix (RDM), a summary statistic that abstracts from the roles of individual neurons (or responses channels) and characterizes the discriminability of stimuli. Here we explore a further step of abstraction: from the geometry to the topology of brain representations. We propose topological representational similarity analysis (tRSA), an extension of representational similarity analysis (RSA) that uses a family of geo-topological summary statistics that generalizes the RDM to characterize the topology while de-emphasizing the geometry. We evaluate this new family of statistics in terms of the sensitivity and specificity for model selection using both simulations and fMRI data. In the simulations, the ground truth is a data-generating layer representation in a neural network model and the models are the same and other layers in different model instances (trained from different random seeds). In fMRI, the ground truth is a visual area and the models are the same and other areas measured in different subjects. Results show that topology-sensitive characterizations of population codes are robust to noise and interindividual variability and maintain excellent sensitivity to the unique representational signatures of different neural network layers and brain regions. These methods enable researchers to calibrate comparisons among representations in brains and models to be sensitive to the geometry, the topology, or a combination of both.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11028v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Baihan Lin, Nikolaus Kriegeskorte</dc:creator>
    </item>
    <item>
      <title>Spiking mode-based neural networks</title>
      <link>https://arxiv.org/abs/2310.14621</link>
      <description>arXiv:2310.14621v2 Announce Type: replace 
Abstract: Spiking neural networks play an important role in brain-like neuromorphic computations and in studying working mechanisms of neural circuits. One drawback of training a large scale spiking neural network is that updating all weights is quite expensive. Furthermore, after training, all information related to the computational task is hidden into the weight matrix, prohibiting us from a transparent understanding of circuit mechanisms. Therefore, in this work, we address these challenges by proposing a spiking mode-based training protocol, where the recurrent weight matrix is explained as a Hopfield-like multiplication of three matrices: input, output modes and a score matrix. The first advantage is that the weight is interpreted by input and output modes and their associated scores characterizing the importance of each decomposition term. The number of modes is thus adjustable, allowing more degrees of freedom for modeling the experimental data. This significantly reduces the training cost because of significantly reduced space complexity for learning. Training spiking networks is thus carried out in the mode-score space. The second advantage is that one can project the high dimensional neural activity (filtered spike train) in the state space onto the mode space which is typically of a low dimension, e.g., a few modes are sufficient to capture the shape of the underlying neural manifolds. We successfully apply our framework in two computational tasks -- digit classification and selective sensory integration tasks. Our method accelerate the training of spiking neural networks by a Hopfield-like decomposition, and moreover this training leads to low-dimensional attractor structures of high-dimensional neural dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.14621v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhanghan Lin, Haiping Huang</dc:creator>
    </item>
    <item>
      <title>Motor Imagery Task Alters Dynamics of Human Body Posture</title>
      <link>https://arxiv.org/abs/2405.19228</link>
      <description>arXiv:2405.19228v3 Announce Type: replace 
Abstract: Motor Imagery (MI) is gaining traction in both rehabilitation and sports settings, but its immediate influence on human postural control is not yet clearly understood. The focus of this study is to examine the effects of MI on the dynamics of the Center of Pressure (COP), a crucial metric for evaluating postural stability. In the experiment, thirty healthy young adults participated in four different scenarios: normal standing with both open and closed eyes, and kinesthetic motor imagery focused on mediolateral (ML) and anteroposterior (AP) sway movements. A mathematical model was developed to characterize the nonlinear dynamics of the COP and to assess the impact of MI on these dynamics. Our results show a statistically significant increase (p-value&lt;0.05) in variables such as COP path length and Long-Range Correlation (LRC) during MI compared to the closed-eye and normal standing conditions. These observations align well with psycho-neuromuscular theory, which suggests that imagining a specific movement activates neural pathways, consequently affecting postural control. This study presents compelling evidence that motor imagery not only has a quantifiable impact on COP dynamics but also that changes in the Center of Pressure (COP) are directionally consistent with the imagined movements. This finding holds significant implications for the field of rehabilitation science, suggesting that motor imagery could be strategically utilized to induce targeted postural adjustments. Nonetheless, additional research is required to fully understand the complex mechanisms that underlie this relationship and to corroborate these results across a more diverse set of populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19228v3</guid>
      <category>q-bio.NC</category>
      <category>eess.SP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Fatemeh Delavari, Seyyed Mohammad Reza Hashemi Golpayegani, Mohammad Ali Ahmadi-Pajouh</dc:creator>
    </item>
    <item>
      <title>The Multiscale Surface Vision Transformer</title>
      <link>https://arxiv.org/abs/2303.11909</link>
      <description>arXiv:2303.11909v2 Announce Type: replace-cross 
Abstract: Surface meshes are a favoured domain for representing structural and functional information on the human cortex, but their complex topology and geometry pose significant challenges for deep learning analysis. While Transformers have excelled as domain-agnostic architectures for sequence-to-sequence learning, the quadratic cost of the self-attention operation remains an obstacle for many dense prediction tasks. Inspired by some of the latest advances in hierarchical modelling with vision transformers, we introduce the Multiscale Surface Vision Transformer (MS-SiT) as a backbone architecture for surface deep learning. The self-attention mechanism is applied within local-mesh-windows to allow for high-resolution sampling of the underlying data, while a shifted-window strategy improves the sharing of information between windows. Neighbouring patches are successively merged, allowing the MS-SiT to learn hierarchical representations suitable for any prediction task. Results demonstrate that the MS-SiT outperforms existing surface deep learning methods for neonatal phenotyping prediction tasks using the Developing Human Connectome Project (dHCP) dataset. Furthermore, building the MS-SiT backbone into a U-shaped architecture for surface segmentation demonstrates competitive results on cortical parcellation using the UK Biobank (UKB) and manually-annotated MindBoggle datasets. Code and trained models are publicly available at https://github.com/metrics-lab/surface-vision-transformers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.11909v2</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Dahan, Logan Z. J. Williams, Daniel Rueckert, Emma C. Robinson</dc:creator>
    </item>
    <item>
      <title>Semantically-correlated memories in a dense associative model</title>
      <link>https://arxiv.org/abs/2404.07123</link>
      <description>arXiv:2404.07123v3 Announce Type: replace-cross 
Abstract: I introduce a novel associative memory model named Correlated Dense Associative Memory (CDAM), which integrates both auto- and hetero-association in a unified framework for continuous-valued memory patterns. Employing an arbitrary graph structure to semantically link memory patterns, CDAM is theoretically and numerically analysed, revealing four distinct dynamical modes: auto-association, narrow hetero-association, wide hetero-association, and neutral quiescence. Drawing inspiration from inhibitory modulation studies, I employ anti-Hebbian learning rules to control the range of hetero-association, extract multi-scale representations of community structures in graphs, and stabilise the recall of temporal sequences. Experimental demonstrations showcase CDAM's efficacy in handling real-world data, replicating a classical neuroscience experiment, performing image retrieval, and simulating arbitrary finite automata.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07123v3</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas F Burns</dc:creator>
    </item>
  </channel>
</rss>
