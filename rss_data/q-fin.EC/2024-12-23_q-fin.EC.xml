<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.EC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.EC</link>
    <description>q-fin.EC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.EC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 03:45:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Countries across the world use more land for golf courses than wind or solar energy</title>
      <link>https://arxiv.org/abs/2412.15376</link>
      <description>arXiv:2412.15376v1 Announce Type: new 
Abstract: Land use is a critical factor in the siting of renewable energy facilities and is often scrutinized due to perceived conflicts with other land demands. Meanwhile, substantial areas are devoted to activities such as golf, which are accessible to only a select few and have a significant land and environmental footprint. Our study shows that in countries such as the United States and the United Kingdom, far more land is allocated to golf courses than to renewable energy facilities. Areas equivalent to those currently used for golf could support the installation of up to 842 GW of solar and 659 GW of wind capacity in the top ten countries with the most golf courses. In many of these countries, this potential exceeds both current installed capacity and medium-term projections. These findings underscore the untapped potential of rethinking land use priorities to accelerate the transition to renewable energy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15376v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jann Weinand, Tristan Pelser, Max Kleinebrahm, Detlef Stolten</dc:creator>
    </item>
    <item>
      <title>A Scenario-Based Assessment of the Long-Term Funding Adequacy of the German Nuclear Waste Fund KENFO</title>
      <link>https://arxiv.org/abs/2412.16126</link>
      <description>arXiv:2412.16126v1 Announce Type: new 
Abstract: The German site selection process for high-level nuclear waste was initially planned to conclude in 2031, with the deep geological repository sealed around the year 2080. However, in 2022, substantial delays were announced by the responsible federal agency, pushing the site selection to 2046 or even 2068. With this delay come uncertainties regarding the duration, consequential knowledge management, and funding. German nuclear waste management activities are funded by the external segregated fund KENFO, which is designed to ensure sufficient funding via generating returns on investments (ROI) in the coming decades. Given recent developments, we assess the adequacy of the fund volume based on seven scenarios depicting potential process delays. We find that the target ROI of 3.7% will not suffice in any case, even if the site selection concludes in 2031, and that cash injections of up to EUR31.07 billion are necessary today to ensure that the fund volume will suffice. We conclude that cost estimations must be updated, KENFO must increase its target ROIs, potential capital injections must be openly discussed by policymakers, and a procedural acceleration should be implemented to ensure that financial liabilities for nuclear waste management are minimized for future taxpayers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16126v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Awawda, Alexander Wimmers</dc:creator>
    </item>
    <item>
      <title>Revisiting Global Income Convergence in the 21st Century</title>
      <link>https://arxiv.org/abs/2412.16127</link>
      <description>arXiv:2412.16127v1 Announce Type: new 
Abstract: This paper revisits the debate on income convergence between poor and rich countries. I challenge the view that there is little to no catch-up, and that changes in total factor productivity (TFP) drives cross-country income differences. Since 2000, income levels in poor countries have converged with rich countries at 0.8% annually, rising to 1.5% when excluding Sub-Saharan Africa. A growth accounting exercise incorporating capital income share heterogeneity shows that most convergence since 1980, and over half since 2000 outside Sub-Saharan Africa, results from convergence in physical and human capital inputs rather than TFP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16127v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bipul Verma</dc:creator>
    </item>
    <item>
      <title>Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs</title>
      <link>https://arxiv.org/abs/2412.15239</link>
      <description>arXiv:2412.15239v1 Announce Type: cross 
Abstract: Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters from Wattpad, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15239v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hortense Fong, George Gui</dc:creator>
    </item>
    <item>
      <title>Quantifying detection rates for dangerous capabilities: a theoretical model of dangerous capability evaluations</title>
      <link>https://arxiv.org/abs/2412.15433</link>
      <description>arXiv:2412.15433v1 Announce Type: cross 
Abstract: We present a quantitative model for tracking dangerous AI capabilities over time. Our goal is to help the policy and research community visualise how dangerous capability testing can give us an early warning about approaching AI risks. We first use the model to provide a novel introduction to dangerous capability testing and how this testing can directly inform policy. Decision makers in AI labs and government often set policy that is sensitive to the estimated danger of AI systems, and may wish to set policies that condition on the crossing of a set threshold for danger. The model helps us to reason about these policy choices. We then run simulations to illustrate how we might fail to test for dangerous capabilities. To summarise, failures in dangerous capability testing may manifest in two ways: higher bias in our estimates of AI danger, or larger lags in threshold monitoring. We highlight two drivers of these failure modes: uncertainty around dynamics in AI capabilities and competition between frontier AI labs. Effective AI policy demands that we address these failure modes and their drivers. Even if the optimal targeting of resources is challenging, we show how delays in testing can harm AI policy. We offer preliminary recommendations for building an effective testing ecosystem for dangerous capabilities and advise on a research agenda.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15433v1</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.MA</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paolo Bova, Alessandro Di Stefano, The Anh Han</dc:creator>
    </item>
    <item>
      <title>Multi-scale reconstruction of large supply networks</title>
      <link>https://arxiv.org/abs/2412.16122</link>
      <description>arXiv:2412.16122v1 Announce Type: cross 
Abstract: The structure of the supply chain network has important implications for modelling economic systems, from growth trajectories to responses to shocks or natural disasters. However, reconstructing firm-to-firm networks from available information poses several practical and theoretical challenges: the lack of publicly available data, the complexity of meso-scale structures, and the high level of heterogeneity of firms. With this work we contribute to the literature on economic network reconstruction by proposing a novel methodology based on a recently developed multi-scale model. This approach has three main advantages over other methods: its parameters are defined to maintain statistical consistency at different scales of node aggregation, it can be applied in a multi-scale setting, and it is computationally more tractable for very large graphs. The consistency at different scales of aggregation, inherent to the model definition, is preserved for any hierarchy of coarse-grainings. The arbitrariness of the aggregation allows us to work across different scales, making it possible to estimate model parameters even when node information is inconsistent, such as when some nodes are firms while others are countries or regions. Finally, the model can be fitted at an aggregate scale with lower computational requirements, since the parameters are invariant to the grouping of nodes. We assess the advantages and limitations of this approach by testing it on two complementary datasets of Dutch firms constructed from inter-client transactions on the bank accounts of two major Dutch banking institutions. We show that the model reliably predicts important topological properties of the observed network in several scenarios of practical interest and is therefore a suitable candidate for reconstructing firm-to-firm networks at scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16122v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonardo Niccol\`o Ialongo, Sylvain Bangma, Fabian Jansen, Diego Garlaschelli</dc:creator>
    </item>
    <item>
      <title>Trust Dynamics and Market Behavior in Cryptocurrency: A Comparative Study of Centralized and Decentralized Exchanges</title>
      <link>https://arxiv.org/abs/2404.17227</link>
      <description>arXiv:2404.17227v2 Announce Type: replace 
Abstract: In the rapidly evolving cryptocurrency landscape, trust is a critical yet underexplored factor shaping market behaviors and driving user preferences between centralized exchanges (CEXs) and decentralized exchanges (DEXs). Despite its importance, trust remains challenging to measure, limiting the study of its effects on market dynamics. The collapse of FTX, a major CEX, provides a unique natural experiment to examine the measurable impacts of trust and its sudden erosion on the cryptocurrency ecosystem. This pivotal event raised questions about the resilience of centralized trust systems and accelerated shifts toward decentralized alternatives. This research investigates the impacts of the FTX collapse on user trust, focusing on token valuation, trading flows, and sentiment dynamics. Employing causal inference methods, including Regression Discontinuity Design (RDD) and Difference-in-Differences (DID), we reveal significant declines in WETH prices and NetFlow from CEXs to DEXs, signaling a measurable transfer of trust. Additionally, natural language processing methods, including topic modeling and sentiment analysis, uncover the complexities of user responses, highlighting shifts from functional discussions to emotional fragmentation in Binance's community, while Uniswap's sentiment exhibits a gradual upward trend. Despite data limitations and external influences, the findings underscore the intricate interplay between trust, sentiment, and market behavior in the cryptocurrency ecosystem. By bridging blockchain analytics, behavioral finance, and decentralized finance (DeFi), this study contributes to interdisciplinary research, offering a deeper understanding of distributed trust mechanisms and providing critical insights for future investigations into the socio-technical dimensions of trust in digital economies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17227v2</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xintong Wu, Wanlin Deng, Yutong Quan, Luyao Zhang</dc:creator>
    </item>
    <item>
      <title>Golden parachutes under the threat of accidents</title>
      <link>https://arxiv.org/abs/2312.02101</link>
      <description>arXiv:2312.02101v3 Announce Type: replace-cross 
Abstract: This paper addresses a continuous-time contracting model that extends the problem introduced by Sannikov and later rigorously analysed by Possama\"{i} and Touzi. In our model, a principal hires a risk-averse agent to carry out a project. Specifically, the agent can perform two different tasks, namely to increase the instantaneous growth rate of the project's value, and to reduce the likelihood of accidents occurring. In order to compensate for these costly actions, the principal offers a continuous stream of payments throughout the entire duration of a contract, which concludes at a random time, potentially resulting in a lump-sum payment. We examine the consequences stemming from the introduction of accidents, modelled by a compound Poisson process that negatively impact the project's value. Furthermore, we investigate whether certain economic scenarii are still characterised by a golden parachute as in Sannikov's model. A golden parachute refers to a situation where the agent stops working and subsequently receives a compensation, which may be either a lump-sum payment leading to termination of the contract or a continuous stream of payments, thereby corresponding to a pension.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.02101v3</guid>
      <category>math.PR</category>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Possama\"i, Chiara Rossato</dc:creator>
    </item>
    <item>
      <title>News Deja Vu: Connecting Past and Present with Semantic Search</title>
      <link>https://arxiv.org/abs/2406.15593</link>
      <description>arXiv:2406.15593v2 Announce Type: replace-cross 
Abstract: Social scientists and the general public often analyze contemporary events by drawing parallels with the past, a process complicated by the vast, noisy, and unstructured nature of historical texts. For example, hundreds of millions of page scans from historical newspapers have been noisily transcribed. Traditional sparse methods for searching for relevant material in these vast corpora, e.g., with keywords, can be brittle given complex vocabularies and OCR noise. This study introduces News Deja Vu, a novel semantic search tool that leverages transformer large language models and a bi-encoder approach to identify historical news articles that are most similar to modern news queries. News Deja Vu first recognizes and masks entities, in order to focus on broader parallels rather than the specific named entities being discussed. Then, a contrastively trained, lightweight bi-encoder retrieves historical articles that are most similar semantically to a modern query, illustrating how phenomena that might seem unique to the present have varied historical precedents. Aimed at social scientists, the user-friendly News Deja Vu package is designed to be accessible for those who lack extensive familiarity with deep learning. It works with large text datasets, and we show how it can be deployed to a massive scale corpus of historical, open-source news articles. While human expertise remains important for drawing deeper insights, News Deja Vu provides a powerful tool for exploring parallels in how people have perceived past and present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15593v2</guid>
      <category>cs.CL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brevin Franklin, Emily Silcock, Abhishek Arora, Tom Bryan, Melissa Dell</dc:creator>
    </item>
  </channel>
</rss>
