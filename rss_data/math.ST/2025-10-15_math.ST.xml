<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Random Modulation with Spherical Symmetry</title>
      <link>https://arxiv.org/abs/2510.12928</link>
      <description>arXiv:2510.12928v1 Announce Type: new 
Abstract: We consider the modulation of data given by random vectors $X_n \in \mathbb{R}^{d_n}$, $n \in \mathbb{N}$. For each $X_n$, one chooses an independent modulating random vector $\Xi_n \in \mathbb{R}^{d_n}$ and forms the projection $Y_n = \Xi_n'X_n$. It is shown, under regularity conditions on $X_n$ and $\Xi_n$, that $Y_n|\Xi_n$ converges weakly in probability to a normal distribution. More broadly, the conditional joint distribution of a family of projections constructed from random samples from $X_n$ and $\Xi_n$ is shown to converge weakly to a matrix normal distribution. We derive, \textit{via} G. P\'olya's characterization of the normal distribution, a necessary and sufficient condition on $Y_n$ for $\Xi_n$ to be normally distributed. When $\Xi_n$ has a spherically symmetric distribution we deduce, through I. J. Schoenberg's characterization of the spherically symmetric characteristic functions on Hilbert spaces, that the probability density function of $Y_n|\Xi_n$ converges pointwise in certain $p$th means to a mixture of normal densities and the rate of convergence is quantified, resulting in uniform convergence. The cumulative distribution function of $Y_n|\Xi_n$ is shown to converge uniformly in those $p$th means to the distribution function of the same mixture, and a Lipschitz property is obtained. Examples of distributions satisfying our results are provided; these include Bingham distributions on hyperspheres of random radii, uniform distributions on hyperspheres and hypercubes of random volumes, and multivariate normal distributions; and examples of such $\Xi_n$ include the multivariate $t$-, multivariate Laplace, and spherically symmetric stable distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12928v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Armine Bagyan, Donald Richards</dc:creator>
    </item>
    <item>
      <title>A Generalized Notion of Completeness and Its Application</title>
      <link>https://arxiv.org/abs/2510.13174</link>
      <description>arXiv:2510.13174v1 Announce Type: new 
Abstract: From the perspective of data reduction, the notions of minimal sufficient and complete statistics together play an important role in determining optimal statistics (estimators). The classical notion of sufficiency and completeness are not adequate in many robust estimations that are based on different divergences. Recently, the notion of generalized sufficiency based on a generalized likelihood function was introduced in the literature. It is important to note that the concept of sufficiency alone does not necessarily produce optimal statistics (estimators). Thus, in line with the generalized sufficiency, we introduce a generalized notion of completeness with respect to a generalized likelihood function. We then characterize the family of probability distributions that possesses completeness with respect to the generalized likelihood function associated with the density power divergence (DPD). Moreover, we show that the family of distributions associated with the logarithmic density power divergence (LDPD) is not complete. Further, we extend the Lehmann-Scheff\'e theorem and the Basu's theorem for the generalized likelihood estimation. Subsequently, we obtain the generalized uniformly minimum variance unbiased estimator (UMVUE) for the $\mathcal{B^{(\alpha)}}$-family. Further, we derive an formula of the asymptotic expected deficiency (AED) that is used to compare the performance between the minimum density power divergence estimator (MDPDE) and the generalized UMVUE for $\mathcal{B^{(\alpha)}}$-family. Finally, we provide an application of the developed results in stress-strength reliability model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13174v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Himanshi Singh, Tanmay Sahoo, Nil Kamal Hazra</dc:creator>
    </item>
    <item>
      <title>On Generalized Likelihood Estimation Based on the Logarithmic Norm Relative Entropy</title>
      <link>https://arxiv.org/abs/2510.13179</link>
      <description>arXiv:2510.13179v1 Announce Type: new 
Abstract: Traditional likelihood based methods for parameter estimation get highly affected when the given data is contaminated by outliers even in a small proportion. In this paper, we consider a robust parameter estimation method, namely the minimum logarithmic norm relative entropy (LNRE) estimation procedure, and study different (generalized) sufficiency principles associated with it. We introduce a new two-parameter power-law family of distributions (namely, $\mathcal{M}^{(\alpha,\beta)}$-family), which is shown to have a fixed number of sufficient statistics, independent of the sample size, with respect to the generalized likelihood function associated with the LNRE. Then, we obtain the generalized minimal sufficient statistic for this family and derive the generalized Rao-Blackwell theorem and the generalized Cram\'{e}r-Rao lower bound for the minimum LNRE estimation. We also study the minimum LNRE estimators (MLNREEs) for the family of Student's distributions particularly in detail. Our general results reduces to the classical likelihood based results under the exponential family of distributions at specific choices of the tuning parameter $\alpha$ and $\beta$. Finally, we present simulation studies followed by a real data analysis, which highlight the practical utility of the MLNREEs for data contaminated by possible outliers. Along the way we also correct a mistake found in a recent paper on related theory of generalized likelihoods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13179v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Himanshi Singh, Abhik Ghosh, Nil Kamal Hazra</dc:creator>
    </item>
    <item>
      <title>Control variates for variance-reduced ratio of means estimators</title>
      <link>https://arxiv.org/abs/2510.13504</link>
      <description>arXiv:2510.13504v1 Announce Type: new 
Abstract: The control variates method is a classical variance reduction technique for Monte Carlo estimators that exploits correlated auxiliary variables without introducing bias. In many applications, the quantity of interest can be expressed as a ratio of expectations. We propose a variance-reduced estimator for such ratios, which applies control variates to both the numerator and the denominator. The control variate coefficients are optimized jointly to minimize the variance of the resulting estimator. This approach theoretically guarantees variance reduction and naturally extends to approximate control variates. Simulation studies show significant variance reduction, particularly when correlations between variables and control variates are strong. The practical value of the method is illustrated with a multi-fidelity aircraft design use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13504v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Louison Bocquet-Nouaille, J\'er\^ome Morio, Benjamin Bobbia</dc:creator>
    </item>
    <item>
      <title>The generalized Marshall-Olkin Lomax distribution with applications to AIDS and COVID-19 data</title>
      <link>https://arxiv.org/abs/2510.13605</link>
      <description>arXiv:2510.13605v1 Announce Type: new 
Abstract: The generalized Marshall-Olkin Lomax distribution is introduced, and its properties are easily obtained from those of the Lomax distribution. A regression model for censored data is proposed. The parameters are estimated through maximum likelihood, and consistency is verified by simulations. Three real datasets are selected to illustrate the superiority of the new models compared to those from two well-known classes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13605v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexsandro A. Ferreira, Gauss M. Cordeiro</dc:creator>
    </item>
    <item>
      <title>Towards an Asymptotic Efficiency Theory on Regular Parameter Manifolds</title>
      <link>https://arxiv.org/abs/2510.13703</link>
      <description>arXiv:2510.13703v1 Announce Type: new 
Abstract: Asymptotic efficiency theory is one of the pillars in the foundations of modern mathematical statistics. Not only does it serve as a rigorous theoretical benchmark for evaluating statistical methods, but it also sheds light on how to develop and unify novel statistical procedures. For example, the calculus of influence functions has led to many important statistical breakthroughs in the past decades. Responding to the pressing challenge of analyzing increasingly complex datasets, particularly those with non-Euclidean/nonlinear structures, many novel statistical models and methods have been proposed in recent years. However, the existing efficiency theory is not always readily applicable to these cases, as the theory was developed, for the most part, under the often neglected premise that both the sample space and the parameter space are normed linear spaces. As a consequence, efficiency results outside normed linear spaces are quite rare and isolated, obtained on a case-by-case basis. This paper aims to develop a more unified asymptotic efficiency theory, allowing the sample space, the parameter space, or both to be Riemannian manifolds satisfying certain regularity conditions. We build a vocabulary that helps translate essential concepts in efficiency theory from normed linear spaces to Riemannian manifolds, such as (locally) regular estimators, differentiable functionals, etc. Efficiency bounds are established under conditions parallel to those for normed linear spaces. We also demonstrate the conceptual advantage of the new framework by applying it to two concrete examples in statistics: the population Frechet mean and the regression coefficient vector of Single-Index Models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13703v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lvfang Sun, Zhenhua Lin, Lin Liu</dc:creator>
    </item>
    <item>
      <title>Optimal Bounds for Tyler's M-Estimator for Elliptical Distributions</title>
      <link>https://arxiv.org/abs/2510.13751</link>
      <description>arXiv:2510.13751v1 Announce Type: new 
Abstract: A fundamental problem in statistics is estimating the shape matrix of an Elliptical distribution. This generalizes the familiar problem of Gaussian covariance estimation, for which the sample covariance achieves optimal estimation error. For Elliptical distributions, Tyler proposed a natural M-estimator and showed strong statistical properties in the asymptotic regime, independent of the underlying distribution. Numerical experiments show that this estimator performs very well, and that Tyler's iterative procedure converges quickly to the estimator. Franks and Moitra recently provided the first distribution-free error bounds in the finite sample setting, as well as the first rigorous convergence analysis of Tyler's iterative procedure. However, their results exceed the sample complexity of the Gaussian setting by a $\log^{2} d$ factor. We close this gap by proving optimal sample threshold and error bounds for Tyler's M-estimator for all Elliptical distributions, fully matching the Gaussian result. Moreover, we recover the algorithmic convergence even at this lower sample threshold. Our approach builds on the operator scaling connection of Franks and Moitra by introducing a novel pseudorandom condition, which we call $\infty$-expansion. We show that Elliptical distributions satisfy $\infty$-expansion at the optimal sample threshold, and then prove a novel scaling result for inputs satisfying this condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13751v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lap Chi Lau, Akshay Ramachandran</dc:creator>
    </item>
    <item>
      <title>The $\phi$-PCA Framework: A Unified and Efficiency-Preserving Approach with Robust Variants</title>
      <link>https://arxiv.org/abs/2510.13159</link>
      <description>arXiv:2510.13159v1 Announce Type: cross 
Abstract: Principal component analysis (PCA) is a fundamental tool in multivariate statistics, yet its sensitivity to outliers and limitations in distributed environments restrict its effectiveness in modern large-scale applications. To address these challenges, we introduce the $\phi$-PCA framework which provides a unified formulation of robust and distributed PCA. The class of $\phi$-PCA methods retains the asymptotic efficiency of standard PCA, while aggregating multiple local estimates using a proper $\phi$ function enhances ordering-robustness, leading to more accurate eigensubspace estimation under contamination. Notably, the harmonic mean PCA (HM-PCA), corresponding to the choice $\phi(u)=u^{-1}$, achieves optimal ordering-robustness and is recommended for practical use. Theoretical results further show that robustness increases with the number of partitions, a phenomenon seldom explored in the literature on robust or distributed PCA. Altogether, the partition-aggregation principle underlying $\phi$-PCA offers a general strategy for developing robust and efficiency-preserving methodologies applicable to both robust and distributed data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13159v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hung Hung, Zhi-Yu Jou, Su-Yun Huang, Shinto Eguchi</dc:creator>
    </item>
    <item>
      <title>$L_2$-Regularized Empirical Risk Minimization Guarantees Small Smooth Calibration Error</title>
      <link>https://arxiv.org/abs/2510.13450</link>
      <description>arXiv:2510.13450v1 Announce Type: cross 
Abstract: Calibration of predicted probabilities is critical for reliable machine learning, yet it is poorly understood how standard training procedures yield well-calibrated models. This work provides the first theoretical proof that canonical $L_{2}$-regularized empirical risk minimization directly controls the smooth calibration error (smCE) without post-hoc correction or specialized calibration-promoting regularizer. We establish finite-sample generalization bounds for smCE based on optimization error, regularization strength, and the Rademacher complexity. We then instantiate this theory for models in reproducing kernel Hilbert spaces, deriving concrete guarantees for kernel ridge and logistic regression. Our experiments confirm these specific guarantees, demonstrating that $L_{2}$-regularized ERM can provide a well-calibrated model without boosting or post-hoc recalibration. The source code to reproduce all experiments is available at https://github.com/msfuji0211/erm_calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13450v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masahiro Fujisawa, Futoshi Futami</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic goodness-of-fit tests and model selection in valued stochastic blockmodels</title>
      <link>https://arxiv.org/abs/2510.13636</link>
      <description>arXiv:2510.13636v1 Announce Type: cross 
Abstract: A valued stochastic blockmodel (SBM) is a general way to view networked data in which nodes are grouped into blocks and links between them are measured by counts or labels. This family allows for varying dyad sampling schemes, thereby including the classical, Poisson, and labeled SBMs, as well as those in which some edge observations are censored. This paper addresses the question of testing goodness-of-fit of such non-Bernoulli SBMs, focusing in particular on finite-sample tests. We derive explicit Markov bases moves necessary to generate samples from reference distributions and define goodness-of-fit statistics for determining model fit, comparable to those in the literature for related model families.
  For the labeled SBM, which includes in particular the censored-edge model, we study the asymptotic behavior of said statistics. One of the main purposes of testing goodness-of-fit of an SBM is to determine whether block membership of the nodes influences network formation. Power and Type 1 error rates are verified on simulated data. Additionally, we discuss the use of asymptotic results in selecting the number of blocks under the latent-block modeling assumption. The method derived for Poisson SBM is applied to ecological networks of host-parasite interactions. Our data analysis conclusions differ in selecting the number of blocks for the species from previous results in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13636v1</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>F\'elix Almendra-Hern\'andez, Miles Bakenhus, Vishesh Karwa, Mitsunori Ogawa, Sonja Petrovi\'c</dc:creator>
    </item>
    <item>
      <title>Pathwise guessing in categorical time series with unbounded alphabets</title>
      <link>https://arxiv.org/abs/2501.06547</link>
      <description>arXiv:2501.06547v3 Announce Type: replace 
Abstract: The following learning problem arises naturally in various applications: Given a finite sample from a categorical or count time series, can we learn a function of the sample that (nearly) maximizes the probability of correctly guessing the values of a given portion of the data using the values from the remaining parts? Unlike classical approaches in statistical inference, our approach avoids explicitly estimating the conditional probabilities.
  We propose a non-parametric guessing function with a learning rate independent of the alphabet size. Our analysis focuses on a broad class of time series models that encompasses finite-order Markov chains, some hidden Markov chains, Poisson regression for count processes, and one-dimensional Gibbs measures.
  We provide a margin condition that controls the rate of convergence for the risk. Additionally, we establish a minimax lower bound for the convergence rate of the risk associated with our guessing problem. This lower bound matches the upper bound achieved by our estimator up to a logarithmic factor, demonstrating its near-optimality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06547v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. -R. Chazottes, S. Gallo, D. Takahashi</dc:creator>
    </item>
    <item>
      <title>Higher-arity PAC learning, VC dimension and packing lemma</title>
      <link>https://arxiv.org/abs/2510.02420</link>
      <description>arXiv:2510.02420v2 Announce Type: replace-cross 
Abstract: The aim of this note is to overview some of our work in Chernikov, Towsner'20 (arXiv:2010.00726) developing higher arity VC theory (VC$_n$ dimension), including a generalization of Haussler packing lemma, and an associated tame (slice-wise) hypergraph regularity lemma; and to demonstrate that it characterizes higher arity PAC learning (PAC$_n$ learning) in $n$-fold product spaces with respect to product measures introduced by Kobayashi, Kuriyama and Takeuchi'15. We also point out how some of the recent results in arXiv:2402.14294, arXiv:2505.15688, arXiv:2509.20404 follow from our work in arXiv:2010.00726.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02420v2</guid>
      <category>stat.ML</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>math.LO</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Artem Chernikov, Henry Towsner</dc:creator>
    </item>
    <item>
      <title>A coupling-based approach to f-divergences diagnostics for Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.07559</link>
      <description>arXiv:2510.07559v2 Announce Type: replace-cross 
Abstract: A long-standing gap exists between the theoretical analysis of Markov chain Monte Carlo convergence, which is often based on statistical divergences, and the diagnostics used in practice. We introduce the first general convergence diagnostics for Markov chain Monte Carlo based on any f-divergence, allowing users to directly monitor, among others, the Kullback--Leibler and the $\chi^2$ divergences as well as the Hellinger and the total variation distances. Our first key contribution is a coupling-based `weight harmonization' scheme that produces a direct, computable, and consistent weighting of interacting Markov chains with respect to their target distribution. The second key contribution is to show how such consistent weightings of empirical measures can be used to provide upper bounds to f-divergences in general. We prove that these bounds are guaranteed to tighten over time and converge to zero as the chains approach stationarity, providing a concrete diagnostic. Numerical experiments demonstrate that our method is a practical and competitive diagnostic tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07559v2</guid>
      <category>stat.CO</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adrien Corenflos, Hai-Dang Dau</dc:creator>
    </item>
  </channel>
</rss>
