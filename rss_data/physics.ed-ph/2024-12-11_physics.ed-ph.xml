<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.ed-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.ed-ph</link>
    <description>physics.ed-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.ed-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 12 Dec 2024 02:46:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Using Large Language Models to Assign Partial Credits to Students' Problem-Solving Process: Grade at Human Level Accuracy with Grading Confidence Index and Personalized Student-facing Feedback</title>
      <link>https://arxiv.org/abs/2412.06910</link>
      <description>arXiv:2412.06910v1 Announce Type: new 
Abstract: This study examines the feasibility and potential advantages of using large language models, in particular GPT-4o, to perform partial credit grading of large numbers of students written response to introductory level physics problems. Students were instructed to write down verbal explanation of their reasoning process when solving three multi-step conceptual or numerical calculation problems. The responses were then graded according to a 3-item rubric with each item grades as binary yes/no. We first demonstrate that machine grading using GPT-4o with no examples nor reference answer can be made to reliably agree with human raters on 70%-80% of all cases, which is equal to or more than the level at which two human graders agree with each other. Two methods are essential for achieving this level of accuracy: 1. Adding explanation language to each rubric item that targets the weakness of machine grading. 2. Running the grading process five times and taking the most frequent outcome. Next, we show that the variance in outcomes between five machine grading attempts can serve as a grading confidence index. The index allows a human expert to identify ~40% of all potentially incorrect gradings by reviewing just 10 - 15% of all responses with the highest variance. Finally, we found that it is straightforward to use GPT-4o to write clear and detailed explanation of the partial credit grading, which allows students understand their grades and raise different opinions. Almost all feedback generated were rated 3 or above on a 5-point scale by two experienced instructors. The entire grading and feedback generating process costs roughly $5 per 100 student answers, which shows immense promise for automating labor-intensive grading process by a combination of machine grading with human input and supervision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06910v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Zhongzhou Chen, Tong Wan</dc:creator>
    </item>
    <item>
      <title>Leveraging AI for Rapid Generation of Physics Simulations in Education: Building Your Own Virtual Lab</title>
      <link>https://arxiv.org/abs/2412.07482</link>
      <description>arXiv:2412.07482v1 Announce Type: new 
Abstract: Seemingly we are not so far from Star Trek's food replicator. Generative artificial intelligence is rapidly becoming an integral part of both science and education, offering not only automation of processes but also the dynamic creation of complex, personalized content for educational purposes. With such advancement, educators are now crafting exams, building tutors, creating writing partners for students, and developing an array of other powerful tools for supporting our educational practices and student learning. We share a new class of opportunities for supporting learners and educators through the development of AI-generated simulations of physical phenomena and models. While we are not at the stage of "Computer: make me a mathematical simulation depicting the quantum wave functions of electrons in the hydrogen atom", we are not far off.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.07482v1</guid>
      <category>physics.ed-ph</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yossi Ben-Zion (Department of Physics, Bar-Ilan University, Ramat Gan, Israel), Roi Einhorn Zarzecki (Department of Physics, Bar-Ilan University, Ramat Gan, Israel), Joshua Glazer (Department of Physics, Bar-Ilan University, Ramat Gan, Israel), Noah D. Finkelstein (Department of Physics, University of Colorado Boulder, Boulder, Colorado, USA)</dc:creator>
    </item>
    <item>
      <title>Overview of couplet scoring in content-focused physics assessments</title>
      <link>https://arxiv.org/abs/2307.03099</link>
      <description>arXiv:2307.03099v3 Announce Type: replace 
Abstract: Content-focused research-based assessment instruments typically use items (i.e., questions) as the unit of assessment for scoring, reporting, and validation. Couplet scoring employs an alternative unit of assessment called a couplet, which is essentially an item viewed and scored through the lens of a specific assessment objective. With couplet scoring, a single item may have more than one assessment objective and therefore more than one couplet and thus more than one score. We outline the components of traditional item scoring, discuss couplet scoring and its benefits, and use both a recently developed content research-based assessment instrument and an existing one to ground our discussion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.03099v3</guid>
      <category>physics.ed-ph</category>
      <pubDate>Wed, 11 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Vignal, Gayle Geschwind, Marcos D. Caballero, H. J. Lewandowski</dc:creator>
    </item>
  </channel>
</rss>
