<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 01:36:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The Chonkers Algorithm: Content-Defined Chunking with Strict Guarantees on Size and Locality</title>
      <link>https://arxiv.org/abs/2509.11121</link>
      <description>arXiv:2509.11121v1 Announce Type: new 
Abstract: This paper presents the Chonkers algorithm, a novel content-defined chunking method providing simultaneous strict guarantees on chunk size and edit locality. Unlike existing algorithms such as Rabin fingerprinting and anchor-based methods, Chonkers achieves bounded propagation of edits and precise control over chunk sizes. I describe the algorithm's layered structure, theoretical guarantees, implementation considerations, and introduce the Yarn datatype, a deduplicated, merge-tree-based string representation benefiting from Chonkers' strict guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11121v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Benjamin Berger</dc:creator>
    </item>
    <item>
      <title>Triangle-Covered Graphs: Algorithms, Complexity, and Structure</title>
      <link>https://arxiv.org/abs/2509.11448</link>
      <description>arXiv:2509.11448v1 Announce Type: new 
Abstract: The widely studied edge modification problems ask how to minimally alter a graph to satisfy certain structural properties. In this paper, we introduce and study a new edge modification problem centered around transforming a given graph into a triangle-covered graph (one in which every vertex belongs to at least one triangle). We first present tight lower bounds on the number of edges in any connected triangle-covered graph of order $n$, and then we characterize all connected graphs that attain this minimum edge count. For a graph $G$, we define the notion of a $\Delta$-completion set as a set of non-edges of $G$ whose addition to $G$ results in a triangle-covered graph. We prove that the decision problem of finding a $\Delta$-completion set of size at most $t\geq0$ is $\mathbb{NP}$-complete and does not admit a constant-factor approximation algorithm under standard complexity assumptions. Moreover, we show that this problem remains $\mathbb{NP}$-complete even when the input is restricted to connected bipartite graphs. We then study the problem from an algorithmic perspective, providing tight bounds on the minimum $\Delta$-completion set size for several graph classes, including trees, chordal graphs, and cactus graphs. Furthermore, we show that the triangle-covered problem admits an $(\ln n +1)$-approximation algorithm for general graphs. For trees and chordal graphs, we design algorithms that compute minimum $\Delta$-completion sets. Finally, we show that the threshold for a random graph $\mathbb{G}(n, p)$ to be triangle-covered occurs at $n^{-2/3}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11448v1</guid>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Amirali Madani, Anil Maheshwari, Babak Miraftab, Pawe{\l} \.Zyli\'nski</dc:creator>
    </item>
    <item>
      <title>On the Smallest Size of Internal Collage Systems</title>
      <link>https://arxiv.org/abs/2509.11602</link>
      <description>arXiv:2509.11602v1 Announce Type: new 
Abstract: A Straight-Line Program (SLP) for a stirng $T$ is a context-free grammar in Chomsky normal form that derives $T$ only, which can be seen as a compressed form of $T$. Kida et al.\ introduced collage systems [Theor. Comput. Sci., 2003] to generalize SLPs by adding repetition rules and truncation rules. The smallest size $c(T)$ of collage systems for $T$ has gained attention to see how these generalized rules improve the compression ability of SLPs. Navarro et al. [IEEE Trans. Inf. Theory, 2021] showed that $c(T) \in O(z(T))$ and there is a string family with $c(T) \in \Omega(b(T) \log |T|)$, where $z(T)$ is the number of Lempel-Ziv parsing of $T$ and $b(T)$ is the smallest size of bidirectional schemes for $T$. They also introduced a subclass of collage systems, called internal collage systems, and proved that its smallest size $\hat{c}(T)$ for $T$ is at least $b(T)$. While $c(T) \le \hat{c}(T)$ is obvious, it is unknown how large $\hat{c}(T)$ is compared to $c(T)$. In this paper, we prove that $\hat{c}(T) = \Theta(c(T))$ by showing that any collage system of size $m$ can be transformed into an internal collage system of size $O(m)$ in $O(m^2)$ time. Thanks to this result, we can focus on internal collage systems to study the asymptotic behavior of $c(T)$, which helps to suppress excess use of truncation rules. As a direct application, we get $b(T) = O(c(T))$, which answers an open question posed in [Navarro et al., IEEE Trans. Inf. Theory, 2021]. We also give a MAX-SAT formulation to compute $\hat{c}(T)$ for a given $T$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11602v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soichiro Migita, Kyotaro Uehata, Tomohiro I</dc:creator>
    </item>
    <item>
      <title>An ETH-Tight FPT Algorithm for Rejection-Proof Set Packing with Applications to Kidney Exchange</title>
      <link>https://arxiv.org/abs/2509.11965</link>
      <description>arXiv:2509.11965v1 Announce Type: new 
Abstract: We study the parameterized complexity of a recently introduced multi-agent variant of the Kidney Exchange problem. Given a directed graph $G$ and integers $d$ and $k$, the standard problem asks whether $G$ contains a packing of vertex-disjoint cycles, each of length $\leq d$, covering at least $k$ vertices in total. In the multi-agent setting we consider, the vertex set is partitioned over several agents who reject a cycle packing as solution if it can be modified into an alternative packing that covers more of their own vertices. A cycle packing is called rejection-proof if no agent rejects it and the problem asks whether such a packing exists that covers at least $k$ vertices.
  We exploit the sunflower lemma on a set packing formulation of the problem to give a kernel for this $\Sigma_2^P$-complete problem that is polynomial in $k$ for all constant values of $d$. We also provide a $2^{\mathcal{O}(k \log k)} + n^{\mathcal{O}(1)}$ algorithm based on it and show that this FPT algorithm is asymptotically optimal under the ETH. Further, we generalize the problem by including an additional positive integer $c$ in the input that naturally captures how much agents can modify a given cycle packing to reject it. For every constant $c$, the resulting problem simplifies from being $\Sigma_2^P$-complete to NP-complete. With a single-exponential algorithm for the setting where $c = 1$, we show this to be strictly easier under the ETH than when $c = 2$. In turn, we show that any $c \geq 2$ yields a problem that is essentially as hard as the original problem with $c$ unbounded. This displays an interesting discrepancy between the classical and parameterized complexity of the problem and gives a good view of what makes it hard.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11965v1</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bart M. P. Jansen, Jeroen S. K. Lamme, Ruben F. A. Verhaegh</dc:creator>
    </item>
    <item>
      <title>A Tree Clock Data Structure for Causal Orderings in Concurrent Executions</title>
      <link>https://arxiv.org/abs/2201.06325</link>
      <description>arXiv:2201.06325v1 Announce Type: cross 
Abstract: Dynamic techniques are a scalable and effective way to analyze concurrent programs. Instead of analyzing all behaviors of a program, these techniques detect errors by focusing on a single program execution. Often a crucial step in these techniques is to define a causal ordering between events in the execution, which is then computed using vector clocks, a simple data structure that stores logical times of threads. The two basic operations of vector clocks, namely join and copy, require $\Theta(k)$ time, where $k$ is the number of threads. Thus they are a computational bottleneck when $k$ is large.
  In this work, we introduce tree clocks, a new data structure that replaces vector clocks for computing causal orderings in program executions. Joining and copying tree clocks takes time that is roughly proportional to the number of entries being modified, and hence the two operations do not suffer the a-priori $\Theta(k)$ cost per application. We show that when used to compute the classic happens-before (HB) partial order, tree clocks are optimal, in the sense that no other data structure can lead to smaller asymptotic running time. Moreover, we demonstrate that tree clocks can be used to compute other partial orders, such as schedulable-happens-before (SHB) and the standard Mazurkiewicz (MAZ) partial order, and thus are a versatile data structure. Our experiments show that just by replacing vector clocks with tree clocks, the computation becomes from $2.02 \times$ faster (MAZ) to $2.66 \times$ (SHB) and $2.97 \times$ (HB) on average per benchmark. These results illustrate that tree clocks have the potential to become a standard data structure with wide applications in concurrent analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.06325v1</guid>
      <category>cs.LO</category>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umang Mathur, Andreas Pavlogiannis, H\"unkar Can Tun\c{c}, Mahesh Viswanathan</dc:creator>
    </item>
    <item>
      <title>Efficient Decrease-and-Conquer Linearizability Monitoring</title>
      <link>https://arxiv.org/abs/2410.04581</link>
      <description>arXiv:2410.04581v5 Announce Type: cross 
Abstract: Linearizability has become the de facto correctness specification for implementations of concurrent data structures. While formally verifying such implementations remains challenging, linearizability monitoring has emerged as a promising first step to rule out early problems in the development of custom implementations, and serves as a key component in approaches that stress test such implementations. In this work, we investigate linearizability monitoring -- check if an execution history of an implementation is linearizable. While this problem is intractable in general, a systematic understanding of when it becomes tractable has remained elusive. We revisit this problem and first present a unified `decrease-and-conquer' algorithmic framework for linearizability monitoring. At its heart, this framework asks to identify special linearizability-preserving values in a given history -- values whose presence yields an equilinearizable sub-history when removed, and whose absence indicates non-linearizability. We prove that a polynomial time algorithm for the problem of identifying linearizability-preserving values, yields a polynomial time algorithm for linearizability monitoring, while conversely, intractability of this problem implies intractability of the monitoring problem. We demonstrate our framework's effectiveness by instantiating it for several popular data types -- sets, stacks, queues and priority queues -- deriving polynomial time algorithms for each, with the unambiguity restriction, where each insertion to the underlying data structure adds a distinct value. We optimize these algorithms to achieve the optimal log-linear time complexity by amortizing the cost of solving sub-problems through efficient data structures. Our implementation and evaluation on publicly available implementations show that our approach scales to large histories and outperforms existing tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.04581v5</guid>
      <category>cs.PL</category>
      <category>cs.DS</category>
      <category>cs.LO</category>
      <category>cs.SE</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lee Zheng Han, Umang Mathur</dc:creator>
    </item>
    <item>
      <title>Foundational theory for optimal decision tree problems. I. Algorithmic and geometric foundations</title>
      <link>https://arxiv.org/abs/2509.11226</link>
      <description>arXiv:2509.11226v1 Announce Type: cross 
Abstract: In the first paper (part I) of this series of two, we introduce four novel definitions of the ODT problems: three for size-constrained trees and one for depth-constrained trees. These definitions are stated unambiguously through executable recursive programs, satisfying all criteria we propose for a formal specification. In this sense, they resemble the "standard form" used in the study of general-purpose solvers.
  Grounded in algebraic programming theory-a relational formalism for deriving correct-by-construction algorithms from specifications-we can not only establish the existence or nonexistence of dynamic programming solutions but also derive them constructively whenever they exist. Consequently, the four generic problem definitions yield four novel optimal algorithms for ODT problems with arbitrary splitting rules that satisfy the axioms and objective functions of a given form. These algorithms encompass the known depth-constrained, axis-parallel ODT algorithm as the special case, while providing a unified, efficient, and elegant solution for the general ODT problem.
  In Part II, we present the first optimal hypersurface decision tree algorithm and provide comprehensive experiments against axis-parallel decision tree algorithms, including heuristic CART and state-of-the-art optimal methods. The results demonstrate the significant potential of decision trees with flexible splitting rules. Moreover, our framework is readily extendable to support algorithms for constructing even more flexible decision trees, including those with mixed splitting rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11226v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xi He</dc:creator>
    </item>
    <item>
      <title>Efficient Algorithms for Partitioning Circulant Graphs with Optimal Spectral Approximation</title>
      <link>https://arxiv.org/abs/2509.11382</link>
      <description>arXiv:2509.11382v1 Announce Type: cross 
Abstract: The Marcus-Spielman-Srivastava theorem (Annals of Mathematics, 2015) for the Kadison-Singer conjecture implies the following result in spectral graph theory: For any undirected graph $G = (V,E)$ with a maximum edge effective resistance at most $\alpha$, there exists a partition of its edge set $E$ into $E_1 \cup E_2$ such that the two edge-induced subgraphs of $G$ spectrally approximates $(1/2)G$ with a relative error $O(\sqrt{\alpha})$. However, the proof of this theorem is non-constructive. It remains an open question whether such a partition can be found in polynomial time, even for special classes of graphs.
  In this paper, we explore polynomial-time algorithms for partitioning circulant graphs via partitioning their generators. We develop an efficient algorithm that partitions a circulant graph whose generators form an arithmetic progression, with an error matching that in the Marcus-Spielman-Srivastava theorem and optimal, up to a constant. On the other hand, we prove that if the generators of a circulant graph are ``far" from an arithmetic progression, no partition of the generators can yield two circulant subgraphs with an error matching that in the Marcus-Spielman-Srivastava theorem.
  In addition, we extend our algorithm to Cayley graphs whose generators are from a product of multiple arithmetic progressions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11382v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Surya Teja Gavva, Peng Zhang</dc:creator>
    </item>
    <item>
      <title>The Horton-Strahler number of butterfly trees</title>
      <link>https://arxiv.org/abs/2509.11384</link>
      <description>arXiv:2509.11384v1 Announce Type: cross 
Abstract: The Horton-Strahler number (HS) is a measure of branching complexity of rooted trees, introduced in hydrology and later studied in parallel computing under the name register function. While its order of growth is well understood for classical random trees, fluctuation behavior has largely resisted analysis. In this work we investigate the HS in the setting of butterfly trees -- binary trees constructed from butterfly permutations, a rich class of separable permutations with origins in numerical linear algebra and parallel architectures. For the subclass of simple butterfly trees, we exploit their recursive gluing structure to model the HS as an additive functional of a finite-state Markov process. This framework yields sharp distributional results, including a law of large numbers and a Central Limit Theorem with explicit variance growth, providing what appears to be the first genuine Gaussian limit law for the HS in a nontrivial random tree model. Extending to biased constructions, we further establish functional limit theorems via analytic and probabilistic tools. For general butterfly trees, while exact analysis remains open, empirical sampling shows that the HS distribution is confined to a narrower support than in classical models, and appears to concentrate tightly near the upper bound $\lfloor \log_4 N\rfloor$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11384v1</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Peca-Medlin</dc:creator>
    </item>
    <item>
      <title>A Dichotomy Theorem for Multi-Pass Streaming CSPs</title>
      <link>https://arxiv.org/abs/2509.11399</link>
      <description>arXiv:2509.11399v1 Announce Type: cross 
Abstract: We show a dichotomy result for $p$-pass streaming algorithms for all CSPs and for up to polynomially many passes. More precisely, we prove that for any arity parameter $k$, finite alphabet $\Sigma$, collection $\mathcal{F}$ of $k$-ary predicates over $\Sigma$ and any $c\in (0,1)$, there exists $0&lt;s\leq c$ such that:
  1. For any $\varepsilon&gt;0$ there is a constant pass, $O_{\varepsilon}(\log n)$-space randomized streaming algorithm solving the promise problem $\text{MaxCSP}(\mathcal{F})[c,s-\varepsilon]$. That is, the algorithm accepts inputs with value at least $c$ with probability at least $2/3$, and rejects inputs with value at most $s-\varepsilon$ with probability at least $2/3$.
  2. For all $\varepsilon&gt;0$, any $p$-pass (even randomized) streaming algorithm that solves the promise problem $\text{MaxCSP}(\mathcal{F})[c,s+\varepsilon]$ must use $\Omega_{\varepsilon}(n^{1/3}/p)$ space.
  Our approximation algorithm is based on a certain linear-programming relaxation of the CSP and on a distributed algorithm that approximates its value. This part builds on the works [Yoshida, STOC 2011] and [Saxena, Singer, Sudan, Velusamy, SODA 2025]. For our hardness result we show how to translate an integrality gap of the linear program into a family of hard instances, which we then analyze via studying a related communication complexity problem. That analysis is based on discrete Fourier analysis and builds on a prior work of the authors and on the work [Chou, Golovnev, Sudan, Velingker, Velusamy, J.ACM 2024].</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11399v1</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yumou Fei, Dor Minzer, Shuo Wang</dc:creator>
    </item>
    <item>
      <title>Optimal Micro-Transit Zoning via Clique Generation and Integer Programming</title>
      <link>https://arxiv.org/abs/2509.11445</link>
      <description>arXiv:2509.11445v1 Announce Type: cross 
Abstract: Micro-transit services offer a promising solution to enhance urban mobility and access, particularly by complementing existing public transit. However, effectively designing these services requires determining optimal service zones for these on-demand shuttles, a complex challenge often constrained by operating budgets and transit agency priorities. This paper presents a novel two-phase algorithmic framework for designing optimal micro-transit service zones based on the objective of maximizing served demand. A key innovation is our adaptation of the shareability graph concept from its traditional use in dynamic trip assignment to the distinct challenge of static spatial zoning. We redefine shareability by considering geographical proximity within a specified diameter constraint, rather than trip characteristics. In Phase 1, the framework employs a highly scalable algorithm to generate a comprehensive set of candidate zones. In Phase 2, it formulates the selection of a specified number of zones as a Weighted Maximum Coverage Problem, which can be efficiently solved by an integer programming solver. Evaluations on real-world data from Chattanooga, TN, and synthetic datasets show that our framework outperforms a baseline algorithm, serving 27.03% more demand in practice and up to 49.5% more demand in synthetic settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11445v1</guid>
      <category>math.OC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hins Hu, Rhea Goswami, Hongyi Jiang, Samitha Samaranayake</dc:creator>
    </item>
    <item>
      <title>Fast Percolation Centrality Approximation with Importance Sampling</title>
      <link>https://arxiv.org/abs/2509.11454</link>
      <description>arXiv:2509.11454v1 Announce Type: cross 
Abstract: In this work we present PercIS, an algorithm based on Importance Sampling to approximate the percolation centrality of all the nodes of a graph. Percolation centrality is a generalization of betweenness centrality to attributed graphs, and is a useful measure to quantify the importance of the vertices in a contagious process or to diffuse information. However, it is impractical to compute it exactly on modern-sized networks.
  First, we highlight key limitations of state-of-the-art sampling-based approximation methods for the percolation centrality, showing that in most cases they cannot achieve accurate solutions efficiently. Then, we propose and analyze a novel sampling algorithm based on Importance Sampling, proving tight sample size bounds to achieve high-quality approximations.
  Our extensive experimental evaluation shows that PercIS computes high-quality estimates and scales to large real-world networks, while significantly outperforming, in terms of sample sizes, accuracy and running times, the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11454v1</guid>
      <category>cs.SI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antonio Cruciani, Leonardo Pellegrina</dc:creator>
    </item>
    <item>
      <title>Liar's vertex-edge domination in unit disk graph</title>
      <link>https://arxiv.org/abs/2509.11775</link>
      <description>arXiv:2509.11775v1 Announce Type: cross 
Abstract: Let $G=(V, E)$ be a simple undirected graph. A closed neighbourhood of an edge $e=uv$ between two vertices $u$ and $v$ of $G$, denoted by $N_G[e]$, is the set of vertices in the neighbourhood of $u$ and $v$ including $\{u,v\}$. A subset $L$ of $V$ is said to be liar's vertex-edge dominating set if $(i)$ for every edge $e\in E$, $|N_G[e]\cap L|\geq 2$ and $(ii)$ for every pair of distinct edges $e,e'$, $|(N_G[e]\cup N_G[e'])\cap L|\geq 3$. The minimum liar's vertex-edge domination problem is to find the liar's vertex-edge dominating set of minimum cardinality. In this article, we show that the liar's vertex-edge domination problem is NP-complete in unit disk graphs, and we design a polynomial time approximation scheme(PTAS) for the minimum liar's vertex-edge domination problem in unit disk graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11775v1</guid>
      <category>math.CO</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Debojyoti Bhattacharya, Subhabrata Paul</dc:creator>
    </item>
    <item>
      <title>Foundational theory for optimal decision tree problems. II. Optimal hypersurface decision tree algorithm</title>
      <link>https://arxiv.org/abs/2509.12057</link>
      <description>arXiv:2509.12057v1 Announce Type: cross 
Abstract: Decision trees are a ubiquitous model for classification and regression tasks due to their interpretability and efficiency. However, solving the optimal decision tree (ODT) problem remains a challenging combinatorial optimization task. Even for the simplest splitting rules--axis-parallel hyperplanes--it is NP-hard to optimize. In Part I of this series, we rigorously defined the proper decision tree model through four axioms and, based on these, introduced four formal definitions of the ODT problem. From these definitions, we derived four generic algorithms capable of solving ODT problems for arbitrary decision trees satisfying the axioms. We also analyzed the combinatorial geometric properties of hypersurfaces, showing that decision trees defined by polynomial hypersurface splitting rules satisfy the proper axioms that we proposed.
  In this second paper (Part II) of this two-part series, building on the algorithmic and geometric foundations established in Part I, we introduce the first hypersurface decision tree (HODT) algorithm. To the best of our knowledge, existing optimal decision tree methods are, to date, limited to hyperplane splitting rules--a special case of hypersurfaces--and rely on general-purpose solvers. In contrast, our HODT algorithm addresses the general hypersurface decision tree model without requiring external solvers.
  Using synthetic datasets generated from ground-truth hyperplane decision trees, we vary tree size, data size, dimensionality, and label and feature noise. Results showing that our algorithm recovers the ground truth more accurately than axis-parallel trees and exhibits greater robustness to noise. We also analyzed generalization performance across 30 real-world datasets, showing that HODT can achieve up to 30% higher accuracy than the state-of-the-art optimal axis-parallel decision tree algorithm when tree complexity is properly controlled.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12057v1</guid>
      <category>cs.LG</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xi He</dc:creator>
    </item>
    <item>
      <title>SAQ: Pushing the Limits of Vector Quantization through Code Adjustment and Dimension Segmentation</title>
      <link>https://arxiv.org/abs/2509.12086</link>
      <description>arXiv:2509.12086v1 Announce Type: cross 
Abstract: Approximate Nearest Neighbor Search (ANNS) plays a critical role in applications such as search engines, recommender systems, and RAG for LLMs. Vector quantization (VQ), a crucial technique for ANNS, is commonly used to reduce space overhead and accelerate distance computations. However, despite significant research advances, state-of-the-art VQ methods still face challenges in balancing encoding efficiency and quantization accuracy. To address these limitations, we propose a novel VQ method called SAQ. To improve accuracy, SAQ employs a new dimension segmentation technique to strategically partition PCA-projected vectors into segments along their dimensions. By prioritizing leading dimension segments with larger magnitudes, SAQ allocates more bits to high-impact segments, optimizing the use of the available space quota. An efficient dynamic programming algorithm is developed to optimize dimension segmentation and bit allocation, ensuring minimal quantization error. To speed up vector encoding, SAQ devises a code adjustment technique to first quantize each dimension independently and then progressively refine quantized vectors using a coordinate-descent-like approach to avoid exhaustive enumeration. Extensive experiments demonstrate SAQ's superiority over classical methods (e.g., PQ, PCA) and recent state-of-the-art approaches (e.g., LVQ, Extended RabitQ). SAQ achieves up to 80% reduction in quantization error and accelerates encoding speed by over 80x compared to Extended RabitQ.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12086v1</guid>
      <category>cs.DB</category>
      <category>cs.DS</category>
      <category>cs.IR</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hui Li, Shiyuan Deng, Xiao Yan, Xiangyu Zhi, James Cheng</dc:creator>
    </item>
    <item>
      <title>Decision-Theoretic Approaches for Improved Learning-Augmented Algorithms</title>
      <link>https://arxiv.org/abs/2501.17701</link>
      <description>arXiv:2501.17701v2 Announce Type: replace 
Abstract: We initiate the systematic study of decision-theoretic metrics in the design and analysis of algorithms with machine-learned predictions. We introduce approaches based on both deterministic measures such as distance-based evaluation, that help us quantify how close the algorithm is to an ideal solution, and stochastic measures that balance the trade-off between the algorithm's performance and the risk associated with the imperfect oracle. These approaches allow us to quantify the algorithm's performance across the full spectrum of the prediction error, and thus choose the best algorithm within an entire class of otherwise incomparable ones. We apply our framework to three well-known problems from online decision making, namely ski-rental, one-max search, and contract scheduling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17701v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Spyros Angelopoulos, Christoph D\"urr, Georgii Melidi</dc:creator>
    </item>
    <item>
      <title>A Dynamic, Self-balancing k-d Tree</title>
      <link>https://arxiv.org/abs/2509.08148</link>
      <description>arXiv:2509.08148v4 Announce Type: replace 
Abstract: The original description of the k-d tree recognized that rebalancing techniques, used for building an AVL or red-black tree, are not applicable to a k-d tree, because these techniques involve cyclic exchange of tree nodes that violates the invariant of the k-d tree. For this reason, a static, balanced k-d tree is often built from all of the k-dimensional data en masse. However, it is possible to build a dynamic k-d tree that self-balances when necessary after insertion or deletion of each k-dimensional datum. This article describes insertion, deletion, and rebalancing algorithms for a dynamic, self-balancing k-d tree, and measures their performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08148v4</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Russell A. Brown</dc:creator>
    </item>
    <item>
      <title>On the satisfability of random k-Horn formulae</title>
      <link>https://arxiv.org/abs/cs/0007029</link>
      <description>arXiv:cs/0007029v2 Announce Type: replace 
Abstract: We determine the asymptotical satisfiability probability of a random at-most-k-Horn formula, via a probabilistic analysis of a simple version, called PUR, of positive unit resolution. We show that for $k=k(n)\rightarrow \infty$ the problem can be ``reduced'' to the case k(n)=n, that was solved in cs.DS/9912001. On the other hand, in the case k= a constant the behavior of PUR is modeled by a simple queuing chain, leading to a closed-form solution when $k=2$. Our analysis predicts an ``easy-hard-easy'' pattern in this latter case. Under a rescaled parameter, the graphs of satisfaction probability corresponding to finite values of k converge to the one for the uniform case, a ``dimension-dependent behavior'' similar to the one found experimentally by Kirkpatrick and Selman (Science'94) for k-SAT. The phenomenon is qualitatively explained by a threshold property for the number of iterations of PUR makes on random satisfiable Horn formulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:cs/0007029v2</guid>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Graphs, Morphisms, and Statistical Physics: Proceedings of the DIMACS Workshop on Graphs, Morphisms and Statistical Physics, March 19-21, 2001</arxiv:journal_reference>
      <dc:creator>Gabriel Istrate</dc:creator>
    </item>
    <item>
      <title>Testing classical properties from quantum data</title>
      <link>https://arxiv.org/abs/2411.12730</link>
      <description>arXiv:2411.12730v3 Announce Type: replace-cross 
Abstract: Properties of Boolean functions can often be tested much faster than the functions can be learned. However, this advantage usually disappears when testers are limited to random samples of a function $f$--a natural setting for data science--rather than queries. In this work we initiate the study of a quantum version of this "data science scenario": quantum algorithms that test properties of $f$ solely from quantum data in the form of copies of the function state $|f\rangle \propto \sum_x|x,f(x)\rangle$.
  $\bullet$ New tests. For three well-established properties--monotonicity, symmetry, and triangle-freeness--we show that the speedup lost when restricting classical testers to sampled data can be recovered by quantum algorithms operating solely from quantum data.
  $\bullet$ Inadequacy of Fourier sampling. Our new testers use techniques beyond quantum Fourier sampling, and we show that this necessary. In particular, there is no constant-complexity tester for symmetry relying solely on Fourier sampling and random classical samples.
  $\bullet$ Classical queries vs. quantum data. We exhibit a testing problem that can be solved from $O(1)$ classical queries but that requires $\Omega(2^{n/2})$ function state copies. The Forrelation problem provides a separation of the same magnitude in the opposite direction, so we conclude that quantum data and classical queries are "maximally incomparable" resources for testing.
  $\bullet$ Towards lower bounds. We also begin the study of lower bounds for testing from quantum data. For quantum monotonicity testing, we prove that the ensembles of Goldreich et al. (2000) and Black (2023), which give exponential lower bounds for classical sample-based testing, do not yield any nontrivial lower bounds for testing from quantum data. New insights specific to quantum data will be required for proving copy complexity lower bounds for testing in this model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12730v3</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthias C. Caro, Preksha Naik, Joseph Slote</dc:creator>
    </item>
    <item>
      <title>The Planted Orthogonal Vectors Problem</title>
      <link>https://arxiv.org/abs/2505.00206</link>
      <description>arXiv:2505.00206v2 Announce Type: replace-cross 
Abstract: In the $k$-Orthogonal Vectors ($k$-OV) problem we are given $k$ sets, each containing $n$ binary vectors of dimension $d=n^{o(1)}$, and our goal is to pick one vector from each set so that at each coordinate at least one vector has a zero. It is a central problem in fine-grained complexity, conjectured to require $n^{k-o(1)}$ time in the worst case.
  We propose a way to \emph{plant} a solution among vectors with i.i.d. $p$-biased entries, for appropriately chosen $p$, so that the planted solution is the unique one. Our conjecture is that the resulting $k$-OV instances still require time $n^{k-o(1)}$ to solve, \emph{on average}.
  Our planted distribution has the property that any subset of strictly less than $k$ vectors has the \emph{same} marginal distribution as in the model distribution, consisting of i.i.d. $p$-biased random vectors. We use this property to give average-case search-to-decision reductions for $k$-OV.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00206v2</guid>
      <category>cs.CC</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David K\"uhnemann, Adam Polak, Alon Rosen</dc:creator>
    </item>
    <item>
      <title>Computing the probability of intersection</title>
      <link>https://arxiv.org/abs/2507.10329</link>
      <description>arXiv:2507.10329v3 Announce Type: replace-cross 
Abstract: Let $\Omega_1, \ldots, \Omega_m$ be probability spaces, let $\Omega=\Omega_1 \times \cdots \times \Omega_m$ be their product and let $A_1, \ldots, A_n \subset \Omega$ be events. Suppose that each event $A_i$ depends on $r_i$ coordinates of a point $x \in \Omega$, $x=\left(\xi_1, \ldots, \xi_m\right)$, and that for each event $A_i$ there are $\Delta_i$ of other events $A_j$ that depend on some of the coordinates that $A_i$ depends on. Let $\Delta=\max\{5,\ \Delta_i: i=1, \ldots, n\}$ and let $\mu_i=\min\{r_i,\ \Delta_i+1\}$ for $i=1, \ldots, n$. We prove that if $P(A_i) &lt; (3\Delta)^{-3\mu_i}$ for all $i$, then for any $0 &lt; \epsilon &lt; 1$, the probability $P\left( \bigcap_{i=1}^n \overline{A}_i\right)$ of the intersection of the complements of all $A_i$ can be computed within relative error $\epsilon$ in polynomial time from the probabilities $P\left(A_{i_1} \cap \ldots \cap A_{i_k}\right)$ of $k$-wise intersections of the events $A_i$ for $k = e^{O(\Delta)} \ln (n/\epsilon)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.10329v3</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>math.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Barvinok</dc:creator>
    </item>
  </channel>
</rss>
