<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>astro-ph.IM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/astro-ph.IM</link>
    <description>astro-ph.IM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/astro-ph.IM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Secular aberration drift in stellar proper motions: An additional term due to the change in line-of-sight direction</title>
      <link>https://arxiv.org/abs/2407.19182</link>
      <description>arXiv:2407.19182v1 Announce Type: new 
Abstract: The motion of the barycenter of the Solar System (SSB), the origin of the International Celestial Reference System, causes a directional displacement known as secular aberration. The secular aberration drift caused by the galactocentric acceleration of the SSB has been modeled in the third generation of the International Celestial Reference Frame. We aim to address another secular aberration drift effect due to the change in the line-of-sight direction and study its implications for stellar proper motions. A complete formula of secular aberration drift is derived, and its influence on stellar proper motion is computed based on the astrometric data in \textit{Gaia} Data Release 3. We found that the secular aberration drift due to the change in the line-of-sight direction tends to decrease the observed proper motions for stars with galactic longitudes between $0^{\circ}$ and $180^{\circ}$, and increase the observed proper motion for stars in the remaining region. If this secular aberration drift effect is ignored, it will induce an additional proper motion of $&gt;1\,\mathrm{mas\,yr^{-1}}$ for 84 stars and $&gt;0.02\,\mathrm{mas\,yr^{-1}}$ for 5\,944\,879 stars, which is comparable to or several times greater than the typical formal uncertainty of the \textit{Gaia} proper motion measurements at $G&lt;13$. The secular aberration drift due to the change in the line-of-sight direction and the acceleration of the SSB should be modeled to make the stellar reference frame consistent with the extragalactic reference frame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19182v1</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.SR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Niu Liu, Zi Zhu, Jia-Cheng Liu</dc:creator>
    </item>
    <item>
      <title>AO3k at Subaru: First on-sky results of the facility extreme-AO</title>
      <link>https://arxiv.org/abs/2407.19188</link>
      <description>arXiv:2407.19188v1 Announce Type: new 
Abstract: The facility adaptive optics of the Subaru Telescope AO188 recently received some long-awaited upgrades: a new 3224-actuator deformable mirror (DM) from ALPAO (hence the name change to AO3000 or AO3k), an upgraded GPU-based real-time computer, a visible nonlinear curvature wavefront sensor and a near-infrared wavefront sensor (NIR WFS), closing the loop at up to 2~kHz. The wavefront sensors were added in 2023, while the DM will be installed at the beginning of 2024. With these new features, AO3k will provide extreme-AO level of correction to all the instruments on the IR Nasmyth platform: The NIR-MIR camera and spectrograph IRCS, the high-resolution Doppler spectrograph IRD, and the high-contrast instrument SCExAO. AO3k will also support laser tomography (LTAO), delivering high Strehl ratio imaging with large sky coverage.
  The high Strehl will especially benefit SCExAO for high-contrast imaging, both in infrared and visible. The second stage extreme AO will no longer have to chase large residual atmospheric turbulence, and will focus on truly high-contrast techniques to create and stabilize dark holes, as well as coherent differential imaging techniques. We will finally be able to leverage the several high performance coronagraphs tested in SCExAO, even in the visible.
  AO3k will answer crucial questions as a precursor for future adaptive optics systems for ELTs, especially as a technology demonstrator for the HCI Planetary Systems Imager on the Thirty Meter Telescope. A lot of questions are still unanswered on the on-sky behavior of high actuator counts DMs, NIR wavefront sensing, the effect of rolling shutters or persistence.
  We present here the first on-sky results of AO3k, before the system gets fully offered to the observers in the second half of 2024. These results give us some insight on the great scientific results we hope to achieve in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19188v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Julien Lozi, Kyohoon Ahn, Hannah Blue, Alicia Chun, Christophe Clergeon, Vincent Deo, Olivier Guyon, Takashi Hattori, Yosuke Minowa, Shogo Nishiyama, Yoshito Ono, Shin Oya, Yuhei Takagi, Sebastien Vievard, Maria Vincent</dc:creator>
    </item>
    <item>
      <title>Stellar Blend Image Classification Using Computationally Efficient Gaussian Processes</title>
      <link>https://arxiv.org/abs/2407.19297</link>
      <description>arXiv:2407.19297v1 Announce Type: new 
Abstract: Stellar blends, where two or more stars appear blended in an image, pose a significant visualization challenge in astronomy. Traditionally, distinguishing these blends from single stars has been costly and resource-intensive, involving sophisticated equipment and extensive expert analysis. This is especially problematic for analyzing the vast data volumes from surveys, such as Legacy Survey of Space and Time (LSST), Sloan Digital Sky Survey (SDSS), Dark Energy Spectroscopic Instrument (DESI), Legacy Imaging Survey and the Zwicky Transient Facility (ZTF). To address these challenges, we apply different normalizations and data embeddings on low resolution images of single stars and stellar blends, which are passed as inputs into machine learning methods and to a computationally efficient Gaussian process model (MuyGPs). MuyGPs consistently outperforms the benchmarked models, particularly on limited training data. Moreover, MuyGPs with $r^\text{th}$ root local min-max normalization achieves 83.8% accuracy. Furthermore, MuyGPs' ability to produce confidence bands ensures that predictions with low confidence can be redirected to a specialist for efficient human-assisted labeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19297v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chinedu Eleh, Yunli Zhang, Rafael Bidese, Benjamin W. Priest, Amanda L. Muyskens, Roberto Molinari, Nedret Billor</dc:creator>
    </item>
    <item>
      <title>Automated Detection of Satellite Trails in Ground-Based Observations Using U-Net and Hough Transform</title>
      <link>https://arxiv.org/abs/2407.19461</link>
      <description>arXiv:2407.19461v1 Announce Type: new 
Abstract: The expansion of satellite constellations poses a significant challenge to optical ground-based astronomical observations, as satellite trails degrade observational data and compromise research quality. Addressing these challenges requires developing robust detection methods to enhance data processing pipelines, creating a reliable approach for detecting and analyzing satellite trails that can be easily reproduced and applied by other observatories and data processing groups. Our method, called ASTA (Automated Satellite Tracking for Astronomy), combines deep learning and computer vision techniques for effective satellite trail detection. It employs a U-Net based deep learning network to initially detect trails, followed by a Probabilistic Hough Transform to refine the output. ASTA's U-Net model was trained on a dataset with manually labelled full-field MeerLICHT images prepared using the LABKIT annotation tool, ensuring high-quality and precise annotations. This annotation process was crucial for the model to learn and generalize the characteristics of satellite trails effectively. Furthermore, the user-friendly LABKIT tool facilitated quick and efficient data refinements, streamlining the overall model development process. ASTA's performance was evaluated on a test set of 20,000 image patches, both with and without satellite trails, to rigorously assess its precision and recall. Additionally, ASTA was applied to approximately 200,000 full-field MeerLICHT images, demonstrating its effectiveness in identifying and characterizing satellite trails. The software's results were validated by cross-referencing detected trails with known public satellite catalogs, confirming its reliability and showcasing its ability to uncover previously untracked objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19461v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>F. Stoppa, P. J. Groot, R. Stuik, P. Vreeswijk, S. Bloemen, D. L. A. Pieterse, P. A. Woudt</dc:creator>
    </item>
    <item>
      <title>Characterisation of SATCOM Networks for Rapid Message Delivery: Early In-Orbit Results</title>
      <link>https://arxiv.org/abs/2407.19623</link>
      <description>arXiv:2407.19623v1 Announce Type: new 
Abstract: Traditional nanosatellite communication links rely on infrequent ground-station access windows. While this is well suited to both payload data and detailed scheduling information, the resulting long periods without contact are ill-suited for both opportunistic tasking of satellites and triggers generated by autonomous operations. Existing orbital infrastructure in the form of satellite communication (SATCOM) networks, such as Iridium and others provide a readily available and cost effective solution to this problem. While these networks continue to be utilized onboard nanosatellites, a full characterization of their utility and performance in-orbit is vital to understand the reliability and potential for high-timeliness message delivery. The SpIRIT 6U nanosatellite is a mission led by The University of Melbourne in cooperation with the Italian Space Agency and supported by the Australian Space Agency. Developed over the last four years and launched in a 510km Polar Sun Synchronous Orbit in late 2023, SpIRIT carries multiple subsystems for scientific and technology demonstration. The Mercury subsystem provides a demonstration and characterization test bed for SATCOM utilization in-orbit, while also providing the capability of rapid down-link of detection events generated by the main scientific payload of the mission, the HERMES instrument for the detection of high-energy astrophysical transients. This paper first presents a brief payload characterization experiment overview. Early in-orbit results are then presented. This work not only sheds light on the utility of these networks for autonomous operations, and on their potential impact to enable greater utilization of nanosatellites for scientific missions, but also offers insights into the practical challenges related to the design and implementation of utilizing these networks in-orbit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19623v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Robert Mearns, Airlie Chapman, Michele Trenti</dc:creator>
    </item>
    <item>
      <title>Additive manufacturing applications in astronomy: a review</title>
      <link>https://arxiv.org/abs/2407.19839</link>
      <description>arXiv:2407.19839v1 Announce Type: new 
Abstract: Despite the established role of additive manufacturing (AM) in aerospace and medical fields, its adoption in astronomy remains low. Encouraging AM integration in a risk-averse community necessitates documentation and dissemination of previous case studies. The objective of this study is to create the first review of AM in astronomy hardware, answering: where is AM currently being used in astronomy, what is the status of its adoption, and what challenges are preventing its widespread use? The review starts with an introduction to astronomical instruments size/cost challenges, alongside the role of manufacturing innovation. This is followed by highlighting the benefits/challenges of AM and used materials/processes in both space-based and ground-based applications. The review case studies include mirrors, optomechanical structures, compliant mechanisms, brackets and tooling applications that are either in research phase or are implemented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19839v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younes Chahid, Carolyn Atkins, Greg Lister, Rhys Tuck, Stephen Watson, Katherine Morris, David Isherwood, Jonathan Strachan, Joel Harman, Pearachad Chartsiriwattana, Deno Stelter, Werner Laun</dc:creator>
    </item>
    <item>
      <title>Reducing instrumental errors in Parkes Pulsar Timing Array data</title>
      <link>https://arxiv.org/abs/2407.20015</link>
      <description>arXiv:2407.20015v1 Announce Type: new 
Abstract: This paper demonstrates the impact of state-of-the-art instrumental calibration techniques on the precision of arrival times obtained from 9.6 years of observations of millisecond pulsars using the Murriyang 64-m CSIRO Parkes Radio Telescope. Our study focuses on 21-cm observations of 25 high-priority pulsars that are regularly observed as part of the Parkes Pulsar Timing Array (PPTA) project, including those predicted to be the most susceptible to calibration errors. We employ Measurement Equation Template Matching (METM) for instrumental calibration and Matrix Template Matching (MTM) for arrival time estimation, resulting in significantly improved timing residuals with up to a sixfold reduction in white noise compared to arrival times estimated using Scalar Template Matching and conventional calibration based on the Ideal Feed Assumption. The median relative reduction in white noise is 33 percent, and the maximum absolute reduction is 4.5 microseconds. For PSR J0437-4715, METM and MTM reduce the best-fit power-law amplitude (2.7 sigma) and spectral index (1.7 sigma) of the red noise in the arrival time residuals, which can can be tentatively interpreted as mitigation of 1/f noise due to otherwise unmodeled steps in polarimetric response. These findings demonstrate the potential to directly enhance the sensitivity of pulsar timing array experiments through more accurate methods of instrumental calibration and arrival time estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20015v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Axl F. Rogers, Willem van Straten, Sergei Gulyaev, Aditya Parthasarathy, George Hobbs, Zu-Cheng Chen, Yi Feng, Boris Goncharov, Agastya Kapur, Xiaojin Liu, Daniel Reardon, Christopher J. Russell, Andrew Zic</dc:creator>
    </item>
    <item>
      <title>Prospects of using closure traces directly for imaging in Very Long Baseline Interferometry</title>
      <link>https://arxiv.org/abs/2407.20190</link>
      <description>arXiv:2407.20190v1 Announce Type: new 
Abstract: The reconstruction of the polarization of a source in radio interferometry is a challenging calibration problem since the reconstruction strongly depends on the gains and leakages that need to be inferred along with the image. This is particularly true for the Event Horizon Telescope (EHT) due to its small number of antennas, small signal-to-noise ratio and large gain corruptions. To recover linear polarization, one either has to infer the leakages and gains together with the image structure, or rely completely on calibration independent closure quantities. While the first approach has been explored in Very Long Baseline Interferometry (VLBI) for a long time, the later one has been less studied for polarimetry. Closure traces are a recently proposed concept of closure quantities that, in contrast to closure phases and closure amplitudes, are independent against both gains and leakages and carry the relevant information about the polarization of the source. Here we explore, how closure traces could be directly fitted to create an image and point out an imaging pipeline that succeeds in the direct imaging from closure traces. Since closure traces have a number of inherent degeneracies, multiple local image modes that can fit the data are detected. Therefore, a multiobjective imaging technique is needed to correctly sample this multimodality. Closure traces are not constraining enough for the EHT configuration in 2017 to recover an image directly, mainly due to the small number of antennas. For planned successors of the EHT however (with a significantly larger number of antennas), this option becomes feasible and performs competitive to the imaging with residual leakages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20190v1</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hendrik M\"uller</dc:creator>
    </item>
    <item>
      <title>The ANTARESS workflow I. Optimal extraction of spatially resolved stellar spectra with high-resolution transit spectroscopy</title>
      <link>https://arxiv.org/abs/2407.19012</link>
      <description>arXiv:2407.19012v1 Announce Type: cross 
Abstract: High-resolution spectrographs open a detailed window onto the atmospheres of stars and planets. As the number of systems observed with different instruments grows, it is crucial to develop a standard in analyzing spectral time series of exoplanet transits and occultations, for the benefit of reproducibility. Here, we introduce the ANTARESS workflow, a set of methods aimed at processing high-resolution spectroscopy datasets in a robust way and extracting accurate exoplanetary and stellar spectra. While a fast preliminary analysis can be run on order-merged 1D spectra and cross-correlation functions (CCFs), the workflow was optimally designed for extracted 2D echelle spectra to remain close to the original detector counts, limit the spectral resampling, and propagate the correlated noise. Input data from multiple instruments and epochs were corrected for relevant environmental and instrumental effects, processed homogeneously, and analyzed independently or jointly. In this first paper, we show how planet-occulted stellar spectra extracted along the transit chord and cleaned from planetary contamination provide a direct comparison with theoretical stellar models and enable a spectral and spatial mapping of the photosphere. We illustrate this application of the workflow to archival ESPRESSO data, using the Rossiter-McLaughlin effect Revolutions (RMR) technique to confirm the spin-orbit alignment of HD\,209458b and unveil biases in WASP-76b's published orbital architecture. Because the workflow is modular and its concepts are general, it can support new methods and be extended to additional spectrographs to find a range of applications beyond the proposed scope. In a companion paper, we will present how planet-occulted spectra can be processed further to extract and analyze planetary spectra decontaminated from the star, providing clean and direct measurements of atmospheric properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19012v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>astro-ph.SR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>V. Bourrier, J. -B. Delisle, C. Lovis, H. M. Cegla, M. Cretignier, R. Allart, K. Al Moulla, S. Tavella, O. Attia, D. Mounzer, V. Vaulato, M. Steiner, T. Vrignaud, S. Mercier, X. Dumusque, D. Ehrenreich, J. V. Seidel, A. Wyttenbach, W. Dethier, F. Pepe</dc:creator>
    </item>
    <item>
      <title>Rapid Likelihood Free Inference of Compact Binary Coalescences using Accelerated Hardware</title>
      <link>https://arxiv.org/abs/2407.19048</link>
      <description>arXiv:2407.19048v1 Announce Type: cross 
Abstract: We report a gravitational-wave parameter estimation algorithm, AMPLFI, based on likelihood-free inference using normalizing flows. The focus of AMPLFI is to perform real-time parameter estimation for candidates detected by machine-learning based compact binary coalescence search, Aframe. We present details of our algorithm and optimizations done related to data-loading and pre-processing on accelerated hardware. We train our model using binary black-hole (BBH) simulations on real LIGO-Virgo detector noise. Our model has $\sim 6$ million trainable parameters with training times $\lesssim 24$ hours. Based on online deployment on a mock data stream of LIGO-Virgo data, Aframe + AMPLFI is able to pick up BBH candidates and infer parameters for real-time alerts from data acquisition with a net latency of $\sim 6$s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19048v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deep Chatterjee, Ethan Marx, William Benoit, Ravi Kumar, Malina Desai, Ekaterina Govorkova, Alec Gunny, Eric Moreno, Rafia Omer, Ryan Raikman, Muhammed Saleem, Shrey Aggarwal, Michael W. Coughlin, Philip Harris, Erik Katsavounidis</dc:creator>
    </item>
    <item>
      <title>A camera system for real-time optical calibration of water-based neutrino telescopes</title>
      <link>https://arxiv.org/abs/2407.19111</link>
      <description>arXiv:2407.19111v1 Announce Type: cross 
Abstract: Calibrating the optical properties within the detection medium of a neutrino telescope is crucial for determining its angular resolution and energy scale. For the next generation of neutrino telescopes planned to be constructed in deep water, such as the TRopIcal DEep-sea Neutrino Telescope (TRIDENT), there are additional challenges due to the dynamic nature and potential non-uniformity of the water medium. This necessitates a real-time optical calibration system distributed throughout the large detector array. This study introduces a custom-designed CMOS camera system equipped with rapid image processing algorithms, providing a real-time optical calibration method for TRIDENT and other similar projects worldwide. In September 2021, the TRIDENT Pathfinder experiment (TRIDENT Explorer, T-REX for short) successfully deployed this camera system in the West Pacific Ocean at a depth of 3420 meters. Within 30 minutes, about 3000 images of the T-REX light source were captured, allowing for the in-situ measurement of seawater attenuation and absorption lengths under three wavelengths. This deep-sea experiment for the first time showcased a technical demonstration of a functioning camera calibration system in a dynamic neutrino telescope site, solidifying a substantial part of the calibration strategies for the future TRIDENT project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19111v1</guid>
      <category>physics.ins-det</category>
      <category>astro-ph.IM</category>
      <category>hep-ex</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Wei Tian, Wei Zhi, Qiao Xue, Wenlian Li, Zhenyu Wei, Fan Hu, Qichao Chang, MingXin Wang, Zhengyang Sun, Xiaohui Liu, Ziping Ye, Peng Miao, Xinliang Tian, Jianglai Liu, Donglian Xu</dc:creator>
    </item>
    <item>
      <title>Machine-assisted classification of potential biosignatures in earth-like exoplanets using low signal-to-noise ratio transmission spectra</title>
      <link>https://arxiv.org/abs/2407.19167</link>
      <description>arXiv:2407.19167v1 Announce Type: cross 
Abstract: The search for atmospheric biosignatures in Earth-like exoplanets is one of the most pressing challenges in observational astrobiology. Detecting biogenic gases in terrestrial planets requires high resolution and long integration times. In this work, we developed and tested a machine-learning general methodology, intended to classify transmission spectra with low Signal-to-Noise Ratio according to their potential to contain biosignatures. For that purpose, we trained a set of models capable of classifying noisy transmission spectra, as having methane, ozone, and/or water (multilabel classification), or simply as being interesting for follow-up observations (binary classification). The models were trained with $\sim\, 10^6$ synthetic spectra of planets similar to TRAPPIST-1 e, which were generated with the package MultiREx, especially developed for this work. The trained algorithms correctly classified test planets with transmission spectra having SNR $&lt;$ 6 and containing methane and/or ozone at mixing ratios similar to those of modern and Proterozoic Earth. Tests on realistic synthetic spectra based on the current Earth\'s atmosphere show at least one of our models would classify as likely having biosignatures and using only one transit, most of the inhabited terrestrial planets observed with the JWST/NIRSpec PRISM around M-dwarfs located at distances similar or smaller than that of TRAPPIST-1 e. The implication of this result for the designing of observing programs and future surveys is enormous since machine-assisted strategies similar to those presented here could significantly optimize the usage of JWST resources for biosignature searching, while maximizing the chances of a real discovery after dedicated follow-up observations of promising candidates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19167v1</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David S. Duque-Casta\~no, Jorge I. Zuluaga, Lauren Flor-Torres</dc:creator>
    </item>
    <item>
      <title>What can we learn about Reionization astrophysical parameters using Gaussian Process Regression?</title>
      <link>https://arxiv.org/abs/2407.19481</link>
      <description>arXiv:2407.19481v1 Announce Type: cross 
Abstract: Reionization is one of the least understood processes in the evolution history of the Universe, mostly because of the numerous astrophysical processes occurring simultaneously about which we do not have a very clear idea so far. In this article, we use the Gaussian Process Regression (GPR) method to learn the reionization history and infer the astrophysical parameters. We reconstruct the UV luminosity density function using the HFF and early JWST data. From the reconstructed history of reionization, the global differential brightness temperature fluctuation during this epoch has been computed. We perform MCMC analysis of the global 21-cm signal using the instrumental specifications of SARAS, in combination with Lyman-$\alpha$ ionization fraction data, Planck optical depth measurements and UV luminosity data. Our analysis reveals that GPR can help infer the astrophysical parameters in a model-agnostic way than conventional methods. Additionally, we analyze the 21-cm power spectrum using the reconstructed history of reionization and demonstrate how the future 21-cm mission SKA, in combination with Planck and Lyman-$\alpha$ forest data, improves the bounds on the reionization astrophysical parameters by doing a joint MCMC analysis for the astrophysical parameters plus 6 cosmological parameters for $\Lambda$CDM model. The results make the GPR-based reconstruction technique a robust learning process and the inferences on the astrophysical parameters obtained therefrom are quite reliable that can be used for future analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19481v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Purba Mukherjee, Antara Dey, Supratik Pal</dc:creator>
    </item>
    <item>
      <title>Improving Pulsar Timing Precision with Single Pulse Fluence Clustering</title>
      <link>https://arxiv.org/abs/2407.19615</link>
      <description>arXiv:2407.19615v1 Announce Type: cross 
Abstract: Traditional pulsar timing techniques involve averaging large numbers of single pulses to obtain a high signal-to-noise (S/N) profile, which is matched to a template to measure a time of arrival (TOA). However, the morphology of individual single pulses varies greatly due to pulse jitter. Pulses of different fluence contribute differently to the S/N of the pulse average. Our study proposes a method that accounts for these variations by identifying a range of states and timing each separately. We selected two 1-hour observations of PSR J2145-0750, each in a different frequency band with the Green Bank Telescope. We normalized the pulse amplitudes to account for scintillation effects and probed different excision algorithms to reduce radio-frequency interference. We then measured four pulse parameters (amplitude, position, width, and energy) to classify the single pulses using automated clustering algorithms. For each cluster, we calculated an average pulse profile and template and used both to obtain a TOA and TOA error. Finally, we computed the weighted average TOA and TOA error, the latter as a metric of the total timing precision for the epoch. The TOA is shifted relative to the one obtained without clustering, and we can estimate the shift with this weighting using the same data. For the 820 MHz and 1400 MHz bands, we obtained TOA uncertainties of 0.057 microseconds and 0.46 microseconds, compared to 0.066 microseconds and 0.74 microseconds when no clustering is applied. We conclude that tailoring this method could help improve the timing precision for certain bright pulsars in NANOGrav's dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19615v1</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sofia V. Sosa Fiscella, Michael T. Lam, Maura A. McLaughlin</dc:creator>
    </item>
    <item>
      <title>Reconstructing source motion from gravitational wave strain</title>
      <link>https://arxiv.org/abs/2407.20006</link>
      <description>arXiv:2407.20006v1 Announce Type: cross 
Abstract: Searches for un-modelled burst gravitational wave signals return potential candidates for short duration signals. As there is no clear model for the source in these searches, one needs to understand and reconstruct the system that produced the gravitational waves. Here we aim to reconstruct the source motion and masses of a system based only on its gravitational wave strain. We train a normalising flow on two models including circular orbits and random unphysical motion of two point masses. These models are used as a toy problem to illustrate the technique. We then reconstruct a distribution of possible motions of masses that can produce the given gravitational wave signal. We demonstrate the ability to reconstruct the full distribution of possible mass motions from strain data across multiple gravitational wave detectors. This distribution encompasses the degenerate motions that are expected to produce identical gravitational wave strains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20006v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Bayley, Chris Messenger, Graham Woan</dc:creator>
    </item>
    <item>
      <title>Ionospheric contributions to the excess power in high-redshift 21-cm power-spectrum observations with LOFAR</title>
      <link>https://arxiv.org/abs/2407.20220</link>
      <description>arXiv:2407.20220v1 Announce Type: cross 
Abstract: The turbulent ionosphere causes phase shifts to incoming radio waves on a broad range of temporal and spatial scales. When an interferometer is not sufficiently calibrated for the direction-dependent ionospheric effects, the time-varying phase shifts can cause the signal to decorrelate. The ionosphere's influence over various spatiotemporal scales introduces a baseline-dependent effect on the interferometric array. We study the impact of baseline-dependent decorrelation on high-redshift observations with the Low Frequency Array (LOFAR). Datasets with a range of ionospheric corruptions are simulated using a thin-screen ionosphere model, and calibrated using the state-of-the-art LOFAR Epoch of Reionisation pipeline. For the first time ever, we show the ionospheric impact on various stages of the calibration process including an analysis of the transfer of gain errors from longer to shorter baselines using realistic end-to-end simulations. We find that direction-dependent calibration for source subtraction leaves excess power of up to two orders of magnitude above the thermal noise at the largest spectral scales in the cylindrically averaged auto-power spectrum under normal ionospheric conditions. However, we demonstrate that this excess power can be removed through Gaussian process regression, leaving no excess power above the ten per cent level for a $5~$km diffractive scale. We conclude that ionospheric errors, in the absence of interactions with other aggravating effects, do not constitute a dominant component in the excess power observed in LOFAR Epoch of Reionisation observations of the North Celestial Pole. Future work should therefore focus on less spectrally smooth effects, such as beam modelling errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20220v1</guid>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. A. Brackenhoff, M. Mevius, L. V. E. Koopmans, A. Offringa, E. Ceccotti, J. K. Chege, B. K. Gehlot, S. Ghosh, C. H\"ofer, F. G. Mertens, S. Munshi, S. Zaroubi</dc:creator>
    </item>
    <item>
      <title>Assessing your Observatory's Impact: Best Practices in Establishing and Maintaining Observatory Bibliographies</title>
      <link>https://arxiv.org/abs/2401.00060</link>
      <description>arXiv:2401.00060v2 Announce Type: replace 
Abstract: Observatories need to measure and evaluate the scientific output and overall impact of their facilities. An observatory bibliography consists of the papers published using that observatory's data, typically gathered by searching the major journals for relevant keywords. Recently, the volume of literature and methods by which the publications pool is evaluated has increased. Efficient and standardized procedures are necessary to assign meaningful metadata; enable user-friendly retrieval; and provide the opportunity to derive reports, statistics, and visualizations to impart a deeper understanding of the research output. In 2021, a group of observatory bibliographers from around the world convened online to continue the discussions presented in Lagerstrom (2015). We worked to extract general guidelines from our experiences, techniques, and lessons learnt. The paper explores the development, application, and current status of telescope bibliographies and future trends. This paper briefly describes the methodologies employed in constructing databases, along with the various bibliometric techniques used to analyze and interpret them. We explain reasons for non-standardization and why it is essential for each observatory to identify metadata and metrics that are meaningful for them; caution the (over-)use of comparisons among facilities that are, ultimately, not comparable through bibliometrics; and highlight the benefits of telescope bibliographies, both for researchers within the astronomical community and for stakeholders beyond the specific observatories. There is tremendous diversity in the ways bibliographers track publications and maintain databases, due to parameters such as resources, type of observatory, historical practices, and reporting requirements to funders and outside agencies. However, there are also common sets of Best Practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00060v2</guid>
      <category>astro-ph.IM</category>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator> Observatory Bibliographers Collaboration, Raffaele D'Abrusco, Monique Gomez, Uta Grothkopf, Sharon Hunt, Ruth Kneale, Mika Konuma, Jenny Novacescu, Luisa Rebull, Elena Scire, Erin Scott, Donna Thompson, Lance Utley, Christopher Wilkinson, Sherry Winkelman</dc:creator>
    </item>
    <item>
      <title>Exploring Time Delay Interferometry Ranging as a Practical Ranging Approach in the Bayesian Framework</title>
      <link>https://arxiv.org/abs/2403.03060</link>
      <description>arXiv:2403.03060v5 Announce Type: replace 
Abstract: Time Delay Interferometry (TDI) is an indispensable step in the whole data processing procedure of space-based gravitational wave detection, as it mitigates the overwhelming laser frequency noise, which would otherwise completely bury the gravitational wave signals. Knowledge on the inter-spacecraft optical paths (i.e. delays) is one of the key elements of TDI. Conventional method for inter-spacecraft ranging mainly relies on the pseudo-random noise (PRN) code signal modulated onto the lasers. To ensure the reliability and robustness of this ranging information, it would be highly beneficial to develop other methods which could serve as cross-validations or backups. This paper explores the practical implementation of an alternative data-driven approach - time delay interferometry ranging (TDIR) - as a ranging technique independent of the PRN signal. Distinguished from previous research, our TDIR algorithm significantly relaxes the stringent requirement for clock synchronization imposed by traditional TDI procedure. By framing TDIR as a Bayesian parameter estimation problem and employing a general polynomial parametrization, we demonstrate our algorithm with simulated data based on the numerical orbit of Taiji. In the presence of laser frequency noise and secondary noises, the estimated median values of delays are only 5.28 ns away from the ground truths, capable of suppressing laser frequency noise to the desired level. Additionally, we have also analysed the requirements of mitigating optical bench noise and clock noise on TDIR, and presented an illustrative example for the impact of laser locking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03060v5</guid>
      <category>astro-ph.IM</category>
      <category>gr-qc</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rinp.2024.107819</arxiv:DOI>
      <arxiv:journal_reference>Results in Physics, Volume 62, July 2024, 107819</arxiv:journal_reference>
      <dc:creator>Minghui Du, Pengzhan Wu, Ziren Luo, Peng Xu</dc:creator>
    </item>
    <item>
      <title>Communicating the gravitational-wave discoveries of the LIGO-Virgo-KAGRA Collaboration</title>
      <link>https://arxiv.org/abs/2407.18638</link>
      <description>arXiv:2407.18638v2 Announce Type: replace 
Abstract: The LIGO-Virgo-KAGRA (LVK) Collaboration has made breakthrough discoveries in gravitational-wave astronomy, a new field of astronomy that provides a different means of observing our Universe. Gravitational-wave discoveries are possible thanks to the work of thousands of people from across the globe working together. In this article, we discuss the range of engagement activities used to communicate LVK gravitational-wave discoveries and the stories of the people behind the science using the activities surrounding the release of the third Gravitational Wave Transient Catalog as a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18638v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.HE</category>
      <category>gr-qc</category>
      <category>physics.ed-ph</category>
      <category>physics.pop-ph</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Middleton, Christopher P L Berry, Nicolas Arnaud, David Blair, Jacqueline Bondell, Nicolas Bonne, Debarati Chatterjee, Sylvain Chaty, Storm Colloms, Lynn Cominsky, Livia Conti, Isabel Cordero-Carri\'on, Robert Coyne, Zoheyr Doctor, Andreas Freise, Aaron Geller, Jen Gupta, Daniel Holz, William Katzman, David Keitel, Joey Shapiro Key, Nutsinee Kijbunchoo, Carl Knox, Coleman Krawczyk, Ryan N Lang, Shane L Larson, Chris North, Sascha Rieger, Aurore Simonnet, Andrew Spencer</dc:creator>
    </item>
    <item>
      <title>GERry: A Code to Optimise the Hunt for the Electromagnetic Counter-parts to Gravitational Wave Events</title>
      <link>https://arxiv.org/abs/2407.18642</link>
      <description>arXiv:2407.18642v2 Announce Type: replace 
Abstract: The search for the electromagnetic counterparts to gravitational wave (GW) events has been rapidly gathering pace in recent years thanks to the increasing number and capabilities of both gravitational wave detectors and wide field survey telescopes. Difficulties remain, however, in detecting these counterparts due to their inherent scarcity, faintness and rapidly evolving nature. To find these counterparts, it is important that one optimises the observing strategy for their recovery. This can be difficult due to the large number of potential variables at play. Such follow-up campaigns are also capable of detecting hundreds or potentially thousands of unrelated transients, particularly for GW events with poor localisation. Even if the observations are capable of detecting a counterpart, finding it among the numerous contaminants can prove challenging. Here we present the Gravitational wave Electromagnetic RecovRY code (GERry) to perform detailed analysis and survey-agnostic quantification of observing campaigns attempting to recover electromagnetic counterparts. GERry considers the campaign's spatial, temporal and wavelength coverage, in addition to Galactic extinction and the expected counterpart light curve evolution from the GW 3D localisation volume. It returns quantified statistics that can be used to: determine the probability of having detected the counterpart, identify the most promising sources, and assess and refine strategy. Here we demonstrate the code to look at the performance and parameter space probed by current and upcoming wide-field surveys such as GOTO &amp; VRO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18642v2</guid>
      <category>astro-ph.IM</category>
      <category>astro-ph.HE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David O'Neill, Joseph Lyman, Kendall Ackley, Danny Steeghs, Duncan Galloway, Vik Dhillon, Paul O'Brien, Gavin Ramsay, Kanthanakorn Noysena, Rubina Kotak, Rene Breton, Laura Nuttall, Enric Pall\'e, Don Pollacco, Krzysztof Ulaczyk, Martin Dyer, Felipe Jim\'enez-Ibarra, Tom Killestein, Amit Kumar, Lisa Kelsey, Ben Godson, Dan Jarvis</dc:creator>
    </item>
    <item>
      <title>Quantifying energy fluence and its uncertainty for radio emission from particle cascades in the presence of noise</title>
      <link>https://arxiv.org/abs/2407.18654</link>
      <description>arXiv:2407.18654v2 Announce Type: replace 
Abstract: Measurements of radio signals induced by an astroparticle generating a cascade present a challenge because they are always superposed with an irreducible noise contribution. Quantifying these signals constitutes a non-trivial task, especially at low signal-to-noise ratios (SNR). Because of the randomness of the noise phase, the measurements can be either a constructive or a destructive superposition of signal and noise. To recover the electromagnetic energy of the cascade from the radio measurements, the energy fluence, i.e. the time integral of the Poynting vector, has to be estimated. Conventionally, noise subtraction in the time domain has been employed for energy fluence reconstruction, yielding significant biases, including even non-physical and negative values. To mitigate the effect of this bias, usually an SNR threshold cut is imposed, at the expense of excluding valuable data from the analyses. Additionally, the uncertainties derived from the conventional method are underestimated, even for large SNR values. This work tackles these challenges by detailing a method to correctly estimate the uncertainties and lower the reconstruction bias in quantifying radio signals, thereby, ideally, eliminating the need for an SNR cut. The development of the method is based on a robust theoretical and statistical background, and the estimation of the fluence is performed in the frequency domain, allowing for the improvement of further analyses by providing access to frequency-dependent fluence estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18654v2</guid>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Martinelli, Tim Huege, Diego Ravignani, Harm Schoorlemmer</dc:creator>
    </item>
    <item>
      <title>Modeling the effectiveness of radiation shielding materials for astronaut protection on Mars</title>
      <link>https://arxiv.org/abs/2205.13786</link>
      <description>arXiv:2205.13786v3 Announce Type: replace-cross 
Abstract: The surface of Mars is bombarded by energetic charged particles of solar and cosmic origin with little shielding compared to Earth. As space agencies are planning for crewed missions to the red planet, a major concern is the impact of ionizing radiation on astronaut health. Keeping exposure below acceptable radiation dose levels is crucial for the health of the crew. In this study, our goal is to understand the radiation environment of Mars and describe the main strategies to be adopted to protect astronauts from the harmful impacts of cosmic radiation. Specifically, we investigate the shielding properties of various materials in the Martian radiation field using the Geant4 numerical model, after validating its accuracy with in-situ instrument measurements by MSL RAD. Our results indicate that composite materials such as types of plastic, rubber or synthetic fibers, have a similar response against cosmic rays and are the best shields. Martian regolith has an intermediate behavior and therefore could be used as an additional practical option. We show that the most widely used aluminum could be helpful when combined with other low atomic number materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.13786v3</guid>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>physics.space-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dionysios Gakis, Dimitra Atri</dc:creator>
    </item>
    <item>
      <title>Dark Matter Annihilation and Pair-Instability Supernovae</title>
      <link>https://arxiv.org/abs/2310.20044</link>
      <description>arXiv:2310.20044v2 Announce Type: replace-cross 
Abstract: We study the evolution of heavy stars ($M\ge40{\rm M}_\odot$) undergoing pair-instability in the presence of annihilating dark matter. Focusing on the scenario where the dark matter is in capture-annihilation equilibrium, we model the profile of energy injections in the local thermal equilibrium approximation. We find that significant changes to masses of astrophysical black holes formed by (pulsational) pair-instability supernovae can occur when the ambient dark matter density $ \rho_{\rm DM} \gtrsim10^9 \rm \, GeV \, cm^{-3}$. There are two distinct outcomes, depending on the dark matter mass. For masses $m_{\rm DM}\gtrsim1$ GeV the DM is primarily confined to the core. The annihilation increases the lifetime of core helium burning, resulting in more oxygen being formed, fueling a more violent explosion during the pair-instability-induced contraction. This drives stronger pulsations, leading to lighter black holes being formed than predicted by the standard model. For masses $m_{\rm DM}\lesssim0.5$ GeV there is significant dark matter in the envelope, leading to a phase where the star is supported by the energy from the annihilation. This reduces the core temperature and density, allowing the star to evade the pair-instability allowing heavier black holes to be formed. We find a mass gap for all models studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20044v2</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.CO</category>
      <category>astro-ph.IM</category>
      <category>gr-qc</category>
      <category>hep-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Phys.Rev.D 109 (2024) 10, 103021</arxiv:journal_reference>
      <dc:creator>Djuna Croon, Jeremy Sakstein</dc:creator>
    </item>
    <item>
      <title>Glitch veto based on unphysical gravitational wave binary inspiral templates</title>
      <link>https://arxiv.org/abs/2401.15237</link>
      <description>arXiv:2401.15237v3 Announce Type: replace-cross 
Abstract: Transient signals arising from instrumental or environmental factors, commonly referred to as glitches, constitute the predominant background of false alarms in gravitational wave searches with ground-based detectors. Therefore, effective data analysis methods for vetoing glitch-induced false alarms are crucial to enhancing the sensitivity of a search. We present a veto method for glitches that impact matched filtering-based searches for binary inspiral signals. The veto uses unphysical sectors in the space of chirp time parameters as well as an unphysical extension including negative chirp times to efficiently segregate glitches from gravitational wave signals in data from a single detector. Inhabited predominantly by glitches but nearly depopulated of genuine gravitational wave signals, these unphysical sectors can be efficiently explored using Particle Swarm Optimization. In a test carried out on data taken from both LIGO detectors spanning multiple observation runs, the veto was able to reject $99.9\%$ of glitches with no loss of signals detected with signal-to-noise ratio $\geq 9.0$ and total detector frame mass $\leq 80$ $M_\odot$. Our results show that extending a matched filter search to unphysical parts of a signal parameter space promises to be an effective strategy for mitigating glitches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.15237v3</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevD.110.023037</arxiv:DOI>
      <arxiv:journal_reference>Physical Review D, vol. 110, 023037 (2024)</arxiv:journal_reference>
      <dc:creator>Raghav Girgaonkar, Soumya D. Mohanty</dc:creator>
    </item>
    <item>
      <title>Detection prospects of very and ultra high-energy gamma rays from extended sources with ASTRI, CTA, and LHAASO</title>
      <link>https://arxiv.org/abs/2403.03731</link>
      <description>arXiv:2403.03731v2 Announce Type: replace-cross 
Abstract: Context. The recent discovery of several ultra high-energy gamma-ray emitters in our Galaxy represents a significant advancement towards the characterisation of its most powerful accelerators. Nonetheless, in order to unambiguously locate the regions where the highest energy particles are produced and understand the responsible physical mechanisms, detailed spectral and morphological studies are required, especially given that most of the observed sources were found to be significantly extended. Aims. In these regards, pointing observations with the next-generation Imaging Atmospheric Cherenkov Telescopes, such as the Cherenkov Telescope Array (CTA) Observatory and the ASTRI Mini-Array (ASTRI), are expected to provide significant improvements. Here we aim to identify the most promising sources to target in future observations. Methods. For this purpose, we performed a comparative analysis of the expected performance of ASTRI and CTA, computing their differential sensitivities towards extended sources, and further explored their capabilities with respect to specific case studies, including follow-ups of existing gamma-ray source catalogues. Results. We find that almost all of the sources thus far detected by LHAASO-WCDA and in the H.E.S.S. Galactic Plane Survey will be in the reach of ASTRI and CTA with about 300 and 50 hours of exposure, respectively. For the highest energy emitters detected by LHAASO-KM2A, in turn, we provide a list of the most promising objects that would require further investigation. We additionally examined specific classes of sources in order to identify potentially detectable gamma-ray emitters, such as passive molecular clouds (i.e. illuminated by the cosmic-ray sea) and pulsars surrounded by a halo of runaway particles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03731v2</guid>
      <category>astro-ph.HE</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Silvia Celli, Giada Peron</dc:creator>
    </item>
    <item>
      <title>Enhancing noise characterization with robust time delay interferometry combination</title>
      <link>https://arxiv.org/abs/2406.11305</link>
      <description>arXiv:2406.11305v3 Announce Type: replace-cross 
Abstract: Time delay interferometry (TDI) is essential for suppressing laser frequency noise and achieving the targeted sensitivity for space-borne gravitational wave (GW) missions. In Paper I, we examined the performance of the fiducial second-generation TDI Michelson configuration versus an alternative, the hybrid Relay, in noise suppression and data analysis. The results showed that both TDI schemes have comparable performances in mitigating laser and clock noises. However, when analyzing chirp signal from the coalescence of massive binary black holes, the Michelson configuration becomes inferior due to its vulnerable T channel and numerous null frequencies. In contrast, the hybrid Relay is more robust in dynamic unequal-arm scenarios. In this work, we further investigate the noise characterization capabilities of these two TDI configurations. Our investigations demonstrate that hybrid Relay achieves more robust noise parameter inference than the Michelson configuration. Moreover, the performance can be enhanced by replacing the T channel of hybrid Relay with a second-generation TDI null stream $C^{12}_3$. The combined three data streams, including two science observables from the hybrid Relay and $C^{12}_3$, could reduce the instabilities of noise spectra in the targeting frequency band and form an optimal dataset for characterizing noises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.11305v3</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gang Wang</dc:creator>
    </item>
    <item>
      <title>Precise and Efficient Orbit Prediction in LEO with Machine Learning using Exogenous Variables</title>
      <link>https://arxiv.org/abs/2407.11026</link>
      <description>arXiv:2407.11026v2 Announce Type: replace-cross 
Abstract: The increasing volume of space objects in Earth's orbit presents a significant challenge for Space Situational Awareness (SSA). And in particular, accurate orbit prediction is crucial to anticipate the position and velocity of space objects, for collision avoidance and space debris mitigation. When performing Orbit Prediction (OP), it is necessary to consider the impact of non-conservative forces, such as atmospheric drag and gravitational perturbations, that contribute to uncertainty around the future position of spacecraft and space debris alike. Conventional propagator methods like the SGP4 inadequately account for these forces, while numerical propagators are able to model the forces at a high computational cost. To address these limitations, we propose an orbit prediction algorithm utilizing machine learning. This algorithm forecasts state vectors on a spacecraft using past positions and environmental variables like atmospheric density from external sources. The orbital data used in the paper is gathered from precision ephemeris data from the International Laser Ranging Service (ILRS), for the period of almost a year. We show how the use of machine learning and time-series techniques can produce low positioning errors at a very low computational cost, thus significantly improving SSA capabilities by providing faster and reliable orbit determination for an ever increasing number of space objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.11026v2</guid>
      <category>cs.LG</category>
      <category>astro-ph.EP</category>
      <category>astro-ph.IM</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Francisco Caldas, Cl\'audia Soares</dc:creator>
    </item>
  </channel>
</rss>
