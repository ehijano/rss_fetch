<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 22 Aug 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 22 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Exploring the use of Generative AI to Support Automated Just-in-Time Programming for Visual Scene Displays</title>
      <link>https://arxiv.org/abs/2408.11137</link>
      <description>arXiv:2408.11137v1 Announce Type: new 
Abstract: Millions of people worldwide rely on alternative and augmentative communication devices to communicate. Visual scene displays (VSDs) can enhance communication for these individuals by embedding communication options within contextualized images. However, existing VSDs often present default images that may lack relevance or require manual configuration, placing a significant burden on communication partners. In this study, we assess the feasibility of leveraging large multimodal models (LMM), such as GPT-4V, to automatically create communication options for VSDs. Communication options were sourced from a LMM and speech-language pathologists (SLPs) and AAC researchers (N=13) for evaluation through an expert assessment conducted by the SLPs and AAC researchers. We present the study's findings, supplemented by insights from semi-structured interviews (N=5) about SLP's and AAC researchers' opinions on the use of generative AI in augmentative and alternative communication devices. Our results indicate that the communication options generated by the LMM were contextually relevant and often resembled those created by humans. However, vital questions remain that must be addressed before LMMs can be confidently implemented in AAC devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11137v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Cynthia Zastudil, Christine Holyfield, Christine Kapp, Xandria Crosland, Elizabeth Lorah, Tara Zimmerman, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>Predictive Anchoring: A Novel Interaction to Support Contextualized Suggestions for Grid Displays</title>
      <link>https://arxiv.org/abs/2408.11140</link>
      <description>arXiv:2408.11140v1 Announce Type: new 
Abstract: Grid displays are the most common form of augmentative and alternative communication device recommended by speech-language pathologists for children. Grid displays present a large variety of vocabulary which can be beneficial for a users' language development. However, the extensive navigation and cognitive overhead required of users of grid displays can negatively impact users' ability to actively participate in social interactions, which is an important factor of their language development. We present a novel interaction technique for grid displays, Predictive Anchoring, based on user interaction theory and language development theory. Our design is informed by existing literature in AAC research, presented in the form of a set of design goals and a preliminary design sketch. Future work in user studies and interaction design are also discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11140v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Cynthia Zastudil, Christine Holyfield, June A. Smith, Hannah Vy Nguyen, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>Non-verbal Hands-free Control for Smart Glasses using Teeth Clicks</title>
      <link>https://arxiv.org/abs/2408.11346</link>
      <description>arXiv:2408.11346v1 Announce Type: new 
Abstract: Smart glasses are emerging as a popular wearable computing platform potentially revolutionizing the next generation of human-computer interaction. The widespread adoption of smart glasses has created a pressing need for discreet and hands-free control methods. Traditional input techniques, such as voice commands or tactile gestures, can be intrusive and non-discreet. Additionally, voice-based control may not function well in noisy acoustic conditions. We propose a novel, discreet, non-verbal, and non-tactile approach to controlling smart glasses through subtle vibrations on the skin induced by teeth clicking. We demonstrate that these vibrations can be sensed by accelerometers embedded in the glasses with a low-footprint predictive model. Our proposed method, called STEALTHsense, utilizes a temporal broadcasting-based neural network architecture with just 88K trainable parameters and 7.14M Multiply and Accumulate (MMAC) per inference unit. We benchmark our proposed STEALTHsense against state-of-the-art deep learning approaches and traditional low-footprint machine learning approaches. We conducted a study across 21 participants to collect representative samples for two distinct teeth-clicking patterns and many non-patterns for robust training of STEALTHsense, achieving an average cross-person accuracy of 0.93. Field testing confirmed its effectiveness, even in noisy conditions, underscoring STEALTHsense's potential for real-world applications, offering a promising solution for smart glasses interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11346v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Payal Mohapatra, Ali Aroudi, Anurag Kumar, Morteza Khaleghimeybodi</dc:creator>
    </item>
    <item>
      <title>Audio Description Customization</title>
      <link>https://arxiv.org/abs/2408.11406</link>
      <description>arXiv:2408.11406v1 Announce Type: new 
Abstract: Blind and low-vision (BLV) people use audio descriptions (ADs) to access videos. However, current ADs are unalterable by end users, thus are incapable of supporting BLV individuals' potentially diverse needs and preferences. This research investigates if customizing AD could improve how BLV individuals consume videos. We conducted an interview study (Study 1) with fifteen BLV participants, which revealed desires for customizing properties like length, emphasis, speed, voice, format, tone, and language. At the same time, concerns like interruptions and increased interaction load due to customization emerged. To examine AD customization's effectiveness and tradeoffs, we designed CustomAD, a prototype that enables BLV users to customize AD content and presentation. An evaluation study (Study 2) with twelve BLV participants showed using CustomAD significantly enhanced BLV people's video understanding, immersion, and information navigation efficiency. Our work illustrates the importance of AD customization and offers a design that enhances video accessibility for BLV individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11406v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3663548.3675617</arxiv:DOI>
      <dc:creator>Rosiana Natalie, Ruei-Che Chang, Smitha Sheshadri, Anhong Guo, Kotaro Hara</dc:creator>
    </item>
    <item>
      <title>The Problems with Proxies: Making Data Work Visible through Requester Practices</title>
      <link>https://arxiv.org/abs/2408.11667</link>
      <description>arXiv:2408.11667v1 Announce Type: new 
Abstract: Fairness in AI and ML systems is increasingly linked to the proper treatment and recognition of data workers involved in training dataset development. Yet, those who collect and annotate the data, and thus have the most intimate knowledge of its development, are often excluded from critical discussions. This exclusion prevents data annotators, who are domain experts, from contributing effectively to dataset contextualization. Our investigation into the hiring and engagement practices of 52 data work requesters on platforms like Amazon Mechanical Turk reveals a gap: requesters frequently hold naive or unchallenged notions of worker identities and capabilities and rely on ad-hoc qualification tasks that fail to respect the workers' expertise. These practices not only undermine the quality of data but also the ethical standards of AI development. To rectify these issues, we advocate for policy changes to enhance how data annotation tasks are designed and managed and to ensure data workers are treated with the respect they deserve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11667v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Annabel Rothschild, Ding Wang, Niveditha Jayakumar Vilvanathan, Lauren Wilcox, Carl DiSalvo, Betsy DiSalvo</dc:creator>
    </item>
    <item>
      <title>Improved Visual Saliency of Graph Clusters with Orderable Node-Link Layouts</title>
      <link>https://arxiv.org/abs/2408.11673</link>
      <description>arXiv:2408.11673v1 Announce Type: new 
Abstract: Graphs are often used to model relationships between entities. The identification and visualization of clusters in graphs enable insight discovery in many application areas, such as life sciences and social sciences. Force-directed graph layouts promote the visual saliency of clusters, as they bring adjacent nodes closer together, and push non-adjacent nodes apart. At the same time, matrices can effectively show clusters when a suitable row/column ordering is applied, but are less appealing to untrained users not providing an intuitive node-link metaphor. It is thus worth exploring layouts combining the strengths of the node-link metaphor and node ordering. In this work, we study the impact of node ordering on the visual saliency of clusters in orderable node-link diagrams, namely radial diagrams, arc diagrams and symmetric arc diagrams. Through a crowdsourced controlled experiment, we show that users can count clusters consistently more accurately, and to a large extent faster, with orderable node-link diagrams than with three state-of-the art force-directed layout algorithms, i.e., `Linlog', `Backbone' and `sfdp'. The measured advantage is greater in case of low cluster separability and/or low compactness. A free copy of this paper and all supplemental materials are available at https://osf.io/kc3dg/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11673v1</guid>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nora Al-Naami, Nicolas M\'edoc, Matteo Magnani, Mohammad Ghoniem</dc:creator>
    </item>
    <item>
      <title>Cultural Windows: Towards a Workflow for Immersive Journeys into Global Living Spaces</title>
      <link>https://arxiv.org/abs/2408.11723</link>
      <description>arXiv:2408.11723v1 Announce Type: new 
Abstract: "Cultural Windows" is a research initiative designed to enrich cross-cultural understanding through immersive extended reality (XR) experiences. This project proposes a workflow for deploying AR and VR platforms, allowing users to explore living spaces from diverse cultures and socio-economic statuses. The process involves 3D scanning of culturally significant objects, creating accurate models of living spaces, and integrating them into immersive systems to facilitate engagement with global living designs. Targeted at individuals curious about how people live in different parts of the world, the project aims to expand cross-cultural understanding and design perspectives, providing insights into the effectiveness of immersive technologies in cultural education. By detailing its conceptual framework, "Cultural Windows" aims to enhance comprehension and appreciation of global domestic aesthetics by comparing participants' perceptions with immersive, realistic representations of living spaces from different cultures. This can help bridge the gap between preconceived notions and the actual appearance and feel of these spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11723v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hessam Djavaherpour, Pierre Dragicevic, Yvonne Jansen</dc:creator>
    </item>
    <item>
      <title>Esports Training in StarCraft II: Stance Stability and Grip Strength</title>
      <link>https://arxiv.org/abs/2408.11764</link>
      <description>arXiv:2408.11764v1 Announce Type: new 
Abstract: Esports are a mostly sedentary activity. There is a growing need for investigation into how biomechanical and physical abilities can be optimized for esports through training. One such research avenue concerns the ability of esports players to perform balance tasks due to the prolonged sedentary states that are required to reach the top echelon of performance.
  Our aim for this work is to describe and compare physical abilities (balance, grip strength, and self-reported training habits) of top Polish StarCraft~2 tournament players.
  Esports players differed significantly from the reference group in their ability to balance on one leg. Additionally, in a grip strength test, the esports group fared worse than the reference group in all consecutive attempts.
  Despite self-reported physical activity in the esports group, player fitness requires further research. Training optimization could offset the issues arising from sedentary activity, and intensifying esports training so it could take less time overall.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11764v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Andrzej Bia{\l}ecki, Micha{\l} Staniszewski, Robert Bia{\l}ecki, Jan Gajewski</dc:creator>
    </item>
    <item>
      <title>Dynamic Code Orchestration: Harnessing the Power of Large Language Models for Adaptive Script Execution</title>
      <link>https://arxiv.org/abs/2408.11060</link>
      <description>arXiv:2408.11060v1 Announce Type: cross 
Abstract: Computer programming initially required humans to directly translate their goals into machine code. These goals could have easily been expressed as a written (or human) language directive. Computers, however, had no capacity to satisfactorily interpret written language. Large language model's provide exactly this capability; automatic generation of computer programs or even assembly code from written language directives. This research examines dynamic code execution of written language directives within the context of a running application. It implements a text editor whose business logic is purely backed by large language model prompts. That is, the program's execution uses prompts and written language directives to dynamically generate application logic at the point in time it is needed. The research clearly shows how written language directives, backed by a large language model, offer radically new programming and operating system paradigms. For example, empowerment of users to directly implement requirements via written language directives, thus supplanting the need for a team ofprogrammers, a release schedule and the like. Or, new security mechanisms where static executables, always a target for reverse engineering or fuzzing, no longer exist. They are replaced by ephemeral executables that may continually change, be completely removed, and are easily updated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.11060v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Justin Del Vecchio, Andrew Perreault, Eliana Furmanek</dc:creator>
    </item>
    <item>
      <title>GigSense: An LLM-Infused Tool for Workers Collective Intelligence</title>
      <link>https://arxiv.org/abs/2405.02528</link>
      <description>arXiv:2405.02528v3 Announce Type: replace 
Abstract: Collective intelligence among gig workers yields considerable advantages, including improved information exchange, deeper social bonds, and stronger advocacy for better labor conditions. Especially as it enables workers to collaboratively pinpoint shared challenges and devise optimal strategies for addressing these issues. However, enabling collective intelligence remains challenging, as existing tools often overestimate gig workers' available time and uniformity in analytical reasoning. To overcome this, we introduce GigSense, a tool that leverages large language models alongside theories of collective intelligence and sensemaking. GigSense enables gig workers to rapidly understand and address shared challenges effectively, irrespective of their diverse backgrounds. Our user study showed that GigSense users outperformed those using a control interface in problem identification and generated solutions more quickly and of higher quality, with better usability experiences reported. GigSense not only empowers gig workers but also opens up new possibilities for supporting workers more broadly, demonstrating the potential of large language model interfaces to enhance collective intelligence efforts in the evolving workplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02528v3</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kashif Imteyaz, Claudia Flores-Saviaga, Saiph Savage</dc:creator>
    </item>
    <item>
      <title>Dynamic Color Assignment for Hierarchical Data</title>
      <link>https://arxiv.org/abs/2407.14742</link>
      <description>arXiv:2407.14742v4 Announce Type: replace 
Abstract: Assigning discriminable and harmonic colors to samples according to their class labels and spatial distribution can generate attractive visualizations and facilitate data exploration. However, as the number of classes increases, it is challenging to generate a high-quality color assignment result that accommodates all classes simultaneously. A practical solution is to organize classes into a hierarchy and then dynamically assign colors during exploration. However, existing color assignment methods fall short in generating high-quality color assignment results and dynamically aligning them with hierarchical structures. To address this issue, we develop a dynamic color assignment method for hierarchical data, which is formulated as a multi-objective optimization problem. This method simultaneously considers color discriminability, color harmony, and spatial distribution at each hierarchical level. By using the colors of parent classes to guide the color assignment of their child classes, our method further promotes both consistency and clarity across hierarchical levels. We demonstrate the effectiveness of our method in generating dynamic color assignment results with quantitative experiments and a user study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14742v4</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiashu Chen, Weikai Yang, Zelin Jia, Lanxi Xiao, Shixia Liu</dc:creator>
    </item>
    <item>
      <title>A Literature-based Visualization Task Taxonomy for Gantt Charts</title>
      <link>https://arxiv.org/abs/2408.04050</link>
      <description>arXiv:2408.04050v2 Announce Type: replace 
Abstract: Gantt charts are a widely-used idiom for visualizing temporal discrete event sequence data where dependencies exist between events. They are popular in domains such as manufacturing and computing for their intuitive layout of such data. However, these domains frequently generate data at scales which tax both the visual representation and the ability to render it at interactive speeds. To aid visualization developers who use Gantt charts in these situations, we develop a task taxonomy of low level visualization tasks supported by Gantt charts and connect them to the data queries needed to support them. Our taxonomy is derived through a literature survey of visualizations using Gantt charts over the past 30 years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04050v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sayef Azad Sakin, Katherine E. Isaacs</dc:creator>
    </item>
    <item>
      <title>Use Cases for Prospective Sensemaking of Human-AI-Collaboration</title>
      <link>https://arxiv.org/abs/2408.10812</link>
      <description>arXiv:2408.10812v2 Announce Type: replace 
Abstract: Our study explores the potential of human-AI collaboration (HAIC) through semi-structured interviews with 14 executives. We identify 63 HAIC use cases and classify them using a novel matrix combining value chain and group work activities. Most use cases identified are related to firm infrastructure and technology development, with very few pertaining to services and procurement, and none to logistics. HAIC is predominantly seen as support for choosing and executing group tasks, with an emphasis on choosing in supporting activities of the value chain. In contrast, primary activities such as operations and marketing focus more on executing group tasks. Few use cases involve negotiating tasks. Beyond identifying and classifying HAIC use cases, we discuss their potential as a tool for prospective sensemaking and to foster strategic managerial decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10812v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ishara Sudeeptha, Wieland Mueller, Michael Leyer, Alexander Richter, Ferry Nolte</dc:creator>
    </item>
    <item>
      <title>ICE: Interactive 3D Game Character Editing via Dialogue</title>
      <link>https://arxiv.org/abs/2403.12667</link>
      <description>arXiv:2403.12667v3 Announce Type: replace-cross 
Abstract: ost recent popular Role-Playing Games (RPGs) allow players to create in-game characters with hundreds of adjustable parameters, including bone positions and various makeup options. Although text-driven auto-customization systems have been developed to simplify the complex process of adjusting these intricate character parameters, they are limited by their single-round generation and lack the capability for further editing and fine-tuning. In this paper, we propose an Interactive Character Editing framework (ICE) to achieve a multi-round dialogue-based refinement process. In a nutshell, our ICE offers a more user-friendly way to enable players to convey creative ideas iteratively while ensuring that created characters align with the expectations of players. Specifically, we propose an Instruction Parsing Module (IPM) that utilizes large language models (LLMs) to parse multi-round dialogues into clear editing instruction prompts in each round. To reliably and swiftly modify character control parameters at a fine-grained level, we propose a Semantic-guided Low-dimension Parameter Solver (SLPS) that edits character control parameters according to prompts in a zero-shot manner. Our SLPS first localizes the character control parameters related to the fine-grained modification, and then optimizes the corresponding parameters in a low-dimension space to avoid unrealistic results. Extensive experimental results demonstrate the effectiveness of our proposed ICE for in-game character creation and the superior editing performance of ICE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.12667v3</guid>
      <category>cs.MM</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoqian Wu, Minda Zhao, Zhipeng Hu, Lincheng Li, Weijie Chen, Rui Zhao, Changjie Fan, Xin Yu</dc:creator>
    </item>
    <item>
      <title>TAGIFY: LLM-powered Tagging Interface for Improved Data Findability on OGD portals</title>
      <link>https://arxiv.org/abs/2407.18764</link>
      <description>arXiv:2407.18764v2 Announce Type: replace-cross 
Abstract: Efforts directed towards promoting Open Government Data (OGD) have gained significant traction across various governmental tiers since the mid-2000s. As more datasets are published on OGD portals, finding specific data becomes harder, leading to information overload. Complete and accurate documentation of datasets, including association of proper tags with datasets is key to improving dataset findability and accessibility. Analysis conducted on the Estonian Open Data Portal, revealed that 11% datasets have no associated tags, while 26% had only one tag assigned to them, which underscores challenges in data findability and accessibility within the portal, which, according to the recent Open Data Maturity Report, is considered trend-setter. The aim of this study is to propose an automated solution to tagging datasets to improve data findability on OGD portals. This paper presents Tagify - a prototype of tagging interface that employs large language models (LLM) such as GPT-3.5-turbo and GPT-4 to automate dataset tagging, generating tags for datasets in English and Estonian, thereby augmenting metadata preparation by data publishers and improving data findability on OGD portals by data users. The developed solution was evaluated by users and their feedback was collected to define an agenda for future prototype improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18764v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kevin Kliimask, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Humboldt: Metadata-Driven Extensible Data Discovery</title>
      <link>https://arxiv.org/abs/2408.05439</link>
      <description>arXiv:2408.05439v2 Announce Type: replace-cross 
Abstract: Data discovery is crucial for data management and analysis and can benefit from better utilization of metadata. For example, users may want to search data using queries like ``find the tables created by Alex and endorsed by Mike that contain sales numbers.'' They may also want to see how the data they view relates to other data, its lineage, or the quality and compliance of its upstream datasets, all metadata. Yet, effectively surfacing metadata through interactive user interfaces (UIs) to augment data discovery poses challenges. Constantly revamping UIs with each update to metadata sources (or providers) consumes significant development resources and lacks scalability and extensibility. In response, we introduce Humboldt, a new framework enabling interactive data systems to effectively leverage metadata for data discovery and rapidly evolve their UIs to support metadata changes. Humboldt decouples metadata sources from the implementation of data discovery UIs that support search and dataset visualization using metadata fields. It automatically generates interactive data discovery interfaces from declarative specifications, avoiding costly metadata-specific (re)implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05439v2</guid>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex B\"auerle, \c{C}a\u{g}atay Demiralp, Michael Stonebraker</dc:creator>
    </item>
    <item>
      <title>BEYOND DIALOGUE: A Profile-Dialogue Alignment Framework Towards General Role-Playing Language Model</title>
      <link>https://arxiv.org/abs/2408.10903</link>
      <description>arXiv:2408.10903v2 Announce Type: replace-cross 
Abstract: The rapid advancement of large language models (LLMs) has revolutionized role-playing, enabling the development of general role-playing models. However, current role-playing training has two significant issues: (I) Using a predefined role profile to prompt dialogue training for specific scenarios usually leads to inconsistencies and even conflicts between the dialogue and the profile, resulting in training biases. (II) The model learns to imitate the role based solely on the profile, neglecting profile-dialogue alignment at the sentence level. In this work, we propose a simple yet effective framework called BEYOND DIALOGUE, designed to overcome these hurdles. This framework innovatively introduces "beyond dialogue" tasks to align dialogue with profile traits based on each specific scenario, thereby eliminating biases during training. Furthermore, by adopting an innovative prompting mechanism that generates reasoning outcomes for training, the framework allows the model to achieve fine-grained alignment between profile and dialogue at the sentence level. The aforementioned methods are fully automated and low-cost. Additionally, the integration of automated dialogue and objective evaluation methods forms a comprehensive framework, paving the way for general role-playing. Experimental results demonstrate that our model excels in adhering to and reflecting various dimensions of role profiles, outperforming most proprietary general and specialized role-playing baselines. All code and datasets are available at https://github.com/yuyouyu32/BeyondDialogue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10903v2</guid>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yeyong Yu, Rusheng Yu, Haojie Wei, Zhanqiu Zhang, Quan Qian</dc:creator>
    </item>
  </channel>
</rss>
