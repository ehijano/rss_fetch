<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.MA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.MA</link>
    <description>cs.MA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.MA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 22 Jul 2024 04:01:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 22 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multi-robot maze exploration using an efficient cost-utility method</title>
      <link>https://arxiv.org/abs/2407.14218</link>
      <description>arXiv:2407.14218v1 Announce Type: new 
Abstract: In the field of modern robotics, robots are proving to be useful in tackling high-risk situations, such as navigating hazardous environments like burning buildings, earthquake-stricken areas, or patrolling crime-ridden streets, as well as exploring uncharted caves. These scenarios share similarities with maze exploration problems in terms of complexity. While several methods have been proposed for single-agent systems, ranging from potential fields to flood-fill methods, recent research endeavors have focused on creating methods tailored for multiple agents to enhance the quality and efficiency of maze coverage. The contribution of this paper is the implementation of established maze exploration methods and their comparison with a new cost-utility algorithm designed for multiple agents, which combines the existing methodologies to optimize exploration outcomes. Through a comprehensive and comparative analysis, this paper evaluates the performance of the new approach against the implemented baseline methods from the literature, highlighting its efficacy and potential advantages in various scenarios. The code and experimental results supporting this study are available in the following repository (https://github.com/manouslinard/multiagent-exploration/).</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14218v1</guid>
      <category>cs.MA</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Manousos Linardakis, Iraklis Varlamis, Georgios Th. Papadopoulos</dc:creator>
    </item>
    <item>
      <title>The Vision of Autonomic Computing: Can LLMs Make It a Reality?</title>
      <link>https://arxiv.org/abs/2407.14402</link>
      <description>arXiv:2407.14402v1 Announce Type: cross 
Abstract: The Vision of Autonomic Computing (ACV), proposed over two decades ago, envisions computing systems that self-manage akin to biological organisms, adapting seamlessly to changing environments. Despite decades of research, achieving ACV remains challenging due to the dynamic and complex nature of modern computing systems. Recent advancements in Large Language Models (LLMs) offer promising solutions to these challenges by leveraging their extensive knowledge, language understanding, and task automation capabilities. This paper explores the feasibility of realizing ACV through an LLM-based multi-agent framework for microservice management. We introduce a five-level taxonomy for autonomous service maintenance and present an online evaluation benchmark based on the Sock Shop microservice demo project to assess our framework's performance. Our findings demonstrate significant progress towards achieving Level 3 autonomy, highlighting the effectiveness of LLMs in detecting and resolving issues within microservice architectures. This study contributes to advancing autonomic computing by pioneering the integration of LLMs into microservice management frameworks, paving the way for more adaptive and self-managing computing systems. The code will be made available at https://aka.ms/ACV-LLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14402v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiyang Zhang, Fangkai Yang, Xiaoting Qin, Jue Zhang, Qingwei Lin, Gong Cheng, Dongmei Zhang, Saravan Rajmohan, Qi Zhang</dc:creator>
    </item>
    <item>
      <title>Stackelberg POMDP: A Reinforcement Learning Approach for Economic Design</title>
      <link>https://arxiv.org/abs/2210.03852</link>
      <description>arXiv:2210.03852v4 Announce Type: replace-cross 
Abstract: We introduce a reinforcement learning framework for economic design where the interaction between the environment designer and the participants is modeled as a Stackelberg game. In this game, the designer (leader) sets up the rules of the economic system, while the participants (followers) respond strategically. We integrate algorithms for determining followers' response strategies into the leader's learning environment, providing a formulation of the leader's learning problem as a POMDP that we call the Stackelberg POMDP. We prove that the optimal leader's strategy in the Stackelberg game is the optimal policy in our Stackelberg POMDP under a limited set of possible policies, establishing a connection between solving POMDPs and Stackelberg games. We solve our POMDP under a limited set of policy options via the centralized training with decentralized execution framework. For the specific case of followers that are modeled as no-regret learners, we solve an array of increasingly complex settings, including problems of indirect mechanism design where there is turn-taking and limited communication by agents. We demonstrate the effectiveness of our training framework through ablation studies. We also give convergence results for no-regret learners to a Bayesian version of a coarse-correlated equilibrium, extending known results to correlated types.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.03852v4</guid>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca Brero, Alon Eden, Darshan Chakrabarti, Matthias Gerstgrasser, Amy Greenwald, Vincent Li, David C. Parkes</dc:creator>
    </item>
  </channel>
</rss>
