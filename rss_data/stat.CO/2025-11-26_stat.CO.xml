<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 02:41:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An Infinite BART model</title>
      <link>https://arxiv.org/abs/2511.20087</link>
      <description>arXiv:2511.20087v1 Announce Type: new 
Abstract: Bayesian additive regression trees (BART) are popular Bayesian ensemble models used in regression and classification analysis. Under this modeling framework, the regression function is approximated by an ensemble of decision trees, interpreted as weak learners that capture different features of the data. In this work, we propose a generalization of the BART model that has two main features: first, it automatically selects the number of decision trees using the given data; second, the model allows clusters of observations to have different regression functions since each data point can only use a selection of weak learners, instead of all of them. This model generalization is accomplished by including a binary weight matrix in the conditional distribution of the response variable, which activates only a specific subset of decision trees for each observation. Such a matrix is endowed with an Indian Buffet process prior, and sampled within the MCMC sampler, together with the other BART parameters. We then compare the Infinite BART model with the classic one on simulated and real datasets. Specifically, we provide examples illustrating variable importance, partial dependence and causal estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20087v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Battiston, Yu Luo</dc:creator>
    </item>
    <item>
      <title>$MC^2$ Mixed Integer and Linear Programming</title>
      <link>https://arxiv.org/abs/2511.20575</link>
      <description>arXiv:2511.20575v1 Announce Type: new 
Abstract: In this paper, we design $MC^2$ algorithms for Mixed Integer and Linear Programming. By expressing a constrained optimisation as one of simulation from a Boltzmann distribution, we reformulate integer and linear programming as Monte Carlo optimisation problems. The key insight is that solving these optimisation problems requires the ability to simulate from truncated distributions, namely multivariate exponentials and Gaussians. Efficient simulation can be achieved using the algorithms of Kent and Davis. We demonstrate our methodology on portfolio optimisation and the classical farmer problem from stochastic programming. Finally, we conclude with directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20575v1</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Nick Polson, Vadim Sokolov</dc:creator>
    </item>
    <item>
      <title>Optimization and Regularization Under Arbitrary Objectives</title>
      <link>https://arxiv.org/abs/2511.19628</link>
      <description>arXiv:2511.19628v1 Announce Type: cross 
Abstract: This study investigates the limitations of applying Markov Chain Monte Carlo (MCMC) methods to arbitrary objective functions, focusing on a two-block MCMC framework which alternates between Metropolis-Hastings and Gibbs sampling. While such approaches are often considered advantageous for enabling data-driven regularization, we show that their performance critically depends on the sharpness of the employed likelihood form. By introducing a sharpness parameter and exploring alternative likelihood formulations proportional to the target objective function, we demonstrate how likelihood curvature governs both in-sample performance and the degree of regularization inferred by the training data. Empirical applications are conducted on reinforcement learning tasks: including a navigation problem and the game of tic-tac-toe. The study concludes with a separate analysis examining the implications of extreme likelihood sharpness on arbitrary objective functions stemming from the classic game of blackjack, where the first block of the two-block MCMC framework is replaced with an iterative optimization step. The resulting hybrid approach achieves performance nearly identical to the original MCMC framework, indicating that excessive likelihood sharpness effectively collapses posterior mass onto a single dominant mode.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19628v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani, Etienne Pienaar</dc:creator>
    </item>
    <item>
      <title>Quantum Framework for Wavelet Shrinkage</title>
      <link>https://arxiv.org/abs/2511.19855</link>
      <description>arXiv:2511.19855v1 Announce Type: cross 
Abstract: This paper develops a unified framework for quantum wavelet shrinkage, extending classical denoising ideas into the quantum domain. Shrinkage is interpreted as a completely positive trace-preserving process, so attenuation of coefficients is carried out through controlled decoherence rather than nonlinear thresholding. Phase damping and ancilla-driven constructions realize this behavior coherently and show that statistical adaptivity and quantum unitarity can be combined within a single circuit model. The same physical mechanisms that reduce quantum coherence, such as dephasing and amplitude damping, are repurposed as programmable resources for noise suppression. Practical demonstrations implemented with Qiskit illustrate how circuits and channels emulate coefficientwise attenuation, and all examples are provided as Jupyter notebooks in the companion GitHub repository. Encoding schemes for amplitude, phase, and hybrid representations are examined in relation to transform coherence and measurement feasibility, and realizations suited to current noisy intermediate-scale quantum devices are discussed. The work provides a conceptual and experimental link between wavelet-based statistical inference and quantum information processing, and shows how engineered decoherence can act as an operational surrogate for classical shrinkage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19855v1</guid>
      <category>quant-ph</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Brani Vidakovic</dc:creator>
    </item>
    <item>
      <title>A Generalized Additive Partial-Mastery Cognitive Diagnosis Model</title>
      <link>https://arxiv.org/abs/2511.20191</link>
      <description>arXiv:2511.20191v1 Announce Type: cross 
Abstract: Cognitive diagnosis models (CDMs) are restricted latent class models widely used for measuring attributes of interest in diagnostic assessments in education, psychology, biomedical sciences, and related fields. Partial-mastery CDMs (PM-CDMs) are an important extension of CDMs. They model individuals' status for each attribute to be continuous for measuring the partial mastery level, which relaxes the restrictive discrete-attribute assumption of classical CDMs. As a result, PM-CDMs often yield better fits for real-world data and refined measurement of the substantive attributes of interest. However, these models inherit some strong parametric assumptions from the traditional CDMs about the item response functions and, thus, still suffer from a significant risk of model misspecification. This paper proposes a generalized additive PM-CDM (GaPM-CDM) that substantially relaxes the parametric assumptions of PM-CDMs. This proposal leverages model parsimony and interpretability by modeling each item response function as a mixture of nonparametric monotone functions of attributes. A method for the estimation of GaPM-CDM is developed, which combines the marginal maximum likelihood estimator with a sieve approximation of the nonparametric functions. The new model is applicable under both confirmatory and exploratory settings, depending on whether prior knowledge is available about the relationship between observed variables and attributes. The proposed method is applied to two measurement problems from educational testing and healthcare research, respectively, and further evaluated and compared with PM-CDMs through extensive simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20191v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Camilo C\'ardenas-Hurtado, Yunxiao Chen, Irini Moustaki</dc:creator>
    </item>
    <item>
      <title>Nonuniform-Grid Markov Chain Approximation of Continuous Processes with Time-Linear Moments</title>
      <link>https://arxiv.org/abs/2511.20416</link>
      <description>arXiv:2511.20416v2 Announce Type: cross 
Abstract: We propose a method to approximate continuous-time, continuous-state stochastic processes by a discrete-time Markov chain defined on a nonuniform grid. Our method provides exact moment matching for processes whose first and second moments are linear functions of time. In particular, we show that, under certain conditions, the transition probabilities of a Markov chain can be chosen so that its first two moments match prescribed linear functions of time. These conditions depend on the grid points of the Markov chain and the coefficients of the linear mean and variance functions. Our proof relies on two recurrence relations for the expectation and variance across time. This approach enables simulation-based numerical analysis of continuous processes while preserving their key characteristics. We illustrate its efficacy by approximating continuous processes describing heat diffusion and geometric Brownian motion (GBM). For heat diffusion, we show that the heat profile at a set of points can be investigated by embedding those points inside the nonuniform grid of our Markov chain. For GBM, numerical simulations demonstrate that our approach, combined with suitable nonuniform grids, yields accurate approximations, with consistently small empirical Wasserstein-1 distances at long time horizons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20416v2</guid>
      <category>math.PR</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Do Hyun Kim, Ahmet Cetinkaya</dc:creator>
    </item>
    <item>
      <title>A label-switching algorithm for fast core-periphery identification</title>
      <link>https://arxiv.org/abs/2506.02069</link>
      <description>arXiv:2506.02069v2 Announce Type: replace 
Abstract: Core-periphery (CP) structure is frequently observed in networks where the nodes form two distinct groups: a small, densely interconnected core and a sparse periphery. Borgatti and Everett (2000) proposed one of the most popular methods to identify and quantify CP structure by comparing the observed network with an ``ideal'' CP structure. While this metric has been widely used, an improved algorithm is still needed. In this work, we detail a greedy, label-switching algorithm to identify CP structure that is both fast and accurate. By leveraging a mathematical reformulation of the CP metric, our proposed heuristic offers an order-of-magnitude improvement on the number of operations compared to a naive implementation. We prove that the algorithm converges to a local minimum while consistently yielding solutions within 90\% of the global optimum on small toy networks. On synthetic networks, our algorithm exhibits superior classification accuracies and run-times compared to a popular competing method, and on one-real world network is 340 times faster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02069v2</guid>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Srijan Sengupta</dc:creator>
    </item>
    <item>
      <title>Quantile Fourier Transform, Quantile Series, and Nonparametric Estimation of Quantile Spectra</title>
      <link>https://arxiv.org/abs/2211.05844</link>
      <description>arXiv:2211.05844v4 Announce Type: replace-cross 
Abstract: A nonparametric method is proposed for estimating the quantile spectra and cross-spectra introduced in Li (2012; 2014) as bivariate functions of frequency and quantile level. The method is based on the quantile discrete Fourier transform (QDFT) defined by trigonometric quantile regression and the quantile series (QSER) defined by the inverse Fourier transform of the QDFT. A nonparametric spectral estimator is constructed from the autocovariance function of the QSER using the lag-window (LW) approach. Smoothing techniques are also employed to reduce the statistical variability of the LW estimator across quantiles when the underlying spectrum varies smoothly with respect to the quantile level. The performance of the proposed estimation method is evaluated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.05844v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1080/03610918.2025.2509820</arxiv:DOI>
      <dc:creator>Ta-Hsin Li</dc:creator>
    </item>
    <item>
      <title>Sparse Techniques for Regression in Deep Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.11355</link>
      <description>arXiv:2505.11355v2 Announce Type: replace-cross 
Abstract: Gaussian processes (GPs) have gained popularity as flexible machine learning models for regression and function approximation with an in-built method for uncertainty quantification. However, GPs suffer when the amount of training data is large or when the underlying function contains multi-scale features that are difficult to represent by a stationary kernel. To address the former, training of GPs with large-scale data is often performed through inducing point approximations, also known as sparse GP regression (GPR), where the size of the covariance matrices in GPR is reduced considerably through a greedy search on the data set. To aid the latter, deep GPs have gained traction as hierarchical models that resolve multi-scale features by combining multiple GPs. Posterior inference in deep GPs requires a sampling or, more usual, a variational approximation. Variational approximations lead to large-scale stochastic, non-convex optimisation problems and the resulting approximation tends to represent uncertainty incorrectly. In this work, we combine variational learning with MCMC to develop a particle-based expectation-maximisation method to simultaneously find inducing points within the large-scale data (variationally) and accurately train the deep GPs (sampling-based). The result is a highly efficient and accurate methodology for deep GP training on large-scale data. We test our method on standard benchmark problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11355v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Latz, Aretha L. Teckentrup, Simon Urbainczyk</dc:creator>
    </item>
    <item>
      <title>Scaling Up ROC-Optimizing Support Vector Machines</title>
      <link>https://arxiv.org/abs/2511.04979</link>
      <description>arXiv:2511.04979v2 Announce Type: replace-cross 
Abstract: The ROC-SVM, originally proposed by Rakotomamonjy, directly maximizes the area under the ROC curve (AUC) and has become an attractive alternative of the conventional binary classification under the presence of class imbalance. However, its practical use is limited by high computational cost, as training involves evaluating all $O(n^2)$. To overcome this limitation, we develop a scalable variant of the ROC-SVM that leverages incomplete U-statistics, thereby substantially reducing computational complexity. We further extend the framework to nonlinear classification through a low-rank kernel approximation, enabling efficient training in reproducing kernel Hilbert spaces. Theoretical analysis establishes an error bound that justifies the proposed approximation, and empirical results on both synthetic and real datasets demonstrate that the proposed method achieves comparable AUC performance to the original ROC-SVM with drastically reduced training time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04979v2</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gimun Bae, Seung Jun Shin</dc:creator>
    </item>
    <item>
      <title>Faster estimation of the transformation-free linear simplicial-simplicial regression via constrained iterative reweighted least squares</title>
      <link>https://arxiv.org/abs/2511.13296</link>
      <description>arXiv:2511.13296v2 Announce Type: replace-cross 
Abstract: Simplicial-simplicial regression refers to the regression setting where both the responses and predictor variables lie within the simplex space, i.e. they are compositional. \cite{fiksel2022} proposed a transformation-free linear regression model, that minimizes the Kullback-Leibler divergence from the observed to the fitted compositions, where the EM algorithm is used to estimate the regression coefficients. We formulate the model as a constrained logistic regression, in the spirit of \cite{tsagris2025}, and we estimate the regression coefficients using constrained iteratively reweighted least squares. The simulation studies depict that this algorithm makes the estimation procedure significantly faster, uses less memory, and in some cases gives a better solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13296v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 26 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michail Tsagris</dc:creator>
    </item>
  </channel>
</rss>
