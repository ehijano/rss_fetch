<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Aug 2024 02:29:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Comprehensive Survey on Root Cause Analysis in (Micro) Services: Methodologies, Challenges, and Trends</title>
      <link>https://arxiv.org/abs/2408.00803</link>
      <description>arXiv:2408.00803v1 Announce Type: new 
Abstract: The complex dependencies and propagative faults inherent in microservices, characterized by a dense network of interconnected services, pose significant challenges in identifying the underlying causes of issues. Prompt identification and resolution of disruptive problems are crucial to ensure rapid recovery and maintain system stability. Numerous methodologies have emerged to address this challenge, primarily focusing on diagnosing failures through symptomatic data. This survey aims to provide a comprehensive, structured review of root cause analysis (RCA) techniques within microservices, exploring methodologies that include metrics, traces, logs, and multi-model data. It delves deeper into the methodologies, challenges, and future trends within microservices architectures. Positioned at the forefront of AI and automation advancements, it offers guidance for future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00803v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tingting Wang, Guilin Qi</dc:creator>
    </item>
    <item>
      <title>ArchCode: Incorporating Software Requirements in Code Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2408.00994</link>
      <description>arXiv:2408.00994v1 Announce Type: new 
Abstract: This paper aims to extend the code generation capability of large language models (LLMs) to automatically manage comprehensive software requirements from given textual descriptions. Such requirements include both functional (i.e. achieving expected behavior for inputs) and non-functional (e.g., time/space performance, robustness, maintainability) requirements. However, textual descriptions can either express requirements verbosely or may even omit some of them. We introduce ARCHCODE, a novel framework that leverages in-context learning to organize requirements observed in descriptions and to extrapolate unexpressed requirements from them. ARCHCODE generates requirements from given descriptions, conditioning them to produce code snippets and test cases. Each test case is tailored to one of the requirements, allowing for the ranking of code snippets based on the compliance of their execution results with the requirements. Public benchmarks show that ARCHCODE enhances to satisfy functional requirements, significantly improving Pass@k scores. Furthermore, we introduce HumanEval-NFR, the first evaluation of LLMs' non-functional requirements in code generation, demonstrating ARCHCODE's superiority over baseline methods. The implementation of ARCHCODE and the HumanEval-NFR benchmark are both publicly accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00994v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hojae Han, Jaejin Kim, Jaeseok Yoo, Youngwon Lee, Seung-won Hwang</dc:creator>
    </item>
    <item>
      <title>The Impact of Hyperparameters on Large Language Model Inference Performance: An Evaluation of vLLM and HuggingFace Pipelines</title>
      <link>https://arxiv.org/abs/2408.01050</link>
      <description>arXiv:2408.01050v1 Announce Type: new 
Abstract: The recent surge of open-source large language models (LLMs) enables developers to create AI-based solutions while maintaining control over aspects such as privacy and compliance, thereby providing governance and ownership of the model deployment process. To utilize these LLMs, inference engines are needed. These engines load the model's weights onto available resources, such as GPUs, and process queries to generate responses. The speed of inference, or performance, of the LLM, is critical for real-time applications, as it computes millions or billions of floating point operations per inference. Recently, advanced inference engines such as vLLM have emerged, incorporating novel mechanisms such as efficient memory management to achieve state-of-the-art performance. In this paper, we analyze the performance, particularly the throughput (tokens generated per unit of time), of 20 LLMs using two inference libraries: vLLM and HuggingFace's pipelines. We investigate how various hyperparameters, which developers must configure, influence inference performance. Our results reveal that throughput landscapes are irregular, with distinct peaks, highlighting the importance of hyperparameter optimization to achieve maximum performance. We also show that applying hyperparameter optimization when upgrading or downgrading the GPU model used for inference can improve throughput from HuggingFace pipelines by an average of 9.16% and 13.7%, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01050v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matias Martinez</dc:creator>
    </item>
    <item>
      <title>LLM as Runtime Error Handler: A Promising Pathway to Adaptive Self-Healing of Software Systems</title>
      <link>https://arxiv.org/abs/2408.01055</link>
      <description>arXiv:2408.01055v1 Announce Type: new 
Abstract: Unanticipated runtime errors, lacking predefined handlers, can abruptly terminate execution and lead to severe consequences, such as data loss or system crashes. Despite extensive efforts to identify potential errors during the development phase, such unanticipated errors remain a challenge to to be entirely eliminated, making the runtime mitigation measurements still indispensable to minimize their impact. Automated self-healing techniques, such as reusing existing handlers, have been investigated to reduce the loss coming through with the execution termination. However, the usability of existing methods is retained by their predefined heuristic rules and they fail to handle diverse runtime errors adaptively. Recently, the advent of Large Language Models (LLMs) has opened new avenues for addressing this problem. Inspired by their remarkable capabilities in understanding and generating code, we propose to deal with the runtime errors in a real-time manner using LLMs.
  Specifically, we propose Healer, the first LLM-assisted self-healing framework for handling runtime errors. When an unhandled runtime error occurs, Healer will be activated to generate a piece of error-handling code with the help of its internal LLM and the code will be executed inside the runtime environment owned by the framework to obtain a rectified program state from which the program should continue its execution. Our exploratory study evaluates the performance of Healer using four different code benchmarks and three state-of-the-art LLMs, GPT-3.5, GPT-4, and CodeQwen-7B. Results show that, without the need for any fine-tuning, GPT-4 can successfully help programs recover from 72.8% of runtime errors, highlighting the potential of LLMs in handling runtime errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01055v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhensu Sun, Haotian Zhu, Bowen Xu, Xiaoning Du, Li Li, David Lo</dc:creator>
    </item>
    <item>
      <title>Validation of an Analysability Model in Hybrid Quantum Software</title>
      <link>https://arxiv.org/abs/2408.01105</link>
      <description>arXiv:2408.01105v1 Announce Type: new 
Abstract: In the context of quantum-classical hybrid computing, evaluating analysability, which is the ease of understanding and modifying software, presents significant challenges due to the complexity and novelty of quantum algorithms. Although advances have been made in quantum software development, standard software quality evaluation methods do not fully address the specifics of quantum components, resulting in a gap in the ability to ensure and maintain the quality of hybrid software products. In this registered report proposal, we intend to validate a quality model focused on the analysability of hybrid software through an international collab orative approach involving academic institutions from Italy and Spain through a controlled experiment. This approach allows for a more detailed analysis and validation methodology and establishes a framework for future research and developments in software quality assessment in quantum computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01105v1</guid>
      <category>cs.SE</category>
      <category>quant-ph</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>D\'iaz-Mu\~noz Ana, Cruz-Lemus Jos\'e A., Rodr\'iguez Mois\'es, Piattini Mario, Baldassarre Maria Teresa</dc:creator>
    </item>
    <item>
      <title>Epistemic Ensembles in Semantic and Symbolic Environments (Extended Version with Proofs)</title>
      <link>https://arxiv.org/abs/2408.01115</link>
      <description>arXiv:2408.01115v1 Announce Type: new 
Abstract: An epistemic ensemble is composed of knowledge-based agents capable of retrieving and sharing knowledge and beliefs about themselves and their peers. These agents access a global knowledge state and use actions to communicate and cooperate, altering the collective knowledge state. We study two types of mathematical semantics for epistemic ensembles based on a common syntactic operational ensemble semantics: a semantic environment defined by a class of global epistemic states, and a symbolic environment consisting of a set of epistemic formul{\ae}. For relating these environments, we use the concept of {\Phi}-equivalence, where a class of epistemic states and a knowledge base are {\Phi}-equivalent, if any formula of {\Phi} holds in the class of epistemic states if, and only if, it is an element of the knowledge base. Our main theorem shows that {\Phi}-equivalent configurations simulate each other and satisfy the same dynamic epistemic ensemble formulae.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01115v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Rolf Hennicker, Alexander Knapp, Martin Wirsing</dc:creator>
    </item>
    <item>
      <title>The Impact of Program Reduction on Automated Program Repair</title>
      <link>https://arxiv.org/abs/2408.01134</link>
      <description>arXiv:2408.01134v1 Announce Type: new 
Abstract: Correcting bugs using modern Automated Program Repair (APR) can be both time-consuming and resource-expensive. We describe a program repair approach that aims to improve the scalability of modern APR tools. The approach leverages program reduction in the form of program slicing to eliminate code irrelevant to fixing the bug, which improves the APR tool's overall performance. We investigate slicing's impact on all three phases of the repair process: fault localization, patch generation, and patch validation. Our empirical exploration finds that the proposed approach, on average, enhances the repair ability of the TBar APR tool, but we also discovered a few cases where it was less successful. Specifically, on examples from the widely used Defects4J dataset, we obtain a substantial reduction in median repair time, which falls from 80 minutes to just under 18 minutes. We conclude that program reduction can improve the performance of APR without degrading repair quality, but this improvement is not universal.
  A replication package is available via Zenodo at https://doi.org/10.5281/zenodo.13074333.
  Keywords: automated program repair, dynamic program slicing, fault localization, test-suite reduction, hybrid techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01134v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Linas Vidziunas, David Binkley, Leon Moonen</dc:creator>
    </item>
    <item>
      <title>Automatic Pull Request Description Generation Using LLMs: A T5 Model Approach</title>
      <link>https://arxiv.org/abs/2408.00921</link>
      <description>arXiv:2408.00921v1 Announce Type: cross 
Abstract: Developers create pull request (PR) descriptions to provide an overview of their changes and explain the motivations behind them. These descriptions help reviewers and fellow developers quickly understand the updates. Despite their importance, some developers omit these descriptions. To tackle this problem, we propose an automated method for generating PR descriptions based on commit messages and source code comments. This method frames the task as a text summarization problem, for which we utilized the T5 text-to-text transfer model. We fine-tuned a pre-trained T5 model using a dataset containing 33,466 PRs. The model's effectiveness was assessed using ROUGE metrics, which are recognized for their strong alignment with human evaluations. Our findings reveal that the T5 model significantly outperforms LexRank, which served as our baseline for comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00921v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Nazmus Sakib, Md Athikul Islam, Md Mashrur Arifin</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models for Mobile App Review Feature Extraction</title>
      <link>https://arxiv.org/abs/2408.01063</link>
      <description>arXiv:2408.01063v1 Announce Type: cross 
Abstract: Mobile app review analysis presents unique challenges due to the low quality, subjective bias, and noisy content of user-generated documents. Extracting features from these reviews is essential for tasks such as feature prioritization and sentiment analysis, but it remains a challenging task. Meanwhile, encoder-only models based on the Transformer architecture have shown promising results for classification and information extraction tasks for multiple software engineering processes. This study explores the hypothesis that encoder-only large language models can enhance feature extraction from mobile app reviews. By leveraging crowdsourced annotations from an industrial context, we redefine feature extraction as a supervised token classification task. Our approach includes extending the pre-training of these models with a large corpus of user reviews to improve contextual understanding and employing instance selection techniques to optimize model fine-tuning. Empirical evaluations demonstrate that this method improves the precision and recall of extracted features and enhances performance efficiency. Key contributions include a novel approach to feature extraction, annotated datasets, extended pre-trained models, and an instance selection mechanism for cost-effective fine-tuning. This research provides practical methods and empirical evidence in applying large language models to natural language processing tasks within mobile app reviews, offering improved performance in feature extraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01063v1</guid>
      <category>cs.CL</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Quim Motger, Alessio Miaschi, Felice Dell'Orletta, Xavier Franch, Jordi Marco</dc:creator>
    </item>
    <item>
      <title>A Systematic Mapping Study on SDN Controllers for Enhancing Security in IoT Networks</title>
      <link>https://arxiv.org/abs/2408.01303</link>
      <description>arXiv:2408.01303v1 Announce Type: cross 
Abstract: Context: The increase in Internet of Things (IoT) devices gives rise to an increase in deceptive manipulations by malicious actors. These actors should be prevented from targeting the IoT networks. Cybersecurity threats have evolved and become dynamically sophisticated, such that they could exploit any vulnerability found in IoT networks. However, with the introduction of the Software Defined Network (SDN) in the IoT networks as the central monitoring unit, IoT networks are less vulnerable and less prone to threats. %Although, the SDN itself is vulnerable to several threats.
  Objective: To present a comprehensive and unbiased overview of the state-of-the-art on IoT networks security enhancement using SDN controllers.
  Method: We review the current body of knowledge on enhancing the security of IoT networks using SDN with a Systematic Mapping Study (SMS) following the established guidelines.
  Results: The SMS result comprises 33 primary studies analyzed against four major research questions. The SMS highlights current research trends and identifies gaps in the SDN-IoT network security.
  Conclusion: We conclude that the SDN controller architecture commonly used for securing IoT networks is the centralized controller architecture. However, this architecture is not without its limitations. Additionally, the predominant technique utilized for risk mitigation is machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01303v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Oredola, Adnan Ashraf</dc:creator>
    </item>
    <item>
      <title>MCGMark: An Encodable and Robust Online Watermark for LLM-Generated Malicious Code</title>
      <link>https://arxiv.org/abs/2408.01354</link>
      <description>arXiv:2408.01354v1 Announce Type: cross 
Abstract: With the advent of large language models (LLMs), numerous software service providers (SSPs) are dedicated to developing LLMs customized for code generation tasks, such as CodeLlama and Copilot. However, these LLMs can be leveraged by attackers to create malicious software, which may pose potential threats to the software ecosystem. For example, they can automate the creation of advanced phishing malware. To address this issue, we first conduct an empirical study and design a prompt dataset, MCGTest, which involves approximately 400 person-hours of work and consists of 406 malicious code generation tasks. Utilizing this dataset, we propose MCGMark, the first robust, code structure-aware, and encodable watermarking approach to trace LLM-generated code. We embed encodable information by controlling the token selection and ensuring the output quality based on probabilistic outliers. Additionally, we enhance the robustness of the watermark by considering the structural features of malicious code, preventing the embedding of the watermark in easily modified positions, such as comments. We validate the effectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves an embedding success rate of 88.9% within a maximum output limit of 400 tokens. Furthermore, it also demonstrates strong robustness and has minimal impact on the quality of the output code. Our approach assists SSPs in tracing and holding responsible parties accountable for malicious code generated by LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01354v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaiwen Ning, Jiachi Chen, Qingyuan Zhong, Tao Zhang, Yanlin Wang, Wei Li, Yu Zhang, Weizhe Zhang, Zibin Zheng</dc:creator>
    </item>
    <item>
      <title>RCAgent: Cloud Root Cause Analysis by Autonomous Agents with Tool-Augmented Large Language Models</title>
      <link>https://arxiv.org/abs/2310.16340</link>
      <description>arXiv:2310.16340v3 Announce Type: replace 
Abstract: Large language model (LLM) applications in cloud root cause analysis (RCA) have been actively explored recently. However, current methods are still reliant on manual workflow settings and do not unleash LLMs' decision-making and environment interaction capabilities. We present RCAgent, a tool-augmented LLM autonomous agent framework for practical and privacy-aware industrial RCA usage. Running on an internally deployed model rather than GPT families, RCAgent is capable of free-form data collection and comprehensive analysis with tools. Our framework combines a variety of enhancements, including a unique Self-Consistency for action trajectories, and a suite of methods for context management, stabilization, and importing domain knowledge. Our experiments show RCAgent's evident and consistent superiority over ReAct across all aspects of RCA -- predicting root causes, solutions, evidence, and responsibilities -- and tasks covered or uncovered by current rules, as validated by both automated metrics and human evaluations. Furthermore, RCAgent has already been integrated into the diagnosis and issue discovery workflow of the Real-time Compute Platform for Apache Flink of Alibaba Cloud.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16340v3</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3627673.3680016</arxiv:DOI>
      <dc:creator>Zefan Wang, Zichuan Liu, Yingying Zhang, Aoxiao Zhong, Jihong Wang, Fengbin Yin, Lunting Fan, Lingfei Wu, Qingsong Wen</dc:creator>
    </item>
    <item>
      <title>A higher-order transformation approach to the formalization and analysis of BPMN using graph transformation systems</title>
      <link>https://arxiv.org/abs/2311.05243</link>
      <description>arXiv:2311.05243v5 Announce Type: replace 
Abstract: The Business Process Modeling Notation (BPMN) is a widely used standard notation for defining intra- and inter-organizational workflows. However, the informal description of the BPMN execution semantics leads to different interpretations of BPMN elements and difficulties in checking behavioral properties. In this article, we propose a formalization of the execution semantics of BPMN that, compared to existing approaches, covers more BPMN elements while also facilitating property checking. Our approach is based on a higher-order transformation from BPMN models to graph transformation systems. To show the capabilities of our approach, we implemented it as an open-source web-based tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05243v5</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Kr\"auter, Adrian Rutle, Harald K\"onig, Yngve Lamo</dc:creator>
    </item>
    <item>
      <title>Efficient Test Data Generation for MC/DC with OCL and Search</title>
      <link>https://arxiv.org/abs/2401.03469</link>
      <description>arXiv:2401.03469v3 Announce Type: replace 
Abstract: System-level testing of avionics software systems requires compliance with different international safety standards such as DO-178C. An important consideration of the avionics industry is automated test data generation according to the criteria suggested by safety standards. One of the recommended criteria by DO-178C is the modified condition/decision coverage (MC/DC) criterion. The current model-based test data generation approaches use constraints written in Object Constraint Language (OCL), and apply search techniques to generate test data. These approaches either do not support MC/DC criterion or suffer from performance issues while generating test data for large-scale avionics systems. In this paper, we propose an effective way to automate MC/DC test data generation during model-based testing. We develop a strategy that utilizes case-based reasoning (CBR) and range reduction heuristics designed to solve MC/DC-tailored OCL constraints. We performed an empirical study to compare our proposed strategy for MC/DC test data generation using CBR, range reduction, both CBR and range reduction, with an original search algorithm, and random search. We also empirically compared our strategy with existing constraint-solving approaches. The results show that both CBR and range reduction for MC/DC test data generation outperform the baseline approach. Moreover, the combination of both CBR and range reduction for MC/DC test data generation is an effective approach compared to existing constraint solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03469v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hassan Sartaj, Muhammad Zohaib Iqbal, Atif Aftab Ahmed Jilani, Muhammad Uzair Khan</dc:creator>
    </item>
    <item>
      <title>Understanding Fairness in Software Engineering: Insights from Stack Exchange</title>
      <link>https://arxiv.org/abs/2402.19038</link>
      <description>arXiv:2402.19038v3 Announce Type: replace 
Abstract: Software practitioners discuss problems at work with peers, in-person and online. These discussions can be technical (e.g., how to fix a bug?) and social (e.g., how to assign work fairly?). While there is a growing body of knowledge exploring fairness problems and solutions in the human and social factors of software engineering, most focus has been on specific problems. This study provides fairness discussions by software practitioners on Stack Exchange sites. We present an exploratory study presenting the fairness experience of software practitioners and fairness expectations in software teams. We also want to identify the fairness aspects software practitioners talk about the most. For example, do they care more about fairness in income or how they are treated in the workplace?
  Our investigation of fairness discussions on eight Stack Exchange sites resulted in a list of 136 posts (28 questions and 108 answers) manually curated from 4,178 candidate posts. The study reveals that the majority of fairness discussions (24 posts) revolve around the topic of income suggesting that many software practitioners are highly interested in matters related to their pay and how it is fairly distributed. Further, we noted that while not discussed as often, discussions on fairness in recruitment tend to receive the highest number of views and scores. Interestingly, the study shows that unfairness experiences extend beyond the protected attributes. In this study, only 25 out of 136 posts mention protected attributes, with gender mainly being discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.19038v3</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3674805.3686687</arxiv:DOI>
      <dc:creator>Emeralda Sesari, Federica Sarro, Ayushi Rastogi</dc:creator>
    </item>
    <item>
      <title>Automated System-level Testing of Unmanned Aerial Systems</title>
      <link>https://arxiv.org/abs/2403.15857</link>
      <description>arXiv:2403.15857v2 Announce Type: replace 
Abstract: Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems. The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes. The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings. The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation. In this paper, we propose a novel approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test scenarios are generated on the fly, i.e., during test execution based on the environmental context at runtime. The approach is supported by a toolset. We empirically evaluate the proposed approach on two core components of UAS, an autopilot system of an unmanned aerial vehicle (UAV) and cockpit display systems (CDS) of the ground control station (GCS). The results show that the AITester effectively generates test scenarios causing deviations from the expected behavior of the UAV autopilot and reveals potential flaws in the GCS-CDS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15857v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10515-024-00462-9</arxiv:DOI>
      <arxiv:journal_reference>Autom Softw Eng 31, 64 (2024)</arxiv:journal_reference>
      <dc:creator>Hassan Sartaj, Asmar Muqeet, Muhammad Zohaib Iqbal, Muhammad Uzair Khan</dc:creator>
    </item>
    <item>
      <title>Initial Insights on MLOps: Perception and Adoption by Practitioners</title>
      <link>https://arxiv.org/abs/2408.00463</link>
      <description>arXiv:2408.00463v2 Announce Type: replace 
Abstract: The accelerated adoption of AI-based software demands precise development guidelines to guarantee reliability, scalability, and ethical compliance. MLOps (Machine Learning and Operations) guidelines have emerged as the principal reference in this field, paving the way for the development of high-level automated tools and applications. Despite the introduction of MLOps guidelines, there is still a degree of skepticism surrounding their implementation, with a gradual adoption rate across many companies. In certain instances, a lack of awareness about MLOps has resulted in organizations adopting similar approaches unintentionally, frequently without a comprehensive understanding of the associated best practices and principles. The objective of this study is to gain insight into the actual adoption of MLOps (or comparable) guidelines in different business contexts. To this end, we surveyed practitioners representing a range of business environments to understand how MLOps is adopted and perceived in their companies. The results of this survey also shed light on other pertinent aspects related to the advantages and challenges of these guidelines, the learning curve associated with them, and the future trends that can be derived from this information. This study aims to provide deeper insight into MLOps and its impact on the next phase of innovation in machine learning. By doing so, we aim to lay the foundation for more efficient, reliable, and creative AI applications in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00463v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Moreschi, David H\"astbacka, Andrea Janes, Valentina Lenarduzzi, Davide Taibi</dc:creator>
    </item>
    <item>
      <title>A new framework for calibrating COVID-19 SEIR models with spatial-/time-varying coefficients using genetic and sliding window algorithms</title>
      <link>https://arxiv.org/abs/2402.08524</link>
      <description>arXiv:2402.08524v2 Announce Type: replace-cross 
Abstract: A susceptible-exposed-infected-removed (SEIR) model assumes spatial-/time-varying coefficients to model the effect of non-pharmaceutical interventions (NPIs) on the regional and temporal distribution of COVID-19 disease epidemics. A significant challenge in using such model is their fast and accurate calibration to observed data from geo-referenced hospitalized data, i.e., efficient estimation of the spatial-/time-varying parameters. In this work, a new calibration framework is proposed towards optimizing the spatial-/time-varying parameters of the SEIR model. We also devise a method for combing the overlapping sliding window technique (OSW) with a genetic algorithm (GA) calibration routine to automatically search the segmented parameter space. Parallelized GA is used to reduce the computational burden. Our framework abstracts the implementation complexity of the method away from the user. It provides high-level APIs for setting up a customized calibration system and consuming the optimized values of parameters. We evaluated the application of our method on the calibration of a spatial age-structured microsimulation model using a single objective function that comprises observed COVID-19-related ICU demand. The results reflect the effectiveness of the proposed method towards estimating the parameters in a changing environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08524v2</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <category>physics.soc-ph</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Zhou, Ralf Schneider</dc:creator>
    </item>
    <item>
      <title>AIM: Automated Input Set Minimization for Metamorphic Security Testing</title>
      <link>https://arxiv.org/abs/2402.10773</link>
      <description>arXiv:2402.10773v3 Announce Type: replace-cross 
Abstract: Although the security testing of Web systems can be automated by generating crafted inputs, solutions to automate the test oracle, i.e., distinguishing correct from incorrect outputs, remain preliminary. Specifically, previous work has demonstrated the potential of metamorphic testing; indeed, security failures can be determined by metamorphic relations that turn valid inputs into malicious inputs. However, without further guidance, metamorphic relations are typically executed on a large set of inputs, which is time-consuming and thus makes metamorphic testing impractical. We propose AIM, an approach that automatically selects inputs to reduce testing costs while preserving vulnerability detection capabilities. AIM includes a clustering-based black box approach, to identify similar inputs based on their security properties. It also relies on a novel genetic algorithm able to efficiently select diverse inputs while minimizing their total cost. Further, it contains a problem-reduction component to reduce the search space and speed up the minimization process. We evaluated the effectiveness of AIM on two well-known Web systems, Jenkins and Joomla, with documented vulnerabilities. We compared AIM's results with four baselines. Overall, AIM reduced metamorphic testing time by 84% for Jenkins and 82% for Joomla, while preserving vulnerability detection. Furthermore, AIM outperformed all the considered baselines regarding vulnerability coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10773v3</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nazanin Bayati Chaleshtari, Yoann Marquer, Fabrizio Pastore, Lionel C. Briand</dc:creator>
    </item>
    <item>
      <title>Comprehensive Library of Variational LSE Solvers</title>
      <link>https://arxiv.org/abs/2404.09916</link>
      <description>arXiv:2404.09916v2 Announce Type: replace-cross 
Abstract: Linear systems of equations can be found in various mathematical domains, as well as in the field of machine learning. By employing noisy intermediate-scale quantum devices, variational solvers promise to accelerate finding solutions for large systems. Although there is a wealth of theoretical research on these algorithms, only fragmentary implementations exist. To fill this gap, we have developed the variational-lse-solver framework, which realizes existing approaches in literature, and introduces several enhancements. The user-friendly interface is designed for researchers that work at the abstraction level of identifying and developing end-to-end applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09916v2</guid>
      <category>quant-ph</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nico Meyer, Martin R\"ohn, Jakob Murauer, Axel Plinge, Christopher Mutschler, Daniel D. Scherer</dc:creator>
    </item>
  </channel>
</rss>
