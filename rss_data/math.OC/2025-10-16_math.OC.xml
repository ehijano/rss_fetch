<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 16 Oct 2025 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Average-case thresholds for exact regularization of linear programs</title>
      <link>https://arxiv.org/abs/2510.13083</link>
      <description>arXiv:2510.13083v1 Announce Type: new 
Abstract: Small regularizers can preserve linear programming solutions exactly. This paper provides the first average-case analysis of exact regularization: with a standard Gaussian cost vector and fixed constraint set, bounds are established for the probability that exact regularization succeeds as a function of regularization strength. Failure is characterized via the Gaussian measure of inner cones, controlled by novel two-sided bounds on the measure of shifted cones. Results reveal dimension-dependent scaling laws and connect exact regularization of linear programs to their polyhedral geometry via the normal fan and the Gaussian (solid-angle) measure of its cones. Computable bounds are obtained in several canonical settings, including regularized optimal transport. Numerical experiments corroborate the predicted scalings and thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13083v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael P. Friedlander, Sharvaj Kubal, Yaniv Plan, Matthew S. Scott</dc:creator>
    </item>
    <item>
      <title>Unstable optimal transport maps</title>
      <link>https://arxiv.org/abs/2510.13265</link>
      <description>arXiv:2510.13265v1 Announce Type: new 
Abstract: The stability of optimal transport maps with respect to perturbations of the marginals is a question of interest for several reasons, ranging from the justification of the linearized optimal transport framework to numerical analysis and statistics. Under various assumptions on the source measure, it is known that optimal transport maps are stable with respect to variations of the target measure. In this note, we focus on the mechanisms that can, on the contrary, lead to instability. We identify two of them, which we illustrate through examples of absolutely continuous source measures $\rho$ in $\mathbb{R}^d$ for which optimal transport maps are less stable, or even very unstable. We first show that instability may arise from the unboundedness of the density: we exhibit a source density on the unit ball of $\mathbb{R}^d$ which blows up superpolynomially at two points of the boundary and for which optimal transport maps are highly unstable. Then we prove that even for uniform densities on bounded open sets, optimal transport maps can be rather unstable close enough to configurations where uniqueness of optimal plans is lost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13265v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cyril Letrouit (CNRS, LMO, PARMA)</dc:creator>
    </item>
    <item>
      <title>An Augmented Lagrangian Method on GPU for Security-Constrained AC Optimal Power Flow</title>
      <link>https://arxiv.org/abs/2510.13333</link>
      <description>arXiv:2510.13333v1 Announce Type: new 
Abstract: We present a new algorithm for solving large-scale security-constrained optimal power flow in polar form (AC-SCOPF). The method builds on Nonlinearly Constrained augmented Lagrangian (NCL), an augmented Lagrangian method in which the subproblems are solved using an interior-point method. NCL has two key advantages for large-scale SC-OPF. First, NCL handles difficult problems such as infeasible ones or models with complementarity constraints. Second, the augmented Lagrangian term naturally regularizes the Newton linear systems within the interior-point method, enabling to solve the Newton systems with a pivoting-free factorization that can be efficiently parallelized on GPUs. We assess the performance of our implementation, called MadNCL, on large-scale corrective AC-SCOPFs, with complementarity constraints modeling the corrective actions. Numerical results show that MadNCL can solve AC-SCOPF with 500 buses and 256 contingencies fully on the GPU in less than 3 minutes, whereas Knitro takes more than 3 hours to find an equivalent solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13333v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Pacaud, Armin Nurkanovi\'c, Anton Pozharskiy, Alexis Montoison, Sungho Shin</dc:creator>
    </item>
    <item>
      <title>Target Controllability Score</title>
      <link>https://arxiv.org/abs/2510.13354</link>
      <description>arXiv:2510.13354v1 Announce Type: new 
Abstract: We introduce the target controllability score (TCS), a concept for evaluating node importance under actuator constraints and designated target objectives, formulated within a virtual system setting. The TCS consists of the target volumetric controllability score (VCS) and the target average energy controllability score (AECS), each defined as an optimal solution to a convex optimization problem associated with the output controllability Gramian. We establish the existence and uniqueness (for almost all time horizons) and develop a projected gradient method for their computation. To enable scalability, we construct a target-only reduced virtual system and derive non-asymptotic bounds showing that weak cross-coupling and a low or negative logarithmic norm of the system matrix yield accurate approximations of target VCS/AECS, particularly over short or moderate time horizons. Experiments on human brain networks reveal a clear trade-off: at short horizons, both target VCS and target AECS are well approximated by their reduced formulations, while at long horizons, target AECS remains robust but target VCS deteriorates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13354v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kazuhiro Sato</dc:creator>
    </item>
    <item>
      <title>A non-parametric Zermelo navigation equation for strictly convex control sets</title>
      <link>https://arxiv.org/abs/2510.13458</link>
      <description>arXiv:2510.13458v1 Announce Type: new 
Abstract: We study a generalized version of Zermelo's navigation problem in which the admissible set of control velocities is a strictly convex compact set, rather than the classical spherical or ball-shaped one. After establishing existence results under the natural assumption of weak currents, we derive necessary optimality conditions via Pontryagin's maximum principle and convex analysis. In particular, we prove that strictly convex control sets ensure smoothness of optimal controls. In dimension two, this regularity allows us to eliminate the adjoint variables and obtain a second-order differential equation for the optimal control, which extends the classical Zermelo navigation equation to strictly convex control sets in a non-parametric setting. We also develop the case of an affine current, with a particular emphasis on the constant one where optimal trajectories reduce to straight lines. The results are illustrated with examples relevant to ship routing with asymmetric or sail-assisted propulsion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13458v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Della Rossa, Lorenzo Freddi, Mattia Pinatto</dc:creator>
    </item>
    <item>
      <title>Data-driven learning of feedback maps for explicit robust predictive control: an approximation theoretic view</title>
      <link>https://arxiv.org/abs/2510.13522</link>
      <description>arXiv:2510.13522v1 Announce Type: new 
Abstract: We establish an algorithm to learn feedback maps from data for a class of robust model predictive control (MPC) problems. The algorithm accounts for the approximation errors due to the learning directly at the synthesis stage, ensuring recursive feasibility by construction. The optimal control problem consists of a linear noisy dynamical system, a quadratic stage and quadratic terminal costs as the objective, and convex constraints on the state, control, and disturbance sequences; the control minimizes and the disturbance maximizes the objective. We proceed via two steps -- (a) Data generation: First, we reformulate the given minmax problem into a convex semi-infinite program and employ recently developed tools to solve it in an exact fashion on grid points of the state space to generate (state, action) data. (b) Learning approximate feedback maps: We employ a couple of approximation schemes that furnish tight approximations within preassigned uniform error bounds on the admissible state space to learn the unknown feedback policy. The stability of the closed-loop system under the approximate feedback policies is also guaranteed under a standard set of hypotheses. Two benchmark numerical examples are provided to illustrate the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13522v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siddhartha Ganguly, Shubham Gupta, Debasish Chatterjee</dc:creator>
    </item>
    <item>
      <title>Non-Gaussian Distribution Steering in Nonlinear Dynamics with Conjugate Unscented Transformation</title>
      <link>https://arxiv.org/abs/2510.12946</link>
      <description>arXiv:2510.12946v1 Announce Type: cross 
Abstract: In highly nonlinear systems such as the ones commonly found in astrodynamics, Gaussian distributions generally evolve into non-Gaussian distributions. This paper introduces a method for effectively controlling non-Gaussian distributions in nonlinear environments using optimized linear feedback control. This paper utilizes Conjugate Unscented Transformation to quantify the higher-order statistical moments of non-Gaussian distributions. The formulation focuses on controlling and constraining the sigma points associated with the uncertainty quantification, which would thereby reflect the control of the entire distribution and constraints on the moments themselves. This paper develops an algorithm to solve this problem with sequential convex programming, and it is demonstrated through a two-body and three-body example. The examples show that individual moments can be directly controlled, and the moments are accurately approximated for non-Gaussian distributions throughout the controller's time horizon in nonlinear dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12946v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel C. Qi, Kenshiro Oguri, Puneet Singla, Maruthi R. Akella</dc:creator>
    </item>
    <item>
      <title>Competitive EV charging station location with queues</title>
      <link>https://arxiv.org/abs/2510.12961</link>
      <description>arXiv:2510.12961v1 Announce Type: cross 
Abstract: Electric vehicle (EV) public charging infrastructure planning faces significant challenges in competitive markets, where multiple service providers affect congestion and user behavior. This work extends existing modeling frameworks by incorporating the presence of competitors' stations and more realistic queueing systems.
  First, we analyze three finite queueing systems, M/M/1/K, M/M/s/K, and M/Er/s/K, with varying numbers of servers (charging outlets) and service time distributions, deriving analytic expressions for user behavior metrics. Second, we embed the queueing-based user behavior model into a bilevel program, where the upper level locates new charging stations to maximize accessibility (throughput), and the lower level captures users' station choices via a user equilibrium. Third, we apply a reformulation from competitive congested user-choice facility location models to approximately solve the bilevel problem and introduce a surrogate-based heuristic to enhance scalability. Fourth, we showcase our methodology on a real-world case study of an urban area in Montreal (Canada), offering managerial insights into how user-choice behavior assumptions and competition affect throughput and location decisions. The results demonstrate that our model yields (re)location strategies that outperform the existing network. More broadly, this approach provides a tool for incorporating charging service quality-through queueing metrics-and existing competition into station planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12961v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>The Minh Nguyen, Nagisa Sugishita, Margarida Carvalho, Amira Dems</dc:creator>
    </item>
    <item>
      <title>Time-Varying Optimization for Streaming Data Via Temporal Weighting</title>
      <link>https://arxiv.org/abs/2510.13052</link>
      <description>arXiv:2510.13052v1 Announce Type: cross 
Abstract: Classical optimization theory deals with fixed, time-invariant objective functions. However, time-varying optimization has emerged as an important subject for decision-making in dynamic environments. In this work, we study the problem of learning from streaming data through a time-varying optimization lens. Unlike prior works that focus on generic formulations, we introduce a structured, \emph{weight-based} formulation that explicitly captures the streaming-data origin of the time-varying objective, where at each time step, an agent aims to minimize a weighted average loss over all the past data samples. We focus on two specific weighting strategies: (1) uniform weights, which treat all samples equally, and (2) discounted weights, which geometrically decay the influence of older data. For both schemes, we derive tight bounds on the ``tracking error'' (TE), defined as the deviation between the model parameter and the time-varying optimum at a given time step, under gradient descent (GD) updates. We show that under uniform weighting, the TE vanishes asymptotically with a $\mathcal{O}(1/t)$ decay rate, whereas discounted weighting incurs a nonzero error floor controlled by the discount factor and the number of gradient updates performed at each time step. Our theoretical findings are validated through numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13052v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SP</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Faraz Ul Abrar, Nicol\`o Michelusi, Erik G. Larsson</dc:creator>
    </item>
    <item>
      <title>Achieving Logarithmic Regret in KL-Regularized Zero-Sum Markov Games</title>
      <link>https://arxiv.org/abs/2510.13060</link>
      <description>arXiv:2510.13060v1 Announce Type: cross 
Abstract: Reverse Kullback-Leibler (KL) divergence-based regularization with respect to a fixed reference policy is widely used in modern reinforcement learning to preserve the desired traits of the reference policy and sometimes to promote exploration (using uniform reference policy, known as entropy regularization). Beyond serving as a mere anchor, the reference policy can also be interpreted as encoding prior knowledge about good actions in the environment. In the context of alignment, recent game-theoretic approaches have leveraged KL regularization with pretrained language models as reference policies, achieving notable empirical success in self-play methods. Despite these advances, the theoretical benefits of KL regularization in game-theoretic settings remain poorly understood. In this work, we develop and analyze algorithms that provably achieve improved sample efficiency under KL regularization. We study both two-player zero-sum Matrix games and Markov games: for Matrix games, we propose OMG, an algorithm based on best response sampling with optimistic bonuses, and extend this idea to Markov games through the algorithm SOMG, which also uses best response sampling and a novel concept of superoptimistic bonuses. Both algorithms achieve a logarithmic regret in $T$ that scales inversely with the KL regularization strength $\beta$ in addition to the standard $\widetilde{\mathcal{O}}(\sqrt{T})$ regret independent of $\beta$ which is attained in both regularized and unregularized settings</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13060v1</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anupam Nayak, Tong Yang, Osman Yagan, Gauri Joshi, Yuejie Chi</dc:creator>
    </item>
    <item>
      <title>VPREG: An Optimal Control Formulation for Diffeomorphic Image Registration Based on the Variational Principle Grid Generation Method</title>
      <link>https://arxiv.org/abs/2510.13109</link>
      <description>arXiv:2510.13109v1 Announce Type: cross 
Abstract: This paper introduces VPreg, a novel diffeomorphic image registration method. This work provides several improvements to our past work on mesh generation and diffeomorphic image registration. VPreg aims to achieve excellent registration accuracy while controlling the quality of the registration transformations. It ensures a positive Jacobian determinant of the spatial transformation and provides an accurate approximation of the inverse of the registration, a crucial property for many neuroimaging workflows. Unlike conventional methods, VPreg generates this inverse transformation within the group of diffeomorphisms rather than operating on the image space. The core of VPreg is a grid generation approach, referred to as \emph{Variational Principle} (VP), which constructs non-folding grids with prescribed Jacobian determinant and curl. These VP-generated grids guarantee diffeomorphic spatial transformations essential for computational anatomy and morphometry, and provide a more accurate inverse than existing methods. To assess the potential of the proposed approach, we conduct a performance analysis for 150 registrations of brain scans from the OASIS-1 dataset. Performance evaluation based on Dice scores for 35 regions of interest, along with an empirical analysis of the properties of the computed spatial transformations, demonstrates that VPreg outperforms state-of-the-art methods in terms of Dice scores, regularity properties of the computed transformation, and accuracy and consistency of the provided inverse map. We compare our results to ANTs-SyN, Freesurfer-Easyreg, and FSL-Fnirt.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13109v1</guid>
      <category>cs.CV</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zicong Zhou, Baihan Zhao, Andreas Mang, Guojun Liao</dc:creator>
    </item>
    <item>
      <title>Convergence, design and training of continuous-time dropout as a random batch method</title>
      <link>https://arxiv.org/abs/2510.13134</link>
      <description>arXiv:2510.13134v1 Announce Type: cross 
Abstract: We study dropout regularization in continuous-time models through the lens of random-batch methods -- a family of stochastic sampling schemes originally devised to reduce the computational cost of interacting particle systems. We construct an unbiased, well-posed estimator that mimics dropout by sampling neuron batches over time intervals of length $h$. Trajectory-wise convergence is established with linear rate in $h$ for the expected uniform error. At the distribution level, we establish stability for the associated continuity equation, with total-variation error of order $h^{1/2}$ under mild moment assumptions. During training with fixed batch sampling across epochs, a Pontryagin-based adjoint analysis bounds deviations in the optimal cost and control, as well as in gradient-descent iterates. On the design side, we compare convergence rates for canonical batch sampling schemes, recover standard Bernoulli dropout as a special case, and derive a cost--accuracy trade-off yielding a closed-form optimal $h$. We then specialize to a single-layer neural ODE and validate the theory on classification and flow matching, observing the predicted rates, regularization effects, and favorable runtime and memory profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13134v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio \'Alvarez-L\'opez, Mart\'in Hern\'andez</dc:creator>
    </item>
    <item>
      <title>Stochastic optimal transport for the Langevin dynamics and its zero--mass limit</title>
      <link>https://arxiv.org/abs/2510.13238</link>
      <description>arXiv:2510.13238v1 Announce Type: cross 
Abstract: We introduce a stochastic optimal transport for the Langevin dynamics with positive mass and study its zero--mass limit. The new aspect of this paper is that we only fix the initial and terminal probability distributions of the positions of particles under consideration, but not those of their velocities with Heisenberg's uncertainty principle in mind. In the zero--mass limit, we show that the minimizer of our stochastic optimal transport is tight if and only if the initial momentum of a particle converges to zero. We also show that the limit of a minimizer of our stochastic optimal transport is a minimizer of a standard stochastic optimal transport for continuous semimartingales.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13238v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Toshio Mikami</dc:creator>
    </item>
    <item>
      <title>A Robust EDM Optimization Approach for 3D Single-Source Localization with Angle and Range Measurements</title>
      <link>https://arxiv.org/abs/2510.13498</link>
      <description>arXiv:2510.13498v1 Announce Type: cross 
Abstract: For the problem of source localization, three elements usually play a very important role in accurate localization. They are the range measurements, the angle measurements and the least absolute deviation criterion, which is regarded as a robust metric for denoising the measurements. Building the three elements into a computationally tractable model is challenging. In this paper, we introduce a robust Euclidean Distance Matrix (EDM) optimization model that simultaneously incorporates the three elements. For the first time, we show that for the case of 3D single-source localization (3DSSL), the angle measurements can be represented as a simple box constraint of distances. It is achieved by reducing each of the 3D angle measurements to a two-dimensional nonlinear optimization problem, whose global minimum and maximum solutions can be characterized and utilized to get the lower and upper bounds of the distances from the unknown source to the sensors. We further develop an efficient algorithm. The high quality of the localization by the new EDM model is assessed through extensive numerical experiments in comparison with leading solvers for 3DSSL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13498v1</guid>
      <category>eess.SP</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingyu Zhao, Qingna Li, Hou-Duo Qi</dc:creator>
    </item>
    <item>
      <title>Nash Flows Over Time with Tolls</title>
      <link>https://arxiv.org/abs/2510.13518</link>
      <description>arXiv:2510.13518v1 Announce Type: cross 
Abstract: We study a dynamic routing game motivated by traffic flows. The base model for an edge is the Vickrey bottleneck model. That is, edges are equipped with a free flow transit time and a capacity. When the inflow into an edge exceeds its capacity, a queue forms and the following particles experience a waiting time. In this paper, we enhance the model by introducing tolls, i.e., a cost each flow particle must pay for traversing an edge. In this setting we consider non-atomic equilibria, which means flows over time in which every particle is on a cheapest path, when summing up toll and travel time. We first show that unlike in the non-tolled version of this model, dynamic equilibria are not unique in terms of costs and do not necessarily reach a steady state. As a main result, we provide a procedure to compute steady states in the model with tolls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13518v1</guid>
      <category>cs.GT</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shaul Rosner, Marc Schr\"oder, Laura Vargas Koch</dc:creator>
    </item>
    <item>
      <title>What is the objective of reasoning with reinforcement learning?</title>
      <link>https://arxiv.org/abs/2510.13651</link>
      <description>arXiv:2510.13651v1 Announce Type: cross 
Abstract: We show that several popular algorithms for reinforcement learning in large language models with binary rewards can be viewed as stochastic gradient ascent on a monotone transform of the probability of a correct answer given a prompt. In particular, the transformation associated with rejection sampling algorithms is the logarithm and that associated with the GRPO algorithm is the arcsine of the square root.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13651v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Damek Davis, Benjamin Recht</dc:creator>
    </item>
    <item>
      <title>Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe</title>
      <link>https://arxiv.org/abs/2510.13713</link>
      <description>arXiv:2510.13713v1 Announce Type: cross 
Abstract: Pruning is a common technique to reduce the compute and storage requirements of Neural Networks. While conventional approaches typically retrain the model to recover pruning-induced performance degradation, state-of-the-art Large Language Model (LLM) pruning methods operate layer-wise, minimizing the per-layer pruning error on a small calibration dataset to avoid full retraining, which is considered computationally prohibitive for LLMs. However, finding the optimal pruning mask is a hard combinatorial problem and solving it to optimality is intractable. Existing methods hence rely on greedy heuristics that ignore the weight interactions in the pruning objective. In this work, we instead consider the convex relaxation of these combinatorial constraints and solve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method drastically reduces the per-layer pruning error, outperforms strong baselines on state-of-the-art GPT architectures, and remains memory-efficient. We provide theoretical justification by showing that, combined with the convergence guarantees of the FW algorithm, we obtain an approximate solution to the original combinatorial problem upon rounding the relaxed solution to integrality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13713v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christophe Roux, Max Zimmer, Alexandre d'Aspremont, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>An Adaptive Parameter-free and Projection-free Restarting Level Set Method for Constrained Convex Optimization Under the Error Bound Condition</title>
      <link>https://arxiv.org/abs/2010.15267</link>
      <description>arXiv:2010.15267v3 Announce Type: replace 
Abstract: Recent efforts to accelerate first-order methods have focused on convex optimization problems that satisfy a geometric property known as error-bound condition, which covers a broad class of problems, including piece-wise linear programs and strongly convex programs. Parameter-free first-order methods that employ projection-free updates have the potential to broaden the benefit of acceleration. Such a method has been developed for unconstrained convex optimization but is lacking for general constrained convex optimization. We propose a parameter-free level-set method for the latter constrained case based on projection-free subgradient method that exhibits accelerated convergence for problems that satisfy an error-bound condition. Our method maintains a separate copy of the level-set sub-problem for each level parameter value and restarts the computation of these copies based on objective function progress. Applying such a restarting scheme in a level-set context is novel and results in an algorithm that dynamically adapts the precision of each copy. This property is key to extending prior restarting methods based on static precision that have been proposed for unconstrained convex optimization to handle constraints. We report promising numerical performance relative to benchmark methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.15267v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qihang Lin, Negar Soheili, Runchao Ma, Selvaprabu Nadarajah</dc:creator>
    </item>
    <item>
      <title>Symplectic model order reduction of port-Hamiltonian systems</title>
      <link>https://arxiv.org/abs/2203.07751</link>
      <description>arXiv:2203.07751v2 Announce Type: replace 
Abstract: This work proposes a novel structure-preserving model order reduction (MOR) method for linear, time-invariant port-Hamiltonian (pH) systems. Our goal is to construct a reduced order pH system, which can still be interpreted in the physical domain of the full order model. By this we mean, that if an electrical circuit is the initial high-dimensional pH system, we want the reduced order model to be still interpretable as an electronic circuit. In the case of the well-known mass spring damper (MSD) system, there are MOR methods available, which already guarantee the preservation of this particular structure. Moreover, we show that our new structure-preserving MOR method, which is based on symplectic MOR methods, will recover the known second-order Arnoldi method in the case of MSD systems. However, for the example of an electrical circuit pH model (and more models of similar block structure), our method yields a novel model reduction method. We present numerical results on the aforementioned electronic circuit model, highlighting the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.07751v2</guid>
      <category>math.OC</category>
      <category>math.CA</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Silke Glas, Mir Mamunuzzaman, Hongliang Mu, Hans Zwart</dc:creator>
    </item>
    <item>
      <title>Convergence and sample complexity of natural policy gradient primal-dual methods for constrained MDPs</title>
      <link>https://arxiv.org/abs/2206.02346</link>
      <description>arXiv:2206.02346v4 Announce Type: replace 
Abstract: We study the sequential decision making problem of maximizing the expected total reward while satisfying a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon optimal control problem for Constrained Markov Decision Processes (constrained MDPs). Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) method that updates the primal variable via natural policy gradient ascent and the dual variable via projected subgradient descent. Although the underlying maximization involves a nonconcave objective function and a nonconvex constraint set, under the softmax policy parametrization, we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. Such convergence is independent of the size of the state-action space, i.e., it is~dimension-free. Furthermore, for log-linear and general smooth policy parametrizations, we establish sublinear convergence rates up to a function approximation error caused by restricted policy parametrization. We also provide convergence and finite-sample complexity guarantees for two sample-based NPG-PD algorithms. We use a set of computational experiments to showcase the effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.02346v4</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dongsheng Ding, Kaiqing Zhang, Jiali Duan, Tamer Ba\c{s}ar, Mihailo R. Jovanovi\'c</dc:creator>
    </item>
    <item>
      <title>Nash Equilibria, Regularization and Computation in Optimal Transport-Based Distributionally Robust Optimization</title>
      <link>https://arxiv.org/abs/2303.03900</link>
      <description>arXiv:2303.03900v4 Announce Type: replace 
Abstract: We study optimal transport-based distributionally robust optimization problems where a fictitious adversary, often envisioned as nature, can choose the distribution of the uncertain problem parameters by reshaping a prescribed reference distribution at a finite transportation cost. In this framework, we show that robustification is intimately related to various forms of variation and Lipschitz regularization even if the transportation cost function fails to be (some power of) a metric. We also derive conditions for the existence and the computability of a Nash equilibrium between the decision-maker and nature, and we demonstrate numerically that nature's Nash strategy can be viewed as a distribution that is supported on remarkably deceptive adversarial samples. Finally, we identify practically relevant classes of optimal transport-based distributionally robust optimization problems that can be addressed with efficient gradient descent algorithms even if the loss function or the transportation cost function are nonconvex (but not both at the same time).</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.03900v4</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soroosh Shafiee, Liviu Aolaritei, Florian D\"orfler, Daniel Kuhn</dc:creator>
    </item>
    <item>
      <title>An extended Merton problem with relaxed benchmark tracking</title>
      <link>https://arxiv.org/abs/2304.10802</link>
      <description>arXiv:2304.10802v5 Announce Type: replace 
Abstract: This paper studies Merton's problem in an extended formulation by incorporating the benchmark tracking on the wealth process. We consider a tracking formulation where the fund manager aims to maximize the trade-off between the expected utility of consumption and the expected largest shortfall of the wealth with reference to the benchmark level. Equivalently, the problem can be interpreted as a mixed stochastic control problem if a fictitious capital injection singular control is allowed, subjecting to the dynamic constraint that the wealth process compensated by the costly capital injection outperforms the benchmark at all times. By considering an auxiliary state process, we formulate an equivalent stochastic control problem with state reflections at zero. For general utility functions and Ito's diffusion benchmark process, we develop a convex duality theorem, new to the literature, to the auxiliary stochastic control problem with state reflections in which the dual process also exhibits reflections from above. For CRRA utility and geometric Brownian motion benchmark process, we further derive the optimal portfolio and consumption in feedback form using the new duality theorem, allowing us to discuss some interesting financial implications induced by the additional risk-taking from the capital injection and the goal of tracking.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.10802v5</guid>
      <category>math.OC</category>
      <category>q-fin.PM</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lijun Bo, Yijie Huang, Xiang Yu</dc:creator>
    </item>
    <item>
      <title>Convergence Rates of the Regularized Optimal Transport : Disentangling Suboptimality and Entropy</title>
      <link>https://arxiv.org/abs/2306.06940</link>
      <description>arXiv:2306.06940v2 Announce Type: replace 
Abstract: We study the convergence of the transport plans $\gamma_\epsilon$ towards $\gamma_0$ as well as the cost of the entropy-regularized optimal transport $(c,\gamma_\epsilon)$ towards $(c,\gamma_0)$ as the regularization parameter $\epsilon$ vanishes in the setting of finite entropy marginals. We show that under the assumption of infinitesimally twisted cost and compactly supported marginals the distance $W_2(\gamma_\epsilon,\gamma_0)$ is asymptotically greater than $C\sqrt{\epsilon}$ and the suboptimality $(c,\gamma_\epsilon)-(c,\gamma_0)$ is of order $\epsilon$. In the quadratic cost case the compactness assumption is relaxed into a moment of order $2+\delta$ assumption. Moreover, in the case of a Lipschitz transport map for the non-regularized problem, the distance $W_2(\gamma_\epsilon,\gamma_0)$ converges to $0$ at rate $\sqrt{\epsilon}$. Finally, if in addition the marginals have finite Fisher information, we prove $(c,\gamma_\epsilon)-(c,\gamma_0) \sim d\epsilon/2$ and we provide a companion expansion of $H(\gamma_\epsilon)$. These results are achieved by disentangling the role of the cost and the entropy in the regularized problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06940v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hugo Malamut (CEREMADE), Maxime Sylvestre (CEREMADE)</dc:creator>
    </item>
    <item>
      <title>Partial State-Feedback Reduced-Order Switching Predictive Models for Next-Generation Optical Lithography Systems</title>
      <link>https://arxiv.org/abs/2402.13928</link>
      <description>arXiv:2402.13928v4 Announce Type: replace 
Abstract: This paper presents a partial state-feedback reduced-order switching predictive model designed to support the next-generation lithography roadmap. The proposed approach addresses the trade-off between increasing the number of measurements to improve overlay accuracy and the resulting challenges, including higher measurement noise, reduced throughput and overlay/placement errors under uncertain operating conditions.. By minimizing (die-) placement errors and reducing unnecessary measurements, the method enhances system performance and throughput. This solution employs a streamlined model with adaptive switching logic to manage time-varying uncertainties induced by fluctuating operating conditions. The methodology is implemented on a state-of-the-art lithographic scanner to mitigate the spatial-temporal dynamics of reticle heating, serving as a representative industrial application. Reticle heating, which worsens with increased throughput, introduces spatial-temporal distortions that directly degrade die placement accuracy. Experimental results demonstrate significant improvements: placement errors are reduced by a factor of $2-3$x, and throughput is improved by $0.3$seconds per wafer. Importantly, the method accounts for the fact that increased throughput can exacerbate reticle heating, which directly impacts overlay performance. By actively compensating for these thermomechanical effects, the proposed approach ensures that overlay accuracy is maintained or improved -- even under increased throughput conditions -- highlighting its potential for broader application in advanced lithographic systems, particularly in thermal and vibration control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13928v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Raaja Ganapathy Subramanian, Barry Moest, Bart Paarhuis</dc:creator>
    </item>
    <item>
      <title>A tactical time slot management problem under mixed logit demand</title>
      <link>https://arxiv.org/abs/2407.02308</link>
      <description>arXiv:2407.02308v5 Announce Type: replace 
Abstract: We study the tactical time slot management problem under mixed logit demand for attended home delivery in subscription settings. We propose a static mixed-integer linear programming model that integrates delivery slot assortment, price discount decisions, and routing optimization while capturing customer heterogeneity through the mixed logit model. To overcome the computational challenges posed by simulation-based choice probabilities, we develop a simulation-based Adaptive Large Neighborhood Search method aligned with a Sample Average Approximation reformulation. Computational experiments on large-scale instances demonstrate the effectiveness of our approach in capturing stochastic customer behavior and preference heterogeneity, providing a scalable and flexible method for optimizing time slot management under complex demand structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02308v5</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dorsa Abdolhamidi, Virginie Lurkin</dc:creator>
    </item>
    <item>
      <title>Nesterov's Accelerated Jacobi-Type Methods for Large-scale Symmetric Positive Semidefinite Linear Systems</title>
      <link>https://arxiv.org/abs/2407.03272</link>
      <description>arXiv:2407.03272v2 Announce Type: replace 
Abstract: Solving symmetric positive semidefinite linear systems is an essential task in many scientific computing problems. While Jacobi-type methods, including the classical Jacobi method and the weighted Jacobi method, exhibit simplicity in their forms and friendliness to parallelization, they are not attractive either because of the potential convergence failure or their slow convergence rate. This paper aims to showcase the possibility of improving classical Jacobi-type methods by employing Nesterov's acceleration technique that results in an accelerated Jacobi-type method with improved convergence properties. Simultaneously, it preserves the appealing features for parallel implementation. In particular, we show that the proposed method has an $O\left(t^{-2}\right)$ convergence rate in terms of objective function values of the associated convex quadratic optimization problem, where $t\geq 1$ denotes the iteration counter. To further improve the practical performance of the proposed method, we also develop and analyze a restarted variant of the method, which is shown to have an $O\left((\log_2(t))^2t^{-2}\right)$ convergence rate when the coefficient matrix is positive definite. Furthermore, we conduct appropriate numerical experiments to evaluate the efficiency of the proposed method. Our numerical results demonstrate that the proposed method outperforms the classical Jacobi-type methods and the conjugate gradient method, and shows a comparable performance as the preconditioned conjugate gradient method with a diagonal preconditioner. Finally, we develop a parallel implementation and conduct speed-up tests on some large-scale systems. Our results indicate that the proposed framework is highly scalable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03272v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Liang, Qiyuan Pang, Kim-Chuan Toh, Haizhao Yang</dc:creator>
    </item>
    <item>
      <title>NewVEM: A Newton Vertex Exchange Method for a Class of Constrained Self-Concordant Minimization Problems</title>
      <link>https://arxiv.org/abs/2407.03294</link>
      <description>arXiv:2407.03294v2 Announce Type: replace 
Abstract: We propose \textbf{NewVEM}, a Newton vertex exchange method for efficiently solving self-concordant minimization problems under generalized simplex constraints. The algorithm features a two-level structure: the outer loop employs a projected Newton method, and the inner loop uses a vertex exchange approach to solve strongly convex quadratic subproblems. Both loops converge locally at linear rates under technical conditions, resulting in a ``fast $\times$ fast'' framework that demonstrates high efficiency and scalability in practice. To get a feasible initial point to execute the algorithm, we also present and analyze a highly efficient semismooth Newton method for computing the projection onto the generalized simplex. The excellent practical performance of the proposed algorithms is demonstrated by a set of numerical experiments. Our results further motivate the potential real-world applications of the considered model and the proposed algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03294v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Liang, Kim-Chuan Toh, Haizhao Yang</dc:creator>
    </item>
    <item>
      <title>Some Unified Theory for Variance Reduced Prox-Linear Methods</title>
      <link>https://arxiv.org/abs/2412.15008</link>
      <description>arXiv:2412.15008v2 Announce Type: replace 
Abstract: This work considers the nonconvex, nonsmooth problem of minimizing a composite objective of the form $f(g(x))+h(x)$ where the inner mapping $g$ is a smooth finite summation or expectation amenable to variance reduction. In such settings, prox-linear methods can enjoy variance-reduced speed-ups despite the existence of nonsmoothness. We provide a unified convergence theory applicable to a wide range of common variance-reduced vector and Jacobian constructions. All the technical conditions we required for variance-reduced methods can be summarized in a single unified assumption. Our theory (i) only requires operator norm bounds on Jacobians (whereas prior works used potentially much larger Frobenius norms), (ii) provides state-of-the-art high probability guarantees, and (iii) allows inexactness in proximal computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15008v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yue Wu, Benjamin Grimmer</dc:creator>
    </item>
    <item>
      <title>An Efficient Quadratic Penalty Method for a Class of Graph Clustering Problems</title>
      <link>https://arxiv.org/abs/2501.02187</link>
      <description>arXiv:2501.02187v2 Announce Type: replace 
Abstract: Community-based graph clustering is one of the most popular topics in the analysis of complex social networks. This type of clustering involves grouping vertices that are considered to share more connections, whereas vertices in different groups share fewer connections. A successful clustering result forms densely connected induced subgraphs. This paper studies a specific form of graph clustering problems that can be formulated as semi-assignment problems, where the objective function exhibits block properties. We reformulate these problems as sparse-constrained optimization problems and relax them to continuous optimization models. We then apply the quadratic penalty method and the quadratic penalty regularized method to the relaxation problem, respectively. Extensive numerical experiments demonstrate that both methods effectively solve graph clustering tasks for both synthetic and real-world network datasets. For small-scale problems, the quadratic penalty regularized method demonstrates greater efficiency, whereas the quadratic penalty method proves more suitable for large-scale cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.02187v2</guid>
      <category>math.OC</category>
      <category>cs.SI</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenshun Teng, Qingna Li</dc:creator>
    </item>
    <item>
      <title>Yet Another Distributional Bellman Equation</title>
      <link>https://arxiv.org/abs/2505.21098</link>
      <description>arXiv:2505.21098v2 Announce Type: replace 
Abstract: We consider non-standard Markov Decision Processes (MDPs) where the target function is not only a simple expectation of the accumulated reward. Instead, we consider rather general functionals of the joint distribution of terminal state and accumulated reward which have to be optimized. For finite state and compact action space, we show how to solve these problems by defining a lifted MDP whose state space is the space of distributions over the true states of the process. We derive a Bellman equation in this setting, which can be considered as a distributional Bellman equation. Well-known cases like the standard MDP and quantile MDPs are shown to be special examples of our framework. We also apply our model to a variant of an optimal transport problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21098v2</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicole B\"auerle, Tamara G\"oll, Anna Ja\'skiewicz</dc:creator>
    </item>
    <item>
      <title>Non-convex entropic mean-field optimization via Best Response flow</title>
      <link>https://arxiv.org/abs/2505.22760</link>
      <description>arXiv:2505.22760v2 Announce Type: replace 
Abstract: We study the problem of minimizing non-convex functionals on the space of probability measures, regularized by the relative entropy (KL divergence) with respect to a fixed reference measure, as well as the corresponding problem of solving entropy-regularized non-convex-non-concave min-max problems. We utilize the Best Response flow (also known in the literature as the fictitious play flow) and study how its convergence is influenced by the relation between the degree of non-convexity of the functional under consideration, the regularization parameter and the tail behaviour of the reference measure. In particular, we demonstrate how to choose the regularizer, given the non-convex functional, so that the Best Response operator becomes a contraction with respect to the $L^1$-Wasserstein distance, which ensures the existence of its unique fixed point that is then shown to be the unique global minimizer for our optimization problem. This extends recent results where the Best Response flow was applied to solve convex optimization problems regularized by the relative entropy with respect to arbitrary reference measures, and with arbitrary values of the regularization parameter. Our results explain precisely how the assumption of convexity can be relaxed, at the expense of making a specific choice of the regularizer. Additionally, we demonstrate how these results can be applied in reinforcement learning in the context of policy optimization for Markov Decision Processes and Markov games with softmax parametrized policies in the mean-field regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.22760v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Razvan-Andrei Lascu, Mateusz B. Majka</dc:creator>
    </item>
    <item>
      <title>Relative Explanations for Contextual Problems with Endogenous Uncertainty: An Application to Competitive Facility Location</title>
      <link>https://arxiv.org/abs/2506.19155</link>
      <description>arXiv:2506.19155v3 Announce Type: replace 
Abstract: In this paper, we consider contextual stochastic optimization problems under endogenous uncertainty, where decisions affect the underlying distributions. To implement such decisions in practice, it is crucial to ensure that their outcomes are interpretable and trustworthy. To this end, we compute relative counterfactual explanations that provide practitioners with concrete changes in the contextual covariates required for a solution to satisfy specific constraints. Whereas relative explanations have been introduced in prior literature, to the best of our knowledge this is the first work focusing on problems with binary decision variables and endogenous uncertainty. We propose a methodology that uses the Wasserstein distance as a regularization term, which leads to a reduction in computation times compared to its unregularized counterpart. We illustrate the method using a choice-based competitive facility location problem and present numerical experiments that demonstrate its ability to efficiently compute sparse and interpretable explanations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19155v3</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jasone Ram\'irez-Ayerbe, Emma Frejinger</dc:creator>
    </item>
    <item>
      <title>Benamou-Brenier and Kantorovich on sub-Riemannian manifolds with no abnormal geodesics</title>
      <link>https://arxiv.org/abs/2507.20959</link>
      <description>arXiv:2507.20959v2 Announce Type: replace 
Abstract: We prove that the Benamou-Brenier formulation of the Optimal Transport problem and the Kantorovich formulation are equivalent on a sub-Riemannian connected and complete manifold $M$ without boundary and with no non-trivial abnormal geodesics, when the problems are considered between two measures with finite $2$-momentum. Furthermore, we prove the existence of a minimizer for the Benamou-Brenier formulation and link it to the optimal transport plan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.20959v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanna Citti, Mattia Galeotti, Andrea Pinamonti</dc:creator>
    </item>
    <item>
      <title>Sparse Polyak: an adaptive step size rule for high-dimensional M-estimation</title>
      <link>https://arxiv.org/abs/2509.09802</link>
      <description>arXiv:2509.09802v2 Announce Type: replace 
Abstract: We propose and study Sparse Polyak, a variant of Polyak's adaptive step size, designed to solve high-dimensional statistical estimation problems where the problem dimension is allowed to grow much faster than the sample size. In such settings, the standard Polyak step size performs poorly, requiring an increasing number of iterations to achieve optimal statistical precision-even when, the problem remains well conditioned and/or the achievable precision itself does not degrade with problem size. We trace this limitation to a mismatch in how smoothness is measured: in high dimensions, it is no longer effective to estimate the Lipschitz smoothness constant. Instead, it is more appropriate to estimate the smoothness restricted to specific directions relevant to the problem (restricted Lipschitz smoothness constant). Sparse Polyak overcomes this issue by modifying the step size to estimate the restricted Lipschitz smoothness constant. We support our approach with both theoretical analysis and numerical experiments, demonstrating its improved performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09802v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianqi Qiao, Marie Maros</dc:creator>
    </item>
    <item>
      <title>Dual-Regularized Riccati Recursions for Interior-Point Optimal Control</title>
      <link>https://arxiv.org/abs/2509.16370</link>
      <description>arXiv:2509.16370v2 Announce Type: replace 
Abstract: We derive closed-form extensions of Riccati's recursions (both sequential and parallel) for solving dual-regularized LQR problems. We show how these methods can be used to solve general constrained, non-convex, discrete-time optimal control problems via a regularized interior point method, while guaranteeing that each step is a descent direction of an Augmented Barrier-Lagrangian merit function. We provide MIT-licensed implementations of our methods in C++ and JAX.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.16370v2</guid>
      <category>math.OC</category>
      <category>cs.MS</category>
      <category>cs.RO</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jo\~ao Sousa-Pinto, Dominique Orban</dc:creator>
    </item>
    <item>
      <title>The value of storage in electricity distribution: The role of markets</title>
      <link>https://arxiv.org/abs/2510.12435</link>
      <description>arXiv:2510.12435v2 Announce Type: replace 
Abstract: Electricity distribution companies deploy battery storage to defer grid upgrades by reducing peak demand. In deregulated jurisdictions, such storage often sits idle because regulatory constraints bar participation in electricity markets. Here, we develop an optimization framework that, to our knowledge, provides the first formal model of market participation constraints within storage investment and operation planning. Applying the framework to a Massachusetts case study, we find that market participation could deliver similar savings as peak demand reduction. Under current conditions, market participation does not increase storage investment, but at very low storage costs, could incentivize deployment beyond local distribution needs. This might run contrary to the separation of distribution from generation in deregulated markets. Our framework can identify investment levels appropriate for local distribution needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12435v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>econ.GN</category>
      <category>eess.SY</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dirk Lauinger, Deepjyoti Deka, Sungho Shin</dc:creator>
    </item>
    <item>
      <title>Parallel splitting method for large-scale quadratic programs</title>
      <link>https://arxiv.org/abs/2503.16977</link>
      <description>arXiv:2503.16977v2 Announce Type: replace-cross 
Abstract: Current algorithms for large-scale industrial optimization problems typically face a trade-off: they either require exponential time to reach optimal solutions, or employ problem-specific heuristics. To overcome these limitations, we introduce SPLIT, a general-purpose quantum-inspired framework for decomposing large-scale quadratic programs into smaller subproblems, which are then solved in parallel. SPLIT accounts for cross-interactions between subproblems, which are usually neglected in other decomposition techniques. The SPLIT framework can integrate generic subproblem solvers, ranging from standard branch-and-bound methods to quantum optimization algorithms. We demonstrate its effectiveness through comparisons with commercial solvers on the MaxCut and Antenna Placement Problems, with up to 20,000 decision variables. Our results show that SPLIT is capable of providing drastic reductions in computational time, while delivering high-quality solutions. In these regards, the proposed method is particularly suited for near real-time applications that require a solution within a strict time frame, or when the problem size exceeds the hardware limitations of dedicated devices, such as current quantum computers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16977v2</guid>
      <category>quant-ph</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Vandelli, Francesco Ferrari, Daniele Dragoni</dc:creator>
    </item>
    <item>
      <title>The frequency $K_i$s for symmetrical traveling salesman problem</title>
      <link>https://arxiv.org/abs/2504.19608</link>
      <description>arXiv:2504.19608v3 Announce Type: replace-cross 
Abstract: The frequency $K_i$s ($i\in[4,n]$) are studied for symmetrical traveling salesman problem ($TSP$) to identify the edges in optimal Hamiltonian cycle ($OHC$). A frequency $K_i$ is computed with a sort of ${{i}\choose{2}}$ optimal $i$-vertex paths with given endpoints (optimal $i$-vertex path) in a corresponding $K_i$ in $K_n$. In frequency $K_i$, the frequency of an edge is the number of the optimal $i$-vertex paths containing the edge in the corresponding $K_i$. Given an $OHC$ edge related to $K_i$, it has a frequency bigger than $\frac{1}{2}{{i}\choose{2}}$ in the corresponding frequency $K_i$, and that of an ordinary edge not in $OHC$ is smaller than $\frac{1}{2}{{i}\choose{2}}$. On average, an $OHC$ edge in $K_i$ has the expected frequency bigger than $\frac{i^2-4i+7}{2}$ whereas an ordinary edge has the expected frequency smaller than 2. Moreover, given a frequency $K_i$ containing an $OHC$ edge related to $K_n$, the frequency of the $OHC$ edge is bigger than $\frac{1}{2}{{i}\choose{2}}$ in the worst average case. It implies that the average frequency of an $OHC$ edge computed with frequency $K_i$s is bigger than $\frac{1}{2}{{i}\choose{2}}$. It also found that the probability that an $OHC$ edge is contained in optimal $i$-vertex paths keeps stable or increases according to $i\in [4, n]$. As the frequency $K_i$s are used to compute the frequency of an edge, each $OHC$ edge has its own peak frequency at $i=P_0$ where $P_0=\frac{n}{2} + 2$ for even $n$ or $\frac{n+1}{2} + 1$ for odd $n$. For ordinary edges out of $OHC$, the probability that they are contained in optimal $i$-vertex paths decreases according to $i$. Moreover, the average frequency of an ordinary edge will be smaller than $\frac{1}{2}{{i}\choose{2}}$ if $i \geq [0.3660n + 5.5849]$. Based on these findings, an algorithm is presented to find $OHC$ in $O(n^62^{0.3660n})$ time using dynamic programming.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19608v3</guid>
      <category>cs.DM</category>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yong Wang</dc:creator>
    </item>
    <item>
      <title>Isogeometric Topology Optimization Based on Topological Derivatives</title>
      <link>https://arxiv.org/abs/2509.09236</link>
      <description>arXiv:2509.09236v3 Announce Type: replace-cross 
Abstract: Topology optimization is a valuable tool in engineering, facilitating the design of optimized structures. However, topological changes often require a remeshing step, which can become challenging. In this work, we propose an isogeometric approach to topology optimization driven by topological derivatives. The combination of a level-set method together with an immersed isogeometric framework allows seamless geometry updates without the necessity of remeshing. At the same time, topological derivatives provide topological modifications without the need to define initial holes [7]. We investigate the influence of higher-degree basis functions in both the level-set representation and the approximation of the solution. Two numerical examples demonstrate the proposed approach, showing that employing higher-degree basis functions for approximating the solution improves accuracy, while linear basis functions remain sufficient for the level-set function representation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09236v3</guid>
      <category>math.NA</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guilherme Henrique Teixeira, Nepomuk Krenn, Peter Gangl, Benjamin Marussig</dc:creator>
    </item>
    <item>
      <title>Forward Euler for Wasserstein Gradient Flows: Breakdown and Regularization</title>
      <link>https://arxiv.org/abs/2509.13260</link>
      <description>arXiv:2509.13260v2 Announce Type: replace-cross 
Abstract: Wasserstein gradient flows have become a central tool for optimization problems over probability measures. A natural numerical approach is forward-Euler time discretization. We show, however, that even in the simple case where the energy functional is the Kullback-Leibler (KL) divergence against a smooth target density, forward-Euler can fail dramatically: the scheme does not converge to the gradient flow, despite the fact that the first variation $\nabla\frac{\delta F}{\delta\rho}$ remains formally well defined at every step. We identify the root cause as a loss of regularity induced by the discretization, and prove that a suitable regularization of the functional restores the necessary smoothness, making forward-Euler a viable solver that converges in discrete time to the global minimizer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13260v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yewei Xu, Qin Li</dc:creator>
    </item>
    <item>
      <title>Machine Unlearning Meets Adversarial Robustness via Constrained Interventions on LLMs</title>
      <link>https://arxiv.org/abs/2510.03567</link>
      <description>arXiv:2510.03567v2 Announce Type: replace-cross 
Abstract: With the increasing adoption of Large Language Models (LLMs), more customization is needed to ensure privacy-preserving and safe generation. We address this objective from two critical aspects: unlearning of sensitive information and robustness to jail-breaking attacks. We investigate various constrained optimization formulations that address both aspects in a \emph{unified manner}, by finding the smallest possible interventions on LLM weights that either make a given vocabulary set unreachable or embed the LLM with robustness to tailored attacks by shifting part of the weights to a \emph{safer} region. Beyond unifying two key properties, this approach contrasts with previous work in that it doesn't require an oracle classifier that is typically not available or represents a computational overhead. Surprisingly, we find that the simplest point-wise constraint-based intervention we propose leads to better performance than max-min interventions, while having a lower computational cost. Comparison against state-of-the-art defense methods demonstrates superior performance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03567v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Thu, 16 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fatmazohra Rezkellah, Ramzi Dakhmouche</dc:creator>
    </item>
  </channel>
</rss>
