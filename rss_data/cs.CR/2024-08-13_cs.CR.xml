<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CR</link>
    <description>cs.CR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 13 Aug 2024 04:00:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 13 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Comment on "An Efficient Privacy-Preserving Ranked Multi-Keyword Retrieval for Multiple Data Owners in Outsourced Cloud"</title>
      <link>https://arxiv.org/abs/2408.05218</link>
      <description>arXiv:2408.05218v1 Announce Type: new 
Abstract: Protecting the privacy of keywords in the field of search over outsourced cloud data is a challenging task. In IEEE Transactions on Services Computing (Vol. 17 No. 2, March/April 2024), Li et al. proposed PRMKR: efficient privacy-preserving ranked multi-keyword retrieval scheme, which was claimed to resist keyword guessing attack. However, we show that the scheme fails to resist keyword guessing attack, index privacy, and trapdoor privacy. Further, we propose a solution to address the above said issues by correcting the errors in the important equations of the scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05218v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uma Sankararao Varri</dc:creator>
    </item>
    <item>
      <title>LightPHE: Integrating Partially Homomorphic Encryption into Python with Extensive Cloud Environment Evaluations</title>
      <link>https://arxiv.org/abs/2408.05219</link>
      <description>arXiv:2408.05219v1 Announce Type: new 
Abstract: Homomorphic encryption enables computations on encrypted data without accessing private keys, enhancing security in cloud environments. Without this technology, updates need to be performed on-premises or require transmitting private keys to the cloud, increasing security risks. Fully homomorphic encryption (FHE) supports both additive and multiplicative operations on ciphertexts, while partially homomorphic encryption (PHE) supports either addition or multiplication, offering a more efficient and practical solution.
  This study introduces LightPHE, a lightweight hybrid PHE framework for Python, designed to address the lack of existing PHE libraries. LightPHE integrates multiple PHE algorithms with a modular and extensible design, ensuring robustness and usability for rapid prototyping and secure application development.
  Cloud-based experiments were conducted on Google Colab (Normal, A100 GPU, L4 GPU, T4 High RAM, TPU2) and Microsoft Azure Spark to evaluate LightPHE's performance and scalability. Key metrics such as key generation, encryption, decryption, and homomorphic operations were assessed. Results showed LightPHE's superior performance in high-computation environments like Colab A100 GPU and TPU2, while also offering viable options for cost-effective setups like Colab Normal and Azure Spark.
  Comparative analyses demonstrated LightPHE's efficiency and scalability, making it suitable for various applications. The benchmarks offer insights into selecting appropriate cloud environments based on performance needs, highlighting LightPHE's potential to advance homomorphic encryption for secure and efficient cloud-based data processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05219v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sefik Ilkin Serengil, Alper Ozpinar</dc:creator>
    </item>
    <item>
      <title>Zero-day attack and ransomware detection</title>
      <link>https://arxiv.org/abs/2408.05244</link>
      <description>arXiv:2408.05244v1 Announce Type: new 
Abstract: Zero-day and ransomware attacks continue to challenge traditional Network Intrusion Detection Systems (NIDS), revealing their limitations in timely threat classification. Despite efforts to reduce false positives and negatives, significant attacks persist, highlighting the need for advanced solutions. Machine Learning (ML) models show promise in enhancing NIDS. This study uses the UGRansome dataset to train various ML models for zero-day and ransomware attacks detection. The finding demonstrates that Random Forest Classifier (RFC), XGBoost, and Ensemble Methods achieved perfect scores in accuracy, precision, recall, and F1-score. In contrast, Support Vector Machine (SVM) and Naive Bayes (NB) models performed poorly. Comparison with other studies shows Decision Trees and Ensemble Methods improvements, with accuracy around 99.4% and 97.7%, respectively. Future research should explore Synthetic Minority Over-sampling Techniques (SMOTEs) and diverse or versatile datasets to improve real-time recognition of zero-day and ransomware attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05244v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Steven Jabulani Nhlapo, Mike Nkongolo Wa Nkongolo</dc:creator>
    </item>
    <item>
      <title>Differentially Private Data Release on Graphs: Inefficiencies and Unfairness</title>
      <link>https://arxiv.org/abs/2408.05246</link>
      <description>arXiv:2408.05246v1 Announce Type: new 
Abstract: Networks are crucial components of many sectors, including telecommunications, healthcare, finance, energy, and transportation.The information carried in such networks often contains sensitive user data, like location data for commuters and packet data for online users. Therefore, when considering data release for networks, one must ensure that data release mechanisms do not leak information about individuals, quantified in a precise mathematical sense. Differential Privacy (DP) is the widely accepted, formal, state-of-the-art technique, which has found use in a variety of real-life settings including the 2020 U.S. Census, Apple users' device data, or Google's location data. Yet, the use of DP comes with new challenges, as the noise added for privacy introduces inaccuracies or biases and further, DP techniques can also distribute these biases disproportionately across different populations, inducing fairness issues. The goal of this paper is to characterize the impact of DP on bias and unfairness in the context of releasing information about networks, taking a departure from previous work which has studied these effects in the context of private population counts release (such as in the U.S. Census). To this end, we consider a network release problem where the network structure is known to all, but the weights on edges must be released privately. We consider the impact of this private release on a simple downstream decision-making task run by a third-party, which is to find the shortest path between any two pairs of nodes and recommend the best route to users. This setting is of highly practical relevance, mirroring scenarios in transportation networks, where preserving privacy while providing accurate routing information is crucial. Our work provides theoretical foundations and empirical evidence into the bias and unfairness arising due to privacy in these networked decision problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05246v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ferdinando Fioretto, Diptangshu Sen, Juba Ziani</dc:creator>
    </item>
    <item>
      <title>The Role and Applications of Airport Digital Twin in Cyberattack Protection during the Generative AI Era</title>
      <link>https://arxiv.org/abs/2408.05248</link>
      <description>arXiv:2408.05248v1 Announce Type: new 
Abstract: In recent years, the threat facing airports from growing and increasingly sophisticated cyberattacks has become evident. Airports are considered a strategic national asset, so protecting them from attacks, specifically cyberattacks, is a crucial mission. One way to increase airports' security is by using Digital Twins (DTs). This paper shows and demonstrates how DTs can enhance the security mission. The integration of DTs with Generative AI (GenAI) algorithms can lead to synergy and new frontiers in fighting cyberattacks. The paper exemplifies ways to model cyberattack scenarios using simulations and generate synthetic data for testing defenses. It also discusses how DTs can be used as a crucial tool for vulnerability assessment by identifying weaknesses, prioritizing, and accelerating remediations in case of cyberattacks. Moreover, the paper demonstrates approaches for anomaly detection and threat hunting using Machine Learning (ML) and GenAI algorithms. Additionally, the paper provides impact prediction and recovery coordination methods that can be used by DT operators and stakeholders. It also introduces ways to harness the human factor by integrating training and simulation algorithms with Explainable AI (XAI) into the DT platforms. Lastly, the paper offers future applications and technologies that can be utilized in DT environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05248v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Itzhak Weinberg</dc:creator>
    </item>
    <item>
      <title>Monero Traceability Heuristics: Wallet Application Bugs and the Mordinal-P2Pool Perspective</title>
      <link>https://arxiv.org/abs/2408.05332</link>
      <description>arXiv:2408.05332v1 Announce Type: new 
Abstract: Privacy-focused cryptoassets like Monero are intentionally difficult to trace. Over the years, several traceability heuristics have been proposed, most of which have been rendered ineffective with subsequent protocol upgrades. Between 2019 and 2023, Monero wallet application bugs "Differ By One" and "10 Block Decoy Bug" have been observed and identified and discussed in the Monero community. In addition, a decentralized mining pool named P2Pool has proliferated, and a controversial UTXO NFT imitation known as Mordinals has been tried for Monero. In this paper, we systematically describe the traceability heuristics that have emerged from these developments, and evaluate their quality based on ground truth, and through pairwise comparisons. We also explore the temporal perspective, and show which of these heuristics have been applicable over the past years, what fraction of decoys could be eliminated and what the remaining effective ring size is. Our findings illustrate that most of the heuristics have a high precision, that the "10 Block Decoy Bug" and the Coinbase decoy identification heuristics have had the most impact between 2019 and 2023, and that the former could be used to evaluate future heuristics, if they are also applicable during that time frame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05332v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nada Hammad, Friedhelm Victor</dc:creator>
    </item>
    <item>
      <title>Detecting Masquerade Attacks in Controller Area Networks Using Graph Machine Learning</title>
      <link>https://arxiv.org/abs/2408.05427</link>
      <description>arXiv:2408.05427v1 Announce Type: new 
Abstract: Modern vehicles rely on a myriad of electronic control units (ECUs) interconnected via controller area networks (CANs) for critical operations. Despite their ubiquitous use and reliability, CANs are susceptible to sophisticated cyberattacks, particularly masquerade attacks, which inject false data that mimic legitimate messages at the expected frequency. These attacks pose severe risks such as unintended acceleration, brake deactivation, and rogue steering. Traditional intrusion detection systems (IDS) often struggle to detect these subtle intrusions due to their seamless integration into normal traffic. This paper introduces a novel framework for detecting masquerade attacks in the CAN bus using graph machine learning (ML). We hypothesize that the integration of shallow graph embeddings with time series features derived from CAN frames enhances the detection of masquerade attacks. We show that by representing CAN bus frames as message sequence graphs (MSGs) and enriching each node with contextual statistical attributes from time series, we can enhance detection capabilities across various attack patterns compared to using only graph-based features. Our method ensures a comprehensive and dynamic analysis of CAN frame interactions, improving robustness and efficiency. Extensive experiments on the ROAD dataset validate the effectiveness of our approach, demonstrating statistically significant improvements in the detection rates of masquerade attacks compared to a baseline that uses only graph-based features, as confirmed by Mann-Whitney U and Kolmogorov-Smirnov tests (p &lt; 0.05).</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05427v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William Marfo, Pablo Moriano, Deepak K. Tosh, Shirley V. Moore</dc:creator>
    </item>
    <item>
      <title>PointNCBW: Towards Dataset Ownership Verification for Point Clouds via Negative Clean-label Backdoor Watermark</title>
      <link>https://arxiv.org/abs/2408.05500</link>
      <description>arXiv:2408.05500v1 Announce Type: new 
Abstract: Recently, point clouds have been widely used in computer vision, whereas their collection is time-consuming and expensive. As such, point cloud datasets are the valuable intellectual property of their owners and deserve protection. To detect and prevent unauthorized use of these datasets, especially for commercial or open-sourced ones that cannot be sold again or used commercially without permission, we intend to identify whether a suspicious third-party model is trained on our protected dataset under the black-box setting. We achieve this goal by designing a scalable clean-label backdoor-based dataset watermark for point clouds that ensures both effectiveness and stealthiness. Unlike existing clean-label watermark schemes, which are susceptible to the number of categories, our method could watermark samples from all classes instead of only from the target one. Accordingly, it can still preserve high effectiveness even on large-scale datasets with many classes. Specifically, we perturb selected point clouds with non-target categories in both shape-wise and point-wise manners before inserting trigger patterns without changing their labels. The features of perturbed samples are similar to those of benign samples from the target class. As such, models trained on the watermarked dataset will have a distinctive yet stealthy backdoor behavior, i.e., misclassifying samples from the target class whenever triggers appear, since the trained DNNs will treat the inserted trigger pattern as a signal to deny predicting the target label. We also design a hypothesis-test-guided dataset ownership verification based on the proposed watermark. Extensive experiments on benchmark datasets are conducted, verifying the effectiveness of our method and its resistance to potential removal methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05500v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cheng Wei, Yang Wang, Kuofeng Gao, Shuo Shao, Yiming Li, Zhibo Wang, Zhan Qin</dc:creator>
    </item>
    <item>
      <title>Cryptographically Secure Pseudo-Random Number Generation (CS-PRNG) Design using Robust Chaotic Tent Map (RCTM)</title>
      <link>https://arxiv.org/abs/2408.05580</link>
      <description>arXiv:2408.05580v1 Announce Type: new 
Abstract: Chaos, a nonlinear dynamical system, favors cryptography due to their inherent sensitive dependence on the initial condition, mixing, and ergodicity property. In recent years, the nonlinear behavior of chaotic maps has been utilized as a random source to generate pseudo-random number generation for cryptographic services. For chaotic maps having Robust chaos, dense, chaotic orbits exist for the range of parameter space the occurrence of chaotic attractors in some neighborhoods of parameter space and the absence of periodic windows. Thus, the robust chaotic map shows assertive chaotic behavior for larger parameters space with a positive Lyapunov exponent. This paper presents a novel method to generate cryptographically secure pseudo-random numbers (CSPRNG) using a robust chaotic tent map (RCTM). We proposed a new set of equations featuring modulo and scaling operators that achieve vast parameter space by keeping chaotic orbit globally stable and robust. The dynamic behavior of the RCTM is studied first by plotting the bifurcation diagram that shows chaotic behavior for different parameters, which the positive Lyapunov exponent verifies. We iterated the RCTM to generate pseudo-random bits using a simple thresholding method. Various statistical tests are performed that ascertain the randomness of generated secure pseudo-random bits. It includes NIST 800-22 test suite, ENT statistical test suite, TestU01 test suite, key space analysis, key sensitivity analysis, correlation analysis, histogram analysis, and differential analysis. The proposed scheme has achieved larger key space as compared with existing methods. The results show that the proposed PRBG algorithm can generate CSPRNG.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05580v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muhammad Irfan, Muhammad Asif Khan</dc:creator>
    </item>
    <item>
      <title>Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites</title>
      <link>https://arxiv.org/abs/2408.05667</link>
      <description>arXiv:2408.05667v1 Announce Type: new 
Abstract: In this paper, we introduce PhishLang, an open-source, lightweight Large Language Model (LLM) specifically designed for phishing website detection through contextual analysis of the website. Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats and deep learning models that are computationally intensive, our model utilizes the advanced language processing capabilities of LLMs to learn granular features that are characteristic of phishing attacks. Furthermore, PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning tools, while being significantly faster and less resource-intensive. Over a 3.5-month testing period, PhishLang successfully identified approximately 26K phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to aid current detection measures. We also evaluate PhishLang against several realistic adversarial attacks and develop six patches that make it very robust against such threats. Furthermore, we integrate PhishLang with GPT-3.5 Turbo to create \textit{explainable blocklisting} - warnings that provide users with contextual information about different features that led to a website being marked as phishing. Finally, we have open-sourced the PhishLang framework and developed a Chromium-based browser extension and URL scanner website, which implement explainable warnings for end-users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05667v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayak Saha Roy, Shirin Nilizadeh</dc:creator>
    </item>
    <item>
      <title>Swarm-Net: Firmware Attestation in IoT Swarms using Graph Neural Networks and Volatile Memory</title>
      <link>https://arxiv.org/abs/2408.05680</link>
      <description>arXiv:2408.05680v1 Announce Type: new 
Abstract: The Internet of Things (IoT) is a network of billions of interconnected, primarily low-end embedded devices. Despite large-scale deployment, studies have highlighted critical security concerns in IoT networks, many of which stem from firmware-related issues. Furthermore, IoT swarms have become more prevalent in industries, smart homes, and agricultural applications, among others. Malicious activity on one node in a swarm can propagate to larger network sections. Although several Remote Attestation (RA) techniques have been proposed, they are limited by their latency, availability, complexity, hardware assumptions, and uncertain access to firmware copies under Intellectual Property (IP) rights. We present Swarm-Net, a novel swarm attestation technique that exploits the inherent, interconnected, graph-like structure of IoT networks along with the runtime information stored in the Static Random Access Memory (SRAM) using Graph Neural Networks (GNN) to detect malicious firmware and its downstream effects. We also present the first datasets on SRAM-based swarm attestation encompassing different types of firmware and edge relationships. In addition, a secure swarm attestation protocol is presented. Swarm-Net is not only computationally lightweight but also does not require a copy of the firmware. It achieves a 99.96% attestation rate on authentic firmware, 100% detection rate on anomalous firmware, and 99% detection rate on propagated anomalies, at a communication overhead and inference latency of ~1 second and ~10^{-5} seconds (on a laptop CPU), respectively. In addition to the collected datasets, Swarm-Net's effectiveness is evaluated on simulated trace replay, random trace perturbation, and dropped attestation responses, showing robustness against such threats. Lastly, we compare Swarm-Net with past works and present a security analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05680v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Varun Kohli, Bhavya Kohli, Muhammad Naveed Aman, Biplab Sikdar</dc:creator>
    </item>
    <item>
      <title>ICSFuzz: Collision Detector Bug Discovery in Autonomous Driving Simulators</title>
      <link>https://arxiv.org/abs/2408.05694</link>
      <description>arXiv:2408.05694v1 Announce Type: new 
Abstract: With the increasing adoption of autonomous vehicles, ensuring the reliability of autonomous driving systems (ADSs) deployed on autonomous vehicles has become a significant concern. Driving simulators have emerged as crucial platforms for testing autonomous driving systems, offering realistic, dynamic, and configurable environments. However, existing simulation-based ADS testers have largely overlooked the reliability of the simulators, potentially leading to overlooked violation scenarios and subsequent safety security risks during real-world deployment. In our investigations, we identified that collision detectors in simulators could fail to detect and report collisions in certain collision scenarios, referred to as ignored collision scenarios.
  This paper aims to systematically discover ignored collision scenarios to improve the reliability of autonomous driving simulators. To this end, we present ICSFuzz, a black-box fuzzing approach to discover ignored collision scenarios efficiently. Drawing upon the fact that the ignored collision scenarios are a sub-type of collision scenarios, our approach starts with the determined collision scenarios. Following the guidance provided by empirically studied factors contributing to collisions, we selectively mutate arbitrary collision scenarios in a step-wise manner toward the ignored collision scenarios and effectively discover them.
  We compare ICSFuzz with DriveFuzz, a state-of-the-art simulation-based ADS testing method, by replacing its oracle with our ignored-collision-aware oracle. The evaluation demonstrates that ICSFuzz outperforms DriveFuzz by finding 10-20x more ignored collision scenarios with a 20-70x speedup. All the discovered ignored collisions have been confirmed by developers with one CVE ID assigned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05694v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiwei Fu, Heqing Huang, Yifan Zhang, Ke Zhang, Jin Huang, Wei-Bin Lee, Jianping Wang</dc:creator>
    </item>
    <item>
      <title>Disposable-key-based image encryption for collaborative learning of Vision Transformer</title>
      <link>https://arxiv.org/abs/2408.05737</link>
      <description>arXiv:2408.05737v1 Announce Type: new 
Abstract: We propose a novel method for securely training the vision transformer (ViT) with sensitive data shared from multiple clients similar to privacy-preserving federated learning. In the proposed method, training images are independently encrypted by each client where encryption keys can be prepared by each client, and ViT is trained by using these encrypted images for the first time. The method allows clients not only to dispose of the keys but to also reduce the communication costs between a central server and the clients. In image classification experiments, we verify the effectiveness of the proposed method on the CIFAR-10 dataset in terms of classification accuracy and the use of restricted random permutation matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05737v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rei Aso, Sayaka Shiota, Hitoshi Kiya</dc:creator>
    </item>
    <item>
      <title>Devlore: Extending Arm CCA to Integrated Devices A Journey Beyond Memory to Interrupt Isolation</title>
      <link>https://arxiv.org/abs/2408.05835</link>
      <description>arXiv:2408.05835v1 Announce Type: new 
Abstract: Arm Confidential Computing Architecture (CCA) executes sensitive computation in an abstraction called realm VMs and protects it from the hypervisor, host OS, and other co-resident VMs. However, CCA does not allow integrated devices on the platform to access realm VMs and doing so requires intrusive changes to software and is simply not possible to achieve securely for some devices. In this paper, we present Devlore which allows realm VMs to directly access integrated peripherals. Devlore memory isolation re-purposes CCA hardware primitives (granule protection and stage-two page tables), while our interrupt isolation adapts a delegate-but-check strategy. Our choice of offloading interrupt management to the hypervisor but adding correctness checks in the trusted software allows Devlore to preserve compatibility and performance. We evaluate Devlore on Arm FVP to demonstrate 5 diverse peripherals attached to realm VMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05835v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrin Bertschi, Supraja Sridhara, Friederike Groschupp, Mark Kuhne, Benedict Schl\"uter, Cl\'ement Thorens, Nicolas Dutly, Srdjan Capkun, Shweta Shinde</dc:creator>
    </item>
    <item>
      <title>Using Retriever Augmented Large Language Models for Attack Graph Generation</title>
      <link>https://arxiv.org/abs/2408.05855</link>
      <description>arXiv:2408.05855v1 Announce Type: new 
Abstract: As the complexity of modern systems increases, so does the importance of assessing their security posture through effective vulnerability management and threat modeling techniques. One powerful tool in the arsenal of cybersecurity professionals is the attack graph, a representation of all potential attack paths within a system that an adversary might exploit to achieve a certain objective. Traditional methods of generating attack graphs involve expert knowledge, manual curation, and computational algorithms that might not cover the entire threat landscape due to the ever-evolving nature of vulnerabilities and exploits. This paper explores the approach of leveraging large language models (LLMs), such as ChatGPT, to automate the generation of attack graphs by intelligently chaining Common Vulnerabilities and Exposures (CVEs) based on their preconditions and effects. It also shows how to utilize LLMs to create attack graphs from threat reports.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05855v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renascence Tarafder Prapty, Ashish Kundu, Arun Iyengar</dc:creator>
    </item>
    <item>
      <title>Integrative Approaches in Cybersecurity and AI</title>
      <link>https://arxiv.org/abs/2408.05888</link>
      <description>arXiv:2408.05888v1 Announce Type: new 
Abstract: In recent years, the convergence of cybersecurity, artificial intelligence (AI), and data management has emerged as a critical area of research, driven by the increasing complexity and interdependence of modern technological ecosystems. This paper provides a comprehensive review and analysis of integrative approaches that harness AI techniques to enhance cybersecurity frameworks and optimize data management practices. By exploring the synergies between these domains, we identify key trends, challenges, and future directions that hold the potential to revolutionize the way organizations protect, analyze, and leverage their data. Our findings highlight the necessity of cross-disciplinary strategies that incorporate AI-driven automation, real-time threat detection, and advanced data analytics to build more resilient and adaptive security architectures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05888v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marwan Omar</dc:creator>
    </item>
    <item>
      <title>Multimodal Large Language Models for Phishing Webpage Detection and Identification</title>
      <link>https://arxiv.org/abs/2408.05941</link>
      <description>arXiv:2408.05941v1 Announce Type: new 
Abstract: To address the challenging problem of detecting phishing webpages, researchers have developed numerous solutions, in particular those based on machine learning (ML) algorithms. Among these, brand-based phishing detection that uses models from Computer Vision to detect if a given webpage is imitating a well-known brand has received widespread attention. However, such models are costly and difficult to maintain, as they need to be retrained with labeled dataset that has to be regularly and continuously collected. Besides, they also need to maintain a good reference list of well-known websites and related meta-data for effective performance.
  In this work, we take steps to study the efficacy of large language models (LLMs), in particular the multimodal LLMs, in detecting phishing webpages. Given that the LLMs are pretrained on a large corpus of data, we aim to make use of their understanding of different aspects of a webpage (logo, theme, favicon, etc.) to identify the brand of a given webpage and compare the identified brand with the domain name in the URL to detect a phishing attack. We propose a two-phase system employing LLMs in both phases: the first phase focuses on brand identification, while the second verifies the domain. We carry out comprehensive evaluations on a newly collected dataset. Our experiments show that the LLM-based system achieves a high detection rate at high precision; importantly, it also provides interpretable evidence for the decisions. Our system also performs significantly better than a state-of-the-art brand-based phishing detection system while demonstrating robustness against two known adversarial attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05941v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jehyun Lee, Peiyuan Lim, Bryan Hooi, Dinil Mon Divakaran</dc:creator>
    </item>
    <item>
      <title>Nob-MIAs: Non-biased Membership Inference Attacks Assessment on Large Language Models with Ex-Post Dataset Construction</title>
      <link>https://arxiv.org/abs/2408.05968</link>
      <description>arXiv:2408.05968v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) has triggered legal and ethical concerns, especially regarding the unauthorized use of copyrighted materials in their training datasets. This has led to lawsuits against tech companies accused of using protected content without permission. Membership Inference Attacks (MIAs) aim to detect whether specific documents were used in a given LLM pretraining, but their effectiveness is undermined by biases such as time-shifts and n-gram overlaps.
  This paper addresses the evaluation of MIAs on LLMs with partially inferable training sets, under the ex-post hypothesis, which acknowledges inherent distributional biases between members and non-members datasets. We propose and validate algorithms to create ``non-biased'' and ``non-classifiable'' datasets for fairer MIA assessment. Experiments using the Gutenberg dataset on OpenLamma and Pythia show that neutralizing known biases alone is insufficient. Our methods produce non-biased ex-post datasets with AUC-ROC scores comparable to those previously obtained on genuinely random datasets, validating our approach. Globally, MIAs yield results close to random, with only one being effective on both random and our datasets, but its performance decreases when bias is removed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05968v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>C\'edric Eichler, Nathan Champeil, Nicolas Anciaux, Alexandra Bensamoun, Heber Hwang Arcolezi, Jos\'e Maria De Fuentes</dc:creator>
    </item>
    <item>
      <title>Formalizing the Cryptographic Migration Problem</title>
      <link>https://arxiv.org/abs/2408.05997</link>
      <description>arXiv:2408.05997v1 Announce Type: new 
Abstract: With the advancements in quantum computing, transitioning to post-quantum cryptography is becoming increasingly critical to maintain the security of modern systems. This paper introduces a formal definition of the cryptographic migration problem and explores its complexities using a suitable directed graph model. Characteristics of the resulting migration graphs are analyzed and trade-offs discussed. By using classical mathematical results from combinatorics, probability theory and combinatorial analysis, we assess the challenges of migrating ``random'' large cryptographic IT-infrastructures. We show that any sufficiently large migration project that follows our model has an intrinsic complexity, either due to many dependent (comparatively easy) migration steps or due to at least one complicated migration step. This proves that in a suitable sense cryptographic migration is hard in general.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05997v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Loebenberger, Stefan-Lukas Gazdag, Daniel Herzinger, Eduard Hirsch, Christian N\"ather</dc:creator>
    </item>
    <item>
      <title>Understanding Byzantine Robustness in Federated Learning with A Black-box Server</title>
      <link>https://arxiv.org/abs/2408.06042</link>
      <description>arXiv:2408.06042v1 Announce Type: new 
Abstract: Federated learning (FL) becomes vulnerable to Byzantine attacks where some of participators tend to damage the utility or discourage the convergence of the learned model via sending their malicious model updates. Previous works propose to apply robust rules to aggregate updates from participators against different types of Byzantine attacks, while at the same time, attackers can further design advanced Byzantine attack algorithms targeting specific aggregation rule when it is known. In practice, FL systems can involve a black-box server that makes the adopted aggregation rule inaccessible to participants, which can naturally defend or weaken some Byzantine attacks. In this paper, we provide an in-depth understanding on the Byzantine robustness of the FL system with a black-box server. Our investigation demonstrates the improved Byzantine robustness of a black-box server employing a dynamic defense strategy. We provide both empirical evidence and theoretical analysis to reveal that the black-box server can mitigate the worst-case attack impact from a maximum level to an expectation level, which is attributed to the inherent inaccessibility and randomness offered by a black-box server.The source code is available at https://github.com/alibaba/FederatedScope/tree/Byzantine_attack_defense to promote further research in the community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06042v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangyuan Zhao, Yuexiang Xie, Xuebin Ren, Bolin Ding, Shusen Yang, Yaliang Li</dc:creator>
    </item>
    <item>
      <title>Uncovering the Role of Support Infrastructure in Clickbait PDF Campaigns</title>
      <link>https://arxiv.org/abs/2408.06133</link>
      <description>arXiv:2408.06133v1 Announce Type: new 
Abstract: Clickbait PDFs, an entry point for multiple Web attacks, are distributed via SEO poisoning and rank high in search results due to being massively uploaded on abused or compromised websites. The central role of these hosts in the distribution of clickbait PDFs remains understudied, and it is unclear whether attackers differentiate the types of hosting for PDF uploads, how long they rely on hosts, and how affected parties respond to abuse.
  To address this, we conducted real-time analyses on hosts, collecting data on 4,648,939 clickbait PDFs served by 177,835 hosts over 17 months. Our results revealed a diverse infrastructure, with hosts falling into three main hosting types. We also identified at scale the presence of eight software components which facilitate file uploads and which are likely exploited for clickbait PDF distribution. We contact affected parties to report the misuse of their resources via a large-scale vulnerability notification. While we observed some effectiveness in terms of number of cleaned-up PDFs following the notification, long-term improvement in this infrastructure remained insignificant. This finding raises questions about the hosting providers' role in combating abuse and the actual impact of vulnerability notifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06133v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giada Stivala, Gianluca De Stefano, Andrea Mengascini, Mariano Graziano, Giancarlo Pellegrino</dc:creator>
    </item>
    <item>
      <title>Lancelot: Towards Efficient and Privacy-Preserving Byzantine-Robust Federated Learning within Fully Homomorphic Encryption</title>
      <link>https://arxiv.org/abs/2408.06197</link>
      <description>arXiv:2408.06197v1 Announce Type: new 
Abstract: In sectors such as finance and healthcare, where data governance is subject to rigorous regulatory requirements, the exchange and utilization of data are particularly challenging. Federated Learning (FL) has risen as a pioneering distributed machine learning paradigm that enables collaborative model training across multiple institutions while maintaining data decentralization. Despite its advantages, FL is vulnerable to adversarial threats, particularly poisoning attacks during model aggregation, a process typically managed by a central server. However, in these systems, neural network models still possess the capacity to inadvertently memorize and potentially expose individual training instances. This presents a significant privacy risk, as attackers could reconstruct private data by leveraging the information contained in the model itself. Existing solutions fall short of providing a viable, privacy-preserving BRFL system that is both completely secure against information leakage and computationally efficient. To address these concerns, we propose Lancelot, an innovative and computationally efficient BRFL framework that employs fully homomorphic encryption (FHE) to safeguard against malicious client activities while preserving data privacy. Our extensive testing, which includes medical imaging diagnostics and widely-used public image datasets, demonstrates that Lancelot significantly outperforms existing methods, offering more than a twenty-fold increase in processing speed, all while maintaining data privacy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06197v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyang Jiang, Hao Yang, Qipeng Xie, Chuan Ma, Sen Wang, Guoliang Xing</dc:creator>
    </item>
    <item>
      <title>120 Domain-Specific Languages for Security</title>
      <link>https://arxiv.org/abs/2408.06219</link>
      <description>arXiv:2408.06219v1 Announce Type: new 
Abstract: Security engineering, from security requirements engineering to the implementation of cryptographic protocols, is often supported by domain-specific languages (DSLs). Unfortunately, a lack of knowledge about these DSLs, such as which security aspects are addressed and when, hinders their effective use and further research. This systematic literature review examines 120 security-oriented DSLs based on six research questions concerning security aspects and goals, language-specific characteristics, integration into the software development lifecycle (SDLC), and effectiveness of the DSLs. We observe a high degree of fragmentation, which leads to opportunities for integration. We also need to improve the usability and evaluation of security DSLs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06219v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Markus Krausz, Sven Peldszus, Francesco Regazzoni, Thorsten Berger, Tim Tim G\"uneysu</dc:creator>
    </item>
    <item>
      <title>Decentralized Intelligence Health Network (DIHN)</title>
      <link>https://arxiv.org/abs/2408.06240</link>
      <description>arXiv:2408.06240v1 Announce Type: new 
Abstract: Decentralized Intelligence Health Network (DIHN) is a theoretical framework addressing significant challenges of health data sovereignty and AI utilization in healthcare caused by data fragmentation across providers and institutions. It establishes a sovereign architecture for healthcare provision as a prerequisite to a sovereign health network, then facilitates effective AI utilization by overcoming barriers to accessing diverse medical data sources. This comprehensive framework leverages: 1) self-sovereign identity architecture coupled with a personal health record (PHR) as a prerequisite for health data sovereignty; 2) a scalable federated learning (FL) protocol implemented on a public blockchain for decentralized AI training in healthcare, where health data remains with participants and only model parameter updates are shared; and 3) a scalable, trustless rewards mechanism to incentivize participation and ensure fair reward distribution. This framework ensures that no entity can prevent or control access to training on health data offered by participants or determine financial benefits, as these processes operate on a public blockchain with an immutable record and without a third party. It supports effective AI training in healthcare, allowing patients to maintain control over their health data, benefit financially, and contribute to a decentralized, scalable ecosystem that leverages collective AI to develop beneficial healthcare algorithms. Patients receive rewards into their digital wallets as an incentive to opt-in to the FL protocol, with a long-term roadmap to funding decentralized insurance solutions. This approach introduces a novel, self-financed healthcare model that adapts to individual needs, complements existing systems, and redefines universal coverage. It highlights the potential to transform healthcare data management and AI utilization while empowering patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06240v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Nash</dc:creator>
    </item>
    <item>
      <title>A RAG-Based Question-Answering Solution for Cyber-Attack Investigation and Attribution</title>
      <link>https://arxiv.org/abs/2408.06272</link>
      <description>arXiv:2408.06272v1 Announce Type: new 
Abstract: In the constantly evolving field of cybersecurity, it is imperative for analysts to stay abreast of the latest attack trends and pertinent information that aids in the investigation and attribution of cyber-attacks. In this work, we introduce the first question-answering (QA) model and its application that provides information to the cybersecurity experts about cyber-attacks investigations and attribution. Our QA model is based on Retrieval Augmented Generation (RAG) techniques together with a Large Language Model (LLM) and provides answers to the users' queries based on either our knowledge base (KB) that contains curated information about cyber-attacks investigations and attribution or on outside resources provided by the users. We have tested and evaluated our QA model with various types of questions, including KB-based, metadata-based, specific documents from the KB, and external sources-based questions. We compared the answers for KB-based questions with those from OpenAI's GPT-3.5 and the latest GPT-4o LLMs. Our proposed QA model outperforms OpenAI's GPT models by providing the source of the answers and overcoming the hallucination limitations of the GPT models, which is critical for cyber-attack investigation and attribution. Additionally, our analysis showed that when the RAG QA model is given few-shot examples rather than zero-shot instructions, it generates better answers compared to cases where no examples are supplied in addition to the query.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06272v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sampath Rajapaksha, Ruby Rani, Erisa Karafili</dc:creator>
    </item>
    <item>
      <title>Hound: Locating Cryptographic Primitives in Desynchronized Side-Channel Traces Using Deep-Learning</title>
      <link>https://arxiv.org/abs/2408.06296</link>
      <description>arXiv:2408.06296v1 Announce Type: new 
Abstract: Side-channel attacks allow to extract sensitive information from cryptographic primitives by correlating the partially known computed data and the measured side-channel signal. Starting from the raw side-channel trace, the preprocessing of the side-channel trace to pinpoint the time at which each cryptographic primitive is executed, and, then, to re-align all the collected data to this specific time represent a critical step to setup a successful side-channel attack. The use of hiding techniques has been widely adopted as a low-cost solution to hinder the preprocessing of side-channel traces thus limiting side-channel attacks in real scenarios. This work introduces Hound, a novel deep learning-based pipeline to locate the execution of cryptographic primitives within the side-channel trace even in the presence of trace deformations introduced by the use of dynamic frequency scaling actuators. Hound has been validated through successful attacks on various cryptographic primitives executed on an FPGA-based system-on-chip incorporating a RISC-V CPU, while dynamic frequency scaling is active. Experimental results demonstrate the possibility of identifying the cryptographic primitives in DFS-deformed side-channel traces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06296v1</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Davide Galli, Giuseppe Chiari, Davide Zoni</dc:creator>
    </item>
    <item>
      <title>Control-Flow Attestation: Concepts, Solutions, and Open Challenges</title>
      <link>https://arxiv.org/abs/2408.06304</link>
      <description>arXiv:2408.06304v1 Announce Type: new 
Abstract: Control-flow attestation (CFA) unifies the worlds of control-flow integrity and platform attestation by measuring and reporting a target's run-time behaviour to a verifier. Trust assurances in the target are provided by testing whether its execution follows an authorised control-flow path. The problem has been explored in various settings, such as assessing the trustworthiness of cyber-physical systems, Internet of Things devices, cloud platforms, and many others. Despite a significant number of proposals being made in recent years, the area remains fragmented, addressing different adversarial behaviours, verification paradigms, and deployment challenges. In this paper, we present the first survey of control-flow attestation, examining the core ideas and solutions in state-of-the-art schemes. In total, we survey over 30 papers published between 2016-2024, consolidate and compare their key features, and pose several challenges and recommendations for future research in the area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06304v1</guid>
      <category>cs.CR</category>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhanyu Sha, Carlton Shepherd, Amir Rafi, Konstantinos Markantonakis</dc:creator>
    </item>
    <item>
      <title>Asynchronous Approximate Agreement with Quadratic Communication</title>
      <link>https://arxiv.org/abs/2408.05495</link>
      <description>arXiv:2408.05495v1 Announce Type: cross 
Abstract: We consider an asynchronous network of $n$ message-sending parties, up to $t$ of which are byzantine. We study approximate agreement, where the parties obtain approximately equal outputs in the convex hull of their inputs. The seminal protocol of Abraham, Amit and Dolev [OPODIS '04] achieves approximate agreement in $\mathbb{R}$ with the optimal resilience $t &lt; \frac{n}{3}$ by making each party reliably broadcast its input. This takes $\Omega(n^2)$ messages per reliable broadcast, or $\Omega(n^3)$ messages in total.
  In this work, we present optimally resilient asynchronous approximate agreement protocols which forgo reliable broadcast and thus require communication proportional to $n^2$ instead of $n^3$. First, we achieve $\omega$-dimensional barycentric agreement with $\mathcal{O}(\omega n^2)$ small messages. Then, we achieve edge agreement in a tree of diameter $D$ with $\lceil \log_2 D \rceil$ iterations of a multivalued graded consensus variant for which we design an efficient protocol. This results in a $\mathcal{O}(\log\frac{1}{\varepsilon})$-round protocol for $\varepsilon$-agreement in $[0, 1]$ with $\mathcal{O}(n^2\log\frac{1}{\varepsilon})$ messages and $\mathcal{O}(n^2\log\frac{1}{\varepsilon}\log\log\frac{1}{\varepsilon})$ bits of communication, improving over the state of the art which matches this complexity only when the inputs are all either $0$ or $1$. Finally, we extend our edge agreement protocol to achieve edge agreement in $\mathbb{Z}$ and thus $\varepsilon$-agreement in $\mathbb{R}$ with quadratic communication, in $\mathcal{O}(\log\frac{M}{\varepsilon})$ rounds where $M$ is the maximum honest input magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05495v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mose Mizrahi Erbes, Roger Wattenhofer</dc:creator>
    </item>
    <item>
      <title>Trustworthiness for an Ultra-Wideband Localization Service</title>
      <link>https://arxiv.org/abs/2408.05527</link>
      <description>arXiv:2408.05527v1 Announce Type: cross 
Abstract: Trustworthiness assessment is an essential step to assure that interdependent systems perform critical functions as anticipated, even under adverse conditions. In this paper, a holistic trustworthiness assessment framework for ultra-wideband self-localization is proposed, including attributes of reliability, security, privacy, and resilience. Our goal is to provide guidance for evaluating a system's trustworthiness based on objective evidence, so-called trustworthiness indicators. These indicators are carefully selected through the threat analysis of the particular system. Our approach guarantees that the resulting trustworthiness indicators correspond to chosen real-world threats. Moreover, experimental evaluations are conducted to demonstrate the effectiveness of the proposed method. While the framework is tailored for this specific use case, the process itself serves as a versatile template, which can be used in other applications in the domains of the Internet of Things or cyber-physical systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05527v1</guid>
      <category>eess.SP</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Peterseil, Bernhard Etzlinger, Jan Hor\'a\v{c}ek, Roya Khanzadeh, Andreas Springer</dc:creator>
    </item>
    <item>
      <title>Improving Adversarial Transferability with Neighbourhood Gradient Information</title>
      <link>https://arxiv.org/abs/2408.05745</link>
      <description>arXiv:2408.05745v1 Announce Type: cross 
Abstract: Deep neural networks (DNNs) are known to be susceptible to adversarial examples, leading to significant performance degradation. In black-box attack scenarios, a considerable attack performance gap between the surrogate model and the target model persists. This work focuses on enhancing the transferability of adversarial examples to narrow this performance gap. We observe that the gradient information around the clean image, i.e. Neighbourhood Gradient Information, can offer high transferability. Leveraging this, we propose the NGI-Attack, which incorporates Example Backtracking and Multiplex Mask strategies, to use this gradient information and enhance transferability fully. Specifically, we first adopt Example Backtracking to accumulate Neighbourhood Gradient Information as the initial momentum term. Multiplex Mask, which forms a multi-way attack strategy, aims to force the network to focus on non-discriminative regions, which can obtain richer gradient information during only a few iterations. Extensive experiments demonstrate that our approach significantly enhances adversarial transferability. Especially, when attacking numerous defense models, we achieve an average attack success rate of 95.8%. Notably, our method can plugin with any off-the-shelf algorithm to improve their attack performance without additional time cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05745v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haijing Guo, Jiafeng Wang, Zhaoyu Chen, Kaixun Jiang, Lingyi Hong, Pinxue Guo, Jinglun Li, Wenqiang Zhang</dc:creator>
    </item>
    <item>
      <title>SZKP: A Scalable Accelerator Architecture for Zero-Knowledge Proofs</title>
      <link>https://arxiv.org/abs/2408.05890</link>
      <description>arXiv:2408.05890v1 Announce Type: cross 
Abstract: Zero-Knowledge Proofs (ZKPs) are an emergent paradigm in verifiable computing. In the context of applications like cloud computing, ZKPs can be used by a client (called the verifier) to verify the service provider (called the prover) is in fact performing the correct computation based on a public input. A recently prominent variant of ZKPs is zkSNARKs, generating succinct proofs that can be rapidly verified by the end user. However, proof generation itself is very time consuming per transaction. Two key primitives in proof generation are the Number Theoretic Transform (NTT) and Multi-scalar Multiplication (MSM). These primitives are prime candidates for hardware acceleration, and prior works have looked at GPU implementations and custom RTL. However, both algorithms involve complex dataflow patterns -- standard NTTs have irregular memory accesses for butterfly computations from stage to stage, and MSMs using Pippenger's algorithm have data-dependent memory accesses for partial sum calculations. We present SZKP, a scalable accelerator framework that is the first ASIC to accelerate an entire proof on-chip by leveraging structured dataflows for both NTTs and MSMs. SZKP achieves conservative full-proof speedups of over 400$\times$, 3$\times$, and 12$\times$ over CPU, ASIC, and GPU implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05890v1</guid>
      <category>cs.AR</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3656019.3676898</arxiv:DOI>
      <dc:creator>Alhad Daftardar, Brandon Reagen, Siddharth Garg</dc:creator>
    </item>
    <item>
      <title>Gender of Recruiter Makes a Difference: A study into Cybersecurity Graduate Recruitment</title>
      <link>https://arxiv.org/abs/2408.05895</link>
      <description>arXiv:2408.05895v1 Announce Type: cross 
Abstract: An ever-widening workforce gap exists in the global cybersecurity industry but diverse talent is underutilized. The global cybersecurity workforce is only 25% female. Much research exists on the effect of gender bias on the hiring of women into the technical workforce, but little on how the gender of the recruiter (gender difference) affects recruitment decisions. This research reveals differences between the non-technical skills sought by female vs non-female cybersecurity recruiters. The former look for recruits with people-focused skills while the latter look for task-focused skills, highlighting the need for gender diversity in recruitment panels.
  Recruiters are increasingly seeking non-technical (soft) skills in technical graduate recruits. This requires STEM curriculum in Universities to adapt to match. Designing an industry-ready cybersecurity curriculum requires knowledge of these non-technical skills. An online survey of cybersecurity professionals was used to determine the most sought after non-technical skills in the field. Analysis of the data reveals distinct gender differences in the non-technical skills most valued in a recruit, based on the gender of the recruiter (not the recruited). The gender differences discovered do not correspond to the higher proportion of women employed in non-technical cybersecurity roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05895v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Joanne L. Hall, Asha Rao</dc:creator>
    </item>
    <item>
      <title>Blind-Match: Efficient Homomorphic Encryption-Based 1:N Matching for Privacy-Preserving Biometric Identification</title>
      <link>https://arxiv.org/abs/2408.06167</link>
      <description>arXiv:2408.06167v1 Announce Type: cross 
Abstract: We present Blind-Match, a novel biometric identification system that leverages homomorphic encryption (HE) for efficient and privacy-preserving 1:N matching. Blind-Match introduces a HE-optimized cosine similarity computation method, where the key idea is to divide the feature vector into smaller parts for processing rather than computing the entire vector at once. By optimizing the number of these parts, Blind-Match minimizes execution time while ensuring data privacy through HE. Blind-Match achieves superior performance compared to state-of-the-art methods across various biometric datasets. On the LFW face dataset, Blind-Match attains a 99.63% Rank-1 accuracy with a 128-dimensional feature vector, demonstrating its robustness in face recognition tasks. For fingerprint identification, Blind-Match achieves a remarkable 99.55% Rank-1 accuracy on the PolyU dataset, even with a compact 16-dimensional feature vector, significantly outperforming the state-of-the-art method, Blind-Touch, which achieves only 59.17%. Furthermore, Blind-Match showcases practical efficiency in large-scale biometric identification scenarios, such as Naver Cloud's FaceSign, by processing 6,144 biometric samples in 0.74 seconds using a 128-dimensional feature vector.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06167v1</guid>
      <category>cs.CV</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3627673.3680017</arxiv:DOI>
      <dc:creator>Hyunmin Choi, Jiwon Kim, Chiyoung Song, Simon S. Woo, Hyoungshick Kim</dc:creator>
    </item>
    <item>
      <title>Integration of blockchain in smart systems: problems and opportunities for real-time sensor data storage</title>
      <link>https://arxiv.org/abs/2408.06331</link>
      <description>arXiv:2408.06331v1 Announce Type: cross 
Abstract: The internet of things (IoT) and other emerging ubiquitous technologies are supporting the rapid spread of smart systems, which has underlined the need for safe, open, and decentralized data storage solutions. With its inherent decentralization and immutability, blockchain offers itself as a potential solution for these requirements. However, the practicality of incorporating blockchain into real-time sensor data storage systems is a topic that demands in-depth examination. While blockchain promises unmatched data security and auditability, some intrinsic qualities, namely scalability restrictions, transactional delays, and escalating storage demands, impede its seamless deployment in high-frequency, voluminous data contexts typical of real-time sensors. This essay launches a methodical investigation into these difficulties, illuminating their underlying causes, potential effects, and potential countermeasures. In addition, we present a novel pragmatic experimental setup and analysis of blockchain for smart system applications, with an extended discussion of the benefits and disadvantages of deploying blockchain based solutions for smart system ecosystems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06331v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1117/12.3013828</arxiv:DOI>
      <dc:creator>Naseem Alsadi, Syed Zaidi, Mankaran Rooprai, Stephen A. Gadsden, John Yawney</dc:creator>
    </item>
    <item>
      <title>SoK: Content Moderation Schemes in End-to-End Encrypted Systems</title>
      <link>https://arxiv.org/abs/2208.11147</link>
      <description>arXiv:2208.11147v5 Announce Type: replace 
Abstract: This paper aims to survey various techniques utilized for content moderation in end-to-end encryption systems. We assess the challenging aspect of content moderation: maintaining a safe platform while assuring user privacy. We study the unique features of some content moderation techniques, such as message franking and perceptual hashing, and highlight their limitations. Currently implemented content moderation techniques violate the goals of end-to-end encrypted messaging to some extent. This has led researchers to develop remediations and design new security primitives to make content moderation compatible with end-to-end encryption systems. We detail these developments, analyze the proposed research efforts, assess their security guarantees, correlate them with other proposed solutions, and determine suitable improvements under specific scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.11147v5</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaitanya Rahalkar, Anushka Virgaonkar</dc:creator>
    </item>
    <item>
      <title>A Diamond Model Analysis on Twitter's Biggest Hack</title>
      <link>https://arxiv.org/abs/2306.15878</link>
      <description>arXiv:2306.15878v3 Announce Type: replace 
Abstract: Cyberattacks have prominently increased over the past few years now, and have targeted actors from a wide variety of domains. Understanding the motivation, infrastructure, attack vectors, etc. behind such attacks is vital to proactively work against preventing such attacks in the future and also to analyze the economic and social impact of such attacks. In this paper, we leverage the diamond model to perform an intrusion analysis case study of the 2020 Twitter account hijacking Cyberattack. We follow this standardized incident response model to map the adversary, capability, infrastructure, and victim and perform a comprehensive analysis of the attack, and the impact posed by the attack from a Cybersecurity policy standpoint.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15878v3</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaitanya Rahalkar</dc:creator>
    </item>
    <item>
      <title>Collaborative Learning Framework to Detect Attacks in Transactions and Smart Contracts</title>
      <link>https://arxiv.org/abs/2308.15804</link>
      <description>arXiv:2308.15804v3 Announce Type: replace 
Abstract: With the escalating prevalence of malicious activities exploiting vulnerabilities in blockchain systems, there is an urgent requirement for robust attack detection mechanisms. To address this challenge, this paper presents a novel collaborative learning framework designed to detect attacks in blockchain transactions and smart contracts by analyzing transaction features. Our framework exhibits the capability to classify various types of blockchain attacks, including intricate attacks at the machine code level (e.g., injecting malicious codes to withdraw coins from users unlawfully), which typically necessitate significant time and security expertise to detect. To achieve that, the proposed framework incorporates a unique tool that transforms transaction features into visual representations, facilitating efficient analysis and classification of low-level machine codes. Furthermore, we propose an advanced collaborative learning model to enable real-time detection of diverse attack types at distributed mining nodes. Our model can efficiently detect attacks in smart contracts and transactions for blockchain systems without the need to gather all data from mining nodes into a centralized server. In order to evaluate the performance of our proposed framework, we deploy a pilot system based on a private Ethereum network and conduct multiple attack scenarios to generate a novel dataset. To the best of our knowledge, our dataset is the most comprehensive and diverse collection of transactions and smart contracts synthesized in a laboratory for cyberattack detection in blockchain systems. Our framework achieves a detection accuracy of approximately 94% through extensive simulations and 91% in real-time experiments with a throughput of over 2,150 transactions per second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.15804v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tran Viet Khoa, Do Hai Son, Chi-Hieu Nguyen, Dinh Thai Hoang, Diep N. Nguyen, Tran Thi Thuy Quynh, Trong-Minh Hoang, Nguyen Viet Ha, Eryk Dutkiewicz, Abu Alsheikh, Nguyen Linh Trung</dc:creator>
    </item>
    <item>
      <title>Survey on Quality Assurance of Smart Contracts</title>
      <link>https://arxiv.org/abs/2311.00270</link>
      <description>arXiv:2311.00270v3 Announce Type: replace 
Abstract: With the increasing adoption of smart contracts, ensuring their security has become a critical concern. Numerous vulnerabilities and attacks have been identified and exploited, resulting in significant financial losses. In response, researchers have developed various tools and techniques to identify and prevent vulnerabilities in smart contracts. In this survey, we present a systematic overview of the quality assurance of smart contracts, covering vulnerabilities, attacks, defenses, and tool support. By classifying vulnerabilities based on known attacks, we can identify patterns and common weaknesses that need to be addressed. Moreover, in order to effectively protect smart contracts, we have created a labeled dataset to evaluate various vulnerability detection tools and compare their effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00270v3</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiyuan Wei, Jing Sun, Zijian Zhang, Xianhao Zhang, Xiaoxuan Yang, Liehuang Zhu</dc:creator>
    </item>
    <item>
      <title>PoisonedRAG: Knowledge Corruption Attacks to Retrieval-Augmented Generation of Large Language Models</title>
      <link>https://arxiv.org/abs/2402.07867</link>
      <description>arXiv:2402.07867v2 Announce Type: replace 
Abstract: Large language models (LLMs) have achieved remarkable success due to their exceptional generative capabilities. Despite their success, they also have inherent limitations such as a lack of up-to-date knowledge and hallucination. Retrieval-Augmented Generation (RAG) is a state-of-the-art technique to mitigate these limitations. The key idea of RAG is to ground the answer generation of an LLM on external knowledge retrieved from a knowledge database. Existing studies mainly focus on improving the accuracy or efficiency of RAG, leaving its security largely unexplored. We aim to bridge the gap in this work. We find that the knowledge database in a RAG system introduces a new and practical attack surface. Based on this attack surface, we propose PoisonedRAG, the first knowledge corruption attack to RAG, where an attacker could inject a few malicious texts into the knowledge database of a RAG system to induce an LLM to generate an attacker-chosen target answer for an attacker-chosen target question. We formulate knowledge corruption attacks as an optimization problem, whose solution is a set of malicious texts. Depending on the background knowledge (e.g., black-box and white-box settings) of an attacker on a RAG system, we propose two solutions to solve the optimization problem, respectively. Our results show PoisonedRAG could achieve a 90% attack success rate when injecting five malicious texts for each target question into a knowledge database with millions of texts. We also evaluate several defenses and our results show they are insufficient to defend against PoisonedRAG, highlighting the need for new defenses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.07867v2</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Zou, Runpeng Geng, Binghui Wang, Jinyuan Jia</dc:creator>
    </item>
    <item>
      <title>Unbridled Icarus: A Survey of the Potential Perils of Image Inputs in Multimodal Large Language Model Security</title>
      <link>https://arxiv.org/abs/2404.05264</link>
      <description>arXiv:2404.05264v2 Announce Type: replace 
Abstract: Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities that increasingly influence various aspects of our daily lives, constantly defining the new boundary of Artificial General Intelligence (AGI). Image modalities, enriched with profound semantic information and a more continuous mathematical nature compared to other modalities, greatly enhance the functionalities of MLLMs when integrated. However, this integration serves as a double-edged sword, providing attackers with expansive vulnerabilities to exploit for highly covert and harmful attacks. The pursuit of reliable AI systems like powerful MLLMs has emerged as a pivotal area of contemporary research. In this paper, we endeavor to demostrate the multifaceted risks associated with the incorporation of image modalities into MLLMs. Initially, we delineate the foundational components and training processes of MLLMs. Subsequently, we construct a threat model, outlining the security vulnerabilities intrinsic to MLLMs. Moreover, we analyze and summarize existing scholarly discourses on MLLMs' attack and defense mechanisms, culminating in suggestions for the future research on MLLM security. Through this comprehensive analysis, we aim to deepen the academic understanding of MLLM security challenges and propel forward the development of trustworthy MLLM systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05264v2</guid>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihe Fan, Yuxin Cao, Ziyu Zhao, Ziyao Liu, Shaofeng Li</dc:creator>
    </item>
    <item>
      <title>Detecting Android Malware: From Neural Embeddings to Hands-On Validation with BERTroid</title>
      <link>https://arxiv.org/abs/2405.03620</link>
      <description>arXiv:2405.03620v2 Announce Type: replace 
Abstract: As cyber threats and malware attacks increasingly alarm both individuals and businesses, the urgency for proactive malware countermeasures intensifies. This has driven a rising interest in automated machine learning solutions. Transformers, a cutting-edge category of attention-based deep learning methods, have demonstrated remarkable success. In this paper, we present BERTroid, an innovative malware detection model built on the BERT architecture. Overall, BERTroid emerged as a promising solution for combating Android malware. Its ability to outperform state-of-the-art solutions demonstrates its potential as a proactive defense mechanism against malicious software attacks. Additionally, we evaluate BERTroid on multiple datasets to assess its performance across diverse scenarios. In the dynamic landscape of cybersecurity, our approach has demonstrated promising resilience against the rapid evolution of malware on Android systems. While the machine learning model captures broad patterns, we emphasize the role of manual validation for deeper comprehension and insight into these behaviors. This human intervention is critical for discerning intricate and context-specific behaviors, thereby validating and reinforcing the model's findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.03620v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meryam Chaieb, Mostafa Anouar Ghorab, Mohamed Aymen Saied</dc:creator>
    </item>
    <item>
      <title>Asymptotic utility of spectral anonymization</title>
      <link>https://arxiv.org/abs/2405.20779</link>
      <description>arXiv:2405.20779v2 Announce Type: replace 
Abstract: In the contemporary data landscape characterized by multi-source data collection and third-party sharing, ensuring individual privacy stands as a critical concern. While various anonymization methods exist, their utility preservation and privacy guarantees remain challenging to quantify. In this work, we address this gap by studying the utility and privacy of the spectral anonymization (SA) algorithm, particularly in an asymptotic framework. Unlike conventional anonymization methods that directly modify the original data, SA operates by perturbing the data in a spectral basis and subsequently reverting them to their original basis. Alongside the original version $\mathcal{P}$-SA, employing random permutation transformation, we introduce two novel SA variants: $\mathcal{J}$-spectral anonymization and $\mathcal{O}$-spectral anonymization, which employ sign-change and orthogonal matrix transformations, respectively. We show how well, under some practical assumptions, these SA algorithms preserve the first and second moments of the original data. Our results reveal, in particular, that the asymptotic efficiency of all three SA algorithms in covariance estimation is exactly 50% when compared to the original data. To assess the applicability of these asymptotic results in practice, we conduct a simulation study with finite data and also evaluate the privacy protection offered by these algorithms using distance-based record linkage. Our research reveals that while no method exhibits clear superiority in finite-sample utility, $\mathcal{O}$-SA distinguishes itself for its exceptional privacy preservation, never producing identical records, albeit with increased computational complexity. Conversely, $\mathcal{P}$-SA emerges as a computationally efficient alternative, demonstrating unmatched efficiency in mean estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20779v2</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Katariina Perkonoja, Joni Virta</dc:creator>
    </item>
    <item>
      <title>Decentralized Intelligence Network (DIN)</title>
      <link>https://arxiv.org/abs/2407.02461</link>
      <description>arXiv:2407.02461v3 Announce Type: replace 
Abstract: Decentralized Intelligence Network (DIN) is a theoretical framework addressing data fragmentation and siloing challenges, enabling scalable AI through data sovereignty. It facilitates effective AI utilization within sovereign networks by overcoming barriers to accessing diverse data sources, leveraging: 1) personal data stores to ensure data sovereignty, where data remains securely within Participants' control; 2) a scalable federated learning protocol implemented on a public blockchain for decentralized AI training, where only model parameter updates are shared, keeping data within the personal data stores; and 3) a scalable, trustless cryptographic rewards mechanism on a public blockchain to incentivize participation and ensure fair reward distribution through a decentralized auditing protocol. This approach guarantees that no entity can prevent or control access to training data or influence financial benefits, as coordination and reward distribution are managed on the public blockchain with an immutable record. The framework supports effective AI training by allowing Participants to maintain control over their data, benefit financially, and contribute to a decentralized, scalable ecosystem that leverages collective AI to develop beneficial algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02461v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abraham Nash</dc:creator>
    </item>
    <item>
      <title>An Alternative to Multi-Factor Authentication with a Triple-Identity Authentication Scheme</title>
      <link>https://arxiv.org/abs/2407.19459</link>
      <description>arXiv:2407.19459v3 Announce Type: replace 
Abstract: Every user authentication scheme involves three login credentials, i.e. a username, a password and a hash value, but only one of them is associated with a user identity. However, this single identity is not robust enough to protect the whole system and the login entries (i.e., the username and password forms) have not been effectively authenticated. Therefore, a multi-factor authentication service is utilized to help guarantee the account security by transmitting an extra factor to the user to use. If more identities can be employed for the two login forms to associate with the corresponding login credentials, and if the identifiers are neither transmitted through the network nor accessible to users, such a system can be more robust even without relying on a third-party service. To achieve this, a triple-identity authentication scheme is designed within a dual-password login-authentication system, by which the identities for the username and the login password can be defined respectively. Therefore, in addition to the traditional server verification, the system can also verify the identity of a user at the username and password forms simultaneously. In the triple-identity authentication, the identifiers are entirely managed by the system without involvement of users or any third-party service, and they are concealed, incommunicable, inaccessible and independent of personal information. Thus, such truly unique identifiers are useless in online attacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19459v3</guid>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Suyun Borjigin</dc:creator>
    </item>
    <item>
      <title>Efficiently and Effectively: A Two-stage Approach to Balance Plaintext and Encrypted Text for Traffic Classification</title>
      <link>https://arxiv.org/abs/2407.19687</link>
      <description>arXiv:2407.19687v2 Announce Type: replace 
Abstract: Encrypted traffic classification is the task of identifying the application or service associated with encrypted network traffic. One effective approach for this task is to use deep learning methods to encode the raw traffic bytes directly and automatically extract features for classification (byte-based models). However, current byte-based models input raw traffic bytes, whether plaintext or encrypted text, for automated feature extraction, neglecting the distinct impacts of plaintext and encrypted text on downstream tasks. Additionally, these models primarily focus on improving classification accuracy, with little emphasis on the efficiency of models. In this paper, for the first time, we analyze the impact of plaintext and encrypted text on the model's effectiveness and efficiency. Based on our observations and findings, we propose a two-phase approach to balance the trade-off between plaintext and encrypted text in traffic classification. Specifically, Stage one is to Determine whether the Plain text is enough to be accurately Classified (DPC) using the proposed DPC Selector. This stage quickly identifies samples that can be classified using plaintext, leveraging explicit byte features in plaintext to enhance model's efficiency. Stage two aims to adaptively make a classification with the result from stage one. This stage incorporates encrypted text information for samples that cannot be classified using plaintext alone, ensuring the model's effectiveness on traffic classification tasks. Experiments on two datasets demonstrate that our proposed model achieves state-of-the-art results in both effectiveness and efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19687v2</guid>
      <category>cs.CR</category>
      <category>cs.CL</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Peng</dc:creator>
    </item>
    <item>
      <title>PsybORG+: Modeling and Simulation for Detecting Cognitive Biases in Advanced Persistent Threats</title>
      <link>https://arxiv.org/abs/2408.01310</link>
      <description>arXiv:2408.01310v2 Announce Type: replace 
Abstract: Advanced Persistent Threats (APTs) bring significant challenges to cybersecurity due to their sophisticated and stealthy nature. Traditional cybersecurity measures fail to defend against APTs. Cognitive vulnerabilities can significantly influence attackers' decision-making processes, which presents an opportunity for defenders to exploit. This work introduces PsybORG$^+$, a multi-agent cybersecurity simulation environment designed to model APT behaviors influenced by cognitive vulnerabilities. A classification model is built for cognitive vulnerability inference and a simulator is designed for synthetic data generation. Results show that PsybORG$^+$ can effectively model APT attackers with different loss aversion and confirmation bias levels. The classification model has at least a 0.83 accuracy rate in predicting cognitive vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01310v2</guid>
      <category>cs.CR</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Huang, Fred Jones, Nikolos Gurney, David Pynadath, Kunal Srivastava, Stoney Trent, Peggy Wu, Quanyan Zhu</dc:creator>
    </item>
    <item>
      <title>Rag and Roll: An End-to-End Evaluation of Indirect Prompt Manipulations in LLM-based Application Frameworks</title>
      <link>https://arxiv.org/abs/2408.05025</link>
      <description>arXiv:2408.05025v2 Announce Type: replace 
Abstract: Retrieval Augmented Generation (RAG) is a technique commonly used to equip models with out of distribution knowledge. This process involves collecting, indexing, retrieving, and providing information to an LLM for generating responses. Despite its growing popularity due to its flexibility and low cost, the security implications of RAG have not been extensively studied. The data for such systems are often collected from public sources, providing an attacker a gateway for indirect prompt injections to manipulate the responses of the model. In this paper, we investigate the security of RAG systems against end-to-end indirect prompt manipulations. First, we review existing RAG framework pipelines, deriving a prototypical architecture and identifying critical parameters. We then examine prior works searching for techniques that attackers can use to perform indirect prompt manipulations. Finally, we implemented Rag 'n Roll, a framework to determine the effectiveness of attacks against end-to-end RAG applications. Our results show that existing attacks are mostly optimized to boost the ranking of malicious documents during the retrieval phase. However, a higher rank does not immediately translate into a reliable attack. Most attacks, against various configurations, settle around a 40% success rate, which could rise to 60% when considering ambiguous answers as successful attacks (those that include the expected benign one as well). Additionally, when using unoptimized documents, attackers deploying two of them (or more) for a target query can achieve similar results as those using optimized ones. Finally, exploration of the configuration space of a RAG showed limited impact in thwarting the attacks, where the most successful combination severely undermines functionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05025v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gianluca De Stefano, Lea Sch\"onherr, Giancarlo Pellegrino</dc:creator>
    </item>
    <item>
      <title>Tackling the Local Bias in Federated Graph Learning</title>
      <link>https://arxiv.org/abs/2110.12906</link>
      <description>arXiv:2110.12906v2 Announce Type: replace-cross 
Abstract: Federated graph learning (FGL) has become an important research topic in response to the increasing scale and the distributed nature of graph-structured data in the real world. In FGL, a global graph is distributed across different clients, where each client holds a subgraph. Existing FGL methods often fail to effectively utilize cross-client edges, losing structural information during the training; additionally, local graphs often exhibit significant distribution divergence. These two issues make local models in FGL less desirable than in centralized graph learning, namely the local bias problem in this paper. To solve this problem, we propose a novel FGL framework to make the local models similar to the model trained in a centralized setting. Specifically, we design a distributed learning scheme, fully leveraging cross-client edges to aggregate information from other clients. In addition, we propose a label-guided sampling approach to alleviate the imbalanced local data and meanwhile, distinctly reduce the training overhead. Extensive experiments demonstrate that local bias can compromise the model performance and slow down the convergence during training. Experimental results also verify that our framework successfully mitigates local bias, achieving better performance than other baselines with lower time and memory overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.12906v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Binchi Zhang, Minnan Luo, Shangbin Feng, Ziqi Liu, Jun Zhou, Qinghua Zheng</dc:creator>
    </item>
    <item>
      <title>Graph Agent Network: Empowering Nodes with Decentralized Communications Capabilities for Adversarial Resilience</title>
      <link>https://arxiv.org/abs/2306.06909</link>
      <description>arXiv:2306.06909v2 Announce Type: replace-cross 
Abstract: End-to-end training with global optimization have popularized graph neural networks (GNNs) for node classification, yet inadvertently introduced vulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit the inherent opened interfaces of GNNs' input and output, perturbing critical edges and thus manipulating the classification results. Current defenses, due to their persistent utilization of global-optimization-based end-to-end training schemes, inherently encapsulate the vulnerabilities of GNNs. This is specifically evidenced in their inability to defend against targeted secondary attacks. In this paper, we propose the Graph Agent Network (GAgN) to address the aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent network in which each node is designed as an 1-hop-view agent. Through the decentralized interactions between agents, they can learn to infer global perceptions to perform tasks including inferring embeddings, degrees and neighbor relationships for given nodes. This empowers nodes to filtering adversarial edges while carrying out classification tasks. Furthermore, agents' limited view prevents malicious messages from propagating globally in GAgN, thereby resisting global-optimization-based secondary attacks. We prove that single-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient to achieve these functionalities. Experimental results show that GAgN effectively implements all its intended capabilities and, compared to state-of-the-art defenses, achieves optimal classification accuracy on the perturbed datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06909v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.NE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ao Liu, Wenshan Li, Tao Li, Beibei Li, Guangquan Xu, Pan Zhou, Wengang Ma, Hanyuan Huang</dc:creator>
    </item>
    <item>
      <title>Sui Lutris: A Blockchain Combining Broadcast and Consensus</title>
      <link>https://arxiv.org/abs/2310.18042</link>
      <description>arXiv:2310.18042v4 Announce Type: replace-cross 
Abstract: Sui Lutris is the first smart-contract platform to sustainably achieve sub-second finality. It achieves this significant decrease by employing consensusless agreement not only for simple payments but for a large variety of transactions. Unlike prior work, Sui Lutris neither compromises expressiveness nor throughput and can run perpetually without restarts. Sui Lutris achieves this by safely integrating consensuless agreement with a high-throughput consensus protocol that is invoked out of the critical finality path but ensures that when a transaction is at risk of inconsistent concurrent accesses, its settlement is delayed until the total ordering is resolved. Building such a hybrid architecture is especially delicate during reconfiguration events, where the system needs to preserve the safety of the consensusless path without compromising the long-term liveness of potentially misconfigured clients. We thus develop a novel reconfiguration protocol, the first to provably show the safe and efficient reconfiguration of a consensusless blockchain. Sui Lutris is currently running in production and underpins the Sui smart-contract platform. Combined with the use of Objects instead of accounts it enables the safe execution of smart contracts that expose objects as a first-class resource. In our experiments Sui Lutris achieves latency lower than 0.5 seconds for throughput up to 5,000 certificates per second (150k ops/s with transaction blocks), compared to the state-of-the-art real-world consensus latencies of 3 seconds. Furthermore, it gracefully handles validators crash-recovery and does not suffer visible performance degradation during reconfiguration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.18042v4</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Blackshear, Andrey Chursin, George Danezis, Anastasios Kichidis, Lefteris Kokoris-Kogias, Xun Li, Mark Logan, Ashok Menon, Todd Nowacki, Alberto Sonnino, Brandon Williams, Lu Zhang</dc:creator>
    </item>
    <item>
      <title>Private Fine-tuning of Large Language Models with Zeroth-order Optimization</title>
      <link>https://arxiv.org/abs/2401.04343</link>
      <description>arXiv:2401.04343v2 Announce Type: replace-cross 
Abstract: Differentially private stochastic gradient descent (DP-SGD) allows models to be trained in a privacy-preserving manner, but has proven difficult to scale to the era of foundation models. We introduce DP-ZO, a private fine-tuning framework for large language models by privatizing zeroth order optimization methods. A key insight into the design of our method is that the direction of the gradient in the zeroth-order optimization we use is random and the only information from training data is the step size, i.e., a scalar. Therefore, we only need to privatize the scalar step size, which is memory-efficient. DP-ZO provides a strong privacy-utility trade-off across different tasks, and model sizes that are comparable to DP-SGD in $(\varepsilon,\delta)$-DP. Notably, DP-ZO possesses significant advantages over DP-SGD in memory efficiency, and obtains higher utility in $\varepsilon$-DP when using the Laplace mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.04343v2</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xinyu Tang, Ashwinee Panda, Milad Nasr, Saeed Mahloujifar, Prateek Mittal</dc:creator>
    </item>
    <item>
      <title>Protecting Copyrighted Material with Unique Identifiers in Large Language Model Training</title>
      <link>https://arxiv.org/abs/2403.15740</link>
      <description>arXiv:2403.15740v2 Announce Type: replace-cross 
Abstract: A major public concern regarding the training of large language models (LLMs) is whether they abusing copyrighted online text. Previous membership inference methods may be misled by similar examples in vast amounts of training data. Additionally, these methods are often too complex for general users to understand and use, making them centralized, lacking transparency, and trustworthiness. To address these issues, we propose an alternative \textit{insert-and-detection} methodology, advocating that web users and content platforms employ \textbf{\textit{unique identifiers}} for reliable and independent membership inference. Users and platforms can create their own identifiers, embed them in copyrighted text, and independently detect them in future LLMs. As an initial demonstration, we introduce \textit{ghost sentences}, a primitive form of unique identifiers, consisting primarily of passphrases made up of random words. By embedding one ghost sentences in a few copyrighted texts, users can detect its membership using a perplexity test and a \textit{user-friendly} last-$k$ words test. The perplexity test is based on the fact that LLMs trained on natural language should exhibit high perplexity when encountering unnatural passphrases. As the repetition increases, users can leverage the verbatim memorization ability of LLMs to perform a last-$k$ words test by chatting with LLMs without writing any code. Both tests offer rigorous statistical guarantees for membership inference. For LLaMA-13B, a perplexity test on 30 ghost sentences with an average of 7 repetitions in 148K examples yields a 0.891 ROC AUC. For the last-$k$ words test with OpenLLaMA-3B, 11 out of 16 users, with an average of 24 examples each, successfully identify their data from 1.8M examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15740v2</guid>
      <category>cs.CL</category>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shuai Zhao, Linchao Zhu, Ruijie Quan, Yi Yang</dc:creator>
    </item>
    <item>
      <title>SAMM: Sharded Automated Market Maker</title>
      <link>https://arxiv.org/abs/2406.05568</link>
      <description>arXiv:2406.05568v3 Announce Type: replace-cross 
Abstract: Automated Market Makers (AMMs) are a cornerstone of decentralized finance (DeFi) blockchain-based platforms. They enable direct exchange of virtual tokens: Traders exchange tokens with the AMM, paying a fee; liquidity comes from liquidity providers, paid by those fees. Despite growing demand, the performance of AMMs is limited. State-of-the-art blockchain platforms allow for parallel execution of transactions. However, we show that AMMs do not enjoy these gains since their operations are not parallelizable.
  We present SAMM, an AMM comprising multiple independent shards. All shards operate in the same chain, but they allow for parallel execution as each is independent. The challenge is that traders are incentivized to split each trade among all AMMs in existing designs, leading to lower throughput. SAMM addresses this issue with a novel design of the trading fees. Traders are incentivized to use only a single smallest shard. We show that all Subgame-Perfect Nash Equilibria (SPNE) fit the desired behavior: Liquidity providers balance the liquidity among all shards, so the system converges to the state where trades are evenly distributed, overcoming destabilization attacks.
  Evaluation in the Sui and Solana blockchains shows that SAMM improves throughput by 5 and by 16, respectively, approaching their limit. SAMM is directly deployable, allowing trading at scale for individuals and DeFi applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05568v3</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongyin Chen, Amit Vaisman, Ittay Eyal</dc:creator>
    </item>
  </channel>
</rss>
