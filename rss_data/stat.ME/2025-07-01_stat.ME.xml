<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 01 Jul 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fair Box ordinate transform for forecasts following a multivariate Gaussian law</title>
      <link>https://arxiv.org/abs/2506.22601</link>
      <description>arXiv:2506.22601v1 Announce Type: new 
Abstract: Monte Carlo techniques are the method of choice for making probabilistic predictions of an outcome in several disciplines. Usually, the aim is to generate calibrated predictions which are statistically indistinguishable from the outcome. Developers and users of such Monte Carlo predictions are interested in evaluating the degree of calibration of the forecasts. Here, we consider predictions of $p$-dimensional outcomes sampling a multivariate Gaussian distribution and apply the Box ordinate transform (BOT) to assess calibration. However, this approach is known to fail to reliably indicate calibration when the sample size n is moderate. For some applications, the cost of obtaining Monte-Carlo estimates is significant, which can limit the sample size, for instance, in model development when the model is improved iteratively. Thus, it would be beneficial to be able to reliably assess calibration even if the sample size n is moderate. To address this need, we introduce a fair, sample size- and dimension-dependent version of the Gaussian sample BOT. In a simulation study, the fair Gaussian sample BOT is compared with alternative BOT versions for different miscalibrations and for different sample sizes. Results confirm that the fair Gaussian sample BOT is capable of correctly identifying miscalibration when the sample size is moderate in contrast to the alternative BOT versions. Subsequently, the fair Gaussian sample BOT is applied to two to 12-dimensional predictions of temperature and vector wind using operational ensemble forecasts of the European Centre for Medium-Range Weather Forecasts (ECMWF). Firstly, perfectly reliable situations are considered where the outcome is replaced by a forecast that samples the same distribution as the members in the ensemble. Secondly, the BOT is computed using estimates of the actual temperature and vector wind from ECMWF analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22601v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'andor Baran, Martin Leutbecher</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit Tests for Combined Unilateral and Bilateral Data</title>
      <link>https://arxiv.org/abs/2506.22605</link>
      <description>arXiv:2506.22605v1 Announce Type: new 
Abstract: Clinical trials involving paired organs often yield a mixture of unilateral and bilateral data, where each subject may contribute either one or two responses under certain circumstances. While unilateral responses from different individuals can be treated as independent, bilateral responses from the same individual are likely correlated. Various statistical methods have been developed to account for this intra-subject correlation in the bilateral data, and in practice it is crucial to select an appropriate model for accurate inference. Tang et. al. (2012) discussed model selection issues using a variety of goodness-of-fit test statistics for correlated bilateral data for two groups, and Liu and Ma (2020) extended these methods to settings with $g\ge2$ groups.
  In this work, we investigate the goodness-of-fit statistics for the combined unilateral and bilateral data under different statistical models that address the intra-subject correlation, including the Clayton copula model, in addition to those considered in prior studies. Simulation results indicate that the performance of the goodness-of-fit tests is model-dependent, especially when the sample size is small and/or the intra-subject correlation is high, which is consistent with the findings by Liu and Ma (2020) for purely bilateral data. Applications to real data from otolaryngologic and ophthalmologic studies are included.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22605v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jia Zhou, Chang-Xing Ma</dc:creator>
    </item>
    <item>
      <title>When do composite estimands answer non-causal questions?</title>
      <link>https://arxiv.org/abs/2506.22610</link>
      <description>arXiv:2506.22610v1 Announce Type: new 
Abstract: Under a composite estimand strategy, the occurrence of the intercurrent event is incorporated into the endpoint definition, for instance by assigning a poor outcome value to patients who experience the event. Composite strategies are sometimes used for intercurrent events that result in changes to assigned treatment, such as treatment discontinuation or use of rescue medication. Here, we show that a composite strategy for these types of intercurrent events can lead to the outcome being defined differently between treatment arms, resulting in estimands that are not based on causal comparisons. This occurs when the intercurrent event can be categorised, such as based on its timing, and at least one category applies to one treatment arm only. For example, in a trial comparing a 6 vs. 12-month treatment regimen on an "unfavourable" outcome, treatment discontinuation can be categorised as occurring between 0-6 or 6-12 months. A composite strategy then results in treatment discontinuations between 6-12 months being part of the outcome definition in the 12-month arm, but not the 6-month arm. Using a simulation study, we show that this can dramatically affect conclusions; for instance, in a scenario where the intervention had no direct effect on either a clinical outcome or occurrence of the intercurrent event, a composite strategy led to an average risk difference of -10% and rejected the null hypothesis almost 90% of the time. We conclude that a composite strategy should not be used if it results in different outcome definitions being used across treatment arms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22610v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brennan C Kahan, Tra My Pham, Conor Tweed, Tim P Morris</dc:creator>
    </item>
    <item>
      <title>Doubly robust estimation of causal effects for random object outcomes with continuous treatments</title>
      <link>https://arxiv.org/abs/2506.22754</link>
      <description>arXiv:2506.22754v1 Announce Type: new 
Abstract: Causal inference is central to statistics and scientific discovery, enabling researchers to identify cause-and-effect relationships beyond associations. While traditionally studied within Euclidean spaces, contemporary applications increasingly involve complex, non-Euclidean data structures that reside in abstract metric spaces, known as random objects, such as images, shapes, networks, and distributions. This paper introduces a novel framework for causal inference with continuous treatments applied to non-Euclidean data. To address the challenges posed by the lack of linear structures, we leverage Hilbert space embeddings of the metric spaces to facilitate Fr\'echet mean estimation and causal effect mapping. Motivated by a study on the impact of exposure to fine particulate matter on age-at-death distributions across U.S. counties, we propose a nonparametric, doubly-debiased causal inference approach for outcomes as random objects with continuous treatments. Our framework can accommodate moderately high-dimensional vector-valued confounders and derive efficient influence functions for estimation to ensure both robustness and interpretability. We establish rigorous asymptotic properties of the cross-fitted estimators and employ conformal inference techniques for counterfactual outcome prediction. Validated through numerical experiments and applied to real-world environmental data, our framework extends causal inference methodologies to complex data structures, broadening its applicability across scientific disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22754v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satarupa Bhattacharjee, Bing Li, Xiao Wu, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Deep Semiparametric Partial Differential Equation Models</title>
      <link>https://arxiv.org/abs/2506.22779</link>
      <description>arXiv:2506.22779v1 Announce Type: new 
Abstract: In many scientific fields, the generation and evolution of data are governed by partial differential equations (PDEs) which are typically informed by established physical laws at the macroscopic level to describe general and predictable dynamics. However, some complex influences may not be fully captured by these laws at the microscopic level due to limited scientific understanding. This work proposes a unified framework to model, estimate, and infer the mechanisms underlying data dynamics. We introduce a general semiparametric PDE (SemiPDE) model that combines interpretable mechanisms based on physical laws with flexible data-driven components to account for unknown effects. The physical mechanisms enhance the SemiPDE model's stability and interpretability, while the data-driven components improve adaptivity to complex real-world scenarios. A deep profiling M-estimation approach is proposed to decouple the solutions of PDEs in the estimation procedure, leveraging both the accuracy of numerical methods for solving PDEs and the expressive power of neural networks. For the first time, we establish a semiparametric inference method and theory for deep M-estimation, considering both training dynamics and complex PDE models. We analyze how the PDE structure affects the convergence rate of the nonparametric estimator, and consequently, the parametric efficiency and inference procedure enable the identification of interpretable mechanisms governing data dynamics. Simulated and real-world examples demonstrate the effectiveness of the proposed methodology and support the theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22779v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziyuan Chen, Shunxing Yan, Fang Yao</dc:creator>
    </item>
    <item>
      <title>The Flexible Accumulation Model for High Density Temporal Exposures</title>
      <link>https://arxiv.org/abs/2506.22805</link>
      <description>arXiv:2506.22805v1 Announce Type: new 
Abstract: Emerging technologies enable continuous monitoring of temporal exposures to disease risk factors, leading to complex structures in the exposure process that consists of a subject-specific number and duration of exposure episodes. A key scientific question is how does the number and duration of episodic exposure affect disease risk. We address this question by introducing the FLexible Accumulation ModEl (FLAME) and the associated inferential tools, whose finite sample performance is evaluated through comprehensive simulations. FLAME is motivated by and applied to quantifying the association between hypotensive exposure during cardiac surgery and acute kidney injury (AKI). Our results characterize the AKI risk accumulation pattern as a function of hypotensive duration and shows that while 60 one-minute episodes is associated with an AKI probability of 0.23, a single sustained sixty-minute hypotensive episode raises that probability to 0.32, a 37\% increase despite the same total duration. These results offer direct guidance for improving hemodynamics risk management strategies during intraoperative care. Our method is accompanied by the R package flame.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22805v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xinkai Zhou, Lee Goeddel, Nauder Faraday, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Robust estimation of optimal dynamic treatment regimes with nonignorable missing covariates</title>
      <link>https://arxiv.org/abs/2506.22892</link>
      <description>arXiv:2506.22892v1 Announce Type: new 
Abstract: Estimating optimal dynamic treatment regimes (DTRs) using observational data is often challenged by nonignorable missing covariates arsing from informative monitoring of patients in clinical practice. To address nonignorable missingness of pseudo-outcomes induced by nonignorable missing covariates, a weighted Q-learning approach using parametric Q-function models and a semiparametric missingness propensity model has recently been proposed. However, misspecification of parametric Q-functions at later stages of a DTR can propagate estimation errors to earlier stages via the pseudo-outcomes themselves and indirectly through biased estimation of the missingness propensity of the pseudo-outcomes. This robustness concern motivates us to develop a direct-search-based optimal DTR estimator built on a robust and efficient value estimator, where nonparametric methods are employed for treatment propensity and Q-function estimation, and inverse probability weighting is applied using missingness propensity estimated with the aid of nonresponse instrumental variables. Specifically, in our value estimator, we replace weights estimated by prediction models of treatment propensity with stable weights estimated by balancing covariate functions in a reproducing-kernel Hilbert space (RKHS). Augmented by Q-functions estimated by RKHS-based smoothing splines, our value estimator mitigates the misspecification risk of the weighted Q-learning approach while maintaining the efficiency gain from employing pseudo-outcomes in missing data scenarios. The asymptotic properties of the proposed estimator are derived, and simulations demonstrate its superior performance over weighted Q-learning under model misspecification. We apply the proposed methods to investigate the optimal fluid strategy for sepsis patients using data from the MIMIC database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22892v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Sun, Bo Fu, Li Su</dc:creator>
    </item>
    <item>
      <title>Semi-tail Units: A Universal Scale for Test Statistics and Efficiency</title>
      <link>https://arxiv.org/abs/2506.22910</link>
      <description>arXiv:2506.22910v1 Announce Type: new 
Abstract: We introduce $\zeta$- and $s$-values as quantile-based standardizations that are particularly suited for hypothesis testing. Unlike p-values, which express tail probabilities, $s$-values measure the number of semi-tail units into a distribution's tail, where each unit represents a halving of the tail area. This logarithmic scale provides intuitive interpretation: $s=3.3$ corresponds to the 10th percentile, $s=4.3$ to the 5th percentile, and $s=5.3$ to the 2.5th percentile. For two-tailed tests, $\zeta$-values extend this concept symmetrically around the median.
  We demonstrate how these measures unify the interpretation of all test statistics on a common scale, eliminating the need for distribution-specific tables. The approach offers practical advantages: critical values follow simple arithmetic progressions, combining evidence from independent studies reduces to the addition of $s$-values, and semi-tail units provide the natural scale for expressing Bahadur slopes. This leads to a new asymptotic efficiency measure based on differences rather than ratios of slopes, where a difference of 0.15 semi-tail units means that the more efficient test moves samples 10\% farther into the tail. Through examples ranging from standardized test scores to poker hand rankings, we show how semi-tail units provide a natural and interpretable scale for quantifying extremeness in any ordered distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22910v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul W. Vos</dc:creator>
    </item>
    <item>
      <title>Confidence sequences with informative, bounded-influence priors</title>
      <link>https://arxiv.org/abs/2506.22925</link>
      <description>arXiv:2506.22925v1 Announce Type: new 
Abstract: Confidence sequences are collections of confidence regions that simultaneously cover the true parameter for every sample size at a prescribed confidence level. Tightening these sequences is of practical interest and can be achieved by incorporating prior information through the method of mixture martingales. However, confidence sequences built from informative priors are vulnerable to misspecification and may become vacuous when the prior is poorly chosen. We study this trade-off for Gaussian observations with known variance. By combining the method of mixtures with a global informative prior whose tails are polynomial or exponential and the extended Ville's inequality, we construct confidence sequences that are sharper than their non-informative counterparts whenever the prior is well specified, yet remain bounded under arbitrary misspecification. The theory is illustrated with several classical priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22925v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefano Cortinovis, Valentin Kilian, Fran\c{c}ois Caron</dc:creator>
    </item>
    <item>
      <title>Imputing With Predictive Mean Matching Can Be Severely Biased When Values Are Missing At Random</title>
      <link>https://arxiv.org/abs/2506.22981</link>
      <description>arXiv:2506.22981v1 Announce Type: new 
Abstract: Predictive mean matching (PMM) is a popular imputation strategy that imputes missing values by borrowing observed values from other cases with similar expectations. We show that, unlike other imputation strategies, PMM is not guaranteed to be consistent -- and in fact can be severely biased -- when values are missing at random (when the probability a value is missing depends only on values that are observed).
  We demonstrate the bias in a simple situation where a complete variable $X$ is both strongly correlated with $Y$ and strongly predictive of whether $Y$ is missing. The bias in the estimated regression slope can be as large as 80 percent, and persists even when we reduce the correlation between $X$ and $Y$. To make the bias vanish, the sample must be large ($n$=1,000) \emph{and} $Y$ values must be missing independently of $X$ (i.e., missing completely at random).
  Compared to other imputation methods, it seems that PMM requires larger samples and is more sensitive to the pattern of missing values. We cannot recommend PMM as a default approach to imputation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22981v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul T. von Hippel</dc:creator>
    </item>
    <item>
      <title>Simultaneous Sieve Estimation and Inference for Time-Varying Nonlinear Time Series Regression</title>
      <link>https://arxiv.org/abs/2506.23069</link>
      <description>arXiv:2506.23069v1 Announce Type: new 
Abstract: In this paper, we investigate time-varying nonlinear time series regression for a broad class of locally stationary time series. First, we propose sieve nonparametric estimators for the time-varying regression functions that achieve uniform consistency. Second, we develop a unified simultaneous inferential theory to conduct both structural and exact form tests on these functions. Additionally, we introduce a multiplier bootstrap procedure for practical implementation. Our methodology and theory require only mild assumptions on the regression functions, allow for unbounded domain support, and effectively address the issue of identifiability for practical interpretation. Technically, we establish sieve approximation theory for 2-D functions in unbounded domains, prove two Gaussian approximation results for affine forms of high-dimensional locally stationary time series, and calculate critical values for the maxima of the Gaussian random field arising from locally stationary time series, which may be of independent interest. Numerical simulations and two data analyses support our results, and we have developed an $\mathtt{R}$ package, $\mathtt{SIMle}$, to facilitate implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23069v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiucai Ding, Zhou Zhou</dc:creator>
    </item>
    <item>
      <title>Some Results on Point Estimation of the Association Parameter of a Bivariate Frank Copula</title>
      <link>https://arxiv.org/abs/2506.23110</link>
      <description>arXiv:2506.23110v1 Announce Type: new 
Abstract: This work deals with estimation of the association parameter of a bivariate Frank Copula in a comprehensive way. Even though Frank Copula is a member of Archimedean class of copulas, and has been widely used in finance, relatively little attention has been paid to its association parameter from a statistical inferential point of view. Most of the existing works which have used Frank Copula have focused on estimating the parameter computationally, and then proceeded with its application in the applied fields, mostly in finance. Here, in this investigation, we have looked at the point estimation of the association parameter in a comprehensive manner, and studied three estimators in terms of bias, mean squared error (MSE), relative bias and relative MSE. It has been noted that in the neighborhood of zero, the method of moment estimators (MMEs) do perform well compared to the maximum likelihood estimator (MLE), even though the latter has the best overall performance. Further, in terms of bias, MMEs and MLE have opposite behavior. However, some of our results do not match with those reported by Genest (1987) \cite{Genest1987}. Nevertheless, this study complements Genest's (1987)\cite{Genest1987} expository work, and provides some interesting insights into the behaviors of three point estimators including the MLE whose asymptotic behavior holds pretty well, as we have found, for $n\ge 75$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23110v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yen-Anh Thi Pham, Huynh To Uyen, Nabendu Pal</dc:creator>
    </item>
    <item>
      <title>A simulation study comparing statistical approaches for estimating extreme quantile regression with an application to forecasting of fire risk</title>
      <link>https://arxiv.org/abs/2506.23161</link>
      <description>arXiv:2506.23161v1 Announce Type: new 
Abstract: This simulation study compares statistical approaches for estimating extreme quantile regression, with a specific application to fire risk forecasting. A simulation-based framework is designed to evaluate the effectiveness of different methods in capturing extreme dependence structures and accurately predicting extreme quantiles. These approaches are applied to fire occurrence data from the Fez-Meknes region, where a positive relationship is observed between increasing maximum temperatures and fire frequency. The study highlights the comparative performance of each technique and advocates for a hybrid strategy that combines their complementary strengths to enhance both the accuracy and interpretability of forecasts for extreme events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23161v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amina El Bernoussi, Mohamed El Arrouchi</dc:creator>
    </item>
    <item>
      <title>Causal Inference in Panel Data with a Continuous Treatment</title>
      <link>https://arxiv.org/abs/2506.23226</link>
      <description>arXiv:2506.23226v1 Announce Type: new 
Abstract: This paper proposes a framework that incorporates the two-way fixed effects model as a special case to conduct causal inference with a continuous treatment. Treatments are allowed to change over time and potential outcomes are dependent on historical treatments. Regression models on potential outcomes, along with the sequentially conditional independence assumptions (SCIAs) are introduced to identify the treatment effects, which are measured by aggre causal responses. Least squares and generalized method of moments (GMM) estimators are developed for model parameters, which are then used to estimate the aggregate causal effects. We establish the asymptotic properties of these aggregate estimators. Additionally, we propose employing directed acyclic graphs (DAGs) to test the validity of the SCIAs. An application examining the aid-growth relationship illustrates the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23226v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhiguo Xiao, Peikai Wu</dc:creator>
    </item>
    <item>
      <title>Auto-Doubly Robust Estimation of Causal Effects on a Network</title>
      <link>https://arxiv.org/abs/2506.23332</link>
      <description>arXiv:2506.23332v1 Announce Type: new 
Abstract: This paper develops new methods for causal inference in observational studies on a single large network of interconnected units, addressing two key challenges: long-range dependence among units and the presence of general interference. We introduce a novel network version of Augmented Inverse Propensity Weighted, which combines propensity score and outcome models defined on the network to achieve doubly robust identification and estimation of both direct and spillover causal effects. Under a network version of conditional ignorability, the proposed approach identifies the expected potential outcome for a unit given the treatment assignment vector for its network neighborhood up to a user-specified distance, while marginalizing over treatment assignments for the rest of the network. By introducing two additional assumptions on the outcome, we establish a new doubly robust identification result for the expected potential outcome under a hypothetical intervention on the treatment assignment vector for the entire network. Under a union of Chain Graph models - one governing the propensity model and the other the outcome model - we propose a corresponding semiparametric estimator based on parametric models naturally arising from the chain graph representation of the network. We formally prove that, under weak network dependence, the proposed estimators are asymptotically normal and we characterize the impact of model misspecification on the asymptotic variance. Extensive simulation studies highlight the practical relevance of our approach. We further demonstrate its application in an empirical analysis of the NNAHRAY study, evaluating the impact of incarceration on individual socioeconomic outcomes in Brooklyn, New York.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23332v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jizhou Liu, Dake Zhang, Eric J. Tchetgen Tchetgen</dc:creator>
    </item>
    <item>
      <title>Covariate-informed link prediction with extreme taxonomic bias</title>
      <link>https://arxiv.org/abs/2506.23370</link>
      <description>arXiv:2506.23370v1 Announce Type: new 
Abstract: Biotic interactions provide a valuable window into the inner workings of complex ecological communities and capture the loss of ecological function often precipitated by environmental change. However, the financial and logistical challenges associated with collecting interaction data result in networks that are recorded with geographical and taxonomic bias, particularly when studies are narrowly focused. We develop an approach to reduce bias in link prediction in the common scenario in which data are derived from studies focused on a small number of species. Our Extended Covariate-Informed Link Prediction (COIL+) framework utilizes a latent factor model that flexibly borrows information between species and incorporates dependence on covariates and phylogeny, and introduces a framework for borrowing information from multiple studies to reduce bias due to uncertain species occurrence. Additionally, we propose a new trait matching procedure which permits heterogeneity in trait-interaction propensity associations at the species level. We illustrate the approach through an application to a literature compilation data set of 268 sources reporting frugivory in Afrotropical forests and compare the performance with and without correction for uncertainty in occurrence. Our method results in a substantial improvement in link prediction, revealing 5,255 likely but unobserved frugivory interactions, and increasing model discrimination under conditions of great taxonomic bias and narrow study focus. This framework generalizes to a variety of network contexts and offers a useful tool for link prediction given networks recorded with bias.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23370v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jennifer N. Kampe, Camille DeSisto, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Zero-disparity Distribution Synthesis: Fast Exact Calculation of Chi-Squared Statistic Distribution for Discrete Uniform Histograms</title>
      <link>https://arxiv.org/abs/2506.23416</link>
      <description>arXiv:2506.23416v1 Announce Type: new 
Abstract: Pearson's chi-squared test is widely used to assess the uniformity of discrete histograms, typically relying on a continuous chi-squared distribution to approximate the test statistic, since computing the exact distribution is computationally too costly. While effective in many cases, this approximation allegedly fails when expected bin counts are low or tail probabilities are needed. Here, Zero-disparity Distribution Synthesis is presented, a fast dynamic programming approach for computing the exact distribution, enabling detailed analysis of approximation errors. The results dispel some existing misunderstandings and also reveal subtle, but significant pitfalls in approximation that are only apparent with exact values. The Python source code is available at https://github.com/DiscreteTotalVariation/ChiSquared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23416v1</guid>
      <category>stat.ME</category>
      <category>cs.MS</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikola Bani\'c, Neven Elezovi\'c</dc:creator>
    </item>
    <item>
      <title>Multiple Hypothesis Testing in Genomics</title>
      <link>https://arxiv.org/abs/2506.23428</link>
      <description>arXiv:2506.23428v1 Announce Type: new 
Abstract: This analysis report presents an in-depth exploration of multiple hypothesis testing in the context of Genomics RNA-seq differential expression (DE) analysis, with a primary focus on techniques designed to control the false discovery rate (FDR). While RNA-seq has become a cornerstone in transcriptomic research, accurately detecting expression changes remains challenging due to the high-dimensional nature of the data. This report delves into the Benjamini-Hochberg (BH) procedure, Benjamini-Yekutieli (BY) approach, and Storey's method, emphasizing their importance in addressing multiple testing issues and improving the reliability of results in large-scale genomic studies. We provide an overview of how these methods can be applied to control FDR while maintaining statistical power, and demonstrate their effectiveness through simulated data analysis.
  The discussion highlights the significance of using adaptive methods like Storey's q-value, particularly in high-dimensional datasets where traditional approaches may struggle. Results are presented through typical plots (e.g., Volcano, MA, PCA) and confusion matrices to visualize the impact of these techniques on gene discovery. The limitations section also touches on confounding factors like gene correlations and batch effects, which are often encountered in real-world data.
  Ultimately, the analysis achieves a robust framework for handling multiple hypothesis comparisons, offering insights into how these methods can be used to interpret complex gene expression data while minimizing errors. The report encourages further validation and exploration of these techniques in future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23428v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shyam Gupta</dc:creator>
    </item>
    <item>
      <title>New Tests of Randomness for Circular Data</title>
      <link>https://arxiv.org/abs/2506.23522</link>
      <description>arXiv:2506.23522v1 Announce Type: new 
Abstract: Randomness or mutual independence is an important underlying assumption for most widely used statistical methods for circular data. Verifying this assumption is essential to ensure the validity and reliability of the resulting inferences. In this paper, we introduce two tests for assessing the randomness assumption in circular statistics, based on random circular arc graphs (RCAGs). We define and analyze RCAGs in detail, showing that their key properties depend solely on the i.i.d. nature of the data and are independent of the particular underlying continuous circular distribution. Specifically, we derive the edge probability and vertex degree distribution of RCAGs under the randomness assumption. Using these results, we construct two tests: RCAG-EP, which is based on edge probability, and RCAG-DD, which relies on the vertex degree distribution. Through extensive simulations, we demonstrate that both tests are effective, with RCAG-DD often exhibiting higher power than RCAG-EP. Additionally, we explore several real-world applications where these tests can be useful.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23522v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shriya Gehlot, Arnab Kumar Laha</dc:creator>
    </item>
    <item>
      <title>An easily verifiable dispersion order for discrete distributions</title>
      <link>https://arxiv.org/abs/2506.23677</link>
      <description>arXiv:2506.23677v1 Announce Type: new 
Abstract: Dispersion is a fundamental concept in statistics, yet standard approaches to measuring it - especially via stochastic orders - face limitations in the discrete setting. In particular, the classical dispersive order, while well-established for continuous distributions, becomes overly restrictive when applied to discrete random variables due to support inclusion requirements. To address this, we propose a novel weak dispersive order tailored for discrete distributions. This order retains desirable properties while relaxing structural constraints, thereby broadening applicability. We further introduce a class of variability measures grounded in the notion of probability concentration, offering robust and interpretable alternatives that conform to classical axioms. Several empirical illustrations highlight the practical relevance of the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23677v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andreas Eberl, Bernhard Klar, Alfonso Su\'arez-Llorens</dc:creator>
    </item>
    <item>
      <title>Developing a Synthetic Socio-Economic Index through Autoencoders: Evidence from Florence's Suburban Areas</title>
      <link>https://arxiv.org/abs/2506.23849</link>
      <description>arXiv:2506.23849v1 Announce Type: new 
Abstract: The interest in summarizing complex and multidimensional phenomena often related to one or more specific sectors (social, economic, environmental, political, etc.) to make them easily understandable even to non-experts is far from waning. A widely adopted approach for this purpose is the use of composite indices, statistical measures that aggregate multiple indicators into a single comprehensive measure. In this paper, we present a novel methodology called AutoSynth, designed to condense potentially extensive datasets into a single synthetic index or a hierarchy of such indices. AutoSynth leverages an Autoencoder, a neural network technique, to represent a matrix of features in a lower-dimensional space. Although this approach is not limited to the creation of a particular composite index and can be applied broadly across various sectors, the motivation behind this work arises from a real-world need. Specifically, we aim to assess the vulnerability of the Italian city of Florence at the suburban level across three dimensions: economic, demographic, and social. To demonstrate the methodology's effectiveness, it is also applied to estimate a vulnerability index using a rich, publicly available dataset on U.S. counties and validated through a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23849v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giulio Grossi, Emilia Rocco</dc:creator>
    </item>
    <item>
      <title>Large Language Models for Statistical Inference: Context Augmentation with Applications to the Two-Sample Problem and Regression</title>
      <link>https://arxiv.org/abs/2506.23862</link>
      <description>arXiv:2506.23862v1 Announce Type: new 
Abstract: We introduce context augmentation, a data-augmentation approach that uses large language models (LLMs) to generate contexts around observed strings as a means of facilitating valid frequentist inference. These generated contexts serve to reintroduce uncertainty, incorporate auxiliary information, and facilitate interpretability. For example, in the two-sample test, we compare the log-probability of strings under contexts from its own versus the other group. We show on synthetic data that the method's t-statistics exhibit the expected null behaviour while maintaining power and, through a replication, that the method is powerful and interpretable. We next introduce text-on-text regression. Contexts generated around the predictor string are treated as mediating variables between the predictor and outcome strings. Using negative controls, we then distinguish between semantic and syntactic dimensions of prediction. Analysis of real-world dialogic data illustrates behaviour predicted from a psycholinguistic framework. Theoretically, we provide identification conditions, derive an influence-function decomposition, and show that repeated cross-fitting of a pivotal statistic yields higher-order efficiency. We derive bounds linking estimation error, context count, and number of cross-fits. Taken together, context augmentation offers the ability to connect LLMs with longstanding statistical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23862v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Ratkovic</dc:creator>
    </item>
    <item>
      <title>Upgrading survival models with CARE</title>
      <link>https://arxiv.org/abs/2506.23870</link>
      <description>arXiv:2506.23870v1 Announce Type: new 
Abstract: Clinical risk prediction models are regularly updated as new data, often with additional covariates, become available. We propose CARE (Convex Aggregation of relative Risk Estimators) as a general approach for combining existing "external" estimators with a new data set in a time-to-event survival analysis setting. Our method initially employs the new data to fit a flexible family of reproducing kernel estimators via penalised partial likelihood maximisation. The final relative risk estimator is then constructed as a convex combination of the kernel and external estimators, with the convex combination coefficients and regularisation parameters selected using cross-validation. We establish high-probability bounds for the $L_2$-error of our proposed aggregated estimator, showing that it achieves a rate of convergence that is at least as good as both the optimal kernel estimator and the best external model. Empirical results from simulation studies align with the theoretical results, and we illustrate the improvements our methods provide for cardiovascular disease risk modelling. Our methodology is implemented in the Python package care-survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23870v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William G. Underwood, Henry W. J. Reeve, Oliver Y. Feng, Samuel A. Lambert, Bhramar Mukherjee, Richard J. Samworth</dc:creator>
    </item>
    <item>
      <title>CoMMiT: Co-informed inference of microbiome-metabolome interactions via transfer learning</title>
      <link>https://arxiv.org/abs/2506.24013</link>
      <description>arXiv:2506.24013v1 Announce Type: new 
Abstract: Recent multi-omic microbiome studies enable integrative analysis of microbes and metabolites, uncovering their associations with various host conditions. Such analyses require multivariate models capable of accounting for the complex correlation structures between microbes and metabolites. However, existing multivariate models often suffer from low statistical power for detecting microbiome-metabolome interactions due to small sample sizes and weak biological signals. To address these challenges, we introduce CoMMiT, Co-informed inference of Microbiome-Metabolome Interactions via novel Transfer learning models. Unlike conventional transfer-learning methods that borrow information from external datasets, CoMMiT leverages similarities across metabolites within a single cohort, reducing the risk of negative transfer often caused by differences in sequencing platforms and bioinformatic pipelines across studies. CoMMiT operates under the flexible assumption that auxiliary metabolites are collectively informative for the target metabolite, without requiring individual auxiliary metabolites to be informative. CoMMiT uses a novel data-driven approach to selecting the optimal set of auxiliary metabolites. Using this optimal set, CoMMiT employs a de-biasing framework to enable efficient calculation of p-values, facilitating the identification of statistically significant microbiome-metabolome interactions. Applying CoMMiT to a feeding study reveals biologically meaningful microbiome-metabolome interactions under a low glycemic load diet, demonstrating the diet-host link through gut metabolism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24013v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leiyue Li, Chenglong Ye, Tim Randolph, Meredith Hullar, Johanna Lampe, Marian Neuhouser, Daniel Raftery, Yue Wang</dc:creator>
    </item>
    <item>
      <title>Sensitivity analysis method in the presence of a missing not at random ordinal independent variable</title>
      <link>https://arxiv.org/abs/2506.24025</link>
      <description>arXiv:2506.24025v1 Announce Type: new 
Abstract: Data analysis often encounters missing data, which can result in inaccurate conclusions, especially when it comes to ordinal variables. In trauma data, the Glasgow Coma Scale is useful for assessing the level of consciousness. This score is often missing in patients who are intubated or under sedation upon arrival at the hospital, and those with normal reactivity without head injury, suggesting a Missing Not At Random (MNAR) mechanism. The problem with MNAR is the absence of a definitive analysis. While sensitivity analysis is often recommended, practical limitations sometimes restrict the analysis to a basic comparison between results under Missing Completely At Random (MCAR) and Missing At Random (MAR) assumptions, disregarding MNAR plausibility. Our objective is to propose a flexible and accessible sensitivity analysis method in the presence of a MNAR ordinal independent variable. The method is inspired by the sensitivity analysis approach proposed by Leurent et al. (2018) for a continuous response variable. We propose an extension for an independent ordinal variable. The method is evaluated on simulated data before being applied to Pan-Canadian trauma data from April 2013 to March 2018. The simulation shows that MNAR estimates are less biased than MAR estimates and more precise than complete case analysis (CC) estimates. The confidence intervals coverage rates are relatively better for MNAR estimates than CC and MAR estimates. In the application, it is observed that the Glasgow Coma Scale is significant under MNAR, unlike MCAR and MAR assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24025v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdoulaye Dioni, Alexandre Bureau, Lynne Moore, Aida Eslami</dc:creator>
    </item>
    <item>
      <title>Controlling the false discovery rate under a non-parametric graphical dependence model</title>
      <link>https://arxiv.org/abs/2506.24126</link>
      <description>arXiv:2506.24126v1 Announce Type: new 
Abstract: We propose sufficient conditions and computationally efficient procedures for false discovery rate control in multiple testing when the $p$-values are related by a known \emph{dependency graph} -- meaning that we assume independence of $p$-values that are not within each other's neighborhoods, but otherwise leave the dependence unspecified. Our methods' rejection sets coincide with that of the Benjamini--Hochberg (BH) procedure whenever there are no edges between BH rejections, and we find in simulations and a genomics data example that their power approaches that of the BH procedure when there are few such edges, as is commonly the case. Because our methods ignore all hypotheses not in the BH rejection set, they are computationally efficient whenever that set is small. Our fastest method, the IndBH procedure, typically finishes within seconds even in simulations with up to one million hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24126v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Drew T. Nguyen, William Fithian</dc:creator>
    </item>
    <item>
      <title>Convergence to the Truth</title>
      <link>https://arxiv.org/abs/2410.11399</link>
      <description>arXiv:2410.11399v2 Announce Type: cross 
Abstract: This article reviews and develops an epistemological tradition in the philosophy of science, known as convergentism, which holds that inference methods should be assessed based on their ability to converge to the truth across a range of possible scenarios. Emphasis is placed on its historical origins in the work of C. S. Peirce and its recent developments in formal epistemology and data science (including statistics and machine learning). Comparisons are made with three other traditions: (1) explanationism, which holds that theory choice should be guided by a theory's overall balance of explanatory virtues, such as simplicity and fit with data; (2) instrumentalism, which maintains that scientific inference should be driven by the goal of obtaining useful models rather than true theories; and (3) Bayesianism, which shifts the focus from all-or-nothing beliefs to degrees of belief.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11399v2</guid>
      <category>stat.OT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Hanti Lin</dc:creator>
    </item>
    <item>
      <title>Strategic analysis of hydrogen market dynamics across collaboration models</title>
      <link>https://arxiv.org/abs/2506.22690</link>
      <description>arXiv:2506.22690v1 Announce Type: cross 
Abstract: The global energy landscape is experiencing a transformative shift, with an increasing emphasis on sustainable and clean energy sources. Hydrogen remains a promising candidate for decarbonization, energy storage, and as an alternative fuel. This study explores the landscape of hydrogen pricing and demand dynamics by evaluating three collaboration scenarios: market-based pricing, cooperative integration, and coordinated decision-making. It incorporates price-sensitive demand, environmentally friendly production methods, and market penetration effects, to provide insights into maximizing market share, profitability, and sustainability within the hydrogen industry. This study contributes to understanding the complexities of collaboration by analyzing those structures and their role in a fast transition to clean hydrogen production by balancing economic viability and environmental goals. The findings reveal that the cooperative integration strategy is the most effective for sustainable growth, increasing green hydrogen's market share to 19.06 % and highlighting the potential for environmentally conscious hydrogen production. They also suggest that the coordinated decision-making approach enhances profitability through collaborative tariff contracts while balancing economic viability and environmental goals. This study also underscores the importance of strategic pricing mechanisms, policy alignment, and the role of hydrogen hubs in achieving sustainable growth in the hydrogen sector. By highlighting the uncertainties and potential barriers, this research offers actionable guidance for policymakers and industry players in shaping a competitive and sustainable energy marketplace.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22690v1</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rser.2024.115001</arxiv:DOI>
      <dc:creator>Mohammad Asghari, Hamid Afshari, Mohamad Y Jaber, Cory Searcy</dc:creator>
    </item>
    <item>
      <title>FuzzCoh: Robust Canonical Coherence-Based Fuzzy Clustering of Multivariate Time Series</title>
      <link>https://arxiv.org/abs/2506.22861</link>
      <description>arXiv:2506.22861v1 Announce Type: cross 
Abstract: Brain cognitive and sensory functions are often associated with electrophysiological activity at specific frequency bands. Clustering multivariate time series (MTS) data like EEGs is important for understanding brain functions but challenging due to complex non-stationary cross-dependencies, gradual transitions between cognitive states, noisy measurements, and ambiguous cluster boundaries. To address these issues, we develop a robust fuzzy clustering framework in the spectral domain. Our method leverages Kendall's tau-based canonical coherence, which extracts meaningful frequency-specific monotonic relationships between groups of channels or regions. KenCoh effectively captures dominant coherence structures while remaining robust against outliers and noise, making it suitable for real EEG datasets that typically contain artifacts. Our method first projects each MTS object onto vectors derived from the KenCoh estimates (i.e, canonical directions), which capture relevant information on the connectivity structure of oscillatory signals in predefined frequency bands. These spectral features are utilized to determine clusters of epochs using a fuzzy partitioning strategy, accommodating gradual transitions and overlapping class structure. Lastly, we demonstrate the effectiveness of our approach to EEG data where latent cognitive states such as alertness and drowsiness exhibit frequency-specific dynamics and ambiguity. Our method captures both spectral and spatial features by locating the frequency-dependent structure and brain functional connectivity. Built on the KenCoh framework for fuzzy clustering, it handles the complexity of high-dimensional time series data and is broadly applicable to domains such as neuroscience, wearable sensing, environmental monitoring, and finance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22861v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziling Ma, Mara Sherlin Talento, Ying Sun, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Design-Based and Network Sampling-Based Uncertainties in Network Experiments</title>
      <link>https://arxiv.org/abs/2506.22989</link>
      <description>arXiv:2506.22989v1 Announce Type: cross 
Abstract: OLS estimators are widely used in network experiments to estimate spillover effects via regressions on exposure mappings that summarize treatment and network structure. We study the causal interpretation and inference of such OLS estimators when both design-based uncertainty in treatment assignment and sampling-based uncertainty in network links are present. We show that correlations among elements of the exposure mapping can contaminate the OLS estimand, preventing it from aggregating heterogeneous spillover effects for clear causal interpretation. We derive the estimator's asymptotic distribution and propose a network-robust variance estimator. Simulations and an empirical application reveal sizable contamination bias and inflated spillover estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.22989v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kensuke Sakamoto, Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>Training of Spiking Neural Networks with Expectation-Propagation</title>
      <link>https://arxiv.org/abs/2506.23757</link>
      <description>arXiv:2506.23757v1 Announce Type: cross 
Abstract: In this paper, we propose a unifying message-passing framework for training spiking neural networks (SNNs) using Expectation-Propagation. Our gradient-free method is capable of learning the marginal distributions of network parameters and simultaneously marginalizes nuisance parameters, such as the outputs of hidden layers. This framework allows for the first time, training of discrete and continuous weights, for deterministic and stochastic spiking networks, using batches of training samples. Although its convergence is not ensured, the algorithm converges in practice faster than gradient-based methods, without requiring a large number of passes through the training data. The classification and regression results presented pave the way for new efficient training methods for deep Bayesian networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23757v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dan Yao, Steve McLaughlin, Yoann Altmann</dc:creator>
    </item>
    <item>
      <title>Minimax and Bayes Optimal Best-arm Identification: Adaptive Experimental Design for Treatment Choice</title>
      <link>https://arxiv.org/abs/2506.24007</link>
      <description>arXiv:2506.24007v1 Announce Type: cross 
Abstract: This study investigates adaptive experimental design for treatment choice, also known as fixed-budget best-arm identification. We consider an adaptive procedure consisting of a treatment-allocation phase followed by a treatment-choice phase, and we design an adaptive experiment for this setup to efficiently identify the best treatment arm, defined as the one with the highest expected outcome. In our designed experiment, the treatment-allocation phase consists of two stages. The first stage is a pilot phase, where we allocate each treatment arm uniformly with equal proportions to eliminate clearly suboptimal arms and estimate outcome variances. In the second stage, we allocate treatment arms in proportion to the variances estimated in the first stage. After the treatment-allocation phase, the procedure enters the treatment-choice phase, where we choose the treatment arm with the highest sample mean as our estimate of the best treatment arm. We prove that this single design is simultaneously asymptotically minimax and Bayes optimal for the simple regret, with upper bounds that match our lower bounds up to exact constants. Therefore, our designed experiment achieves the sharp efficiency limits without requiring separate tuning for minimax and Bayesian objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.24007v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>A Nonparametric Maximum Likelihood Approach to Mixture of Regression</title>
      <link>https://arxiv.org/abs/2108.09816</link>
      <description>arXiv:2108.09816v2 Announce Type: replace 
Abstract: We study mixture of linear regression (random coefficient) models, which capture population heterogeneity by allowing the regression coefficients to follow an unknown distribution $G^*$. In contrast to common parametric methods that fix the mixing distribution form and rely on the EM algorithm, we develop a fully nonparametric maximum likelihood estimator (NPMLE). We show that this estimator exists under broad conditions and can be computed via a discrete approximation procedure inspired by the exemplar method. We further establish theoretical guarantees demonstrating that the NPMLE achieves near-parametric rates in estimating the conditional density of $Y|X$, both for fixed and random designs, when $\sigma$ is known and $G^*$ has compact support. In the random design setting, we also prove consistency of the estimated mixing distribution in the L\'evy-Prokhorov distance. Numerical experiments indicate that our approach performs well and additionally enables posterior-based individualized coefficient inference through an empirical Bayes framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.09816v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansheng Jiang, Adityanand Guntuboyina</dc:creator>
    </item>
    <item>
      <title>Annealed Leap-Point Sampler for Multimodal Target Distributions</title>
      <link>https://arxiv.org/abs/2112.12908</link>
      <description>arXiv:2112.12908v2 Announce Type: replace 
Abstract: In Bayesian statistics, exploring high-dimensional multimodal posterior distributions poses major challenges for existing MCMC approaches. This paper introduces the Annealed Leap-Point Sampler (ALPS), which augments the target distribution state space with modified annealed (cooled) distributions, in contrast to traditional tempering approaches. The coldest state is chosen such that its annealed density is well-approximated locally by a Laplace approximation. This allows for automated setup of a scalable mode-leaping independence sampler. ALPS requires an exploration component to search for the mode locations, which can either be run adaptively in parallel to improve these mode-jumping proposals, or else as a pre-computation step. A theoretical analysis shows that for a d-dimensional problem the coolest temperature level required only needs to be linear in dimension, $\mathcal{O}(d)$, implying that the number of iterations needed for ALPS to converge is $\mathcal{O}(d)$ (typically leading to overall complexity $\mathcal{O}(d^3)$ when computational cost per iteration is taken into account). ALPS is illustrated on several complex, multimodal distributions that arise from real-world applications. This includes a seemingly-unrelated regression (SUR) model of longitudinal data from U.S. manufacturing firms, as well as a spectral density model that is used in analytical chemistry for identification of molecular biomarkers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.12908v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicholas G. Tawn, Matthew T. Moores, Hugo Queniat, Gareth O. Roberts</dc:creator>
    </item>
    <item>
      <title>Nonstationary Spatial Process Models with Spatially Varying Covariance Kernels</title>
      <link>https://arxiv.org/abs/2203.11873</link>
      <description>arXiv:2203.11873v4 Announce Type: replace 
Abstract: Building spatial process models that capture nonstationary behavior while delivering computationally efficient inference is challenging. Nonstationary spatially varying kernels (see, e.g., Paciorek, 2003) offer flexibility and richness, but computation is impeded by high-dimensional parameter spaces resulting from spatially varying process parameters. Matters are exacerbated if the number of locations recording measurements is massive. With limited theoretical tractability, obviating computational bottlenecks requires synergy between model construction and algorithm development. We build a class of scalable nonstationary spatial process models using spatially varying covariance kernels. We implement a Bayesian modeling framework using Hybrid Monte Carlo with nested interweaving. We conduct experiments on synthetic data sets to explore model selection and parameter identifiability, and assess inferential improvements accrued from nonstationary modeling. We illustrate strengths and pitfalls with a data set on remote sensed normalized difference vegetation index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2203.11873v4</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>S\'ebastien Coube-Sisqueille, Sudipto Banerjee, Beno\^it Liquet</dc:creator>
    </item>
    <item>
      <title>Conformal inference for random objects</title>
      <link>https://arxiv.org/abs/2405.00294</link>
      <description>arXiv:2405.00294v2 Announce Type: replace 
Abstract: We develop an inferential toolkit for analyzing object-valued responses, which correspond to data situated in general metric spaces, paired with Euclidean predictors within the conformal framework. To this end we introduce conditional profile average transport costs, where we compare distance profiles that correspond to one-dimensional distributions of probability mass falling into balls of increasing radius through the optimal transport cost when moving from one distance profile to another. The average transport cost to transport a given distance profile to all others is crucial for statistical inference in metric spaces and underpins the proposed conditional profile scores. A key feature of the proposed approach is to utilize the distribution of conditional profile average transport costs as conformity score for general metric space-valued responses, which facilitates the construction of prediction sets by the split conformal algorithm. We derive the uniform convergence rate of the proposed conformity score estimators and establish asymptotic conditional validity for the prediction sets. The finite sample performance for synthetic data in various metric spaces demonstrates that the proposed conditional profile score outperforms existing methods in terms of both coverage level and size of the resulting prediction sets, even in the special case of scalar Euclidean responses. We also demonstrate the practical utility of conditional profile scores for network data from New York taxi trips and for compositional data reflecting energy sourcing of U.S. states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00294v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1214/25-AOS2495</arxiv:DOI>
      <dc:creator>Hang Zhou, Hans-Georg M\"uller</dc:creator>
    </item>
    <item>
      <title>The inverse Kalman filter</title>
      <link>https://arxiv.org/abs/2407.10089</link>
      <description>arXiv:2407.10089v5 Announce Type: replace 
Abstract: We introduce the inverse Kalman filter, which enables exact matrix-vector multiplication between a covariance matrix from a dynamic linear model and any real-valued vector with linear computational cost. We integrate the inverse Kalman filter with the conjugate gradient algorithm, which substantially accelerates the computation of matrix inversion for a general form of covariance matrix, where other approximation approaches may not be directly applicable. We demonstrate the scalability and efficiency of the proposed approach through applications in nonparametric estimation of particle interaction functions, using both simulations and cell trajectories from microscopy data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10089v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyi Fang, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>Neymanian inference in randomized experiments</title>
      <link>https://arxiv.org/abs/2409.12498</link>
      <description>arXiv:2409.12498v2 Announce Type: replace 
Abstract: In his seminal 1923 work, Neyman studied the variance estimation problem for the difference-in-means estimator of the average treatment effect in completely randomized experiments. He proposed a variance estimator that is conservative in general and unbiased under homogeneous treatment effects. While widely used under complete randomization, there is no unique or natural way to extend this estimator to more complex designs. To this end, we show that Neyman's estimator can be alternatively derived in two ways, leading to two novel variance estimation approaches: the imputation approach and the contrast approach. While both approaches recover Neyman's estimator under complete randomization, they yield fundamentally different variance estimators for more general designs. In the imputation approach, the variance is expressed in terms of observed and missing potential outcomes and then estimated by imputing the missing potential outcomes, akin to Fisherian inference. In the contrast approach, the variance is expressed in terms of unobservable contrasts of potential outcomes and then estimated by exchanging each unobservable contrast with an observable contrast. We examine the properties of both approaches, showing that for a large class of designs, each produces non-negative, conservative variance estimators that are unbiased in finite samples or asymptotically under homogeneous treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12498v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ambarish Chattopadhyay, Guido W. Imbens</dc:creator>
    </item>
    <item>
      <title>Robust Variable Selection for High-dimensional Regression with Missing Data and Measurement Errors</title>
      <link>https://arxiv.org/abs/2410.16722</link>
      <description>arXiv:2410.16722v3 Announce Type: replace 
Abstract: In our paper, we focus on robust variable selection for missing data and measurement error. Missing data and measurement errors can lead to confusing data distribution. We propose an exponential loss function with a tuning parameter to apply to Missing and measurement errors data. By adjusting the parameter, the loss function can be better and more robust under various data distributions. We use inverse probability weighting and additive error models to address missing data and measurement errors. Also, we find that the Atan punishment method works better. We used Monte Carlo simulations to assess the validity of robust variable selection and validated our findings with the breast cancer dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16722v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenhao Zhang, Yunquan Song</dc:creator>
    </item>
    <item>
      <title>Bayes-assisted Confidence Regions: Focal Point Estimator and Bounded-influence Priors</title>
      <link>https://arxiv.org/abs/2410.20169</link>
      <description>arXiv:2410.20169v2 Announce Type: replace 
Abstract: The Frequentist, Assisted by Bayes (FAB) framework constructs confidence regions that leverage prior information about parameter values. FAB confidence regions (FAB-CRs) have smaller volume for values of the parameter that are likely under the prior while maintaining exact frequentist coverage. This work introduces several methodological and theoretical contributions to the FAB framework. For Gaussian likelihoods, we show that the posterior mean of the mean parameter is contained in the FAB-CR. More generally, this result extends to the posterior mean of the natural parameter for likelihoods in the natural exponential family. These results provide a natural Bayes-assisted estimator to be reported alongside the FAB-CR. Furthermore, for Gaussian likelihoods, we show that power-law tail conditions on the marginal likelihood induce robust FAB-CRs that are uniformly bounded and revert to standard frequentist confidence intervals for extreme observations. We translate this result into practice by proposing a class of shrinkage priors for the FAB framework that satisfy this condition without sacrificing analytic tractability. The resulting FAB estimators equal prominent Bayesian shrinkage estimators, including the horseshoe estimator, thereby establishing insightful connections between robust FAB-CRs and Bayesian shrinkage methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20169v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefano Cortinovis, Fran\c{c}ois Caron</dc:creator>
    </item>
    <item>
      <title>Doubly robust inference via calibration</title>
      <link>https://arxiv.org/abs/2411.02771</link>
      <description>arXiv:2411.02771v2 Announce Type: replace 
Abstract: Doubly robust estimators are widely used for estimating average treatment effects and other linear summaries of regression functions. While consistency requires only one of two nuisance functions to be estimated consistently, asymptotic normality typically require sufficiently fast convergence of both. In this work, we correct this mismatch: we show that calibrating the nuisance estimators within a doubly robust procedure yields doubly robust asymptotic normality for linear functionals. We introduce a general framework, calibrated debiased machine learning (calibrated DML), and propose a specific estimator that augments standard DML with a simple isotonic regression adjustment. Our theoretical analysis shows that the calibrated DML estimator remains asymptotically normal if either the regression or the Riesz representer of the functional is estimated sufficiently well, allowing the other to converge arbitrarily slowly or even inconsistently. We further propose a simple bootstrap method for constructing confidence intervals, enabling doubly robust inference without additional nuisance estimation. In a range of semi-synthetic benchmark datasets, calibrated DML reduces bias and improves coverage relative to standard DML. Our method can be integrated into existing DML pipelines by adding just a few lines of code to calibrate cross-fitted estimates via isotonic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02771v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Alex Luedtke, Marco Carone</dc:creator>
    </item>
    <item>
      <title>Thompson, Ulam, or Gauss? Multi-criteria recommendations for posterior probability computation methods in Bayesian response-adaptive trials</title>
      <link>https://arxiv.org/abs/2411.19871</link>
      <description>arXiv:2411.19871v2 Announce Type: replace 
Abstract: Bayesian adaptive designs enable flexible clinical trials by adapting features based on accumulating data. Among these, Bayesian Response-Adaptive Randomization (BRAR) skews patient allocation towards more promising treatments based on interim data. Implementing BRAR requires the relatively quick evaluation of posterior probabilities. However, the limitations of existing closed-form solutions mean trials often rely on computationally intensive approximations which can impact accuracy and the scope of scenarios explored. While faster Gaussian approximations exist, their reliability is not guaranteed. Critically, the approximation method used is often poorly reported, and the literature lacks practical guidance for selecting and comparing these methods, particularly regarding the trade-offs between computational speed, inferential accuracy, and their implications for patient benefit.
  In this paper, we focus on BRAR trials with binary endpoints, developing a novel algorithm that efficiently and exactly computes these posterior probabilities, enabling a robust assessment of existing approximation methods in use. Leveraging these exact computations, we establish a comprehensive benchmark for evaluating approximation methods based on their computational speed, patient benefit, and inferential accuracy. Our comprehensive analysis, conducted through a range of simulations in the two-armed case and a re-analysis of the three-armed Established Status Epilepticus Treatment Trial, reveals that the exact calculation algorithm is often the fastest, even for up to 12 treatment arms. Furthermore, we demonstrate that commonly used approximation methods can lead to significant power loss and Type I error rate inflation. We conclude by providing practical guidance to aid practitioners in selecting the most appropriate computation method for various clinical trial settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19871v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Kaddaj, Lukas Pin, Stef Baas, Edwin Y. N. Tang, David S. Robertson, Sof\'ia S. Villar</dc:creator>
    </item>
    <item>
      <title>The R Package WMAP: Tools for Causal Meta-Analysis by Integrating Multiple Observational Studies</title>
      <link>https://arxiv.org/abs/2501.01041</link>
      <description>arXiv:2501.01041v3 Announce Type: replace 
Abstract: Integrating multiple observational studies for meta-analysis has sparked much interest. The presented R package WMAP (Weighted Meta-Analysis with Pseudo-Population) addresses a critical gap in the implementation of integrative weighting approaches for multiple observational studies and causal inferences about various groups of subjects, such as disease subtypes. The package features three weighting approaches, each representing a special case of the unified weighting framework introduced by Guha and Li (2024), which includes an extension of inverse probability weights for data integration settings. It performs meta-analysis on user-inputted datasets as follows: (i) it first estimates the propensity scores for study-group combinations, calculates subject balancing weights, and determines the effective sample size (ESS) for a user-specified weighting method; and (ii) it then estimates various features of multiple counterfactual group outcomes, such as group medians and differences in group means for the mRNA expression of eight genes. Additionally, bootstrap variability estimates are provided. Among the implemented weighting methods, we highlight the FLEXible, Optimized, and Realistic (FLEXOR) method, which is specifically designed to maximize the ESS within the unified framework. The use of the software is illustrated by simulations as well as a multi-site breast cancer study conducted in seven medical centers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01041v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Subharup Guha, Mengqi Xu, Kashish Priyam, Yi Li</dc:creator>
    </item>
    <item>
      <title>Partial Information Rate Decomposition</title>
      <link>https://arxiv.org/abs/2502.04550</link>
      <description>arXiv:2502.04550v3 Announce Type: replace 
Abstract: Partial Information Decomposition (PID) is a principled and flexible method to unveil complex high-order interactions in multi-unit network systems. Though being defined exclusively for random variables, PID is ubiquitously applied to multivariate time series taken as realizations of random processes with temporal statistical structure. Here, to overcome the incorrect depiction of high-order effects by PID schemes applied to dynamic networks, we introduce the framework of Partial Information Rate Decomposition (PIRD). PIRD is first formalized applying lattice theory to decompose the information shared dynamically between a target random process and a set of source processes, and then implemented for Gaussian processes through a spectral expansion of information rates. The new framework is validated in simulated network systems and demonstrated in the practical analysis of time series from large-scale climate oscillations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04550v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luca Faes, Laura Sparacino, Gorana Mijatovic, Yuri Antonacci, Leonardo Ricci, Daniele Marinazzo, Sebastiano Stramaglia</dc:creator>
    </item>
    <item>
      <title>Decomposing Multivariate Information Rates in Networks of Random Processes</title>
      <link>https://arxiv.org/abs/2502.04555</link>
      <description>arXiv:2502.04555v3 Announce Type: replace 
Abstract: The Partial Information Decomposition (PID) framework has emerged as a powerful tool for analyzing high-order interdependencies in complex network systems. However, its application to dynamic processes remains challenging due to the implicit assumption of memorylessness, which often falls in real-world scenarios. In this work, we introduce the framework of Partial Information Rate Decomposition (PIRD) that extends PID to random processes with temporal correlations. By leveraging mutual information rate (MIR) instead of mutual information (MI), our approach decomposes the dynamic information shared by multivariate random processes into unique, redundant, and synergistic contributions obtained aggregating information rate atoms in a principled manner. To solve PIRD, we define a pointwise redundancy rate function based on the minimum MI principle applied locally in the frequency-domain representation of the processes. The framework is validated in benchmark simulations of Gaussian systems, demonstrating its advantages over traditional PID in capturing temporal correlations and showing how the spectral representation may reveal scale-specific higher-order interactions that are obscured in the time domain. Furthermore, we apply PIRD to a physiological network comprising cerebrovascular and cardiovascular variables, revealing frequency-dependent redundant information exchange during a protocol of postural stress. Our results highlight the necessity of accounting for the full temporal statistical structure and spectral content of vector random processes to meaningfully perform information decomposition in network systems with dynamic behavior such as those typically encountered in neuroscience and physiology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04555v3</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Laura Sparacino, Gorana Mijatovic, Yuri Antonacci, Leonardo Ricci, Daniele Marinazzo, Sebastiano Stramaglia, Luca Faes</dc:creator>
    </item>
    <item>
      <title>Few-Round Distributed Principal Component Analysis: Closing the Statistical Efficiency Gap by Consensus</title>
      <link>https://arxiv.org/abs/2503.03123</link>
      <description>arXiv:2503.03123v3 Announce Type: replace 
Abstract: Distributed algorithms and theories are called for in this era of big data. Under weaker local signal-to-noise ratios, we improve upon the celebrated one-round distributed principal component analysis (PCA) algorithm designed in the spirit of divide-and-conquer, by introducing a few additional communication rounds of consensus. The proposed shifted subspace iteration algorithm is able to close the local phase transition gap, reduce the asymptotic variance, and also alleviate the potential bias. Our estimation procedure is easy to implement and tuning-free. The resulting estimator is shown to be statistically efficient after an acceptable number of iterations. We also discuss extensions to distributed elliptical PCA for heavy-tailed data. Empirical experiments on synthetic and benchmark datasets demonstrate our method's statistical advantage over the divide-and-conquer approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03123v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>ZeYu Li, Xinsheng Zhang, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>Exploratory Hierarchical Factor Analysis with an Application to Psychological Measurement</title>
      <link>https://arxiv.org/abs/2505.09043</link>
      <description>arXiv:2505.09043v2 Announce Type: replace 
Abstract: Hierarchical factor models, which include the bifactor model as a special case, are useful in social and behavioural sciences for measuring hierarchically structured constructs. Specifying a hierarchical factor model involves imposing hierarchically structured zero constraints on a factor loading matrix, which is often challenging. Therefore, an exploratory analysis is needed to learn the hierarchical factor structure from data. Unfortunately, there does not exist an identifiability theory for the learnability of this hierarchical structure and a computationally efficient method with provable performance. The method of Schmid-Leiman transformation, which is often regarded as the default method for exploratory hierarchical factor analysis, is flawed and likely to fail. The contribution of this paper is three-fold. First, an identifiability result is established for general hierarchical factor models, which shows that the hierarchical factor structure is learnable under mild regularity conditions. Second, a computationally efficient divide-and-conquer approach is proposed for learning the hierarchical factor structure. Finally, asymptotic theory is established for the proposed method, showing that it can consistently recover the true hierarchical factor structure as the sample size grows to infinity. The power of the proposed method is shown via simulation studies and a real data application to a personality test. The computation code for the proposed method is publicly available at https://anonymous.4open.science/r/Exact-Exploratory-Hierarchical-Factor-Analysis-F850.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09043v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Qiao, Yunxiao Chen, Zhiliang Ying</dc:creator>
    </item>
    <item>
      <title>Constrained Denoising, Empirical Bayes, and Optimal Transport</title>
      <link>https://arxiv.org/abs/2506.09986</link>
      <description>arXiv:2506.09986v2 Announce Type: replace 
Abstract: In the statistical problem of denoising, Bayes and empirical Bayes methods can "overshrink" their output relative to the latent variables of interest. This work is focused on constrained denoising problems which mitigate such phenomena. At the oracle level, i.e., when the latent variable distribution is assumed known, we apply tools from the theory of optimal transport to characterize the solution to (i) variance-constrained, (ii) distribution-constrained, and (iii) general-constrained denoising problems. At the empirical level, i.e., when the latent variable distribution is not known, we use empirical Bayes methodology to estimate these oracle denoisers. Our approach is modular, and transforms any suitable (unconstrained) empirical Bayes denoiser into a constrained empirical Bayes denoiser. We prove explicit rates of convergence for our proposed methodologies, which both extend and sharpen existing asymptotic results that have previously considered only variance constraints. We apply our methodology in two applications: one in astronomy concerning the relative chemical abundances in a large catalog of red-clump stars, and one in baseball concerning minor- and major league batting skill for rookie players.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.09986v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adam Quinn Jaffe, Nikolaos Ignatiadis, Bodhisattva Sen</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power</title>
      <link>https://arxiv.org/abs/2506.20523</link>
      <description>arXiv:2506.20523v2 Announce Type: replace 
Abstract: Adaptive experiments such as multi-armed bandits offer efficiency gains over traditional randomized experiments but pose two major challenges: invalid inference on the Average Treatment Effect (ATE) due to adaptive sampling and low statistical power for sub-optimal treatments. We address both issues by extending the Mixture Adaptive Design framework (arXiv:2311.05794). First, we propose MADCovar, a covariate-adjusted ATE estimator that is unbiased and preserves anytime-valid inference guarantees while substantially improving ATE precision. Second, we introduce MADMod, which dynamically reallocates samples to underpowered arms, enabling more balanced statistical power across treatments without sacrificing valid inference. Both methods retain MAD's core advantage of constructing asymptotic confidence sequences (CSs) that allow researchers to continuously monitor ATE estimates and stop data collection once a desired precision or significance criterion is met. Empirically, we validate both methods using simulations and real-world data. In simulations, MADCovar reduces CS width by up to $60\%$ relative to MAD. In a large-scale political RCT with $\approx32,000$ participants, MADCovar achieves similar precision gains. MADMod improves statistical power and inferential precision across all treatment arms, particularly for suboptimal treatments. Simulations show that MADMod sharply reduces Type II error while preserving the efficiency benefits of adaptive allocation. Together, MADCovar and MADMod make adaptive experiments more practical, reliable, and efficient for applied researchers across many domains. Our proposed methods are implemented through an open-source software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20523v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Molitor, Samantha Gold</dc:creator>
    </item>
    <item>
      <title>The Effect of Omitted Variables on the Sign of Regression Coefficients</title>
      <link>https://arxiv.org/abs/2208.00552</link>
      <description>arXiv:2208.00552v4 Announce Type: replace-cross 
Abstract: We show that, depending on how the impact of omitted variables is measured, it can be substantially easier for omitted variables to flip coefficient signs than to drive them to zero. This behavior occurs with "Oster's delta" (Oster 2019), a widely reported robustness measure. Consequently, any time this measure is large -- suggesting that omitted variables may be unimportant -- a much smaller value reverses the sign of the parameter of interest. We propose a modified measure of robustness to address this concern. We illustrate our results in four empirical applications and two meta-analyses. We implement our methods in the companion Stata module regsensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.00552v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>BELIEF in Dependence: Leveraging Atomic Linearity in Data Bits for Rethinking Generalized Linear Models</title>
      <link>https://arxiv.org/abs/2210.10852</link>
      <description>arXiv:2210.10852v3 Announce Type: replace-cross 
Abstract: Two linearly uncorrelated binary variables must be also independent because non-linear dependence cannot manifest with only two possible states. This inherent linearity is the atom of dependency constituting any complex form of relationship. Inspired by this observation, we develop a framework called binary expansion linear effect (BELIEF) for understanding arbitrary relationships with a binary outcome. Models from the BELIEF framework are easily interpretable because they describe the association of binary variables in the language of linear models, yielding convenient theoretical insight and striking Gaussian parallels. With BELIEF, one may study generalized linear models (GLM) through transparent linear models, providing insight into how the choice of link affects modeling. For example, setting a GLM interaction coefficient to zero does not necessarily lead to the kind of no-interaction model assumption as understood under their linear model counterparts. Furthermore, for a binary response, maximum likelihood estimation for GLMs paradoxically fails under complete separation, when the data are most discriminative, whereas BELIEF estimation automatically reveals the perfect predictor in the data that is responsible for complete separation. We explore these phenomena and provide related theoretical results. We also provide preliminary empirical demonstration of some theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.10852v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Annals of Statistics 2025, Vol. 53, No. 3, 1068-1094</arxiv:journal_reference>
      <dc:creator>Benjamin Brown, Kai Zhang, Xiao-Li Meng</dc:creator>
    </item>
    <item>
      <title>Bayesian Strategies for Repulsive Spatial Point Processes</title>
      <link>https://arxiv.org/abs/2404.15133</link>
      <description>arXiv:2404.15133v2 Announce Type: replace-cross 
Abstract: There is increasing interest to develop Bayesian inferential algorithms for point process models with intractable likelihoods. A purpose of this paper is to illustrate the utility of using simulation based strategies, including approximate Bayesian computation (ABC) and Markov chain Monte Carlo (MCMC) methods for this task. Shirota and Gelfand (2017) proposed an extended version of an ABC approach for repulsive spatial point processes, including the Strauss point process and the determinantal point process, but their algorithm was not correctly detailed. We explain that is, in general, intractable and therefore impractical to use, except in some restrictive situations. This motivates us to instead consider an ABC-MCMC algorithm developed by Fearnhead and Prangle (2012). We further explore the use of the exchange algorithm, together with the recently proposed noisy Metropolis-Hastings algorithm (Alquier et al., 2016). As an extension of the exchange algorithm, which requires a single simulation from the likelihood at each iteration, the noisy Metropolis-Hastings algorithm considers multiple draws from the same likelihood function. We find that both of these inferential approaches yield good performance for repulsive spatial point processes in both simulated and real data applications and should be considered as viable approaches for the analysis of these models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.15133v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoyi Lu, Nial Friel</dc:creator>
    </item>
    <item>
      <title>On the Identifying Power of Monotonicity for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.14104</link>
      <description>arXiv:2405.14104v3 Announce Type: replace-cross 
Abstract: In the context of a binary outcome, treatment, and instrument, Balke and Pearl (1993, 1997) establish that the monotonicity condition of Imbens and Angrist (1994) has no identifying power beyond instrument exogeneity for average potential outcomes and average treatment effects in the sense that adding it to instrument exogeneity does not decrease the identified sets for those parameters whenever those restrictions are consistent with the distribution of the observable data. This paper shows that this phenomenon holds in a broader setting with a multi-valued outcome, treatment, and instrument, under an extension of the monotonicity condition that we refer to as generalized monotonicity. We further show that this phenomenon holds for any restriction on treatment response that is stronger than generalized monotonicity provided that these stronger restrictions do not restrict potential outcomes. Importantly, many models of potential treatments previously considered in the literature imply generalized monotonicity, including the types of monotonicity restrictions considered by Kline and Walters (2016), Kirkeboen et al. (2016), and Heckman and Pinto (2018), and the restriction that treatment selection is determined by particular classes of additive random utility models. We show through a series of examples that restrictions on potential treatments can provide identifying power beyond instrument exogeneity for average potential outcomes and average treatment effects when the restrictions imply that the generalized monotonicity condition is violated. In this way, our results shed light on the types of restrictions required for help in identifying average potential outcomes and average treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14104v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Shunzhuang Huang, Sarah Moon, Azeem M. Shaikh, Edward J. Vytlacil</dc:creator>
    </item>
    <item>
      <title>A General Framework on Conditions for Constraint-based Causal Learning</title>
      <link>https://arxiv.org/abs/2408.07575</link>
      <description>arXiv:2408.07575v2 Announce Type: replace-cross 
Abstract: Most constraint-based causal learning algorithms provably return the correct causal graph under certain correctness conditions, such as faithfulness. By representing any constraint-based causal learning algorithm using the notion of a property, we provide a general framework to obtain and study correctness conditions for these algorithms. From the framework, we provide exact correctness conditions for the PC algorithm, which are then related to the correctness conditions of some other existing causal discovery algorithms. The framework also suggests a paradigm for designing causal learning algorithms which allows for the correctness conditions of algorithms to be controlled for before designing the actual algorithm, and has the following implications. We show that the sparsest Markov representation condition is the weakest correctness condition for algorithms that output ancestral graphs or directed acyclic graphs satisfying any existing notions of minimality. We also reason that Pearl-minimality is necessary for meaningful causal learning but not sufficient to relax the faithfulness condition and, as such, has to be strengthened, such as by including background knowledge, for causal learning beyond faithfulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07575v2</guid>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Kai Z. Teh, Kayvan Sadeghi, Terry Soo</dc:creator>
    </item>
    <item>
      <title>Semiparametric Double Reinforcement Learning with Applications to Long-Term Causal Inference</title>
      <link>https://arxiv.org/abs/2501.06926</link>
      <description>arXiv:2501.06926v3 Announce Type: replace-cross 
Abstract: Long-term causal effects often must be estimated from short-term data due to limited follow-up in healthcare, economics, and online platforms. Markov Decision Processes (MDPs) provide a natural framework for capturing such long-term dynamics through sequences of states, actions, and rewards. Double Reinforcement Learning (DRL) enables efficient inference on policy values in MDPs, but nonparametric implementations require strong intertemporal overlap assumptions and often exhibit high variance and instability. We propose a semiparametric extension of DRL for efficient inference on linear functionals of the Q-function--such as policy values--in infinite-horizon, time-homogeneous MDPs. By imposing structural restrictions on the Q-function, our approach relaxes the strong overlap conditions required by nonparametric methods and improves statistical efficiency. Under model misspecification, our estimators target the functional of the best-approximating Q-function, with only second-order bias. We provide conditions for valid inference using sieve methods and data-driven model selection. A central challenge in DRL is the estimation of nuisance functions, such as density ratios, which often involve difficult minimax optimization. To address this, we introduce a novel plug-in estimator based on isotonic Bellman calibration, which combines fitted Q-iteration with an isotonic regression adjustment. The estimator is debiased without requiring estimation of additional nuisance functions and reduces high-dimensional overlap assumptions to a one-dimensional condition. Bellman calibration extends isotonic calibration--widely used in prediction and classification--to the MDP setting and may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06926v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, David Hubbard, Allen Tran, Nathan Kallus, Aur\'elien Bibaut</dc:creator>
    </item>
    <item>
      <title>Generalized Venn and Venn-Abers Calibration with Applications in Conformal Prediction</title>
      <link>https://arxiv.org/abs/2502.05676</link>
      <description>arXiv:2502.05676v2 Announce Type: replace-cross 
Abstract: Ensuring model calibration is critical for reliable prediction, yet popular distribution-free methods such as histogram binning and isotonic regression offer only asymptotic guarantees. We introduce a unified framework for Venn and Venn-Abers calibration that extends Vovk's approach beyond binary classification to a broad class of prediction problems defined by generic loss functions. Our method transforms any perfectly in-sample calibrated predictor into a set-valued predictor that, in finite samples, outputs at least one marginally calibrated point prediction. These set predictions shrink asymptotically and converge to a conditionally calibrated prediction, capturing epistemic uncertainty. We further propose Venn multicalibration, a new approach for achieving finite-sample calibration across subpopulations. For quantile loss, our framework recovers group-conditional and multicalibrated conformal prediction as special cases and yields novel prediction intervals with quantile-conditional coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.05676v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars van der Laan, Ahmed Alaa</dc:creator>
    </item>
    <item>
      <title>A Consequentialist Critique of Binary Classification Evaluation Practices</title>
      <link>https://arxiv.org/abs/2504.04528</link>
      <description>arXiv:2504.04528v2 Announce Type: replace-cross 
Abstract: ML-supported decisions, such as ordering tests or determining preventive custody, often involve binary classification based on probabilistic forecasts. Evaluation frameworks for such forecasts typically consider whether to prioritize independent-decision metrics (e.g., Accuracy) or top-K metrics (e.g., Precision@K), and whether to focus on fixed thresholds or threshold-agnostic measures like AUC-ROC. We highlight that a consequentialist perspective, long advocated by decision theorists, should naturally favor evaluations that support independent decisions using a mixture of thresholds given their prevalence, such as Brier scores and Log loss. However, our empirical analysis reveals a strong preference for top-K metrics or fixed thresholds in evaluations at major conferences like ICML, FAccT, and CHIL. To address this gap, we use this decision-theoretic framework to map evaluation metrics to their optimal use cases, along with a Python package, briertools, to promote the broader adoption of Brier scores. In doing so, we also uncover new theoretical connections, including a reconciliation between the Brier Score and Decision Curve Analysis, which clarifies and responds to a longstanding critique by (Assel, et al. 2017) regarding the clinical utility of proper scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.04528v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gerardo Flores, Abigail Schiff, Alyssa H. Smith, Julia A Fukuyama, Ashia C. Wilson</dc:creator>
    </item>
    <item>
      <title>A Framework of Decision-Relevant Observability: Reinforcement Learning Converges Under Relative Ignorability</title>
      <link>https://arxiv.org/abs/2504.07722</link>
      <description>arXiv:2504.07722v5 Announce Type: replace-cross 
Abstract: From clinical dosing algorithms to autonomous robots, sequential decision-making systems routinely operate with missing or incomplete data. Classical reinforcement learning theory, which is commonly used to solve sequential decision problems, assumes Markovian observability, which may not hold under partial observability. Causal inference paradigms formalise ignorability of missingness. We show these views can be unified and generalized in order to guarantee Q-learning convergence even when the Markov property fails. To do so, we introduce the concept of \emph{relative ignorability}. Relative ignorability is a graphical-causal criterion which refines the requirements for accurate decision-making based on incomplete data. Theoretical results and simulations both reveal that non-markovian stochastic processes whose missingness is relatively ignorable with respect to causal estimands can still be optimized using standard Reinforcement Learning algorithms. These results expand the theoretical foundations of safe, data-efficient AI to real-world environments where complete information is unattainable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07722v5</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MaryLena Bleile</dc:creator>
    </item>
    <item>
      <title>Optimal Post-Hoc Theorizing</title>
      <link>https://arxiv.org/abs/2505.10370</link>
      <description>arXiv:2505.10370v2 Announce Type: replace-cross 
Abstract: For many economic questions, the empirical results are not interesting unless they are strong. For these questions, theorizing before the results are known is not always optimal. Instead, the optimal sequencing of theory and empirics trades off a ``Darwinian Learning'' effect from theorizing first with a ``Statistical Learning'' effect from examining the data first. This short paper formalizes the tradeoff in a Bayesian model. In the modern era of mature economic theory and enormous datasets, I argue that post hoc theorizing is typically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10370v2</guid>
      <category>econ.EM</category>
      <category>q-fin.GN</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Y. Chen</dc:creator>
    </item>
    <item>
      <title>Discretion in the Loop: Human Expertise in Algorithm-Assisted College Advising</title>
      <link>https://arxiv.org/abs/2505.13325</link>
      <description>arXiv:2505.13325v2 Announce Type: replace-cross 
Abstract: In higher education, many institutions use algorithmic alerts to flag at-risk students and deliver advising at scale. While much research has focused on evaluating algorithmic predictions, relatively little is known about how discretionary interventions by human experts shape outcomes in algorithm-assisted settings. We study this question using rich quantitative and qualitative data from a randomized controlled trial of an algorithm-assisted advising program at Georgia State University. Taking a mixed-methods approach, we examine whether and how advisors use context unavailable to an algorithm to guide interventions and influence student success. We develop a causal graphical framework for human expertise in the interventional setting, extending prior work on discretion in purely predictive settings. We then test a necessary condition for discretionary expertise using structured advisor logs and student outcomes data, identifying several interventions that meet the criterion for statistical significance. Accordingly, we estimate that 2 out of 3 interventions taken by advisors in the treatment arm were plausibly "expertly targeted" to students using non-algorithmic context. Systematic qualitative analysis of advisor notes corroborates these findings, showing a pattern of advisors incorporating diverse forms of contextual information--such as personal circumstances, financial issues, and student engagement--into their decisions. Our results offer theoretical and practical insight into the real-world effectiveness of algorithm-supported college advising, and underscore the importance of accounting for human expertise in the design, evaluation, and implementation of algorithmic decision systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.13325v2</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kara Schechtman, Benjamin Brandon, Jenise Stafford, Hannah Li, Lydia T. Liu</dc:creator>
    </item>
    <item>
      <title>What Makes Treatment Effects Identifiable? Characterizations and Estimators Beyond Unconfoundedness</title>
      <link>https://arxiv.org/abs/2506.04194</link>
      <description>arXiv:2506.04194v2 Announce Type: replace-cross 
Abstract: Most of the widely used estimators of the average treatment effect (ATE) in causal inference rely on the assumptions of unconfoundedness and overlap. Unconfoundedness requires that the observed covariates account for all correlations between the outcome and treatment. Overlap requires the existence of randomness in treatment decisions for all individuals. Nevertheless, many types of studies frequently violate unconfoundedness or overlap, for instance, observational studies with deterministic treatment decisions - popularly known as Regression Discontinuity designs - violate overlap.
  In this paper, we initiate the study of general conditions that enable the identification of the average treatment effect, extending beyond unconfoundedness and overlap. In particular, following the paradigm of statistical learning theory, we provide an interpretable condition that is sufficient and necessary for the identification of ATE. Moreover, this condition also characterizes the identification of the average treatment effect on the treated (ATT) and can be used to characterize other treatment effects as well. To illustrate the utility of our condition, we present several well-studied scenarios where our condition is satisfied and, hence, we prove that ATE can be identified in regimes that prior works could not capture. For example, under mild assumptions on the data distributions, this holds for the models proposed by Tan (2006) and Rosenbaum (2002), and the Regression Discontinuity design model introduced by Thistlethwaite and Campbell (1960). For each of these scenarios, we also show that, under natural additional assumptions, ATE can be estimated from finite samples.
  We believe these findings open new avenues for bridging learning-theoretic insights and causal inference methodologies, particularly in observational studies with complex treatment mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04194v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Cai, Alkis Kalavasis, Katerina Mamali, Anay Mehrotra, Manolis Zampetakis</dc:creator>
    </item>
    <item>
      <title>Bayesian Non-Negative Matrix Factorization with Correlated Mutation Type Probabilities for Mutational Signatures</title>
      <link>https://arxiv.org/abs/2506.15855</link>
      <description>arXiv:2506.15855v2 Announce Type: replace-cross 
Abstract: Somatic mutations, or alterations in DNA of a somatic cell, are key markers of cancer. In recent years, mutational signature analysis has become a prominent field of study within cancer research, commonly with Nonnegative Matrix Factorization (NMF) and Bayesian NMF. However, current methods assume independence across mutation types in the signatures matrix. This paper expands upon current Bayesian NMF methodologies by proposing novel methods that account for the dependencies between the mutation types. First, we implement the Bayesian NMF specification with a Multivariate Truncated Normal prior on the signatures matrix in order to model the covariance structure using external information, in our case estimated from the COSMIC signatures database. This model converges in fewer iterations, using MCMC, when compared to a model with independent Truncated Normal priors on elements of the signatures matrix and results in improvements in accuracy, especially on small sample sizes. In addition, we develop a hierarchical model that allows the covariance structure of the signatures matrix to be discovered rather than specified upfront, giving the algorithm more flexibility. This flexibility for the algorithm to learn the dependence structure of the signatures allows a better understanding of biological interactions and how these change across different types of cancer. The code for this project is contributed to an open-source R software package. Our work lays the groundwork for future research to incorporate dependency structure across mutation types in the signatures matrix and is also applicable to any use of NMF beyond just single-base substitution (SBS) mutational signatures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15855v2</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 01 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Iris Lang, Jenna Landy, Giovanni Parmigiani</dc:creator>
    </item>
  </channel>
</rss>
