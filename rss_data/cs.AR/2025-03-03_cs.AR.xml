<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.AR updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.AR</link>
    <description>cs.AR updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.AR" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Mar 2025 05:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Quarter of a Century of Neuromorphic Architectures on FPGAs -- an Overview</title>
      <link>https://arxiv.org/abs/2502.20415</link>
      <description>arXiv:2502.20415v1 Announce Type: new 
Abstract: FPGA-based neuromorphic architectures pose a promising viewpoint on the future of computing, but the efforts in this domain are disorganized, without a clear classification scheme for implemented structures. This results in the lack of consensus on what is important for specific groups of neuromorphic systems, e.g., based on the platform they are implemented on or the intended use case. Here, we review the approach to implementing such architectures on FPGAs over the last 25 years. We propose a new taxonomy for those systems that could help the researchers better understand the closest environment their designs reside in and devise new metrics that could fairly grade them against the other ideas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20415v1</guid>
      <category>cs.AR</category>
      <category>cs.NE</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wiktor J. Szczerek, Artur Podobas</dc:creator>
    </item>
    <item>
      <title>$R^4$: A Racetrack Register File with Runtime Software Reconfiguration</title>
      <link>https://arxiv.org/abs/2502.20775</link>
      <description>arXiv:2502.20775v1 Announce Type: new 
Abstract: Arising disruptive memory technologies continuously make their way into the memory hierarchy at various levels. Racetrack memory is one promising candidate for future memory due to the overall low energy consumption, access latency and high endurance. However, the access dependent shift property of racetrack memory can make it easily a poor candidate, when the number of shifts is not properly reduced. Therefore, we explore how a register file can be constructed by using non-volatile racetrack memories with a properly reduced number of shifts. Our proposed architecture allows allocating registers in a horizontal or vertical allocation mode, where registers are either scattered across nanotracks or allocated along tracks. In this paper, we propose a dynamic approach, where the allocation can be altered at any access between horizontal and vertical. Control flow graph based static program analysis with simulation-based branch probabilities supplies crucially important recommendations for the dynamic allocation, which are applied at runtime. Experimental evaluation, including a custom gem5 simulation setup, reveals the need for this type of runtime reconfiguration.
  While the performance in terms of energy consumption, for instance, can be comparably high as SRAM when no runtime reconfiguration is done, the dynamic approach reduces it by up to $\approx 6\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20775v1</guid>
      <category>cs.AR</category>
      <category>cs.ET</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Hakert, Shuo-Han Chen, Kay Heider, Roland K\"uhn, Yun-Chih Chen, Jens Teubner, Jian-Jia Chen</dc:creator>
    </item>
    <item>
      <title>Understanding intra-node communication in HPC systems and Datacenters</title>
      <link>https://arxiv.org/abs/2502.20965</link>
      <description>arXiv:2502.20965v1 Announce Type: new 
Abstract: Over the past decade, specialized computing and storage devices, such as GPUs, TPUs, and high-speed storage, have been increasingly integrated into server nodes within Supercomputers and Data Centers. The advent of high-bandwidth memory (HBM) has facilitated a more compact design for these components, enabling multiple units to be interconnected within a single server node through intra-node networks like PCIe, NVLink, or Ethernet. These networks allow for scaling up the number of dedicated computing and storage devices per node. Additionally, inter-node networks link these devices across thousands of server nodes in large-scale computing systems. However, as communication demands among accelerators grow-especially in workloads like generative AI-both intra- and inter-node networks risk becoming critical bottlenecks. Although modern intra-node network architectures attempt to mitigate this issue by boosting bandwidth, we demonstrate in this paper that such an approach can inadvertently degrade inter-node communication. This occurs when high-bandwidth intra-node traffic interferes with incoming traffic from external nodes, leading to congestion. To evaluate this phenomenon, we analyze the communication behavior of realistic traffic patterns commonly found in generative AI applications. Using OMNeT++, we developed a general simulation model that captures both intra- and inter-node network interactions. Through extensive simulations, our findings reveal that increasing intra-node bandwidth and the number of accelerators per node can actually hinder overall inter-node communication performance rather than improve it.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20965v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joaquin Tarraga-Moreno, Jesus Escudero-Sahuquillo, Pedro Javier Garcia, Francisco J. Quiles</dc:creator>
    </item>
    <item>
      <title>A RISC-V Multicore and GPU SoC Platform with a Qualifiable Software Stack for Safety Critical Systems</title>
      <link>https://arxiv.org/abs/2502.21027</link>
      <description>arXiv:2502.21027v1 Announce Type: new 
Abstract: In the context of the Horizon Europe project, METASAT, a hardware platform was developed as a prototype of future space systems. The platform is based on a multiprocessor NOEL-V, an established space-grade processor, which is integrated with the SPARROW AI accelerator and connected to a GPU, Vortex. Both processing systems follow the RISC-V specification. This is a novel hardware architecture for the space domain as the use of massive parallel processing units, such as GPUs, is starting to be considered for upcoming space missions due to the increased performance required to future space-related workloads, in particular, related to AI. However, such solutions are only currently adopted for New Space, since their limitations come not only from the hardware, but also from the software, which needs to be qualified before being deployed on an institutional mission. For this reason, the METASAT platform is one of the first endeavors towards enabling the use of high performance hardware in a qualifiable environment for safety critical systems. The software stack is based on baremetal, RTEMS and the XtratuM hypervisor, providing different options for applications of various degrees of criticality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21027v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marc Sol\'e i Bonet (Universitat Polit\`ecnica de Catalunya, Barcelona Supercomputing Center), Jannis Wolf (Barcelona Supercomputing Center), Leonidas Kosmidis (Barcelona Supercomputing Center, Universitat Polit\`ecnica de Catalunya)</dc:creator>
    </item>
    <item>
      <title>AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2502.21196</link>
      <description>arXiv:2502.21196v1 Announce Type: new 
Abstract: Graph Neural Networks (GNNs) have recently gained attention due to their performance on non-Euclidean data. The use of custom hardware architectures proves particularly beneficial for GNNs due to their irregular memory access patterns, resulting from the sparse structure of graphs. However, existing FPGA accelerators are limited by their double buffering mechanism, which doesn't account for the irregular node distribution in typical graph datasets. To address this, we introduce \textbf{AMPLE} (Accelerated Message Passing Logic Engine), an FPGA accelerator leveraging a new event-driven programming flow. We develop a mixed-arithmetic architecture, enabling GNN inference to be quantized at a node-level granularity. Finally, prefetcher for data and instructions is implemented to optimize off-chip memory access and maximize node parallelism. Evaluation on citation and social media graph datasets ranging from $2$K to $700$K nodes showed a mean speedup of $243\times$ and $7.2\times$ against CPU and GPU counterparts, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21196v1</guid>
      <category>cs.AR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Gimenes, Yiren Zhao, George Constantinides</dc:creator>
    </item>
    <item>
      <title>Recurrent CircuitSAT Sampling for Sequential Circuits</title>
      <link>https://arxiv.org/abs/2502.21226</link>
      <description>arXiv:2502.21226v1 Announce Type: new 
Abstract: In this work, we introduce a novel GPU-accelerated circuit satisfiability (CircuitSAT) sampling technique for sequential circuits. This work is motivated by the requirement in constrained random verification (CRV) to generate input stimuli to validate the functionality of digital hardware circuits. A major challenge in CRV is generating inputs for sequential circuits, along with the appropriate number of clock cycles required to meet design constraints. Traditional approaches often use Boolean satisfiability (SAT) samplers to generate inputs by unrolling state transitions over a fixed number of clock cycles. However, these methods do not guarantee that a solution exists for the given number of cycles. Consequently, producing input stimuli together with the required clock cycles is essential for thorough testing and verification. Our approach converts the logical constraints and temporal behavior of sequential circuits into a recurrent CircuitSAT problem, optimized via gradient descent to efficiently explore a diverse set of valid solutions, including their associated number of clock cycles. By operating directly on the circuit structure, our method reinterprets the sampling process as a supervised multi-output regression task. This differentiable framework enables independent element-wise operations on each tensor element, facilitating parallel execution during learning. As a result, we achieve GPU-accelerated sampling with substantial runtime improvements (up to 105.1x) over state-of-the-art heuristic samplers. We demonstrate the effectiveness of our method through extensive evaluations on circuit problems from the ISCAS-89 and ITC'99 benchmark suites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21226v1</guid>
      <category>cs.AR</category>
      <category>cs.PF</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arash Ardakani, Kevin He, John Wawrzynek</dc:creator>
    </item>
    <item>
      <title>Optimizing and Exploring System Performance in Compact Processing-in-Memory-based Chips</title>
      <link>https://arxiv.org/abs/2502.21259</link>
      <description>arXiv:2502.21259v1 Announce Type: new 
Abstract: Processing-in-memory (PIM) is a promising computing paradigm to tackle the "memory wall" challenge. However, PIM system-level benefits over traditional von Neumann architecture can be reduced when the memory array cannot fully store all the neural network (NN) weights. The NN size is increasing while the PIM design size cannot scale up accordingly due to area constraints. Therefore, this work targets the system performance optimization and exploration for compact PIM designs. We first analyze the impact of data movement on compact designs. Then, we propose a novel pipeline method that maximizes the reuse of NN weights to improve the throughput and energy efficiency of inference in compact chips. To further boost throughput, we introduce a scheduling algorithm to mitigate the pipeline bubble problem. Moreover, we investigate the trade-off between the network size and system performance for a compact PIM chip. Experimental results show that the proposed algorithm achieves 2.35x and 0.5% improvement in throughput and energy efficiency, respectively. Compared to the area-unlimited design, our compact chip achieves approximately 56.5% of the throughput and 58.6% of the energy efficiency while using only one-third of the chip area, along with 1.3x improvement in area efficiency. Our compact design also outperforms the modern GPU with 4.56x higher throughput and 157x better energy efficiency. Besides, our compact design uses less than 20% of the system energy for data movement as batch size scales up.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21259v1</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peilin Chen, Xiaoxuan Yang</dc:creator>
    </item>
    <item>
      <title>Enhancing software-hardware co-design for HEP by low-overhead profiling of single- and multi-threaded programs on diverse architectures with Adaptyst</title>
      <link>https://arxiv.org/abs/2502.20947</link>
      <description>arXiv:2502.20947v1 Announce Type: cross 
Abstract: Given the recent technological trends and novel computing paradigms spanning both software and hardware, physicists and software developers can no longer just rely on computers becoming faster to meet the ever-increasing computing demands of their research. Adapting systems to the new environment may be difficult though, especially in case of large and complex applications. Therefore, we introduce Adaptyst (formerly AdaptivePerf): an open-source and architecture-agnostic tool aiming for making these computational and procurement challenges easier to address. At the moment, Adaptyst profiles on- and off-CPU activity of codes, traces all threads and processes spawned by them, and analyses low-level software-hardware interactions to the extent supported by hardware. The tool addresses the main shortcomings of Linux "perf" and has been successfully tested on x86-64, arm64, and RISC-V instruction set architectures. Adaptyst is planned to be evolved towards a software-hardware co-design framework which scales from embedded to high-performance computing in both legacy and new applications and takes into account a bigger picture than merely choosing between CPUs and GPUs. Our paper describes the current development of the project and its roadmap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20947v1</guid>
      <category>cs.PF</category>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maksymilian Graczyk, Stefan Roiser</dc:creator>
    </item>
    <item>
      <title>LarQucut: A New Cutting and Mapping Approach for Large-sized Quantum Circuits in Distributed Quantum Computing (DQC) Environments</title>
      <link>https://arxiv.org/abs/2502.21000</link>
      <description>arXiv:2502.21000v1 Announce Type: cross 
Abstract: Distributed quantum computing (DQC) is a promising way to achieve large-scale quantum computing. However, mapping large-sized quantum circuits in DQC is a challenging job; for example, it is difficult to find an ideal cutting and mapping solution when many qubits, complicated qubit operations, and diverse QPUs are involved. In this study, we propose LarQucut, a new quantum circuit cutting and mapping approach for large-sized circuits in DQC. LarQucut has several new designs. (1) LarQucut can have cutting solutions that use fewer cuts, and it does not cut a circuit into independent sub-circuits, therefore reducing the overall cutting and computing overheads. (2) LarQucut finds isomorphic sub-circuits and reuses their execution results. So, LarQucut can reduce the number of sub-circuits that need to be executed to reconstruct the large circuit's output, reducing the time spent on sampling the sub-circuits. (3) We design an adaptive quantum circuit mapping approach, which identifies qubit interaction patterns and accordingly enables the best-fit mapping policy in DQC. The experimental results show that, for large circuits with hundreds to thousands of qubits in DQC, LarQucut can provide a better cutting and mapping solution with lower overall overheads and achieves results closer to the ground truth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.21000v1</guid>
      <category>quant-ph</category>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinglei Dou, Lei Liu, Zhuohao Wang, Pengyu Li</dc:creator>
    </item>
    <item>
      <title>PIUMA: Programmable Integrated Unified Memory Architecture</title>
      <link>https://arxiv.org/abs/2010.06277</link>
      <description>arXiv:2010.06277v2 Announce Type: replace 
Abstract: High performance large scale graph analytics are essential to timely analyze relationships in big data sets. Conventional processor architectures suffer from inefficient resource usage and bad scaling on those workloads. To enable efficient and scalable graph analysis, Intel developed the Programmable Integrated Unified Memory Architecture (PIUMA) as a part of the DARPA Hierarchical Identify Verify Exploit (HIVE) program. PIUMA consists of many multi-threaded cores, fine-grained memory and network accesses, a globally shared address space, powerful offload engines and a tightly integrated optical interconnection network. By utilizing co-packaged optical silicon photonics and extending the on-chip mesh protocol directly to the optical fabric, all PIUMA chips in a system are glued together in a large virtual die which allows for extremely low socket-to-socket latencies even as the system scales to thousands of sockets. Performance estimations project that a PIUMA node will outperform a conventional compute node by one to two orders of magnitude. Furthermore, PIUMA continues to scale across multiple nodes, which is a challenge in conventional multi-node setups.
  This paper presents the PIUMA architecture, and documents our experience in designing and building a prototype chip and its bring-up process. We summarize the methodology for our co-design of the architecture together with the software stack using simulation tools and FPGA emulation. These tools provided early performance estimations of realistic applications and allowed us to implement many optimizations across the hardware, compilers, libraries and applications. We built the PIUMA chip as a 316mm2 7nm FinFET CMOS die and constructed a 16-node system. PIUMA silicon has successfully powered on demonstrating key aspects of the architecture, some of which will be incorporated into future Intel products.</description>
      <guid isPermaLink="false">oai:arXiv.org:2010.06277v2</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sriram Aananthakrishnan, Shamsul Abedin, Vincent Cave, Fabio Checconi, Kristof Du Bois, Stijn Eyerman, Joshua B. Fryman, Wim Heirman, Jason Howard, Ibrahim Hur, Samkit Jain, Marek M. ~Landowski, Kevin Ma, Jarrod Nelson, Robert Pawlowski, Fabrizio Petrini Sebastian Szkoda, Sanjaya Tayal, Jesmin Jahan Tithi, Yves Vandriessche</dc:creator>
    </item>
    <item>
      <title>All-rounder: A Flexible AI Accelerator with Diverse Data Format Support and Morphable Structure for Multi-DNN Processing</title>
      <link>https://arxiv.org/abs/2310.16757</link>
      <description>arXiv:2310.16757v2 Announce Type: replace 
Abstract: Recognizing the explosive increase in the use of AI-based applications, several industrial companies developed custom ASICs (e.g., Google TPU, IBM RaPiD, Intel NNP-I/NNP-T) and constructed a hyperscale cloud infrastructure with them. These ASICs perform operations of the inference or training process of AI models which are requested by users. Since the AI models have different data formats and types of operations, the ASICs need to support diverse data formats and various operation shapes. However, the previous ASIC solutions do not or less fulfill these requirements. To overcome these limitations, we first present an area-efficient multiplier, named all-in-one multiplier, that supports multiple bit-widths for both integer and floating point data types. Then, we build a MAC array equipped with these multipliers with multi-format support. In addition, the MAC array can be partitioned into multiple blocks that can be flexibly fused to support various DNN operation types. We evaluate the practical effectiveness of the proposed MAC array by making an accelerator out of it, named All-rounder. According to our evaluation, the proposed all-in-one multiplier occupies 1.49x smaller area compared to the baselines with dedicated multipliers for each data format. Then, we compare the performance and energy efficiency of the proposed All-rounder with three different accelerators showing consistent speedup and higher efficiency across various AI benchmarks from vision to LLM-based language tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.16757v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seock-Hwan Noh, Seungpyo Lee, Banseok Shin, Sehun Park, Yongjoo Jang, Jaeha Kung</dc:creator>
    </item>
    <item>
      <title>WWW: What, When, Where to Compute-in-Memory</title>
      <link>https://arxiv.org/abs/2312.15896</link>
      <description>arXiv:2312.15896v3 Announce Type: replace 
Abstract: Matrix multiplication is the dominant computation during Machine Learning (ML) inference. To efficiently perform such multiplication operations, Compute-in-memory (CiM) paradigms have emerged as a highly energy efficient solution. However, integrating compute in memory poses key questions, such as 1) What type of CiM to use: Given a multitude of CiM design characteristics, determining their suitability from architecture perspective is needed. 2) When to use CiM: ML inference includes workloads with a variety of memory and compute requirements, making it difficult to identify when CiM is more beneficial than standard processing cores. 3) Where to integrate CiM: Each memory level has different bandwidth and capacity, creating different data reuse opportunities for CiM integration.
  To answer such questions regarding on-chip CiM integration for accelerating ML workloads, we use an analytical architecture-evaluation methodology with tailored mapping algorithm. The mapping algorithm aims to achieve highest weight reuse and reduced data movements for a given CiM prototype and workload. Our analysis considers the integration of CiM prototypes into the cache levels of a tensor-core-like architecture, and shows that CiM integrated memory improves energy efficiency by up to 3.4x and throughput by up to 15.6x compared to established baseline with INT-8 precision. We believe the proposed work provides insights into what type of CiM to use, and when and where to optimally integrate it in the cache hierarchy for efficient matrix multiplication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15896v3</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tanvi Sharma, Mustafa Ali, Indranil Chakraborty, Kaushik Roy</dc:creator>
    </item>
    <item>
      <title>EdgeLLM: A Highly Efficient CPU-FPGA Heterogeneous Edge Accelerator for Large Language Models</title>
      <link>https://arxiv.org/abs/2407.21325</link>
      <description>arXiv:2407.21325v2 Announce Type: replace 
Abstract: The rapid advancements in artificial intelligence (AI), particularly the Large Language Models (LLMs), have profoundly affected our daily work and communication forms. However, it is still a challenge to deploy LLMs on resource-constrained edge devices (such as robots), due to the intensive computation requirements, heavy memory access, diverse operator types and difficulties in compilation. In this work, we proposed EdgeLLM to address the above issues. Firstly, focusing on the computation, we designed mix-precision processing element array together with group systolic architecture, that can efficiently support both FP16*FP16 for the MHA block (Multi-Head Attention) and FP16*INT4 for the FFN layer (Feed-Forward Network). Meanwhile specific optimization on log-scale structured weight sparsity, has been used to further increase the efficiency. Secondly, to address the compilation and deployment issue, we analyzed the whole operators within LLM models and developed a universal data parallelism scheme, by which all of the input and output features maintain the same data shape, enabling to process different operators without any data rearrangement. Then we proposed an end-to-end compiler to map the whole LLM model on CPU-FPGA heterogeneous system (AMD Xilinx VCU128 FPGA). The accelerator achieves 1.91x higher throughput and 7.55x higher energy efficiency than the commercial GPU (NVIDIA A100-SXM4-80G). When compared with state-of-the-art FPGA accelerator of FlightLLM, it shows 10-24% better performance in terms of HBM bandwidth utilization, energy efficiency and LLM throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.21325v2</guid>
      <category>cs.AR</category>
      <pubDate>Mon, 03 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingqiang Huang, Ao Shen, Kai Li, Haoxiang Peng, Boyu Li, Yupeng Su, Hao Yu</dc:creator>
    </item>
  </channel>
</rss>
