<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Dec 2024 05:00:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>TurboFFT: Co-Designed High-Performance and Fault-Tolerant Fast Fourier Transform on GPUs</title>
      <link>https://arxiv.org/abs/2412.05824</link>
      <description>arXiv:2412.05824v1 Announce Type: new 
Abstract: GPU-based fast Fourier transform (FFT) is extremely important for scientific computing and signal processing. However, we find the inefficiency of existing FFT libraries and the absence of fault tolerance against soft error. To address these issues, we introduce TurboFFT, a new FFT prototype co-designed for high performance and online fault tolerance. For FFT, we propose an architecture-aware, padding-free, and template-based prototype to maximize hardware resource utilization, achieving a competitive or superior performance compared to the state-of-the-art closed-source library, cuFFT. For fault tolerance, we 1) explore algorithm-based fault tolerance (ABFT) at the thread and threadblock levels to reduce additional memory footprint, 2) address the error propagation by introducing a two-side ABFT with location encoding, and 3) further modify the threadblock-level FFT from 1-transaction to multi-transaction in order to bring more parallelism for ABFT. Our two-side strategy enables online correction without additional global memory while our multi-transaction design averages the expensive threadblock-level reduction in ABFT with zero additional operations. Experimental results on an NVIDIA A100 server GPU and a Tesla Turing T4 GPU demonstrate that TurboFFT without fault tolerance is comparable to or up to 300\% faster than cuFFT and outperforms VkFFT. TurboFFT with fault tolerance maintains an overhead of 7\% to 15\%, even under tens of error injections per minute for both FP32 and FP64.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05824v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixun Wu, Yujia Zhai, Jinyang Liu, Jiajun Huang, Zizhe Jian, Huangliang Dai, Sheng Di, Franck Cappello, Zizhong Chen</dc:creator>
    </item>
    <item>
      <title>Dual UAV Cluster-Assisted Maritime Physical Layer Secure Communications via Collaborative Beamforming</title>
      <link>https://arxiv.org/abs/2412.05949</link>
      <description>arXiv:2412.05949v1 Announce Type: new 
Abstract: Unmanned aerial vehicles (UAVs) can be utilized as relay platforms to assist maritime wireless communications. However, complex channels and multipath effects at sea can adversely affect the quality of UAV transmitted signals. Collaborative beamforming (CB) can enhance the signal strength and range to assist the UAV relay for remote maritime communications. However, due to the open nature of UAV channels, security issue requires special consideration. This paper proposes a dual UAV cluster-assisted system via CB to achieve physical layer security in maritime wireless communications. Specifically, one UAV cluster forms a maritime UAV-enabled virtual antenna array (MUVAA) relay to forward data signals to the remote legitimate vessel, and the other UAV cluster forms an MUVAA jammer to send jamming signals to the remote eavesdropper. In this system, we formulate a secure and energy-efficient maritime communication multi-objective optimization problem (SEMCMOP) to maximize the signal-to-interference-plus-noise ratio (SINR) of the legitimate vessel, minimize the SINR of the eavesdropping vessel and minimize the total flight energy consumption of UAVs. Since the SEMCMOP is an NP-hard and large-scale optimization problem, we propose an improved swarm intelligence optimization algorithm with chaotic solution initialization and hybrid solution update strategies to solve the problem. Simulation results indicate that the proposed algorithm outperforms other comparison algorithms, and it can achieve more efficient signal transmission by using the CB-based method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05949v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Huang, Aimin Wang, Geng Sun, Jiahui Li, Jiacheng Wang, Hongyang Du, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Cloud Platforms for Developing Generative AI Solutions: A Scoping Review of Tools and Services</title>
      <link>https://arxiv.org/abs/2412.06044</link>
      <description>arXiv:2412.06044v1 Announce Type: new 
Abstract: Generative AI is transforming enterprise application development by enabling machines to create content, code, and designs. These models, however, demand substantial computational power and data management. Cloud computing addresses these needs by offering infrastructure to train, deploy, and scale generative AI models. This review examines cloud services for generative AI, focusing on key providers like Amazon Web Services (AWS), Microsoft Azure, Google Cloud, IBM Cloud, Oracle Cloud, and Alibaba Cloud. It compares their strengths, weaknesses, and impact on enterprise growth. We explore the role of high-performance computing (HPC), serverless architectures, edge computing, and storage in supporting generative AI. We also highlight the significance of data management, networking, and AI-specific tools in building and deploying these models. Additionally, the review addresses security concerns, including data privacy, compliance, and AI model protection. It assesses the performance and cost efficiency of various cloud providers and presents case studies from healthcare, finance, and entertainment. We conclude by discussing challenges and future directions, such as technical hurdles, vendor lock-in, sustainability, and regulatory issues. Put together, this work can serve as a guide for practitioners and researchers looking to adopt cloud-based generative AI solutions, serving as a valuable guide to navigating the intricacies of this evolving field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06044v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dhavalkumar Patel, Ganesh Raut, Satya Narayan Cheetirala, Girish N Nadkarni, Robert Freeman, Benjamin S. Glicksberg, Eyal Klang, Prem Timsina</dc:creator>
    </item>
    <item>
      <title>Efficient Probabilistic Workflow Scheduling for IaaS Clouds</title>
      <link>https://arxiv.org/abs/2412.06073</link>
      <description>arXiv:2412.06073v1 Announce Type: new 
Abstract: The flexibility and the variety of computing resources offered by the cloud make it particularly attractive for executing user workloads. However, IaaS cloud environments pose non-trivial challenges in the case of workflow scheduling under deadlines and monetary cost constraints. Indeed, given the typical uncertain performance behavior of cloud resources, scheduling algorithms that assume deterministic execution times may fail, thus requiring probabilistic approaches. However, existing probabilistic algorithms are computationally expensive, mainly due to the greater complexity of the workflow scheduling problem in its probabilistic form, and they hardily scale with the size of the problem instance. In this article, we propose EPOSS, a novel workflow scheduling algorithm for IaaS cloud environments based on a probabilistic formulation. Our solution blends together the low execution latency of state-of-the-art scheduling algorithms designed for the case of deterministic execution times and the capability to enforce probabilistic constraints.Designed with computational efficiency in mind, EPOSS achieves one to two orders lower execution times in comparison with existing probabilistic schedulers. Furthermore, it ensures good scaling with respect to workflow size and number of heterogeneous virtual machine types offered by the IaaS cloud environment. We evaluated the benefits of our algorithm via an experimental comparison over a variety of workloads and characteristics of IaaS cloud environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06073v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriele Russo Russo, Romolo Marotta, Flavio Cordari, Francesco Quaglia, Valeria Cardellini, Pierangelo Di Sanzo</dc:creator>
    </item>
    <item>
      <title>Harpagon: Minimizing DNN Serving Cost via Efficient Dispatching, Scheduling and Splitting</title>
      <link>https://arxiv.org/abs/2412.06161</link>
      <description>arXiv:2412.06161v1 Announce Type: new 
Abstract: Advances in deep neural networks (DNNs) have significantly contributed to the development of real-time video processing applications. Efficient scheduling of DNN workloads in cloud-hosted inference systems is crucial to minimizing serving costs while meeting application latency constraints. However, existing systems suffer from excessive module latency during request dispatching, low execution throughput during module scheduling, and wasted latency budget during latency splitting for multi-DNN application, which undermines their capability to minimize the serving cost.
  In this paper, we design a DNN inference system called Harpagon, which minimizes the serving cost under latency constraints with a three-level design. It first maximizes the batch collection rate with a batch-aware request dispatch policy to minimize the module latency. It then maximizes the module throughput with multi-tuple configurations and proper amount of dummy requests. It also carefully splits the end-to-end latency into per-module latency budget to minimize the total serving cost for multi-DNN applications. Evaluation shows that Harpagon outperforms the state of the art by 1.49 to 2.37 times in serving cost while satisfying the latency objectives. Additionally, compared to the optimal solution using brute force search, Harpagon derives the lower bound of serving cost for 91.5% workloads with millisecond level runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06161v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhixin Zhao, Yitao Hu, Ziqi Gong, Guotao Yang, Wenxin Li, Xiulong Liu, Keqiu Li, Hao Wang</dc:creator>
    </item>
    <item>
      <title>Fed-LDR: Federated Local Data-infused Graph Creation with Node-centric Model Refinement</title>
      <link>https://arxiv.org/abs/2411.04936</link>
      <description>arXiv:2411.04936v1 Announce Type: cross 
Abstract: The rapid acceleration of global urbanization has introduced novel challenges in enhancing urban infrastructure and services. Spatio-temporal data, integrating spatial and temporal dimensions, has emerged as a critical tool for understanding urban phenomena and promoting sustainability. In this context, Federated Learning (FL) has gained prominence as a distributed learning paradigm aligned with the privacy requirements of urban IoT environments. However, integrating traditional and deep learning models into the FL framework poses significant challenges, particularly in capturing complex spatio-temporal dependencies and adapting to diverse urban conditions. To address these challenges, we propose the Federated Local Data-Infused Graph Creation with Node-centric Model Refinement (Fed-LDR) algorithm. Fed-LDR leverages FL and Graph Convolutional Networks (GCN) to enhance spatio-temporal data analysis in urban environments. The algorithm comprises two key modules: (1) the Local Data-Infused Graph Creation (LDIGC) module, which dynamically reconfigures adjacency matrices to reflect evolving spatial relationships within urban environments, and (2) the Node-centric Model Refinement (NoMoR) module, which customizes model parameters for individual urban nodes to accommodate heterogeneity. Evaluations on the PeMSD4 and PeMSD8 datasets demonstrate Fed-LDR's superior performance over six baseline methods. Fed-LDR achieved the lowest Mean Absolute Error (MAE) values of 20.15 and 17.30, and the lowest Root Mean Square Error (RMSE) values of 32.30 and 27.15, respectively, while maintaining a high correlation coefficient of 0.96 across both datasets. Notably, on the PeMSD4 dataset, Fed-LDR reduced MAE and RMSE by up to 81\% and 78\%, respectively, compared to the best-performing baseline FedMedian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04936v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.SI</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiechao Gao, Yuangang Li, Syeda Faiza Ahmed</dc:creator>
    </item>
    <item>
      <title>RISC-V Word-Size Modular Instructions for Residue Number Systems</title>
      <link>https://arxiv.org/abs/2412.05286</link>
      <description>arXiv:2412.05286v1 Announce Type: cross 
Abstract: Residue Number Systems (RNS) are parallel number systems that allow the computation on large numbers. They are used in high performance digital signal processing devices and cryptographic applications. However, the rigidity of instruction set architectures of the market-dominant microprocessors limits the use of such number systems in software applications. This article presents the impact of word-size modular arithmetic specific RISC-V instructions on the software implementation of Residue Number Systems. We evaluate this impact on several RNS modular multiplication sequential algorithms. We observe that the fastest implementation uses the Kawamura et. al. base extension. Simulations of architectures with GEM5 simulator show that RNS modular multiplication with Kawamura's base extension is 2.76 times faster using specific word-size modular arithmetic instructions than pseudo-Mersenne moduli for In Order processors. It is more than 3 times for Out of Order processors. Compared to x86 architectures, RISC-V simulations show that using specific instructions requires 4.5 times less cycles in In Order processors and 8 less in Out of Order ones.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05286v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-73122-8_5</arxiv:DOI>
      <arxiv:journal_reference>Future Technologies Conference (FTC) 2024, The Science and Information (SAI) Organization, Nov 2024, London, United Kingdom. pp.68-86</arxiv:journal_reference>
      <dc:creator>Laurent-St\'ephane Didier (IMATH), Jean-Marc Robert (IMATH)</dc:creator>
    </item>
    <item>
      <title>A High Energy-Efficiency Multi-core Neuromorphic Architecture for Deep SNN Training</title>
      <link>https://arxiv.org/abs/2412.05302</link>
      <description>arXiv:2412.05302v1 Announce Type: cross 
Abstract: There is a growing necessity for edge training to adapt to dynamically changing environment. Neuromorphic computing represents a significant pathway for high-efficiency intelligent computation in energy-constrained edges, but existing neuromorphic architectures lack the ability of directly training spiking neural networks (SNNs) based on backpropagation. We develop a multi-core neuromorphic architecture with Feedforward-Propagation, Back-Propagation, and Weight-Gradient engines in each core, supporting high efficient parallel computing at both the engine and core levels. It combines various data flows and sparse computation optimization by fully leveraging the sparsity in SNN training, obtaining a high energy efficiency of 1.05TFLOPS/W@ FP16 @ 28nm, 55 ~ 85% reduction of DRAM access compared to A100 GPU in SNN trainings, and a 20-core deep SNN training and a 5-worker federated learning on FPGAs. Our study develops the first multi-core neuromorphic architecture supporting the direct SNN training, facilitating the neuromorphic computing in edge-learnable applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05302v1</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingjing Li, Huihui Zhou, Xiaofeng Xu, Zhiwei Zhong, Puli Quan, Xueke Zhu, Yanyu Lin, Wenjie Lin, Hongyu Guo, Junchao Zhang, Yunhao Ma, Wei Wang, Zhengyu Ma, Guoqi Li, Xiaoxin Cui, Yonghong Tian</dc:creator>
    </item>
    <item>
      <title>FogROS2-FT: Fault Tolerant Cloud Robotics</title>
      <link>https://arxiv.org/abs/2412.05408</link>
      <description>arXiv:2412.05408v1 Announce Type: cross 
Abstract: Cloud robotics enables robots to offload complex computational tasks to cloud servers for performance and ease of management. However, cloud compute can be costly, cloud services can suffer occasional downtime, and connectivity between the robot and cloud can be prone to variations in network Quality-of-Service (QoS). We present FogROS2-FT (Fault Tolerant) to mitigate these issues by introducing a multi-cloud extension that automatically replicates independent stateless robotic services, routes requests to these replicas, and directs the first response back. With replication, robots can still benefit from cloud computations even when a cloud service provider is down or there is low QoS. Additionally, many cloud computing providers offer low-cost spot computing instances that may shutdown unpredictably. Normally, these low-cost instances would be inappropriate for cloud robotics, but the fault tolerance nature of FogROS2-FT allows them to be used reliably. We demonstrate FogROS2-FT fault tolerance capabilities in 3 cloud-robotics scenarios in simulation (visual object detection, semantic segmentation, motion planning) and 1 physical robot experiment (scan-pick-and-place). Running on the same hardware specification, FogROS2-FT achieves motion planning with up to 2.2x cost reduction and up to a 5.53x reduction on 99 Percentile (P99) long-tail latency. FogROS2-FT reduces the P99 long-tail latency of object detection and semantic segmentation by 2.0x and 2.1x, respectively, under network slowdown and resource contention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05408v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiyuan Chen, Kush Hari, Trinity Chung, Michael Wang, Nan Tian, Christian Juette, Jeffrey Ichnowski, Liu Ren, John Kubiatowicz, Ion Stoica, Ken Goldberg</dc:creator>
    </item>
    <item>
      <title>Partially Synchronous BFT Consensus Made Practical in Wireless Networks</title>
      <link>https://arxiv.org/abs/2412.05512</link>
      <description>arXiv:2412.05512v1 Announce Type: cross 
Abstract: Consensus is becoming increasingly important in wireless networks. Partially synchronous BFT consensus, a significant branch of consensus, has made considerable progress in wired networks. However, its implementation in wireless networks, especially in dynamic ad hoc wireless networks, remains challenging. Existing wireless synchronous consensus protocols, despite being well-developed, are not readily adaptable to partially synchronous settings. Additionally, reliable communication, a cornerstone of BFT consensus, can lead to high message and time complexity in wireless networks. To address these challenges, we propose a wireless communication protocol called ReduceCatch (Reduce and Catch) that supports reliable 1-to-N, N-to-1, and N-to-N communications. We employ ReduceCatch to tailor three partially synchronous BFT consensus protocols (PBFT, Tendermint, and HotStuff) for seamless adaptation from wired to ad hoc wireless networks. To evaluate the performance of the ReduceCatch-enabled consensus protocols, we develop a three-layer wireless consensus testbed, based on which we implement 20 distinct consensus protocols and measure their latency and throughput. The experimental results demonstrate the superiority of the ReduceCatch-based consensus protocol in terms of latency and throughput.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05512v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shuo Liu, Minghui Xu, Yuezhou Zheng, Yifei Zou, Wangjie Qiu, Gang Qu, Xiuzhen Cheng</dc:creator>
    </item>
    <item>
      <title>Dynamic Digital Twins of Blockchain Systems: State Extraction and Mirroring</title>
      <link>https://arxiv.org/abs/2412.05527</link>
      <description>arXiv:2412.05527v1 Announce Type: cross 
Abstract: Blockchain adoption is reaching an all-time high, with a plethora of blockchain architectures being developed to cover the needs of applications eager to integrate blockchain into their operations. However, blockchain systems suffer from the trilemma trade-off problem, which limits their ability to scale without sacrificing essential metrics such as decentralisation and security. The balance of the trilemma trade-off is primarily dictated by the consensus protocol used. Since consensus protocols are designed to function well under specific system conditions, and consequently, due to the blockchain's complex and dynamic nature, systems operating under a single consensus protocol are bound to face periods of inefficiency. The work presented in this paper constitutes part of an effort to design a Digital Twin-based blockchain management framework to balance the trilemma trade-off problem, which aims to adapt the consensus process to fit the conditions of the underlying system. Specifically, this work addresses the problems of extracting the blockchain system and mirroring it in its digital twin by proposing algorithms that overcome the challenges posed by blockchains' decentralised and asynchronous nature and the fundamental problems of global state and synchronisation in such systems. The robustness of the proposed algorithms is experimentally evaluated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05527v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Diamantopoulos, Nikos Tziritas, Rami Bahsoon, Nan Zhang, Georgios Theodoropoulos</dc:creator>
    </item>
    <item>
      <title>Upcycling Noise for Federated Unlearning</title>
      <link>https://arxiv.org/abs/2412.05529</link>
      <description>arXiv:2412.05529v1 Announce Type: cross 
Abstract: In Federated Learning (FL), multiple clients collaboratively train a model without sharing raw data. This paradigm can be further enhanced by Differential Privacy (DP) to protect local data from information inference attacks and is thus termed DPFL. An emerging privacy requirement, ``the right to be forgotten'' for clients, poses new challenges to DPFL but remains largely unexplored. Despite numerous studies on federated unlearning (FU), they are inapplicable to DPFL because the noise introduced by the DP mechanism compromises their effectiveness and efficiency. In this paper, we propose Federated Unlearning with Indistinguishability (FUI) to unlearn the local data of a target client in DPFL for the first time. FUI consists of two main steps: local model retraction and global noise calibration, resulting in an unlearning model that is statistically indistinguishable from the retrained model. Specifically, we demonstrate that the noise added in DPFL can endow the unlearning model with a certain level of indistinguishability after local model retraction, and then fortify the degree of unlearning through global noise calibration. Additionally, for the efficient and consistent implementation of the proposed FUI, we formulate a two-stage Stackelberg game to derive optimal unlearning strategies for both the server and the target client. Privacy and convergence analyses confirm theoretical guarantees, while experimental results based on four real-world datasets illustrate that our proposed FUI achieves superior model performance and higher efficiency compared to mainstream FU schemes. Simulation results further verify the optimality of the derived unlearning strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05529v1</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jianan Chen, Qin Hu, Fangtian Zhong, Yan Zhuang, Minghui Xu</dc:creator>
    </item>
    <item>
      <title>Radiant: Large-scale 3D Gaussian Rendering based on Hierarchical Framework</title>
      <link>https://arxiv.org/abs/2412.05546</link>
      <description>arXiv:2412.05546v1 Announce Type: cross 
Abstract: With the advancement of computer vision, the recently emerged 3D Gaussian Splatting (3DGS) has increasingly become a popular scene reconstruction algorithm due to its outstanding performance. Distributed 3DGS can efficiently utilize edge devices to directly train on the collected images, thereby offloading computational demands and enhancing efficiency. However, traditional distributed frameworks often overlook computational and communication challenges in real-world environments, hindering large-scale deployment and potentially posing privacy risks. In this paper, we propose Radiant, a hierarchical 3DGS algorithm designed for large-scale scene reconstruction that considers system heterogeneity, enhancing the model performance and training efficiency. Via extensive empirical study, we find that it is crucial to partition the regions for each edge appropriately and allocate varying camera positions to each device for image collection and training. The core of Radiant is partitioning regions based on heterogeneous environment information and allocating workloads to each device accordingly. Furthermore, we provide a 3DGS model aggregation algorithm that enhances the quality and ensures the continuity of models' boundaries. Finally, we develop a testbed, and experiments demonstrate that Radiant improved reconstruction quality by up to 25.7\% and reduced up to 79.6\% end-to-end latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05546v1</guid>
      <category>cs.CV</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haosong Peng, Tianyu Qi, Yufeng Zhan, Hao Li, Yalun Dai, Yuanqing Xia</dc:creator>
    </item>
    <item>
      <title>FedRBE -- a decentralized privacy-preserving federated batch effect correction tool for omics data based on limma</title>
      <link>https://arxiv.org/abs/2412.05894</link>
      <description>arXiv:2412.05894v1 Announce Type: cross 
Abstract: Batch effects in omics data obscure true biological signals and constitute a major challenge for privacy-preserving analyses of distributed patient data. Existing batch effect correction methods either require data centralization, which may easily conflict with privacy requirements, or lack support for missing values and automated workflows. To bridge this gap, we developed fedRBE, a federated implementation of limma's removeBatchEffect method. We implemented it as an app for the FeatureCloud platform. Unlike its existing analogs, fedRBE effectively handles data with missing values and offers an automated, user-friendly online user interface (https://featurecloud.ai/app/fedrbe). Leveraging secure multi-party computation provides enhanced security guarantees over classical federated learning approaches. We evaluated our fedRBE algorithm on simulated and real omics data, achieving performance comparable to the centralized method with negligible differences (no greater than 3.6E-13). By enabling collaborative correction without data sharing, fedRBE facilitates large-scale omics studies where batch effect correction is crucial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05894v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuliya Burankova, Julian Klemm, Jens J. G. Lohmann, Ahmad Taheri, Niklas Probul, Jan Baumbach, Olga Zolotareva</dc:creator>
    </item>
    <item>
      <title>Fully Distributed Online Training of Graph Neural Networks in Networked Systems</title>
      <link>https://arxiv.org/abs/2412.06105</link>
      <description>arXiv:2412.06105v1 Announce Type: cross 
Abstract: Graph neural networks (GNNs) are powerful tools for developing scalable, decentralized artificial intelligence in large-scale networked systems, such as wireless networks, power grids, and transportation networks. Currently, GNNs in networked systems mostly follow a paradigm of `centralized training, distributed execution', which limits their adaptability and slows down their development cycles. In this work, we fill this gap for the first time by developing a communication-efficient, fully distributed online training approach for GNNs applied to large networked systems. For a mini-batch with $B$ samples, our approach of training an $L$-layer GNN only adds $L$ rounds of message passing to the $LB$ rounds required by GNN inference, with doubled message sizes. Through numerical experiments in graph-based node regression, power allocation, and link scheduling in wireless networks, we demonstrate the effectiveness of our approach in training GNNs under supervised, unsupervised, and reinforcement learning paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06105v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rostyslav Olshevskyi, Zhongyuan Zhao, Kevin Chan, Gunjan Verma, Ananthram Swami, Santiago Segarra</dc:creator>
    </item>
    <item>
      <title>Lightweight Federated Learning with Differential Privacy and Straggler Resilience</title>
      <link>https://arxiv.org/abs/2412.06120</link>
      <description>arXiv:2412.06120v1 Announce Type: cross 
Abstract: Federated learning (FL) enables collaborative model training through model parameter exchanges instead of raw data. To avoid potential inference attacks from exchanged parameters, differential privacy (DP) offers rigorous guarantee against various attacks. However, conventional methods of ensuring DP by adding local noise alone often result in low training accuracy. Combining secure multi-party computation (SMPC) with DP, while improving the accuracy, incurs high communication and computation overheads and straggler vulnerability, in either client-to-server or client-to-client links. In this paper, we propose LightDP-FL, a novel lightweight scheme that ensures provable DP against untrusted peers and server, while maintaining straggler-resilience, low overheads and high training accuracy. Our approach incorporates both individual and pairwise noise into each client's parameter, which can be implemented with minimal overheads. Given the uncertain straggler and colluder sets, we utilize the upper bound on the numbers of stragglers and colluders to prove sufficient noise variance conditions to ensure DP in the worst case. Moreover, we optimize the expected convergence bound to ensure accuracy performance by flexibly controlling the noise variances. Using the CIFAR-10 dataset, our experimental results demonstrate that LightDP-FL achieves faster convergence and stronger straggler resilience of our scheme compared to baseline methods of the same DP level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06120v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Hong, Xiaojun Lin, Lingjie Duan</dc:creator>
    </item>
    <item>
      <title>Federated Split Learning with Model Pruning and Gradient Quantization in Wireless Networks</title>
      <link>https://arxiv.org/abs/2412.06414</link>
      <description>arXiv:2412.06414v1 Announce Type: cross 
Abstract: As a paradigm of distributed machine learning, federated learning typically requires all edge devices to train a complete model locally. However, with the increasing scale of artificial intelligence models, the limited resources on edge devices often become a bottleneck for efficient fine-tuning. To address this challenge, federated split learning (FedSL) implements collaborative training across the edge devices and the server through model splitting. In this paper, we propose a lightweight FedSL scheme, that further alleviates the training burden on resource-constrained edge devices by pruning the client-side model dynamicly and using quantized gradient updates to reduce computation overhead. Additionally, we apply random dropout to the activation values at the split layer to reduce communication overhead. We conduct theoretical analysis to quantify the convergence performance of the proposed scheme. Finally, simulation results verify the effectiveness and advantages of the proposed lightweight FedSL in wireless network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.06414v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/TVT.2024.3515083</arxiv:DOI>
      <dc:creator>Junhe Zhang, Wanli Ni, Dongyu Wang</dc:creator>
    </item>
    <item>
      <title>Regular Model Checking Upside-Down: An Invariant-Based Approach</title>
      <link>https://arxiv.org/abs/2205.03060</link>
      <description>arXiv:2205.03060v5 Announce Type: replace 
Abstract: Regular model checking is a technique for the verification of infinite-state systems whose configurations can be represented as finite words over a suitable alphabet. The form we are studying applies to systems whose set of initial configurations is regular, and whose transition relation is captured by a length-preserving transducer. To verify safety properties, regular model checking iteratively computes automata recognizing increasingly larger regular sets of reachable configurations, and checks if they contain unsafe configurations. Since this procedure often does not terminate, acceleration, abstraction, and widening techniques have been developed to compute a regular superset of the reachable configurations.
  In this paper, we develop a complementary procedure. Instead of approaching the set of reachable configurations from below, we start with the set of all configurations and approach it from above. We use that the set of reachable configurations is equal to the intersection of all inductive invariants of the system. Since this intersection is non-regular in general, we introduce b-invariants, defined as those representable by CNF-formulas with at most b clauses. We prove that, for every $b\geq0$, the intersection of all inductive b-invariants is regular, and we construct an automaton recognizing it. We show that whether this automaton accepts some unsafe configuration is in EXPSPACE for every $b\geq0$, and PSPACE-complete for b=1. Finally, we study how large must b be to prove safety properties of a number of benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.03060v5</guid>
      <category>cs.DC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Javier Esparza, Michael Raskin, Christoph Welzel-Mohr</dc:creator>
    </item>
    <item>
      <title>Energy Efficient Offloading Policies in Multi-Access Edge Computing Systems with Task Handover</title>
      <link>https://arxiv.org/abs/2306.15185</link>
      <description>arXiv:2306.15185v3 Announce Type: replace 
Abstract: The rapid growth of mobile devices and the increasing complexity of tasks have made energy efficiency a critical challenge in Multi-Access Edge Computing (MEC) systems. This paper explores energy-efficient offloading strategies in large-scale MEC systems with heterogeneous mobile users, diverse network components, and frequent task handovers to capture user mobility. The problem is inherently complex due to the system's scale, task and resource diversity, and the need to maintain real-time performance. Traditional optimization approaches are often computationally infeasible for such scenarios. To tackle these challenges, we model the offloading problem using the restless multi-armed bandit (RMAB) framework and develop two scalable online policies that prioritize resources based on their marginal costs. The proposed policies dynamically adapt to the system's heterogeneity and mobility while ensuring near-optimal energy efficiency. Through extensive numerical simulations, we demonstrate that the policies significantly outperform baseline methods in power conservation and show robust performance under non-exponentially distributed task lifespans. These results highlight the practical applicability and scalability of our approach in dynamic MEC environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.15185v3</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Hou, Shi Li, Zhishu Shen, Jing Fu, Jingjin Wu, Jiong Jin</dc:creator>
    </item>
    <item>
      <title>The Internet of Things in the Era of Generative AI: Vision and Challenges</title>
      <link>https://arxiv.org/abs/2401.01923</link>
      <description>arXiv:2401.01923v4 Announce Type: replace 
Abstract: Advancements in Generative AI hold immense promise to push Internet of Things (IoT) to the next level. In this article, we share our vision on IoT in the era of Generative AI. We discuss some of the most important applications of Generative AI in IoT-related domains. We also identify some of the most critical challenges and discuss current gaps as well as promising opportunities on enabling Generative AI for IoT. We hope this article can inspire new research on IoT in the era of Generative AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01923v4</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Wang, Zhongwei Wan, Arvin Hekmati, Mingyu Zong, Samiul Alam, Mi Zhang, Bhaskar Krishnamachari</dc:creator>
    </item>
    <item>
      <title>FIARSE: Model-Heterogeneous Federated Learning via Importance-Aware Submodel Extraction</title>
      <link>https://arxiv.org/abs/2407.19389</link>
      <description>arXiv:2407.19389v3 Announce Type: replace 
Abstract: In federated learning (FL), accommodating clients' varied computational capacities poses a challenge, often limiting the participation of those with constrained resources in global model training. To address this issue, the concept of model heterogeneity through submodel extraction has emerged, offering a tailored solution that aligns the model's complexity with each client's computational capacity. In this work, we propose Federated Importance-Aware Submodel Extraction (FIARSE), a novel approach that dynamically adjusts submodels based on the importance of model parameters, thereby overcoming the limitations of previous static and dynamic submodel extraction methods. Compared to existing works, the proposed method offers a theoretical foundation for the submodel extraction and eliminates the need for additional information beyond the model parameters themselves to determine parameter importance, significantly reducing the overhead on clients. Extensive experiments are conducted on various datasets to showcase the superior performance of the proposed FIARSE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.19389v3</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Feijie Wu, Xingchen Wang, Yaqing Wang, Tianci Liu, Lu Su, Jing Gao</dc:creator>
    </item>
    <item>
      <title>Impact of Conflicting Transactions in Blockchain: Detecting and Mitigating Potential Attacks</title>
      <link>https://arxiv.org/abs/2407.20980</link>
      <description>arXiv:2407.20980v2 Announce Type: replace 
Abstract: Conflicting transactions within blockchain networks not only pose performance challenges but also introduce security vulnerabilities, potentially facilitating malicious attacks. In this paper, we explore the impact of conflicting transactions on blockchain attack vectors. Through modeling and simulation, we delve into the dynamics of four pivotal attacks - block withholding, double spending, balance, and distributed denial of service (DDoS), all orchestrated using conflicting transactions. Our analysis not only focuses on the mechanisms through which these attacks exploit transaction conflicts but also underscores their potential impact on the integrity and reliability of blockchain networks. Additionally, we propose a set of countermeasures for mitigating these attacks. Through implementation and evaluation, we show their effectiveness in lowering attack rates and enhancing overall network performance seamlessly, without introducing additional overhead. Our findings emphasize the critical importance of actively managing conflicting transactions to reinforce blockchain security and performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20980v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faisal Haque Bappy, Tariqul Islam, Kamrul Hasan, Joon S. Park, Carlos Caicedo</dc:creator>
    </item>
    <item>
      <title>Securing Proof of Stake Blockchains: Leveraging Multi-Agent Reinforcement Learning for Detecting and Mitigating Malicious Nodes</title>
      <link>https://arxiv.org/abs/2407.20983</link>
      <description>arXiv:2407.20983v2 Announce Type: replace 
Abstract: Proof of Stake (PoS) blockchains offer promising alternatives to traditional Proof of Work (PoW) systems, providing scalability and energy efficiency. However, blockchains operate in a decentralized manner and the network is composed of diverse users. This openness creates the potential for malicious nodes to disrupt the network in various ways. Therefore, it is crucial to embed a mechanism within the blockchain network to constantly monitor, identify, and eliminate these malicious nodes without involving any central authority. In this paper, we propose MRL-PoS+, a novel consensus algorithm to enhance the security of PoS blockchains by leveraging Multi-agent Reinforcement Learning (MRL) techniques. Our proposed consensus algorithm introduces a penalty-reward scheme for detecting and eliminating malicious nodes. This approach involves the detection of behaviors that can lead to potential attacks in a blockchain network and hence penalizes the malicious nodes, restricting them from performing certain actions. Our developed Proof of Concept demonstrates effectiveness in eliminating malicious nodes for six types of major attacks. Experimental results demonstrate that MRL-PoS+ significantly improves the attack resilience of PoS blockchains compared to the traditional schemes without incurring additional computation overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20983v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faisal Haque Bappy, Tariqul Islam, Kamrul Hasan, Md Sajidul Islam Sajid, Mir Mehedi Ahsan Pritom</dc:creator>
    </item>
    <item>
      <title>Edge System Design Using Containers and Unikernels for IoT Applications</title>
      <link>https://arxiv.org/abs/2412.03032</link>
      <description>arXiv:2412.03032v2 Announce Type: replace 
Abstract: Edge computing is emerging as a key enabler of low-latency, high-efficiency processing for the Internet of Things (IoT) and other real-time applications. To support these demands, containerization has gained traction in edge computing due to its lightweight virtualization and efficient resource management. However, there is currently no established framework to leverage both containers and unikernels on edge devices for optimized IoT deployments. This paper proposes a hybrid edge system design that leverages container and unikernel technologies to optimize resource utilization based on application complexity. Containers are employed for resource-intensive applications, e.g., computer vision, providing faster processing, flexibility, and ease of deployment. In contrast, unikernels are used for lightweight applications, offering enhanced resource performance with minimal overhead. Our system design also incorporates container orchestration to efficiently manage multiple instances across the edge efficiently, ensuring scalability and reliability. We demonstrate our hybrid approach's performance and efficiency advantages through real-world computer vision and data science applications on ARM-powered edge device. Our results demonstrate that this hybrid approach improves resource utilization and reduces latency compared to traditional virtualized solutions. This work provides insights into optimizing edge infrastructures, enabling more efficient and specialized deployment strategies for diverse application workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03032v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shahidullah Kaiser, Ali Saman Tosun, Turgay Korkmaz</dc:creator>
    </item>
    <item>
      <title>An Approach to Optimizing the VABA Protocol Using $\kappa$-size Committee</title>
      <link>https://arxiv.org/abs/2412.03789</link>
      <description>arXiv:2412.03789v2 Announce Type: replace 
Abstract: Byzantine agreement protocols in asynchronous networks have gained renewed attention due to their independence from network timing assumptions to ensure termination. Traditional asynchronous Byzantine agreement protocols require every party to broadcast its requests (e.g., transactions), leading to high communication costs as parties ultimately agree on one party's request. This inefficiency is particularly significant in multi-valued Byzantine agreement protocols, where parties aim to agree on one party's requests under the assumption $n=3f+1$, where $n$ is the total number of parties, and $f$ is the number of Byzantine parties.
  To address these inefficiencies, we propose Efficient-VABA (eVABA), an optimized protocol for the asynchronous Byzantine agreement (ABA) problem. By limiting broadcasts to a selected subset of parties, the protocol reduces the number of messages and computation overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.03789v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nasit S Sony</dc:creator>
    </item>
    <item>
      <title>TriMe++: Multi-threaded triangular meshing in two dimensions</title>
      <link>https://arxiv.org/abs/2309.13824</link>
      <description>arXiv:2309.13824v3 Announce Type: replace-cross 
Abstract: We present TriMe++, a multi-threaded software library designed for generating two-dimensional meshes for intricate geometric shapes using the Delaunay triangulation. Multi-threaded parallel computing is implemented throughout the meshing procedure, making it suitable for fast generation of large-scale meshes. Three iterative meshing algorithms are implemented: the DistMesh algorithm, the centroidal Voronoi diagram meshing, and a hybrid of the two. We compare the performance of the three meshing methods in TriMe++, and show that the hybrid method retains the advantages of the other two. The software library achieves significant parallel speedup when generating large-scale meshes containing between $10^4$ to $10^7$ points. TriMe++ can handle complicated geometries and generates adaptive meshes of high quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.13824v3</guid>
      <category>cs.CG</category>
      <category>cs.DC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>physics.app-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiayin Lu, Chris H. Rycroft</dc:creator>
    </item>
    <item>
      <title>AGD: an Auto-switchable Optimizer using Stepwise Gradient Difference for Preconditioning Matrix</title>
      <link>https://arxiv.org/abs/2312.01658</link>
      <description>arXiv:2312.01658v2 Announce Type: replace-cross 
Abstract: Adaptive optimizers, such as Adam, have achieved remarkable success in deep learning. A key component of these optimizers is the so-called preconditioning matrix, providing enhanced gradient information and regulating the step size of each gradient direction. In this paper, we propose a novel approach to designing the preconditioning matrix by utilizing the gradient difference between two successive steps as the diagonal elements. These diagonal elements are closely related to the Hessian and can be perceived as an approximation of the inner product between the Hessian row vectors and difference of the adjacent parameter vectors. Additionally, we introduce an auto-switching function that enables the preconditioning matrix to switch dynamically between Stochastic Gradient Descent (SGD) and the adaptive optimizer. Based on these two techniques, we develop a new optimizer named AGD that enhances the generalization performance. We evaluate AGD on public datasets of Natural Language Processing (NLP), Computer Vision (CV), and Recommendation Systems (RecSys). Our experimental results demonstrate that AGD outperforms the state-of-the-art (SOTA) optimizers, achieving highly competitive or significantly better predictive performance. Furthermore, we analyze how AGD is able to switch automatically between SGD and the adaptive optimizer and its actual effects on various scenarios. The code is available at https://github.com/intelligent-machine-learning/atorch/tree/main/atorch/optimizers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01658v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yun Yue, Zhiling Ye, Jiadi Jiang, Yongchao Liu, Ke Zhang</dc:creator>
    </item>
    <item>
      <title>SPEAR:Exact Gradient Inversion of Batches in Federated Learning</title>
      <link>https://arxiv.org/abs/2403.03945</link>
      <description>arXiv:2403.03945v3 Announce Type: replace-cross 
Abstract: Federated learning is a framework for collaborative machine learning where clients only share gradient updates and not their private data with a server. However, it was recently shown that gradient inversion attacks can reconstruct this data from the shared gradients. In the important honest-but-curious setting, existing attacks enable exact reconstruction only for batch size of $b=1$, with larger batches permitting only approximate reconstruction. In this work, we propose SPEAR, the first algorithm reconstructing whole batches with $b &gt;1$ exactly. SPEAR combines insights into the explicit low-rank structure of gradients with a sampling-based algorithm. Crucially, we leverage ReLU-induced gradient sparsity to precisely filter out large numbers of incorrect samples, making a final reconstruction step tractable. We provide an efficient GPU implementation for fully connected networks and show that it recovers high-dimensional ImageNet inputs in batches of up to $b \lesssim 25$ exactly while scaling to large networks. Finally, we show theoretically that much larger batches can be reconstructed with high probability given exponential time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03945v3</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Dimitar I. Dimitrov, Maximilian Baader, Mark Niklas M\"uller, Martin Vechev</dc:creator>
    </item>
    <item>
      <title>Asynchronous Federated Reinforcement Learning with Policy Gradient Updates: Algorithm Design and Convergence Analysis</title>
      <link>https://arxiv.org/abs/2404.08003</link>
      <description>arXiv:2404.08003v4 Announce Type: replace-cross 
Abstract: To improve the efficiency of reinforcement learning (RL), we propose a novel asynchronous federated reinforcement learning (FedRL) framework termed AFedPG, which constructs a global model through collaboration among $N$ agents using policy gradient (PG) updates. To address the challenge of lagged policies in asynchronous settings, we design a delay-adaptive lookahead technique \textit{specifically for FedRL} that can effectively handle heterogeneous arrival times of policy gradients. We analyze the theoretical global convergence bound of AFedPG, and characterize the advantage of the proposed algorithm in terms of both the sample complexity and time complexity. Specifically, our AFedPG method achieves $O(\frac{{\epsilon}^{-2.5}}{N})$ sample complexity for global convergence at each agent on average. Compared to the single agent setting with $O(\epsilon^{-2.5})$ sample complexity, it enjoys a linear speedup with respect to the number of agents. Moreover, compared to synchronous FedPG, AFedPG improves the time complexity from $O(\frac{t_{\max}}{N})$ to $O({\sum_{i=1}^{N} \frac{1}{t_{i}}})^{-1}$, where $t_{i}$ denotes the time consumption in each iteration at agent $i$, and $t_{\max}$ is the largest one. The latter complexity $O({\sum_{i=1}^{N} \frac{1}{t_{i}}})^{-1}$ is always smaller than the former one, and this improvement becomes significant in large-scale federated settings with heterogeneous computing powers ($t_{\max}\gg t_{\min}$). Finally, we empirically verify the improved performance of AFedPG in four widely-used MuJoCo environments with varying numbers of agents. We also demonstrate the advantages of AFedPG in various computing heterogeneity scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08003v4</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guangchen Lan, Dong-Jun Han, Abolfazl Hashemi, Vaneet Aggarwal, Christopher G. Brinton</dc:creator>
    </item>
    <item>
      <title>A Survey on Privacy-Preserving Caching at Network Edge: Classification, Solutions, and Challenges</title>
      <link>https://arxiv.org/abs/2405.01844</link>
      <description>arXiv:2405.01844v3 Announce Type: replace-cross 
Abstract: Caching content at the edge network is a popular and effective technique widely deployed to alleviate the burden of network backhaul, shorten service delay and improve service quality. However, there has been some controversy over privacy violations in caching content at the edge network. On the one hand, the multi-access open edge network provides an ideal entrance or interface for external attackers to obtain private data from edge caches by extracting sensitive information. On the other hand, privacy can be infringed on by curious edge caching providers through caching trace analysis targeting the achievement of better caching performance or higher profits. Therefore, an in-depth understanding of privacy issues in edge caching networks is vital and indispensable for creating a privacy-preserving caching service at the edge network. In this article, we are among the first to fill this gap by examining privacy-preserving techniques for caching content at the edge network. Firstly, we provide an introduction to the background of privacy-preserving edge caching (PPEC). Next, we summarize the key privacy issues and present a taxonomy for caching at the edge network from the perspective of private information. Additionally, we conduct a retrospective review of the state-of-the-art countermeasures against privacy leakage from content caching at the edge network. Finally, we conclude the survey and envision challenges for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01844v3</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3706630</arxiv:DOI>
      <dc:creator>Xianzhi Zhang, Yipeng Zhou, Di Wu, Quan Z. Sheng, Shazia Riaz, Miao Hu, Linchang Xiao</dc:creator>
    </item>
    <item>
      <title>Credible fusion of evidence in distributed system subject to cyberattacks</title>
      <link>https://arxiv.org/abs/2412.04496</link>
      <description>arXiv:2412.04496v2 Announce Type: replace-cross 
Abstract: Given that distributed systems face adversarial behaviors such as eavesdropping and cyberattacks, how to ensure the evidence fusion result is credible becomes a must-be-addressed topic. Different from traditional research that assumes nodes are cooperative, we focus on three requirements for evidence fusion, i.e., preserving evidence's privacy, identifying attackers and excluding their evidence, and dissipating high-conflicting among evidence caused by random noise and interference. To this end, this paper proposes an algorithm for credible evidence fusion against cyberattacks. Firstly, the fusion strategy is constructed based on conditionalized credibility to avoid counterintuitive fusion results caused by high-conflicting. Under this strategy, distributed evidence fusion is transformed into the average consensus problem for the weighted average value by conditional credibility of multi-source evidence (WAVCCME), which implies a more concise consensus process and lower computational complexity than existing algorithms. Secondly, a state decomposition and reconstruction strategy with weight encryption is designed, and its effectiveness for privacy-preserving under directed graphs is guaranteed: decomposing states into different random sub-states for different neighbors to defend against internal eavesdroppers, and encrypting the sub-states' weight in the reconstruction to guard against out-of-system eavesdroppers. Finally, the identities and types of attackers are identified by inter-neighbor broadcasting and comparison of nodes' states, and the proposed update rule with state corrections is used to achieve the consensus of the WAVCCME. The states of normal nodes are shown to converge to their WAVCCME, while the attacker's evidence is excluded from the fusion, as verified by the simulation on a distributed unmanned reconnaissance swarm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04496v2</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 10 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chaoxiong Ma, Yan Liang</dc:creator>
    </item>
  </channel>
</rss>
