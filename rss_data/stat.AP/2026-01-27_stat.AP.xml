<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 27 Jan 2026 05:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Bayesian Multiple Testing for Suicide Risk in Pharmacoepidemiology: Leveraging Co-Prescription Patterns</title>
      <link>https://arxiv.org/abs/2601.17985</link>
      <description>arXiv:2601.17985v1 Announce Type: new 
Abstract: Suicide is the tenth leading cause of death in the United States, yet evidence on medication-related risk or protection remains limited. Most post-marketing studies examine one drug class at a time or rely on empirical-Bayes shrinkage with conservative multiplicity corrections, sacrificing power to detect clinically meaningful signals. We introduce a unified Bayesian spike-and-slab framework that advances both applied suicide research and statistical methodology. Substantively, we screen 922 prescription drugs across 150 million patients in U.S. commercial claims (2003 to 2014), leveraging real-world co-prescription patterns to inform a covariance prior that adaptively borrows strength across pharmacologically related agents. Statistically, the model couples this structured prior with Bayesian false-discovery-rate control, illustrating how network-guided variable selection can improve rare-event surveillance in high dimensions. Relative to the seminal empirical-Bayes analysis of Gibbons et al. (2019), our approach reconfirms the key harmful (e.g., alprazolam, hydrocodone) and protective (e.g., mirtazapine, folic acid) signals while revealing additional associations, such as a high-risk opioid combination and several folate-linked agents with potential preventive benefit that had been overlooked. A focused re-analysis of 18 antidepressants shows how alternative co-prescription metrics modulate effect estimates, shedding light on competitive versus complementary prescribing. These findings generate actionable hypotheses for clinicians and regulators and showcase the value of structured Bayesian modeling in pharmacovigilance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17985v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Kwan Hur, Dulal K. Bhaumik, Robert Gibbons</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification in Coupled Multiphysics Systems via Gaussian Process Surrogates: Application to Fuel Assembly Bow</title>
      <link>https://arxiv.org/abs/2601.18480</link>
      <description>arXiv:2601.18480v1 Announce Type: new 
Abstract: Predicting fuel assembly bow in pressurized water reactors requires solving tightly coupled fluid-structure interaction problems, whose direct simulations can be computationally prohibitive, making large-scale uncertainty quantification (UQ) very challenging. This work introduces a general mathematical framework for coupling Gaussian process (GP) surrogate models representing distinct physical solvers, aimed at enabling rigorous UQ in coupled multiphysics systems. A theoretical analysis establishes that the predictive variance of the coupled GP system remains bounded under mild regularity and stability assumptions, ensuring that uncertainty does not grow uncontrollably through the iterative coupling process. The methodology is then applied to the coupled hydraulic-structural simulation of fuel assembly bow, enabling global sensitivity analysis and full UQ at a fraction of the computational cost of direct code coupling. The results demonstrate accurate uncertainty propagation and stable predictions, establishing a solid mathematical basis for surrogate-based coupling in large-scale multiphysics simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18480v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali Abboud, Josselin Garnier, Bertrand Leturcq, Stanislas de Lambert</dc:creator>
    </item>
    <item>
      <title>A varying-coefficient model for characterizing duration-driven heterogeneity in flood-related health impacts</title>
      <link>https://arxiv.org/abs/2601.18656</link>
      <description>arXiv:2601.18656v1 Announce Type: new 
Abstract: Previous work revealed associations between flood exposure and adverse health outcomes during and in the aftermath of flood events. Floods are highly heterogeneous events, largely owing to vast differences in flood durations, i.e., flash-floods versus slow-moving floods. However, little to no work has incorporated exposure duration into the modeling of flood-related health impacts or has investigated duration-driven effect heterogeneity. To address this gap, we propose an exposure duration varying coefficient modeling (EDVCM) framework for estimating exposure day-specific health effects of consecutive-day environmental exposures that vary in duration. We develop the EDVCM within an area-level self-matched study design to eliminate time-invariant confounding followed by conditional Poisson regression modeling for exposure effect estimation and adjustment of time-varying confounders. Using a Bayesian framework, we introduce duration- and exposure day-specific exposure coefficients within the conditional Poisson model and assign them a two-dimensional Gaussian process prior to allow for sharing of information across both duration and exposure day. This approach enables highly-resolved insights into duration-driven effect heterogeneity while ensuring model stability through information sharing. Through simulations, we demonstrate that the EDVCM out-performs conventional approaches in terms of both effect estimation and uncertainty quantification. We apply the EDVCM to nationwide, multi-decade Medicare claims data linked with high-resolution flood exposure measures to investigate duration-driven heterogeneity in flood effects on musculoskeletal system disease hospitalizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18656v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarika Aggarwal, Phillip B. Nicol, Brent A. Coull, Rachel C. Nethery</dc:creator>
    </item>
    <item>
      <title>Optimizing the Landscape of LLM Embeddings with Dynamic Exploratory Graph Analysis for Generative Psychometrics: A Monte Carlo Study</title>
      <link>https://arxiv.org/abs/2601.17010</link>
      <description>arXiv:2601.17010v1 Announce Type: cross 
Abstract: Large language model (LLM) embeddings are increasingly used to estimate dimensional structure in psychological item pools prior to data collection, yet current applications treat embeddings as static, cross-sectional representations. This approach implicitly assumes uniform contribution across all embedding coordinates and overlooks the possibility that optimal structural information may be concentrated in specific regions of the embedding space. This study reframes embeddings as searchable landscapes and adapts Dynamic Exploratory Graph Analysis (DynEGA) to systematically traverse embedding coordinates, treating the dimension index as a pseudo-temporal ordering analogous to intensive longitudinal trajectories. A large-scale Monte Carlo simulation embedded items representing five dimensions of grandiose narcissism using OpenAI's text-embedding-3-small model, generating network estimations across systematically varied item pool sizes (3-40 items per dimension) and embedding depths (3-1,298 dimensions). Results reveal that Total Entropy Fit Index (TEFI) and Normalized Mutual Information (NMI) leads to competing optimization trajectories across the embedding landscape. TEFI achieves minima at deep embedding ranges (900--1,200 dimensions) where entropy-based organization is maximal but structural accuracy degrades, whereas NMI peaks at shallow depths where dimensional recovery is strongest but entropy-based fit remains suboptimal. Single-metric optimization produces structurally incoherent solutions, whereas a weighted composite criterion identifies embedding dimensions depth regions that jointly balance accuracy and organization. Optimal embedding depth scales systematically with item pool size. These findings establish embedding landscapes as non-uniform semantic spaces requiring principled optimization rather than default full-vector usage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17010v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hudson Golino</dc:creator>
    </item>
    <item>
      <title>Evaluating Aggregated Relational Data Models with Simple Diagnostics</title>
      <link>https://arxiv.org/abs/2601.17153</link>
      <description>arXiv:2601.17153v1 Announce Type: cross 
Abstract: Aggregated Relational Data (ARD) contain summary information about individual social networks and are widely used to estimate social network characteristics and the size of populations of interest. Although a variety of ARD estimators exist, practitioners currently lack guidance on how to evaluate whether a selected model adequately fits the data. We introduce a diagnostic framework for ARD models that provides a systematic, reproducible process for assessing covariate structure, distributional assumptions, and correlation. The diagnostics are based on point estimates, using either maximum likelihood or maximum a posteriori optimization, which allows quick evaluation without requiring repeated Bayesian model fitting. Through simulation studies and applications to large ARD datasets, we show that the proposed workflow identifies common sources of model misfit and helps researchers select an appropriate model that adequately explains the data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17153v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ian Laga, Benjamin Vogel, Jieyun Wang, Anna Smith, Owen Ward</dc:creator>
    </item>
    <item>
      <title>An Empirical Method for Analyzing Count Data</title>
      <link>https://arxiv.org/abs/2601.17233</link>
      <description>arXiv:2601.17233v1 Announce Type: cross 
Abstract: Count endpoints are common in clinical trials, particularly for recurrent events such as hypoglycemia. When interest centers on comparing overall event rates between treatment groups, negative binomial (NB) regression is widely used because it accommodates overdispersion and requires only event counts and exposure times. However, NB regression can be numerically unstable when events are sparse, and the efficiency gains from baseline covariate adjustment may be sensitive to model misspecification. We propose an empirical method that targets the same marginal estimand as NB regression -- the ratio of marginal event rates -- while avoiding distributional assumptions on the count outcome. Simulation studies show that the empirical method maintains appropriate Type I error control across diverse scenarios, including extreme overdispersion and zero inflation, achieves power comparable to NB regression, and yields consistent efficiency gains from baseline covariate adjustment. We illustrate the approach using severe hypoglycemia data from the QWINT-5 trial comparing insulin efsitora alfa with insulin degludec in adults with type 1 diabetes. In this sparse-event setting, the empirical method produced stable marginal rate estimates and rate ratios closely aligned with observed rates, while NB regression exhibited greater sensitivity and larger deviations from the observed rates in the sparsest intervals. The proposed empirical method provides a robust and numerically stable alternative to NB regression, particularly when the number of events is low or when numerical stability is a concern.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17233v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiren Sun, Linda Amoafo, Yongming Qu</dc:creator>
    </item>
    <item>
      <title>Safety, Mobility, and Environmental Impacts of Driver-Assistance-Enabled Electric Vehicles: An Empirical Study</title>
      <link>https://arxiv.org/abs/2601.17256</link>
      <description>arXiv:2601.17256v1 Announce Type: cross 
Abstract: The advancement of vehicle automation and the growing adoption of electric vehicles (EVs) are reshaping transportation systems. While fully automated vehicles are expected to improve traffic stability, efficiency, and sustainability, recent studies suggest that partially automated vehicles, such as those equipped with adaptive cruise control (ACC), may adversely affect traffic flow. These drawbacks may not extend to ACC-enabled EVs due to their distinct mechanical characteristics, including regenerative braking and smoother torque delivery. As a result, the impacts of EVs operating under ACC remain insufficiently understood.
  To address this gap, this study develops an empirical framework using the OpenACC dataset to compare ACC-enabled EVs and internal combustion engine vehicles. Dynamic time warping aligns comparable lead-vehicle trajectories. Results show that EVs exhibit smoother speed profiles, lower speed variability, and shorter spacing, leading to higher efficiency. EVs reduce critical safety events by over 85% and lower platoon-level emissions by up to 26.2%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17256v1</guid>
      <category>cs.ET</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Geffen, Jun Zhao, Mingfeng Shang, Shian Wang, Yao-Jan Wu</dc:creator>
    </item>
    <item>
      <title>Comparisons of policies based on relevation and replacement by a new one unit in reliability</title>
      <link>https://arxiv.org/abs/2601.17518</link>
      <description>arXiv:2601.17518v1 Announce Type: cross 
Abstract: The purpose of this paper is to study the role of the relevation transform, where a failed unit is replaced by a used unit with the same age as the failed one, as an alternative to the policy based on the replacement by a new one. In particular, we compare the stochastic processes arising from a policy based on the replacement of a failed unit by a new one and from the one in which the unit is being continuously subjected to a relevation policy. The comparisons depend on the aging properties of the units under repair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.17518v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11749-020-00710-6</arxiv:DOI>
      <arxiv:journal_reference>TEST (2021), 30, 211-227</arxiv:journal_reference>
      <dc:creator> Belzunce,  F.,  Mart\'inez-Riquelme,  C.,  Mercader, J. A.,  Ruiz, J. M</dc:creator>
    </item>
    <item>
      <title>The effect of collinearity and sample size on linear regression results: a simulation study</title>
      <link>https://arxiv.org/abs/2601.18072</link>
      <description>arXiv:2601.18072v1 Announce Type: cross 
Abstract: Background: Multicollinearity inflates the variance of OLS coefficients, widening confidence intervals and reducing inferential reliability. Yet fixed variance inflation factor (VIF) cut-offs are often applied uniformly across studies with very different sample sizes, even though collinearity is a finite-sample problem. We quantify how collinearity and sample size jointly affect linear regression performance and provide practical guidance for interpreting VIFs.
  Methods: We simulated data across sample sizes N=100-100,000 and collinearity levels VIF=1-50. For each scenario we generated 1,000 datasets, fitted OLS models, and assessed coverage, mean absolute error (MAE), bias, traditional power (CI excludes 0), and precision assurance (probability the 95% CI lies within a prespecified margin around the true effect). We also evaluated a biased, misspecified setting by omitting a relevant predictor to study bias amplification.
  Results: Under correct specification, collinearity did not materially affect nominal coverage and did not introduce systematic bias, but it reduced precision in small samples: at N=100, even mild collinearity (VIF&lt;2) inflated MAE and markedly reduced both power metrics, whereas at N&gt;=50,000 estimates were robust even at VIF=50. Under misspecification, collinearity strongly amplified bias, increasing errors, reducing coverage, and sharply degrading both precision assurance and traditional power even at low VIF.
  Conclusion: VIF thresholds should not be applied mechanically. Collinearity must be interpreted in relation to sample size and potential sources of bias; removing predictors solely to reduce VIF can worsen inference via omitted-variable bias. The accompanying heatmaps provide a practical reference across study sizes and modelling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18072v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephanie CC van der Lubbe, Jose M Valderas, Evangelos Kontopantelis</dc:creator>
    </item>
    <item>
      <title>"Crash Test Dummies" for AI-Enabled Clinical Assessment: Validating Virtual Patient Scenarios with Virtual Learners</title>
      <link>https://arxiv.org/abs/2601.18085</link>
      <description>arXiv:2601.18085v1 Announce Type: cross 
Abstract: Background: In medical and health professions education (HPE), AI is increasingly used to assess clinical competencies, including via virtual standardized patients. However, most evaluations rely on AI-human interrater reliability and lack a measurement framework for how cases, learners, and raters jointly shape scores. This leaves robustness uncertain and can expose learners to misguidance from unvalidated systems. We address this by using AI "simulated learners" to stress-test and psychometrically characterize assessment pipelines before human use.
  Objective: Develop an open-source AI virtual patient platform and measurement model for robust competency evaluation across cases and rating conditions.
  Methods: We built a platform with virtual patients, virtual learners with tunable ACGME-aligned competency profiles, and multiple independent AI raters scoring encounters with structured Key-Features items. Transcripts were analyzed with a Bayesian HRM-SDT model that treats ratings as decisions under uncertainty and separates learner ability, case performance, and rater behavior; parameters were estimated with MCMC.
  Results: The model recovered simulated learners' competencies, with significant correlations to the generating competencies across all ACGME domains despite a non-deterministic pipeline. It estimated case difficulty by competency and showed stable rater detection (sensitivity) and criteria (severity/leniency thresholds) across AI raters using identical models/prompts but different seeds. We also propose a staged "safety blueprint" for deploying AI tools with learners, tied to entrustment-based validation milestones.
  Conclusions: Combining a purpose-built virtual patient platform with a principled psychometric model enables robust, interpretable, generalizable competency estimates and supports validation of AI-assisted assessment prior to use with human learners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18085v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Brian Gin, Ahreum Lim, Fl\'avia Silva e Oliveira, Kuan Xing, Xiaomei Song, Gayana Amiyangoda, Thilanka Seneviratne, Alison F. Doubleday, Ananya Gangopadhyaya, Bob Kiser, Lukas Shum-Tim, Dhruva Patel, Kosala Marambe, Lauren Maggio, Ara Tekian, Yoon Soo Park</dc:creator>
    </item>
    <item>
      <title>Vaccine Efficacy Estimands Implied by Common Estimators Used in Individual Randomized Field Trials</title>
      <link>https://arxiv.org/abs/2601.18587</link>
      <description>arXiv:2601.18587v1 Announce Type: cross 
Abstract: We review vaccine efficacy (VE) estimands for susceptibility in individual randomized trials with natural (unmeasured) exposure, where individual responses are measured as time from vaccination until an event (e.g., disease from the infectious agent). Common VE estimands are written as $1-\theta$, where $\theta$ is some ratio effect measure (e.g., ratio of incidence rates, cumulative incidences, hazards, or odds) comparing outcomes under vaccination versus control. Although the ratio effects are approximately equal with low control event rates, we explore the quality of that approximation using a nonparametric formulation. Traditionally, the primary endpoint VE estimands are full immunization (or biological) estimands that represent a subset of the intent-to-treat population, excluding those that have the event before the vaccine has been able to ramp-up to its full effect, requiring care for proper causal interpretation. Besides these primary VE estimands that summarize an effect of the vaccine over the full course of the study, we also consider local VE estimands that measure the effect at particular time points. We discuss interpretational difficulties of local VE estimands (e.g., depletion of susceptibles bias), and using frailty models as sensitivity analyses for the individual-level causal effects over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18587v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael P. Fay, Dean Follmann, Bruce J. Swihart, Lauren E. Dang</dc:creator>
    </item>
    <item>
      <title>Goodness-of-Fit Checks for Joint Models</title>
      <link>https://arxiv.org/abs/2601.18598</link>
      <description>arXiv:2601.18598v1 Announce Type: cross 
Abstract: Joint models for longitudinal and time-to-event data are widely used in many disciplines. Nonetheless, existing model comparison criteria do not indicate whether a model adequately fits the data or which components may be misspecified. We introduce a Bayesian posterior predictive checks framework for assessing a joint model's fit to the longitudinal and survival processes and their association. The framework supports multiple settings, including existing subjects, new subjects with only covariates, dynamic prediction at intermediate follow-up times, and cross-validated assessment. For the longitudinal component, goodness-of-fit is assessed through the mean, variance, and correlation structure, while the survival component is evaluated using empirical cumulative distributions and probability integral transforms. The association between processes is examined using time-dependent concordance statistics. We apply these checks to the Bio-SHiFT heart failure study, and a simulation study demonstrates that they can identify model misspecification that standard information criteria fail to detect. The proposed methodology is implemented in the freely available R package JMbayes2.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18598v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dimitris Rizopoulos, Jeremy M. G. Taylor, Isabella Kardys</dc:creator>
    </item>
    <item>
      <title>The Blown Lead Paradox: Conditional Laws for the Running Maximum of Binary Doob Martingales</title>
      <link>https://arxiv.org/abs/2601.18774</link>
      <description>arXiv:2601.18774v1 Announce Type: cross 
Abstract: Live win-probability forecasts are now ubiquitous in sports broadcasts, and retrospective commentary often cites the largest win probability attained by a team that ultimately loses as evidence of a "collapse." Interpreting such extrema requires a reference distribution under correct specification. Modeling the forecast sequence as the Doob martingale of conditional win probabilities for a binary terminal outcome, we derive sharp distributional laws for its path maximum, including the conditional law given an eventual loss. In discrete time, we quantify explicit correction terms (last-step crossings and overshoots); under continuous-path regularity these corrections disappear, yielding exact identities. We further obtain closed-form distributions for two extensions: the maximal win probability attained by the eventual loser in a two-player game and the minimal win probability attained by the eventual winner in an n-player game. The resulting formulas furnish practical benchmarks for diagnosing sequential forecast calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.18774v1</guid>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Pipping-Gam\'on, Abraham J. Wyner</dc:creator>
    </item>
    <item>
      <title>Sequential Federated Analysis of Early Outbreak Data Applied to Incubation Period Estimation</title>
      <link>https://arxiv.org/abs/2404.14895</link>
      <description>arXiv:2404.14895v4 Announce Type: replace 
Abstract: Early outbreak data analysis is critical for informing about their potential impact and interventions. However, data obtained early in outbreaks are often sensitive and subject to strict privacy restrictions. Thus, federated analysis, which implies decentralised collaborative analysis where no raw data sharing is required, emerged as an attractive paradigm to solve issues around data privacy and confidentiality. In the present study, we propose two approaches which require neither data sharing nor direct communication between devices/servers. The first approach approximates the joint posterior distributions via a multivariate normal distribution and uses this information to update prior distributions sequentially. The second approach uses summaries from parameters' posteriors obtained locally at different locations (sites) to perform a meta-analysis via a hierarchical model. We test these models on simulated and on real outbreak data to estimate the incubation period of multiple infectious diseases. Results indicate that both approaches can recover incubation period parameters accurately, but they present different inferential advantages. While the approximation approach permits to work with full posterior distributions, thus providing a better quantification of uncertainty; the meta-analysis approach allows for an explicit hierarchical structure, which can make some parameters more interpretable. We provide a framework for federated analysis of early outbreak data where the public health contexts are complex.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14895v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.epidem.2026.100890</arxiv:DOI>
      <dc:creator>Simon Busch-Moreno, Moritz U. G. Kraemer</dc:creator>
    </item>
    <item>
      <title>Unsupervised cell segmentation by fast Gaussian Processes</title>
      <link>https://arxiv.org/abs/2505.18902</link>
      <description>arXiv:2505.18902v3 Announce Type: replace 
Abstract: Cell boundary information is crucial for analyzing cell behaviors from time-lapse microscopy videos. Existing supervised cell segmentation tools, such as ImageJ, require tuning various parameters and rely on restrictive assumptions about the shape of the objects. While recent supervised segmentation tools based on convolutional neural networks enhance accuracy, they depend on high-quality labeled images, making them unsuitable for segmenting new types of objects not in the database. We developed a novel unsupervised cell segmentation algorithm based on fast Gaussian processes for noisy microscopy images without the need for parameter tuning or restrictive assumptions about the shape of the object. We derived robust thresholding criteria adaptive for heterogeneous images containing distinct brightness at different parts to separate objects from the background, and employed watershed segmentation to distinguish touching cell objects. Both simulated studies and real-data analysis of large microscopy images demonstrate the scalability and accuracy of our approach compared with the alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18902v3</guid>
      <category>stat.AP</category>
      <category>cs.CV</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Baracaldo, Blythe King, Haoran Yan, Yizi Lin, Nina Miolane, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>On the relationship between the Wasserstein distance and differences in life expectancy at birth</title>
      <link>https://arxiv.org/abs/2508.17235</link>
      <description>arXiv:2508.17235v4 Announce Type: replace 
Abstract: The Wasserstein distance is a metric for assessing distributional differences. The measure originates in optimal transport theory and can be interpreted as the minimal cost of transforming one distribution into another. In this paper, the Wasserstein distance is applied to life table age-at-death distributions. The main finding is that, under certain conditions, the Wasserstein distance between two age-at-death distributions equals the corresponding gap in life expectancy at birth ($e_0$). More specifically, the paper shows mathematically and empirically that this equivalence holds whenever the survivorship functions do not cross. For example, this applies when comparing mortality between women and men from 1990 to 2020 using data from the Human Mortality Database. In such cases, the gap in $e_0$ reflects not only a difference in mean ages at death but can also be interpreted directly as a measure of distributional difference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17235v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Sauerberg</dc:creator>
    </item>
    <item>
      <title>Beyond Expected Goals: A Probabilistic Framework for Shot Occurrences in Soccer</title>
      <link>https://arxiv.org/abs/2512.00203</link>
      <description>arXiv:2512.00203v2 Announce Type: replace 
Abstract: Expected goals (xG) models estimate the probability that a shot results in a goal from its context (e.g., location, pressure), but they operate only on observed shots. We propose xG+, a possession-level framework that first estimates the probability that a shot occurs within the next second and its corresponding xG if it were to occur. We also introduce ways to aggregate this joint probability estimate over the course of a possession. By jointly modeling shot-taking behavior and shot quality, xG+ remedies the conditioning-on-shots limitation of standard xG. We show that this improves predictive accuracy at the team level and produces a more persistent player skill signal than standard xG models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00203v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>eess.IV</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Pipping-Gam\'on, Tianshu Feng, R. Paul Sabin</dc:creator>
    </item>
    <item>
      <title>Kicking for Goal or Touch? An Expected Points Framework for Penalty Decisions in Rugby Union</title>
      <link>https://arxiv.org/abs/2512.00312</link>
      <description>arXiv:2512.00312v2 Announce Type: replace 
Abstract: Following a penalty in rugby union, teams typically choose between attempting a shot at goal or kicking to touch to pursue a try. We develop an Expected Points (EP) framework that quantifies the value of each option as a function of both field location and game context. Using phase-level data from the 2018/19 Premiership Rugby season (35,199 phases across 132 matches) and an angle-distance model of penalty kick success estimated from international records, we construct two surfaces: (i) the expected points of a possession beginning with a lineout, and (ii) the expected points of a kick at goal, taking into account the in-game consequences of made and missed kicks. We then compare these surfaces to produce decision maps that indicate where kicking for goal or kicking to touch maximizes expected return, and we analyze how the boundary shifts with game context and the expected meters gained to touch. Our results provide a unified, data-driven method for evaluating penalty decisions and can be tailored to team-specific kickers and lineout units. This study offers, to our knowledge, the first comprehensive EP-based assessment of penalty strategy in rugby union and outlines extensions to win-probability analysis and richer tracking data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.00312v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenny Watts, Jonathan Pipping-Gam\'on</dc:creator>
    </item>
    <item>
      <title>On the Anchoring Effect of Monetary Policy on the Labor Share of Income and the Rationality of Its Setting Mechanism</title>
      <link>https://arxiv.org/abs/2601.13675</link>
      <description>arXiv:2601.13675v2 Announce Type: replace 
Abstract: Modern macroeconomic monetary theory suggests that the labor share of income has effectively become a core macroe-conomic parameter anchored by top policymakers through Open Market Operations (OMO). However, the setting of this parameter remains a subject of intense economic debate. This paper provides a detailed summary of these controversies, analyzes the scope of influence exerted by market agents other than the top policymakers on the labor share, and explores the rationality of its setting mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.13675v2</guid>
      <category>stat.AP</category>
      <category>econ.EM</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Li Tuobang</dc:creator>
    </item>
    <item>
      <title>A Multimodal Feature Distillation with Mamba-Transformer Network for Brain Tumor Segmentation with Incomplete Modalities</title>
      <link>https://arxiv.org/abs/2404.14019</link>
      <description>arXiv:2404.14019v2 Announce Type: replace-cross 
Abstract: Existing brain tumor segmentation methods usually utilize multiple Magnetic Resonance Imaging (MRI) modalities in brain tumor images for segmentation, which can achieve better segmentation performance. However, in clinical applications, some modalities are often missing due to resource constraints, resulting in significant performance degradation for methods that rely on complete modality segmentation. In this paper, we propose a Multimodal feature distillation with Mamba-Transformer hybrid network (MMTSeg) for accurate brain tumor segmentation with missing modalities. We first employ a Multimodal Feature Distillation (MFD) module to distill feature-level multimodal knowledge into different unimodalities to extract complete modality information. We further develop an Unimodal Feature Enhancement (UFE) module to model the semantic relationship between global and local information. Finally, we built a Cross-Modal Fusion (CMF) module to explicitly align the global correlations across modalities, even when some modalities are missing. Complementary features within and across modalities are refined by the Mamba-Transformer hybrid architectures in both the UFE and CMF modules, dynamically capturing long-range dependencies and global semantic information for complex spatial contexts. A boundary-wise loss function is employed as the segmentation loss of the proposed MMTSeg to minimize boundary discrepancies for a distance-based metric. Our ablation study demonstrates the importance of the proposed feature enhancement and fusion modules in the proposed network and the Transformer with Mamba block for improving the performance of brain tumor segmentation with missing modalities. Extensive experiments on the BraTS 2018 and BraTS 2020 datasets demonstrate that the proposed MMTSeg framework outperforms state-of-the-art methods when modalities are missing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14019v2</guid>
      <category>cs.CV</category>
      <category>eess.SP</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ming Kang, Fung Fung Ting, Shier Nee Saw, Rapha\"el C. -W. Phan, Zongyuan Ge, Chee-Ming Ting</dc:creator>
    </item>
    <item>
      <title>Causal Inference Using Augmented Epidemic Models</title>
      <link>https://arxiv.org/abs/2410.11743</link>
      <description>arXiv:2410.11743v3 Announce Type: replace-cross 
Abstract: Epidemic models describe the evolution of a communicable disease over time. These models are often modified to include the effects of interventions (control measures) such as vaccination, social distancing, school closings etc. Many such models were proposed during the COVID-19 epidemic. Inevitably these models are used to answer the question: What is the effect of the intervention on the epidemic? These models can either be interpreted as data generating models describing observed random variables or as causal models for counterfactual random variables. These two interpretations are often conflated in the literature. We discuss the difference between these two types of models, and then we discuss how to estimate the parameters of the model. Our focus is causal inference for parameters in epidemic models by adjusting for confounders, allowing time varying interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11743v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Heejong Bong, Val\'erie Ventura, Larry Wasserman</dc:creator>
    </item>
    <item>
      <title>Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework</title>
      <link>https://arxiv.org/abs/2502.14479</link>
      <description>arXiv:2502.14479v4 Announce Type: replace-cross 
Abstract: The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14479v4</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Roland Breedt</dc:creator>
    </item>
    <item>
      <title>Set-valued data analysis for interlaboratory comparisons</title>
      <link>https://arxiv.org/abs/2510.23170</link>
      <description>arXiv:2510.23170v2 Announce Type: replace-cross 
Abstract: This article introduces tools to analyze set-valued data statistically. The tools were initially developed to analyze results from an interlaboratory comparison made by the Electromagnetic Compatibility Working Group of Eurolab France, where the goal was to select a consensual set of injection points on an electrical device. Families based on the Hamming-distance from a consensus set are introduced and Fisher's noncentral hypergeometric distribution is proposed to model the number of deviations. A Bayesian approach is used and two types of techniques are proposed for the inference. Hierarchical models are also considered to quantify a possible within-laboratory effect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.23170v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S\'ebastien Petit (LNE), S\'ebastien Marmin (LNE), Nicolas Fischer (LNE)</dc:creator>
    </item>
    <item>
      <title>The use of spectral indices in environmental monitoring of smouldering coal-waste dumps</title>
      <link>https://arxiv.org/abs/2601.11603</link>
      <description>arXiv:2601.11603v3 Announce Type: replace-cross 
Abstract: The study aimed to evaluate the applicability of environmental indices in the monitoring of smouldering coal-waste dumps. A dump located in the Upper Silesian Coal Basin served as the research site for a multi-method analysis combining remote sensing and field-based data. Two UAV survey campaigns were conducted, capturing RGB, infrared, and multispectral imagery. These were supplemented with direct ground measurements of subsurface temperature and detailed vegetation mapping. Additionally, publicly available satellite data from the Landsat and Sentinel missions were analysed. A range of vegetation and fire-related indices (NDVI, SAVI, EVI, BAI, among others) were calculated to identify thermally active zones and assess vegetation conditions within these degraded areas. The results revealed strong seasonal variability in vegetation indices on thermally active sites, with evidence of disrupted vegetation cycles, including winter greening in moderately heated root zones - a pattern indicative of stress and degradation processes. While satellite data proved useful in reconstructing the fire history of the dump, their spatial resolution was insufficient for detailed monitoring of small-scale thermal anomalies. The study highlights the diagnostic potential of UAV-based remote sensing in post-industrial environments undergoing land degradation but emphasises the importance of field validation for accurate environmental assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.11603v3</guid>
      <category>physics.geo-ph</category>
      <category>physics.ins-det</category>
      <category>stat.AP</category>
      <pubDate>Tue, 27 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.rsase.2025.101865</arxiv:DOI>
      <arxiv:journal_reference>Remote Sens. Appl.: Soc. Environ., 41 (2026)</arxiv:journal_reference>
      <dc:creator>Anna Abramowicz, Michal Laska, Adam Nadudvari, Oimahmad Rahmonov</dc:creator>
    </item>
  </channel>
</rss>
