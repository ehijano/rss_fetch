<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Nov 2024 02:48:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tree Species Classification using Machine Learning and 3D Tomographic SAR -- a case study in Northern Europe</title>
      <link>https://arxiv.org/abs/2411.12897</link>
      <description>arXiv:2411.12897v1 Announce Type: cross 
Abstract: Tree species classification plays an important role in nature conservation, forest inventories, forest management, and the protection of endangered species. Over the past four decades, remote sensing technologies have been extensively utilized for tree species classification, with Synthetic Aperture Radar (SAR) emerging as a key technique. In this study, we employed TomoSense, a 3D tomographic dataset, which utilizes a stack of single-look complex (SLC) images, a byproduct of SAR, captured at different incidence angles to generate a three-dimensional representation of the terrain. Our research focuses on evaluating multiple tabular machine-learning models using the height information derived from the tomographic image intensities to classify eight distinct tree species. The SLC data and tomographic imagery were analyzed across different polarimetric configurations and geosplit configurations. We investigated the impact of these variations on classification accuracy, comparing the performance of various tabular machine-learning models and optimizing them using Bayesian optimization. Additionally, we incorporated a proxy for actual tree height using point cloud data from Light Detection and Ranging (LiDAR) to provide height statistics associated with the model's predictions. This comparison offers insights into the reliability of tomographic data in predicting tree species classification based on height.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12897v1</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:journal_reference>Neurips 2024 - Climate Change Workshop</arxiv:journal_reference>
      <dc:creator>Colverd Grace, Schade Laura, Takami Jumpei, Bot Karol, Gallego Joseph</dc:creator>
    </item>
    <item>
      <title>Probability distributions and calculations for Hake's ratio statistics in measuring effect size</title>
      <link>https://arxiv.org/abs/2411.12938</link>
      <description>arXiv:2411.12938v1 Announce Type: cross 
Abstract: Ratio statistics and distributions play a crucial role in various fields, including linear regression, metrology, nuclear physics, operations research, econometrics, biostatistics, genetics, and engineering. In this work, we examine the statistical properties and probability calculations of the Hake normalized gain as a measure of effect size and educational effectiveness in physics education. Leveraging existing knowledge about the Hake ratio as a ratio of normal variables and utilizing open data science tools, we developed two novel computational approaches for computing ratio distributions. Our pilot numerical study demonstrates the speed, accuracy, and reliability of calculating ratio distributions through (1) DE quadrature with/without barycentric interpolation, a very quick and efficient quadrature method, and (2) a 2D vectorized numerical inversion of characteristic functions, which offers broader applicability by not requiring knowledge of PDFs or the independence of ratio constituents. These numerical explorations not only deepen the understanding of the Hake ratio's distribution but also showcase the efficiency, precision, and versatility of our proposed methods, making them highly suitable for fast data analysis based on exact probability ratio distributions. This capability has potential applications in multidimensional statistics and uncertainty analysis in metrology, where precise and reliable data handling is essential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12938v1</guid>
      <category>stat.CO</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jozef Han\v{c}, Martina Han\v{c}ov\'a, Dominik Borovsk\'y</dc:creator>
    </item>
    <item>
      <title>Extraction of gravitational wave signals in realistic LISA data</title>
      <link>https://arxiv.org/abs/2411.13402</link>
      <description>arXiv:2411.13402v1 Announce Type: cross 
Abstract: The Laser Interferometer Space Antenna (LISA) mission is being developed by ESA with NASA participation. As it has recently passed the Mission Adoption milestone, models of the instruments and noise performance are becoming more detailed, and likewise prototype data analyses must as well. Assumptions such as Gaussianity, Stationarity, and continuous data continuity are unrealistic, and must be replaced with physically motivated data simulations, and data analysis methods adapted to accommodate such likely imperfections. To this end, the LISA Data Challenges have produced datasets featuring time-varying and unequal constellation armlength, and measurement artifacts including data interruptions and instrumental transients. In this work, we assess the impact of these data artifacts on the inference of Galactic Binary and Massive Black Hole properties. Our analysis shows that the treatment of noise transients and gaps is necessary for effective parameter estimation. We find that straightforward mitigation techniques can significantly suppress artifacts, albeit leaving a non-negligible impact on aspects of the science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13402v1</guid>
      <category>gr-qc</category>
      <category>astro-ph.IM</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eleonora Castelli, Quentin Baghi, John G. Baker, Jacob Slutsky, J\'er\^ome Bobin, Nikolaos Karnesis, Antoine Petiteau, Orion Sauter, Peter Wass, William J. Weber</dc:creator>
    </item>
    <item>
      <title>Decomposing force fields as flows on graphs reconstructed from stochastic trajectories</title>
      <link>https://arxiv.org/abs/2409.07479</link>
      <description>arXiv:2409.07479v3 Announce Type: replace-cross 
Abstract: Disentangling irreversible and reversible forces from random fluctuations is a challenging problem in the analysis of stochastic trajectories measured from real-world dynamical systems. We present an approach to approximate the dynamics of a stationary Langevin process as a discrete-state Markov process evolving over a graph-representation of phase-space, reconstructed from stochastic trajectories. Next, we utilise the analogy of the Helmholtz-Hodge decomposition of an edge-flow on a contractible simplicial complex with the associated decomposition of a stochastic process into its irreversible and reversible parts. This allows us to decompose our reconstructed flow and to differentiate between the irreversible currents and reversible gradient flows underlying the stochastic trajectories. We validate our approach on a range of solvable and nonlinear systems and apply it to derive insight into the dynamics of flickering red-blood cells and healthy and arrhythmic heartbeats. In particular, we capture the difference in irreversible circulating currents between healthy and passive cells and healthy and arrhythmic heartbeats. Our method breaks new ground at the interface of data-driven approaches to stochastic dynamics and graph signal processing, with the potential for further applications in the analysis of biological experiments and physiological recordings. Finally, it prompts future analysis of the convergence of the Helmholtz-Hodge decomposition in discrete and continuous spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07479v3</guid>
      <category>cond-mat.stat-mech</category>
      <category>math.DS</category>
      <category>physics.data-an</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ram\'on Nartallo-Kaluarachchi, Paul Expert, David Beers, Alexander Strang, Morten L. Kringelbach, Renaud Lambiotte, Alain Goriely</dc:creator>
    </item>
    <item>
      <title>Decomposition Pipeline for Large-Scale Portfolio Optimization with Applications to Near-Term Quantum Computing</title>
      <link>https://arxiv.org/abs/2409.10301</link>
      <description>arXiv:2409.10301v2 Announce Type: replace-cross 
Abstract: Industrially relevant constrained optimization problems, such as portfolio optimization and portfolio rebalancing, are often intractable or difficult to solve exactly. In this work, we propose and benchmark a decomposition pipeline targeting portfolio optimization and rebalancing problems with constraints. The pipeline decomposes the optimization problem into constrained subproblems, which are then solved separately and aggregated to give a final result. Our pipeline includes three main components: preprocessing of correlation matrices based on random matrix theory, modified spectral clustering based on Newman's algorithm, and risk rebalancing. Our empirical results show that our pipeline consistently decomposes real-world portfolio optimization problems into subproblems with a size reduction of approximately 80%. Since subproblems are then solved independently, our pipeline drastically reduces the total computation time for state-of-the-art solvers. Moreover, by decomposing large problems into several smaller subproblems, the pipeline enables the use of near-term quantum devices as solvers, providing a path toward practical utility of quantum computers in portfolio optimization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10301v2</guid>
      <category>math.OC</category>
      <category>physics.data-an</category>
      <category>q-fin.PM</category>
      <category>q-fin.RM</category>
      <category>quant-ph</category>
      <pubDate>Thu, 21 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atithi Acharya, Romina Yalovetzky, Pierre Minssen, Shouvanik Chakrabarti, Ruslan Shaydulin, Rudy Raymond, Yue Sun, Dylan Herman, Ruben S. Andrist, Grant Salton, Martin J. A. Schuetz, Helmut G. Katzgraber, Marco Pistoia</dc:creator>
    </item>
  </channel>
</rss>
