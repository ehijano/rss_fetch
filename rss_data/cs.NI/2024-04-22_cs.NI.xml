<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 23 Apr 2024 04:00:00 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Resource Slicing with Cross-Cell Coordination in Satellite-Terrestrial Integrated Networks</title>
      <link>https://arxiv.org/abs/2404.13158</link>
      <description>arXiv:2404.13158v1 Announce Type: new 
Abstract: Satellite-terrestrial integrated networks (STIN) are envisioned as a promising architecture for ubiquitous network connections to support diversified services. In this paper, we propose a novel resource slicing scheme with cross-cell coordination in STIN to satisfy distinct service delay requirements and efficient resource usage. To address the challenges posed by spatiotemporal dynamics in service demands and satellite mobility, we formulate the resource slicing problem into a long-term optimization problem and propose a distributed resource slicing (DRS) scheme for scalable and flexible resource management across different cells. Specifically, a hybrid data-model co-driven approach is developed, including an asynchronous multi-agent reinforcement learning-based algorithm to determine the optimal satellite set serving each cell and a distributed optimization-based algorithm to make the resource reservation decisions for each slice. Simulation results demonstrate that the proposed scheme outperforms benchmark methods in terms of resource usage and delay performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13158v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingcheng He (Sherman), Huaqing Wu (Sherman), Conghao Zhou (Sherman),  Xuemin (Sherman),  Shen</dc:creator>
    </item>
    <item>
      <title>Network-Level Analysis of Integrated Sensing and Communication Using Stochastic Geometry</title>
      <link>https://arxiv.org/abs/2404.13197</link>
      <description>arXiv:2404.13197v1 Announce Type: new 
Abstract: To meet the demands of densely deploying communication and sensing devices in the next generation of wireless networks, integrated sensing and communication (ISAC) technology is employed to alleviate spectrum scarcity, while stochastic geometry (SG) serves as a tool for low-complexity performance evaluation. To assess network-level performance, there is a natural interaction between ISAC technology and the SG method. From ISAC network perspective, we illustrate how to leverage SG analytical framework to evaluate ISAC network performance by introducing point process distributions and stochastic fading channel models. From SG framework perspective, we summarize the unique performance metrics and research objectives of ISAC networks, thereby extending the scope of SG research in the field of wireless communications. Additionally, considering the limited discussion in the existing SG-based ISAC works in terms of distribution and channel modeling, a case study is designed to exploit topology and channel fading awareness to provide relevant network insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13197v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ruibo Wang, Baha Eddine Youcef Belmekki, Xue Zhang, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>5G-WAVE: A Core Network Framework with Decentralized Authorization for Network Slices</title>
      <link>https://arxiv.org/abs/2404.13242</link>
      <description>arXiv:2404.13242v1 Announce Type: new 
Abstract: 5G mobile networks leverage Network Function Virtualization (NFV) to offer services in the form of network slices. Each network slice is a logically isolated fragment constructed by service chaining a set of Virtual Network Functions (VNFs). The Network Repository Function (NRF) acts as a central OpenAuthorization (OAuth) 2.0 server to secure inter-VNF communications resulting in a single point of failure. Thus, we propose 5G-WAVE, a decentralized authorization framework for the 5G core by leveraging the WAVE framework and integrating it into the OpenAirInterface (OAI) 5G core. Our design relies on Side-Car Proxies (SCPs) deployed alongside individual VNFs, allowing point-to-point authorization. Each SCP acts as a WAVE engine to create entities and attestations and verify incoming service requests. We measure the authorization latency overhead for VNF registration, 5G Authentication and Key Agreement (AKA), and data session setup and observe that WAVE verification introduces 155ms overhead to HTTP transactions for decentralizing authorization. Additionally, we evaluate the scalability of 5G-WAVE by instantiating more network slices to observe 1.4x increase in latency with 10x growth in network size. We also discuss how 5G-WAVE can significantly reduce the 5G attack surface without using OAuth 2.0 while addressing several key issues of 5G standardization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13242v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pragya Sharma, Tolga Atalay, Hans-Andrew Gibbs, Dragoslav Stojadinovic, Angelos Stavrou, Haining Wang</dc:creator>
    </item>
    <item>
      <title>ABACUS: An Impairment Aware Joint Optimal Dynamic RMLSA in Elastic Optical Networks</title>
      <link>https://arxiv.org/abs/2404.13308</link>
      <description>arXiv:2404.13308v1 Announce Type: new 
Abstract: The challenge of optimal Routing and Spectrum Assignment (RSA) is significant in Elastic Optical Networks. Integrating adaptive modulation formats into the RSA problem - Routing, Modulation Level, and Spectrum Assignment - broadens allocation options and increases complexity. The conventional RSA approach entails predetermining fixed paths and then allocating spectrum within them separately. However, expanding the path set for optimality may not be advisable due to the substantial increase in paths with network size expansion. This paper delves into a novel approach called RMLSA, which proposes a comprehensive solution addressing both route determination and spectrum assignment simultaneously. An objective function named ABACUS, Adaptive Balance of Average Clustering and Utilization of Spectrum, is chosen for its capability to adjust and assign significance to average clustering and spectrum utilization. Our approach involves formulating an Integer Linear Programming model with a straightforward relationship between path and spectrum constraints. The model also integrates Physical Layer Impairments to ensure end-to-end Quality of Transmission for requested connections while maintaining existing ones. We demonstrate that ILP can offer an optimal solution for a dynamic traffic scenario within a reasonable time complexity. To achieve this goal, we adopt a structured formulation approach where essential information is determined beforehand, thus minimizing the need for online computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13308v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M Jyothi Kiran, Venkatesh Chebolu, Goutam Das, Raja Datta</dc:creator>
    </item>
    <item>
      <title>Socialized Learning: A Survey of the Paradigm Shift for Edge Intelligence in Networked Systems</title>
      <link>https://arxiv.org/abs/2404.13348</link>
      <description>arXiv:2404.13348v1 Announce Type: new 
Abstract: Amidst the robust impetus from artificial intelligence (AI) and big data, edge intelligence (EI) has emerged as a nascent computing paradigm, synthesizing AI with edge computing (EC) to become an exemplary solution for unleashing the full potential of AI services. Nonetheless, challenges in communication costs, resource allocation, privacy, and security continue to constrain its proficiency in supporting services with diverse requirements. In response to these issues, this paper introduces socialized learning (SL) as a promising solution, further propelling the advancement of EI. SL is a learning paradigm predicated on social principles and behaviors, aimed at amplifying the collaborative capacity and collective intelligence of agents within the EI system. SL not only enhances the system's adaptability but also optimizes communication, and networking processes, essential for distributed intelligence across diverse devices and platforms. Therefore, a combination of SL and EI may greatly facilitate the development of collaborative intelligence in the future network. This paper presents the findings of a literature review on the integration of EI and SL, summarizing the latest achievements in existing research on EI and SL. Subsequently, we delve comprehensively into the limitations of EI and how it could benefit from SL. Special emphasis is placed on the communication challenges and networking strategies and other aspects within these systems, underlining the role of optimized network solutions in improving system efficacy. Based on these discussions, we elaborate in detail on three integrated components: socialized architecture, socialized training, and socialized inference, analyzing their strengths and weaknesses. Finally, we identify some possible future applications of combining SL and EI, discuss open problems and suggest some future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13348v1</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofei Wang, Yunfeng Zhao, Chao Qiu, Qinghua Hu, Victor C. M. Leung</dc:creator>
    </item>
    <item>
      <title>Urgent Edge Computing</title>
      <link>https://arxiv.org/abs/2404.13411</link>
      <description>arXiv:2404.13411v1 Announce Type: new 
Abstract: This position paper introduces Urgent Edge Computing (UEC) as a paradigm shift addressing the evolving demands of time-sensitive applications in distributed edge environments, in time-critical scenarios. With a focus on ultra-low latency, availability, resource management, decentralization, self-organization, and robust security, UEC aims to facilitate operations in critical scenarios such as disaster response, environmental monitoring, and smart city management. This paper outlines and discusses the key requirements, challenges, and enablers along with a conceptual architecture. The paper also outlines the potential applications of Urgent Edge Computing</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13411v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3659994.3660315</arxiv:DOI>
      <dc:creator>Patrizio Dazzi, Luca Ferrucci, Marco Danelutto, Konstantinos Tserpes, Antonis Makris, Theodoros Theodoropoulos, Jacopo Massa, Emanuele Carlini, Matteo Mordacchini</dc:creator>
    </item>
    <item>
      <title>Improving Web Content Delivery with HTTP/3 and Non-Incremental EPS</title>
      <link>https://arxiv.org/abs/2404.13460</link>
      <description>arXiv:2404.13460v1 Announce Type: new 
Abstract: HTTP/3 marks a significant advancement in protocol development, utilizing QUIC as its underlying transport layer to exploit multiplexing capabilities and minimize head-of-line blocking. The introduction of the Extensible Prioritization Scheme (EPS) offers a signaling mechanism for controlling the order of resource delivery. In this study, we propose mappings from Chromium priority hints to EPS urgency levels with the goal of enhancing the key web performance metrics. The mappings are evaluated using EPS's urgency-based, non-incremental resource delivery method. The results of the experimental evaluation show that the proposed mappings improve the Quality of Experience metrics across a range of websites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13460v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abhinav Gupta, Radim Bartos</dc:creator>
    </item>
    <item>
      <title>A Grassroots Architecture to Supplant Global Digital Platforms by a Global Digital Democracy</title>
      <link>https://arxiv.org/abs/2404.13468</link>
      <description>arXiv:2404.13468v1 Announce Type: new 
Abstract: We present an architectural alternative to global digital platforms termed grassroots, designed to serve the social, economic, civic, and political needs of local digital communities, as well as their federation. Grassroots platforms may offer local communities an alternative to global digital platforms while operating solely on the smartphones of their members, forsaking any global resources other than the network itself. Such communities may form digital economies without initial capital or external credit, exercise sovereign democratic governance, and federate, ultimately resulting in the grassroots formation of a global digital democracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13468v1</guid>
      <category>cs.NI</category>
      <category>cs.CY</category>
      <category>cs.DC</category>
      <category>cs.MA</category>
      <category>cs.SI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ehud Shapiro</dc:creator>
    </item>
    <item>
      <title>An FPTAS for Shortest-Longest Path Problem</title>
      <link>https://arxiv.org/abs/2404.13488</link>
      <description>arXiv:2404.13488v1 Announce Type: new 
Abstract: Motivated by multi-domain Service Function Chain (SFC) orchestration, we define the Shortest-Longest Path (SLP) problem, prove its hardness, and design an efficient Fully Polynomial Time Approximation Scheme (FPTAS) using the scaling and rounding technique to compute an approximation solution with provable performance guarantee. The SLP problem and its solution algorithm have theoretical significance in multicriteria optimization and also have application potential in QoS routing and multi-domain network resource allocation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13488v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianwei Zhang</dc:creator>
    </item>
    <item>
      <title>An Integrated Communication and Computing Scheme for Wi-Fi Networks based on Generative AI and Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2404.13598</link>
      <description>arXiv:2404.13598v1 Announce Type: new 
Abstract: The continuous evolution of future mobile communication systems is heading towards the integration of communication and computing, with Mobile Edge Computing (MEC) emerging as a crucial means of implementing Artificial Intelligence (AI) computation. MEC could enhance the computational performance of wireless edge networks by offloading computing-intensive tasks to MEC servers. However, in edge computing scenarios, the sparse sample problem may lead to high costs of time-consuming model training. This paper proposes an MEC offloading decision and resource allocation solution that combines generative AI and deep reinforcement learning (DRL) for the communication-computing integration scenario in the 802.11ax Wi-Fi network. Initially, the optimal offloading policy is determined by the joint use of the Generative Diffusion Model (GDM) and the Twin Delayed DDPG (TD3) algorithm. Subsequently, resource allocation is accomplished by using the Hungarian algorithm. Simulation results demonstrate that the introduction of Generative AI significantly reduces model training costs, and the proposed solution exhibits significant reductions in system task processing latency and total energy consumption costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13598v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyang Du, Xuming Fang</dc:creator>
    </item>
    <item>
      <title>Performance Analysis for Deterministic System using Time Sensitive Network</title>
      <link>https://arxiv.org/abs/2404.13639</link>
      <description>arXiv:2404.13639v1 Announce Type: new 
Abstract: Modern technology necessitates the use of dependable, fast, and inexpensive networks as the backbone for data transmission. Switched Ethernet coupled with the Time Sensitive Networking</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13639v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Mehedi Hasan, He Feng</dc:creator>
    </item>
    <item>
      <title>MSTG: A Flexible and Scalable Microservices Infrastructure Generator</title>
      <link>https://arxiv.org/abs/2404.13665</link>
      <description>arXiv:2404.13665v1 Announce Type: new 
Abstract: The last few years in the software engineering field has seen a paradigm shift from monolithic application towards architectures in which the application is split in various smaller entities (i.e., microservices) fueled by the improved availability and ease of use of containers technologies such as Docker and Kubernetes. Those microservices communicate between each other using networking technologies in place of function calls in traditional monolithic software. In order to be able to evaluate the potential, the modularity, and the scalability of this new approach, many tools, such as microservices benchmarking, have been developed with that objective in mind. Unfortunately, many of these tend to focus only on the application layer while not taking the underlying networking infrastructure into consideration.
  In this paper, we introduce and evaluate the performance of a new modular and scalable tool, MicroServices Topology Generator (MSTG), that allows to simulate both the application and networking layers of a microservices architecture. Based on a topology described in YAML format, MSTG generates the configuration file(s) for deploying the architecture on either Docker Composer or Kubernetes. Furthermore, MSTG encompasses telemetry tools, such as Application Performance Monitoring (APM) relying on OpenTelemetry. This paper fully describes MSTG, evaluates its performance, and demonstrates its potential through several use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13665v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emilien Wansart, Maxime Goffart, Justin Iurman, Benoit Donnet</dc:creator>
    </item>
    <item>
      <title>Efficient Digital Twin Data Processing for Low-Latency Multicast Short Video Streaming</title>
      <link>https://arxiv.org/abs/2404.13749</link>
      <description>arXiv:2404.13749v1 Announce Type: new 
Abstract: In this paper, we propose a novel efficient digital twin (DT) data processing scheme to reduce service latency for multicast short video streaming. Particularly, DT is constructed to emulate and analyze user status for multicast group update and swipe feature abstraction. Then, a precise measurement model of DT data processing is developed to characterize the relationship among DT model size, user dynamics, and user clustering accuracy. A service latency model, consisting of DT data processing delay, video transcoding delay, and multicast transmission delay, is constructed by incorporating the impact of user clustering accuracy. Finally, a joint optimization problem of DT model size selection and bandwidth allocation is formulated to minimize the service latency. To efficiently solve this problem, a diffusion-based resource management algorithm is proposed, which utilizes the denoising technique to improve the action-generation process in the deep reinforcement learning algorithm. Simulation results based on the real-world dataset demonstrate that the proposed DT data processing scheme outperforms benchmark schemes in terms of service latency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13749v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Huang, Shisheng Hu, Mushu Li, Cheng Huang, Xuemin Shen</dc:creator>
    </item>
    <item>
      <title>Cross-Modal Generative Semantic Communications for Mobile AIGC: Joint Semantic Encoding and Prompt Engineering</title>
      <link>https://arxiv.org/abs/2404.13898</link>
      <description>arXiv:2404.13898v1 Announce Type: new 
Abstract: Employing massive Mobile AI-Generated Content (AIGC) Service Providers (MASPs) with powerful models, high-quality AIGC services can become accessible for resource-constrained end users. However, this advancement, referred to as mobile AIGC, also introduces a significant challenge: users should download large AIGC outputs from the MASPs, leading to substantial bandwidth consumption and potential transmission failures. In this paper, we apply cross-modal Generative Semantic Communications (G-SemCom) in mobile AIGC to overcome wireless bandwidth constraints. Specifically, we utilize a series of cross-modal attention maps to indicate the correlation between user prompts and each part of AIGC outputs. In this way, the MASP can analyze the prompt context and filter the most semantically important content efficiently. Only semantic information is transmitted, with which users can recover the entire AIGC output with high quality while saving mobile bandwidth. Since the transmitted information not only preserves the semantics but also prompts the recovery, we formulate a joint semantic encoding and prompt engineering problem to optimize the bandwidth allocation among users. Particularly, we present a human-perceptual metric named Joint Perpetual Similarity and Quality (JPSQ), which is fused by two learning-based measurements regarding semantic similarity and aesthetic quality, respectively. Furthermore, we develop the Attention-aware Deep Diffusion (ADD) algorithm, which learns attention maps and leverages the diffusion process to enhance the environment exploration ability. Extensive experiments demonstrate that our proposal can reduce the bandwidth consumption of mobile users by 49.4% on average, with almost no perceptual difference in AIGC output quality. Moreover, the ADD algorithm shows superior performance over baseline DRL methods, with 1.74x higher overall reward.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13898v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinqiu Liu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Ping Zhang, Xuemin Shen</dc:creator>
    </item>
    <item>
      <title>5GC$^2$ache: Improving 5G UPF Performance via Cache Optimization</title>
      <link>https://arxiv.org/abs/2404.13991</link>
      <description>arXiv:2404.13991v1 Announce Type: new 
Abstract: Last Level Cache (LLC) is a precious and critical resource that impacts the performance of applications running on top of CPUs. In this paper, we reveal the significant impact of LLC on the performance of the 5G user plane function (UPF) when running a cloudified 5G core on general-purposed servers. With extensive measurements showing that the throughput can degrade by over 50\% when the precious LLC resource of UPF is not properly allocated, we identify three categories of performance degradation caused by incorrect LLC usage: DMA leakage problem, hot/cold mbuf problem and cache contention. To address these problems, we introduce the design and implementation of 5GC$^2$ache that monitors the LLC status as well as the throughput performance and dynamically adjusts key parameters of the LLC resource allocation. Our experiments show that 5GC$^2$ache enables a commercial 5G core to increase its throughput to 76.41Gbps, 39.41\% higher than the original performance and 29.55\% higher than the state-of-the-art.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13991v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haonan Jia, Meng Wang, Biyi Li, Yirui Liu, Junchen Guo, Pengyu Zhang</dc:creator>
    </item>
    <item>
      <title>Access-Point to Access-Point Connectivity for PON-based OWC Spine and Leaf Data Centre Architecture</title>
      <link>https://arxiv.org/abs/2404.14143</link>
      <description>arXiv:2404.14143v1 Announce Type: new 
Abstract: In this paper, we propose incorporating Optical Wireless Communication (OWC) and Passive Optical Network (PON) technologies into next generation spine-and-leaf Data Centre Networks (DCNs). In this work, OWC systems are used to connect the Data Centre (DC) racks through Wavelength Division Multiplexing (WDM) Infrared (IR) transceivers. The transceivers are placed on top of the racks and at distributed Access Points (APs) in the ceiling. Each transceiver on a rack is connected to a leaf switch that connects the servers within the rack. We replace the spine switches by Optical Line Terminal (OLT) and Network Interface Cards (NIC) in the APs to achieve the desired connectivity. We benchmark the power consumption of the proposed OWC-PON-based spine-and-leaf DC against traditional spine-and-leaf DC and report 46% reduction in the power consumption when considering eight racks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14143v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Abrar S. Alhazmi, Sanaa H. Mohamed, Ahmad Qidan, T. E. H. El-Gorashi, Jaafar M. H. Elmirghani</dc:creator>
    </item>
    <item>
      <title>Dismantling Common Internet Services for Ad-Malware Detection</title>
      <link>https://arxiv.org/abs/2404.14190</link>
      <description>arXiv:2404.14190v1 Announce Type: new 
Abstract: Online advertising represents a main instrument for publishers to fund content on the World Wide Web. Unfortunately, a significant number of online advertisements often accommodates potentially malicious content, such as cryptojacking hidden in web banners - even on reputable websites. In order to protect Internet users from such online threats, the thorough detection of ad-malware campaigns plays a crucial role for a safe Web. Today, common Internet services like VirusTotal can label suspicious content based on feedback from contributors and from the entire Web community. However, it is open to which extent ad-malware is actually taken into account and whether the results of these services are consistent. In this pre-study, we evaluate who defines ad-malware on the Internet. In a first step, we crawl a vast set of websites and fetch all HTTP requests (particularly to online advertisements) within these websites. Then we query these requests both against popular filtered DNS providers and VirusTotal. The idea is to validate, how much content is labeled as a potential threat. The results show that up to 0.47% of the domains found during crawling are labeled as suspicious by DNS providers and up to 8.8% by VirusTotal. Moreover, only about 0.7% to 3.2% of these domains are categorized as ad-malware. The overall responses from the used Internet services paint a divergent picture: All considered services have different understandings to the definition of suspicious content. Thus, we outline potential research efforts to the automated detection of ad-malware. We further bring up the open question of a common definition of ad-malware to the Web community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14190v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Nettersheim, Stephan Arlt, Michael Rademacher</dc:creator>
    </item>
    <item>
      <title>TrimCaching: Parameter-sharing Edge Caching for AI Model Downloading</title>
      <link>https://arxiv.org/abs/2404.14204</link>
      <description>arXiv:2404.14204v1 Announce Type: new 
Abstract: Next-generation mobile networks are expected to facilitate fast AI model downloading to end users. By caching models on edge servers, mobile networks can deliver models to end users with low latency, resulting in a paradigm called edge model caching. In this paper, we develop a novel model placement scheme, called parameter-sharing model caching (TrimCaching). TrimCaching exploits the key observation that a wide range of AI models, such as convolutional neural networks or large language models, can share a significant proportion of parameter blocks containing reusable knowledge, thereby improving storage efficiency. To this end, we formulate a parameter-sharing model placement problem to maximize the cache hit ratio in multi-edge wireless networks by balancing the fundamental tradeoff between storage efficiency and service latency. We show that the formulated problem is a submodular maximization problem with submodular constraints, for which no polynomial-time approximation algorithm exists. To overcome this challenge, we study an important special case, where a small fixed number of parameter blocks are shared across models, which often holds in practice. In such a case, a polynomial-time algorithm with $\left(1-\epsilon\right)/2$-approximation guarantee is developed. Subsequently, we address the original problem for the general case by developing a greedy algorithm. Simulation results demonstrate that the proposed TrimCaching framework significantly improves the cache hit ratio compared with state-of-the-art content caching without exploiting shared parameters in AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14204v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanqiao Qu, Zheng Lin, Qian Chen, Jian Li, Fangming Liu, Xianhao Chen, Kaibin Huang</dc:creator>
    </item>
    <item>
      <title>EcoPull: Sustainable IoT Image Retrieval Empowered by TinyML Models</title>
      <link>https://arxiv.org/abs/2404.14236</link>
      <description>arXiv:2404.14236v1 Announce Type: new 
Abstract: This paper introduces EcoPull, a sustainable Internet of Things (IoT) framework empowered by tiny machine learning (TinyML) models for fetching images from wireless visual sensor networks. Two types of learnable TinyML models are installed in the IoT devices: i) a behavior model and ii) an image compressor model. The first filters out irrelevant images for the current task, reducing unnecessary transmission and resource competition among the devices. The second allows IoT devices to communicate with the receiver via latent representations of images, reducing communication bandwidth usage. However, integrating learnable modules into IoT devices comes at the cost of increased energy consumption due to inference. The numerical results show that the proposed framework can save &gt; 70% energy compared to the baseline while maintaining the quality of the retrieved images at the ES.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14236v1</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Thorsager, Victor Croisfelt, Junya Shiraishi, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>Beyond the Edge: An Advanced Exploration of Reinforcement Learning for Mobile Edge Computing, its Applications, and Future Research Trajectories</title>
      <link>https://arxiv.org/abs/2404.14238</link>
      <description>arXiv:2404.14238v1 Announce Type: new 
Abstract: Mobile Edge Computing (MEC) broadens the scope of computation and storage beyond the central network, incorporating edge nodes close to end devices. This expansion facilitates the implementation of large-scale "connected things" within edge networks. The advent of applications necessitating real-time, high-quality service presents several challenges, such as low latency, high data rate, reliability, efficiency, and security, all of which demand resolution. The incorporation of reinforcement learning (RL) methodologies within MEC networks promotes a deeper understanding of mobile user behaviors and network dynamics, thereby optimizing resource use in computing and communication processes. This paper offers an exhaustive survey of RL applications in MEC networks, initially presenting an overview of RL from its fundamental principles to the latest advanced frameworks. Furthermore, it outlines various RL strategies employed in offloading, caching, and communication within MEC networks. Finally, it explores open issues linked with software and hardware platforms, representation, RL robustness, safe RL, large-scale scheduling, generalization, security, and privacy. The paper proposes specific RL techniques to mitigate these issues and provides insights into their practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14238v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Yang, Shuo Chen, Haijun Zhang, Randall Berry</dc:creator>
    </item>
    <item>
      <title>DE-LIoT: The Data-Energy Networking Paradigm for Sustainable Light-Based Internet of Things</title>
      <link>https://arxiv.org/abs/2404.14333</link>
      <description>arXiv:2404.14333v1 Announce Type: new 
Abstract: The growing demand for Internet of Things (IoT) networks has sparked interest in sustainable, zero-energy designs through Energy Harvesting (EH) to extend the lifespans of IoT sensors. Visible Light Communication (VLC) is particularly promising, integrating signal transmission with optical power harvesting to enable both data exchange and energy transfer in indoor network nodes. VLC indoor channels, however, can be unstable due to their line-of-sight nature and indoor movements. In conventional EH-based IoT networks, maximum Energy Storage (ES) capacity might halt further harvesting or waste excess energy, leading to resource inefficiency. Addressing these issues, this paper proposes a novel VLC-based WPANs concept that enhances both data and energy harvesting efficiency. The architecture employs densely distributed nodes and a central controller for simultaneous data and energy network operation, ensuring efficient energy exchange and resource optimisation. This approach, with centralised control and energy-state-aware nodes, aims for long-term energy autonomy. The feasibility of the Data-Energy Networking-enabled Light-based Internet of Things (DE-LIoT) concept is validated through real hardware implementation, demonstrating its sustainability and practical applicability. Results show significant improvements in the lifetime of resource-limited nodes, confirming the effectiveness of this new data and energy networking model in enhancing sustainability and resource optimisation in VLC-based WPANs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14333v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amila Perera, Roshan Godaliyadda, Marcos Katz</dc:creator>
    </item>
    <item>
      <title>Poisoning Attacks on Federated Learning-based Wireless Traffic Prediction</title>
      <link>https://arxiv.org/abs/2404.14389</link>
      <description>arXiv:2404.14389v1 Announce Type: new 
Abstract: Federated Learning (FL) offers a distributed framework to train a global control model across multiple base stations without compromising the privacy of their local network data. This makes it ideal for applications like wireless traffic prediction (WTP), which plays a crucial role in optimizing network resources, enabling proactive traffic flow management, and enhancing the reliability of downstream communication-aided applications, such as IoT devices, autonomous vehicles, and industrial automation systems. Despite its promise, the security aspects of FL-based distributed wireless systems, particularly in regression-based WTP problems, remain inadequately investigated. In this paper, we introduce a novel fake traffic injection (FTI) attack, designed to undermine the FL-based WTP system by injecting fabricated traffic distributions with minimal knowledge. We further propose a defense mechanism, termed global-local inconsistency detection (GLID), which strategically removes abnormal model parameters that deviate beyond a specific percentile range estimated through statistical methods in each dimension. Extensive experimental evaluations, performed on real-world wireless traffic datasets, demonstrate that both our attack and defense strategies significantly outperform existing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14389v1</guid>
      <category>cs.NI</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zifan Zhang, Minghong Fang, Jiayuan Huang, Yuchen Liu</dc:creator>
    </item>
    <item>
      <title>Adaptive Heterogeneous Client Sampling for Federated Learning over Wireless Networks</title>
      <link>https://arxiv.org/abs/2404.13804</link>
      <description>arXiv:2404.13804v1 Announce Type: cross 
Abstract: Federated learning (FL) algorithms usually sample a fraction of clients in each round (partial participation) when the number of participants is large and the server's communication bandwidth is limited. Recent works on the convergence analysis of FL have focused on unbiased client sampling, e.g., sampling uniformly at random, which suffers from slow wall-clock time for convergence due to high degrees of system heterogeneity and statistical heterogeneity. This paper aims to design an adaptive client sampling algorithm for FL over wireless networks that tackles both system and statistical heterogeneity to minimize the wall-clock convergence time. We obtain a new tractable convergence bound for FL algorithms with arbitrary client sampling probability. Based on the bound, we analytically establish the relationship between the total learning time and sampling probability with an adaptive bandwidth allocation scheme, which results in a non-convex optimization problem. We design an efficient algorithm for learning the unknown parameters in the convergence bound and develop a low-complexity algorithm to approximately solve the non-convex problem. Our solution reveals the impact of system and statistical heterogeneity parameters on the optimal client sampling design. Moreover, our solution shows that as the number of sampled clients increases, the total convergence time first decreases and then increases because a larger sampling number reduces the number of rounds for convergence but results in a longer expected time per-round due to limited wireless bandwidth. Experimental results from both hardware prototype and simulation demonstrate that our proposed sampling scheme significantly reduces the convergence time compared to several baseline sampling schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13804v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bing Luo, Wenli Xiao, Shiqiang Wang, Jianwei Huang, Leandros Tassiulas</dc:creator>
    </item>
    <item>
      <title>ICST-DNET: An Interpretable Causal Spatio-Temporal Diffusion Network for Traffic Speed Prediction</title>
      <link>https://arxiv.org/abs/2404.13853</link>
      <description>arXiv:2404.13853v1 Announce Type: cross 
Abstract: Traffic speed prediction is significant for intelligent navigation and congestion alleviation. However, making accurate predictions is challenging due to three factors: 1) traffic diffusion, i.e., the spatial and temporal causality existing between the traffic conditions of multiple neighboring roads, 2) the poor interpretability of traffic data with complicated spatio-temporal correlations, and 3) the latent pattern of traffic speed fluctuations over time, such as morning and evening rush. Jointly considering these factors, in this paper, we present a novel architecture for traffic speed prediction, called Interpretable Causal Spatio-Temporal Diffusion Network (ICST-DNET). Specifically, ICST-DENT consists of three parts, namely the Spatio-Temporal Causality Learning (STCL), Causal Graph Generation (CGG), and Speed Fluctuation Pattern Recognition (SFPR) modules. First, to model the traffic diffusion within road networks, an STCL module is proposed to capture both the temporal causality on each individual road and the spatial causality in each road pair. The CGG module is then developed based on STCL to enhance the interpretability of the traffic diffusion procedure from the temporal and spatial perspectives. Specifically, a time causality matrix is generated to explain the temporal causality between each road's historical and future traffic conditions. For spatial causality, we utilize causal graphs to visualize the diffusion process in road pairs. Finally, to adapt to traffic speed fluctuations in different scenarios, we design a personalized SFPR module to select the historical timesteps with strong influences for learning the pattern of traffic speed fluctuations. Extensive experimental results prove that ICST-DNET can outperform all existing baselines, as evidenced by the higher prediction accuracy, ability to explain causality, and adaptability to different scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13853v1</guid>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Rong, Yingchi Mao, Yinqiu Liu, Ling Chen, Xiaoming He, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>LLAMP: Assessing Network Latency Tolerance of HPC Applications with Linear Programming</title>
      <link>https://arxiv.org/abs/2404.14193</link>
      <description>arXiv:2404.14193v1 Announce Type: cross 
Abstract: The shift towards high-bandwidth networks driven by AI workloads in data centers and HPC clusters has unintentionally aggravated network latency, adversely affecting the performance of communication-intensive HPC applications. As large-scale MPI applications often exhibit significant differences in their network latency tolerance, it is crucial to accurately determine the extent of network latency an application can withstand without significant performance degradation. Current approaches to assessing this metric often rely on specialized hardware or network simulators, which can be inflexible and time-consuming. In response, we introduce LLAMP, a novel toolchain that offers an efficient, analytical approach to evaluating HPC applications' network latency tolerance using the LogGPS model and linear programming. LLAMP equips software developers and network architects with essential insights for optimizing HPC infrastructures and strategically deploying applications to minimize latency impacts. Through our validation on a variety of MPI applications like MILC, LULESH, and LAMMPS, we demonstrate our tool's high accuracy, with relative prediction errors generally below 2%. Additionally, we include a case study of the ICON weather and climate model to illustrate LLAMP's broad applicability in evaluating collective algorithms and network topologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.14193v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.PF</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Siyuan Shen, Langwen Huang, Marcin Chrapek, Timo Schneider, Jai Dayal, Manisha Gajbe, Robert Wisniewski, Torsten Hoefler</dc:creator>
    </item>
    <item>
      <title>A High-Performance Design, Implementation, Deployment, and Evaluation of The Slim Fly Network</title>
      <link>https://arxiv.org/abs/2310.03742</link>
      <description>arXiv:2310.03742v3 Announce Type: replace 
Abstract: Novel low-diameter network topologies such as Slim Fly (SF) offer significant cost and power advantages over the established Fat Tree, Clos, or Dragonfly. To spearhead the adoption of low-diameter networks, we design, implement, deploy, and evaluate the first real-world SF installation. We focus on deployment, management, and operational aspects of our test cluster with 200 servers and carefully analyze performance. We demonstrate techniques for simple cabling and cabling validation as well as a novel high-performance routing architecture for InfiniBand-based low-diameter topologies. Our real-world benchmarks show SF's strong performance for many modern workloads such as deep neural network training, graph analytics, or linear algebra kernels. SF outperforms non-blocking Fat Trees in scalability while offering comparable or better performance and lower cost for large network sizes. Our work can facilitate deploying SF while the associated (open-source) routing architecture is fully portable and applicable to accelerate any low-diameter interconnect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03742v3</guid>
      <category>cs.NI</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Blach, Maciej Besta, Daniele De Sensi, Jens Domke, Hussein Harake, Shigang Li, Patrick Iff, Marek Konieczny, Kartik Lakhotia, Ales Kubicek, Marcel Ferrari, Fabrizio Petrini, Torsten Hoefler</dc:creator>
    </item>
  </channel>
</rss>
