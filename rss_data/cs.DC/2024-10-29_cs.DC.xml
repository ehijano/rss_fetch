<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 30 Oct 2024 02:04:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Scored Non-Deterministic Finite Automata Processor for Sequence Alignment</title>
      <link>https://arxiv.org/abs/2410.19758</link>
      <description>arXiv:2410.19758v1 Announce Type: new 
Abstract: The rapid growth of symbolic data in areas like internet, biological, and financial data has increased the demand for efficient pattern matching and regular expression processing. Non-deterministic Finite Automata (NFA) are used for these tasks, but general-purpose platforms often face memory bottlenecks due to the concurrent nature of NFAs. To address this, Domain-Specific Architectures (DSAs) like FPGA and ASIC-based automata processors have been developed for improved efficiency. However, many modern applications require identifying the optimal match path, such as in DNA sequence alignment, which demands scoring methods to evaluate the best match. This work enhances the FPGA-based NAPOLY automata processor by integrating scoring capabilities, creating an extended version called NAPOLY+ that assigns weights to transitions, enabling the identification of the highest scoring path. Implementing this approach introduces challenges, including increased state space complexity and resource demands due to multiple active paths. The NAPOLY+ system addresses these by incorporating arithmetic components to calculate scores along paths and using efficient memory management to maintain scalability. Experimental evaluation on the Zynq Ultrascale+ ZCU104 FPGA demonstrated high device utilization and performance variations based on array size and fan-out. While results are preliminary, ongoing testing will include real datasets to assess the end-to-end performance of NAPOLY+ in practical applications such as BLAST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19758v1</guid>
      <category>cs.DC</category>
      <category>cs.ET</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Karbowniczak Rasha Karakchi</dc:creator>
    </item>
    <item>
      <title>SANSee: A Physical-layer Semantic-aware Networking Framework for Distributed Wireless Sensing</title>
      <link>https://arxiv.org/abs/2410.19795</link>
      <description>arXiv:2410.19795v1 Announce Type: new 
Abstract: Contactless device-free wireless sensing has recently attracted significant interest due to its potential to support a wide range of immersive human-machine interactive applications using ubiquitously available radio frequency (RF) signals. Traditional approaches focus on developing a single global model based on a combined dataset collected from different locations. However, wireless signals are known to be location and environment specific. Thus, a global model results in inconsistent and unreliable sensing results. It is also unrealistic to construct individual models for all the possible locations and environmental scenarios. Motivated by the observation that signals recorded at different locations are closely related to a set of physical-layer semantic features, in this paper we propose SANSee, a semantic-aware networking-based framework for distributed wireless sensing. SANSee allows models constructed in one or a limited number of locations to be transferred to new locations without requiring any locally labeled data or model training. SANSee is built on the concept of physical-layer semantic-aware network (pSAN), which characterizes the semantic similarity and the correlations of sensed data across different locations. A pSAN-based zero-shot transfer learning solution is introduced to allow receivers in new locations to obtain location-specific models by directly aggregating the models trained by other receivers. We theoretically prove that models obtained by SANSee can approach the locally optimal models. Experimental results based on real-world datasets are used to verify that the accuracy of the transferred models obtained by SANSee matches that of the models trained by the locally labeled data based on supervised learning approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19795v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huixiang Zhu, Yong Xiao, Yingyu Li, Guangming Shi, Marwan Krunz</dc:creator>
    </item>
    <item>
      <title>Towards Fully Automatic Distributed Lower Bounds</title>
      <link>https://arxiv.org/abs/2410.20224</link>
      <description>arXiv:2410.20224v1 Announce Type: new 
Abstract: In the past few years, a successful line of research has lead to lower bounds for several fundamental local graph problems in the distributed setting. These results were obtained via a technique called round elimination. On a high level, the round elimination technique can be seen as a recursive application of a function that takes as input a problem $\Pi$ and outputs a problem $\Pi'$ that is one round easier than $\Pi$. Applying this function recursively to concrete problems of interest can be highly nontrivial, which is one of the reasons that has made the technique difficult to approach. The contribution of our paper is threefold.
  Firstly, we develop a new and fully automatic method for finding lower bounds of $\Omega(\log_\Delta n)$ and $\Omega(\log_\Delta \log n)$ rounds for deterministic and randomized algorithms, respectively, via round elimination.
  Secondly, we show that this automatic method is indeed useful, by obtaining lower bounds for defective coloring problems. We show that the problem of coloring the nodes of a graph with $3$ colors and defect at most $(\Delta - 3)/2$ requires $\Omega(\log_\Delta n)$ rounds for deterministic algorithms and $\Omega(\log_\Delta \log n)$ rounds for randomized ones. We note that lower bounds for coloring problems are notoriously challenging to obtain, both in general, and via the round elimination technique.
  Both the first and (indirectly) the second contribution build on our third contribution -- a new and conceptually simple way to compute the one-round easier problem $\Pi'$ in the round elimination framework. This new procedure provides a clear and easy recipe for applying round elimination, thereby making a substantial step towards the greater goal of having a fully automatic procedure for obtaining lower bounds in the distributed setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20224v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkida Balliu, Sebastian Brandt, Fabian Kuhn, Dennis Olivetti, Joonatan Saarhelo</dc:creator>
    </item>
    <item>
      <title>Misconfiguration prevention and error cause detection for distributed-cloud applications</title>
      <link>https://arxiv.org/abs/2410.20273</link>
      <description>arXiv:2410.20273v1 Announce Type: new 
Abstract: Major software failures are reported to be due to misconfiguration. As manual configuration is too error-prone to be deemed a reliable strategy for dynamic and complex systems, automated configuration management has become a standard. Countermeasures against misconfiguration can be focused on prevention or, if failure already occurred, detection. Configuration is often used as a broad term for any set of parameters or system states that dictate how an application will behave, but in this paper, we only focus on parameters consumed on process startup, usually from configuration files. Our objective is to enhance configuration management processes in environments based on the distributed cloud model, a novel cloud model that allows dynamic allocation of strategically located resources. The two mechanisms we propose are configuration validation using schemas and configuration version control with support for detecting differences between configuration versions. Our solution reduces the risk of incorrect configuration as schemas prevent any non-compliant configuration from reaching applications. However, if failure still occurs because the schema was incomplete or a valid configuration revealed existing software bugs, the version control system can precisely locate configuration changes that triggered the failure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20273v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tamara Rankovi\'c, Filip \v{S}ilji\'c, Jovan Tomi\'c, Goran Sladi\'c, Milo\v{s} Simi\'c</dc:creator>
    </item>
    <item>
      <title>Configuration management in the distributed cloud</title>
      <link>https://arxiv.org/abs/2410.20276</link>
      <description>arXiv:2410.20276v1 Announce Type: new 
Abstract: Owing to their cost-effectiveness and flexibility, cloud services have been the default choice for the deployment of innumerable software systems over the years. However, novel paradigms are beginning to emerge, as the cloud can't meet the requirements of increasingly many latency- and privacy-sensitive applications. The distributed cloud model, being one of the attempts to overcome these challenges, places a distributed cloud layer between device and cloud layers, intending to bring resources closer to data sources. As application code should be kept separate from its configuration, especially in highly dynamic cloud environments, there is a need to incorporate configuration primitives in future distributed cloud platforms. In this paper, we present the design and implementation of a configuration management subsystem for an open-source distributed cloud platform. Our solution spreads across the cloud and distributed cloud layers and supports configuration versioning, selective dissemination to nodes in the distributed cloud layer, and logical isolation via namespaces. Our work serves as a demonstration of the feasibility and usability of the new cloud-extending models and provides valuable insight into one of the possible implementations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20276v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-71419-1_20</arxiv:DOI>
      <dc:creator>Tamara Rankovi\'c, Ivana Kova\v{c}evi\'c, Veljko Maksimovi\'c, Goran Sladi\'c, Milo\v{s} Simi\'c</dc:creator>
    </item>
    <item>
      <title>EACO-RAG: Edge-Assisted and Collaborative RAG with Adaptive Knowledge Update</title>
      <link>https://arxiv.org/abs/2410.20299</link>
      <description>arXiv:2410.20299v1 Announce Type: new 
Abstract: Large Language Models are revolutionizing Web, mobile, and Web of Things systems, driving intelligent and scalable solutions. However, as Retrieval-Augmented Generation (RAG) systems expand, they encounter significant challenges related to scalability, including increased delay and communication overhead. To address these issues, we propose EACO-RAG, an edge-assisted distributed RAG system that leverages adaptive knowledge updates and inter-node collaboration. By distributing vector datasets across edge nodes and optimizing retrieval processes, EACO-RAG significantly reduces delay and resource consumption while enhancing response accuracy. The system employs a multi-armed bandit framework with safe online Bayesian methods to balance performance and cost. Extensive experimental evaluation demonstrates that EACO-RAG outperforms traditional centralized RAG systems in both response time and resource efficiency. EACO-RAG effectively reduces delay and resource expenditure to levels comparable to, or even lower than, those of local RAG systems, while significantly improving accuracy. This study presents the first systematic exploration of edge-assisted distributed RAG architectures, providing a scalable and cost-effective solution for large-scale distributed environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20299v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaxing Li, Chi Xu, Lianchen Jia, Feng Wang, Cong Zhang, Jiangchuan Liu</dc:creator>
    </item>
    <item>
      <title>Distributed Complexity of $P_k$-freeness: Decision and Certification</title>
      <link>https://arxiv.org/abs/2410.20353</link>
      <description>arXiv:2410.20353v1 Announce Type: new 
Abstract: The class of graphs that do not contain a path on $k$ nodes as an induced subgraph ($P_k$-free graphs) has rich applications in the theory of graph algorithms. This paper explores the problem of deciding $P_k$-freeness from the viewpoint of distributed computing.
  For specific small values of $k$, we present the \textit{first} $\mathsf{CONGEST}$ algorithms specified for $P_k$-freeness, utilizing structural properties of $P_k$-free graphs in a novel way. Specifically, we show that $P_k$-freeness can be decided in $\tilde{O}(1)$ rounds for $k=4$ in the $\mathsf{broadcast\;CONGEST}$ model, and in $\tilde{O}(n)$ rounds for $k=5$ in the $\mathsf{CONGEST}$ model, where $n$ is the number of nodes in the network and $\tilde{O}(\cdot)$ hides a $\mathrm{polylog}(n)$ factor. These results significantly improve the previous $O(n^{2-2/(3k+2)})$ upper bounds by Eden et al. (Dist.~Comp.~2022). We also construct a local certification of $P_5$-freeness with certificates of size $\tilde{O}(n)$. This is nearly optimal, given our $\Omega(n^{1-o(1)})$ lower bound on certificate size, and marks a significant advancement as no nontrivial bounds for proof-labeling schemes of $P_5$-freeness were previously known.
  For general $k$, we establish the first $\mathsf{CONGEST}$ lower bound, which is of the form $n^{2-1/\Theta(k)}$. The $n^{1/\Theta(k)}$ factor is unavoidable, in view of the $O(n^{2-2/(3k+2)})$ upper bound mentioned above. Additionally, our approach yields the \textit{first} superlinear lower bound on certificate size for local certification. This partially answers the conjecture on the optimal certificate size of $P_k$-freeness, asked by Bousquet et al. (arXiv:2402.12148).
  Finally, we propose a novel variant of the problem called ordered $P_k$ detection, and show a linear lower bound and its nontrivial connection to distributed subgraph detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20353v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masayuki Miyamoto</dc:creator>
    </item>
    <item>
      <title>When Less is More: Achieving Faster Convergence in Distributed Edge Machine Learning</title>
      <link>https://arxiv.org/abs/2410.20495</link>
      <description>arXiv:2410.20495v1 Announce Type: new 
Abstract: Distributed Machine Learning (DML) on resource-constrained edge devices holds immense potential for real-world applications. However, achieving fast convergence in DML in these heterogeneous environments remains a significant challenge. Traditional frameworks like Bulk Synchronous Parallel and Asynchronous Stochastic Parallel rely on frequent, small updates that incur substantial communication overhead and hinder convergence speed. Furthermore, these frameworks often employ static dataset sizes, neglecting the heterogeneity of edge devices and potentially leading to straggler nodes that slow down the entire training process. The straggler nodes, i.e., edge devices that take significantly longer to process their assigned data chunk, hinder the overall training speed. To address these limitations, this paper proposes Hermes, a novel probabilistic framework for efficient DML on edge devices. This framework leverages a dynamic threshold based on recent test loss behavior to identify statistically significant improvements in the model's generalization capability, hence transmitting updates only when major improvements are detected, thereby significantly reducing communication overhead. Additionally, Hermes employs dynamic dataset allocation to optimize resource utilization and prevents performance degradation caused by straggler nodes. Our evaluations on a real-world heterogeneous resource-constrained environment demonstrate that Hermes achieves faster convergence compared to state-of-the-art methods, resulting in a remarkable $13.22$x reduction in training time and a $62.1\%$ decrease in communication overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20495v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Advik Raj Basani, Siddharth Chaitra Vivek, Advaith Krishna, Arnab K. Paul</dc:creator>
    </item>
    <item>
      <title>Solving Sequential Greedy Problems Distributedly with Sub-Logarithmic Energy Cost</title>
      <link>https://arxiv.org/abs/2410.20499</link>
      <description>arXiv:2410.20499v1 Announce Type: new 
Abstract: We study the awake complexity of graph problems that belong to the class O-LOCAL, which includes a large subset of problems solvable by sequential greedy algorithms, such as $(\Delta+1)$-coloring, maximal independent set, maximal matching, etc. It is known from previous work that, in $n$-node graphs of maximum degree $\Delta$, any problem in the class O-LOCAL can be solved by a deterministic distributed algorithm with awake complexity $O(\log\Delta+\log^\star n)$.
  In this paper, we show that any problem belonging to the class O-LOCAL can be solved by a deterministic distributed algorithm with awake complexity $O(\sqrt{\log n}\cdot\log^\star n)$. This leads to a polynomial improvement over the state of the art when $\Delta\gg 2^{\sqrt{\log n}}$, e.g., $\Delta=n^\epsilon$ for some arbitrarily small $\epsilon&gt;0$. The key ingredient for achieving our results is the computation of a network decomposition, that uses a small-enough number of colors, in sub-logarithmic time in the Sleeping model, which can be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20499v1</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alkida Balliu, Pierre Fraigniaud, Dennis Olivetti, Mika\"el Rabie</dc:creator>
    </item>
    <item>
      <title>CodeRosetta: Pushing the Boundaries of Unsupervised Code Translation for Parallel Programming</title>
      <link>https://arxiv.org/abs/2410.20527</link>
      <description>arXiv:2410.20527v1 Announce Type: new 
Abstract: Recent advancements in Large Language Models (LLMs) have renewed interest in automatic programming language translation. Encoder-decoder transformer models, in particular, have shown promise in translating between different programming languages. However, translating between a language and its high-performance computing (HPC) extensions remains underexplored due to challenges such as complex parallel semantics. In this paper, we introduce CodeRosetta, an encoder-decoder transformer model designed specifically for translating between programming languages and their HPC extensions. CodeRosetta is evaluated on C++ to CUDA and Fortran to C++ translation tasks. It uses a customized learning framework with tailored pretraining and training objectives to effectively capture both code semantics and parallel structural nuances, enabling bidirectional translation. Our results show that CodeRosetta outperforms state-of-the-art baselines in C++ to CUDA translation by 2.9 BLEU and 1.72 CodeBLEU points while improving compilation accuracy by 6.05%. Compared to general closed-source LLMs, our method improves C++ to CUDA translation by 22.08 BLEU and 14.39 CodeBLEU, with 2.75% higher compilation accuracy. Finally, CodeRosetta exhibits proficiency in Fortran to parallel C++ translation, marking it, to our knowledge, as the first encoder-decoder model for this complex task, improving CodeBLEU by at least 4.63 points compared to closed-source and open-code LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20527v1</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <category>cs.PL</category>
      <category>cs.SE</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ali TehraniJamsaz, Arijit Bhattacharjee, Le Chen, Nesreen K. Ahmed, Amir Yazdanbakhsh, Ali Jannesari</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Survey on Green Blockchain: Developing the Next Generation of Energy Efficient and Sustainable Blockchain Systems</title>
      <link>https://arxiv.org/abs/2410.20581</link>
      <description>arXiv:2410.20581v1 Announce Type: new 
Abstract: Although Blockchain has been successfully used in many different fields and applications, it has been traditionally regarded as an energy-intensive technology, essentially due to the past use of inefficient consensus algorithms that prioritized security over sustainability. However, in the last years, thanks to the significant progress made on key blockchain components, their energy consumption can be decreased noticeably. To achieve this objective, this article analyzes the main components of blockchains and explores strategies to reduce their energy consumption. In this way, this article delves into each component of a blockchain system, including consensus mechanisms, network architecture, data storage and validation, smart contract execution, mining and block creation, and outlines specific strategies to decrease their energy consumption. For such a purpose, consensus mechanisms are compared, recommendations for reducing network communications energy consumption are provided, techniques for data storage and validation are suggested and diverse optimizations are proposed both for software and hardware components. Moreover, the main challenges and limitations of reducing power consumption in blockchain systems are analyzed. As a consequence, this article provides a guideline for the future researchers and developers who aim to develop the next generation of Green Blockchain solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20581v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago M. Fern\'andez-Caram\'es, Paula Fraga-Lamas</dc:creator>
    </item>
    <item>
      <title>Advancing Towards Green Blockchain: A Practical Energy-Efficient Blockchain Based Application for CV Verification</title>
      <link>https://arxiv.org/abs/2410.20605</link>
      <description>arXiv:2410.20605v1 Announce Type: new 
Abstract: Blockchain has been widely criticized due to the use of inefficient consensus protocols and energy-intensive mechanisms that derived into a global enormous power consumption. Fortunately, since the first blockchain was conceived in 2008 (the one that supports Bitcoin), hardware and consensus protocols have evolved, decreasing energy consumption significantly. This article describes a green blockchain solution and quantifies energy savings when deploying the system on traditional computers and embedded Single-Board Computers (SBCs). To illustrate such savings, it is proposed a solution for tackling the problem of academic certificate forgery, which has a significant cost to society, since it harms the trustworthiness of certificates and academic institutions. The proposed solution is aimed at recording and verifying academic records (ARs) through a decentralized application (DApp) that is supported by a smart contract deployed in the Ethereum blockchain. The application stores the raw data (i.e., the data that are not managed by the blockchain) on a decentralized storage system based on Inter-Planetary File System (IPFS). To demonstrate the efficiency of the developed solution, it is evaluated in terms of performance (transaction latency and throughput) and efficiency (CPU usage and energy consumption), comparing the results obtained with a traditional Proof-of-Work (PoW) consensus protocol and the new Proof-of-Authority (PoA) protocol. The results shown in this paper indicate that the latter is clearly greener and demands less CPU load. Moreover, this article compares the performance of a traditional computer and two SBCs (a Raspberry Pi 4 and an Orange Pi One), showing that is possible to make use of the latter low-power devices to implement blockchain nodes for proposed DApp, but at the cost of higher response latency that varies greatly depending on the used SBCs [...]</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20605v1</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Fern\'andez-Blanco, Iv\'an Froiz-M\'iguez, Paula Fraga-Lamas, Tiago M. Fern\'andez-Caram\'es</dc:creator>
    </item>
    <item>
      <title>Fully-Distributed Byzantine Agreement in Sparse Networks</title>
      <link>https://arxiv.org/abs/2410.20865</link>
      <description>arXiv:2410.20865v1 Announce Type: new 
Abstract: Byzantine agreement is a fundamental problem in fault-tolerant distributed networks that has been studied intensively for the last four decades. Most of these works designed protocols for complete networks. A key goal in Byzantine protocols is to tolerate as many Byzantine nodes as possible.
  The work of Dwork, Peleg, Pippenger, and Upfal [STOC 1986, SICOMP 1988] was the first to address the Byzantine agreement problem in sparse, bounded degree networks and presented a protocol that achieved almost-everywhere agreement among honest nodes. In such networks, all known Byzantine agreement protocols (e.g., Dwork, Peleg, Pippenger, and Upfal, STOC 1986; Upfal, PODC 1992; King, Saia, Sanwalani, and Vee, FOCS 2006) that tolerated a large number of Byzantine nodes had a major drawback that they were not fully-distributed -- in those protocols, nodes are required to have initial knowledge of the entire network topology. This drawback makes such protocols inapplicable to real-world communication networks such as peer-to-peer (P2P) networks, which are typically sparse and bounded degree and where nodes initially have only local knowledge of themselves and their neighbors. Indeed, a fundamental open question raised by the above works is whether one can design Byzantine protocols that tolerate a large number of Byzantine nodes in sparse networks that work with only local knowledge, i.e., fully-distributed protocols. The work of Augustine, Pandurangan, and Robinson [PODC 2013] presented the first fully-distributed Byzantine agreement protocol that works in sparse networks, but it tolerated only up to $O(\sqrt{n}/ polylog(n))$ Byzantine nodes (where $n$ is the total network size).
  We answer the earlier open question by presenting fully-distributed Byzantine agreement protocols for sparse, bounded degree networks that tolerate significantly more Byzantine nodes -- up to $O(n/ polylog(n))$ of them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20865v1</guid>
      <category>cs.DC</category>
      <category>cs.DS</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Augustine, Fabien Dufoulon, Gopal Pandurangan</dc:creator>
    </item>
    <item>
      <title>CloudHeatMap: Heatmap-Based Monitoring for Large-Scale Cloud Systems</title>
      <link>https://arxiv.org/abs/2410.21092</link>
      <description>arXiv:2410.21092v1 Announce Type: new 
Abstract: Cloud computing is essential for modern enterprises, requiring robust tools to monitor and manage Large-Scale Cloud Systems (LCS). Traditional monitoring tools often miss critical insights due to the complexity and volume of LCS telemetry data. This paper presents CloudHeatMap, a novel heatmap-based visualization tool for near-real-time monitoring of LCS health. It offers intuitive visualizations of key metrics such as call volumes, response times, and HTTP response codes, enabling operators to quickly identify performance issues. A case study on the IBM Cloud Console demonstrates the tool's effectiveness in enhancing operational monitoring and decision-making. A demonstration is available at https://www.youtube.com/watch?v=3u5K1qp51EA .</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21092v1</guid>
      <category>cs.DC</category>
      <category>cs.SE</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarah Sohana, William Pourmajidi, John Steinbacher, Andriy Miranskyy</dc:creator>
    </item>
    <item>
      <title>A Unified Solution to Diverse Heterogeneities in One-shot Federated Learning</title>
      <link>https://arxiv.org/abs/2410.21119</link>
      <description>arXiv:2410.21119v1 Announce Type: new 
Abstract: One-shot federated learning (FL) limits the communication between the server and clients to a single round, which largely decreases the privacy leakage risks in traditional FLs requiring multiple communications. However, we find existing one-shot FL frameworks are vulnerable to distributional heterogeneity due to their insufficient focus on data heterogeneity while concentrating predominantly on model heterogeneity. Filling this gap, we propose a unified, data-free, one-shot federated learning framework (FedHydra) that can effectively address both model and data heterogeneity. Rather than applying existing value-only learning mechanisms, a structure-value learning mechanism is proposed in FedHydra. Specifically, a new stratified learning structure is proposed to cover data heterogeneity, and the value of each item during computation reflects model heterogeneity. By this design, the data and model heterogeneity issues are simultaneously monitored from different aspects during learning. Consequently, FedHydra can effectively mitigate both issues by minimizing their inherent conflicts. We compared FedHydra with three SOTA baselines on four benchmark datasets. Experimental results show that our method outperforms the previous one-shot FL methods in both homogeneous and heterogeneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21119v1</guid>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jun Bai, Yiliao Song, Di Wu, Atul Sajjanhar, Yong Xiang, Wei Zhou, Xiaohui Tao, Yan Li</dc:creator>
    </item>
    <item>
      <title>SALINA: Towards Sustainable Live Sonar Analytics in Wild Ecosystems</title>
      <link>https://arxiv.org/abs/2410.19742</link>
      <description>arXiv:2410.19742v1 Announce Type: cross 
Abstract: Sonar radar captures visual representations of underwater objects and structures using sound wave reflections, making it essential for exploration, mapping, and continuous surveillance in wild ecosystems. Real-time analysis of sonar data is crucial for time-sensitive applications, including environmental anomaly detection and in-season fishery management, where rapid decision-making is needed. However, the lack of both relevant datasets and pre-trained DNN models, coupled with resource limitations in wild environments, hinders the effective deployment and continuous operation of live sonar analytics.
  We present SALINA, a sustainable live sonar analytics system designed to address these challenges. SALINA enables real-time processing of acoustic sonar data with spatial and temporal adaptations, and features energy-efficient operation through a robust energy management module. Deployed for six months at two inland rivers in British Columbia, Canada, SALINA provided continuous 24/7 underwater monitoring, supporting fishery stewardship and wildlife restoration efforts. Through extensive real-world testing, SALINA demonstrated an up to 9.5% improvement in average precision and a 10.1% increase in tracking metrics. The energy management module successfully handled extreme weather, preventing outages and reducing contingency costs. These results offer valuable insights for long-term deployment of acoustic data systems in the wild.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19742v1</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3666025.3699323</arxiv:DOI>
      <dc:creator>Chi Xu, Rongsheng Qian, Hao Fang, Xiaoqiang Ma, William I. Atlas, Jiangchuan Liu, Mark A. Spoljaric</dc:creator>
    </item>
    <item>
      <title>Scheduling Languages: A Past, Present, and Future Taxonomy</title>
      <link>https://arxiv.org/abs/2410.19927</link>
      <description>arXiv:2410.19927v1 Announce Type: cross 
Abstract: Scheduling languages express to a compiler a sequence of optimizations to apply. Compilers that support a scheduling language interface allow exploration of compiler optimizations, i.e., exploratory compilers. While scheduling languages have become a common feature of tools for expert users, the proliferation of these languages without unifying common features may be confusing to users. Moreover, we recognize a need to organize the compiler developer community around common exploratory compiler infrastructure, and future advances to address, for example, data layout and data movement. To support a broader set of users may require raising the level of abstraction. This paper provides a taxonomy of scheduling languages, first discussing their origins in iterative compilation and autotuning, noting the common features and how they are used in existing frameworks, and then calling for changes to increase their utility and portability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19927v1</guid>
      <category>cs.PL</category>
      <category>cs.DC</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mary Hall, Cosmin Oancea, Anne C. Elster, Ari Rasch, Sameeran Joshi, Amir Mohammad Tavakkoli, Richard Schulze</dc:creator>
    </item>
    <item>
      <title>OReole-FM: successes and challenges toward billion-parameter foundation models for high-resolution satellite imagery</title>
      <link>https://arxiv.org/abs/2410.19965</link>
      <description>arXiv:2410.19965v1 Announce Type: cross 
Abstract: While the pretraining of Foundation Models (FMs) for remote sensing (RS) imagery is on the rise, models remain restricted to a few hundred million parameters. Scaling models to billions of parameters has been shown to yield unprecedented benefits including emergent abilities, but requires data scaling and computing resources typically not available outside industry R&amp;D labs. In this work, we pair high-performance computing resources including Frontier supercomputer, America's first exascale system, and high-resolution optical RS data to pretrain billion-scale FMs. Our study assesses performance of different pretrained variants of vision Transformers across image classification, semantic segmentation and object detection benchmarks, which highlight the importance of data scaling for effective model scaling. Moreover, we discuss construction of a novel TIU pretraining dataset, model initialization, with data and pretrained models intended for public release. By discussing technical challenges and details often lacking in the related literature, this work is intended to offer best practices to the geospatial community toward efficient training and benchmarking of larger FMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19965v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3678717.3691292</arxiv:DOI>
      <arxiv:journal_reference>The 32nd ACM International Conference on Advances in Geographic Information Systems (SIGSPATIAL 24), October 29-November 1, 2024, Atlanta, GA, USA. ACM, New York, NY, USA, 4 pages</arxiv:journal_reference>
      <dc:creator>Philipe Dias, Aristeidis Tsaris, Jordan Bowman, Abhishek Potnis, Jacob Arndt, H. Lexie Yang, Dalton Lunga</dc:creator>
    </item>
    <item>
      <title>Lightweight, Secure and Stateful Serverless Computing with PSL</title>
      <link>https://arxiv.org/abs/2410.20004</link>
      <description>arXiv:2410.20004v1 Announce Type: cross 
Abstract: We present PSL, a lightweight, secure and stateful Function-as-a-Serivce (FaaS) framework for Trusted Execution Environments (TEEs). The framework provides rich programming language support on heterogeneous TEE hardware for statically compiled binaries and/or WebAssembly (WASM) bytecodes, with a familiar Key-Value Store (KVS) interface to secure, performant, network-embedded storage. It achieves near-native execution speeds by utilizing the dynamic memory mapping capabilities of Intel SGX2 to create an in-enclave WASM runtime with Just-In-Time (JIT) compilation. PSL is designed to efficiently operate within an asynchronous environment with a distributed tamper-proof confidential storage system, assuming minority failures. The system exchanges eventually-consistent state updates across nodes while utilizing release-consistent locking mechanisms to enhance transactional capabilities. The execution of PSL is up to 3.7x faster than the state-of-the-art SGX WASM runtime. PSL reaches 95k ops/s with YCSB 100% read workload and 89k ops/s with 50% read/write workload. We demonstrate the scalability and adaptivity of PSL through a case study of secure and distributed training of deep neural networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20004v1</guid>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexander Thomas, Shubham Mishra, Kaiyuan Chen, John Kubiatowicz</dc:creator>
    </item>
    <item>
      <title>Efficient Circuit Wire Cutting Based on Commuting Groups</title>
      <link>https://arxiv.org/abs/2410.20313</link>
      <description>arXiv:2410.20313v1 Announce Type: cross 
Abstract: Current quantum devices face challenges when dealing with large circuits due to error rates as circuit size and the number of qubits increase. The circuit wire-cutting technique addresses this issue by breaking down a large circuit into smaller, more manageable subcircuits. However, the exponential increase in the number of subcircuits and the complexity of reconstruction as more cuts are made poses a great practical challenge. Inspired by ancilla-assisted quantum process tomography and the MUBs-based grouping technique for simultaneous measurement, we propose a new approach that can reduce subcircuit running overhead. The approach first uses ancillary qubits to transform all quantum input initializations into quantum output measurements. These output measurements are then organized into commuting groups for the purpose of simultaneous measurement, based on MUBs-based grouping. This approach significantly reduces the number of necessary subcircuits as well as the total number of shots. Lastly, we provide numerical experiments to demonstrate the complexity reduction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20313v1</guid>
      <category>quant-ph</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinpeng Li, Vinooth Kulkarni, Daniel T. Chen, Qiang Guan, Weiwen Jiang, Ning Xie, Shuai Xu, Vipin Chaudhary</dc:creator>
    </item>
    <item>
      <title>FuseFL: One-Shot Federated Learning through the Lens of Causality with Progressive Model Fusion</title>
      <link>https://arxiv.org/abs/2410.20380</link>
      <description>arXiv:2410.20380v1 Announce Type: cross 
Abstract: One-shot Federated Learning (OFL) significantly reduces communication costs in FL by aggregating trained models only once. However, the performance of advanced OFL methods is far behind the normal FL. In this work, we provide a causal view to find that this performance drop of OFL methods comes from the isolation problem, which means that local isolatedly trained models in OFL may easily fit to spurious correlations due to the data heterogeneity. From the causal perspective, we observe that the spurious fitting can be alleviated by augmenting intermediate features from other clients. Built upon our observation, we propose a novel learning approach to endow OFL with superb performance and low communication and storage costs, termed as FuseFL. Specifically, FuseFL decomposes neural networks into several blocks, and progressively trains and fuses each block following a bottom-up manner for feature augmentation, introducing no additional communication costs. Comprehensive experiments demonstrate that FuseFL outperforms existing OFL and ensemble FL by a significant margin. We conduct comprehensive experiments to show that FuseFL supports high scalability of clients, heterogeneous model training, and low memory costs. Our work is the first attempt using causality to analyze and alleviate data heterogeneity of OFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20380v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenheng Tang, Yonggang Zhang, Peijie Dong, Yiu-ming Cheung, Amelie Chi Zhou, Bo Han, Xiaowen Chu</dc:creator>
    </item>
    <item>
      <title>Smart Space Environments: Key Challenges and Innovative Solutions</title>
      <link>https://arxiv.org/abs/2410.20484</link>
      <description>arXiv:2410.20484v1 Announce Type: cross 
Abstract: The integration of LoRaWAN (Long Range Wide Area Network) technology with both active and passive sensors presents a transformative opportunity for the development of smart home systems. This paper explores how active sensors, such as motion detectors and ultrasonic sensors, and passive sensors, including temperature and humidity sensors, work together to enhance connectivity and efficiency within diverse environments while addressing the challenges of modern living. By leveraging LoRaWAN long-range capabilities and low power consumption, the proposed framework enables effective data transmission from remote sensors, facilitating applications such as smart agriculture, environmental monitoring, and comprehensive home automation. Active sensors emit energy to detect changes in their surroundings, providing real-time data crucial for security and automation, while passive sensors capture ambient energy to monitor environmental conditions, ensuring resource efficiency and user comfort. The synergy between LoRaWAN and these various sensor types promotes innovation, contributing to a more responsive and sustainable living experience. Furthermore, this research highlights the adaptability of the proposed system, allowing for seamless integration of new devices and advanced functionalities. As the landscape of smart home technology continues to evolve, ongoing research in this area will yield advanced solutions tailored to user needs, ultimately paving the way for smarter, safer, and more efficient living environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20484v1</guid>
      <category>cs.ET</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ramakant Kumar</dc:creator>
    </item>
    <item>
      <title>Federated Time Series Generation on Feature and Temporally Misaligned Data</title>
      <link>https://arxiv.org/abs/2410.21072</link>
      <description>arXiv:2410.21072v1 Announce Type: cross 
Abstract: Distributed time series data presents a challenge for federated learning, as clients often possess different feature sets and have misaligned time steps. Existing federated time series models are limited by the assumption of perfect temporal or feature alignment across clients. In this paper, we propose FedTDD, a novel federated time series diffusion model that jointly learns a synthesizer across clients. At the core of FedTDD is a novel data distillation and aggregation framework that reconciles the differences between clients by imputing the misaligned timesteps and features. In contrast to traditional federated learning, FedTDD learns the correlation across clients' time series through the exchange of local synthetic outputs instead of model parameters. A coordinator iteratively improves a global distiller network by leveraging shared knowledge from clients through the exchange of synthetic data. As the distiller becomes more refined over time, it subsequently enhances the quality of the clients' local feature estimates, allowing each client to then improve its local imputations for missing data using the latest, more accurate distiller. Experimental results on five datasets demonstrate FedTDD's effectiveness compared to centralized training, and the effectiveness of sharing synthetic outputs to transfer knowledge of local time series. Notably, FedTDD achieves 79.4% and 62.8% improvement over local training in Context-FID and Correlational scores.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21072v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenrui Fan, Zhi Wen Soi, Aditya Shankar, Abele M\u{a}lan, Lydia Y. Chen</dc:creator>
    </item>
    <item>
      <title>A Survey of Distributed Graph Algorithms on Massive Graphs</title>
      <link>https://arxiv.org/abs/2404.06037</link>
      <description>arXiv:2404.06037v2 Announce Type: replace 
Abstract: Distributed processing of large-scale graph data has many practical applications and has been widely studied. In recent years, a lot of distributed graph processing frameworks and algorithms have been proposed. While many efforts have been devoted to analyzing these, with most analyzing them based on programming models, less research focuses on understanding their challenges in distributed environments. Applying graph tasks to distributed environments is not easy, often facing numerous challenges through our analysis, including parallelism, load balancing, communication overhead, and bandwidth. In this paper, we provide an extensive overview of the current state-of-the-art in this field by outlining the challenges and solutions of distributed graph algorithms. We first conduct a systematic analysis of the inherent challenges in distributed graph processing, followed by presenting an overview of existing general solutions. Subsequently, we survey the challenges highlighted in recent distributed graph processing papers and the strategies adopted to address them. Finally, we discuss the current research trends and identify potential future opportunities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06037v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lingkai Meng, Yu Shao, Long Yuan, Longbin Lai, Peng Cheng, Xue Li, Wenyuan Yu, Wenjie Zhang, Xuemin Lin, Jingren Zhou</dc:creator>
    </item>
    <item>
      <title>GraphPipe: Improving Performance and Scalability of DNN Training with Graph Pipeline Parallelism</title>
      <link>https://arxiv.org/abs/2406.17145</link>
      <description>arXiv:2406.17145v2 Announce Type: replace 
Abstract: Deep neural networks (DNNs) continue to grow rapidly in size, making them infeasible to train on a single device. Pipeline parallelism is commonly used in existing DNN systems to support large-scale DNN training by partitioning a DNN into multiple stages, which concurrently perform DNN training for different micro-batches in a pipeline fashion. However, existing pipeline-parallel approaches only consider sequential pipeline stages and thus ignore the topology of a DNN, resulting in missed model-parallel opportunities. This paper presents graph pipeline parallelism (GPP), a new pipeline-parallel scheme that partitions a DNN into pipeline stages whose dependencies are identified by a directed acyclic graph. GPP generalizes existing sequential pipeline parallelism and preserves the inherent topology of a DNN to enable concurrent execution of computationally-independent operators, resulting in reduced memory requirement and improved GPU performance. In addition, we develop GraphPipe, a distributed system that exploits GPP strategies to enable performant and scalable DNN training. GraphPipe partitions a DNN into a graph of stages, optimizes micro-batch schedules for these stages, and parallelizes DNN training using the discovered GPP strategies. Evaluation on a variety of DNNs shows that GraphPipe outperforms existing pipeline-parallel systems such as PipeDream and Piper by up to 1.6X. GraphPipe also reduces the search time by 9-21X compared to PipeDream and Piper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17145v2</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Byungsoo Jeon, Mengdi Wu, Shiyi Cao, Sunghyun Kim, Sunghyun Park, Neeraj Aggarwal, Colin Unger, Daiyaan Arfeen, Peiyuan Liao, Xupeng Miao, Mohammad Alizadeh, Gregory R. Ganger, Tianqi Chen, Zhihao Jia</dc:creator>
    </item>
    <item>
      <title>DRAM Errors and Cosmic Rays: Space Invaders or Science Fiction?</title>
      <link>https://arxiv.org/abs/2407.16487</link>
      <description>arXiv:2407.16487v2 Announce Type: replace 
Abstract: It is widely accepted that cosmic rays are a plausible cause of DRAM errors in high-performance computing (HPC) systems, and various studies suggest that they could explain some aspects of the observed DRAM error behavior. However, this phenomenon is insufficiently studied in production environments. We analyze the correlations between cosmic rays and DRAM errors on two HPC clusters: a production supercomputer with server-class DDR3-1600 and a prototype with LPDDR3-1600 and no hardware error correction. Our error logs cover 2000 billion MB-hours for the MareNostrum 3 supercomputer and 135 million MB-hours for the Mont-Blanc prototype. Our analysis combines quantitative analysis, formal statistical methods and machine learning. We detect no indications that cosmic rays have any influence on the DRAM errors. To understand whether the findings are specific to systems under study, located at 100 meters above the sea level, the analysis should be repeated on other HPC clusters, especially the ones located on higher altitudes. Also, analysis can (and should) be applied to revisit and extend numerous previous studies which use cosmic rays as a hypothetical explanation for some aspects of the observed DRAM error behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16487v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Boixaderas, Jorge Amaya, Sergi Mor\'e, Javier Bartolome, David Vicente, Osman Unsal, Dimitris Gizopoulos, Paul M. Carpenter, Petar Radojkovi\'c, Eduard Ayguad\'e</dc:creator>
    </item>
    <item>
      <title>Universal Finite-State and Self-Stabilizing Computation in Anonymous Dynamic Networks</title>
      <link>https://arxiv.org/abs/2409.00688</link>
      <description>arXiv:2409.00688v2 Announce Type: replace 
Abstract: A network is said to be "anonymous" if its agents are indistinguishable from each other; it is "dynamic" if its communication links may appear or disappear unpredictably over time. Assuming that an anonymous dynamic network is always connected and each of its $n$ agents is initially given an input, it takes $2n$ communication rounds for the agents to compute an arbitrary (frequency-based) function of such inputs (Di Luna-Viglietta, DISC 2023).
  It is known that, without making additional assumptions on the network and without knowing the number of agents $n$, it is impossible to compute most functions and explicitly terminate. In fact, current state-of-the-art algorithms only achieve stabilization, i.e., allow each agent to return an output after every communication round; outputs can be changed, and are guaranteed to be all correct after $2n$ rounds. Such algorithms rely on the incremental construction of a data structure called "history tree", which is augmented at every round. Thus, they end up consuming an unlimited amount of memory, and are also prone to errors in case of memory loss or corruption.
  In this paper, we provide a general self-stabilizing algorithm for anonymous dynamic networks that stabilizes in $\max\{4n-2h, 2h\}$ rounds (where $h$ measures the amount of corrupted data initially present in the memory of each agent), as well as a general finite-state algorithm that stabilizes in $3n^2$ rounds. Our work improves upon previously known methods that only apply to static networks (Boldi-Vigna, Dist. Comp. 2002). In addition, we develop new fundamental techniques and operations involving history trees, which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00688v2</guid>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giuseppe A. Di Luna, Giovanni Viglietta</dc:creator>
    </item>
    <item>
      <title>Confidential Computing on nVIDIA Hopper GPUs: A Performance Benchmark Study</title>
      <link>https://arxiv.org/abs/2409.03992</link>
      <description>arXiv:2409.03992v3 Announce Type: replace 
Abstract: This report evaluates the performance impact of enabling Trusted Execution Environments (TEE) on nVIDIA Hopper GPUs for large language model (LLM) inference tasks. We benchmark the overhead introduced by TEE mode across various LLMs and token lengths, with a particular focus on the bottleneck caused by CPU-GPU data transfers via PCIe. Our results indicate that while there is minimal computational overhead within the GPU, the overall performance penalty is primarily attributable to data transfer. For the majority of typical LLM queries, the overhead remains below 7%, with larger models and longer sequences experiencing nearly zero overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03992v3</guid>
      <category>cs.DC</category>
      <category>cs.AI</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianwei Zhu, Hang Yin, Peng Deng, Aline Almeida, Shunfan Zhou</dc:creator>
    </item>
    <item>
      <title>A Cut-Matching Game for Constant-Hop Expanders</title>
      <link>https://arxiv.org/abs/2211.11726</link>
      <description>arXiv:2211.11726v2 Announce Type: replace-cross 
Abstract: This paper extends and generalizes the well-known cut-matching game framework and provides a novel cut-strategy that produces constant-hop expanders.
  Constant-hop expanders are a significant strengthening of regular expanders with the additional guarantee that any demand can be (obliviously) routed along constant-hop flow-paths - in contrast to the $\Omega(\log n)$-hop paths in expanders.
  Cut-matching games for expanders are key tools for obtaining linear-time approximation algorithms for many hard problems, including finding (balanced or approximately-largest) sparse cuts, certifying the expansion of a graph by embedding an (explicit) expander, as well as computing expander decompositions, hierarchical cut decompositions, oblivious routings, multi-cuts, and multi-commodity flows.
  The cut-matching game of this paper is crucial in extending this versatile and powerful machinery to constant-hop and length-constrained expanders and has been already been extensively used. For example, as a key ingredient in several recent breakthroughs, including, computing constant-approximate $k$-commodity (min-cost) flows in $(m+k)^{1+\epsilon}$ time as well as the optimal constant-approximate deterministic worst-case fully-dynamic APSP-distance oracle - in all applications the constant-approximation factor directly traces to and crucially relies on the expanders from a cut-matching game guaranteeing constant-hop routing paths.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.11726v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>math.CO</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bernhard Haeupler (r), Jonas Huebotter (r), Mohsen Ghaffari</dc:creator>
    </item>
    <item>
      <title>FedECA: A Federated External Control Arm Method for Causal Inference with Time-To-Event Data in Distributed Settings</title>
      <link>https://arxiv.org/abs/2311.16984</link>
      <description>arXiv:2311.16984v5 Announce Type: replace-cross 
Abstract: External control arms (ECA) can inform the early clinical development of experimental drugs and provide efficacy evidence for regulatory approval. However, the main challenge in implementing ECA lies in accessing real-world or historical clinical trials data. Indeed, regulations protecting patients' rights by strictly controlling data processing make pooling data from multiple sources in a central server often difficult. To address these limitations, we develop a new method, 'FedECA' that leverages federated learning (FL) to enable inverse probability of treatment weighting (IPTW) for time-to-event outcomes on separate cohorts without needing to pool data. To showcase the potential of FedECA, we apply it in different settings of increasing complexity culminating with a real-world use-case in which FedECA provides evidence for a differential effect between two drugs that would have otherwise gone unnoticed. By sharing our code, we hope FedECA will foster the creation of federated research networks and thus accelerate drug development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.16984v5</guid>
      <category>stat.ME</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean Ogier du Terrail, Quentin Klopfenstein, Honghao Li, Imke Mayer, Nicolas Loiseau, Mohammad Hallal, Michael Debouver, Thibault Camalon, Thibault Fouqueray, Jorge Arellano Castro, Zahia Yanes, Laetitia Dahan, Julien Ta\"ieb, Pierre Laurent-Puig, Jean-Baptiste Bachet, Shulin Zhao, Remy Nicolle, J\'erome Cros, Daniel Gonzalez, Robert Carreras-Torres, Adelaida Garcia Velasco, Kawther Abdilleh, Sudheer Doss, F\'elix Balazard, Mathieu Andreux</dc:creator>
    </item>
    <item>
      <title>PyGim: An Efficient Graph Neural Network Library for Real Processing-In-Memory Architectures</title>
      <link>https://arxiv.org/abs/2402.16731</link>
      <description>arXiv:2402.16731v5 Announce Type: replace-cross 
Abstract: Graph Neural Networks (GNNs) are emerging ML models to analyze graph-structure data. Graph Neural Network (GNN) execution involves both compute-intensive and memory-intensive kernels, the latter dominates the total time, being significantly bottlenecked by data movement between memory and processors. Processing-In-Memory (PIM) systems can alleviate this data movement bottleneck by placing simple processors near or inside to memory arrays. In this work, we introduce PyGim, an efficient ML library that accelerates GNNs on real PIM systems. We propose intelligent parallelization techniques for memory-intensive kernels of GNNs tailored for real PIM systems, and develop handy Python API for them. We provide hybrid GNN execution, in which the compute-intensive and memory-intensive kernels are executed in processor-centric and memory-centric computing systems, respectively. We extensively evaluate PyGim on a real-world PIM system with 1992 PIM cores using emerging GNN models, and demonstrate that it outperforms its state-of-the-art CPU counterpart on Intel Xeon by on average 3.04x, and achieves higher resource utilization than CPU and GPU systems. Our work provides useful recommendations for software, system and hardware designers. PyGim is publicly available at https://github.com/CMU-SAFARI/PyGim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16731v5</guid>
      <category>cs.AR</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.PF</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christina Giannoula, Peiming Yang, Ivan Fernandez Vega, Jiacheng Yang, Sankeerth Durvasula, Yu Xin Li, Mohammad Sadrosadati, Juan Gomez Luna, Onur Mutlu, Gennady Pekhimenko</dc:creator>
    </item>
    <item>
      <title>FACT or Fiction: Can Truthful Mechanisms Eliminate Federated Free Riding?</title>
      <link>https://arxiv.org/abs/2405.13879</link>
      <description>arXiv:2405.13879v2 Announce Type: replace-cross 
Abstract: Standard federated learning (FL) approaches are vulnerable to the free-rider dilemma: participating agents can contribute little to nothing yet receive a well-trained aggregated model. While prior mechanisms attempt to solve the free-rider dilemma, none have addressed the issue of truthfulness. In practice, adversarial agents can provide false information to the server in order to cheat its way out of contributing to federated training. In an effort to make free-riding-averse federated mechanisms truthful, and consequently less prone to breaking down in practice, we propose FACT. FACT is the first federated mechanism that: (1) eliminates federated free riding by using a penalty system, (2) ensures agents provide truthful information by creating a competitive environment, and (3) encourages agent participation by offering better performance than training alone. Empirically, FACT avoids free-riding when agents are untruthful, and reduces agent loss by over 4x.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13879v2</guid>
      <category>cs.GT</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>econ.TH</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Bornstein, Amrit Singh Bedi, Abdirisak Mohamed, Furong Huang</dc:creator>
    </item>
    <item>
      <title>Towards Efficient and Scalable Training of Differentially Private Deep Learning</title>
      <link>https://arxiv.org/abs/2406.17298</link>
      <description>arXiv:2406.17298v2 Announce Type: replace-cross 
Abstract: Differentially private stochastic gradient descent (DP-SGD) is the standard algorithm for training machine learning models under differential privacy (DP). The most common DP-SGD privacy accountants rely on Poisson subsampling for ensuring the theoretical DP guarantees. Implementing computationally efficient DP-SGD with Poisson subsampling is not trivial, which leads to many implementations ignoring this requirement. We conduct a comprehensive empirical study to quantify the computational cost of training deep learning models under DP given the requirement of Poisson subsampling, by re-implementing efficient methods using Poisson subsampling and benchmarking them. We find that using the naive implementation DP-SGD with Opacus in PyTorch has between 2.6 and 8 times lower throughput of processed training examples per second than SGD. However, efficient gradient clipping implementations with e.g. Ghost Clipping can roughly halve this cost. We propose alternative computationally efficient ways of implementing DP-SGD with JAX that are using Poisson subsampling and achieve only around 1.2 times lower throughput than SGD based on PyTorch. We highlight important implementation considerations with JAX. Finally, we study the scaling behaviour using up to 80 GPUs and find that DP-SGD scales better than SGD. We share our re-implementations using Poisson subsampling at https://github.com/DPBayes/Towards-Efficient-Scalable-Training-DP-DL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17298v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Rodriguez Beltran, Marlon Tobaben, Joonas J\"alk\"o, Niki Loppi, Antti Honkela</dc:creator>
    </item>
    <item>
      <title>CONGO: Compressive Online Gradient Optimization</title>
      <link>https://arxiv.org/abs/2407.06325</link>
      <description>arXiv:2407.06325v2 Announce Type: replace-cross 
Abstract: We address the challenge of zeroth-order online convex optimization where the objective function's gradient exhibits sparsity, indicating that only a small number of dimensions possess non-zero gradients. Our aim is to leverage this sparsity to obtain useful estimates of the objective function's gradient even when the only information available is a limited number of function samples. Our motivation stems from the optimization of large-scale queueing networks that process time-sensitive jobs. Here, a job must be processed by potentially many queues in sequence to produce an output, and the service time at any queue is a function of the resources allocated to that queue. Since resources are costly, the end-to-end latency for jobs must be balanced with the overall cost of the resources used. While the number of queues is substantial, the latency function primarily reacts to resource changes in only a few, rendering the gradient sparse. We tackle this problem by introducing the Compressive Online Gradient Optimization framework which allows compressive sensing methods previously applied to stochastic optimization to achieve regret bounds with an optimal dependence on the time horizon without the full problem dimension appearing in the bound. For specific algorithms, we reduce the samples required per gradient estimate to scale with the gradient's sparsity factor rather than its full dimensionality. Numerical simulations and real-world microservices benchmarks demonstrate CONGO's superiority over gradient descent approaches that do not account for sparsity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06325v2</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>math.OC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jeremy Carleton, Prathik Vijaykumar, Divyanshu Saxena, Dheeraj Narasimha, Srinivas Shakkottai, Aditya Akella</dc:creator>
    </item>
    <item>
      <title>Parallel Cluster-BFS and Applications to Shortest Paths</title>
      <link>https://arxiv.org/abs/2410.17226</link>
      <description>arXiv:2410.17226v2 Announce Type: replace-cross 
Abstract: Breadth-first Search (BFS) is one of the most important graph processing subroutines, especially for computing the unweighted distance. Many applications may require running BFS from multiple sources. Sequentially, when running BFS on a cluster of nearby vertices, a known optimization is using bit-parallelism. Given a subset of vertices with size $k$ and the distance between any pair of them is no more than $d$, BFS can be applied to all of them in total work $O(dm(k/w+1))$, where $w$ is the length of a word in bits and $m$ is the number of edges. We will refer to this approach as cluster-BFS (C-BFS). Such an approach has been studied and shown effective both in theory and in practice in the sequential setting. However, it remains unknown how this can be combined with thread-level parallelism.
  In this paper, we focus on designing efficient parallel C-BFS based on BFS to answer unweighted distance queries. Our solution combines the strengths of bit-level parallelism and thread-level parallelism, and achieves significant speedup over the plain sequential solution. We also apply our algorithm to real-world applications. In particular, we identified another application (landmark-labeling for the approximate distance oracle) that can take advantage of parallel C-BFS. Under the same memory budget, our new solution improves accuracy and/or time on all the 18 tested graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17226v2</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Tue, 29 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Letong Wang, Guy Blelloch, Yan Gu, Yihan Sun</dc:creator>
    </item>
  </channel>
</rss>
