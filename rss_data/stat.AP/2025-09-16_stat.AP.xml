<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Sep 2025 01:39:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Estimating Global HIV Prevalence in Key Populations: A Cross-Population Hierarchical Modeling Approach</title>
      <link>https://arxiv.org/abs/2509.10664</link>
      <description>arXiv:2509.10664v1 Announce Type: new 
Abstract: Key populations at high risk of HIV infection are critical for understanding and monitoring HIV epidemics, but global estimation is hampered by sparse, uneven data. We analyze data from 199 countries for female sex workers (FSW), men who have sex with men (MSM), and people who inject drugs (PWID) over 2011-2021, and introduce a cross-population hierarchical model that borrows strength across countries, years, and populations. The model combines region- and population-specific means with country random effects, temporal dependence, and cross-population correlations in a Gaussian Markov random-field formulation on the log-prevalence scale. In 5-fold cross-validation, the approach outperforms a regional-median baseline and reduced variants (65 percent reduction in cross-validated MSE) with well-calibrated posterior predictive coverage (93 percent). We map the 2021 prevalence and quantify the change between 2011 and 2021 using posterior prevalence ratios to identify countries with substantial increases or decreases. The framework yields globally comparable and uncertainty-quantified country-by-year prevalence estimates, enhancing evidence for resource allocation and targeted interventions for marginalized populations where routine data are limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10664v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahao Zhang, Keith Sabin, Le Bao</dc:creator>
    </item>
    <item>
      <title>Modelling Under-Reported Data: Pitfalls of Na\"ive Approaches and a New Statistical Framework for Epidemic Curve Reconstruction</title>
      <link>https://arxiv.org/abs/2509.10668</link>
      <description>arXiv:2509.10668v1 Announce Type: new 
Abstract: Count-valued autoregressions are widely used to analyse time-series of reported infectious-disease cases because of their close connection with discrete-time transmission models. However, when such models are applied directly to under-reported case counts, their mechanistic interpretation can break down. We establish new theoretical results quantifying the consequences of ignoring under-reporting in these models. To address this issue, reported cases are often modelled as a binomially thinned version of an underlying count process, but such models are difficult to fit because the unobserved true counts are serially correlated and integer-valued. We develop a new statistical framework for under-reported infectious-disease data that uses a normal-normal approximation to a broad class of thinned count autoregressions and then accurately maps this continuous process back to the integers. Through simulations and applications to rotavirus incidence in a German state and Covid-19 incidence in English conurbations, we demonstrate that our approach both retains the mechanistic appeal of thinned autoregressions and substantially simplifies inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10668v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin J. Slater, Sindi Bebeziqi</dc:creator>
    </item>
    <item>
      <title>Using Drift Diffusion Model to Analyze Cars' Lane Change Decisions behind Heavy Vehicles</title>
      <link>https://arxiv.org/abs/2509.10733</link>
      <description>arXiv:2509.10733v1 Announce Type: new 
Abstract: Heavy vehicles (HVs) pose a significant challenge to maintaining a smooth traffic flow on the freeway because they are slower moving and create large blind spots. It is therefore desirable for the followers of HVs to perform lane changes (LCs) to achieve a higher speed and a safer driving environment. Understanding LC behaviors of vehicles behind HVs is important because LCs can lead to highway capacity drop and induce safety risks. In this paper, a drift-diffusion model (DDM) is proposed to model the LC behavior of cars behind HVs. In this drift-diffusion (DD) process, vehicles consider the surrounding traffic environment and accumulate evidence over time. A LC is made if the evidence threshold is exceeded. By obtaining vehicle trajectories with LC intentions in the Third Generation Simulation (TGSIM) dataset through clustering and fitting them with the DDM, we find that a lower initial headway makes the drivers more likely to LC. Furthermore, a larger distance to the follower on the target lane, an increasing target gap size, and a higher speed difference between the target lane and the leading HV increases the rate of evidence accumulation and leads to a LC execution sooner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10733v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nachuan Li, Hani S. Mahmassani, Soyoung Ahn, Anupam Srivastava</dc:creator>
    </item>
    <item>
      <title>Adaptive Bayesian computation for efficient biobank-scale genomic inference</title>
      <link>https://arxiv.org/abs/2509.10736</link>
      <description>arXiv:2509.10736v1 Announce Type: new 
Abstract: Motivation: Modern biobanks, with unprecedented sample sizes and phenotypic diversity, have become foundational resources for genomic studies, enabling powerful cross-phenotype and population-scale analyses. As studies grow in complexity, Bayesian hierarchical models offer a principled framework for jointly modeling multiple units such as cells, traits, and experimental conditions, increasing statistical power through information sharing. However, adoption of Bayesian hierarchical models in biobank-scale studies remains limited due to computational inefficiencies, particularly in posterior inference over high-dimensional parameter spaces. Deterministic approximations such as variational inference provide scalable alternatives to Markov Chain Monte Carlo, yet current implementations do not fully exploit the structure of genome-wide multi-unit modeling, especially when biological effects of interest are concentrated in a few units.
  Results: We propose an adaptive focus (AF) strategy within a block coordinate ascent variational inference (CAVI) framework that selectively updates subsets of parameters at each iteration, corresponding to units deemed relevant based on current estimates. We illustrate this approach in protein quantitative trait locus (pQTL) mapping using a joint model of hierarchically linked regressions with shared parameters across traits. In both simulated data and real proteomic data from the UK Biobank, AF-CAVI achieves up to a 50\% reduction in runtime while maintaining statistical performance. We also provide a genome-wide pipeline for multi-trait pQTL mapping across thousands of traits, demonstrating AF-CAVI as an efficient scheme for large-scale, multi-unit Bayesian analysis in biobanks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10736v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiran Li, John Whittaker, Sylvia Richardson, Helene Ruffieux</dc:creator>
    </item>
    <item>
      <title>What is in a Price? Estimating Willingness-to-Pay with Bayesian Hierarchical Models</title>
      <link>https://arxiv.org/abs/2509.11089</link>
      <description>arXiv:2509.11089v1 Announce Type: new 
Abstract: For premium consumer products, pricing strategy is not about a single number, but about understanding the perceived monetary value of the features that justify a higher cost. This paper proposes a robust methodology to deconstruct a product's price into the tangible value of its constituent parts. We employ Bayesian Hierarchical Conjoint Analysis, a sophisticated statistical technique, to solve this high-stakes business problem using the Apple iPhone as a universally recognizable case study. We first simulate a realistic choice based conjoint survey where consumers choose between different hypothetical iPhone configurations. We then develop a Bayesian Hierarchical Logit Model to infer consumer preferences from this choice data. The core innovation of our model is its ability to directly estimate the Willingness-to-Pay (WTP) in dollars for specific feature upgrades, such as a "Pro" camera system or increased storage. Our results demonstrate that the model successfully recovers the true, underlying feature valuations from noisy data, providing not just a point estimate but a full posterior probability distribution for the dollar value of each feature. This work provides a powerful, practical framework for data-driven product design and pricing strategy, enabling businesses to make more intelligent decisions about which features to build and how to price them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11089v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srijesh Pillai, Rajesh Kumar Chandrawat</dc:creator>
    </item>
    <item>
      <title>Time-varying Vine Copula model based on R-Vine structure and its application in financial risk research</title>
      <link>https://arxiv.org/abs/2509.11192</link>
      <description>arXiv:2509.11192v1 Announce Type: new 
Abstract: The time-varying Vine Copula model has become a new direction in the Vine Copula class of models due to its time-varying structural parameters. We have observed that the Vine structures of the time-varying Vine Copula model currently used in economics and business research are C-Vine and D-Vine. These two structures are simpler than the R-Vine structure in modeling but will lose more details. Although truncation and simplification of the Vine structure are necessary when the number of variables is large, the number of variables in economics and business research is often small. Therefore, the R-Vine structure is definitely more suitable for constructing time-varying Vine Copula for economic research. Therefore, this paper uses the GAS (Generalized Autoregressive Score) model to dynamically parameterize the R-Vine structure to construct a time-varying Vine Copula model. The application of this model to the study of liquidity risks between China and Southeast Asian countries, including during the pandemic period, reveals that the time-varying model based on the R-Vine structure not only achieves better statistical test results but also better reflects economic and even political realities compared to the other two structural time-varying models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11192v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>XueZeng Yu</dc:creator>
    </item>
    <item>
      <title>Association between Air Pollutants and Hospitalizations for Cardiovascular Diseases: Time-Series Analysis in S\~ao Paulo, 2010-2019</title>
      <link>https://arxiv.org/abs/2509.11546</link>
      <description>arXiv:2509.11546v1 Announce Type: new 
Abstract: Cardiovascular diseases (CVD) remain one of the leading causes of hospitalization in Brazil. Exposure to air pollutants such as PM$_{10}$ $\mu$m, NO$_2$, and SO$_2$ has been associated with the worsening of these diseases, especially in urban areas. This study evaluated the association between the daily concentration of these pollutants and daily hospitalizations for acute myocardial infarction and cerebrovascular diseases in S\~ao Paulo (2010-2019), using generalized additive models with a lag of 0 to 4 days. Two approaches for choosing the degrees of freedom in temporal smoothing were compared: based on pollutant prediction and based on outcome prediction (hospitalizations). Data were obtained from official government databases. The modeling used the quasi-Poisson family in R software (v. 4.4.0). Models with exposure-based smoothing generated more consistent estimates. For PM10{\mu}m, the cumulative risk estimate for exposure was 1.08%, while for hospitalization, it was 1.20%. For NO$_2$, the estimated risk was 1.47% (exposure) versus 1.33% (hospitalization). For SO$_2$, a striking difference was observed: 7.66% (exposure) versus 14.31% (hospitalization). The significant lags were on days 0, 1, and 2. The results show that smoothing based on outcome prediction can generate bias, masking the true effect of pollutants. The appropriate choice of df in the smoothing function is crucial. Smoothing by the pollutant series was more robust and accurate, contributing to methodological improvements in time-series studies and reinforcing the importance of public policies for pollution control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11546v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Souto dos Santos Filho (S\~ao Paulo State University), Ana J\'ulia Alves C\^amara (Federal University of Esp\'irito Santo), Guilherme Aparecido Santos Aguilar (S\~ao Paulo State University)</dc:creator>
    </item>
    <item>
      <title>Wavelet-SARIMA-Transformer: A Hybrid Model for Rainfall Forecasting</title>
      <link>https://arxiv.org/abs/2509.11903</link>
      <description>arXiv:2509.11903v1 Announce Type: new 
Abstract: This study develops and evaluates a novel hybridWavelet SARIMA Transformer, WST framework to forecast using monthly rainfall across five meteorological subdivisions of Northeast India over the 1971 to 2023 period. The approach employs the Maximal Overlap Discrete Wavelet Transform, MODWT with four wavelet families such as, Haar, Daubechies, Symlet, Coiflet etc. to achieve shift invariant, multiresolution decomposition of the rainfall series. Linear and seasonal components are modeled using Seasonal ARIMA, SARIMA, while nonlinear components are modeled by a Transformer network, and forecasts are reconstructed via inverse MODWT. Comprehensive validation using an 80 is to 20 train test split and multiple performance indices such as, RMSE, MAE, SMAPE, Willmotts d, Skill Score, Percent Bias, Explained Variance, and Legates McCabes E1 demonstrates the superiority of the Haar-based hybrid model, WHST. Across all subdivisions, WHST consistently achieved lower forecast errors, stronger agreement with observed rainfall, and unbiased predictions compared with stand alone SARIMA, stand-alone Transformer, and two-stage wavelet hybrids. Residual adequacy was confirmed through the Ljung Box test, while Taylor diagrams provided an integrated assessment of correlation, variance fidelity, and RMSE, further reinforcing the robustness of the proposed approach. The results highlight the effectiveness of integrating multiresolution signal decomposition with complementary linear and deep learning models for hydroclimatic forecasting. Beyond rainfall, the proposed WST framework offers a scalable methodology for forecasting complex environmental time series, with direct implications for flood risk management, water resources planning, and climate adaptation strategies in data-sparse and climate-sensitive regions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11903v1</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Junmoni Saikia, Kuldeep Goswami, Sarat C. Kakaty</dc:creator>
    </item>
    <item>
      <title>A comparison between geostatistical and machine learning models for spatio-temporal prediction of PM2.5 data</title>
      <link>https://arxiv.org/abs/2509.12051</link>
      <description>arXiv:2509.12051v1 Announce Type: new 
Abstract: Ambient air pollution poses significant health and environmental challenges. Exposure to high concentrations of PM$_{2.5}$ have been linked to increased respiratory and cardiovascular hospital admissions, more emergency department visits and deaths. Traditional air quality monitoring systems such as EPA-certified stations provide limited spatial and temporal data. The advent of low-cost sensors has dramatically improved the granularity of air quality data, enabling real-time, high-resolution monitoring. This study exploits the extensive data from PurpleAir sensors to assess and compare the effectiveness of various statistical and machine learning models in producing accurate hourly PM$_{2.5}$ maps across California. We evaluate traditional geostatistical methods, including kriging and land use regression, against advanced machine learning approaches such as neural networks, random forests, and support vector machines, as well as ensemble model. Our findings enhanced the predictive accuracy of PM2.5 concentration by correcting the bias in PurpleAir data with an ensemble model, which incorporating both spatiotemporal dependencies and machine learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12051v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zeinab Mohamed, Wenlong Gong</dc:creator>
    </item>
    <item>
      <title>Evidencing preferential attachment in dependency network evolution</title>
      <link>https://arxiv.org/abs/2509.12135</link>
      <description>arXiv:2509.12135v1 Announce Type: new 
Abstract: Preferential attachment is often suggested to be the underlying mechanism of the growth of a network, largely due to that many real networks are, to a certain extent, scale-free. However, such attribution is usually made under debatable practices of determining scale-freeness and when only snapshots of the degree distribution are observed. In the presence of the evolution history of the network, modelling the increments of the evolution allows us to measure preferential attachment directly. Therefore, we propose a generalised linear model for such purpose, where the in-degrees and their increments are the covariate and response, respectively. Not only are the parameters that describe the preferential attachment directly incorporated, they also ensure that the tail heaviness of the asymptotic degree distribution is realistic. The Bayesian approach to inference enables the hierarchical version of the model to be implemented naturally. The application to the dependency network of R packages reveals subtly different behaviours between new dependencies by new and existing packages, and between addition and removal of dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12135v1</guid>
      <category>stat.AP</category>
      <category>cs.SI</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Clement Lee</dc:creator>
    </item>
    <item>
      <title>(Un)biased data and spin glasses reveal clustering for Turing phase transitions within human-transformer interactions</title>
      <link>https://arxiv.org/abs/2505.02879</link>
      <description>arXiv:2505.02879v2 Announce Type: cross 
Abstract: This paper studies a Large Language Model's ability to exhibit intelligence equivalent to that of a human by analyzing temperature-induced phase transitions, abrupt changes in the macroscopic behavior of a system, in the Turing test. We utilize three approaches: statistical analysis and bias quantification of a human evaluation survey, information retrieval from real human-written versus AI-generated text data using cosine similarity as a comparison metric, and mathematical spin glass model and simulation. We collect text data in the case study of Flitzing, a tradition of emailing poem-like romantic invitations at Dartmouth College because of its richness in information. Across the three approaches, we obtain consistency in phase transition and clustering results, which also align with literature on the mathematics of transformers and metastability. Our work inspires utilizing spin glass theory for the mathematical foundations of artificial intelligence, especially under environmental stochasticity from human interactions, with justification from real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02879v2</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jackson George, Zachariah Yusaf, Stephanie Zoltick, Linh Huynh</dc:creator>
    </item>
    <item>
      <title>A Tutorial on Conducting Mediation Analysis with Exposure Mixtures</title>
      <link>https://arxiv.org/abs/2509.10916</link>
      <description>arXiv:2509.10916v1 Announce Type: cross 
Abstract: Causal mediation analysis is a powerful tool in environmental health research, allowing researchers to uncover the pathways through which exposures influence health outcomes. While traditional mediation methods have been widely applied to individual exposures, real-world scenarios often involve complex mixtures. Such mixtures introduce unique methodological challenges, including multicollinearity, sparsity of active exposures, and potential nonlinear and interactive effects. This paper provides an overview of several commonly used approaches for mediation analysis under exposure mixture settings with clear strategies and code for implementation. The methods include: single exposure mediation analysis (SE-MA), principal component-based mediation analysis, environmental risk score-based mediation analysis, and Bayesian kernel machine regression causal mediation analysis. While SE-MA serves as a baseline that analyzes each exposure individually, the other methods are designed to address the correlation and complexity inherent in exposure mixtures. For each method, we aim to clarify the target estimand and the assumptions that each method is making to render a causal interpretation of the estimates obtained. We conduct a simulation study to systematically evaluate the operating characteristics of these four methods to estimate global indirect effects and to identify individual exposures contributing to the global mediation under varying sample sizes, effect sizes, and exposure-mediator-outcome structures. We also illustrate their real-world applicability by examining data from the PROTECT birth cohort, specifically analyzing the relationship between prenatal exposure to phthalate mixtures and neonatal head circumference Z-score, with leukotriene E4 as a mediator. This example offers practical guidance for conducting mediation analysis in complex environmental contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.10916v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Wang, Yi-Ting Lin, Sean McGrath, John D. Meeker, Sung Kyun Park, Joshua L. Warren, Bhramar Mukherjee</dc:creator>
    </item>
    <item>
      <title>DemandLens: Enhancing Forecast Accuracy Through Product-Specific Hyperparameter Optimization</title>
      <link>https://arxiv.org/abs/2509.11085</link>
      <description>arXiv:2509.11085v1 Announce Type: cross 
Abstract: DemandLens demonstrates an innovative Prophet based forecasting model for the mattress-in-a-box industry, incorporating COVID-19 metrics and SKU-specific hyperparameter optimization. This industry has seen significant growth of E-commerce players in the recent years, wherein the business model majorly relies on outsourcing Mattress manufacturing and related logistics and supply chain operations, focusing on marketing the product and driving conversions through Direct-to-Consumer sales channels. Now, within the United States, there are a limited number of Mattress contract manufacturers available, and hence, it is important that they manage their raw materials, supply chain, and, inventory intelligently, to be able to cater maximum Mattress brands. Our approach addresses the critical need for accurate Sales Forecasting in an industry that is heavily dependent on third-party Contract Manufacturing. This, in turn, helps the contract manufacturers to be prepared, hence, avoiding bottleneck scenarios, and aiding them to source raw materials at optimal rates. The model demonstrates strong predictive capabilities through SKU-specific Hyperparameter optimization, offering the Contract Manufacturers and Mattress brands a reliable tool to streamline supply chain operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11085v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srijesh Pillai, M. I. Jawid Nazir</dc:creator>
    </item>
    <item>
      <title>A New Class of Mark-Specific Proportional Hazards Models for Recurrent Events: Application to Opioid Refills Among Post-Surgical Patients</title>
      <link>https://arxiv.org/abs/2509.11472</link>
      <description>arXiv:2509.11472v1 Announce Type: cross 
Abstract: Prescription opioids relieve moderate-to-severe pain after surgery, but overprescription can lead to misuse and overdose. Understanding factors associated with post-surgical opioid refills is crucial for improving pain management and reducing opioid-related harms. Conventional methods often fail to account for refill size or dosage and capture patient risk dynamics. We address this gap by treating dosage as a continuously varying mark for each refill event and proposing a new class of mark-specific proportional hazards models for recurrent events. Our marginal model, developed on the gap-time scale with a dual weighting scheme, accommodates event proximity to dosage of interest while accounting for the informative number of recurrences. We establish consistency and asymptotic normality of the estimator and provide a sandwich variance estimator for robust inference. Simulations show improved finite-sample performance over competing methods. We apply the model to data from the Michigan Surgical Quality Collaborative and Michigan Automated Prescription System. Results show that high BMI, smoking, cancer, and open surgery increase hazards of high-dosage refills, while inpatient surgeries elevate refill hazards across all dosages. Black race is associated with higher hazards of low-dosage but lower hazards of high-dosage refills. These findings may inform personalized, dosage-specific pain management strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11472v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eileen Yang, Donglin Zeng, Mark Bicket, Yi Li</dc:creator>
    </item>
    <item>
      <title>Mendelian Randomization Methods for Causal Inference: Estimands, Identification and Inference</title>
      <link>https://arxiv.org/abs/2509.11519</link>
      <description>arXiv:2509.11519v1 Announce Type: cross 
Abstract: Mendelian randomization (MR) has become an essential tool for causal inference in biomedical and public health research. By using genetic variants as instrumental variables, MR helps address unmeasured confounding and reverse causation, offering a quasi-experimental framework to evaluate causal effects of modifiable exposures on health outcomes. Despite its promise, MR faces substantial methodological challenges, including invalid instruments, weak instrument bias, and design complexities across different data structures. In this tutorial review, we provide a comprehensive overview of MR methods for causal inference, emphasizing clarity of causal interpretation, study design comparisons, availability of software tools, and practical guidance for applied scientists. We organize the review around causal estimands, ensuring that analyses are anchored to well-defined causal questions. We discuss the problems of invalid and weak instruments, comparing available strategies for their detection and correction. We integrate discussions of population-based versus family-based MR designs, analyses based on individual-level versus summary-level data, and one-sample versus two-sample MR designs, highlighting their relative advantages and limitations. We also summarize recent methodological advances and software developments that extend MR to settings with many weak or invalid instruments and to modern high-dimensional omics data. Real-data applications, including UK Biobank and Alzheimer's disease proteomics studies, illustrate the use of these methods in practice. This review aims to serve as a tutorial-style reference for both methodologists and applied scientists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11519v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minhao Yao, Anqi Wang, Xihao Li, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>Probabilistic modelling of atmosphere-surface coupling with a copula Bayesian network</title>
      <link>https://arxiv.org/abs/2509.11975</link>
      <description>arXiv:2509.11975v1 Announce Type: cross 
Abstract: Land-atmosphere coupling is an important process for correctly modelling near-surface temperature profiles, but it involves various uncertainties due to subgrid-scale processes, such as turbulent fluxes or unresolved surface heterogeneities, suggesting a probabilistic modelling approach. We develop a copula Bayesian network (CBN) to interpolate temperature profiles, acting as alternative to T2m-diagnostics used in numerical weather prediction (NWP) systems. The new CBN results in (1) a reduction of the warm bias inherent to NWP predictions of wintertime stable boundary layers allowing cold temperature extremes to be better represented, and (2) consideration of uncertainty associated with subgrid-scale spatial variability. The use of CBNs combines the advantages of uncertainty propagation inherent to Bayesian networks with the ability to model complex dependence structures between random variables through copulas. By combining insights from copula modelling and information entropy, criteria for the applicability of CBNs in the further development of parameterizations in NWP models are derived.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.11975v1</guid>
      <category>physics.ao-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Laura Mack, Marvin K\"ahnert, Norbert Pirk</dc:creator>
    </item>
    <item>
      <title>On the universal calibration of Pareto-type linear combination tests</title>
      <link>https://arxiv.org/abs/2509.12066</link>
      <description>arXiv:2509.12066v1 Announce Type: cross 
Abstract: It is often of interest to test a global null hypothesis using multiple, possibly dependent, $p$-values by combining their strengths while controlling the Type I error. Recently, several heavy-tailed combinations tests, such as the harmonic mean test and the Cauchy combination test, have been proposed: they map $p$-values into heavy-tailed random variables before combining them in some fashion into a single test statistic. The resulting tests, which are calibrated under the assumption of independence of the $p$-values, have shown to be rather robust to dependence. The complete understanding of the calibration properties of the resulting combination tests of dependent and possibly tail-dependent $p$-values has remained an important open problem in the area. In this work, we show that the powerful framework of multivariate regular variation (MRV) offers a nearly complete solution to this problem.
  We first show that the precise asymptotic calibration properties of a large class of homogeneous combination tests can be expressed in terms of the angular measure -- a characteristic of the asymptotic tail-dependence under MRV. Consequently, we show that under MRV, the Pareto-type linear combination tests, which are equivalent to the harmonic mean test, are universally calibrated regardless of the tail-dependence structure of the underlying $p$-values. In contrast, the popular Cauchy combination test is shown to be universally honest but often conservative; the Tippet combination test, while being honest, is calibrated if and only if the underlying $p$-values are tail-independent.
  One of our major findings is that the Pareto-type linear combination tests are the only universally calibrated ones among the large family of possibly non-linear homogeneous heavy-tailed combination tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12066v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parijat Chakraborty, F. Richard Guo, Kerby Shedden, Stilian Stoev</dc:creator>
    </item>
    <item>
      <title>Fairness-Aware and Interpretable Policy Learning</title>
      <link>https://arxiv.org/abs/2509.12119</link>
      <description>arXiv:2509.12119v1 Announce Type: cross 
Abstract: Fairness and interpretability play an important role in the adoption of decision-making algorithms across many application domains. These requirements are intended to avoid undesirable group differences and to alleviate concerns related to transparency. This paper proposes a framework that integrates fairness and interpretability into algorithmic decision making by combining data transformation with policy trees, a class of interpretable policy functions. The approach is based on pre-processing the data to remove dependencies between sensitive attributes and decision-relevant features, followed by a tree-based optimization to obtain the policy. Since data pre-processing compromises interpretability, an additional transformation maps the parameters of the resulting tree back to the original feature space. This procedure enhances fairness by yielding policy allocations that are pairwise independent of sensitive attributes, without sacrificing interpretability. Using administrative data from Switzerland to analyze the allocation of unemployed individuals to active labor market programs (ALMP), the framework is shown to perform well in a realistic policy setting. Effects of integrating fairness and interpretability constraints are measured through the change in expected employment outcomes. The results indicate that, for this particular application, fairness can be substantially improved at relatively low cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12119v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nora Bearth, Michael Lechner, Jana Mareckova, Fabian Muny</dc:creator>
    </item>
    <item>
      <title>Pedestrian crash causation analysis near bus stops: Insights from random parameters Negative Binomial-Lindley model</title>
      <link>https://arxiv.org/abs/2410.22253</link>
      <description>arXiv:2410.22253v4 Announce Type: replace 
Abstract: Pedestrian safety remains a pressing concern near bus stops along urban transit, where frequent pedestrian-vehicle interactions occur. While prior research has primarily focused on intersections and midblock locations, bus stops have often been treated as secondary contributors rather than as distinct sites requiring targeted safety assessments. This has left a critical gap in understanding how traffic exposure, roadway characteristics, and bus stop design features specifically influence pedestrian crash risks around bus stop locations. To address these gaps, this study develops a comprehensive framework focused on pedestrian safety in the vicinity of bus stops. The proposed approach employs a Random Parameters Negative Binomial-Lindley (RPNB-L) model to account for unobserved heterogeneity and site-specific variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22253v4</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Srinivas R. Geedipally, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>Learning from geometry-aware near-misses to real-time COR: A spatiotemporal grouped random GEV framework</title>
      <link>https://arxiv.org/abs/2509.02871</link>
      <description>arXiv:2509.02871v2 Announce Type: replace 
Abstract: Real-time prediction of corridor-level crash occurrence risk (COR) remains challenging, as existing near-miss based extreme value models oversimplify collision geometry, exclude vehicle-infrastructure (V-I) interactions, and inadequately capture spatial heterogeneity in vehicle dynamics. This study introduces a geometry-aware two-dimensional time-to-collision (2D-TTC) indicator within a Hierarchical Bayesian spatiotemporal grouped random parameter (HBSGRP) framework using a non-stationary univariate generalized extreme value (UGEV) model to estimate short-term COR in urban corridors. High-resolution trajectories from the Argoverse-2 dataset, covering 28 locations along Miami's Biscayne Boulevard, were analyzed to extract extreme V-V and V-I near misses. The model incorporates dynamic variables and roadway features as covariates, with partial pooling across locations to address unobserved heterogeneity. Results show that the HBSGRP-UGEV framework outperforms fixed-parameter alternatives, reducing DIC by up to 7.5% for V-V and 3.1% for V-I near-misses. Predictive validation using ROC-AUC confirms strong performance: 0.89 for V-V segments, 0.82 for V-V intersections, 0.79 for V-I segments, and 0.75 for V-I intersections. Model interpretation reveals that relative speed and distance dominate V-V risks at intersections and segments, with deceleration critical in segments, while V-I risks are driven by speed, boundary proximity, and steering/heading adjustments. These findings highlight the value of a statistically rigorous, geometry-sensitive, and spatially adaptive modeling approach for proactive corridor-level safety management, supporting real-time interventions and long-term design strategies aligned with Vision Zero.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02871v2</guid>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mohammad Anis, Yang Zhou, Dominique Lord</dc:creator>
    </item>
    <item>
      <title>A Path Signature Framework for Detecting Creative Fatigue in Digital Advertising</title>
      <link>https://arxiv.org/abs/2509.09758</link>
      <description>arXiv:2509.09758v2 Announce Type: replace 
Abstract: This paper introduces a novel framework for detecting advertising creative fatigue using path signature analysis, a geometric approach from rough path theory not previously applied in marketing. Creative fatigue - the decline in advertising effectiveness over time - poses a major risk to digital media spend. Our method transforms performance time-series into geometric paths, extracting high-dimensional signatures to capture complex dynamics: volatility shifts, trend reversals, and non-linear decay. Unlike traditional statistical methods, this approach detects subtle, multi-scale changes. We validate the framework using synthetic datasets replicating documented fatigue patterns from marketing literature. Results show superior early detection, enabling actionable insights before significant budget loss. A novel financial impact model quantifies opportunity costs from delayed detection, while computational analysis confirms linear scalability for real-time monitoring of large creative portfolios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09758v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Shaw</dc:creator>
    </item>
    <item>
      <title>A Flexible Model for Record Linkage</title>
      <link>https://arxiv.org/abs/2407.06835</link>
      <description>arXiv:2407.06835v3 Announce Type: replace-cross 
Abstract: Combining data from various sources empowers researchers to explore innovative questions, for example those raised by conducting healthcare monitoring studies. However, the lack of a unique identifier often poses challenges. Record linkage procedures determine whether pairs of observations collected on different occasions belong to the same individual using partially identifying variables (e.g. birth year, postal code). Existing methodologies typically involve a compromise between computational efficiency and accuracy. Traditional approaches simplify this task by condensing information, yet they neglect dependencies among linkage decisions and disregard the one-to-one relationship required to establish coherent links. Modern approaches offer a comprehensive representation of the data generation process, at the expense of computational overhead and reduced flexibility. We propose a flexible method, that adapts to varying data complexities, addressing registration errors and accommodating changes of the identifying information over time. Our approach balances accuracy and scalability, estimating the linkage using a Stochastic Expectation Maximisation algorithm on a latent variable model. We illustrate the ability of our methodology to connect observations using large real data applications and demonstrate the robustness of our model to the linking variables quality in a simulation study. The proposed algorithm FlexRL is implemented and available in an open source R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06835v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssc/qlaf016</arxiv:DOI>
      <arxiv:journal_reference>A flexible model for record linkage, JRSSSC, Volume 74, Issue 4, November 2025, Pages 1100-1127</arxiv:journal_reference>
      <dc:creator>Kayan\'e Robach, St\'ephanie L van der Pas, Mark A van de Wiel, Michel H Hof</dc:creator>
    </item>
    <item>
      <title>Raking mortality rates across cause, population group and geography with uncertainty quantification</title>
      <link>https://arxiv.org/abs/2407.20520</link>
      <description>arXiv:2407.20520v4 Announce Type: replace-cross 
Abstract: The Global Burden of Diseases, Injuries, and Risk Factors Study (GBD) is the single largest and most detailed scientific effort ever conducted to quantify levels and trends in health. This global health model to estimate mortality rates and other health metrics is run at different scales, leading to large data sets of results for a global region and its different sub-regions, or for a cause of death and different sub-causes for example. These models do not necessarily lead to consistent data tables where, for instance, the sum of the number of deaths for each of the sub-regions is equal to the number of deaths for the global region. Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. The results of global health models usually associate to the point estimates an uncertainty, such as standard deviations or confidence intervals. In this paper, we propose an uncertainty propagation approach that obtains, at the cost of a single solve, nearly the same uncertainty estimates as computationally intensive Monte Carlo techniques that pass thousands of observed and marginal samples through the entire raking process. We introduce a convex optimization approach that provides a unified framework to raking extensions such as uncertainty propagation, raking with differential weights, raking with different loss functions in order to ensure that bounds on estimates are respected, verifying the feasibility of the constraints, raking to margins either as hard constraints or as aggregate observations, and handling missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20520v4</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ariane Ducellier (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA)</dc:creator>
    </item>
    <item>
      <title>Adapting Projection-Based Reduced-Order Models using Projected Gaussian Process</title>
      <link>https://arxiv.org/abs/2410.14090</link>
      <description>arXiv:2410.14090v2 Announce Type: replace-cross 
Abstract: Projection-based model reduction is among the most widely adopted methods for constructing parametric Reduced-Order Models (ROM). Utilizing the snapshot data from solving full-order governing equations, the Proper Orthogonal Decomposition (POD) computes the optimal basis modes that represent the data, and a ROM can be constructed in the low-dimensional vector subspace spanned by the POD basis. For parametric governing equations, a potential challenge arises when there is a need to update the POD basis to adapt ROM that accurately capture the variation of a system's behavior over its parameter space (in design, control, uncertainty quantification, digital twins applications, etc.). In this paper, we propose a Projected Gaussian Process (pGP) and formulate the problem of adapting the POD basis as a supervised statistical learning problem, for which the goal is to learn a mapping from the parameter space to the Grassmann manifold that contains the optimal subspaces. A mapping is firstly established between the Euclidean space and the horizontal space of an orthogonal matrix that spans a reference subspace in the Grassmann manifold. A second mapping from the horizontal space to the Grassmann manifold is established through the Exponential/Logarithm maps between the manifold and its tangent space. Finally, given a new parameter, the conditional distribution of a vector can be found in the Euclidean space using the Gaussian Process (GP) regression, and such a distribution is then projected to the Grassmann manifold that enables us to predict the optimal subspace for the new parameter. As a statistical learning approach, the proposed pGP allows us to optimally estimate (or tune) the model parameters from data and quantify the statistical uncertainty associated with the prediction. The advantages of the proposed pGP are demonstrated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.14090v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiao Liu, Jingyi Feng, Xinchao Liu</dc:creator>
    </item>
    <item>
      <title>Modelling the term-structure of default risk under IFRS 9 within a multistate regression framework</title>
      <link>https://arxiv.org/abs/2502.14479</link>
      <description>arXiv:2502.14479v3 Announce Type: replace-cross 
Abstract: The lifetime behaviour of loans is notoriously difficult to model, which can compromise a bank's financial reserves against future losses, if modelled poorly. Therefore, we present a data-driven comparative study amongst three techniques in modelling a series of default risk estimates over the lifetime of each loan, i.e., its term-structure. The behaviour of loans can be described using a nonstationary and time-dependent semi-Markov model, though we model its elements using a multistate regression-based approach. As such, the transition probabilities are explicitly modelled as a function of a rich set of input variables, including macroeconomic and loan-level inputs. Our modelling techniques are deliberately chosen in ascending order of complexity: 1) a Markov chain; 2) beta regression; and 3) multinomial logistic regression. Using residential mortgage data, our results show that each successive model outperforms the previous, likely as a result of greater sophistication. This finding required devising a novel suite of simple model diagnostics, which can itself be reused in assessing sampling representativeness and the performance of other modelling techniques. These contributions surely advance the current practice within banking when conducting multistate modelling. Consequently, we believe that the estimation of loss reserves will be more timeous and accurate under IFRS 9.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.14479v3</guid>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster, Roland Breedt</dc:creator>
    </item>
    <item>
      <title>Intellectual Up-streams of Percentage Scale ($ps$) and Percentage Coefficient ($b_p$) -- Effect Size Analysis (Theory Paper 2)</title>
      <link>https://arxiv.org/abs/2507.13695</link>
      <description>arXiv:2507.13695v2 Announce Type: replace-cross 
Abstract: Percentage thinking, i.e., assessing quantities as parts per hundred, spread from Roman tax ledgers to modern algorithms. Building on Simon Stevin's La Thiende (1585) and the 19th-century metrication that institutionalized base-10 measurement (Cajori, 1925), this article traces how base-10 normalization, especially the 0-1 percentage scale, became a shared language for human and machine understanding. We retrace 1980s efforts at UW-Madison and UNC Chapel Hill to "percentize" variables to make regression coefficients interpretable, and relate these experiments to established indices, notably the Pearson (1895) correlation r (range -1 to 1) and the coefficient of determination r-squared (Wright, 1920). We also revisit Cohen et al.'s (1999) percent of maximum possible (POMP) metric. The lineage of 0-100 and 0-1 scales includes Roman fiscal practice, early American grading at Yale and Harvard, and recurring analyses of percent (0-100) and percentage (0-1, or -1 to 1) scales that repeatedly reinvent the same indices (Durm, 1993; Schneider and Hutt, 2014). In data mining and machine learning, min-max normalization maps any feature to [0, 1] (i.e., 0-100%), equalizing scale ranges and implied units across percentized variables, which improves comparability of predictors. Under the percentage theory of measurement indices, equality of units is the necessary and sufficient condition for comparing indices (Cohen et al., 1999; Zhao et al., 2024; Zhao and Zhang, 2014). Seen this way, the successes of machine learning and artificial intelligence over the past half century constitute large-scale evidence for the comparability of percentage-based indices, foremost the percentage coefficient (bp).</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.13695v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinshu Zhao, Qinru Ruby Ju, Piper Liping Liu, Dianshi Moses Li, Luxi Zhang, Jizhou Francis Ye, Song Harris Ao, Ming Milano Li</dc:creator>
    </item>
    <item>
      <title>Binomial maps: stochastically evolving iterated integer maps for finite populations</title>
      <link>https://arxiv.org/abs/2508.11974</link>
      <description>arXiv:2508.11974v2 Announce Type: replace-cross 
Abstract: Many models of population dynamics are formulated as deterministic iterated maps although real populations are stochastic. This is justifiable in the limit of large population sizes, as the stochastic fluctuations are negligible then. However, this also makes it challenging to use the same models for small populations where finite size effects like demographic noise and extinction cannot be ignored. Moreover, adding noise to the equations does not solve this problem as it can only represent the environmental stochasticity. An approach, sometimes used in ecological literature, but surprisingly uncommon in dynamical systems community, is \emph{Binomial maps}, which allow stochastic evolution of deterministic iterated map models of population. Here we present their formulation in a way so as to make their connection to the agent-based models explicit, and demonstrate it for the Logistic and Ricker maps. We also show that the Binomial maps are not completely equivalent to their deterministic counterparts, and derive sufficient conditions under which the equivalence holds. This approach enables rigorous finite-population analysis within familiar map-based models, bridging the deterministic map models and stochastic agent-based models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11974v2</guid>
      <category>q-bio.PE</category>
      <category>nlin.CD</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Snehal M. Shekatkar</dc:creator>
    </item>
    <item>
      <title>Non-Linear Model-Based Sequential Decision-Making in Agriculture</title>
      <link>https://arxiv.org/abs/2509.01924</link>
      <description>arXiv:2509.01924v2 Announce Type: replace-cross 
Abstract: Sequential decision-making is central to sustainable agricultural management and precision agriculture, where resource inputs must be optimized under uncertainty and over time. However, such decisions must often be made with limited observations, whereas classical bandit and reinforcement learning approaches typically rely on either linear or black-box reward models that may misrepresent domain knowledge or require large amounts of data. We propose a family of \emph{nonlinear, model-based bandit algorithms} that embed domain-specific response curves directly into the exploration-exploitation loop. By coupling (i) principled uncertainty quantification with (ii) closed-form or rapidly computable profit optima, these algorithms achieve sublinear regret and near-optimal sample complexity while preserving interpretability. Theoretical analysis establishes regret and sample complexity bounds, and extensive simulations emulating real-world fertilizer-rate decisions show consistent improvements over both linear and nonparametric baselines (such as linear UCB and $k$-NN UCB) in the low-sample regime, under both well-specified and shape-compatible misspecified models. Because our approach leverages mechanistic insight rather than large data volumes, it is especially suited to resource-constrained settings, supporting sustainable, inclusive, and transparent sequential decision-making across agriculture, environmental management, and allied applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01924v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Wentao Lin</dc:creator>
    </item>
    <item>
      <title>Quantifying model prediction sensitivity to model-form uncertainty</title>
      <link>https://arxiv.org/abs/2509.08708</link>
      <description>arXiv:2509.08708v2 Announce Type: replace-cross 
Abstract: Model-form uncertainty (MFU) in assumptions made during physics-based model development is widely considered a significant source of uncertainty; however, there are limited approaches that can quantify MFU in predictions extrapolating beyond available data. As a result, it is challenging to know how important MFU is in practice, especially relative to other sources of uncertainty in a model, making it difficult to prioritize resources and efforts to drive down error in model predictions. To address these challenges, we present a novel method to quantify the importance of uncertainties associated with model assumptions. We combine parameterized modifications to assumptions (called MFU representations) with grouped variance-based sensitivity analysis to measure the importance of assumptions. We demonstrate how, in contrast to existing methods addressing MFU, our approach can be applied without access to calibration data. However, if calibration data is available, we demonstrate how it can be used to inform the MFU representation, and how variance-based sensitivity analysis can be meaningfully applied even in the presence of dependence between parameters (a common byproduct of calibration).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08708v2</guid>
      <category>cs.CE</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Portone, Rebekah D. White, Joseph L. Hart</dc:creator>
    </item>
    <item>
      <title>Scalable extensions to given-data Sobol' index estimators</title>
      <link>https://arxiv.org/abs/2509.09078</link>
      <description>arXiv:2509.09078v2 Announce Type: replace-cross 
Abstract: Given-data methods for variance-based sensitivity analysis have significantly advanced the feasibility of Sobol' index computation for computationally expensive models and models with many inputs. However, the limitations of existing methods still preclude their application to models with an extremely large number of inputs. In this work, we present practical extensions to the existing given-data Sobol' index method, which allow variance-based sensitivity analysis to be efficiently performed on large models such as neural networks, which have $&gt;10^4$ parameterizable inputs. For models of this size, holding all input-output evaluations simultaneously in memory -- as required by existing methods -- can quickly become impractical. These extensions also support nonstandard input distributions with many repeated values, which are not amenable to equiprobable partitions employed by existing given-data methods.
  Our extensions include a general definition of the given-data Sobol' index estimator with arbitrary partition, a streaming algorithm to process input-output samples in batches, and a heuristic to filter out small indices that are indistinguishable from zero indices due to statistical noise. We show that the equiprobable partition employed in existing given-data methods can introduce significant bias into Sobol' index estimates even at large sample sizes and provide numerical analyses that demonstrate why this can occur. We also show that our streaming algorithm can achieve comparable accuracy and runtimes with lower memory requirements, relative to current methods which process all samples at once. We demonstrate our novel developments on two application problems in neural network modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.09078v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Portone, Bert Debusschere, Samantha Yang, Emiliano Islas-Quinones, T. Patrick Xiao</dc:creator>
    </item>
  </channel>
</rss>
