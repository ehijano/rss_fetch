<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 04:00:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PrISM-Observer: Intervention Agent to Help Users Perform Everyday Procedures Sensed using a Smartwatch</title>
      <link>https://arxiv.org/abs/2407.16785</link>
      <description>arXiv:2407.16785v1 Announce Type: new 
Abstract: We routinely perform procedures (such as cooking) that include a set of atomic steps. Often, inadvertent omission or misordering of a single step can lead to serious consequences, especially for those experiencing cognitive challenges such as dementia. This paper introduces PrISM-Observer, a smartwatch-based, context-aware, real-time intervention system designed to support daily tasks by preventing errors. Unlike traditional systems that require users to seek out information, the agent observes user actions and intervenes proactively. This capability is enabled by the agent's ability to continuously update its belief in the user's behavior in real-time through multimodal sensing and forecast optimal intervention moments and methods. We first validated the steps-tracking performance of our framework through evaluations across three datasets with different complexities. Then, we implemented a real-time agent system using a smartwatch and conducted a user study in a cooking task scenario. The system generated helpful interventions, and we gained positive feedback from the participants. The general applicability of PrISM-Observer to daily tasks promises broad applications, for instance, including support for users requiring more involved interventions, such as people with dementia or post-surgical patients.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16785v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Riku Arakawa, Hiromu Yakura, Mayank Goel</dc:creator>
    </item>
    <item>
      <title>TAMIGO: Empowering Teaching Assistants using LLM-assisted viva and code assessment in an Advanced Computing Class</title>
      <link>https://arxiv.org/abs/2407.16805</link>
      <description>arXiv:2407.16805v1 Announce Type: new 
Abstract: Large Language Models (LLMs) have significantly transformed the educational landscape, offering new tools for students, instructors, and teaching assistants. This paper investigates the application of LLMs in assisting teaching assistants (TAs) with viva and code assessments in an advanced computing class on distributed systems in an Indian University. We develop TAMIGO, an LLM-based system for TAs to evaluate programming assignments.
  For viva assessment, the TAs generated questions using TAMIGO and circulated these questions to the students for answering. The TAs then used TAMIGO to generate feedback on student answers. For code assessment, the TAs selected specific code blocks from student code submissions and fed it to TAMIGO to generate feedback for these code blocks. The TAMIGO-generated feedback for student answers and code blocks was used by the TAs for further evaluation.
  We evaluate the quality of LLM-generated viva questions, model answers, feedback on viva answers, and feedback on student code submissions. Our results indicate that LLMs are highly effective at generating viva questions when provided with sufficient context and background information. However, the results for LLM-generated feedback on viva answers were mixed; instances of hallucination occasionally reduced the accuracy of feedback. Despite this, the feedback was consistent, constructive, comprehensive, balanced, and did not overwhelm the TAs. Similarly, for code submissions, the LLM-generated feedback was constructive, comprehensive and balanced, though there was room for improvement in aligning the feedback with the instructor-provided rubric for code evaluation. Our findings contribute to understanding the benefits and limitations of integrating LLMs into educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16805v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anishka IIITD, Diksha Sethi, Nipun Gupta, Shikhar Sharma, Srishti Jain, Ujjwal Singhal, Dhruv Kumar</dc:creator>
    </item>
    <item>
      <title>Trust Your Gut: Comparing Human and Machine Inference from Noisy Visualizations</title>
      <link>https://arxiv.org/abs/2407.16871</link>
      <description>arXiv:2407.16871v1 Announce Type: new 
Abstract: People commonly utilize visualizations not only to examine a given dataset, but also to draw generalizable conclusions about the underlying models or phenomena. Prior research has compared human visual inference to that of an optimal Bayesian agent, with deviations from rational analysis viewed as problematic. However, human reliance on non-normative heuristics may prove advantageous in certain circumstances. We investigate scenarios where human intuition might surpass idealized statistical rationality. In two experiments, we examine individuals' accuracy in characterizing the parameters of known data-generating models from bivariate visualizations. Our findings indicate that, although participants generally exhibited lower accuracy compared to statistical models, they frequently outperformed Bayesian agents, particularly when faced with extreme samples. Participants appeared to rely on their internal models to filter out noisy visualizations, thus improving their resilience against spurious data. However, participants displayed overconfidence and struggled with uncertainty estimation. They also exhibited higher variance than statistical machines. Our findings suggest that analyst gut reactions to visualizations may provide an advantage, even when departing from rationality. These results carry implications for designing visual analytics tools, offering new perspectives on how to integrate statistical models and analyst intuition for improved inference and decision-making. The data and materials for this paper are available at https://osf.io/qmfv6</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16871v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ratanond Koonchanok, Michael E. Papka, Khairi Reda</dc:creator>
    </item>
    <item>
      <title>HexTiles and Semantic Icons for MAUP-Aware Multivariate Geospatial Visualizations</title>
      <link>https://arxiv.org/abs/2407.16897</link>
      <description>arXiv:2407.16897v1 Announce Type: new 
Abstract: We introduce HexTiles, a domain-agnostic hexagonal-tiling based visual encoding design for multivariate geospatial data. Multivariate geospatial data have presented a challenge due to the graph schema associated with geospatial maps, on which most geospatial data is presented. With HexTiles, we design a multivariate geospatial visualization design that leverages semantic icons to (1) simplify the process of interpreting interactions between multivariate geospatial data, and (2) put the visualization designer in the driver's seat to guide user attention to specific variables and interactions. Additionally with HexTiles, we attempt to explicitly mitigate effects of the Modifiable Areal Unit Problem (MAUP) for interpreting geospatial data, by proposing a confidence encoding for each of the information channels in HexTiles. We calculate weighted variances of the variables in each HexTile to provide a confidence value for each tile, which can be used to interpret the variability of the data within the corresponding geospatial area, an information that can be lost in geospatial visualizations. To validate our approach, we gather quantitative and qualitative feedback from a user study and document domain expert feedback from ecologists and hydrologists experienced in designing geospatial visualizations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16897v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuya Kawakami, Sarah Yuniar, Kwan-Liu Ma</dc:creator>
    </item>
    <item>
      <title>A Framework for AI assisted Musical Devices</title>
      <link>https://arxiv.org/abs/2407.16899</link>
      <description>arXiv:2407.16899v1 Announce Type: new 
Abstract: In this paper we present a novel framework for the study and design of AI assisted musical devices (AIMEs). Initially, we present a taxonomy of these devices and illustrate it with a set of scenarios and personas. Later, we propose a generic architecture for the implementation of AIMEs and present some examples from the scenarios. We show that the proposed framework and architecture are a valid tool for the study of intelligent musical devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16899v1</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <arxiv:DOI>10.5772/intechopen.108898</arxiv:DOI>
      <arxiv:journal_reference>IntechOpen (2023)</arxiv:journal_reference>
      <dc:creator>Miguel Civit, Luis Munoz Saavedra, Francisco Jose Cuadrado, Maria J. Escalona</dc:creator>
    </item>
    <item>
      <title>How Video Passthrough Headsets Influence Perception of Self and Others</title>
      <link>https://arxiv.org/abs/2407.16904</link>
      <description>arXiv:2407.16904v1 Announce Type: new 
Abstract: With the increasing adoption of mixed reality headsets with video passthrough functionality, concerns over perceptual and social effects have surfaced. Building on prior qualitative findings, this study quantitatively investigates the impact of video passthrough on users. Forty participants completed a body transfer task twice, once while wearing a headset in video passthrough and once without a headset. Results indicate that using video passthrough induces simulator sickness, creates social absence, (another person in the physical room feels less present), alters self-reported body schema, and distorts distance perception. On the other hand, compared to past research which showed perceptual aftereffects from video passthrough, the current study found none. We discuss the broader implications for the widespread adoption of mixed reality headsets and their impact on theories surrounding presence and body transfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16904v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Monique Santoso, Jeremy N. Bailenson</dc:creator>
    </item>
    <item>
      <title>Collaboration Between Robots, Interfaces and Humans: Practice-Based and Audience Perspectives</title>
      <link>https://arxiv.org/abs/2407.16966</link>
      <description>arXiv:2407.16966v1 Announce Type: new 
Abstract: This paper provides an analysis of a mixed-media experimental musical work that explores the integration of human musical interaction with a newly developed interface for the violin, manipulated by an improvising violinist, interactive visuals, a robotic drummer and an improvised synthesised orchestra. We first present a detailed technical overview of the systems involved including the design and functionality of each component. We then conduct a practice-based review examining the creative processes and artistic decisions underpinning the work, focusing on the challenges and breakthroughs encountered during its development. Through this introspective analysis, we uncover insights into the collaborative dynamics between the human performer and technological agents, revealing the complexities of blending traditional musical expressiveness with artificial intelligence and robotics. To gauge public reception and interpretive perspectives, we conducted an online survey, sharing a video of the performance with a diverse audience. The feedback collected from this survey offers valuable viewpoints on the accessibility, emotional impact, and perceived artistic value of the work. Respondents' reactions underscore the transformative potential of integrating advanced technologies in musical performance, while also highlighting areas for further exploration and refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16966v1</guid>
      <category>cs.HC</category>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>International Computer Music Conference 2024, Seoul, South Korea</arxiv:journal_reference>
      <dc:creator>Anna Savery, Richard Savery</dc:creator>
    </item>
    <item>
      <title>LLM-Generated Tips Rival Expert-Created Tips in Helping Students Answer Quantum-Computing Questions</title>
      <link>https://arxiv.org/abs/2407.17024</link>
      <description>arXiv:2407.17024v1 Announce Type: new 
Abstract: Individual teaching is among the most successful ways to impart knowledge. Yet, this method is not always feasible due to large numbers of students per educator. Quantum computing serves as a prime example facing this issue, due to the hype surrounding it. Alleviating high workloads for teachers, often accompanied with individual teaching, is crucial for continuous high quality education. Therefore, leveraging Large Language Models (LLMs) such as GPT-4 to generate educational content can be valuable. We conducted two complementary studies exploring the feasibility of using GPT-4 to automatically generate tips for students. In the first one students (N=46) solved four multiple-choice quantum computing questions with either the help of expert-created or LLM-generated tips. To correct for possible biases towards LLMs, we introduced two additional conditions, making some participants believe that they were given expert-created tips, when they were given LLM-generated tips and vice versa. Our second study (N=23) aimed to directly compare the LLM-generated and expert-created tips, evaluating their quality, correctness and helpfulness, with both experienced educators and students participating. Participants in our second study found that the LLM-generated tips were significantly more helpful and pointed better towards relevant concepts than the expert-created tips, while being more prone to be giving away the answer. While participants in the first study performed significantly better in answering the quantum computing questions when given tips labeled as LLM-generated, even if they were created by an expert. This phenomenon could be a placebo effect induced by the participants' biases for LLM-generated content. Ultimately, we find that LLM-generated tips are good enough to be used instead of expert tips in the context of quantum computing basics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17024v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lars Krupp, Jonas Bley, Isacco Gobbi, Alexander Geng, Sabine M\"uller, Sungho Suh, Ali Moghiseh, Arcesio Castaneda Medina, Valeria Bartsch, Artur Widera, Herwig Ott, Paul Lukowicz, Jakob Karolus, Maximilian Kiefer-Emmanouilidis</dc:creator>
    </item>
    <item>
      <title>NewsUnfold: Creating a News-Reading Application That Indicates Linguistic Media Bias and Collects Feedback</title>
      <link>https://arxiv.org/abs/2407.17045</link>
      <description>arXiv:2407.17045v1 Announce Type: new 
Abstract: Media bias is a multifaceted problem, leading to one-sided views and impacting decision-making. A way to address digital media bias is to detect and indicate it automatically through machine-learning methods. However, such detection is limited due to the difficulty of obtaining reliable training data. Human-in-the-loop-based feedback mechanisms have proven an effective way to facilitate the data-gathering process. Therefore, we introduce and test feedback mechanisms for the media bias domain, which we then implement on NewsUnfold, a news-reading web application to collect reader feedback on machine-generated bias highlights within online news articles. Our approach augments dataset quality by significantly increasing inter-annotator agreement by 26.31% and improving classifier performance by 2.49%. As the first human-in-the-loop application for media bias, the feedback mechanism shows that a user-centric approach to media bias data collection can return reliable data while being scalable and evaluated as easy to use. NewsUnfold demonstrates that feedback mechanisms are a promising strategy to reduce data collection expenses and continuously update datasets to changes in context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17045v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Smi Hinterreiter, Martin Wessel, Fabian Schliski, Isao Echizen, Marc Erich Latoschik, Timo Spinde</dc:creator>
    </item>
    <item>
      <title>AI-Gadget Kit: Integrating Swarm User Interfaces with LLM-driven Agents for Rich Tabletop Game Applications</title>
      <link>https://arxiv.org/abs/2407.17086</link>
      <description>arXiv:2407.17086v1 Announce Type: new 
Abstract: While Swarm User Interfaces (SUIs) have succeeded in enriching tangible interaction experiences, their limitations in autonomous action planning have hindered the potential for personalized and dynamic interaction generation in tabletop games. Based on the AI-Gadget Kit we developed, this paper explores how to integrate LLM-driven agents within tabletop games to enable SUIs to execute complex interaction tasks. After defining the design space of this kit, we elucidate the method for designing agents that can extend the meta-actions of SUIs to complex motion planning. Furthermore, we introduce an add-on prompt method that simplifies the design process for four interaction behaviors and four interaction relationships in tabletop games. Lastly, we present several application scenarios that illustrate the potential of AI-Gadget Kit to construct personalized interaction in SUI tabletop games. We expect to use our work as a case study to inspire research on multi-agent-driven SUI for other scenarios with complex interaction tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17086v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijie Guo, Zhenhan Huang, Ruhan Wang, Zhihao Yao, Tianyu Yu, Zhiling Xu, Xinyu Zhao, Xueqing Li, Haipeng Mi</dc:creator>
    </item>
    <item>
      <title>News Ninja: Gamified Annotation of Linguistic Bias in Online News</title>
      <link>https://arxiv.org/abs/2407.17111</link>
      <description>arXiv:2407.17111v1 Announce Type: new 
Abstract: Recent research shows that visualizing linguistic bias mitigates its negative effects. However, reliable automatic detection methods to generate such visualizations require costly, knowledge-intensive training data. To facilitate data collection for media bias datasets, we present News Ninja, a game employing data-collecting game mechanics to generate a crowdsourced dataset. Before annotating sentences, players are educated on media bias via a tutorial. Our findings show that datasets gathered with crowdsourced workers trained on News Ninja can reach significantly higher inter-annotator agreements than expert and crowdsourced datasets with similar data quality. As News Ninja encourages continuous play, it allows datasets to adapt to the reception and contextualization of news over time, presenting a promising strategy to reduce data collection expenses, educate players, and promote long-term bias mitigation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17111v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Smi Hinterreiter, Timo Spinde, Sebastian Oberd\"orfer, Isao Echizen, Marc Erich Latoschik</dc:creator>
    </item>
    <item>
      <title>Spatial Conceptual Modeling: Anchoring Knowledge in the Real World</title>
      <link>https://arxiv.org/abs/2407.17259</link>
      <description>arXiv:2407.17259v1 Announce Type: new 
Abstract: This paper introduces the concept of spatial conceptual modeling, which allows anchoring mental world knowledge in the physical world using augmented reality technologies. For a first formal characterization, we describe a mapping from the spatial information concepts location, field, object, network, and event, as used in spatial computing, to conceptual modeling concepts using the FDMM formalism. This allows to identify necessary adaptations at the metamodeling level to make the approach applicable to arbitrary types of spatial conceptual modeling languages. Finally, possible application areas of spatial conceptual modeling in the medical domain, manufacturing and engineering, physical IT architectures and smart homes, supply chain management and logistics, civil engineering, and smart cities and cultural heritage are discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17259v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-56862-6</arxiv:DOI>
      <dc:creator>Hans-Georg Fill</dc:creator>
    </item>
    <item>
      <title>How Good (Or Bad) Are LLMs at Detecting Misleading Visualizations?</title>
      <link>https://arxiv.org/abs/2407.17291</link>
      <description>arXiv:2407.17291v1 Announce Type: new 
Abstract: In this study, we address the growing issue of misleading charts, a prevalent problem that undermines the integrity of information dissemination. Misleading charts can distort the viewer's perception of data, leading to misinterpretations and decisions based on false information. The development of effective automatic detection methods for misleading charts is an urgent field of research. The recent advancement of multimodal Large Language Models (LLMs) has introduced a promising direction for addressing this challenge. We explored the capabilities of these models in analyzing complex charts and assessing the impact of different prompting strategies on the models' analyses. We utilized a dataset of misleading charts collected from the internet by prior research and crafted nine distinct prompts, ranging from simple to complex, to test the ability of four different multimodal LLMs in detecting over 21 different chart issues. Through three experiments--from initial exploration to detailed analysis--we progressively gained insights into how to effectively prompt LLMs to identify misleading charts and developed strategies to address the scalability challenges encountered as we expanded our detection range from the initial five issues to 21 issues in the final experiment. Our findings reveal that multimodal LLMs possess a strong capability for chart comprehension and critical thinking in data interpretation. There is significant potential in employing multimodal LLMs to counter misleading information by supporting critical thinking and enhancing visualization literacy. This study demonstrates the applicability of LLMs in addressing the pressing concern of misleading charts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17291v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leo Yu-Ho Lo, Huamin Qu</dc:creator>
    </item>
    <item>
      <title>Co-designing an AI Impact Assessment Report Template with AI Practitioners and AI Compliance Experts</title>
      <link>https://arxiv.org/abs/2407.17374</link>
      <description>arXiv:2407.17374v1 Announce Type: new 
Abstract: In the evolving landscape of AI regulation, it is crucial for companies to conduct impact assessments and document their compliance through comprehensive reports. However, current reports lack grounding in regulations and often focus on specific aspects like privacy in relation to AI systems, without addressing the real-world uses of these systems. Moreover, there is no systematic effort to design and evaluate these reports with both AI practitioners and AI compliance experts. To address this gap, we conducted an iterative co-design process with 14 AI practitioners and 6 AI compliance experts and proposed a template for impact assessment reports grounded in the EU AI Act, NIST's AI Risk Management Framework, and ISO 42001 AI Management System. We evaluated the template by producing an impact assessment report for an AI-based meeting companion at a major tech company. A user study with 8 AI practitioners from the same company and 5 AI compliance experts from industry and academia revealed that our template effectively provides necessary information for impact assessments and documents the broad impacts of AI systems. Participants envisioned using the template not only at the pre-deployment stage for compliance but also as a tool to guide the design stage of AI uses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17374v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Edyta Bogucka, Marios Constantinides, Sanja \v{S}\'cepanovi\'c, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>ProvenanceWidgets: A Library of UI Control Elements to Track and Dynamically Overlay Analytic Provenance</title>
      <link>https://arxiv.org/abs/2407.17431</link>
      <description>arXiv:2407.17431v1 Announce Type: new 
Abstract: We present ProvenanceWidgets, a Javascript library of UI control elements such as radio buttons, checkboxes, and dropdowns to track and dynamically overlay a user's analytic provenance. These in situ overlays not only save screen space but also minimize the amount of time and effort needed to access the same information from elsewhere in the UI. In this paper, we discuss how we design modular UI control elements to track how often and how recently a user interacts with them and design visual overlays showing an aggregated summary as well as a detailed temporal history. We demonstrate the capability of ProvenanceWidgets by recreating three prior widget libraries: (1) Scented Widgets, (2) Phosphor objects, and (3) Dynamic Query Widgets. We also evaluated its expressiveness and conducted case studies with visualization developers to evaluate its effectiveness. We find that ProvenanceWidgets enables developers to implement custom provenance-tracking applications effectively. ProvenanceWidgets is available as open-source software at https://github.com/ProvenanceWidgets to help application developers build custom provenance-based systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17431v1</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arpit Narechania, Kaustubh Odak, Mennatallah El-Assady, Alex Endert</dc:creator>
    </item>
    <item>
      <title>A Dataset for Crucial Object Recognition in Blind and Low-Vision Individuals' Navigation</title>
      <link>https://arxiv.org/abs/2407.16777</link>
      <description>arXiv:2407.16777v1 Announce Type: cross 
Abstract: This paper introduces a dataset for improving real-time object recognition systems to aid blind and low-vision (BLV) individuals in navigation tasks. The dataset comprises 21 videos of BLV individuals navigating outdoor spaces, and a taxonomy of 90 objects crucial for BLV navigation, refined through a focus group study. We also provide object labeling for the 90 objects across 31 video segments created from the 21 videos. A deeper analysis reveals that most contemporary datasets used in training computer vision models contain only a small subset of the taxonomy in our dataset. Preliminary evaluation of state-of-the-art computer vision models on our dataset highlights shortcomings in accurately detecting key objects relevant to BLV navigation, emphasizing the need for specialized datasets. We make our dataset publicly available, offering valuable resources for developing more inclusive navigation systems for BLV individuals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16777v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Touhidul Islam, Imran Kabir, Elena Ariel Pearce, Md Alimoor Reza, Syed Masum Billah</dc:creator>
    </item>
    <item>
      <title>Fusion and Cross-Modal Transfer for Zero-Shot Human Action Recognition</title>
      <link>https://arxiv.org/abs/2407.16803</link>
      <description>arXiv:2407.16803v1 Announce Type: cross 
Abstract: Despite living in a multi-sensory world, most AI models are limited to textual and visual interpretations of human motion and behavior. Inertial measurement units (IMUs) provide a salient signal to understand human motion; however, they are challenging to use due to their uninterpretability and scarcity of their data. We investigate a method to transfer knowledge between visual and inertial modalities using the structure of an informative joint representation space designed for human action recognition (HAR). We apply the resulting Fusion and Cross-modal Transfer (FACT) method to a novel setup, where the model does not have access to labeled IMU data during training and is able to perform HAR with only IMU data during testing. Extensive experiments on a wide range of RGB-IMU datasets demonstrate that FACT significantly outperforms existing methods in zero-shot cross-modal transfer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16803v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Abhi Kamboj, Anh Duy Nguyen, Minh Do</dc:creator>
    </item>
    <item>
      <title>GPT-4's One-Dimensional Mapping of Morality: How the Accuracy of Country-Estimates Depends on Moral Domain</title>
      <link>https://arxiv.org/abs/2407.16886</link>
      <description>arXiv:2407.16886v1 Announce Type: cross 
Abstract: Prior research demonstrates that Open AI's GPT models can predict variations in moral opinions between countries but that the accuracy tends to be substantially higher among high-income countries compared to low-income ones. This study aims to replicate previous findings and advance the research by examining how accuracy varies with different types of moral questions. Using responses from the World Value Survey and the European Value Study, covering 18 moral issues across 63 countries, we calculated country-level mean scores for each moral issue and compared them with GPT-4's predictions. Confirming previous findings, our results show that GPT-4 has greater predictive success in high-income than in low-income countries. However, our factor analysis reveals that GPT-4 bases its predictions primarily on a single dimension, presumably reflecting countries' degree of conservatism/liberalism. Conversely, the real-world moral landscape appears to be two-dimensional, differentiating between personal-sexual and violent-dishonest issues. When moral issues are categorized based on their moral domain, GPT-4's predictions are found to be remarkably accurate in the personal-sexual domain, across both high-income (r = .77) and low-income (r = .58) countries. Yet the predictive accuracy significantly drops in the violent-dishonest domain for both high-income (r = .30) and low-income (r = -.16) countries, indicating that GPT-4's one-dimensional world-view does not fully capture the complexity of the moral landscape. In sum, this study underscores the importance of not only considering country-specific characteristics to understand GPT-4's moral understanding, but also the characteristics of the moral issues at hand.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16886v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pontus Strimling, Joel Krueger, Simon Karlsson</dc:creator>
    </item>
    <item>
      <title>A Nested Model for AI Design and Validation</title>
      <link>https://arxiv.org/abs/2407.16888</link>
      <description>arXiv:2407.16888v1 Announce Type: cross 
Abstract: The burgeoning field of artificial intelligence (AI) has yet to fully permeate real-world applications, largely due to issues of trust, transparency, and concerns about fairness and discrimination. Despite the increasing need for new and revised regulations to address the ethical and legal risks of using AI, there is a mismatch between regulatory science and AI, hindering the creation of a consistent framework. This highlights the need for guidance and regulation, especially as new AI legislation emerges. To bridge this gap, we propose a five-layer nested model for AI design and validation. This model is designed to address the challenges faced by AI practitioners and streamline the design and validation of AI applications and workflows, thereby improving fairness, trust, and AI adoption. This model not only parallels regulations and addresses the daily challenges faced by AI practitioners, but also provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each layer. We also provide three recommendations motivated by this model: authors should distinguish between layers when claiming contributions to clarify the specific areas in which the contribution is made and to avoid confusion, authors should explicitly state upstream assumptions to ensure that the context and limitations of their AI system are clearly understood, AI venues should promote thorough testing and validation of AI systems and their compliance with regulatory requirements</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16888v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Akshat Dubey, Zewen Yang, Georges Hattab</dc:creator>
    </item>
    <item>
      <title>Assessing the role of clinical summarization and patient chart review within communications, medical management, and diagnostics</title>
      <link>https://arxiv.org/abs/2407.16905</link>
      <description>arXiv:2407.16905v1 Announce Type: cross 
Abstract: Effective summarization of unstructured patient data in electronic health records (EHRs) is crucial for accurate diagnosis and efficient patient care, yet clinicians often struggle with information overload and time constraints. This review dives into recent literature and case studies on both the significant impacts and outstanding issues of patient chart review on communications, diagnostics, and management. It also discusses recent efforts to integrate artificial intelligence (AI) into clinical summarization tasks, and its transformative impact on the clinician's potential, including but not limited to reductions of administrative burden and improved patient-centered care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16905v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chanseo Lee, Kimon-Aristotelis Vogt, Sonu Kumar</dc:creator>
    </item>
    <item>
      <title>Using Helium Balloon Flying Drones for Introductory CS Education</title>
      <link>https://arxiv.org/abs/2407.16909</link>
      <description>arXiv:2407.16909v1 Announce Type: cross 
Abstract: In the rapidly evolving field of computer science education, novel approaches to teaching fundamental concepts are crucial for engaging a diverse student body. Given the growing demand for a computing-skilled workforce, it is essential to adapt educational methods to capture the interest of a broader audience than what current computing education typically targets. Engaging educational experiences have been shown to have a positive impact on learning outcomes and examination performance, especially within computing education. Moreover, physical computing devices have been shown to correlate with increased student motivation when students are studying computer science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16909v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stanley Cao, Christopher Gregg</dc:creator>
    </item>
    <item>
      <title>SepsisLab: Early Sepsis Prediction with Uncertainty Quantification and Active Sensing</title>
      <link>https://arxiv.org/abs/2407.16999</link>
      <description>arXiv:2407.16999v1 Announce Type: cross 
Abstract: Sepsis is the leading cause of in-hospital mortality in the USA. Early sepsis onset prediction and diagnosis could significantly improve the survival of sepsis patients. Existing predictive models are usually trained on high-quality data with few missing information, while missing values widely exist in real-world clinical scenarios (especially in the first hours of admissions to the hospital), which causes a significant decrease in accuracy and an increase in uncertainty for the predictive models. The common method to handle missing values is imputation, which replaces the unavailable variables with estimates from the observed data. The uncertainty of imputation results can be propagated to the sepsis prediction outputs, which have not been studied in existing works on either sepsis prediction or uncertainty quantification. In this study, we first define such propagated uncertainty as the variance of prediction output and then introduce uncertainty propagation methods to quantify the propagated uncertainty. Moreover, for the potential high-risk patients with low confidence due to limited observations, we propose a robust active sensing algorithm to increase confidence by actively recommending clinicians to observe the most informative variables. We validate the proposed models in both publicly available data (i.e., MIMIC-III and AmsterdamUMCdb) and proprietary data in The Ohio State University Wexner Medical Center (OSUWMC). The experimental results show that the propagated uncertainty is dominant at the beginning of admissions to hospitals and the proposed algorithm outperforms state-of-the-art active sensing methods. Finally, we implement a SepsisLab system for early sepsis prediction and active sensing based on our pre-trained models. Clinicians and potential sepsis patients can benefit from the system in early prediction and diagnosis of sepsis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16999v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3394486.3403129</arxiv:DOI>
      <dc:creator>Changchang Yin, Pin-Yu Chen, Bingsheng Yao, Dakuo Wang, Jeffrey Caterino, Ping Zhang</dc:creator>
    </item>
    <item>
      <title>Pensieve Discuss: Scalable Small-Group CS Tutoring System with AI</title>
      <link>https://arxiv.org/abs/2407.17007</link>
      <description>arXiv:2407.17007v1 Announce Type: cross 
Abstract: Small-group tutoring in Computer Science (CS) is effective, but presents the challenge of providing a dedicated tutor for each group and encouraging collaboration among group members at scale. We present Pensieve Discuss, a software platform that integrates synchronous editing for scaffolded programming problems with online human and AI tutors, designed to improve student collaboration and experience during group tutoring sessions. Our semester-long deployment to 800 students in a CS1 course demonstrated consistently high collaboration rates, positive feedback about the AI tutor's helpfulness and correctness, increased satisfaction with the group tutoring experience, and a substantial increase in question volume. The use of our system was preferred over an interface lacking AI tutors and synchronous editing capabilities. Our experiences suggest that small-group tutoring sessions are an important avenue for future research in educational AI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17007v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yoonseok Yang, Jack Liu, J. D. Zamfirescu-Pereira, John DeNero</dc:creator>
    </item>
    <item>
      <title>Cultural influence on RE activities: An extended analysis of state of the art</title>
      <link>https://arxiv.org/abs/2407.17038</link>
      <description>arXiv:2407.17038v1 Announce Type: cross 
Abstract: Designing mobile software that aligns with cultural contexts is crucial for optimizing human-computer interaction. Considering cultural influences is essential not only for the actual set of functional/non-functional requirements, but also for the whole Requirement Engineering (RE) process. Without a clear understanding of cultural influences on RE activities, it's hardly possible to elaborate a correct and complete set of requirements. This research explores the impact of national culture on RE-related activities based on recent studies. We conducted a Systematic Literature Review (SLR) of studies published in 2019-2023 and compared them to an older SLR covering 2000-2018. We identified 17 relevant studies, extracted 33 cultural influences impacting RE activities, and mapped them to the Hofstede model, widely used for cultural analysis in software development research. Our work highlights the critical role of national culture in RE activities, summarizes current research trends, and helps practitioners consider cultural influences for mobile app/software development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17038v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3640471.3680236</arxiv:DOI>
      <dc:creator>Chowdhury Shahriar Muzammel, Maria Spichkova, James Harland</dc:creator>
    </item>
    <item>
      <title>Nonverbal Immediacy Analysis in Education: A Multimodal Computational Model</title>
      <link>https://arxiv.org/abs/2407.17209</link>
      <description>arXiv:2407.17209v1 Announce Type: cross 
Abstract: This paper introduces a novel computational approach for analyzing nonverbal social behavior in educational settings. Integrating multimodal behavioral cues, including facial expressions, gesture intensity, and spatial dynamics, the model assesses the nonverbal immediacy (NVI) of teachers from RGB classroom videos. A dataset of 400 30-second video segments from German classrooms was constructed for model training and validation. The gesture intensity regressor achieved a correlation of 0.84, the perceived distance regressor 0.55, and the NVI model 0.44 with median human ratings. The model demonstrates the potential to provide a valuable support in nonverbal behavior assessment, approximating the accuracy of individual human raters. Validated against both questionnaire data and trained observer ratings, our models show moderate to strong correlations with relevant educational outcomes, indicating their efficacy in reflecting effective teaching behaviors. This research advances the objective assessment of nonverbal communication behaviors, opening new pathways for educational research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17209v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uro\v{s} Petkovi\'c, Jonas Frenkel, Olaf Hellwich, Rebecca Lazarides</dc:creator>
    </item>
    <item>
      <title>Enabling On-Device LLMs Personalization with Smartphone Sensing</title>
      <link>https://arxiv.org/abs/2407.04418</link>
      <description>arXiv:2407.04418v2 Announce Type: replace 
Abstract: This demo presents a novel end-to-end framework that combines on-device large language models (LLMs) with smartphone sensing technologies to achieve context-aware and personalized services. The framework addresses critical limitations of current personalization solutions via cloud LLMs, such as privacy concerns, latency and cost, and limited personal information. To achieve this, we innovatively proposed deploying LLMs on smartphones with multimodal sensor data through context-aware sensing and customized prompt engineering, ensuring privacy and enhancing personalization performance. A case study involving a university student demonstrated the capability of the framework to provide tailored recommendations. In addition, we show that the framework achieves the best trade-off in privacy, performance, latency, cost, battery and energy consumption between on-device and cloud LLMs. To the best of our knowledge, this is the first framework to provide on-device LLMs personalization with smartphone sensing. Future work will incorporate more diverse sensor data and involve extensive user studies to enhance personalization. Our proposed framework has the potential to substantially improve user experiences across domains including healthcare, productivity, and entertainment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04418v2</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiquan Zhang, Ying Ma, Le Fang, Hong Jia, Simon D'Alfonso, Vassilis Kostakos</dc:creator>
    </item>
    <item>
      <title>Using Case Studies to Teach Responsible AI to Industry Practitioners</title>
      <link>https://arxiv.org/abs/2407.14686</link>
      <description>arXiv:2407.14686v2 Announce Type: replace 
Abstract: Responsible AI (RAI) is the science and the practice of making the design, development, and use of AI socially sustainable: of reaping the benefits of innovation while controlling the risks. Naturally, industry practitioners play a decisive role in our collective ability to achieve the goals of RAI. Unfortunately, we do not yet have consolidated educational materials and effective methodologies for teaching RAI to practitioners. In this paper, we propose a novel stakeholder-first educational approach that uses interactive case studies to achieve organizational and practitioner -level engagement and advance learning of RAI. We discuss a partnership with Meta, an international technology company, to co-develop and deliver RAI workshops to a diverse audience within the company. Our assessment results indicate that participants found the workshops engaging and reported a positive shift in understanding and motivation to apply RAI to their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14686v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Julia Stoyanovich, Rodrigo Kreis de Paula, Armanda Lewis, Chloe Zheng</dc:creator>
    </item>
    <item>
      <title>The Impact of Responsible AI Research on Innovation and Development</title>
      <link>https://arxiv.org/abs/2407.15647</link>
      <description>arXiv:2407.15647v2 Announce Type: replace 
Abstract: Translational research, especially in the fast-evolving field of Artificial Intelligence (AI), is key to converting scientific findings into practical innovations. In Responsible AI (RAI) research, translational impact is often viewed through various pathways, including research papers, blogs, news articles, and the drafting of forthcoming AI legislation (e.g., the EU AI Act). However, the real-world impact of RAI research remains an underexplored area. Our study aims to capture it through two pathways: \emph{patents} and \emph{code repositories}, both of which provide a rich and structured source of data. Using a dataset of 200,000 papers from 1980 to 2022 in AI and related fields, including Computer Vision, Natural Language Processing, and Human-Computer Interaction, we developed a Sentence-Transformers Deep Learning framework to identify RAI papers. This framework calculates the semantic similarity between paper abstracts and a set of RAI keywords, which are derived from the NIST's AI Risk Management Framework; a framework that aims to enhance trustworthiness considerations in the design, development, use, and evaluation of AI products, services, and systems. We identified 1,747 RAI papers published in top venues such as CHI, CSCW, NeurIPS, FAccT, and AIES between 2015 and 2022. By analyzing these papers, we found that a small subset that goes into patents or repositories is highly cited, with the translational process taking between 1 year for repositories and up to 8 years for patents. Interestingly, impactful RAI research is not limited to top U.S. institutions, but significant contributions come from European and Asian institutions. Finally, the multidisciplinary nature of RAI papers, often incorporating knowledge from diverse fields of expertise, was evident as these papers tend to build on unconventional combinations of prior knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15647v2</guid>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ali Akbar Septiandri, Marios Constantinides, Daniele Quercia</dc:creator>
    </item>
    <item>
      <title>Privacy Perceptions and Behaviors of Google Personal Account Holders in Saudi Arabia</title>
      <link>https://arxiv.org/abs/2308.10148</link>
      <description>arXiv:2308.10148v4 Announce Type: replace-cross 
Abstract: While privacy perceptions and behaviors have been investigated in Western societies, little is known about these issues in non-Western societies. To bridge this gap, we interviewed 30 Google personal account holders in Saudi Arabia about their privacy perceptions and behaviors regarding the activity data that Google saves about them. Our study focuses on Google's Activity Controls, which enable users to control whether, and how, Google saves their Web \&amp; App Activity, Location History, and YouTube History. Our results show that although most participants have some level of awareness about Google's data practices and the Activity Controls, many have only vague awareness, and the majority have not used the available controls. When participants viewed their saved activity data, many were surprised by what had been saved. While many participants find Google's use of their data to improve the services provided to them acceptable, the majority find the use of their data for ad purposes unacceptable. We observe that our Saudi participants exhibit similar trends and patterns in privacy awareness, attitudes, preferences, concerns, and behaviors to what has been found in studies in the US. Our results emphasize the need for: 1) improved techniques to inform users about privacy settings during account sign-up, to remind users about their settings, and to raise awareness about privacy settings; 2) improved privacy setting interfaces to reduce the costs that deter many users from changing the settings; and 3) further research to explore privacy concerns in non-Western cultures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.10148v4</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eman Alashwali, Lorrie Faith Cranor</dc:creator>
    </item>
    <item>
      <title>What Do Privacy Advertisements Communicate to Consumers?</title>
      <link>https://arxiv.org/abs/2405.13857</link>
      <description>arXiv:2405.13857v3 Announce Type: replace-cross 
Abstract: When companies release marketing materials aimed at promoting their privacy practices or highlighting specific privacy features, what do they actually communicate to consumers? In this paper, we explore the impact of privacy marketing on: (1) consumers' attitudes toward the organizations providing the campaigns, (2) overall privacy awareness, and (3) the actionability of suggested privacy advice. To this end, we investigated the impact of four privacy advertising videos and one privacy game published by five different technology companies. We conducted 24 semi-structured interviews with participants randomly assigned to view one or two of the videos or play the game. Our findings suggest that awareness of privacy features can contribute to positive perceptions of a company or its products. The ads we tested were more successful in communicating the advertised privacy features than the game we tested. We observed that advertising a single privacy feature using a single metaphor in a short ad increased awareness of the advertised feature. The game failed to communicate privacy features or motivate study participants to use the features. Our results also suggest that privacy campaigns can be useful for raising awareness about privacy features and improving brand image, but may not be the most effective way to teach viewers how to use privacy features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13857v3</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoxin Shen, Eman Alashwali, Lorrie Faith Cranor</dc:creator>
    </item>
    <item>
      <title>VAAD: Visual Attention Analysis Dashboard applied to e-Learning</title>
      <link>https://arxiv.org/abs/2405.20091</link>
      <description>arXiv:2405.20091v3 Announce Type: replace-cross 
Abstract: In this paper, we present an approach in the Multimodal Learning Analytics field. Within this approach, we have developed a tool to visualize and analyze eye movement data collected during learning sessions in online courses. The tool is named VAAD, an acronym for Visual Attention Analysis Dashboard. These eye movement data have been gathered using an eye-tracker and subsequently processed and visualized for interpretation. The purpose of the tool is to conduct a descriptive analysis of the data by facilitating its visualization, enabling the identification of differences and learning patterns among various learner populations. Additionally, it integrates a predictive module capable of anticipating learner activities during a learning session. Consequently, VAAD holds the potential to offer valuable insights into online learning behaviors from both descriptive and predictive perspectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20091v3</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miriam Navarro, \'Alvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Julian Fierrez</dc:creator>
    </item>
    <item>
      <title>Bridging the Gap: Unravelling Local Government Data Sharing Barriers in Estonia and Beyond</title>
      <link>https://arxiv.org/abs/2406.08461</link>
      <description>arXiv:2406.08461v2 Announce Type: replace-cross 
Abstract: Open Government Data (OGD) plays a crucial role in transforming smart cities into sustainable and intelligent entities by providing data for analytics, real-time monitoring, and informed decision-making. This data is increasingly used in urban digital twins, enhancing city management through stakeholder collaboration. However, local administrative data remains underutilized even in digitally advanced countries like Estonia. This study explores barriers preventing Estonian municipalities from sharing OGD, using a qualitative approach through interviews with Estonian municipalities and drawing on the OGD-adapted Innovation Resistance Theory model (IRT). Interviews with local government officials highlight ongoing is-sues in data provision and quality. By addressing overlooked weaknesses in the Estonian open data ecosystem and providing actionable recommendations, this research contributes to a more resilient and sustainable open data ecosystem. Additionally, by validating the OGD-adapted Innovation Resistance Theory model and proposing a revised version tailored for local government contexts, the study advances theoretical frameworks for understanding data sharing resistance. Ultimately, this study serves as a call to action for policymakers and practitioners to prioritize local OGD initiatives, ensuring the full utilization of OGD in smart city development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08461v2</guid>
      <category>cs.CY</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <category>cs.IR</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katrin Rajam\"ae Soosaar, Anastasija Nikiforova</dc:creator>
    </item>
    <item>
      <title>Do Generative AI Models Output Harm while Representing Non-Western Cultures: Evidence from A Community-Centered Approach</title>
      <link>https://arxiv.org/abs/2407.14779</link>
      <description>arXiv:2407.14779v2 Announce Type: replace-cross 
Abstract: Our research investigates the impact of Generative Artificial Intelligence (GAI) models, specifically text-to-image generators (T2Is), on the representation of non-Western cultures, with a focus on Indian contexts. Despite the transformative potential of T2Is in content creation, concerns have arisen regarding biases that may lead to misrepresentations and marginalizations. Through a community-centered approach and grounded theory analysis of 5 focus groups from diverse Indian subcultures, we explore how T2I outputs to English prompts depict Indian culture and its subcultures, uncovering novel representational harms such as exoticism and cultural misappropriation. These findings highlight the urgent need for inclusive and culturally sensitive T2I systems. We propose design guidelines informed by a sociotechnical perspective, aiming to address these issues and contribute to the development of more equitable and representative GAI technologies globally. Our work also underscores the necessity of adopting a community-centered approach to comprehend the sociotechnical dynamics of these models, complementing existing work in this space while identifying and addressing the potential negative repercussions and harms that may arise when these models are deployed on a global scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14779v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sourojit Ghosh, Pranav Narayanan Venkit, Sanjana Gautam, Shomir Wilson, Aylin Caliskan</dc:creator>
    </item>
  </channel>
</rss>
