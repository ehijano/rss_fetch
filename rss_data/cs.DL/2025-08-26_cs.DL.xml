<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Aug 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Named Entity Recognition of Historical Text via Large Language Model</title>
      <link>https://arxiv.org/abs/2508.18090</link>
      <description>arXiv:2508.18090v1 Announce Type: new 
Abstract: Large language models have demonstrated remarkable versatility across a wide range of natural language processing tasks and domains. One such task is Named Entity Recognition (NER), which involves identifying and classifying proper names in text, such as people, organizations, locations, dates, and other specific entities. NER plays a crucial role in extracting information from unstructured textual data, enabling downstream applications such as information retrieval from unstructured text.
  Traditionally, NER is addressed using supervised machine learning approaches, which require large amounts of annotated training data. However, historical texts present a unique challenge, as the annotated datasets are often scarce or nonexistent, due to the high cost and expertise required for manual labeling. In addition, the variability and noise inherent in historical language, such as inconsistent spelling and archaic vocabulary, further complicate the development of reliable NER systems for these sources.
  In this study, we explore the feasibility of applying LLMs to NER in historical documents using zero-shot and few-shot prompting strategies, which require little to no task-specific training data. Our experiments, conducted on the HIPE-2022 (Identifying Historical People, Places and other Entities) dataset, show that LLMs can achieve reasonably strong performance on NER tasks in this setting. While their performance falls short of fully supervised models trained on domain-specific annotations, the results are nevertheless promising. These findings suggest that LLMs offer a viable and efficient alternative for information extraction in low-resource or historically significant corpora, where traditional supervised methods are infeasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18090v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shibingfeng Zhang, Giovanni Colavizza</dc:creator>
    </item>
    <item>
      <title>Red alert: Millions of "homeless" publications in Scopus should be resettled</title>
      <link>https://arxiv.org/abs/2508.18146</link>
      <description>arXiv:2508.18146v1 Announce Type: new 
Abstract: Scopus is increasingly regarded as a high-quality and reliable data source for research and evaluation of scientific and scholarly activity. However, a puzzling phenomenon has been discovered occasionally: millions of records with author affiliation information collected in Scopus are oddly labeled as "country-undefined" by Scopus which is rarely to be detected in its counterpart Web of Science. This huge number of "homeless" records in Scopus is unacceptable for a widely used high-quality bibliographic database. By using data from the past 124 years, this brief communication tries to probe these affiliated but country-undefined records in Scopus. Our analysis identifies four primary causes for these "homeless" records: incomplete author affiliation addresses, Scopus' inability to recognize different variants of country/territory names, misspelled country/territory names in author affiliation addresses, and Scopus' insufficiency in correctly split and identify the clean affiliation addresses. To address this pressing issue, we put forward several recommendations to relevant stakeholders, with the aim of resettling millions of "homeless" records in Scopus and reducing its potential impact on Scopus-based literature retrieval, analysis, and evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18146v1</guid>
      <category>cs.DL</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/asi.25011</arxiv:DOI>
      <arxiv:journal_reference>J Assoc Inf Sci Technol, 2025</arxiv:journal_reference>
      <dc:creator>Weishu Liu, Haifeng Wang</dc:creator>
    </item>
    <item>
      <title>SurveyGen: Quality-Aware Scientific Survey Generation with Large Language Models</title>
      <link>https://arxiv.org/abs/2508.17647</link>
      <description>arXiv:2508.17647v1 Announce Type: cross 
Abstract: Automatic survey generation has emerged as a key task in scientific document processing. While large language models (LLMs) have shown promise in generating survey texts, the lack of standardized evaluation datasets critically hampers rigorous assessment of their performance against human-written surveys. In this work, we present SurveyGen, a large-scale dataset comprising over 4,200 human-written surveys across diverse scientific domains, along with 242,143 cited references and extensive quality-related metadata for both the surveys and the cited papers. Leveraging this resource, we build QUAL-SG, a novel quality-aware framework for survey generation that enhances the standard Retrieval-Augmented Generation (RAG) pipeline by incorporating quality-aware indicators into literature retrieval to assess and select higher-quality source papers. Using this dataset and framework, we systematically evaluate state-of-the-art LLMs under varying levels of human involvement - from fully automatic generation to human-guided writing. Experimental results and human evaluations show that while semi-automatic pipelines can achieve partially competitive outcomes, fully automatic survey generation still suffers from low citation quality and limited critical analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17647v1</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>cs.IR</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>EMNLP2025</arxiv:journal_reference>
      <dc:creator>Tong Bao, Mir Tafseer Nayeem, Davood Rafiei, Chengzhi Zhang</dc:creator>
    </item>
    <item>
      <title>Debian in the Research Software Ecosystem: A Bibliometric Analysis</title>
      <link>https://arxiv.org/abs/2508.18073</link>
      <description>arXiv:2508.18073v1 Announce Type: cross 
Abstract: Context: The Debian system has historically participated in academic works and scientific projects, with well-known examples including NeuroDebian, Debian Med, Debsources, Debian Science, and Debian GIS, where the scientific relevance of Debian and its contribution to the Research Software ecosystem are evident.
  Objective: The objective of this study is to investigate the Debian system through academic publications, with the aim of classifying articles, mapping research, identifying trends, and finding opportunities.
  Method: The study is based on a bibliometric analysis starting with an initial search for the term "Debian" in the titles, abstracts, or keywords of academic publications, using the Scopus database. This analysis calculates metrics of co-citation, co-authorship, and word co-occurrence, and is guided by a set of research questions and criteria for inclusion and exclusion to conduct the bibliometric analysis.
  Results: The study includes a set of articles published across various fields of knowledge, providing a map of the academic publication space about Debian. The study's data will be available in a public repository, reporting demographic and bibliometric trends, including the most cited articles, active countries, researchers, and popular conferences.
  Conclusion: Results includes a bibliometric and demographic analysis identified in publications about Debian, shedding light on the intellectual structure of academic research. The results of the analyses can help researchers gain an overview of existing trends in publications about Debian and identify areas that require more attention from the scientific community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18073v1</guid>
      <category>cs.SE</category>
      <category>cs.DL</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Joenio Marques da Costa, Christina von Flach</dc:creator>
    </item>
    <item>
      <title>Lost Data in Electron Microscopy</title>
      <link>https://arxiv.org/abs/2508.18217</link>
      <description>arXiv:2508.18217v1 Announce Type: cross 
Abstract: The goal of this study is to estimate the amount of lost data in electron microscopy and to analyze the extent to which experimentally acquired images are utilized in peer-reviewed scientific publications. Analysis of the number of images taken on electron microscopes at a core user facility and the number of images subsequently included in peer-reviewed scientific journals revealed low efficiency of data utilization. More than 90% of electron microscopy data generated during routine instrument operation remain unused. Of the more than 150000 electron microscopy images evaluated in this study, only approximately 3500 (just over 2%) were made available in publications. Thus, the amount of lost data in electron microscopy can be estimated as &gt;90% (in terms of data being recorded but not being published in peer-reviewed literature). On the one hand, these results highlight a shortcoming in the optimal use of microscopy images; on the other hand, they indicate the existence of a large pool of electron microscopy data that can facilitate research in data science and the development of AI-based projects. The considerations important to unlock the potential of lost data are discussed in the present article.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18217v1</guid>
      <category>cs.DB</category>
      <category>cond-mat.mtrl-sci</category>
      <category>cs.DL</category>
      <category>physics.chem-ph</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 26 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nina M. Ivanova, Alexey S. Kashin, Valentine P. Ananikov</dc:creator>
    </item>
  </channel>
</rss>
