<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DC</link>
    <description>cs.DC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Aug 2024 04:00:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A parallel particle cluster algorithm using nearest neighbour graphs and passive target communication</title>
      <link>https://arxiv.org/abs/2408.15348</link>
      <description>arXiv:2408.15348v1 Announce Type: new 
Abstract: We present a parallel cluster algorithm for $N$-body simulations which uses a nearest neighbour search algorithm and one-sided messaging passing interface (MPI) communication. The nearest neighbour is defined by the Euclidean distance in three-dimensional space. The resulting directed nearest neighbour graphs that are used to define the clusters are split up in an iterative procedure with MPI remote memory access (RMA) communication. The method has been implemented as part of the elliptical parcel-in-cell (EPIC) method targeting geophysical fluid flows. The parallel scalability of the algorithm is discussed by means of an artificial and a standard fluid dynamics test case. The cluster algorithm shows good weak and strong scalability up to 16,384 cores with a parallel weak scaling efficiency of about 80% for balanced workloads. In poorly balanced problems, MPI synchronisation dominates execution of the cluster algorithm and thus drastically worsens its parallel scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15348v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Frey, Steven B\"oing, Rui F. G. Ap\'ostolo</dc:creator>
    </item>
    <item>
      <title>Analysis of the Performance of the Matrix Multiplication Algorithm on the Cirrus Supercomputer</title>
      <link>https://arxiv.org/abs/2408.15384</link>
      <description>arXiv:2408.15384v1 Announce Type: new 
Abstract: Matrix multiplication is integral to various scientific and engineering disciplines, including machine learning, image processing, and gaming. With the increasing data volumes in areas like machine learning, the demand for efficient parallel processing of large matrices has grown significantly.This study explores the performance of both serial and parallel matrix multiplication on the Cirrus supercomputer at the University of Edinburgh. The results demonstrate the scalability and efficiency of these methods, providing insights for optimizing matrixmultiplication in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15384v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Temitayo Adefemi</dc:creator>
    </item>
    <item>
      <title>Towards observability of scientific applications</title>
      <link>https://arxiv.org/abs/2408.15439</link>
      <description>arXiv:2408.15439v1 Announce Type: new 
Abstract: As software systems increase in complexity, conventional monitoring methods struggle to provide a comprehensive overview or identify performance issues, often missing unexpected problems. Observability, however, offers a holistic approach, providing methods and tools that gather and analyze detailed telemetry data to uncover hidden issues. Originally developed for cloud-native systems, modern observability is less prevalent in scientific computing, particularly in HPC clusters, due to differences in application architecture, execution environments, and technology stacks. This paper proposes and evaluates an end-to-end observability solution tailored for scientific computing in HPC environments. We address several challenges, including collection of application-level metrics, instrumentation, context propagation, and tracing. We argue that typical dashboards with charts are not sufficient for advanced observability-driven analysis of scientific applications. Consequently, we propose a different approach based on data analysis using DataFrames and a Jupyter environment. The proposed solution is implemented and evaluated on two medical scientific pipelines running on an HPC cluster.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15439v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bartosz Balis, Konrad Czerepak, Albert Kuzma, Jan Meizner, Lukasz Wronski</dc:creator>
    </item>
    <item>
      <title>Towards cloud-native scientific workflow management</title>
      <link>https://arxiv.org/abs/2408.15445</link>
      <description>arXiv:2408.15445v1 Announce Type: new 
Abstract: Cloud-native is an approach to building and running scalable applications in modern cloud infrastructures, with the Kubernetes container orchestration platform being often considered as a fundamental cloud-native building block. In this paper, we evaluate alternative execution models for scientific workflows in Kubernetes. We compare the simplest job-based model, its variant with task clustering, and finally we propose a cloud-native model based on microservices comprising auto-scalable worker-pools. We implement the proposed models in the HyperFlow workflow management system, and evaluate them using a large Montage workflow on a Kubernetes cluster. The results indicate that the proposed cloud-native worker-pools execution model achieves best performance in terms of average cluster utilization, resulting in a nearly 20\% improvement of the workflow makespan compared to the best-performing job-based model. However, better performance comes at the cost of significantly higher complexity of the implementation and maintenance. We believe that our experiments provide a valuable insight into the performance, advantages and disadvantages of alternative cloud-native execution models for scientific workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15445v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Orzechowski, Bartosz Balis, Krzysztof Janecki</dc:creator>
    </item>
    <item>
      <title>EdgeLinker: Practical Blockchain-based Framework for Healthcare Fog Applications to Enhance Security in Edge-IoT Data Communications</title>
      <link>https://arxiv.org/abs/2408.15838</link>
      <description>arXiv:2408.15838v1 Announce Type: new 
Abstract: The pervasive adoption of Internet of Things (IoT) has significantly advanced healthcare digitization and modernization. Nevertheless, the sensitive nature of medical data presents security and privacy challenges. On the other hand, resource constraints of IoT devices often necessitates cloud services for data handling, introducing single points of failure, processing delays, and security vulnerabilities. Meanwhile, the blockchain technology offers potential solutions for enhancing security, decentralization, and data ownership. An ideal solution should ensure confidentiality, access control, and data integrity while being scalable, cost-effective, and integrable with the existing systems. However, current blockchain-based studies only address some of these requirements. Accordingly, this paper proposes EdgeLinker; a comprehensive solution incorporating Proof-of-Authority consensus, integrating smart contracts on the Ethereum blockchain for access control, and advanced cryptographic algorithms for secure data communication between IoT edge devices and the fog layer in healthcare fog applications. This novel framework has been implemented in a real-world fog testbed, using COTS fog devices. Based on a comprehensive set of evaluations, EdgeLinker demonstrates significant improvements in security and privacy with reasonable costs, making it an affordable and practical system for healthcare fog applications. Compared with the state-of-the-art, without significant changes in the write-time to the blockchain, EdgeLinker achieves a 35% improvement in data read time. Additionally, it is able to provide better throughput in both reading and writing transactions compared to the existing studies. EdgeLinker has been also examined in terms of energy, resource consumption and channel latency in both secure and non-secure modes, which has shown remarkable improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15838v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Akbari Zarkesh, Ehsan Dastani, Bardia Safaei, Ali Movaghar</dc:creator>
    </item>
    <item>
      <title>Decentralized LLM Inference over Edge Networks with Energy Harvesting</title>
      <link>https://arxiv.org/abs/2408.15907</link>
      <description>arXiv:2408.15907v1 Announce Type: new 
Abstract: Large language models have significantly transformed multiple fields with their exceptional performance in natural language tasks, but their deployment in resource-constrained environments like edge networks presents an ongoing challenge. Decentralized techniques for inference have emerged, distributing the model blocks among multiple devices to improve flexibility and cost effectiveness. However, energy limitations remain a significant concern for edge devices. We propose a sustainable model for collaborative inference on interconnected, battery-powered edge devices with energy harvesting. A semi-Markov model is developed to describe the states of the devices, considering processing parameters and average green energy arrivals. This informs the design of scheduling algorithms that aim to minimize device downtimes and maximize network throughput. Through empirical evaluations and simulated runs, we validate the effectiveness of our approach, paving the way for energy-efficient decentralized inference over edge networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15907v1</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aria Khoshsirat, Giovanni Perin, Michele Rossi</dc:creator>
    </item>
    <item>
      <title>Exploring Selective Layer Fine-Tuning in Federated Learning</title>
      <link>https://arxiv.org/abs/2408.15600</link>
      <description>arXiv:2408.15600v1 Announce Type: cross 
Abstract: Federated learning (FL) has emerged as a promising paradigm for fine-tuning foundation models using distributed data in a privacy-preserving manner. Under limited computational resources, clients often find it more practical to fine-tune a selected subset of layers, rather than the entire model, based on their task-specific data. In this study, we provide a thorough theoretical exploration of selective layer fine-tuning in FL, emphasizing a flexible approach that allows the clients to adjust their selected layers according to their local data and resources. We theoretically demonstrate that the layer selection strategy has a significant impact on model convergence in two critical aspects: the importance of selected layers and the heterogeneous choices across clients. Drawing from these insights, we further propose a strategic layer selection method that utilizes local gradients and regulates layer selections across clients. The extensive experiments on both image and text datasets demonstrate the effectiveness of the proposed strategy compared with several baselines, highlighting its advances in identifying critical layers that adapt to the client heterogeneity and training dynamics in FL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15600v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuchang Sun, Yuexiang Xie, Bolin Ding, Yaliang Li, Jun Zhang</dc:creator>
    </item>
    <item>
      <title>Chop Chop: Byzantine Atomic Broadcast to the Network Limit</title>
      <link>https://arxiv.org/abs/2304.07081</link>
      <description>arXiv:2304.07081v2 Announce Type: replace 
Abstract: At the heart of state machine replication, the celebrated technique enabling decentralized and secure universal computation, lies Atomic Broadcast, a fundamental communication primitive that orders, authenticates, and deduplicates messages. This paper presents Chop Chop, a Byzantine Atomic Broadcast system that uses a novel authenticated memory pool to amortize the cost of ordering, authenticating and deduplicating messages, achieving "line rate" (i.e., closely matching the complexity of a protocol that does not ensure any ordering, authentication or Byzantine resilience) even when processing messages as small as 8 bytes. Chop Chop attains this performance by means of a new form of batching we call distillation. A distilled batch is a set of messages that are fast to authenticate, deduplicate, and order. Batches are distilled using a novel interactive protocol involving brokers, an untrusted layer of facilitating processes between clients and servers. In a geo-distributed deployment of 64 medium-sized servers, Chop Chop processes 43,600,000 messages per second with an average latency of 3.6 seconds. Under the same conditions, state-of-the-art alternatives offer two orders of magnitude less throughput for the same latency. We showcase three simple Chop Chop applications: a Payment system, an Auction house and a "Pixel war" game, respectively achieving 32, 2.3 and 35 million operations per second.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.07081v2</guid>
      <category>cs.DC</category>
      <category>cs.CR</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Camaioni, Rachid Guerraoui, Matteo Monti, Pierre-Louis Roman, Manuel Vidigueira, Gauthier Voron</dc:creator>
    </item>
    <item>
      <title>DDS: DPU-optimized Disaggregated Storage [Extended Report]</title>
      <link>https://arxiv.org/abs/2407.13618</link>
      <description>arXiv:2407.13618v5 Announce Type: replace 
Abstract: This extended report presents DDS, a novel disaggregated storage architecture enabled by emerging networking hardware, namely DPUs (Data Processing Units). DPUs can optimize the latency and CPU consumption of disaggregated storage servers. However, utilizing DPUs for DBMSs requires careful design of the network and storage paths and the interface exposed to the DBMS. To fully benefit from DPUs, DDS heavily uses DMA, zero-copy, and userspace I/O to minimize overhead when improving throughput. It also introduces an offload engine that eliminates host CPUs by executing client requests directly on the DPU. Adopting DDS' API requires minimal DBMS modification. Our experimental study and production system integration show promising results -- DDS achieves higher disaggregated storage throughput with an order of magnitude lower latency, and saves up to tens of CPU cores per storage server.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13618v5</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Qizhen Zhang, Philip Bernstein, Badrish Chandramouli, Jiasheng Hu, Yiming Zheng</dc:creator>
    </item>
    <item>
      <title>Granular Synchrony</title>
      <link>https://arxiv.org/abs/2408.12853</link>
      <description>arXiv:2408.12853v2 Announce Type: replace 
Abstract: Today's mainstream network timing models for distributed computing are synchrony, partial synchrony, and asynchrony. These models are coarse-grained and often make either too strong or too weak assumptions about the network. This paper introduces a new timing model called granular synchrony that models the network as a mixture of synchronous, partially synchronous, and asynchronous communication links. The new model is not only theoretically interesting but also more representative of real-world networks. It also serves as a unifying framework where current mainstream models are its special cases. We present necessary and sufficient conditions for solving crash and Byzantine fault-tolerant consensus in granular synchrony. Interestingly, consensus among $n$ parties can be achieved against $f \geq n/2$ crash faults or $f \geq n/3$ Byzantine faults without resorting to full synchrony.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12853v2</guid>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Neil Giridharan, Ittai Abraham, Natacha Crooks, Kartik Nayak, Ling Ren</dc:creator>
    </item>
    <item>
      <title>LIMO: Load-balanced Offloading with MAPE and Particle Swarm Optimization in Mobile Fog Networks</title>
      <link>https://arxiv.org/abs/2408.14218</link>
      <description>arXiv:2408.14218v2 Announce Type: replace-cross 
Abstract: Fog computing is essentially the expansion of cloud computing towards the network edge, reducing user access time to computing resources and services. Various advantages attribute to fog computing, including reduced latency, and improved user experience. However, user mobility may limit the benefits of fog computing. The displacement of users from one location to another, may increase their distance from a fog server, leading into latency amplification. This would also increase the probability of over utilization of fog servers which are located in popular destinations of mobile edge devices. This creates an unbalanced network of fog devices failing to provide lower makespan and fewer cloud accesses. One solution to maintain latency within an acceptable range is the migration of fog tasks and preserve the distance between the edge devices and the available resources. Although some studies have focused on fog task migration, none of them have considered load balancing in fog nodes. Accordingly, this paper introduces LIMO; an allocation and migration strategy for establishing load balancing in fog networks based on the control loop MAPE (Monitor-Analyze-Plan-Execute) and the Particle Swarm Optimization (PSO) algorithm. The periodical migration of tasks for load balancing aims to enhance the system's efficiency. The performance of LIMO has been modeled and evaluated using the Mobfogsim toolkit. The results show that this technique outperforms the state-of-the-art in terms of network resource utilization with 10% improvement. Furthermore, LIMO reduces the task migration to cloud by more than 15%, while it reduces the request response time by 18%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14218v2</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <pubDate>Thu, 29 Aug 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yasaman Seraj, Soheil Fadaei, Bardia Safaei, Ali Javadi, Amir Mahdi Hosseini Monazzah, Ali Mohammad Afshin Hemmatyar</dc:creator>
    </item>
  </channel>
</rss>
