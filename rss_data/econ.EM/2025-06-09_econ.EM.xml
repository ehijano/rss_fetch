<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 09 Jun 2025 04:01:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Efficient Estimation of Distributional Treatment Effects under Covariate-Adaptive Randomization</title>
      <link>https://arxiv.org/abs/2506.05945</link>
      <description>arXiv:2506.05945v1 Announce Type: new 
Abstract: This paper focuses on the estimation of distributional treatment effects in randomized experiments that use covariate-adaptive randomization (CAR). These include designs such as Efron's biased-coin design and stratified block randomization, where participants are first grouped into strata based on baseline covariates and assigned treatments within each stratum to ensure balance across groups. In practice, datasets often contain additional covariates beyond the strata indicators. We propose a flexible distribution regression framework that leverages off-the-shelf machine learning methods to incorporate these additional covariates, enhancing the precision of distributional treatment effect estimates. We establish the asymptotic distribution of the proposed estimator and introduce a valid inference procedure. Furthermore, we derive the semiparametric efficiency bound for distributional treatment effects under CAR and demonstrate that our regression-adjusted estimator attains this bound. Simulation studies and empirical analyses of microcredit programs highlight the practical advantages of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05945v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the International Conference on Machine Learning, 2025</arxiv:journal_reference>
      <dc:creator>Undral Byambadalai, Tomu Hirata, Tatsushi Oka, Shota Yasui</dc:creator>
    </item>
    <item>
      <title>Statistical significance in choice modelling: computation, usage and reporting</title>
      <link>https://arxiv.org/abs/2506.05996</link>
      <description>arXiv:2506.05996v1 Announce Type: new 
Abstract: This paper offers a commentary on the use of notions of statistical significance in choice modelling. We argue that, as in many other areas of science, there is an over-reliance on 95% confidence levels, and misunderstandings of the meaning of significance. We also observe a lack of precision in the reporting of measures of uncertainty in many studies, especially when using p-values and even more so with star measures. The paper provides a precise discussion on the computation of measures of uncertainty and confidence intervals, discusses the use of statistical tests, and also stresses the importance of considering behavioural or policy significance in addition to statistical significance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05996v1</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephane Hess, Andrew Daly, Michiel Bliemer, Angelo Guevara, Ricardo Daziano, Thijs Dekker</dc:creator>
    </item>
    <item>
      <title>Adaptive stable distribution and Hurst exponent by method of moments moving estimator for nonstationary time series</title>
      <link>https://arxiv.org/abs/2506.05354</link>
      <description>arXiv:2506.05354v1 Announce Type: cross 
Abstract: Nonstationarity of real-life time series requires model adaptation. In classical approaches like ARMA-ARCH there is assumed some arbitrarily chosen dependence type. To avoid their bias, we will focus on novel more agnostic approach: moving estimator, which estimates parameters separately for every time $t$: optimizing $F_t=\sum_{\tau&lt;t} (1-\eta)^{t-\tau} \ln(\rho_\theta (x_\tau))$ local log-likelihood with exponentially weakening weights of the old values. In practice such moving estimates can be found by EMA (exponential moving average) of some parameters, like $m_p=E[|x-\mu|^p]$ absolute central moments, updated by $m_{p,t+1} = m_{p,t} + \eta (|x_t-\mu_t|^p-m_{p,t})$. We will focus here on its applications for alpha-Stable distribution, which also influences Hurst exponent, hence can be used for its adaptive estimation. Its application will be shown on financial data as DJIA time series - beside standard estimation of evolution of center $\mu$ and scale parameter $\sigma$, there is also estimated evolution of $\alpha$ parameter allowing to continuously evaluate market stability - tails having $\rho(x) \sim 1/|x|^{\alpha+1}$ behavior, controlling probability of potentially dangerous extreme events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05354v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jarek Duda</dc:creator>
    </item>
    <item>
      <title>Inference in Unbalanced Panel Data Models with Interactive Fixed Effects</title>
      <link>https://arxiv.org/abs/2004.03414</link>
      <description>arXiv:2004.03414v2 Announce Type: replace 
Abstract: We derive the asymptotic theory of Bai (2009)'s interactive fixed effects estimator in unbalanced panels where the source of attrition is conditionally random. For inference, we propose a method of alternating projections algorithm based on straightforward scalar expressions to compute the residualized variables required for the estimation of the bias terms and the covariance matrix. Simulation experiments confirm our asymptotic results as reliable finite sample approximations. Furthermore, we reassess Acemoglu et al. (2019). Allowing for a more general form of unobserved heterogeneity, we confirm significant effects of democratization on growth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2004.03414v2</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Czarnowske, Amrei Stammann</dc:creator>
    </item>
    <item>
      <title>Causal inference in network experiments: regression-based analysis and design-based properties</title>
      <link>https://arxiv.org/abs/2309.07476</link>
      <description>arXiv:2309.07476v3 Announce Type: replace 
Abstract: Network experiments are powerful tools for studying spillover effects, which avoid endogeneity by randomly assigning treatments to units over networks. However, it is non-trivial to analyze network experiments properly without imposing strong modeling assumptions. We show that regression-based point estimators and standard errors can have strong theoretical guarantees if the regression functions and robust standard errors are carefully specified to accommodate the interference patterns under network experiments. We first recall a well-known result that the H\'ajek estimator is numerically identical to the coefficient from the weighted-least-squares fit based on the inverse probability of the exposure mapping. Moreover, we demonstrate that the regression-based approach offers three notable advantages: its ease of implementation, the ability to derive standard errors through the same regression fit, and the potential to integrate covariates into the analysis to improve efficiency. Recognizing that the regression-based network-robust covariance estimator can be anti-conservative under nonconstant effects, we propose an adjusted covariance estimator to improve the empirical coverage rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07476v3</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengsi Gao, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Identifying Causal Effects in Information Provision Experiments</title>
      <link>https://arxiv.org/abs/2309.11387</link>
      <description>arXiv:2309.11387v4 Announce Type: replace 
Abstract: Information treatments often shift beliefs more for people with weaker belief effects. Since standard TSLS and panel specifications in information provision experiments have weights proportional to belief updating in the first-stage, this dependence attenuates existing estimates. This is natural if people whose decisions depend on their beliefs gather information before the experiment. I propose a local least squares estimator that identifies unweighted average effects in several classes of experiments under progressively stronger versions of Bayesian updating. In five of six recent studies, average effects are larger than-in several cases more than double-estimates in standard specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.11387v4</guid>
      <category>econ.EM</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dylan Balla-Elliott</dc:creator>
    </item>
    <item>
      <title>Understanding the decision-making process of choice modellers</title>
      <link>https://arxiv.org/abs/2411.01704</link>
      <description>arXiv:2411.01704v2 Announce Type: replace 
Abstract: Discrete Choice Modelling serves as a robust framework for modelling human choice behaviour across various disciplines. Building a choice model is a semi structured research process that involves a combination of a priori assumptions, behavioural theories, and statistical methods. This complex set of decisions, coupled with diverse workflows, can lead to substantial variability in model outcomes. To better understand these dynamics, we developed the Serious Choice Modelling Game, which simulates the real world modelling process and tracks modellers' decisions in real time using a stated preference dataset. Participants were asked to develop choice models to estimate Willingness to Pay values to inform policymakers about strategies for reducing noise pollution. The game recorded actions across multiple phases, including descriptive analysis, model specification, and outcome interpretation, allowing us to analyse both individual decisions and differences in modelling approaches. While our findings reveal a strong preference for using data visualisation tools in descriptive analysis, it also identifies gaps in missing values handling before model specification. We also found significant variation in the modelling approach, even when modellers were working with the same choice dataset. Despite the availability of more complex models, simpler models such as Multinomial Logit were often preferred, suggesting that modellers tend to avoid complexity when time and resources are limited. Participants who engaged in more comprehensive data exploration and iterative model comparison tended to achieve better model fit and parsimony, which demonstrate that the methodological choices made throughout the workflow have significant implications, particularly when modelling outcomes are used for policy formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.01704v2</guid>
      <category>econ.EM</category>
      <category>cs.HC</category>
      <pubDate>Mon, 09 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gabriel Nova, Sander van Cranenburgh, Stephane Hess</dc:creator>
    </item>
  </channel>
</rss>
