<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Feb 2026 05:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SurfAge-Net: A Hierarchical Surface-Based Network for Interpretable Fine-Grained Brain Age Prediction</title>
      <link>https://arxiv.org/abs/2602.06994</link>
      <description>arXiv:2602.06994v1 Announce Type: new 
Abstract: Brain age prediction serves as a powerful framework for assessing brain status and detecting deviations associated with neurodevelopmental and neurodegenerative disorders. However, most existing approaches emphasize whole-brain age prediction and therefore overlook the pronounced regional heterogeneity of brain maturation that is crucial for detecting localized atypical trajectories. To address this limitation, we propose a novel spherical surface-based brain age prediction network (SurfAge-Net) that leverages multiple morphological metrics to capture region-specific developmental patterns with enhanced robustness and clinical interpretability. SurfAge-Net establishes a new modeling paradigm by incorporating the connectomic principles of cortical organization: it explicitly models both intra- and inter-hemispheric dependencies through a spatial-channel mixing and a lateralization-aware attention mechanism, enabling the network to characterize the coordinate maturation pattern uniquely associated with each target region. Validated on three fetal and neonatal datasets, SurfAge-Net outperforms existing approaches (global MAE = 0.54, regional MAE = 0.45 in gestational/postmenstrual weeks) and demonstrates strong generalizability across external cohorts. Importantly, it provides spatially precise and biologically interpretable maps of cortical maturation, effectively identifying heterogeneous delays and regional-specific abnormalities in atypical developmental populations. These results established fine-grained brain age prediction as a promising paradigm for advancing neurodevelopmental research and supporting early clinical assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.06994v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongzhao He, Dalin Zhu, Ying Wang, Songhong Yue, Leilei Zhao, Yu Fu, Dan Wu, Bin Hu, Weihao Zheng</dc:creator>
    </item>
    <item>
      <title>Cognitive algorithms and systems of episodic memory, semantic memory and their learnings</title>
      <link>https://arxiv.org/abs/2602.07261</link>
      <description>arXiv:2602.07261v1 Announce Type: new 
Abstract: Declarative memory, the memory that can be "declared" in words or languages, is made up of two dissociated parts: episodic memory and semantic memory. This dissociation has its neuroanatomical basis episodic memory is mostly associated with the hippocampus and semantic memory with the neocortex. The two memories, on the other hand, are closely related. Lesions in the hippocampus often result in various impairments of explicit memory, e.g., anterograde, retrograde and developmental amnesias, and semantic learning deficit. These impairments provide opportunities for us to understand how the two memories may be acquired, stored and organized. This chapter reviews several cognitive systems that are centered to mimic explicit memory, and other systems that are neuroanatomically based and are implemented to simulate those memory impairments mentioned above. This review includes: the structures of the computational systems, their learning rules, and their simulations of memory acquisition and impairments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07261v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-1-4419-1452-1_13</arxiv:DOI>
      <arxiv:journal_reference>Book chapter in Perception-action cycle: Models, Architectures, and Hardware. Springer, 2011</arxiv:journal_reference>
      <dc:creator>Qi Zhang</dc:creator>
    </item>
    <item>
      <title>Training-Driven Representational Geometry Modularization Predicts Brain Alignment in Language Models</title>
      <link>https://arxiv.org/abs/2602.07539</link>
      <description>arXiv:2602.07539v1 Announce Type: new 
Abstract: How large language models (LLMs) align with the neural representation and computation of human language is a central question in cognitive science. Using representational geometry as a mechanistic lens, we addressed this by tracking entropy, curvature, and fMRI encoding scores throughout Pythia (70M-1B) training. We identified a geometric modularization where layers self-organize into stable low- and high-complexity clusters. The low-complexity module, characterized by reduced entropy and curvature, consistently better predicted human language network activity. This alignment followed heterogeneous spatial-temporal trajectories: rapid and stable in temporal regions (AntTemp, PostTemp), but delayed and dynamic in frontal areas (IFG, IFGorb). Crucially, reduced curvature remained a robust predictor of model-brain alignment even after controlling for training progress, an effect that strengthened with model scale. These results links training-driven geometric reorganization to temporal-frontal functional specialization, suggesting that representational smoothing facilitates neural-like linguistic processing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07539v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yixuan Liu, Zhiyuan Ma, Likai Tang, Runmin Gan, Xinche Zhang, Jinhao Li, Chao Xie, Sen Song</dc:creator>
    </item>
    <item>
      <title>Linguistic properties and model scale in brain encoding: from small to compressed language models</title>
      <link>https://arxiv.org/abs/2602.07547</link>
      <description>arXiv:2602.07547v1 Announce Type: new 
Abstract: Recent work has shown that scaling large language models (LLMs) improves their alignment with human brain activity, yet it remains unclear what drives these gains and which representational properties are responsible. Although larger models often yield better task performance and brain alignment, they are increasingly difficult to analyze mechanistically. This raises a fundamental question: what is the minimal model capacity required to capture brain-relevant representations? To address this question, we systematically investigate how constraining model scale and numerical precision affects brain alignment. We compare full-precision LLMs, small language models (SLMs), and compressed variants (quantized and pruned) by predicting fMRI responses during naturalistic language comprehension. Across model families up to 14B parameters, we find that 3B SLMs achieve brain predictivity indistinguishable from larger LLMs, whereas 1B models degrade substantially, particularly in semantic language regions. Brain alignment is remarkably robust to compression: most quantization and pruning methods preserve neural predictivity, with GPTQ as a consistent exception. Linguistic probing reveals a dissociation between task performance and brain predictivity: compression degrades discourse, syntax, and morphology, yet brain predictivity remains largely unchanged. Overall, brain alignment saturates at modest model scales and is resilient to compression, challenging common assumptions about neural scaling and motivating compact models for brain-aligned language modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07547v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subba Reddy Oota, Vijay Rowtula, Satya Sai Srinath Namburi, Khushbu Pahwa, Anant Khandelwal, Manish Gupta, Tanmoy Chakraborty, Bapi S. Raju</dc:creator>
    </item>
    <item>
      <title>How does longer temporal context enhance multimodal narrative video processing in the brain?</title>
      <link>https://arxiv.org/abs/2602.07570</link>
      <description>arXiv:2602.07570v1 Announce Type: new 
Abstract: Understanding how humans and artificial intelligence systems process complex narrative videos is a fundamental challenge at the intersection of neuroscience and machine learning. This study investigates how the temporal context length of video clips (3--12 s clips) and the narrative-task prompting shape brain-model alignment during naturalistic movie watching. Using fMRI recordings from participants viewing full-length movies, we examine how brain regions sensitive to narrative context dynamically represent information over varying timescales and how these neural patterns align with model-derived features. We find that increasing clip duration substantially improves brain alignment for multimodal large language models (MLLMs), whereas unimodal video models show little to no gain. Further, shorter temporal windows align with perceptual and early language regions, while longer windows preferentially align higher-order integrative regions, mirrored by a layer-to-cortex hierarchy in MLLMs. Finally, narrative-task prompts (multi-scene summary, narrative summary, character motivation, and event boundary detection) elicit task-specific, region-dependent brain alignment patterns and context-dependent shifts in clip-level tuning in higher-order regions. Together, our results position long-form narrative movies as a principled testbed for probing biologically relevant temporal integration and interpretable representations in long-context MLLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07570v1</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Prachi Jindal, Anant Khandelwal, Manish Gupta, Bapi S. Raju, Subba Reddy Oota, Tanmoy Chakraborty</dc:creator>
    </item>
    <item>
      <title>Beyond Expertise: Stable Individual Differences in Predictive Eye-Hand Coordination</title>
      <link>https://arxiv.org/abs/2602.07816</link>
      <description>arXiv:2602.07816v1 Announce Type: new 
Abstract: Human eye-hand coordination relies on internal forward models that predict future states and compensate for sensory delays. During line tracing, the gaze typically leads the hand through predictive saccades, yet the extent to which this predictive window reflects expertise or intrinsic individual traits remains unclear. In this study, I examined eye-hand coordination in professional calligraphers and non-experts performing a controlled line tracing task. The temporal coupling between saccade distance (SD) and pen speed (PS) revealed substantial interpersonal variability: SD-PS peak times ranged from approximately -50 to 400 ms, forming stable, participant-specific predictive windows that were consistent across trials. These predictive windows closely matched each individual's pen catch-up time, indicating that the oculomotor system stabilizes fixation in anticipation of the hand's future velocity rather than relying on reactive pursuit. Neither the spatial indices (mean gaze-pen distance, mean saccade distance) nor the temporal index (SD-PS peak time) differed between calligraphers and non-calligraphers, and none of these predictive parameters correlated with tracing accuracy. These findings suggest that diverse predictive strategies can achieve equivalent performance, consistent with the minimum intervention principle of optimal feedback control. Together, the results indicate that predictive timing in eye-hand coordination reflects a stable, idiosyncratic Predictive Protocol shaped by individual neuromotor constraints rather than by expertise or training history.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07816v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emiko Shishido</dc:creator>
    </item>
    <item>
      <title>Bootstrapping Life-Inspired Machine Intelligence: The Biological Route from Chemistry to Cognition and Creativity</title>
      <link>https://arxiv.org/abs/2602.08079</link>
      <description>arXiv:2602.08079v1 Announce Type: new 
Abstract: Achieving advanced machine intelligence remains a central challenge in AI research, often approached through scaling neural architectures and generative models. However, biological systems offer a broader repertoire of strategies for adaptive, goal-directed behavior - strategies that emerged long before nervous systems evolved. This paper advocates a genuinely life-inspired approach to machine intelligence, drawing on principles from biology that enable robustness, autonomy, and open-ended problem-solving across scales. We frame intelligence as flexible problem-solving, following William James, and develop the concept of "cognitive light cones" to characterize the continuum of intelligence in living systems and machines. We argue that biological evolution has discovered a scalable recipe for intelligence - and the progressive expansion of organisms' "cognitive light cone", predictive and control capacities. To explain how this is possible, we distill five design principles - multiscale autonomy, growth through self-assemblage of active components, continuous reconstruction of capabilities, exploitation of physical and embodied constraints, and pervasive signaling enabling self-organization and top-down control from goals - that underpin life's ability to navigate creatively diverse problem spaces. We discuss how these principles contrast with current AI paradigms and outline pathways for integrating them into future autonomous, embodied, and resilient artificial systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08079v1</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Giovanni Pezzulo, Michael Levin</dc:creator>
    </item>
    <item>
      <title>Linguistics and Human Brain: A Perspective of Computational Neuroscience</title>
      <link>https://arxiv.org/abs/2602.08275</link>
      <description>arXiv:2602.08275v1 Announce Type: new 
Abstract: Elucidating the language-brain relationship requires bridging the methodological gap between the abstract theoretical frameworks of linguistics and the empirical neural data of neuroscience. Serving as an interdisciplinary cornerstone, computational neuroscience formalizes the hierarchical and dynamic structures of language into testable neural models through modeling, simulation, and data analysis. This enables a computational dialogue between linguistic hypotheses and neural mechanisms. Recent advances in deep learning, particularly large language models (LLMs), have powerfully advanced this pursuit. Their high-dimensional representational spaces provide a novel scale for exploring the neural basis of linguistic processing, while the "model-brain alignment" framework offers a methodology to evaluate the biological plausibility of language-related theories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08275v1</guid>
      <category>q-bio.NC</category>
      <category>cs.CL</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fudong Zhang, Bo Chai, Yujie Wu, Wai Ting Siok, Nizhuan Wang</dc:creator>
    </item>
    <item>
      <title>Behavior Score Prediction in Resting-State Functional MRI by Deep State Space Modeling</title>
      <link>https://arxiv.org/abs/2602.07131</link>
      <description>arXiv:2602.07131v1 Announce Type: cross 
Abstract: Early clinical assessment of Alzheimer's disease relies on behavior scores that measure a subject's language, memory, and cognitive skills. On the medical imaging side, functional magnetic resonance imaging has provided invaluable insights into the neural pathways underlying Alzheimer's disease. While prior studies have used resting-state functional MRI by extracting functional connectivity matrices, these approaches neglect the temporal dynamics inherent in functional data. In this work, we present a deep state space modeling framework that directly leverages the blood-oxygenation-level-dependent time series to learn a sparse collection of brain regions to predict behavior scores. Our model extracts temporal features that encapsulate nuanced patterns of intrinsic brain activity, thereby enhancing predictive performance compared to traditional connectivity methods. We identify specific brain regions that are most predictive of cognitive impairment through experiments on data provided by the Michigan Alzheimer's Disease Research Center, providing new insights into the neural substrates of early Alzheimer's pathology. These findings have important implications for the possible development of risk monitoring and intervention strategies in Alzheimer's disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07131v1</guid>
      <category>eess.SP</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Javier Salazar Cavazos, Maximillian Egan, Krisanne Litinas, Benjamin Hampstead, Scott Peltier</dc:creator>
    </item>
    <item>
      <title>Extracting Root-Causal Brain Activity Driving Psychopathology from Resting State fMRI</title>
      <link>https://arxiv.org/abs/2602.07233</link>
      <description>arXiv:2602.07233v1 Announce Type: cross 
Abstract: Neuroimaging studies of psychiatric disorders often correlate imaging patterns with diagnostic labels or composite symptom scores, yielding diffuse associations that obscure underlying mechanisms. We instead seek to identify root-causal maps -- localized BOLD disturbances that initiate pathological cascades -- and to link them selectively to symptom dimensions. We introduce a bilevel structural causal model that connects between-subject symptom structure to within-subject resting-state fMRI via independent latent sources with localized direct effects. Based on this model, we develop SOURCE (Symptom-Oriented Uncovering of Root-Causal Elements), a procedure that links interpretable symptom axes to a parsimonious set of localized drivers. Experiments show that SOURCE recovers localized maps consistent with root-causal BOLD drivers and increases interpretability and anatomical specificity relative to existing comparators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07233v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eric V. Strobl</dc:creator>
    </item>
    <item>
      <title>Universal Approximation Theorems for Dynamical Systems with Infinite-Time Horizon Guarantees</title>
      <link>https://arxiv.org/abs/2602.08640</link>
      <description>arXiv:2602.08640v1 Announce Type: cross 
Abstract: Universal approximation theorems establish the expressive capacity of neural network architectures. For dynamical systems, existing results are limited to finite time horizons or systems with a globally stable equilibrium, leaving multistability and limit cycles unaddressed. We prove that Neural ODEs achieve $\varepsilon$-$\delta$ closeness -- trajectories within error $\varepsilon$ except for initial conditions of measure $&lt; \delta$ -- over the \emph{infinite} time horizon $[0,\infty)$ for three target classes: (1) Morse-Smale systems (a structurally stable class) with hyperbolic fixed points, (2) Morse-Smale systems with hyperbolic limit cycles via exact period matching, and (3) systems with normally hyperbolic continuous attractors via discretization. We further establish a temporal generalization bound: $\varepsilon$-$\delta$ closeness implies $L^p$ error $\leq \varepsilon^p + \delta \cdot D^p$ for all $t \geq 0$, bridging topological guarantees to training metrics. These results provide the first universal approximation framework for multistable infinite-horizon dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08640v1</guid>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abel Sagodi, Il Memming Park</dc:creator>
    </item>
    <item>
      <title>Structural coarse-graining enables noise-robust functional connectivity and reveals hidden inter-subject variability</title>
      <link>https://arxiv.org/abs/2602.08910</link>
      <description>arXiv:2602.08910v1 Announce Type: cross 
Abstract: Functional connectivity estimates are highly sensitive to analysis choices and can be dominated by noise when the number of sampled time points is small relative to network dimensionality. This issue is particularly acute in fMRI, where scan resolution is limited. Because scan duration is constrained by practical factors (e.g., motion and fatigue), many datasets remain statistically underpowered for high-dimensional correlation estimation. We introduce a framework that combines diffusion-based structural coarse-graining with spectral noise filtering to recover statistically reliable functional networks from temporally limited data. The method reduces network dimensionality by grouping regions according to diffusion-defined communication. This produces coarse-grained networks with dimensions compatible with available time points, enabling random matrix filtering of noise-dominated modes. We benchmark three common FC pipelines against our approach. We find that raw-signal correlations are strongly influenced by non-stationary fluctuations that can reduce apparent inter-subject variability under limited sampling conditions. In contrast, our pipeline reveals a broader, multimodal landscape of inter-subject variability. These large-scale organization patterns are largely obscured by standard pipelines. Together, these results provide a practical route to reliable functional networks under realistic sampling constraints. This strategy helps separate noise-driven artifacts from reproducible patterns of human brain variability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.08910v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>q-bio.NC</category>
      <category>q-bio.PE</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Izaro Fernandez-Iriondo, Antonio Jimenez-Marin, Jesus Cortes, Pablo Villegas</dc:creator>
    </item>
    <item>
      <title>Generative Modeling of Neural Dynamics via Latent Stochastic Differential Equations</title>
      <link>https://arxiv.org/abs/2412.12112</link>
      <description>arXiv:2412.12112v3 Announce Type: replace 
Abstract: We propose a probabilistic framework for developing computational models of biological neural systems. In this framework, physiological recordings are viewed as discrete-time partial observations of an underlying continuous-time stochastic dynamical system which implements computations through its state evolution. To model this dynamical system, we employ a system of coupled stochastic differential equations with differentiable drift and diffusion functions and use variational inference to infer its states and parameters. This formulation enables seamless integration of existing mathematical models in the literature, neural networks, or a hybrid of both to learn and compare different models. We demonstrate this in our framework by developing a generative model that combines coupled oscillators with neural networks to capture latent population dynamics from single-cell recordings. Evaluation across three neuroscience datasets spanning different species, brain regions, and behavioral tasks show that these hybrid models achieve competitive performance in predicting stimulus-evoked neural and behavioral responses compared to sophisticated black-box approaches while requiring an order of magnitude fewer parameters, providing uncertainty estimates, and offering a natural language for interpretation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.12112v3</guid>
      <category>q-bio.NC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ahmed ElGazzar, Marcel van Gerven</dc:creator>
    </item>
    <item>
      <title>A 7T fMRI dataset of synthetic images for out-of-distribution modeling of vision</title>
      <link>https://arxiv.org/abs/2503.06286</link>
      <description>arXiv:2503.06286v4 Announce Type: replace 
Abstract: Now published in Nature Communications DOI: https://doi.org/10.1038/s41467-026-69345-9
  Large-scale visual neural datasets such as the Natural Scenes Dataset (NSD) are boosting computational neuroscience research by enabling models of the brain with performances beyond what was possible just a decade ago. However, because the stimuli of these datasets typically live within a common naturalistic visual distribution, they do not allow for strict out-of-distribution (OOD) generalization tests which are crucial for the development of more robust models. Here, we address this limitation by releasing NSD-synthetic, a dataset consisting of 7T fMRI responses from the same eight NSD participants for 284 synthetic images. We show that NSD-synthetic's fMRI responses reliably encode stimulus-related information and are OOD with respect to NSD. Furthermore, we provide a proof of principle that OOD generalization tests on NSD-synthetic reveal differences between models of the brain that are not detected with the original NSD data; we demonstrate that the degree of OOD (quantified as the distance between a set of responses and the training data used for modeling) is predictive of the magnitude of model failures; and we show that less strict OOD generalization tests can can be usefully applied even within the domain of naturalistic stimuli. These results showcase how NSD-synthetic enables OOD generalization tests that facilitate the development of more robust models of visual processing and the formulation of more accurate theories of human vision.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.06286v4</guid>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s41467-026-69345-9</arxiv:DOI>
      <arxiv:journal_reference>Nature Communications, 2026</arxiv:journal_reference>
      <dc:creator>Alessandro T. Gifford, Radoslaw M. Cichy, Thomas Naselaris, Kendrick Kay</dc:creator>
    </item>
    <item>
      <title>Task-Conditioned Probing Reveals Brain-Alignment Patterns in Instruction-Tuned Multimodal LLMs</title>
      <link>https://arxiv.org/abs/2506.08277</link>
      <description>arXiv:2506.08277v2 Announce Type: replace 
Abstract: Recent voxel-wise multimodal brain encoding studies have shown that multimodal large language models (MLLMs) exhibit a higher degree of brain alignment compared to unimodal models. More recently, instruction-tuned multimodal (IT) models have been shown to generate task-specific representations that align strongly with brain activity, yet most prior evaluations focus on unimodal stimuli or non-instruction-tuned models under multimodal stimuli. We still lack a clear understanding of whether instruction-tuning is associated with IT-MLLMs organizing their representations around functional task demands or if they simply reflect surface semantics. To address this, we estimate brain alignment by predicting fMRI responses recorded during naturalistic movie watching (video with audio) from MLLM representations. Using instruction-specific embeddings from six video and two audio IT-MLLMs, across 13 video task instructions, we find that instruction-tuned video MLLMs significantly outperform in-context learning (ICL) multimodal models (~9%), non-instruction-tuned multimodal models (~15%), and unimodal baselines (~20%). Our evaluation of MLLMs across video and audio tasks, and language-guided probing produces distinct task-specific MLLM representations that vary across brain regions. We also find that ICL models show strong semantic organization (r=0.78), while IT models show weak coupling to instruction-text semantics (r=0.14), consistent with task-conditioned subspaces associated with higher brain alignment. These findings are consistent with an association between task-specific instructions and stronger brain-MLLM alignment, and open new avenues for mapping joint information processing in both systems. We make the code publicly available [https://github.com/subbareddy248/mllm_videos].</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.08277v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Subba Reddy Oota, Khushbu Pahwa, Prachi Jindal, Satya Sai Srinath Namburi, Maneesh Singh, Tanmoy Chakraborty, Bapi S. Raju, Manish Gupta</dc:creator>
    </item>
    <item>
      <title>Short-term plasticity recalls forgotten memories through a trampoline mechanism</title>
      <link>https://arxiv.org/abs/2511.22848</link>
      <description>arXiv:2511.22848v2 Announce Type: replace 
Abstract: We analyze continuous Hopfield associative memories augmented by additional, rapid short-term associative synaptic plasticity. Through the cavity method, we determine the boundary between the retrieval and forgetting, or spin-glass phase, of the network as a function of the fraction of stored memories and the neuronal gain. We find that short-term synaptic plasticity yields marginal improvements in critical memory capacity. However, through dynamical mean field theory, backed by extensive numerical simulations, we find that short-term synaptic plasticity has a dramatic impact on memory retrieval above the critical capacity. When short-term synaptic plasticity is turned on, the combined neuronal and synaptic dynamics descends a high-dimensional energy landscape over both neurons and synapses. The energy landscape over neurons alone is thus dynamic, and is lowered in the vicinity of recent neuronal patterns visited by the network, just like the surface of a trampoline is lowered in the vicinity of regions recently visited by a heavy ball. This trampoline-like reactivity of the neuronal energy landscape to short-term plasticity in synapses can lead to the recall of stored memories that would otherwise have been forgotten. This occurs because the dynamics without short-term plasticity transiently moves towards a stored memory before departing away from it. Thus short-term plasticity, operating during the transient, lowers the energy in the vicinity of the stored memory, eventually trapping the combined neuronal and synaptic dynamics at a fixed point close to the stored memory. In this manner, short-term plasticity enables the recall of memories that would otherwise be forgotten, by trapping transients that would otherwise escape. We furthermore find an optimal time constant for short-term synaptic plasticity, matched to the transient dynamics, to empower recall of forgotten memories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22848v2</guid>
      <category>q-bio.NC</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martina Del Gaudio, Federico Ghimenti, Surya Ganguli</dc:creator>
    </item>
    <item>
      <title>Rethinking Functional Brain Connectome Analysis: Do Graph Deep Learning Models Help</title>
      <link>https://arxiv.org/abs/2501.17207</link>
      <description>arXiv:2501.17207v2 Announce Type: replace-cross 
Abstract: Graph deep learning models, a class of AI-driven approaches employing a message aggregation mechanism, have gained popularity for analyzing the functional brain connectome in neuroimaging. However, their actual effectiveness remains unclear. In this study, we re-examine graph deep learning versus classical machine learning models based on four large-scale neuroimaging studies. Surprisingly, we find that the message aggregation mechanism, a hallmark of graph deep learning models, does not help with predictive performance as typically assumed, but rather consistently degrades it. To address this issue, we propose a hybrid model combining a linear model with a graph attention network through dual pathways, achieving robust predictions and enhanced interpretability by revealing both localized and global neural connectivity patterns. Our findings urge caution in adopting complex deep learning models for functional brain connectome analysis, emphasizing the need for rigorous experimental designs to establish tangible performance gains and perhaps more importantly, to pursue improvements in model interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.17207v2</guid>
      <category>cs.NE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1038/s44387-025-00067-x</arxiv:DOI>
      <arxiv:journal_reference>npj Artificial Intelligence (2026)</arxiv:journal_reference>
      <dc:creator>Keqi Han, Yao Su, Lifang He, Liang Zhan, Sergey Plis, Vince Calhoun, Carl Yang</dc:creator>
    </item>
    <item>
      <title>RNNs perform task computations by dynamically warping neural representations</title>
      <link>https://arxiv.org/abs/2512.04310</link>
      <description>arXiv:2512.04310v2 Announce Type: replace-cross 
Abstract: Analysing how neural networks represent data features in their activations can help interpret how they perform tasks. Hence, a long line of work has focused on mathematically characterising the geometry of such "neural representations." In parallel, machine learning has seen a surge of interest in understanding how dynamical systems perform computations on time-varying input data. Yet, the link between computation-through-dynamics and representational geometry remains poorly understood. Here, we hypothesise that recurrent neural networks (RNNs) perform computations by dynamically warping their representations of task variables. To test this hypothesis, we develop a Riemannian geometric framework that enables the derivation of the manifold topology and geometry of a dynamical system from the manifold of its inputs. By characterising the time-varying geometry of RNNs, we show that dynamic warping is a fundamental feature of their computations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.04310v2</guid>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.DS</category>
      <category>q-bio.NC</category>
      <pubDate>Tue, 10 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arthur Pellegrino, Angus Chadwick</dc:creator>
    </item>
  </channel>
</rss>
