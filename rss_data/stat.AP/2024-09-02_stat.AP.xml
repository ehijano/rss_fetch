<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Sep 2024 04:00:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Framework for Digital Asset Risks with Insurance Applications</title>
      <link>https://arxiv.org/abs/2408.17227</link>
      <description>arXiv:2408.17227v1 Announce Type: new 
Abstract: The remarkable growth of digital assets, starting from the inception of Bitcoin in 2009 into a 1 trillion market in 2024, underscores the momentum behind disruptive technologies and the global appetite for digital assets. This paper develops a framework to enhance actuaries' understanding of the cyber risks associated with the developing digital asset ecosystem, as well as their measurement methods in the context of digital asset insurance. By integrating actuarial perspectives, we aim to enhance understanding and modeling of cyber risks at both the micro and systemic levels. The qualitative examination sheds light on blockchain technology and its associated risks, while our quantitative framework offers a rigorous approach to modeling cyber risks in digital asset insurance portfolios. This multifaceted approach serves three primary objectives: i) offer a clear and accessible education on the evolving digital asset ecosystem and the diverse spectrum of cyber risks it entails; ii) develop a scientifically rigorous framework for quantifying cyber risks in the digital asset ecosystem; iii) provide practical applications, including pricing strategies and tail risk management. Particularly, we develop frequency-severity models based on real loss data for pricing cyber risks in digit assets and utilize Monte Carlo simulation to estimate the tail risks, offering practical insights for risk management strategies. As digital assets continue to reshape finance, our work serves as a foundational step towards safeguarding the integrity and stability of this rapidly evolving landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17227v1</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengming Li, Jianxi Su, Maochao Xu, Jimmy Yuen</dc:creator>
    </item>
    <item>
      <title>cosimmr: an R package for fast fitting of Stable Isotope Mixing Models with covariates</title>
      <link>https://arxiv.org/abs/2408.17230</link>
      <description>arXiv:2408.17230v1 Announce Type: new 
Abstract: The study of animal diets and the proportional contribution that different foods make to their diets is an important task in ecology. Stable Isotope Mixing Models (SIMMs) are an important tool for studying an animal's diet and understanding how the animal interacts with its environment. We present cosimmr, a new R package designed to include covariates when estimating diet proportions in SIMMs, with simple functions to produce plots and summary statistics. The inclusion of covariates allows for users to perform a more in-depth analysis of their system and to gain new insights into the diets of the organisms being studied. A common problem with the previous generation of SIMMs is that they are very slow to produce a posterior distribution of dietary estimates, especially for more complex model structures, such as when covariates are included. The widely-used Markov chain Monte Carlo (MCMC) algorithm used by many traditional SIMMs often requires a very large number of iterations to reach convergence. In contrast, cosimmr uses Fixed Form Variational Bayes (FFVB), which we demonstrate gives up to an order of magnitude speed improvement with no discernible loss of accuracy. We provide a full mathematical description of the model, which includes corrections for trophic discrimination and concentration dependence, and evaluate its performance against the state of the art MixSIAR model. Whilst MCMC is guaranteed to converge to the posterior distribution in the long term, FFVB converges to an approximation of the posterior distribution, which may lead to sub-optimal performance. However we show that the package produces equivalent results in a fraction of the time for all the examples on which we test. The package is designed to be user-friendly and is based on the existing simmr framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17230v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emma Govan, Andrew L Jackson, Stuart Bearhop, Richard Inger, Brian C Stock, Brice X Semmens, Eric J Ward, Andrew C Parnell</dc:creator>
    </item>
    <item>
      <title>Modelling multiplex testing for outbreak Control</title>
      <link>https://arxiv.org/abs/2408.17239</link>
      <description>arXiv:2408.17239v1 Announce Type: new 
Abstract: During the SARS-CoV-2 pandemic, polymerase chain reaction (PCR) and lateral flow device (LFD) tests were frequently deployed to detect the presence of SARS-CoV-2. Many of these tests were singleplex, and only tested for the presence of a single pathogen. Multiplex tests can test for the presence of several pathogens using only a single swab, which can allow for: surveillance of more pathogens, targeting of antiviral interventions, a reduced burden of testing, and lower costs. Test sensitivity however, particularly in LFD tests, is highly conditional on the viral concentration dynamics of individuals. To inform the use of multiplex testing in outbreak detection it is therefore necessary to investigate the interactions between outbreak detection strategies and the differing viral concentration trajectories of key pathogens. Viral concentration trajectories are estimated for SARS-CoV-2, and Influenza A/B. Testing strategies for the first five symptomatic cases in an outbreak are then simulated and used to evaluate key performance indicators. Strategies that use a combination of multiplex LFD and PCR tests achieve; high levels of detection, detect outbreaks rapidly, and have the lowest burden of testing across multiple pathogens. Influenza B was estimated to have lower rates of detection due to its modelled viral concentration dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17239v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martyn Fyles, Christopher E. Overton, Tom Ward, Emma Bennett, Tom Fowler, Ian Hall</dc:creator>
    </item>
    <item>
      <title>A reproducible pipeline for extracting representative signals from wire cuts</title>
      <link>https://arxiv.org/abs/2405.11012</link>
      <description>arXiv:2405.11012v1 Announce Type: cross 
Abstract: We propose a reproducible pipeline for extracting representative signals from 2D topographic scans of the tips of cut wires. The process fully addresses many potential problems in the quality of wire cuts, including edge effects, extreme values, trends, missing values, angles, and warping. The resulting signals can be further used in source determination, which plays an important role in forensic examinations. With commonly used measurements such as the cross-correlation function, the procedure controls the false positive rate and false negative rate to the desirable values as the manual extraction pipeline but outperforms it with robustness and objectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.11012v1</guid>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhang Lin, Heike Hofmann</dc:creator>
    </item>
    <item>
      <title>Advance Real-time Detection of Traffic Incidents in Highways using Vehicle Trajectory Data</title>
      <link>https://arxiv.org/abs/2408.16773</link>
      <description>arXiv:2408.16773v1 Announce Type: cross 
Abstract: A significant number of traffic crashes are secondary crashes that occur because of an earlier incident on the road. Thus, early detection of traffic incidents is crucial for road users from safety perspectives with a potential to reduce the risk of secondary crashes. The wide availability of GPS devices now-a-days gives an opportunity of tracking and recording vehicle trajectories. The objective of this study is to use vehicle trajectory data for advance real-time detection of traffic incidents on highways using machine learning-based algorithms. The study uses three days of unevenly sequenced vehicle trajectory data and traffic incident data on I-10, one of the most crash-prone highways in Louisiana. Vehicle trajectories are converted to trajectories based on virtual detector locations to maintain spatial uniformity as well as to generate historical traffic data for machine learning algorithms. Trips matched with traffic incidents on the way are separated and along with other trips with similar spatial attributes are used to build a database for modeling. Multiple machine learning algorithms such as Logistic Regression, Random Forest, Extreme Gradient Boost, and Artificial Neural Network models are used to detect a trajectory that is likely to face an incident in the downstream road section. Results suggest that the Random Forest model achieves the best performance for predicting an incident with reasonable recall value and discrimination capability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16773v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sudipta Roy, Samiul Hasan</dc:creator>
    </item>
    <item>
      <title>Multi-faceted Neuroimaging Data Integration via Analysis of Subspaces</title>
      <link>https://arxiv.org/abs/2408.16791</link>
      <description>arXiv:2408.16791v1 Announce Type: cross 
Abstract: Neuroimaging studies, such as the Human Connectome Project (HCP), often collect multi-faceted and multi-block data to study the complex human brain. However, these data are often analyzed in a pairwise fashion, which can hinder our understanding of how different brain-related measures interact with each other. In this study, we comprehensively analyze the multi-block HCP data using the Data Integration via Analysis of Subspaces (DIVAS) method. We integrate structural and functional brain connectivity, substance use, cognition, and genetics in an exhaustive five-block analysis. This gives rise to the important finding that genetics is the single data modality most predictive of brain connectivity, outside of brain connectivity itself. Nearly 14\% of the variation in functional connectivity (FC) and roughly 12\% of the variation in structural connectivity (SC) is attributed to shared spaces with genetics. Moreover, investigations of shared space loadings provide interpretable associations between particular brain regions and drivers of variability, such as alcohol consumption in the substance-use data block. Novel Jackstraw hypothesis tests are developed for the DIVAS framework to establish statistically significant loadings. For example, in the (FC, SC, and Substance Use) shared space, these novel hypothesis tests highlight largely negative functional and structural connections suggesting the brain's role in physiological responses to increased substance use. Furthermore, our findings have been validated using a subset of genetically relevant siblings or twins not studied in the main analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16791v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Ackerman, Zhengwu Zhang, Jan Hannig, Jack Prothero, J. S. Marron</dc:creator>
    </item>
    <item>
      <title>Characterization of point-source transient events with a rolling-shutter compressed sensing system</title>
      <link>https://arxiv.org/abs/2408.16868</link>
      <description>arXiv:2408.16868v1 Announce Type: cross 
Abstract: Point-source transient events (PSTEs) - optical events that are both extremely fast and extremely small - pose several challenges to an imaging system. Due to their speed, accurately characterizing such events often requires detectors with very high frame rates. Due to their size, accurately detecting such events requires maintaining coverage over an extended field-of-view, often through the use of imaging focal plane arrays (FPA) with a global shutter readout. Traditional imaging systems that meet these requirements are costly in terms of price, size, weight, power consumption, and data bandwidth, and there is a need for cheaper solutions with adequate temporal and spatial coverage. To address these issues, we develop a novel compressed sensing algorithm adapted to the rolling shutter readout of an imaging system. This approach enables reconstruction of a PSTE signature at the sampling rate of the rolling shutter, offering a 1-2 order of magnitude temporal speedup and a proportional reduction in data bandwidth. We present empirical results demonstrating accurate recovery of PSTEs using measurements that are spatially undersampled by a factor of 25, and our simulations show that, relative to other compressed sensing algorithms, our algorithm is both faster and yields higher quality reconstructions. We also present theoretical results characterizing our algorithm and corroborating simulations. The potential impact of our work includes the development of much faster, cheaper sensor solutions for PSTE detection and characterization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16868v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>physics.optics</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frank Qiu, Joshua Michalenko, Lilian K. Casias, Cameron J. Radosevich, Jon Slater, Eric A. Shields</dc:creator>
    </item>
    <item>
      <title>Error-controlled non-additive interaction discovery in machine learning models</title>
      <link>https://arxiv.org/abs/2408.17016</link>
      <description>arXiv:2408.17016v1 Announce Type: cross 
Abstract: Machine learning (ML) models are powerful tools for detecting complex patterns within data, yet their "black box" nature limits their interpretability, hindering their use in critical domains like healthcare and finance. To address this challenge, interpretable ML methods have been developed to explain how features influence model predictions. However, these methods often focus on univariate feature importance, overlooking the complex interactions between features that ML models are capable of capturing. Recognizing this limitation, recent efforts have aimed to extend these methods to discover feature interactions, but existing approaches struggle with robustness and error control, especially under data perturbations. In this study, we introduce Diamond, a novel method for trustworthy feature interaction discovery. Diamond uniquely integrates the model-X knockoffs framework to control the false discovery rate (FDR), ensuring that the proportion of falsely discovered interactions remains low. We further address the challenges of using off-the-shelf interaction importance measures by proposing a calibration procedure that refines these measures to maintain the desired FDR. Diamond's applicability spans a wide range of ML models, including deep neural networks, tree-based models, and factorization-based models. Our empirical evaluations on both simulated and real datasets across various biomedical studies demonstrate Diamond's utility in enabling more reliable data-driven scientific discoveries. This method represents a significant step forward in the deployment of ML models for scientific innovation and hypothesis generation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17016v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Winston Chen, Yifan Jiang, William Stafford Noble, Yang Young Lu</dc:creator>
    </item>
    <item>
      <title>Non-parametric Monitoring of Spatial Dependence</title>
      <link>https://arxiv.org/abs/2408.17022</link>
      <description>arXiv:2408.17022v1 Announce Type: cross 
Abstract: In process monitoring applications, measurements are often taken regularly or randomly from different spatial locations in two or three dimensions. Here, we consider streams of regular, rectangular data sets and use spatial ordinal patterns (SOPs) as a non-parametric approach to detect spatial dependencies. A key feature of our proposed SOP charts is that they are distribution-free and do not require prior Phase-I analysis. We conduct an extensive simulation study, demonstrating the superiority and effectiveness of the proposed charts compared to traditional parametric approaches. We apply the SOP-based control charts to detect heavy rainfall in Germany, war-related fires in (eastern) Ukraine, and manufacturing defects in textile production. The wide range of applications and insights illustrate the broad utility of our non-parametric approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17022v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Ad\"ammer, Philipp Wittenberg, Christian H. Wei{\ss}, Murat Caner Testik</dc:creator>
    </item>
    <item>
      <title>Incorporating Memory into Continuous-Time Spatial Capture-Recapture Models</title>
      <link>https://arxiv.org/abs/2408.17278</link>
      <description>arXiv:2408.17278v1 Announce Type: cross 
Abstract: Obtaining reliable and precise estimates of wildlife species abundance and distribution is essential for the conservation and management of animal populations and natural reserves. Remote sensors such as camera traps are increasingly employed to gather data on uniquely identifiable individuals. Spatial capture-recapture (SCR) models provide estimates of population and spatial density from such data. These models introduce spatial correlation between observations of the same individual through a latent activity center. However SCR models assume that observations are independent over time and space, conditional on their given activity center, so that observed sightings at a given time and location do not influence the probability of being seen at future times and/or locations. With detectors like camera traps, this is ecologically unrealistic given the smooth movement of animals over space through time. We propose a new continuous-time modeling framework that incorporates both an individual's (latent) activity center and (known) previous location and time of detection. We demonstrate that standard SCR models can produce substantially biased density estimates when there is correlation in the times and locations of detections, and that our new model performs substantially better than standard SCR models on data simulated through a movement model as well as in a real camera trap study of American martens where an improvement in model fit is observed when incorporating the observed locations and times of previous observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17278v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Clara Panchaud, Ruth King, David Borchers, Hannah Worthington, Ian Durbach, Paul Van Dam-Bates</dc:creator>
    </item>
    <item>
      <title>The Functional Gait Deviation Index</title>
      <link>https://arxiv.org/abs/2310.06674</link>
      <description>arXiv:2310.06674v3 Announce Type: replace 
Abstract: A typical gait analysis requires the examination of the motion of nine joint angles on the left-hand side and six joint angles on the right-hand side across multiple subjects. Due to the quantity and complexity of the data, it is useful to calculate the amount by which a subject's gait deviates from an average normal profile and to represent this deviation as a single number. Such a measure can quantify the overall severity of a condition affecting walking, monitor progress, or evaluate the outcome of an intervention prescribed to improve the gait pattern. The gait deviation index, gait profile score, and the overall abnormality measure are standard benchmarks for quantifying gait abnormality. However, these indices do not account for the intrinsic smoothness of the gait movement at each joint/plane and the potential co-variation between the joints/planes. Utilizing a multivariate functional principal components analysis we propose the functional gait deviation index (FGDI). FGDI accounts for the intrinsic smoothness of the gait movement at each joint/plane and the potential co-variation between the joints. We show that FGDI scales with overall gait function, provides a consistent measure of gait abnormality, and is implemented easily using an interactive web app.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.06674v3</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sajal Kaur Minhas, Morgan Sangeux, Julia Polak, Michelle Carey</dc:creator>
    </item>
    <item>
      <title>A functional regression model for heterogeneous BioGeoChemical Argo data in the Southern Ocean</title>
      <link>https://arxiv.org/abs/2211.04012</link>
      <description>arXiv:2211.04012v2 Announce Type: replace-cross 
Abstract: Leveraging available measurements of our environment can help us understand complex processes. One example is Argo Biogeochemical data, which aims to collect measurements of oxygen, nitrate, pH, and other variables at varying depths in the ocean. We focus on the oxygen data in the Southern Ocean, which has implications for ocean biology and the Earth's carbon cycle. Systematic monitoring of such data has only recently begun to be established, and the data is sparse. In contrast, Argo measurements of temperature and salinity are much more abundant. In this work, we introduce and estimate a functional regression model describing dependence in oxygen, temperature, and salinity data at all depths covered by the Argo data simultaneously. Our model elucidates important aspects of the joint distribution of temperature, salinity, and oxygen. Due to fronts that establish distinct spatial zones in the Southern Ocean, we augment this functional regression model with a mixture component. By modelling spatial dependence in the mixture component and in the data itself, we provide predictions onto a grid and improve location estimates of fronts. Our approach is scalable to the size of the Argo data, and we demonstrate its success in cross-validation and a comprehensive interpretation of the model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.04012v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Moritz Korte-Stapff, Drew Yarger, Stilian Stoev, Tailen Hsing</dc:creator>
    </item>
    <item>
      <title>Backward Joint Model for the Dynamic Prediction of Both Competing Risk and Longitudinal Outcomes</title>
      <link>https://arxiv.org/abs/2311.00878</link>
      <description>arXiv:2311.00878v2 Announce Type: replace-cross 
Abstract: Joint modeling is a useful approach to dynamic prediction of clinical outcomes using longitudinally measured predictors. When the outcomes are competing risk events, fitting the conventional shared random effects joint model often involves intensive computation, especially when multiple longitudinal biomarkers are be used as predictors, as is often desired in prediction problems. This paper proposes a new joint model for the dynamic prediction of competing risk outcomes. The model factorizes the likelihood into the distribution of the competing risks data and the distribution of longitudinal data given the competing risks data. It extends the basic idea of the recently published backward joint model (BJM) to the competing risk setting, and we call this model crBJM. This model also enables the prediction of future longitudinal data trajectories conditional on being at risk at a future time, a practically important problem that has not been studied in the statistical literature. The model fitting with the EM algorithm is efficient, stable and computationally fast, with a one-dimensional integral in the E-step and convex optimization for most parameters in the M-step, regardless of the number of longitudinal predictors. The model also comes with a consistent albeit less efficient estimation method that can be quickly implemented with standard software, ideal for model building and diagnostics. We study the numerical properties of the proposed method using simulations and illustrate its use in a chronic kidney disease study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.00878v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 02 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wenhao Li, Brad C. Astor, Wei Yang, Tom H. Greene, Liang Li</dc:creator>
    </item>
  </channel>
</rss>
