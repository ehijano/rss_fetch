<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-fin.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-fin.ST</link>
    <description>q-fin.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-fin.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 24 Dec 2025 02:30:39 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inferring Latent Market Forces: Evaluating LLM Detection of Gamma Exposure Patterns via Obfuscation Testing</title>
      <link>https://arxiv.org/abs/2512.17923</link>
      <description>arXiv:2512.17923v1 Announce Type: new 
Abstract: We introduce obfuscation testing, a novel methodology for validating whether large language models detect structural market patterns through causal reasoning rather than temporal association. Testing three dealer hedging constraint patterns (gamma positioning, stock pinning, 0DTE hedging) on 242 trading days (95.6% coverage) of S&amp;P 500 options data, we find LLMs achieve 71.5% detection rate using unbiased prompts that provide only raw gamma exposure values without regime labels or temporal context. The WHO-WHOM-WHAT causal framework forces models to identify the economic actors (dealers), affected parties (directional traders), and structural mechanisms (forced hedging) underlying observed market dynamics. Critically, detection accuracy (91.2%) remains stable even as economic profitability varies quarterly, demonstrating that models identify structural constraints rather than profitable patterns. When prompted with regime labels, detection increases to 100%, but the 71.5% unbiased rate validates genuine pattern recognition. Our findings suggest LLMs possess emergent capabilities for detecting complex financial mechanisms through pure structural reasoning, with implications for systematic strategy development, risk management, and our understanding of how transformer architectures process financial market dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17923v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher Regan, Ying Xie</dc:creator>
    </item>
    <item>
      <title>Stylized Facts and Their Microscopic Origins: Clustering, Persistence, and Stability in a 2D Ising Framework</title>
      <link>https://arxiv.org/abs/2512.17925</link>
      <description>arXiv:2512.17925v1 Announce Type: new 
Abstract: The analysis of financial markets using models inspired by statistical physics offers a fruitful approach to understand collective and extreme phenomena [3, 14, 15] In this paper, we present a study based on a 2D Ising network model where each spin represents an agent that interacts only with its immediate neighbors plus a term reated to the mean field [1, 2]. From this simple formulation, we analyze the formation of spin clusters, their temporal persistence, and the morphological evolution of the system as a function of temperature [5, 19]. Furthermore, we introduce the study of the quantity $1/2P\sum_{i}|S_{i}(t)+S_{i}(t+\Delta t)|$, which measures the absolute overlap between consecutive configurations and quantifies the degree of instantaneous correlation between system states. The results show that both the morphology and persistence of the clusters and the dynamics of the absolute sum can explain universal statistical properties observed in financial markets, known as stylized facts [2, 12, 18]: sharp peaks in returns, distributions with heavy tails, and zero autocorrelation. The critical structure of clusters and their reorganization over time thus provide a microscopic mechanism that gives rise to the intermittency and clustered volatility observed in prices [2, 15].</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17925v1</guid>
      <category>q-fin.ST</category>
      <category>nlin.AO</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>Physica A (2025)</arxiv:journal_reference>
      <dc:creator>Hern\'an Ezequiel Ben\'itez, Claudio Oscar Dorso</dc:creator>
    </item>
    <item>
      <title>Reinforcement Learning for Monetary Policy Under Macroeconomic Uncertainty: Analyzing Tabular and Function Approximation Methods</title>
      <link>https://arxiv.org/abs/2512.17929</link>
      <description>arXiv:2512.17929v1 Announce Type: new 
Abstract: We study how a central bank should dynamically set short-term nominal interest rates to stabilize inflation and unemployment when macroeconomic relationships are uncertain and time-varying. We model monetary policy as a sequential decision-making problem where the central bank observes macroeconomic conditions quarterly and chooses interest rate adjustments. Using publically accessible historical Federal Reserve Economic Data (FRED), we construct a linear-Gaussian transition model and implement a discrete-action Markov Decision Process with a quadratic loss reward function. We chose to compare nine different reinforcement learning style approaches against Taylor Rule and naive baselines, including tabular Q-learning variants, SARSA, Actor-Critic, Deep Q-Networks, Bayesian Q-learning with uncertainty quantification, and POMDP formulations with partial observability. Surprisingly, standard tabular Q-learning achieved the best performance (-615.13 +- 309.58 mean return), outperforming both enhanced RL methods and traditional policy rules. Our results suggest that while sophisticated RL techniques show promise for monetary policy applications, simpler approaches may be more robust in this domain, highlighting important challenges in applying modern RL to macroeconomic policy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17929v1</guid>
      <category>q-fin.ST</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheryl Chen, Tony Wang, Kyle Feinstein</dc:creator>
    </item>
    <item>
      <title>Risk-Aware Financial Forecasting Enhanced by Machine Learning and Intuitionistic Fuzzy Multi-Criteria Decision-Making</title>
      <link>https://arxiv.org/abs/2512.17936</link>
      <description>arXiv:2512.17936v1 Announce Type: new 
Abstract: In the face of increasing financial uncertainty and market complexity, this study presents a novel risk-aware financial forecasting framework that integrates advanced machine learning techniques with intuitionistic fuzzy multi-criteria decision-making (MCDM). Tailored to the BIST 100 index and validated through a case study of a major defense company in T\"urkiye, the framework fuses structured financial data, unstructured text data, and macroeconomic indicators to enhance predictive accuracy and robustness. It incorporates a hybrid suite of models, including extreme gradient boosting (XGBoost), long short-term memory (LSTM) network, graph neural network (GNN), to deliver probabilistic forecasts with quantified uncertainty. The empirical results demonstrate high forecasting accuracy, with a net profit mean absolute percentage error (MAPE) of 3.03% and narrow 95% confidence intervals for key financial indicators. The risk-aware analysis indicates a favorable risk-return profile, with a Sharpe ratio of 1.25 and a higher Sortino ratio of 1.80, suggesting relatively low downside volatility and robust performance under market fluctuations. Sensitivity analysis shows that the key financial indicator predictions are highly sensitive to variations of inflation, interest rates, sentiment, and exchange rates. Additionally, using an intuitionistic fuzzy MCDM approach, combining entropy weighting, evaluation based on distance from the average solution (EDAS), and the measurement of alternatives and ranking according to compromise solution (MARCOS) methods, the tabular data learning network (TabNet) outperforms the other models and is identified as the most suitable candidate for deployment. Overall, the findings of this work highlight the importance of integrating advanced machine learning, risk quantification, and fuzzy MCDM methodologies in financial forecasting, particularly in emerging markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17936v1</guid>
      <category>q-fin.ST</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Safiye Turgay, Serkan Erdo\u{g}an, \v{Z}eljko Stevi\'c, Orhan Emre Elma, Tevfik Eren, Zhiyuan Wang, Mahmut Bayda\c{s}</dc:creator>
    </item>
    <item>
      <title>What's the Price of Monotonicity? A Multi-Dataset Benchmark of Monotone-Constrained Gradient Boosting for Credit PD</title>
      <link>https://arxiv.org/abs/2512.17945</link>
      <description>arXiv:2512.17945v1 Announce Type: cross 
Abstract: Financial institutions face a trade-off between predictive accuracy and interpretability when deploying machine learning models for credit risk. Monotonicity constraints align model behavior with domain knowledge, but their performance cost - the price of monotonicity - is not well quantified. This paper benchmarks monotone-constrained versus unconstrained gradient boosting models for credit probability of default across five public datasets and three libraries. We define the Price of Monotonicity (PoM) as the relative change in standard performance metrics when moving from unconstrained to constrained models, estimated via paired comparisons with bootstrap uncertainty. In our experiments, PoM in AUC ranges from essentially zero to about 2.9 percent: constraints are almost costless on large datasets (typically less than 0.2 percent, often indistinguishable from zero) and most costly on smaller datasets with extensive constraint coverage (around 2-3 percent). Thus, appropriately specified monotonicity constraints can often deliver interpretability with small accuracy losses, particularly in large-scale credit portfolios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.17945v1</guid>
      <category>cs.LG</category>
      <category>q-fin.RM</category>
      <category>q-fin.ST</category>
      <pubDate>Tue, 23 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petr Koklev</dc:creator>
    </item>
  </channel>
</rss>
