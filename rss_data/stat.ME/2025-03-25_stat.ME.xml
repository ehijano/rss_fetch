<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 26 Mar 2025 02:15:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Covariate Dependent Bayesian Deep Latent Class Model</title>
      <link>https://arxiv.org/abs/2503.17531</link>
      <description>arXiv:2503.17531v1 Announce Type: new 
Abstract: High-dimensional categorical data arise in diverse scientific domains and are often accompanied by covariates. Latent class regression models are routinely used in such settings, reducing dimensionality by assuming conditional independence of the categorical variables given a single latent class that depends on covariates through a logistic regression model. However, such methods become unreliable as the dimensionality increases. To address this, we propose a flexible family of deep latent class models. Our model satisfies key theoretical properties, including identifiability and posterior consistency, and we establish a Bayes oracle clustering property that ensures robustness against the curse of dimensionality. We develop efficient posterior computation methods, validate them through simulation studies, and apply our model to joint species distribution modeling in ecology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17531v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuren Zhou, Yuqi Gu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Principal Component Analysis When n &lt; p: Challenges and Solutions</title>
      <link>https://arxiv.org/abs/2503.17560</link>
      <description>arXiv:2503.17560v1 Announce Type: new 
Abstract: Principal Component Analysis is a key technique for reducing the complexity of high-dimensional data while preserving its fundamental data structure, ensuring models remain stable and interpretable. This is achieved by transforming the original variables into a new set of uncorrelated variables (principal components) based on the covariance structure of the original variables. However, since the traditional maximum likelihood covariance estimator does not accurately converge to the true covariance matrix, the standard principal component analysis performs poorly as a dimensionality reduction technique in high-dimensional scenarios $n&lt;p$. In this study, inspired by a fundamental issue associated with mean estimation when $n&lt;p$, we proposed a novel estimation called pairwise differences covariance estimation with four regularized versions of it to address the issues with the principal component analysis when n &lt; p high dimensional data settings. In empirical comparisons with existing methods (maximum likelihood estimation and its best alternative method called Ledoit-Wolf estimation) and the proposed method(s), all the proposed regularized versions of pairwise differences covariance estimation perform well compared to those well-known estimators in estimating the covariance and principal components while minimizing the PCs' overdispersion and cosine similarity error. Real data applications are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17560v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nuwan Weeraratne, Lyn Hunt, Jason Kurz</dc:creator>
    </item>
    <item>
      <title>A Joint Model of Longitudinal CVD Risk Factors, Medication Use, and Time-to-Terminal Events</title>
      <link>https://arxiv.org/abs/2503.17576</link>
      <description>arXiv:2503.17576v1 Announce Type: new 
Abstract: We introduce a novel Bayesian approach for jointly modeling longitudinal cardiovascular disease (CVD) risk factor trajectories, medication use, and time-to-events. Our methodology incorporates longitudinal risk factor trajectories into the time-to-event model, considers the temporal aspect of medication use, incorporates uncertainty due to missing medication status and medication switching, and analyzes the impact of medications on CVD events. Our research aims to provide a comprehensive understanding of the effect of CVD progression and medication use on time to death, enhancing predictive accuracy and informing personalized intervention strategies. Using data from a cardiovascular cohort study, we demonstrate the model's ability to capture detailed temporal dynamics and enhance predictive accuracy for CVD events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17576v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeynab Aghabazaz, Michael J Daniels, Donald M Lloyd-Jones, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Combining longitudinal cohort studies to examine cardiovascular risk factor trajectories across the adult lifespan</title>
      <link>https://arxiv.org/abs/2503.17606</link>
      <description>arXiv:2503.17606v1 Announce Type: new 
Abstract: We introduce a statistical framework for combining data from multiple large longitudinal cardiovascular cohorts to enable the study of long-term cardiovascular health starting in early adulthood. Using data from seven cohorts belonging to the Lifetime Risk Pooling Project (LRPP), we present a Bayesian hierarchical multivariate approach that jointly models multiple longitudinal risk factors over time and across cohorts. Because few cohorts in our project cover the entire adult lifespan, our strategy uses information from all risk factors to increase precision for each risk factor trajectory and borrows information across cohorts to fill in unobserved risk factors. We develop novel diagnostic testing and model validation methods to ensure that our model robustly captures and maintains critical relationships over time and across risk factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17606v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zeynab Aghabazaz, Michael J Daniels, Hongyan Ning, Juned Siddique</dc:creator>
    </item>
    <item>
      <title>Graph-based Change Point Detection for Functional Data</title>
      <link>https://arxiv.org/abs/2503.17648</link>
      <description>arXiv:2503.17648v1 Announce Type: new 
Abstract: Modeling functions that are sequentially observed as functional time series is becoming increasingly common. In such models, it is often crucial to ensure data homogeneity. We investigate the sensitivity of graph-based change point detection for changes in the distribution of functional data that demarcate homogeneous regions. Related test statistics and thresholds for detection are given. A key factor in the efficacy of such tests is the graph construction. Practical considerations for constructing a graph on arbitrary data are explored. Simulation experiments investigate tuning parameters for graph construction and evaluate the graph-based methods in comparison to existing functional methods. In addition to sensitivity of lower and higher order changes, robustness to the tuning parameter choices, and practical recommendations, are shown. Applications to multi-year pedestrian counts, high-frequency asset returns, and continuous electricity prices corroborate the simulation results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17648v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeremy VanderDoes, Shojaeddin Chenouri</dc:creator>
    </item>
    <item>
      <title>Graphical Transformation Models</title>
      <link>https://arxiv.org/abs/2503.17845</link>
      <description>arXiv:2503.17845v1 Announce Type: new 
Abstract: Graphical Transformation Models (GTMs) are introduced as a novel approach to effectively model multivariate data with intricate marginals and complex dependency structures non-parametrically, while maintaining interpretability through the identification of varying conditional independencies. GTMs extend multivariate transformation models by replacing the Gaussian copula with a custom-designed multivariate transformation, offering two major advantages. Firstly, GTMs can capture more complex interdependencies using penalized splines, which also provide an efficient regularization scheme. Secondly, we demonstrate how to approximately regularize GTMs using a lasso penalty towards pairwise conditional independencies, akin to Gaussian graphical models. The model's robustness and effectiveness are validated through simulations, showcasing its ability to accurately learn parametric vine copulas and identify conditional independencies. Additionally, the model is applied to a benchmark astrophysics dataset, where the GTM demonstrates favorable performance compared to non-parametric vine copulas in learning complex multivariate distributions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17845v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Herp, Johannes Brachem, Michael Altenbuchinger, Thomas Kneib</dc:creator>
    </item>
    <item>
      <title>A Causal Analysis of the Plots of Intelligent Adversaries</title>
      <link>https://arxiv.org/abs/2503.17863</link>
      <description>arXiv:2503.17863v1 Announce Type: new 
Abstract: In this paper we demonstrate a new advance in causal Bayesian graphical modelling combined with Adversarial Risk Analysis. This research aims to support strategic analyses of various defensive interventions to counter the threat arising from plots of an adversary. These plots are characterised by a sequence of preparatory phases that an adversary must necessarily pass through to achieve their hostile objective. To do this we first define a new general class of plot models. Then we demonstrate that this is a causal graphical family of models - albeit with a hybrid semantic. We show this continues to be so even in this adversarial setting. It follows that this causal graph can be used to guide a Bayesian decision analysis to counter the adversary's plot. We illustrate the causal analysis of a plot with details of a decision analysis designed to frustrate the progress of a planned terrorist attack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17863v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Preetha Ramiah, David I. Hastie, Oliver Bunnin, Silvia Liverani, James Q. Smith</dc:creator>
    </item>
    <item>
      <title>Supervised Manifold Learning for Functional Data</title>
      <link>https://arxiv.org/abs/2503.17943</link>
      <description>arXiv:2503.17943v1 Announce Type: new 
Abstract: Classification is a core topic in functional data analysis. A large number of functional classifiers have been proposed in the literature, most of which are based on functional principal component analysis or functional regression. In contrast, we investigate this topic from the perspective of manifold learning. It is assumed that functional data lie on an unknown low-dimensional manifold, and we expect that better classifiers can be built upon the manifold structure. To this end, we propose a novel proximity measure that takes the label information into account to learn the low-dimensional representations, also known as the supervised manifold learning outcomes. When the outcomes are coupled with multivariate classifiers, the procedure induces a family of new functional classifiers. In theory, we show that our functional classifier induced by the $k$-NN classifier is asymptotically optimal. In practice, we show that our method, coupled with several classical multivariate classifiers, achieves outstanding classification performance compared to existing functional classifiers in both synthetic and real data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17943v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ruoxu Tan, Yiming Zang</dc:creator>
    </item>
    <item>
      <title>Variational inference for hierarchical models with conditional scale and skewness corrections</title>
      <link>https://arxiv.org/abs/2503.18075</link>
      <description>arXiv:2503.18075v2 Announce Type: new 
Abstract: Gaussian variational approximations are widely used for summarizing posterior distributions in Bayesian models, especially in high-dimensional settings. However, a drawback of such approximations is the inability to capture skewness or more complex features of the posterior. Recent work suggests applying skewness corrections to existing Gaussian or other symmetric approximations to address this limitation. We propose to incorporate the skewness correction into the definition of an approximating variational family. We consider approximating the posterior for hierarchical models, in which there are ``global'' and ``local'' parameters. A baseline variational approximation is defined as the product of a Gaussian marginal posterior for global parameters and a Gaussian conditional posterior for local parameters given the global ones. Skewness corrections are then considered. The adjustment of the conditional posterior term for local variables is adaptive to the global parameter value. Optimization of baseline variational parameters is performed jointly with the skewness correction. Our approach allows the location, scale and skewness to be captured separately, without using additional parameters for skewness adjustments. The proposed method substantially improves accuracy for only a modest increase in computational cost compared to state-of-the-art Gaussian approximations. Good performance is demonstrated in generalized linear mixed models and multinomial logit discrete choice models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18075v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucas Kock, Linda S. L. Tan, Prateek Bansal, David J. Nott</dc:creator>
    </item>
    <item>
      <title>Efficient Inference for Covariate-adjusted Bradley-Terry Model with Covariate Shift</title>
      <link>https://arxiv.org/abs/2503.18256</link>
      <description>arXiv:2503.18256v1 Announce Type: new 
Abstract: We propose a general framework for statistical inference on the overall strengths of players in pairwise comparisons, allowing for potential shifts in the covariate distribution. These covariates capture important contextual information that may impact the winning probability of each player. We measure the overall strengths of players under a target distribution through its Kullback-Leibler projection onto a class of covariate-adjusted Bradley-Terry model. Consequently, our estimands remain well-defined without requiring stringent model assumptions. We develop semiparametric efficient estimators and corresponding inferential procedures that allow for flexible estimation of the nuisance functions. When the conditional Bradley-Terry assumption holds, we propose additional estimators that do not require observing all pairwise comparisons. We demonstrate the performance of our proposed method in simulation studies and apply it to assess the alignment of large language models with human preferences in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18256v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Xiudi Li, Sijia Li</dc:creator>
    </item>
    <item>
      <title>Bayesian model criticism using uniform parametrization checks</title>
      <link>https://arxiv.org/abs/2503.18261</link>
      <description>arXiv:2503.18261v1 Announce Type: new 
Abstract: Models are often misspecified in practice, making model criticism a key part of Bayesian analysis. It is important to detect not only when a model is wrong, but which aspects are wrong, and to do so in a computationally convenient and statistically rigorous way. We introduce a novel method for model criticism based on the fact that if the parameters are drawn from the prior, and the dataset is generated according to the assumed likelihood, then a sample from the posterior will be distributed according to the prior. Thus, departures from the assumed likelihood or prior can be detected by testing whether a posterior sample could plausibly have been generated by the prior. Building upon this idea, we propose to reparametrize all random elements of the likelihood and prior in terms of independent uniform random variables, or u-values. This makes it possible to aggregate across arbitrary subsets of the u-values for data points and parameters to test for model departures using classical hypothesis tests for dependence or non-uniformity. We demonstrate empirically how this method of uniform parametrization checks (UPCs) facilitates model criticism in several examples, and we develop supporting theoretical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18261v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian T. Covington, Jeffrey W. Miller</dc:creator>
    </item>
    <item>
      <title>Fitting multivariate Hawkes processes to interval count data with an application to terrorist activity modelling -- a particle Markov chain Monte Carlo approach</title>
      <link>https://arxiv.org/abs/2503.18351</link>
      <description>arXiv:2503.18351v1 Announce Type: new 
Abstract: Terrorist activities often exhibit temporal and spatial clustering, making the multivariate Hawkes process (MHP) a useful statistical model for analysing terrorism across different geographic regions. However, terror attack data from the Global Terrorism Database is reported as total event counts in disjoint observation periods, with precise event times unknown. When the MHP is only observed discretely, the likelihood function becomes intractable, hindering likelihood-based inference. To address this, we design an unbiased estimate of the intractable likelihood function using sequential Monte Carlo (SMC) based on a representation of the unobserved event times as latent variables in a state-space model. The unbiasedness of the SMC estimate allows for its use in place of the true likelihood in a Metropolis-Hastings algorithm, from which we construct a Markov Chain Monte Carlo sample of the distribution over the parameters of the MHP. Using simulated data, we assess the performance of our method and demonstrate that it outperforms an alternative method in the literature based on mean squared error. Terrorist activity in Afghanistan and Pakistan from 2018 to 2021 is analysed based on daily count data to examine the self- and cross-excitation effects of terrorism events.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18351v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jason J. Lambe, Feng Chen, Tom Stindl, Tsz-Kit Jeffrey Kwan</dc:creator>
    </item>
    <item>
      <title>Efficient Inference in First Passage Time Models</title>
      <link>https://arxiv.org/abs/2503.18381</link>
      <description>arXiv:2503.18381v1 Announce Type: new 
Abstract: First passage time models describe the time it takes for a random process to exit a region of interest and are widely used across various scientific fields. Fast and accurate numerical methods for computing the likelihood function in these models are essential for efficient statistical inference. Specifically, in mathematical psychology, generalized drift diffusion models (GDDMs) are an important class of first passage time models that describe the latent psychological processes underlying simple decision-making scenarios. GDDMs model the joint distribution over choices and response times as the first hitting time of a one-dimensional stochastic differential equation (SDE) to possibly time-varying upper and lower boundaries. They are widely applied to extract parameters associated with distinct cognitive and neural mechanisms. However, current likelihood computation methods struggle with common scenarios where drift rates covary dynamically with exogenous covariates in each trial, such as in the attentional drift diffusion model (aDDM). In this work, we propose a fast and flexible algorithm for computing the likelihood function of GDDMs based on a large class of SDEs satisfying the Cherkasov condition. Our method divides each trial into discrete stages, employs fast analytical results to compute stage-wise densities, and integrates these to compute the overall trial-wise likelihood. Numerical examples demonstrate that our method not only yields accurate likelihood evaluations for efficient statistical inference, but also significantly outperforms existing approaches in terms of speed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18381v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sicheng Liu, Alexander Fengler, Michael J. Frank, Matthew T. Harrison</dc:creator>
    </item>
    <item>
      <title>Similarity-Informed Transfer Learning for Multivariate Functional Censored Quantile Regression</title>
      <link>https://arxiv.org/abs/2503.18437</link>
      <description>arXiv:2503.18437v1 Announce Type: new 
Abstract: To address the challenge of utilizing patient data from other organ transplant centers (source cohorts) to improve survival time estimation and inference for a target center (target cohort) with limited samples and strict data-sharing privacy constraints, we propose the Similarity-Informed Transfer Learning (SITL) method. This approach estimates multivariate functional censored quantile regression by flexibly leveraging information from each source cohort based on its similarity to the target cohort. Furthermore, the method is adaptable to continuously updated real-time data. We establish the asymptotic properties of the estimators obtained using the SITL method, demonstrating improved convergence rates. Additionally, we develop an enhanced approach that combines the SITL method with a resampling technique to construct more accurate confidence intervals for functional coefficients, backed by theoretical guarantees. Extensive simulation studies and an application to kidney transplant data illustrate the significant advantages of the SITL method. Compared to methods that rely solely on the target cohort or indiscriminately pool data across source and target cohorts, the SITL method substantially improves both estimation and inference performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18437v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hua Liu, Jiaqi Men, Shouxia Wang, Jinhong You, Jiguo Cao</dc:creator>
    </item>
    <item>
      <title>A tuning-free and scalable method for joint graphical model estimation with sharper bounds</title>
      <link>https://arxiv.org/abs/2503.18722</link>
      <description>arXiv:2503.18722v1 Announce Type: new 
Abstract: Joint estimation of multiple graphical models (i.e., multiple precision matrices) has emerged as an important topic in statistics. Unlike separate estimation, joint estimation can leverage shared structural patterns across multiple graphs to yield more accurate results. In this paper, we present an efficient and tuning-free method named MIGHT (Multi-task Iterative Graphical Hard Thresholding) to jointly estimate multiple graphical models. We reformulate the joint model into a series of multi-task learning problems in a column-by-column manner, and then solve these problems by using an iterative algorithm based on the hard thresholding operator. Theoretically, we derive the non-asymptotic error bound for our method. We prove that, under proper signal conditions, our method attains selection consistency and an improved error bound, and also exhibits asymptotic normality -- properties rarely explored in existing joint graphical model estimation literature. The performance of our method is validated through numerical simulations and real data analysis of a cancer gene-expression RNA-seq dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18722v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shixiang Liu, Yanhang Zhang, Zhifan Li, Jianxin Yin</dc:creator>
    </item>
    <item>
      <title>Local Interference: Removing Interference Bias in Semi-Parametric Causal Models</title>
      <link>https://arxiv.org/abs/2503.18756</link>
      <description>arXiv:2503.18756v1 Announce Type: new 
Abstract: Interference bias is a major impediment to identifying causal effects in real-world settings. For example, vaccination reduces the transmission of a virus in a population such that everyone benefits -- even those who are not treated. This is a source of bias that must be accounted for if one wants to learn the true effect of a vaccine on an individual's immune system. Previous approaches addressing interference bias require strong domain knowledge in the form of a graphical interaction network fully describing interference between units. Moreover, they place additional constraints on the form the interference can take, such as restricting to linear outcome models, and assuming that interference experienced by a unit does not depend on the unit's covariates. Our work addresses these shortcomings. We first provide and justify a novel definition of causal models with local interference. We prove that the True Average Causal Effect, a measure of causality where interference has been removed, can be identified in certain semi-parametric models satisfying this definition. These models allow for non-linearity, and also for interference to depend on a unit's covariates. An analytic estimand for the True Average Causal Effect is given in such settings. We further prove that the True Average Causal Effect cannot be identified in arbitrary models with local interference, showing that identification requires semi-parametric assumptions. Finally, we provide an empirical validation of our method on both simulated and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18756v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael O'Riordan, Ciar\'an M. Gilligan-Lee</dc:creator>
    </item>
    <item>
      <title>Confidence set for mixture order selection</title>
      <link>https://arxiv.org/abs/2503.18790</link>
      <description>arXiv:2503.18790v1 Announce Type: new 
Abstract: A fundamental challenge in the application of finite mixture models is selecting the number of mixture components, also known as order. Traditional approaches rely on selecting a single best model using information criteria. However, in the presence of noisy data, and when models with different orders yield similar fits, model selection uncertainty can be substantial, making it challenging to confidently identify the true number of components. In this paper, we introduce the Model Selection Confidence Set (MSCS) for order selection - a set-valued estimator that, with a predefined confidence level, includes the true mixture order across repeated samples. Rather than selecting a single model, our MSCS identifies all plausible orders by determining whether each candidate model is at least as plausible as the best-selected one, using a screening test based on a penalized likelihood ratio statistic. We provide theoretical guarantees for the asymptotic coverage of our confidence set and demonstrate its practical advantages through simulations and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18790v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari</dc:creator>
    </item>
    <item>
      <title>A primer on inference and prediction with epidemic renewal models and sequential Monte Carlo</title>
      <link>https://arxiv.org/abs/2503.18875</link>
      <description>arXiv:2503.18875v1 Announce Type: new 
Abstract: Renewal models are widely used in statistical epidemiology as semi-mechanistic models of disease transmission. While primarily used for estimating the instantaneous reproduction number, they can also be used for generating projections, estimating elimination probabilities, modelling the effect of interventions, and more. We demonstrate how simple sequential Monte Carlo methods (also known as particle filters) can be used to perform inference on these models. Our goal is to acquaint a reader who has a working knowledge of statistical inference with these methods and models and to provide a practical guide to their implementation. We focus on these methods' flexibility and their ability to handle multiple statistical and other biases simultaneously. We leverage this flexibility to unify existing methods for estimating the instantaneous reproduction number and generating projections. A companion website "SMC and epidemic renewal models" provides additional worked examples, self-contained code to reproduce the examples presented here, and additional materials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18875v1</guid>
      <category>stat.ME</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nicholas Steyn, Kris V. Parag, Robin N. Thompson, Christl A. Donnelly</dc:creator>
    </item>
    <item>
      <title>Beyond Group Means and Into the World of Individuals: A Distributional Spotlight for Experimental Effects on Individuals</title>
      <link>https://arxiv.org/abs/2503.17390</link>
      <description>arXiv:2503.17390v1 Announce Type: cross 
Abstract: Traditionally, experimental effects on humans are investigated at the group level. In this work, we present a distributional "spotlight" to investigate experimental effects at the individual level. Specifically, we estimate the effects on individuals through the changes in the probability distributions of their experimental data across conditions. We test this approach on Reaction Time (RT) data from 10 individuals in a visual search task, examining the effects of (1) information set sizes and (2) the presence or absence of a target on their processing speed. The changes in individuals' RT distributions are measured using three approaches: (i) direct measurements of distributional changes are compared against the changes captured by two established models of RT: (ii) the ex-Gaussian distribution and (iii) the Drift-Diffusion model. We find that direct measurement of distributional changes provides the clearest view of the effects on individuals and highlights the presence of two sub-groups based on the effects experienced: one that shows neither effect and the other showing only the target-presence effect. Moreover, the intra-individual changes across conditions (i.e., the experimental effects) appear much smaller than the intra-individual differences (i.e., the random effects). Generally, these results highlight the merits of going beyond group means and examining the effects on individuals, as well as the effectiveness of the distributional spotlight in such pursuits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17390v1</guid>
      <category>physics.soc-ph</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roussel Rahman</dc:creator>
    </item>
    <item>
      <title>A Roadmap Towards Improving Multi-Agent Reinforcement Learning With Causal Discovery And Inference</title>
      <link>https://arxiv.org/abs/2503.17803</link>
      <description>arXiv:2503.17803v1 Announce Type: cross 
Abstract: Causal reasoning is increasingly used in Reinforcement Learning (RL) to improve the learning process in several dimensions: efficacy of learned policies, efficiency of convergence, generalisation capabilities, safety and interpretability of behaviour. However, applications of causal reasoning to Multi-Agent RL (MARL) are still mostly unexplored. In this paper, we take the first step in investigating the opportunities and challenges of applying causal reasoning in MARL. We measure the impact of a simple form of causal augmentation in state-of-the-art MARL scenarios increasingly requiring cooperation, and with state-of-the-art MARL algorithms exploiting various degrees of collaboration between agents. Then, we discuss the positive as well as negative results achieved, giving us the chance to outline the areas where further research may help to successfully transfer causal RL to the multi-agent setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17803v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Giovanni Briglia, Stefano Mariani, Franco Zambonelli</dc:creator>
    </item>
    <item>
      <title>Two-Sample Tests for Optimal Lifts, Manifold Stability and Reverse Labeling Reflection Shap</title>
      <link>https://arxiv.org/abs/2503.17879</link>
      <description>arXiv:2503.17879v1 Announce Type: cross 
Abstract: We consider a quotient of a complete Riemannian manifold modulo an isometrically and properly acting Lie group and lifts of the quotient to the manifolds in optimal position to a reference point on the manifold. With respect to the pushed forward Riemannian volume onto the quotient we derive continuity and uniqueness a.e. and smoothness to large extents also with respect to the reference point. In consequence we derive a general manifold stability theorem: the Fr\'echet mean lies in the highest dimensional stratum assumed with positive probability, and a strong law for optimal lifts. This allows to define new two-sample tests utilizing individual optimal lifts which outperform existing two-sample tests on simulated data. They also outperform existing tests on a newly derived reverse labeling reflection shape space, that is used to model filament data of microtubules within cells in a biological application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17879v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Do Tran Van, Susovan Pal, Benjamin Eltzner, Stephan F. Huckemann</dc:creator>
    </item>
    <item>
      <title>Minimax Rate-Optimal Inference for Individualized Quantile Treatment Effects in High-dimensional Models</title>
      <link>https://arxiv.org/abs/2503.18523</link>
      <description>arXiv:2503.18523v1 Announce Type: cross 
Abstract: The quantification of treatment effects plays an important role in a wide range of applications, including policy making and bio-pharmaceutical research. In this article, we study the quantile treatment effect (QTE) while addressing two specific types of heterogeneities: (a) personalized heterogeneity, which captures the varying treatment effects for different individuals, and (b) quantile heterogeneity, which accounts for how the impact of covariates varies across different quantile levels. A well-designed debiased estimator for the individualized quantile treatment effect (IQTE) is proposed to capture such heterogeneities effectively. We show that this estimator converges weakly to a Gaussian process as a function of the quantile levels and propose valid statistical inference methods, including the construction of confidence intervals and the development of hypothesis testing decision rules. In addition, the minimax optimality frameworks for these inference procedures are established. Specifically, we derive the minimax optimal rates for the expected length of confidence intervals and the magnitude of the detection boundary for hypothesis testing procedures, illustrating the superiority of the proposed estimator. The effectiveness of our methods is demonstrated through extensive simulations and an analysis of the National Health and Nutrition Examination Survey (NHANES) datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18523v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiachen Sun, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Differentially Private Joint Independence Test</title>
      <link>https://arxiv.org/abs/2503.18721</link>
      <description>arXiv:2503.18721v1 Announce Type: cross 
Abstract: Identification of joint dependence among more than two random vectors plays an important role in many statistical applications, where the data may contain sensitive or confidential information. In this paper, we consider the the d-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of differential privacy. Given the limiting distribution of the empirical estimate of dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy regime is typically based on permutation and bootstrap. To detect joint dependence in privacy, we propose a dHSIC-based testing procedure by employing a differentially private permutation methodology. Our method enjoys privacy guarantee, valid level and pointwise consistency, while the bootstrap counterpart suffers inconsistent power. We further investigate the uniform power of the proposed test in dHSIC metric and $L_2$ metric, indicating that the proposed test attains the minimax optimal power across different privacy regimes. As a byproduct, our results also contain the pointwise and uniform power of the non-private permutation dHSIC, addressing an unsolved question remained in Pfister et al. (2018).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18721v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingwei Liu, Yuexin Chen, Wangli Xu</dc:creator>
    </item>
    <item>
      <title>Confidences in Hypotheses</title>
      <link>https://arxiv.org/abs/2111.10715</link>
      <description>arXiv:2111.10715v5 Announce Type: replace 
Abstract: This article extends the hypotheses assessment method to the case with two competing simple hypotheses. In doing so we further clarify the benefits that hypotheses assessments can bring to classical statistical analyses. Given that confidences in hypotheses are based on conditional probabilities, we address the issue of what to condition on in order to avoid poor conditional properties. This step is essential if the resulting inferences are to be relevant to the data at hand. Admissibility is addressed within a framework of seeking confidences that are relevant to the data at hand and are as powerful as the application allows. Confidence procedures are said to be consistent if they are free of super-relevant betting strategies. For simple hypotheses, the assessment method produces minimum and maximum confidences in each hypothesis. Assessments for both symmetric and asymmetric experiments are included, and the relationship with Bayesian posterior probabilities discussed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2111.10715v5</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Graham N. Bornholt</dc:creator>
    </item>
    <item>
      <title>Causality-oriented robustness: exploiting general noise interventions</title>
      <link>https://arxiv.org/abs/2307.10299</link>
      <description>arXiv:2307.10299v2 Announce Type: replace 
Abstract: Since distribution shifts are common in real-world applications, there is a pressing need to develop prediction models that are robust against such shifts. Existing frameworks, such as empirical risk minimization or distributionally robust optimization, either lack generalizability for unseen distributions or rely on postulated distance measures. Alternatively, causality offers a data-driven and structural perspective to robust predictions. However, the assumptions necessary for causal inference can be overly stringent, and the robustness offered by such causal models often lacks flexibility. In this paper, we focus on causality-oriented robustness and propose Distributional Robustness via Invariant Gradients (DRIG), a method that exploits general noise interventions in training data for robust predictions against unseen interventions, and naturally interpolates between in-distribution prediction and causality. In a linear setting, we prove that DRIG yields predictions that are robust among a data-dependent class of distribution shifts. Furthermore, we show that our framework includes anchor regression as a special case, and that it yields prediction models that protect against more diverse perturbations. We establish finite-sample results and extend our approach to semi-supervised domain adaptation to further improve prediction performance. Finally, we empirically validate our methods on synthetic simulations and on single-cell and intensive health care datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10299v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinwei Shen, Peter B\"uhlmann, Armeen Taeb</dc:creator>
    </item>
    <item>
      <title>Sample-Efficient "Clustering and Conquer" Procedures for Parallel Large-Scale Ranking and Selection</title>
      <link>https://arxiv.org/abs/2402.02196</link>
      <description>arXiv:2402.02196v4 Announce Type: replace 
Abstract: This work seeks to break the sample efficiency bottleneck in parallel large-scale ranking and selection (R&amp;S) problems by leveraging correlation information. We modify the commonly used "divide and conquer" framework in parallel computing by adding a correlation-based clustering step, transforming it into "clustering and conquer". This seemingly simple modification achieves the optimal sample complexity reduction for a widely used class of efficient large-scale R&amp;S procedures. Our approach enjoys two key advantages: 1) it does not require highly accurate correlation estimation or precise clustering, and 2) it allows for seamless integration with various existing R&amp;S procedures, while achieving optimal sample complexity. Theoretically, we develop a novel gradient analysis framework to analyze sample efficiency and guide the design of large-scale R&amp;S procedures. We also introduce a new parallel clustering algorithm tailored for large-scale scenarios. Finally, in large-scale AI applications such as neural architecture search, our methods demonstrate superior performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02196v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zishi Zhang, Yijie Peng</dc:creator>
    </item>
    <item>
      <title>Smooth Transformation Models for Survival Analysis: A Tutorial Using R</title>
      <link>https://arxiv.org/abs/2402.06428</link>
      <description>arXiv:2402.06428v2 Announce Type: replace 
Abstract: Over the last five decades, we have seen strong methodological advances in survival analysis, mainly in two separate strands: One strand is based on a parametric approach that assumes some response distribution. More prominent, however, is the strand of flexible methods which rely mainly on non-/semi-parametric estimation. As the methodological landscape continues to evolve, the task of navigating through the multitude of methods and identifying corresponding available software resources is becoming increasingly difficult. This task becomes particularly challenging in more complex scenarios, such as when dealing with interval-censored or clustered survival data, non-proportional hazards, or dependent censoring.
  In this tutorial, we explore the potential of using smooth transformation models for survival analysis in the R system for statistical computing. These models provide a unified maximum likelihood framework that covers a range of survival models, including well-established ones such as the Weibull model and a fully parameterised version of the famous Cox proportional hazards model, as well as extensions to more complex scenarios. We explore smooth transformation models for non-proportional/crossing hazards, dependent censoring, clustered observations and extensions towards personalised medicine within this framework.
  By fitting these models to survival data from a two-arm randomised controlled trial on rectal cancer therapy, we demonstrate how survival analysis tasks can be seamlessly navigated within the smooth transformation model framework in R. This is achieved by the implementation provided by the "tram" package and few related packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06428v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sandra Siegfried, B\'alint Tam\'asi, Torsten Hothorn</dc:creator>
    </item>
    <item>
      <title>Distributional bias compromises leave-one-out cross-validation</title>
      <link>https://arxiv.org/abs/2406.01652</link>
      <description>arXiv:2406.01652v2 Announce Type: replace 
Abstract: Cross-validation is a common method for estimating the predictive performance of machine learning models. In a data-scarce regime, where one typically wishes to maximize the number of instances used for training the model, an approach called "leave-one-out cross-validation" is often used. In this design, a separate model is built for predicting each data instance after training on all other instances. Since this results in a single test instance available per model trained, predictions are aggregated across the entire dataset to calculate common performance metrics such as the area under the receiver operating characteristic or R2 scores. In this work, we demonstrate that this approach creates a negative correlation between the average label of each training fold and the label of its corresponding test instance, a phenomenon that we term distributional bias. As machine learning models tend to regress to the mean of their training data, this distributional bias tends to negatively impact performance evaluation and hyperparameter optimization. We show that this effect generalizes to leave-P-out cross-validation and persists across a wide range of modeling and evaluation approaches, and that it can lead to a bias against stronger regularization. To address this, we propose a generalizable rebalanced cross-validation approach that corrects for distributional bias for both classification and regression. We demonstrate that our approach improves cross-validation performance evaluation in synthetic simulations, across machine learning benchmarks, and in several published leave-one-out analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01652v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>George I. Austin, Itsik Pe'er, Tal Korem</dc:creator>
    </item>
    <item>
      <title>Embedding Network Autoregression for time series analysis and causal peer effect inference</title>
      <link>https://arxiv.org/abs/2406.05944</link>
      <description>arXiv:2406.05944v2 Announce Type: replace 
Abstract: We propose an Embedding Network Autoregressive Model for multivariate networked longitudinal data. We assume the network is generated from a latent variable model, and these unobserved variables are included in a structural peer effect model or a time series network autoregressive model as additive effects. This approach takes a unified view of two related yet fundamentally different problems: (1) modeling and predicting multivariate networked time series data and (2) causal peer influence estimation in the presence of homophily from finite time longitudinal data. Our estimation strategy comprises estimating latent variables from the observed network followed by least squares estimation of the network autoregressive model. We show that the estimated momentum and peer effect parameters are consistent and asymptotically normally distributed in setups with a growing number of network vertices (N) while considering both a growing number of time points T (for the time series problem) and finite T cases (for the peer effect problem). We allow the number of latent vectors K to grow at appropriate rates, which improves upon existing rates when such results are available for related models. Our theoretical results encompass cases both when the network is modeled with the random dot product graph model (ENAR) and a more general latent space model with both additive and multiplicative effects (AMNAR). We also develop a selection criterion when K is unknown that provably does not under-select and show that the theoretical guarantees hold with the selected number for K as well. Interestingly, even though we propose a unified model, our theoretical results find that different growth rates and restrictions on the latent vectors are needed to induce omitted variable bias in the peer effect problem and to ensure consistent estimation in the time series problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05944v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jae Ho Chang, Subhadeep Paul</dc:creator>
    </item>
    <item>
      <title>The Effective Number of Parameters in Kernel Density Estimation</title>
      <link>https://arxiv.org/abs/2406.14453</link>
      <description>arXiv:2406.14453v2 Announce Type: replace 
Abstract: The quest for a formula that satisfactorily measures the effective degrees of freedom in kernel density estimation (KDE) is a long standing problem with few solutions. Starting from the orthogonal polynomial sequence (OPS) expansion for the ratio of the empirical to the oracle density, we show how convolution with the kernel leads to a new OPS with respect to which one may express the resulting KDE. The expansion coefficients of the two OPS systems can then be related via a kernel sensitivity matrix, and this then naturally leads to a definition of effective parameters by taking the trace of a symmetrized positive semi-definite normalized version. The resulting effective degrees of freedom (EDoF) formula is an oracle-based quantity; the first ever proposed in the literature. Asymptotic properties of the empirical EDoF are worked out through influence functions. Numerical investigations confirm the theoretical insights.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14453v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Sofia Guglielmini, Igor Volobouev, Alexandre Trindade</dc:creator>
    </item>
    <item>
      <title>Inconsistency and Acausality in Bayesian Inference for Physical Problems</title>
      <link>https://arxiv.org/abs/2411.13570</link>
      <description>arXiv:2411.13570v2 Announce Type: replace 
Abstract: Bayesian inference is used to estimate continuous parameter values given measured data in many fields of science. The method relies on conditional probability densities to describe information about both data and parameters, yet the notion of conditional densities is inadmissible: probabilities of the same physical event, computed from conditional densities under different parameterizations, may be inconsistent. We show that this inconsistency, together with acausality in hierarchical methods, invalidate a variety of commonly applied Bayesian methods when applied to problems in the physical world, including trans-dimensional inference, general Bayesian dimensionality reduction methods, and hierarchical and empirical Bayes. Models in parameter spaces of different dimensionalities cannot be compared, invalidating the concept of natural parsimony, the probabilistic counterpart to Occams Razor. Bayes theorem itself is inadmissible, and Bayesian inference applied to parameters that characterize physical properties requires reformulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.13570v2</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Klaus Mosegaard, Andrew Curtis</dc:creator>
    </item>
    <item>
      <title>Asymmetric Errors</title>
      <link>https://arxiv.org/abs/2411.15499</link>
      <description>arXiv:2411.15499v2 Announce Type: replace 
Abstract: We present a procedure for handling asymmetric errors. Many results in particle physics are presented as values with different positive and negative errors, and there is no consistent procedure for handling them. We consider the difference between errors quoted using pdfs and using likelihoods, and the difference between the rms spread of a measurement and the 68\% central confidence region. We provide a comprehensive analysis of the possibilities, and software tools to enable their use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15499v2</guid>
      <category>stat.ME</category>
      <category>hep-ex</category>
      <category>hep-ph</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roger Barlow, Alessandra Brazzale, Igor Volobouev</dc:creator>
    </item>
    <item>
      <title>A note on the construction of augmented designs in square arrays</title>
      <link>https://arxiv.org/abs/2501.08448</link>
      <description>arXiv:2501.08448v3 Announce Type: replace 
Abstract: An augmented design in a square array can be derived from a smaller row-column design (the contraction). Such a contraction has also previously been used to generate a two-replicate resolvable incomplete block design. We demonstrate a parallel between these two uses of the contraction and thereby establish a recently proposed conjecture by linking the average efficiency factor of the augmented design with that of its contraction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08448v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>E. R. Williams, H-P. Piepho</dc:creator>
    </item>
    <item>
      <title>Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)</title>
      <link>https://arxiv.org/abs/2501.19047</link>
      <description>arXiv:2501.19047v3 Announce Type: replace 
Abstract: To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19047v3</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maja Pavlovic</dc:creator>
    </item>
    <item>
      <title>Leveraging Two-Phase Data for Improved Prediction of Survival Outcomes with Application to Nasopharyngeal Cancer</title>
      <link>https://arxiv.org/abs/2503.16732</link>
      <description>arXiv:2503.16732v2 Announce Type: replace 
Abstract: Accurate survival predicting models are essential for improving targeted cancer therapies and clinical care among cancer patients. In this article, we investigate and develop a method to improve predictions of survival in cancer by leveraging two-phase data with expert knowledge and prognostic index. Our work is motivated by two-phase data in nasopharyngeal cancer (NPC), where traditional covariates are readily available for all subjects, but the primary viral factor, Human Papillomavirus (HPV), is substantially missing. To address this challenge, we propose an expert guided method that incorporates prognostic index based on the observed covariates and clinical importance of key factors. The proposed method makes efficient use of available data, not simply discarding patients with unknown HPV status. We apply the proposed method and evaluate it against other existing approaches through a series of simulation studies and real data example of NPC patients. Under various settings, the proposed method consistently outperforms competing methods in terms of c-index, calibration slope, and integrated Brier score. By efficiently leveraging two-phase data, the model provides a more accurate and reliable predictive ability of survival models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.16732v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Jeong Oh, Seungjun Ahn, Tristan Tham, Min Qian</dc:creator>
    </item>
    <item>
      <title>The Local Approach to Causal Inference under Network Interference</title>
      <link>https://arxiv.org/abs/2105.03810</link>
      <description>arXiv:2105.03810v5 Announce Type: replace-cross 
Abstract: We propose a new nonparametric modeling framework for causal inference when outcomes depend on how agents are linked in a social or economic network. Such network interference describes a large literature on treatment spillovers, social interactions, social learning, information diffusion, disease and financial contagion, social capital formation, and more. Our approach works by first characterizing how an agent is linked in the network using the configuration of other agents and connections nearby as measured by path distance. The impact of a policy or treatment assignment is then learned by pooling outcome data across similarly configured agents. We demonstrate the approach by deriving finite-sample bounds on the mean-squared error of a k-nearest-neighbor estimator for the average treatment response as well as proposing an asymptotically valid test for the hypothesis of policy irrelevance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.03810v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Auerbach, Hongchang Guo, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>lqmix: an R package for longitudinal data analysis via linear quantile mixtures</title>
      <link>https://arxiv.org/abs/2302.11363</link>
      <description>arXiv:2302.11363v3 Announce Type: replace-cross 
Abstract: The analysis of longitudinal data gives the chance to observe how unit behaviors change over time, but it also poses a series of issues. These have been the focus of an extensive literature in the context of linear and generalized linear regression moving also, in the last ten years or so, to the context of linear quantile regression for continuous responses. In this paper, we present \textsf{lqmix}, a novel \textsf{R} package that assists in estimating a class of linear quantile regression models for longitudinal data, in the presence of time-constant and/or time-varying, unit-specific, random coefficients, with unspecified distribution. Model parameters are estimated in a maximum likelihood framework, via an extended EM algorithm, and parameters' standard errors are estimated via a block-bootstrap procedure. The analysis of a benchmark dataset is used to give details on the package functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.11363v3</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Alf\'o, Maria Francesca Marino, Maria Giovanna Ranalli, Nicola Salvati</dc:creator>
    </item>
    <item>
      <title>Invariant Causal Set Covering Machines</title>
      <link>https://arxiv.org/abs/2306.04777</link>
      <description>arXiv:2306.04777v3 Announce Type: replace-cross 
Abstract: Rule-based models, such as decision trees, appeal to practitioners due to their interpretable nature. However, the learning algorithms that produce such models are often vulnerable to spurious associations and thus, they are not guaranteed to extract causally-relevant insights. In this work, we build on ideas from the invariant causal prediction literature to propose Invariant Causal Set Covering Machines, an extension of the classical Set Covering Machine algorithm for conjunctions/disjunctions of binary-valued rules that provably avoids spurious associations. We demonstrate both theoretically and empirically that our method can identify the causal parents of a variable of interest in polynomial time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.04777v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thibaud Godon, Baptiste Bauvin, Pascal Germain, Jacques Corbeil, Alexandre Drouin</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Learning for Multi-source Unsupervised Domain Adaptation</title>
      <link>https://arxiv.org/abs/2309.02211</link>
      <description>arXiv:2309.02211v5 Announce Type: replace-cross 
Abstract: Empirical risk minimization often performs poorly when the distribution of the target domain differs from those of source domains. To address such potential distribution shifts, we develop an unsupervised domain adaptation approach that leverages labeled data from multiple source domains and unlabeled data from the target domain. We introduce a distributionally robust model that optimizes an adversarial reward based on the explained variance across a class of target distributions, ensuring generalization to the target domain. We show that the proposed robust model is a weighted average of conditional outcome models from source domains. This formulation allows us to compute the robust model through the aggregation of source models, which can be estimated using various machine learning algorithms of the users' choice, such as random forests, boosting, and neural networks. Additionally, we introduce a bias-correction step to obtain a more accurate aggregation weight, which is effective for various machine learning algorithms. Our framework can be interpreted as a distributionally robust federated learning approach that satisfies privacy constraints while providing insights into the importance of each source for prediction on the target domain. The performance of our method is evaluated on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.02211v5</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Peter B\"uhlmann, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Consistent Validation for Predictive Methods in Spatial Settings</title>
      <link>https://arxiv.org/abs/2402.03527</link>
      <description>arXiv:2402.03527v3 Announce Type: replace-cross 
Abstract: Spatial prediction tasks are key to weather forecasting, studying air pollution impacts, and other scientific endeavors. Determining how much to trust predictions made by statistical or physical methods is essential for the credibility of scientific conclusions. Unfortunately, classical approaches for validation fail to handle mismatch between locations available for validation and (test) locations where we want to make predictions. This mismatch is often not an instance of covariate shift (as commonly formalized) because the validation and test locations are fixed (e.g., on a grid or at select points) rather than i.i.d. from two distributions. In the present work, we formalize a check on validation methods: that they become arbitrarily accurate as validation data becomes arbitrarily dense. We show that classical and covariate-shift methods can fail this check. We propose a method that builds from existing ideas in the covariate-shift literature, but adapts them to the validation data at hand. We prove that our proposal passes our check. And we demonstrate its advantages empirically on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03527v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>David R. Burt, Yunyi Shen, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>Score matching through the roof: linear, nonlinear, and latent variables causal discovery</title>
      <link>https://arxiv.org/abs/2407.18755</link>
      <description>arXiv:2407.18755v2 Announce Type: replace-cross 
Abstract: Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function $\nabla \log p(X)$ of observed variables for causal discovery and propose the following contributions. First, we fine-tune the existing identifiability results with the score on additive noise models, showing that their assumption of nonlinearity of the causal mechanisms is not necessary. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential to infer the equivalence class of causal graphs with hidden variables (while previous results are restricted to the fully observable setting), and we provide sufficient conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm suited for causal discovery on linear, nonlinear, and latent variable models, which we empirically validate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18755v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>4th Conference on Causal Learning and Reasoning (CLeaR 2025)</arxiv:journal_reference>
      <dc:creator>Francesco Montagna, Philipp M. Faller, Patrick Bloebaum, Elke Kirschbaum, Francesco Locatello</dc:creator>
    </item>
    <item>
      <title>Regularized e-processes: anytime valid inference with knowledge-based efficiency gains</title>
      <link>https://arxiv.org/abs/2410.01427</link>
      <description>arXiv:2410.01427v3 Announce Type: replace-cross 
Abstract: Classical statistical methods have theoretical justification when the sample size is predetermined. In applications, however, it's often the case that sample sizes are data-dependent rather than predetermined. The aforementioned methods aren't reliable in this latter case, hence the recent interest in e-processes and methods that are anytime valid, i.e., reliable for any dynamic data-collection plan. But if the investigator has relevant-yet-incomplete prior information about the quantity of interest, then there's an opportunity for efficiency gain. This paper proposes a regularized e-process framework featuring a knowledge-based, imprecise-probabilistic regularization with improved efficiency. A generalized version of Ville's inequality is established, ensuring that inference based on the regularized e-process are anytime valid in a novel, knowledge-dependent sense. Regularized e-processes also facilitate possibility-theoretic uncertainty quantification with strong frequentist-like calibration properties and other Bayesian-like properties: satisfies the likelihood principle, avoids sure-loss, and offers formal decision-making with reliability guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01427v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Martin</dc:creator>
    </item>
    <item>
      <title>Mean Estimation in Banach Spaces Under Infinite Variance and Martingale Dependence</title>
      <link>https://arxiv.org/abs/2411.11271</link>
      <description>arXiv:2411.11271v2 Announce Type: replace-cross 
Abstract: We consider estimating the shared mean of a sequence of heavy-tailed random variables taking values in a Banach space. In particular, we revisit and extend a simple truncation-based mean estimator first proposed by Catoni and Giulini. While existing truncation-based approaches require a bound on the raw (non-central) second moment of observations, our results hold under a bound on either the central or non-central $p$th moment for some $p \in (1,2]$. Our analysis thus handles distributions with infinite variance. The main contributions of the paper follow from exploiting connections between truncation-based mean estimation and the concentration of martingales in smooth Banach spaces. We prove two types of time-uniform bounds on the distance between the estimator and unknown mean: line-crossing inequalities, which can be optimized for a fixed sample size $n$, and iterated logarithm inequalities, which match the tightness of line-crossing inequalities at all points in time up to a doubly logarithmic factor in $n$. Our results do not depend on the dimension of the Banach space, hold under martingale dependence, and all constants in the inequalities are known and small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11271v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 25 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin Whitehouse, Ben Chugg, Diego Martinez-Taboada, Aaditya Ramdas</dc:creator>
    </item>
  </channel>
</rss>
