<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Sep 2025 01:21:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Finding a Multiple Follower Stackelberg Equilibrium: A Fully First-Order Method</title>
      <link>https://arxiv.org/abs/2509.08161</link>
      <description>arXiv:2509.08161v1 Announce Type: new 
Abstract: In this work, we propose the first fully first-order method to compute an epsilon stationary Stackelberg equilibrium with convergence guarantees. To achieve this, we first reframe the leader follower interaction as single level constrained optimization. Second, we define the Lagrangian and show that it can approximate the leaders gradient in response to the equilibrium reached by followers with only first-order gradient evaluations. These findings suggest a fully first order algorithm that alternates between (i) approximating followers best responses through gradient descent and (ii) updating the leaders strategy via approximating the gradient using Lagrangian.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08161v1</guid>
      <category>math.OC</category>
      <category>cs.GT</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>April Niu, Kai Wang, Juba Ziani</dc:creator>
    </item>
    <item>
      <title>OCTANE -- Optimal Control for Tensor-based Autoencoder Network Emergence: Explicit Case</title>
      <link>https://arxiv.org/abs/2509.08169</link>
      <description>arXiv:2509.08169v1 Announce Type: new 
Abstract: This paper presents a novel, mathematically rigorous framework for autoencoder-type deep neural networks that combines optimal control theory and low-rank tensor methods to yield memory-efficient training and automated architecture discovery. The learning task is formulated as an optimization problem constrained by differential equations representing the encoder and decoder components of the network and the corresponding optimality conditions are derived via a Lagrangian approach. Efficient memory compression is enabled by approximating differential equation solutions on low-rank tensor manifolds using an adaptive explicit integration scheme. These concepts are combined to form OCTANE (Optimal Control for Tensor-based Autoencoder Network Emergence) -- a unified training framework that yields compact autoencoder architectures, reduces memory usage, and enables effective learning, even with limited training data. The framework's utility is illustrated with application to image denoising and deblurring tasks and recommendations regarding governing hyperparameters are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08169v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ratna Khatri, Anthony Kolshorn, Colin Olson, Harbir Antil</dc:creator>
    </item>
    <item>
      <title>Some New Insights from Highly Optimized Polyhedral Passages</title>
      <link>https://arxiv.org/abs/2509.08190</link>
      <description>arXiv:2509.08190v1 Announce Type: new 
Abstract: A shape possesses Rupert's property if a hole can be cut through it such that a second identical copy of the shape can cleanly pass straight through the interior of the first. Such a passage proving cubes are Rupert was first shown more than 300 years ago. It remains open whether every polyhedron in three dimensions is Rupert. We propose a customized subgradient method providing high-accuracy local numerical optimization of the quality of a passage for a given polyhedron. From extensive numerical searches, we improve these best-known passages for more than half of the Platonic, Archimedean, and Catalan solids and for numerous Johnson solids. Our high accuracy solves support a new conjecture of a simple form for the Tetrahedron's optimal passage. Despite our computational search, three Archimedean and two Catalan solids remain open, providing further negative evidence against the conjecture that all polyhedrons are Rupert.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08190v1</guid>
      <category>math.OC</category>
      <category>math.MG</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raj Gosain, Benjamin Grimmer</dc:creator>
    </item>
    <item>
      <title>An expanded evaluation matrix for the entropy-weight TODIM method to reduce the rank reversal probability and its application in selecting energy storage technology</title>
      <link>https://arxiv.org/abs/2509.08236</link>
      <description>arXiv:2509.08236v1 Announce Type: new 
Abstract: The TODIM method (an acronym in Portuguese for interactive and multiple criteria decision-making) with entropy weights is influenced by rank reversal, a phenomenon where the order of two alternatives changes following the addition of another alternative. Research on rank reversal has predominantly focused on single decision-making methods. To the best of our knowledge, the reduction of rank reversal probability in hybrid methods, such as the entropy-weight TODIM method, remains an unresolved challenge. To address this, this paper introduces the expanded evaluation matrix, which incorporates virtual alternatives, to reduce the probability of rank reversal in the entropy-weight TODIM method. A simulation study is conducted to assess the effectiveness of the expanded evaluation matrix in mitigating rank reversal. The results demonstrate that the expanded evaluation matrix significantly reduces the rank reversal probability. A case study on selecting energy storage technology showcases the potential real-world applications of the expanded evaluation matrix. The reliability of the expanded evaluation matrix is further validated through sensitivity and comparative analyses. Given the simplicity and ease of implementation of the expanded evaluation matrix, it can be readily adapted to other decision-making methods and holds substantial potential for broad application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08236v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisheng Jiang, Tianyu Zhang, Shiyu Yan, Ran Fang</dc:creator>
    </item>
    <item>
      <title>Combined-distance-based score function of cognitive fuzzy sets and its application in lung cancer pain evaluation</title>
      <link>https://arxiv.org/abs/2509.08239</link>
      <description>arXiv:2509.08239v1 Announce Type: new 
Abstract: In decision making, the cognitive fuzzy set (CFS) is a useful tool in expressing experts' complex assessments of alternatives. The distance of CFS, which plays an important role in decision analyses, is necessary when the CFS is applied in solving practical issues. However, as far as we know, the studies on the distance of CFS are few, and the current Minkowski distance of CFS ignores the hesitancy degree of CFS, which might cause errors. To fill the gap of the studies on the distance of CFS, because of the practicality of the Hausdorff distance, this paper proposes the improved cognitive fuzzy Minkowski (CF-IM) distance and the cognitive fuzzy Hausdorff (CF-H) distance to enrich the studies on the distance of CFS. It is found that the anti-perturbation ability of the CF-H distance is stronger than that of the CF-IM distance, but the information utilization of the CF-IM distance is higher than that of the CF-H distance. To balance the anti-perturbation ability and information utilization of the CF-IM distance and CF-H distance, the cognitive fuzzy combined (CF-C) distance is proposed by establishing the linear combination of the CF-IM distance and CF-H distance. Based on the CF-C distance, a combined-distanced-based score function of CFS is proposed to compare CFSs. The proposed score function is employed in lung cancer pain evaluation issues. The sensitivity and comparison analyses demonstrate the reliability and advantages of the proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08239v1</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lisheng Jiang, Tianyu Zhang, Shiyu Yan, Ran Fang</dc:creator>
    </item>
    <item>
      <title>Lipschitzianity of expected value under decision-dependent uncertainty with moving support</title>
      <link>https://arxiv.org/abs/2509.08252</link>
      <description>arXiv:2509.08252v2 Announce Type: new 
Abstract: This paper addresses the problem of stochastic optimization with decision-dependent uncertainty, a class of problems where the probability distribution of the uncertain parameters is influenced by the decision-maker's actions. While recent literature primarily focuses on solving or analyzing these problems by directly imposing hypotheses on the distribution mapping, in this work we explore some of these properties for a specific construction by means of the moving support and a density function. The construction is motivated by the Bayesian approach to bilevel programming, where the response of a follower is modeled as the uncertainty, drawn from the moving set of optimal responses which depends on the leader's decision. Our main contribution is to establish sufficient conditions for the Lipschitz continuity of the expected value function. We show that Lipschitz continuity can be achieved when the moving support is a Lipschitz continuous set-valued map with full-dimensional, convex, compact values, or when it is the solution set of a fully linear parametric problem. We also provide an example showing that the sole Lipschitz assumption on the moving set itself is not sufficient and that additional conditions are necessary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08252v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John Cotrina, Gonzalo Flores, David Salas, Anton Svensson</dc:creator>
    </item>
    <item>
      <title>Model-Driven Subspaces for Large-Scale Optimization with Local Approximation Strategy</title>
      <link>https://arxiv.org/abs/2509.08256</link>
      <description>arXiv:2509.08256v1 Announce Type: new 
Abstract: Solving large-scale optimization problems is a bottleneck and is very important for machine learning and multiple kinds of scientific problems. Subspace-based methods using the local approximation strategy are one of the most important methods. This paper discusses different and novel kinds of advanced subspaces for such methods and presents a new algorithm with such subspaces, called MD-LAMBO. Theoretical analysis including the subspaces' properties, sufficient function value decrease, and global convergence is given for the new algorithm. The related model construction on the subspaces is given under derivative-free settings. In numerical results, performance profiles, and truncated Newton step errors of MD-LAMBO using different model-driven subspaces are provided, which show subspace-dependent numerical differences and advantages of our methods and subspaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08256v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yitong He, Pengcheng Xie</dc:creator>
    </item>
    <item>
      <title>Nesterov acceleration for strongly convex-strongly concave bilinear saddle point problems: discrete and continuous-time approaches</title>
      <link>https://arxiv.org/abs/2509.08258</link>
      <description>arXiv:2509.08258v1 Announce Type: new 
Abstract: In this paper, we study a bilinear saddle point problem of the form $\min_{x}\max_{y} F(x) + \langle Ax, y \rangle - G(y)$, where $F$ and $G$ are $\mu_F$- and $\mu_G$-strongly convex functions, respectively. By incorporating Nesterov acceleration for strongly convex optimization, we first propose an optimal first-order discrete primal-dual gradient algorithm. We show that it achieves the optimal convergence rate $\mathcal{O}\left(\left(1 - \min\left\{\sqrt{\frac{\mu_F}{L_F}}, \sqrt{\frac{\mu_G}{L_G}}\right\}\right)^k\right)$ for both the primal-dual gap and the iterative, where $L_F$ and $L_G$ denote the smoothness constants of $F$ and $G$, respectively. We further develop a continuous-time accelerated primal-dual dynamical system with constant damping. Using the Lyapunov analysis method, we establish the existence and uniqueness of a global solution, as well as the linear convergence rate $\mathcal{O}(e^{-\min\{\sqrt{\mu_F},\sqrt{\mu_G}\}t})$. Notably, when $A = 0$, our methods recover the classical Nesterov accelerated methods for strongly convex unconstrained problems in both discrete and continuous-time. Numerical experiments are presented to support the theoretical convergence rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08258v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xin He, Ya-Ping Fang</dc:creator>
    </item>
    <item>
      <title>A Stochastic Programming Approach to the Railcar Maintenance Problem with Service Level and Track Capacity Considerations</title>
      <link>https://arxiv.org/abs/2509.08427</link>
      <description>arXiv:2509.08427v1 Announce Type: new 
Abstract: Railcars, as part of the rolling stock, perform regular transportation tasks with respect to a service level agreement (SLA) and undergo preventive maintenance at regular intervals based on the recommendations of train manufacturers. When unexpected failures occur, they need to enter corrective maintenance immediately. However, this reactive approach may result in large SLA violations and an excessive number of corrective maintenance actions. In this study, we utilize a predictive maintenance approach based on the reliability of a railcar. In particular, we propose a stochastic programming model, in which railcar failure scenarios are generated from a Weibull distribution, a common assumption in the reliability literature. The model incorporates both SLA and track-capacity considerations and is solved through the Sample Average Approximation (SAA) method. We generate random instances to compare the stochastic model and a deterministic model adopted from the literature with respect to several system parameters. Our results show that the stochastic model achieves lower total costs, fewer SLA violations, and a reduced number of corrective interventions compared with deterministic approaches, while effectively managing track-capacity constraints. Our results underscore the importance of the predictive approach in the context of the railcar maintenance problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08427v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Murat Elh\"useyni, Burak Kocuk</dc:creator>
    </item>
    <item>
      <title>Hearing the Shape of a Cuboid Room Using Sparse Measure Recovery</title>
      <link>https://arxiv.org/abs/2509.08443</link>
      <description>arXiv:2509.08443v1 Announce Type: new 
Abstract: This article explores a variant of Kac's famous problem, "Can one hear the shape of a drum?", by addressing a geometric inverse problem in acoustics. Our objective is to reconstruct the shape of a cuboid room using acoustic signals measured by microphones placed within the room. By examining this straightforward configuration, we aim to understand the relationship between the acoustic signals propagating in a room and its geometry. This geometric problem can be reduced to locating a finite set of acoustic point sources, known as image sources. We model this issue as a finite-dimensional optimization problem and propose a solution algorithm inspired by super-resolution techniques. This involves a convex relaxation of the finite-dimensional problem to an infinite-dimensional subspace of Radon measures. We provide analytical insights into this problem and demonstrate the efficiency of the algorithm through multiple numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08443v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Inverse Problems, 2025, 41 (9)</arxiv:journal_reference>
      <dc:creator>Antoine Deleforge (MACARON, IRMA), C\'edric Foy (Cerema, UMRAE), Yannick Privat (IECL, SPHINX, IUF), Tom Sprunck (CEA, MACARON)</dc:creator>
    </item>
    <item>
      <title>Hierarchical exact controllability for a parabolic equation with Hardy potential</title>
      <link>https://arxiv.org/abs/2509.08471</link>
      <description>arXiv:2509.08471v1 Announce Type: new 
Abstract: The main objective of this paper is to study the hierarchical exact controllability for a parabolic equation with Hardy potential by Stackelberg-Nash strategy. In linear case, we employ Lax-Milgram theorem to prove the existence of an associated Nash equilibrium pair corresponding to a bi-objective optimal control problem for each leader, which is responsible for an exact controllability property. Then the observability inequality of a coupled parabolic system is established by using global Carleman inequalities, which results in the existence of a leader that drives the controlled system exactly to any prescribed trajectory. In semilinear case, we first prove the well-posedness of the coupled parabolic system to obtain the existence of Nash quasi-equilibrium pair and show that Nash quasi-equilibrium is equivalent to Nash equilibrium. Based on these results, we establish the existence of a leader that drives the controlled system exactly to a prescribed (but arbitrary) trajectory by Leray-Schauder fixed point theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08471v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haiyang Lin, Bo You</dc:creator>
    </item>
    <item>
      <title>Stabilisability and beta representations</title>
      <link>https://arxiv.org/abs/2509.08532</link>
      <description>arXiv:2509.08532v1 Announce Type: new 
Abstract: We consider a one dimensional affine switched system obtained from a formal limit of a two dimensional linear system. We show this is equivalent to minimising the average digit in beta representations with unrestricted digits. We give a countable set of $\beta$ for which the result is given by the usual (greedy) beta expansion, an interval of values for which it is strictly less, and a conditional lower bound for all $\beta$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08532v1</guid>
      <category>math.OC</category>
      <category>math.DS</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carl P. Dettmann (University of Bristol, UK)</dc:creator>
    </item>
    <item>
      <title>Quantum and Simulated Annealing-Based Iterative Algorithms for QUBO Relaxations of the Sparsest $k$-Subgraph Problem</title>
      <link>https://arxiv.org/abs/2509.08544</link>
      <description>arXiv:2509.08544v1 Announce Type: new 
Abstract: In this paper, we introduce three QUBO (Quadratic Unconstrained Binary Optimization) relaxations for the sparsest $k$-subgraph (SkS) problem: a quadratic penalty relaxation, a Lagrangian relaxation, and an augmented Lagrangian relaxation. The effectiveness of these approaches strongly depends on the choice of penalty parameters. We establish theoretical results characterizing the parameter values for which the QUBO relaxations are exact. For practical implementation, we propose three iterative algorithms, which have in their kernel the QUBO relaxations, that update the penalty parameters at each iteration while approximately solving the internal QUBO problems with simulated annealing and quantum processing units. Extensive numerical experiments validate our theoretical findings on exact relaxations and demonstrate the efficiency of the proposed iterative algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08544v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Omkar Bihani, Roman Ku\v{z}el, Janez Povh, Dunja Pucher</dc:creator>
    </item>
    <item>
      <title>Linear Convergence of Gradient Descent for Quadratically Regularized Optimal Transport</title>
      <link>https://arxiv.org/abs/2509.08547</link>
      <description>arXiv:2509.08547v1 Announce Type: new 
Abstract: In optimal transport, quadratic regularization is an alternative to entropic regularization when sparse couplings or small regularization parameters are desired. Here quadratic regularization means that transport couplings are penalized by the squared $L^2$ norm, or equivalently the $\chi^2$ divergence. While a number of computational approaches have been shown to work in practice, quadratic regularization is analytically less tractable than entropic, and we are not aware of a previous theoretical convergence rate analysis. We focus on the gradient descent algorithm for the dual transport problem in continuous and semi-discrete settings. This problem is convex but not strongly convex; its solutions are the potential functions that approximate the Kantorovich potentials of unregularized optimal transport. The gradient descent steps are straightforward to implement, and stable for small regularization parameter -- in contrast to Sinkhorn's algorithm in the entropic setting. Our main result is that gradient descent converges linearly; that is, the $L^2$ distance between the iterates and the limiting potentials decreases exponentially fast. Our analysis centers on the linearization of the gradient descent operator at the optimum and uses functional-analytic arguments to bound its spectrum. These techniques seem to be novel in this area and are substantially different from the approaches familiar in entropic optimal transport.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08547v1</guid>
      <category>math.OC</category>
      <category>math.AP</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Marcel Nutz, Andr\'es Riveros Valdevenito</dc:creator>
    </item>
    <item>
      <title>An Inexact Proximal Framework for Nonsmooth Riemannian Difference-of-Convex Optimization</title>
      <link>https://arxiv.org/abs/2509.08561</link>
      <description>arXiv:2509.08561v1 Announce Type: new 
Abstract: Nonsmooth Riemannian optimization has attracted increasing attention, especially in problems with sparse structures. While existing formulations typically involve convex nonsmooth terms, incorporating nonsmooth difference-of-convex (DC) penalties can enhance recovery accuracy. In this paper, we study a class of nonsmooth Riemannian optimization problems whose objective is the sum of a smooth function and a nonsmooth DC term. We establish, for the first time in the manifold setting, the equivalence between such DC formulations (with suitably chosen nonsmooth DC terms) and their $\ell_0$-regularized or $\ell_0$-constrained counterparts. To solve these problems, we propose an inexact Riemannian proximal DC (iRPDC) algorithmic framework, which returns an $\epsilon$-Riemannian critical point within $\mathcal{O}(\epsilon^{-2})$ outer iterations. Within this framework, we develop several practical algorithms based on different subproblem solvers. Among them, one achieves an overall iteration complexity of $\mathcal{O}(\epsilon^{-3})$, which matches the best-known bound in the literature. In contrast, existing algorithms either lack provable overall complexity or require $\mathcal{O}(\epsilon^{-3})$ iterations in both outer and overall complexity. A notable feature of the iRPDC algorithmic framework is a novel inexactness criterion that not only enables efficient subproblem solutions via first-order methods but also facilitates a linesearch procedure that adaptively captures the local curvature. Numerical results on sparse principal component analysis demonstrate the modeling flexibility of the DC formulaton and the competitive performance of the proposed algorithmic framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08561v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Jiang, Meng Xu, Xingju Cai, Ya-Feng Liu</dc:creator>
    </item>
    <item>
      <title>Optimal control of stochastic networks of $M/M/\infty$ queues with linear costs</title>
      <link>https://arxiv.org/abs/2509.08572</link>
      <description>arXiv:2509.08572v1 Announce Type: new 
Abstract: We consider an arbitrary network of $M/M/\infty$ queues with controlled transitions between queues. We consider optimal control problems where the costs are linear functions of the state and inputs over a finite or infinite horizon. We provide in both cases an explicit characterization of the optimal control policies. We also show that these do not involve state feedback, but they depend on the network topology and system parameters. The results are also illustrated with various examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08572v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giovanni Pugliese Carratelli, Ioannis Lestas</dc:creator>
    </item>
    <item>
      <title>Prescribed performance control of uncertain higher-order nonlinear systems in the presence of delays</title>
      <link>https://arxiv.org/abs/2509.08601</link>
      <description>arXiv:2509.08601v1 Announce Type: new 
Abstract: We propose a novel feedback controller for a class of uncertain higher-order nonlinear systems, subject to delays in both state measurement and control input signals. Building on the prescribed performance control framework, a delay-dependent performance correction mechanism is introduced to ensure the boundedness of all signals in the closed-loop and to keep the output tracking error strictly within a dynamically adjusted performance envelope. This mechanism adapts in response to large delays that may cause performance degradation. In the absence of delays, the correction term vanishes, and the controller recovers the nominal (user-defined) performance envelope. The effectiveness of the proposed approach is validated through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08601v1</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Berger, Lampros N. Bikas, Jan Hachmeister, George A. Rovithakis</dc:creator>
    </item>
    <item>
      <title>Extracting Alternative Solutions from Benders Decomposition</title>
      <link>https://arxiv.org/abs/2509.08671</link>
      <description>arXiv:2509.08671v2 Announce Type: new 
Abstract: We show how to extract alternative solutions for optimization problems solved by Benders Decomposition. In practice, alternative solutions provide useful insights for complex applications; some solvers do support generation of alternative solutions but none appear to support such generation when using Benders Decomposition. We propose a new post-processing method that extracts multiple optimal and near-optimal solutions using the cut-pool generated during Benders Decomposition. Further, we provide a geometric framework for understanding how the adaptive approximation in Benders Decomposition relates to alternative solutions. We demonstrate this technique on stochastic programming and interdiction modeling, and we highlight use cases that require the ability to enumerate all optimal solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08671v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew Viens, William E. Hart, Michael Ferris</dc:creator>
    </item>
    <item>
      <title>Decentralized Stochastic Nonconvex Optimization under the Relaxed Smoothness</title>
      <link>https://arxiv.org/abs/2509.08726</link>
      <description>arXiv:2509.08726v1 Announce Type: new 
Abstract: This paper studies decentralized optimization problem $f(\mathbf{x})=\frac{1}{m}\sum_{i=1}^m f_i(\mathbf{x})$, where each local function has the form of $f_i(\mathbf{x}) = {\mathbb E}\left[F(\mathbf{x};{\xi}_i)\right]$ which is $(L_0,L_1)$-smooth but possibly nonconvex and the random variable ${\xi}_i$ follows distribution ${\mathcal D}_i$. We propose a novel algorithm called decentralized normalized stochastic gradient descent (DNSGD), which can achieve the $\epsilon$-stationary point on each local agent. We present a new framework for analyzing decentralized first-order methods in the relaxed smooth setting, based on the Lyapunov function related to the product of the gradient norm and the consensus error. The analysis shows upper bounds on sample complexity of ${\mathcal O}(m^{-1}(L_f\sigma^2\Delta_f\epsilon^{-4} + \sigma^2\epsilon^{-2} + L_f^{-2}L_1^3\sigma^2\Delta_f\epsilon^{-1} + L_f^{-2}L_1^2\sigma^2))$ per agent and communication complexity of $\tilde{\mathcal O}((L_f\epsilon^{-2} + L_1\epsilon^{-1})\gamma^{-1/2}\Delta_f)$, where $L_f=L_0 +L_1\zeta$, $\sigma^2$ is the variance of the stochastic gradient, $\Delta_f$ is the initial optimal function value gap, $\gamma$ is the spectral gap of the network, and $\zeta$ is the degree of the gradient dissimilarity. In the special case of $L_1=0$, the above results (nearly) match the lower bounds on decentralized nonconvex optimization in the standard smooth setting. We also conduct numerical experiments to show the empirical superiority of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08726v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luo Luo, Xue Cui, Tingkai Jia, Cheng Chen</dc:creator>
    </item>
    <item>
      <title>Bregman Douglas-Rachford Splitting Method</title>
      <link>https://arxiv.org/abs/2509.08739</link>
      <description>arXiv:2509.08739v1 Announce Type: new 
Abstract: In this paper, we propose the Bregman Douglas-Rachford splitting (BDRS) method and its variant Bregman Peaceman-Rachford splitting method for solving maximal monotone inclusion problem. We show that BDRS is equivalent to a Bregman alternating direction method of multipliers (ADMM) when applied to the dual of the problem. A special case of the Bregman ADMM is an alternating direction version of the exponential multiplier method. To the best of our knowledge, algorithms proposed in this paper are new to the literature. We also discuss how to use our algorithms to solve the discrete optimal transport (OT) problem. We prove the convergence of the algorithms under certain assumptions, though we point out that one assumption does not apply to the OT problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08739v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Shiqian Ma, Lin Xiao, Renbo Zhao</dc:creator>
    </item>
    <item>
      <title>Unidimensional semi-discrete partial optimal transport</title>
      <link>https://arxiv.org/abs/2509.08799</link>
      <description>arXiv:2509.08799v1 Announce Type: new 
Abstract: We study the semi-discrete formulation of one-dimensional partial optimal transport with quadratic cost, where a probability density is partially transported to a finite sum of Dirac masses of smaller total mass. This problem arises naturally in applications such as risk management, the modeling of crowd motion, and sliced partial transport algorithms for point cloud registration. Unlike higher-dimensional settings, the dual functional in the unidimensional case exhibits reduced regularity. To overcome this difficulty, we introduce a regularization procedure based on thickening the density along an auxiliary dimension. We prove that the maximizers of the regularized dual problem converge to those of the original dual problem, with quadratic rate in the introduced thickness. We further provide a numerical scheme that leverages the regularized functional, and we validate our analysis with simulations that confirm the quadratic convergence rate. Finally, we compare the semi-discrete and fully discrete settings, demonstrating that our approach offers both improved stability and computational efficiency for unidimensional partial transport problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08799v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adrien Cances, Hugo Leclerc</dc:creator>
    </item>
    <item>
      <title>A Minimalist Bayesian Framework for Stochastic Optimization</title>
      <link>https://arxiv.org/abs/2509.07030</link>
      <description>arXiv:2509.07030v1 Announce Type: cross 
Abstract: The Bayesian paradigm offers principled tools for sequential decision-making under uncertainty, but its reliance on a probabilistic model for all parameters can hinder the incorporation of complex structural constraints. We introduce a minimalist Bayesian framework that places a prior only on the component of interest, such as the location of the optimum. Nuisance parameters are eliminated via profile likelihood, which naturally handles constraints. As a direct instantiation, we develop a MINimalist Thompson Sampling (MINTS) algorithm. Our framework accommodates structured problems, including continuum-armed Lipschitz bandits and dynamic pricing. It also provides a probabilistic lens on classical convex optimization algorithms such as the center of gravity and ellipsoid methods. We further analyze MINTS for multi-armed bandits and establish near-optimal regret guarantees.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07030v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>Optimization Methods and Software for Federated Learning</title>
      <link>https://arxiv.org/abs/2509.08120</link>
      <description>arXiv:2509.08120v1 Announce Type: cross 
Abstract: Federated Learning (FL) is a novel, multidisciplinary Machine Learning paradigm where multiple clients, such as mobile devices, collaborate to solve machine learning problems. Initially introduced in Kone{\v{c}}n{\'y} et al. (2016a,b); McMahan et al. (2017), FL has gained further attention through its inclusion in the National AI Research and Development Strategic Plan (2023 Update) of the United States (Science and on Artificial Intelligence, 2023). The FL training process is inherently decentralized and often takes place in less controlled settings compared to data centers, posing unique challenges distinct from those in fully controlled environments. In this thesis, we identify five key challenges in Federated Learning and propose novel approaches to address them. These challenges arise from the heterogeneity of data and devices, communication issues, and privacy concerns for clients in FL training. Moreover, even well-established theoretical advances in FL require diverse forms of practical implementation to enhance their real-world applicability. Our contributions advance FL algorithms and systems, bridging theoretical advancements and practical implementations. More broadly, our work serves as a guide for researchers navigating the complexities of translating theoretical methods into efficient real-world implementations and software. Additionally, it offers insights into the reverse process of adapting practical implementation aspects back into theoretical algorithm design. This reverse process is particularly intriguing, as the practical perspective compels us to examine the underlying mechanics and flexibilities of algorithms more deeply, often uncovering new dimensions of the algorithms under study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08120v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.25781/KAUST-3TJ71</arxiv:DOI>
      <dc:creator>Konstantin Burlachenko</dc:creator>
    </item>
    <item>
      <title>Contributions to Robust and Efficient Methods for Analysis of High Dimensional Data</title>
      <link>https://arxiv.org/abs/2509.08155</link>
      <description>arXiv:2509.08155v1 Announce Type: cross 
Abstract: A ubiquitous feature of data of our era is their extra-large sizes and dimensions. Analyzing such high-dimensional data poses significant challenges, since the feature dimension is often much larger than the sample size. This thesis introduces robust and computationally efficient methods to address several common challenges associated with high-dimensional data. In my first manuscript, I propose a coherent approach to variable screening that accommodates nonlinear associations. I develop a novel variable screening method that transcends traditional linear assumptions by leveraging mutual information, with an intended application in neuroimaging data. This approach allows for accurate identification of important variables by capturing nonlinear as well as linear relationships between the outcome and covariates. Building on this foundation, I develop new optimization methods for sparse estimation using nonconvex penalties in my second manuscript. These methods address notable challenges in current statistical computing practices, facilitating computationally efficient and robust analyses of complex datasets. The proposed method can be applied to a general class of optimization problems. In my third manuscript, I contribute to robust modeling of high-dimensional correlated observations by developing a mixed-effects model based on Tsallis power-law entropy maximization and discussed the theoretical properties of such distribution. This model surpasses the constraints of conventional Gaussian models by accommodating a broader class of distributions with enhanced robustness to outliers. Additionally, I develop a proximal nonlinear conjugate gradient algorithm that accelerates convergence while maintaining numerical stability, along with rigorous statistical properties for the proposed framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08155v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>physics.data-an</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kai Yang</dc:creator>
    </item>
    <item>
      <title>Quantum Fisher information matrix via its classical counterpart from random measurements</title>
      <link>https://arxiv.org/abs/2509.08196</link>
      <description>arXiv:2509.08196v1 Announce Type: cross 
Abstract: Preconditioning with the quantum Fisher information matrix (QFIM) is a popular approach in quantum variational algorithms. Yet the QFIM is costly to obtain directly, usually requiring more state preparation than its classical counterpart: the classical Fisher information matrix (CFIM). We rigorously prove that averaging the classical Fisher information matrix over Haar-random measurement bases yields $\mathbb{E}_{U\sim\mu_H}[F^U(\boldsymbol{\theta})] = \frac{1}{2}Q(\boldsymbol{\theta})$ for pure states in $\mathbb{C}^N$. Furthermore, we obtain the variance of CFIM ($O(N^{-1})$) and establish non-asymptotic concentration bounds ($\exp(-\Theta(N)t^2)$), demonstrating that using few random measurement bases is sufficient to approximate the QFIM accurately, especially in high-dimensional settings. This work establishes a solid theoretical foundation for efficient quantum natural gradient methods via randomized measurements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08196v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jianfeng Lu, Kecen Sha</dc:creator>
    </item>
    <item>
      <title>Robustness of quantum algorithms: Worst-case fidelity bounds and implications for design</title>
      <link>https://arxiv.org/abs/2509.08481</link>
      <description>arXiv:2509.08481v1 Announce Type: cross 
Abstract: Errors occurring on noisy hardware pose a key challenge to reliable quantum computing. Existing techniques such as error correction, mitigation, or suppression typically separate the error handling from the algorithm analysis and design. In this paper, we develop an alternative, algorithm-centered framework for understanding and improving the robustness against errors. For a given quantum algorithm and error model, we derive worst-case fidelity bounds which can be explicitly computed to certify the robustness. We consider general error models including coherent and (Markovian) incoherent errors and allowing for set-based error descriptions to address uncertainty or time-dependence in the errors. Our results give rise to guidelines for robust algorithm design and compilation by optimizing our theoretical robustness measure. Numerical results on algorithm analysis and robust optimization demonstrate the practicality of the framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08481v1</guid>
      <category>quant-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julian Berberich, Tobias Fellner, Robert L. Kosut, Christian Holm</dc:creator>
    </item>
    <item>
      <title>Modified Loss of Momentum Gradient Descent: Fine-Grained Analysis</title>
      <link>https://arxiv.org/abs/2509.08483</link>
      <description>arXiv:2509.08483v1 Announce Type: cross 
Abstract: We analyze gradient descent with Polyak heavy-ball momentum (HB) whose fixed momentum parameter $\beta \in (0, 1)$ provides exponential decay of memory. Building on Kovachki and Stuart (2021), we prove that on an exponentially attractive invariant manifold the algorithm is exactly plain gradient descent with a modified loss, provided that the step size $h$ is small enough. Although the modified loss does not admit a closed-form expression, we describe it with arbitrary precision and prove global (finite "time" horizon) approximation bounds $O(h^{R})$ for any finite order $R \geq 2$. We then conduct a fine-grained analysis of the combinatorics underlying the memoryless approximations of HB, in particular, finding a rich family of polynomials in $\beta$ hidden inside which contains Eulerian and Narayana polynomials. We derive continuous modified equations of arbitrary approximation order (with rigorous bounds) and the principal flow that approximates the HB dynamics, generalizing Rosca et al. (2023). Approximation theorems cover both full-batch and mini-batch HB. Our theoretical results shed new light on the main features of gradient descent with heavy-ball momentum, and outline a road-map for similar analysis of other optimization algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08483v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Boris Shigida</dc:creator>
    </item>
    <item>
      <title>The rainbow covering number of clean tangled clutters</title>
      <link>https://arxiv.org/abs/2509.08505</link>
      <description>arXiv:2509.08505v1 Announce Type: cross 
Abstract: In this brief note, we prove a min-min equality for a clean tangled clutter, that the rainbow covering number is equal to the connectivity of its setcore.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08505v1</guid>
      <category>math.CO</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ahmad Abdi, G\'erard Cornu\'ejols</dc:creator>
    </item>
    <item>
      <title>A parallel algorithm for generating Pareto-optimal radiosurgery treatment plans</title>
      <link>https://arxiv.org/abs/2509.08602</link>
      <description>arXiv:2509.08602v1 Announce Type: cross 
Abstract: Using inverse planning tools to create radiotherapy treatment plans is an iterative process, where clinical trade-offs are explored by changing the relative importance of different objectives and rerunning the optimizer until a desirable plan is found. We seek to optimize hundreds of radiosurgery treatment plans, corresponding to different weightings of objectives, fast enough to incorporate interactive Pareto navigation of clinical trade-offs into the clinical workflow. We apply the alternating direction method of multipliers (ADMM) to the linear-program formulation of the optimization problem used in Lightning. We implement both a CPU and a GPU version of ADMM in Matlab and compare them to Matlab's built-in, single-threaded dual-simplex solver. The ADMM implementation is adapted to the optimization procedure used in the clinical software, with a bespoke algorithm for maximizing overlap between low-dose points for different objective weights. The method is evaluated on a test dataset consisting of 20 cases from three different indications, with between one and nine targets and total target volumes ranging from 0.66 to 52 cm3, yielding speedups of 1.6-97 and 54-1500 times on CPU and GPU, respectively, compared to simplex. Plan quality was evaluated by rerunning the ADMM optimization 20 times, each with a different random seed, for each test case and for nine objective weightings per case. The resulting clinical metrics closely mimicked those obtained when rerunning the simplex solver, verifying the validity of the method. In conclusion, we show how ADMM can be adapted for radiosurgery plan optimization, allowing hundreds of high-quality Gamma Knife treatment plans to be created in under two minutes on a single GPU, also for very large cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08602v1</guid>
      <category>physics.med-ph</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joakim da Silva, Daniel Hern\'andez Escobar, Tor Kjellsson Lindblom, H{\aa}kan Nordstr\"om, Jens Sj\"olund</dc:creator>
    </item>
    <item>
      <title>Fourier Learning Machines: Nonharmonic Fourier-Based Neural Networks for Scientific Machine Learning</title>
      <link>https://arxiv.org/abs/2509.08759</link>
      <description>arXiv:2509.08759v1 Announce Type: cross 
Abstract: We introduce the Fourier Learning Machine (FLM), a neural network (NN) architecture designed to represent a multidimensional nonharmonic Fourier series. The FLM uses a simple feedforward structure with cosine activation functions to learn the frequencies, amplitudes, and phase shifts of the series as trainable parameters. This design allows the model to create a problem-specific spectral basis adaptable to both periodic and nonperiodic functions. Unlike previous Fourier-inspired NN models, the FLM is the first architecture able to represent a complete, separable Fourier basis in multiple dimensions using a standard Multilayer Perceptron-like architecture. A one-to-one correspondence between the Fourier coefficients and amplitudes and phase-shifts is demonstrated, allowing for the translation between a full, separable basis form and the cosine phase--shifted one. Additionally, we evaluate the performance of FLMs on several scientific computing problems, including benchmark Partial Differential Equations (PDEs) and a family of Optimal Control Problems (OCPs). Computational experiments show that the performance of FLMs is comparable, and often superior, to that of established architectures like SIREN and vanilla feedforward NNs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.08759v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mominul Rubel, Adam Meyers, Gabriel Nicolosi</dc:creator>
    </item>
    <item>
      <title>Damped Proximal Augmented Lagrangian Method for weakly-Convex Problems with Convex Constraints</title>
      <link>https://arxiv.org/abs/2311.09065</link>
      <description>arXiv:2311.09065v2 Announce Type: replace 
Abstract: We give a damped proximal augmented Lagrangian method (DPALM) for solving problems with a weakly-convex objective and convex linear/nonlinear constraints. Instead of taking a full stepsize, DPALM adopts a damped dual stepsize to ensure the boundedness of dual iterates. We show that DPALM can produce a (near) $\vareps$-KKT point within $O(\vareps^{-2})$ outer iterations if each DPALM subproblem is solved to a proper accuracy. In addition, we establish overall iteration complexity of DPALM when the objective is either a regularized smooth function or in a regularized compositional form. For the former case, DPALM achieves the complexity of $\widetilde{\mathcal{O}}\left(\varepsilon^{-2.5} \right)$ to produce an $\varepsilon$-KKT point by applying an accelerated proximal gradient (APG) method to each DPALM subproblem. For the latter case, the complexity of DPALM is $\widetilde{\mathcal{O}}\left(\varepsilon^{-3} \right)$ to produce a near $\varepsilon$-KKT point by using an APG to solve a Moreau-envelope smoothed version of each subproblem. Our outer iteration complexity and the overall complexity either generalize existing best ones from unconstrained or linear-constrained problems to convex-constrained ones, or improve over the best-known results on solving the same-structured problems. Furthermore, numerical experiments on linearly/quadratically constrained non-convex quadratic programs and linear-constrained robust nonlinear least squares are conducted to demonstrate the empirical efficiency of the proposed DPALM over several state-of-the art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09065v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hari Dahal, Wei Liu, Yangyang Xu</dc:creator>
    </item>
    <item>
      <title>On the Sample Complexity of Set Membership Estimation for Linear Systems with Disturbances Bounded by Convex Sets</title>
      <link>https://arxiv.org/abs/2406.00574</link>
      <description>arXiv:2406.00574v3 Announce Type: replace 
Abstract: This paper revisits the set membership identification for linear control systems and establishes its convergence rates under relaxed assumptions on (i) the persistent excitation requirement and (ii) the system disturbances. In particular, instead of assuming persistent excitation exactly, this paper adopts the block-martingale small-ball condition enabled by randomly perturbed control policies to establish the convergence rates of SME with high probability. Further, we relax the assumptions on the shape of the bounded disturbance set and the boundary-visiting condition. Our convergence rates hold for disturbances bounded by general convex sets, which bridges the gap between the previous convergence analysis for general convex sets and the existing convergence rate analysis for $\ell_\infty$ balls. Further, we validate our convergence rates by several numerical experiments.
  This manuscript contains supplementary content in the Appendix.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00574v3</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haonan Xu, Yingying Li</dc:creator>
    </item>
    <item>
      <title>Average Distance of Random Bipartite Matching in One-dimensional Space and Networks</title>
      <link>https://arxiv.org/abs/2409.18292</link>
      <description>arXiv:2409.18292v2 Announce Type: replace 
Abstract: The bipartite matching problem is widely applied in the field of transportation; e.g., to find optimal matches between supply and demand over time and space. Recent efforts have been made on developing analytical formulas to estimate the expected matching distance in bipartite matching with randomly distributed vertices in two- or higher-dimensional spaces, but no accurate formulas currently exist for one-dimensional problems. This paper presents a set of closed-form formulas, without curve-fitting, that can provide accurate average distance estimates for one-dimensional random bipartite matching problems (RBMP). We first focus on a lattice case and propose a new method that relates the corresponding matching distance to the area size between a random walk path and the x-axis. This result directly leads to a straightforward closed-form formula for balanced RBMPs. For unbalanced RBMPs on a lattice, we first analyze the properties of an unbalanced random walk that can be related to balanced RBPMs after optimally removing a subset of unmatched points, and then derive a set of approximate formulas. Additionally, we build upon an optimal point removal strategy to derive a set of recursive formulas that can provide more accurate estimates. Then, we extend the results to three problem variants, including RBMPs with periodic boundaries, uniformly distributed points, and arbitrary-length line. Last, we shift our focus to regular networks, and use the one-dimensional results as building blocks to derive RBMP formulas. To verify the accuracy of the proposed formulas, a set of Monte-Carlo simulations are generated for a variety of matching problems settings. Results indicate that our proposed formulas provide quite accurate distance estimations for one-dimensional line segments and networks under a variety of conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18292v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuhui Zhai, Shiyu Shen, Yanfeng Ouyang</dc:creator>
    </item>
    <item>
      <title>Backstepping for Partial Differential Equations:A Survey</title>
      <link>https://arxiv.org/abs/2410.15146</link>
      <description>arXiv:2410.15146v4 Announce Type: replace 
Abstract: Systems modeled by partial differential equations (PDEs) are at least as ubiquitous as systems that are by nature finite-dimensional and modeled by ordinary differential equations (ODEs). And yet, systematic and readily usable methodologies, for such a significant portion of real systems, have been historically scarce. Around the year 2000, the backstepping approach to PDE control began to offer not only a less abstract alternative to PDE control techniques replicating optimal and spectrum assignment techniques of the 1960s, but also enabled the methodologies of adaptive and nonlinear control, matured in the 1980s and 1990s, to be extended from ODEs to PDEs, allowing feedback synthesis for physical and engineering systems that are uncertain, nonlinear, and infinite-dimensional. The PDE backstepping literature has grown in its nearly a quarter century of development to many hundreds of papers and nearly a dozen books. This survey aims to facilitate the entry, for a new researcher, into this thriving area of overwhelming size and topical diversity. Designs of controllers and observers, for parabolic, hyperbolic, and other classes of PDEs, in one and more dimensions (in box and spherical geometries), with nonlinear, adaptive, sampled-data, and event-triggered extensions, are covered in the survey. The lifeblood of control are technology and physics. The survey places a particular emphasis on applications that have motivated the development of the theory and which have benefited from the theory and designs: applications involving flows, flexible structures, materials, thermal and chemically reacting dynamics, energy (from oil drilling to batteries and magnetic confinement fusions), and vehicles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15146v4</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.automatica.2025.112572</arxiv:DOI>
      <arxiv:journal_reference>Automatica, Volume 183, January 2026, 112572</arxiv:journal_reference>
      <dc:creator>Rafael Vazquez, Jean Auriol, Federico Bribiesca-Argomedo, Miroslav Krstic</dc:creator>
    </item>
    <item>
      <title>A single-loop SPIDER-type stochastic subgradient method for expectation-constrained nonconvex nonsmooth optimization</title>
      <link>https://arxiv.org/abs/2501.19214</link>
      <description>arXiv:2501.19214v2 Announce Type: replace 
Abstract: Many real-world problems, such as those with fairness constraints, involve complex expectation constraints and large datasets, necessitating the design of efficient stochastic methods to solve them. Most existing research focuses on cases with no {constraint} or easy-to-project constraints or deterministic constraints. In this paper, we consider nonconvex nonsmooth stochastic optimization problems with expectation constraints, for which we build a novel exact penalty model. We first show the relationship between the penalty model and the original problem. Then on solving the penalty problem, we present a single-loop SPIDER-type stochastic subgradient method, which utilizes the subgradients of both the objective and constraint functions, as well as the constraint function value at each iteration. Under certain regularity conditions (weaker than Slater-type constraint qualification or strong feasibility assumed in existing works), we establish an iteration complexity result of $O(\epsilon^{-4})$ to reach a near-$\epsilon$ stationary point of the penalized problem in expectation, matching the lower bound for such tasks. Building on the exact penalization, an $(\epsilon,\epsilon)$-KKT point of the original problem is obtained. For a few scenarios, our complexity of either the {objective} sample subgradient or the constraint sample function values can be lower than the state-of-the-art results by a factor of $\epsilon^{-2}$. Moreover, on solving two fairness-constrained problems and a multi-class Neyman-Pearson classification problem, our method is significantly (up to 466 times) faster than the state-of-the-art algorithms, including switching subgradient method and inexact proximal point methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19214v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Liu, Yangyang Xu</dc:creator>
    </item>
    <item>
      <title>A Time-Reversal Control Synthesis for Steering the State of Stochastic Systems</title>
      <link>https://arxiv.org/abs/2504.00238</link>
      <description>arXiv:2504.00238v2 Announce Type: replace 
Abstract: This paper presents a novel approach for steering the state of a stochastic control-affine system to a desired target within a finite time horizon. Our method leverages the time-reversal of diffusion processes to construct the required feedback control law. Specifically, the control law is the so-called score function associated with the time-reversal of random state trajectories that are initialized at the target state and are simulated backwards in time. A neural network is trained to approximate the score function, enabling applicability to both linear and nonlinear stochastic systems. Numerical experiments demonstrate the effectiveness of the proposed method across several benchmark examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00238v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhang Mei, Amirhossein Taghvaei, Ali Pakniyat</dc:creator>
    </item>
    <item>
      <title>Second-order cone programming for distributionally robust compliance optimization of trusses considering input distribution uncertainty</title>
      <link>https://arxiv.org/abs/2504.01678</link>
      <description>arXiv:2504.01678v2 Announce Type: replace 
Abstract: Reliability-based design optimization (RBDO) is a methodology for designing systems and components under the consideration of probabilistic uncertainty. In practical engineering, the number of input data is often limited, which can damage the validity of the optimal results obtained by RBDO. Confidence-based design optimization (CBDO) has been proposed to account for the uncertainty of the input distribution. However, this approach faces challenges, computational cost and accuracy when dealing with highly nonlinear performance constraints. In this paper, we consider the compliance minimization problem of truss structures with uncertain external forces. Armed with the advanced risk measure, conditional Value-at-Risk (CVaR), we formulate a bi-objective optimization problem for the worst-case expected value and the worst-case CVaR of compliance, which allows us to account for the tail risk of performance functions not addressed in CBDO. Employing kernel density estimation for estimation of the input distribution allows us to eliminate the need for modeling the input distribution. We show that this problem reduces to a second-order cone programming when assigning either uniform kernel or triangular kernel. Finally, through numerical experiments, we obtain the Pareto front for the bi-objective optimization problem of the worst-case expected value and CVaR of compliance of truss structures, and confirm the changes in the Pareto solutions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01678v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takumi Fujiyama, Yoshihiro Kanno</dc:creator>
    </item>
    <item>
      <title>A Randomized Zeroth-Order Hierarchical Framework for Heterogeneous Federated Learning</title>
      <link>https://arxiv.org/abs/2504.01839</link>
      <description>arXiv:2504.01839v2 Announce Type: replace 
Abstract: Heterogeneity in federated learning (FL) is a critical and challenging aspect that significantly impacts model performance and convergence. In this paper, we propose a novel framework by formulating heterogeneous FL as a hierarchical optimization problem. This new framework captures both local and global training processes through a bilevel formulation and is capable of the following: (i) addressing client heterogeneity through a personalized learning framework; (ii) capturing the pre-training process on the server side; (iii) updating the global model through nonstandard aggregation; (iv) allowing for nonidentical local steps; and (v) capturing clients' local constraints. We design and analyze an implicit zeroth-order FL method (ZO-HFL), equipped with nonasymptotic convergence guarantees for both the server-agent and the individual client-agents, and asymptotic guarantees for both the server-agent and client-agents in an almost sure sense. Notably, our method does not rely on standard assumptions in heterogeneous FL, such as the bounded gradient dissimilarity condition. We implement our method on image classification tasks and compare with other methods under different heterogeneous settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01839v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuyang Qiu, Kibaek Kim, Farzad Yousefian</dc:creator>
    </item>
    <item>
      <title>Linear Convergence of the Frank-Wolfe Algorithm over Product Polytopes</title>
      <link>https://arxiv.org/abs/2505.11259</link>
      <description>arXiv:2505.11259v2 Announce Type: replace 
Abstract: We study the linear convergence of Frank-Wolfe algorithms over product polytopes. We analyze two condition numbers for the product polytope, namely the \emph{pyramidal width} and the \emph{vertex-facet distance}, based on the condition numbers of individual polytope components. As a result, for convex objectives that are $\mu$-Polyak-{\L}ojasiewicz, we show linear convergence rates quantified in terms of the resulting condition numbers. We apply our results to the problem of approximately finding a feasible point in a polytope intersection in high-dimensions, and demonstrate the practical efficiency of our algorithms through empirical results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11259v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Iommazzo, David Mart\'inez-Rubio, Francisco Criado, Elias Wirth, Sebastian Pokutta</dc:creator>
    </item>
    <item>
      <title>Low degree sum-of-squares bounds for the stability number: a copositive approach</title>
      <link>https://arxiv.org/abs/2509.04949</link>
      <description>arXiv:2509.04949v2 Announce Type: replace 
Abstract: The stability number of a graph $G$, denoted as $\alpha(G)$, is the maximum size of an independent (stable) set in $G$. Semidefinite programming (SDP) methods, which originated from Lov\'asz's theta number and expanded through lift-and-project hierarchies as well as sums of squares (SOS) relaxations, provide powerful tools for approximating $\alpha(G)$.
  We build upon the copositive formulation of $\alpha(G)$ and introduce a novel SDP-based hierarchy of inner approximations to the copositive cone COP$_n$, which is derived from structured SOS representations. This hierarchy preserves essential structural properties that are missing in existing approaches, offers an SDP feasibility formulation at each level despite its non-convexity, and converges finitely to $\alpha(G)$. Our results include examples of graph families that require at least $\alpha(G) - 1$ levels for related hierarchies, indicating the tightness of the de Klerk-Pasechnik conjecture. Notably, on those graph families, our hierarchy achieves $\alpha(G)$ in a single step.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.04949v2</guid>
      <category>math.OC</category>
      <category>math.CO</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Luis Felipe Vargas, Juan C. Vera, Peter J. C. Dickinson</dc:creator>
    </item>
    <item>
      <title>Optimizing a Worldwide-Scale Shipper Transportation Planning in a Carmaker Inbound Supply Chain</title>
      <link>https://arxiv.org/abs/2509.07576</link>
      <description>arXiv:2509.07576v2 Announce Type: replace 
Abstract: We study the shipper-side design of large-scale inbound transportation networks, motivated by Renault's global supply chain. We introduce the Shipper Transportation Design Problem, which integrates consolidation, routing, and regularity constraints, and propose a tailored Iterated Local Search (ILS) metaheuristic. The algorithm combines large-neighborhood search with MILP-based perturbations and exploits bundle-specific decompositions and giant container bounds to obtain scalable lower bounds and effective benchmarks. Computational experiments on real industrial data show that the ILS achieves an average gap of 7.9% to the best available lower bound on world-scale instances with more than 700,000 commodities and 1,200,000 arcs, delivering solutions showing a potential of 23.2% cost reduction compared to the Renault-based benchmark. To our knowledge, this is the first approach to solve shipper-side transportation design problems at such scale. Our analysis further yields managerial insights: accurate bin-packing models are essential for realistic consolidation, highly regular plans offer the best balance between cost and operational stability, and outsourcing is only attractive in low-volume contexts, while large-scale networks benefit from in-house planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07576v2</guid>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathis Brichet, Maximilian Schiffer, Axel Parmentier</dc:creator>
    </item>
    <item>
      <title>SDP bounds on quantum codes</title>
      <link>https://arxiv.org/abs/2408.10323</link>
      <description>arXiv:2408.10323v2 Announce Type: replace-cross 
Abstract: This paper provides a semidefinite programming hierarchy based on state polynomial optimization to determine the existence of quantum codes with given parameters. The hierarchy is complete, in the sense that a $(\!(n, K, {\delta})\!)_2$ code exists if and only if every level of the hierarchy is feasible. It is not limited to stabilizer codes and thus is applicable generally. While the machinery is formally dimension-free, we restrict it to qubit codes through quasi-Clifford algebras. We derive the quantum analog of a range of classical results: first, from an intermediate level a Lov\'asz bound for self-dual quantum codes is recovered. Second, a symmetrization of a minor variation of this Lov\'asz bound recovers the quantum Delsarte bound. Third, a symmetry reduction using the Terwilliger algebra leads to semidefinite programming bounds of size $O(n^4)$. With this we give an alternative proof that there is no $(\!(7, 1, 4)\!)_2$ quantum code, and show that $(\!(8, 9, 3)\!)_2$ and $(\!(10, 5, 4)\!)_2$ codes do not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10323v2</guid>
      <category>quant-ph</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Thu, 11 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gerard Angl\`es Munn\'e, Andrew Nemec, Felix Huber</dc:creator>
    </item>
  </channel>
</rss>
