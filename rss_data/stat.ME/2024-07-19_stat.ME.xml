<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Jul 2024 04:00:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Bayesian Inference and the Principle of Maximum Entropy</title>
      <link>https://arxiv.org/abs/2407.13029</link>
      <description>arXiv:2407.13029v1 Announce Type: new 
Abstract: Bayes' theorem incorporates distinct types of information through the likelihood and prior. Direct observations of state variables enter the likelihood and modify posterior probabilities through consistent updating. Information in terms of expected values of state variables modify posterior probabilities by constraining prior probabilities to be consistent with the information. Constraints on the prior can be exact, limiting hypothetical frequency distributions to only those that satisfy the constraints, or be approximate, allowing residual deviations from the exact constraint to some degree of tolerance. When the model parameters and constraint tolerances are known, posterior probability follows directly from Bayes' theorem. When parameters and tolerances are unknown a prior for them must be specified. When the system is close to statistical equilibrium the computation of posterior probabilities is simplified due to the concentration of the prior on the maximum entropy hypothesis. The relationship between maximum entropy reasoning and Bayes' theorem from this point of view is that maximum entropy reasoning is a special case of Bayesian inference with a constrained entropy-favoring prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13029v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Duncan K. Foley, Ellis Scharfenaker</dc:creator>
    </item>
    <item>
      <title>Combining Climate Models using Bayesian Regression Trees and Random Paths</title>
      <link>https://arxiv.org/abs/2407.13169</link>
      <description>arXiv:2407.13169v1 Announce Type: new 
Abstract: Climate models, also known as general circulation models (GCMs), are essential tools for climate studies. Each climate model may have varying accuracy across the input domain, but no single model is uniformly better than the others. One strategy to improving climate model prediction performance is to integrate multiple model outputs using input-dependent weights. Along with this concept, weight functions modeled using Bayesian Additive Regression Trees (BART) were recently shown to be useful for integrating multiple Effective Field Theories in nuclear physics applications. However, a restriction of this approach is that the weights could only be modeled as piecewise constant functions. To smoothly integrate multiple climate models, we propose a new tree-based model, Random Path BART (RPBART), that incorporates random path assignments into the BART model to produce smooth weight functions and smooth predictions of the physical system, all in a matrix-free formulation. The smoothness feature of RPBART requires a more complex prior specification, for which we introduce a semivariogram to guide its hyperparameter selection. This approach is easy to interpret, computationally cheap, and avoids an expensive cross-validation study. Finally, we propose a posterior projection technique to enable detailed analysis of the fitted posterior weight functions. This allows us to identify a sparse set of climate models that can largely recover the underlying system within a given spatial region as well as quantifying model discrepancy within the model set under consideration. Our method is demonstrated on an ensemble of 8 GCMs modeling the average monthly surface temperature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13169v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John C. Yannotty, Thomas J. Santner, Bo Li, Matthew T. Pratola</dc:creator>
    </item>
    <item>
      <title>Enhanced inference for distributions and quantiles of individual treatment effects in various experiments</title>
      <link>https://arxiv.org/abs/2407.13261</link>
      <description>arXiv:2407.13261v1 Announce Type: new 
Abstract: Understanding treatment effect heterogeneity has become increasingly important in many fields. In this paper we study distributions and quantiles of individual treatment effects to provide a more comprehensive and robust understanding of treatment effects beyond usual averages, despite they are more challenging to infer due to nonidentifiability from observed data. Recent randomization-based approaches offer finite-sample valid inference for treatment effect distributions and quantiles in both completely randomized and stratified randomized experiments, but can be overly conservative by assuming the worst-case scenario where units with large effects are all assigned to the treated (or control) group. We introduce two improved methods to enhance the power of these existing approaches. The first method reinterprets existing approaches as inferring treatment effects among only treated or control units, and then combines the inference for treated and control units to infer treatment effects for all units. The second method explicitly controls for the actual number of treated units with large effects. Both simulation and applications demonstrate the substantial gain from the improved methods. These methods are further extended to sampling-based experiments as well as quasi-experiments from matching, in which the ideas for both improved methods play critical and complementary roles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13261v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Chen, Xinran Li</dc:creator>
    </item>
    <item>
      <title>Heterogeneous Clinical Trial Outcomes via Multi-Output Gaussian Processes</title>
      <link>https://arxiv.org/abs/2407.13283</link>
      <description>arXiv:2407.13283v1 Announce Type: new 
Abstract: We make use of Kronecker structure for scaling Gaussian Process models to large-scale, heterogeneous, clinical data sets. Repeated measures, commonly performed in clinical research, facilitate computational acceleration for nonlinear Bayesian nonparametric models and enable exact sampling for non-conjugate inference, when combinations of continuous and discrete endpoints are observed. Model inference is performed in Stan, and comparisons are made with brms on simulated data and two real clinical data sets, following a radiological image quality theme. Scalable Gaussian Process models compare favourably with parametric models on real data sets with 17,460 observations. Different GP model specifications are explored, with components analogous to random effects, and their theoretical properties are described.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13283v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Owen Thomas, Leiv R{\o}nneberg</dc:creator>
    </item>
    <item>
      <title>Non-zero block selector: A linear correlation coefficient measure for blocking-selection models</title>
      <link>https://arxiv.org/abs/2407.13302</link>
      <description>arXiv:2407.13302v1 Announce Type: new 
Abstract: Multiple-group data is widely used in genomic studies, finance, and social science. This study investigates a block structure that consists of covariate and response groups. It examines the block-selection problem of high-dimensional models with group structures for both responses and covariates, where both the number of blocks and the dimension within each block are allowed to grow larger than the sample size. We propose a novel strategy for detecting the block structure, which includes the block-selection model and a non-zero block selector (NBS). We establish the uniform consistency of the NBS and propose three estimators based on the NBS to enhance modeling efficiency. We prove that the estimators achieve the oracle solution and show that they are consistent, jointly asymptotically normal, and efficient in modeling extremely high-dimensional data. Simulations generate complex data settings and demonstrate the superiority of the proposed method. A gene-data analysis also demonstrates its effectiveness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13302v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weixiong Liang, Yuehan Yang</dc:creator>
    </item>
    <item>
      <title>NIRVAR: Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2407.13314</link>
      <description>arXiv:2407.13314v1 Announce Type: new 
Abstract: High-dimensional panels of time series arise in many scientific disciplines such as neuroscience, finance, and macroeconomics. Often, co-movements within groups of the panel components occur. Extracting these groupings from the data provides a course-grained description of the complex system in question and can inform subsequent prediction tasks. We develop a novel methodology to model such a panel as a restricted vector autoregressive process, where the coefficient matrix is the weighted adjacency matrix of a stochastic block model. This network time series model, which we call the Network Informed Restricted Vector Autoregression (NIRVAR) model, yields a coefficient matrix that has a sparse block-diagonal structure. We propose an estimation procedure that embeds each panel component in a low-dimensional latent space and clusters the embedded points to recover the blocks of the coefficient matrix. Crucially, the method allows for network-based time series modelling when the underlying network is unobserved. We derive the bias, consistency and asymptotic normality of the NIRVAR estimator. Simulation studies suggest that the NIRVAR estimated embedded points are Gaussian distributed around the ground truth latent positions. On three applications to finance, macroeconomics, and transportation systems, NIRVAR outperforms competing factor and network time series models in terms of out-of-sample prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13314v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Francesco Sanna Passino, Mihai Cucuringu, Alessandra Luati</dc:creator>
    </item>
    <item>
      <title>A unifying modelling approach for hierarchical distributed lag models</title>
      <link>https://arxiv.org/abs/2407.13374</link>
      <description>arXiv:2407.13374v1 Announce Type: new 
Abstract: We present a statistical modelling framework for implementing Distributed Lag Models (DLMs), encompassing several extensions of the approach to capture the temporally distributed effect from covariates via regression. We place DLMs in the context of penalised Generalized Additive Models (GAMs) and illustrate that implementation via the R package \texttt{mgcv}, which allows for flexible and interpretable inference in addition to thorough model assessment. We show how the interpretation of penalised splines as random quantities enables approximate Bayesian inference and hierarchical structures in the same practical setting. We focus on epidemiological studies and demonstrate the approach with application to mortality data from Cyprus and Greece. For the Cyprus case study, we investigate for the first time, the joint lagged effects from both temperature and humidity on mortality risk with the unexpected result that humidity severely increases risk during cold rather than hot conditions. Another novel application is the use of the proposed framework for hierarchical pooling, to estimate district-specific covariate-lag risk on morality and the use of posterior simulation to compare risk across districts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13374v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Theo Economou, Daphne Parliari, Aurelio Tobias, Laura Dawkins, Oliver Stoner, Hamish Steptoe, Rachel Lowe, Maria Athanasiadou, Christophe Sarran, Jos Lelieveld</dc:creator>
    </item>
    <item>
      <title>Block-Additive Gaussian Processes under Monotonicity Constraints</title>
      <link>https://arxiv.org/abs/2407.13402</link>
      <description>arXiv:2407.13402v1 Announce Type: new 
Abstract: We generalize the additive constrained Gaussian process framework to handle interactions between input variables while enforcing monotonicity constraints everywhere on the input space. The block-additive structure of the model is particularly suitable in the presence of interactions, while maintaining tractable computations. In addition, we develop a sequential algorithm, MaxMod, for model selection (i.e., the choice of the active input variables and of the blocks). We speed up our implementations through efficient matrix computations and thanks to explicit expressions of criteria involved in MaxMod. The performance and scalability of our methodology are showcased with several numerical examples in dimensions up to 120, as well as in a 5D real-world coastal flooding application, where interpretability is enhanced by the selection of the blocks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13402v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Deronzier, A. F. L\'opez-Lopera, F. Bachoc, O. Roustant, J. Rohmer</dc:creator>
    </item>
    <item>
      <title>Subsampled One-Step Estimation for Fast Statistical Inference</title>
      <link>https://arxiv.org/abs/2407.13446</link>
      <description>arXiv:2407.13446v1 Announce Type: new 
Abstract: Subsampling is an effective approach to alleviate the computational burden associated with large-scale datasets. Nevertheless, existing subsampling estimators incur a substantial loss in estimation efficiency compared to estimators based on the full dataset. Specifically, the convergence rate of existing subsampling estimators is typically $n^{-1/2}$ rather than $N^{-1/2}$, where $n$ and $N$ denote the subsample and full data sizes, respectively. This paper proposes a subsampled one-step (SOS) method to mitigate the estimation efficiency loss utilizing the asymptotic expansions of the subsampling and full-data estimators. The resulting SOS estimator is computationally efficient and achieves a fast convergence rate of $\max\{n^{-1}, N^{-1/2}\}$ rather than $n^{-1/2}$. We establish the asymptotic distribution of the SOS estimator, which can be non-normal in general, and construct confidence intervals on top of the asymptotic distribution. Furthermore, we prove that the SOS estimator is asymptotically normal and equivalent to the full data-based estimator when $n / \sqrt{N} \to \infty$.Simulation studies and real data analyses were conducted to demonstrate the finite sample performance of the SOS estimator. Numerical results suggest that the SOS estimator is almost as computationally efficient as the uniform subsampling estimator while achieving similar estimation efficiency to the full data-based estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13446v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miaomiao Su, Ruoyu Wang</dc:creator>
    </item>
    <item>
      <title>Treatment-control comparisons in platform trials including non-concurrent controls</title>
      <link>https://arxiv.org/abs/2407.13546</link>
      <description>arXiv:2407.13546v1 Announce Type: new 
Abstract: Shared controls in platform trials comprise concurrent and non-concurrent controls. For a given experimental arm, non-concurrent controls refer to data from patients allocated to the control arm before the arm enters the trial. The use of non-concurrent controls in the analysis is attractive because it may increase the trial's power of testing treatment differences while decreasing the sample size. However, since arms are added sequentially in the trial, randomization occurs at different times, which can introduce bias in the estimates due to time trends. In this article, we present methods to incorporate non-concurrent control data in treatment-control comparisons, allowing for time trends. We focus mainly on frequentist approaches that model the time trend and Bayesian strategies that limit the borrowing level depending on the heterogeneity between concurrent and non-concurrent controls. We examine the impact of time trends, overlap between experimental treatment arms and entry times of arms in the trial on the operating characteristics of treatment effect estimators for each method under different patterns for the time trends. We argue under which conditions the methods lead to type 1 error control and discuss the gain in power compared to trials only using concurrent controls by means of a simulation study in which methods are compared.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13546v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marta Bofill Roig, Pavla Krotka, Katharina Hees, Franz Koenig, Dominic Magirr, Peter Jacko, Tom Parke, Martin Posch</dc:creator>
    </item>
    <item>
      <title>Joint modelling of time-to-event and longitudinal response using robust skew normal-independent distributions</title>
      <link>https://arxiv.org/abs/2407.13678</link>
      <description>arXiv:2407.13678v1 Announce Type: new 
Abstract: Joint modelling of longitudinal observations and event times continues to remain a topic of considerable interest in biomedical research. For example, in HIV studies, the longitudinal bio-marker such as CD4 cell count in a patient's blood over follow up months is jointly modelled with the time to disease progression, death or dropout via a random intercept term mostly assumed to be Gaussian. However, longitudinal observations in these kinds of studies often exhibit non-Gaussian behavior (due to high degree of skewness), and parameter estimation is often compromised under violations of the Gaussian assumptions. In linear mixed-effects model assumptions, the distributional assumption for the subject-specific random-effects is taken as Gaussian which may not be true in many situations. Further, this assumption makes the model extremely sensitive to outlying observations. We address these issues in this work by devising a joint model which uses a robust distribution in a parametric setup along with a conditional distributional assumption that ensures dependency of two processes in case the subject-specific random effects is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13678v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Srimanti Dutta, Arindom Chakraborty, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Identifying Research Hotspots and Future Development Trends in Current Psychology: A Bibliometric Analysis of the Past Decade's Publications</title>
      <link>https://arxiv.org/abs/2407.13495</link>
      <description>arXiv:2407.13495v1 Announce Type: cross 
Abstract: By conducting a bibliometric analysis on 4,869 publications in Current Psychology from 2013 to 2022, this paper examined the annual publications and annual citations, as well as the leading institutions, countries, and keywords. CiteSpace, VOSviewer and SCImago Graphica were utilized for visualization analysis. On one hand, this paper analyzed the academic influence of Current Psychology over the past decade. On the other hand, it explored the research hotspots and future development trends within the field of international psychology. The results revealed that the three main research areas covered in the publications of Current Psychology were: the psychological well-being of young people, the negative emotions of adults, and self-awareness and management. The latest research hotspots highlighted in the journal include negative emotions, personality, and mental health. The three main development trends of Current Psychology are: 1) exploring the personality psychology of both adolescents and adults, 2) promoting the interdisciplinary research to study social psychological issues through the use of diversified research methods, and 3) emphasizing the emotional psychology of individuals and their interaction with social reality, from a people-oriented perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13495v1</guid>
      <category>cs.DL</category>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shen Liu, Yan Yang</dc:creator>
    </item>
    <item>
      <title>Topological Analysis of Seizure-Induced Changes in Brain Hierarchy Through Effective Connectivity</title>
      <link>https://arxiv.org/abs/2407.13514</link>
      <description>arXiv:2407.13514v1 Announce Type: cross 
Abstract: Traditional Topological Data Analysis (TDA) methods, such as Persistent Homology (PH), rely on distance measures (e.g., cross-correlation, partial correlation, coherence, and partial coherence) that are symmetric by definition. While useful for studying topological patterns in functional brain connectivity, the main limitation of these methods is their inability to capture the directional dynamics - which is crucial for understanding effective brain connectivity. We propose the Causality-Based Topological Ranking (CBTR) method, which integrates Causal Inference (CI) to assess effective brain connectivity with Hodge Decomposition (HD) to rank brain regions based on their mutual influence. Our simulations confirm that the CBTR method accurately and consistently identifies hierarchical structures in multivariate time series data. Moreover, this method effectively identifies brain regions showing the most significant interaction changes with other regions during seizures using electroencephalogram (EEG) data. These results provide novel insights into the brain's hierarchical organization and illuminate the impact of seizures on its dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13514v1</guid>
      <category>q-bio.NC</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anass B. El-Yaagoubi, Moo K. Chung, Hernando Ombao</dc:creator>
    </item>
    <item>
      <title>Optimal rates for estimating the covariance kernel from synchronously sampled functional data</title>
      <link>https://arxiv.org/abs/2407.13641</link>
      <description>arXiv:2407.13641v1 Announce Type: cross 
Abstract: We obtain minimax-optimal convergence rates in the supremum norm, including infor-mation-theoretic lower bounds, for estimating the covariance kernel of a stochastic process which is repeatedly observed at discrete, synchronous design points. In particular, for dense design we obtain the $\sqrt n$-rate of convergence in the supremum norm without additional logarithmic factors which typically occur in the results in the literature. Surprisingly, in the transition from dense to sparse design the rates do not reflect the two-dimensional nature of the covariance kernel but correspond to those for univariate mean function estimation. Our estimation method can make use of higher-order smoothness of the covariance kernel away from the diagonal, and does not require the same smoothness on the diagonal itself. Hence, as in Mohammadi and Panaretos (2024) we can cover covariance kernels of processes with rough sample paths. Moreover, the estimator does not use mean function estimation to form residuals, and no smoothness assumptions on the mean have to be imposed. In the dense case we also obtain a central limit theorem in the supremum norm, which can be used as the basis for the construction of uniform confidence sets. Simulations and real-data applications illustrate the practical usefulness of the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13641v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Berger, Hajo Holzmann</dc:creator>
    </item>
    <item>
      <title>Tensor Factor Model Estimation by Iterative Projection</title>
      <link>https://arxiv.org/abs/2006.02611</link>
      <description>arXiv:2006.02611v3 Announce Type: replace 
Abstract: Tensor time series, which is a time series consisting of tensorial observations, has become ubiquitous. It typically exhibits high dimensionality. One approach for dimension reduction is to use a factor model structure, in a form similar to Tucker tensor decomposition, except that the time dimension is treated as a dynamic process with a time dependent structure. In this paper we introduce two approaches to estimate such a tensor factor model by using iterative orthogonal projections of the original tensor time series. These approaches extend the existing estimation procedures and improve the estimation accuracy and convergence rate significantly as proven in our theoretical investigation. Our algorithms are similar to the higher order orthogonal projection method for tensor decomposition, but with significant differences due to the need to unfold tensors in the iterations and the use of autocorrelation. Consequently, our analysis is significantly different from the existing ones. Computational and statistical lower bounds are derived to prove the optimality of the sample size requirement and convergence rate for the proposed methods. Simulation study is conducted to further illustrate the statistical properties of these estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2006.02611v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuefeng Han, Rong Chen, Dan Yang, Cun-Hui Zhang</dc:creator>
    </item>
    <item>
      <title>Agglomerative Hierarchical Clustering for Selecting Valid Instrumental Variables</title>
      <link>https://arxiv.org/abs/2101.05774</link>
      <description>arXiv:2101.05774v4 Announce Type: replace 
Abstract: We propose a procedure which combines hierarchical clustering with a test of overidentifying restrictions for selecting valid instrumental variables (IV) from a large set of IVs. Some of these IVs may be invalid in that they fail the exclusion restriction. We show that if the largest group of IVs is valid, our method achieves oracle properties. Unlike existing techniques, our work deals with multiple endogenous regressors. Simulation results suggest an advantageous performance of the method in various settings. The method is applied to estimating the effect of immigration on wages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2101.05774v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nicolas Apfel, Xiaoran Liang</dc:creator>
    </item>
    <item>
      <title>Design Sensitivity and Its Implications for Weighted Observational Studies</title>
      <link>https://arxiv.org/abs/2307.00093</link>
      <description>arXiv:2307.00093v2 Announce Type: replace 
Abstract: Sensitivity to unmeasured confounding is not typically a primary consideration in designing treated-control comparisons in observational studies. We introduce a framework allowing researchers to optimize robustness to omitted variable bias at the design stage using a measure called design sensitivity. Design sensitivity, which describes the asymptotic power of a sensitivity analysis, allows transparent assessment of the impact of different estimation strategies on sensitivity. We apply this general framework to two commonly-used sensitivity models, the marginal sensitivity model and the variance-based sensitivity model. By comparing design sensitivities, we interrogate how key features of weighted designs, including choices about trimming of weights and model augmentation, impact robustness to unmeasured confounding, and how these impacts may differ for the two different sensitivity models. We illustrate the proposed framework on a study examining drivers of support for the 2016 Colombian peace agreement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.00093v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Melody Huang, Dan Soriano, Samuel D. Pimentel</dc:creator>
    </item>
    <item>
      <title>Latent Gaussian dynamic factor modeling and forecasting for multivariate count time series</title>
      <link>https://arxiv.org/abs/2307.10454</link>
      <description>arXiv:2307.10454v2 Announce Type: replace 
Abstract: This work considers estimation and forecasting in a multivariate, possibly high-dimensional count time series model constructed from a transformation of a latent Gaussian dynamic factor series. The estimation of the latent model parameters is based on second-order properties of the count and underlying Gaussian time series, yielding estimators of the underlying covariance matrices for which standard principal component analysis applies. Theoretical consistency results are established for the proposed estimation, building on certain concentration results for the models of the type considered. They also involve the memory of the latent Gaussian process, quantified through a spectral gap, shown to be suitably bounded as the model dimension increases, which is of independent interest. In addition, novel cross-validation schemes are suggested for model selection. The forecasting is carried out through a particle-based sequential Monte Carlo, leveraging Kalman filtering techniques. A simulation study and an application are also considered.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10454v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Marie-Christine D\"uker, Zachary F. Fisher, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>Dynamic survival analysis: modelling the hazard function via ordinary differential equations</title>
      <link>https://arxiv.org/abs/2308.05205</link>
      <description>arXiv:2308.05205v5 Announce Type: replace 
Abstract: The hazard function represents one of the main quantities of interest in the analysis of survival data. We propose a general approach for parametrically modelling the dynamics of the hazard function using systems of autonomous ordinary differential equations (ODEs). This modelling approach can be used to provide qualitative and quantitative analyses of the evolution of the hazard function over time. Our proposal capitalises on the extensive literature of ODEs which, in particular, allow for establishing basic rules or laws on the dynamics of the hazard function via the use of autonomous ODEs. We show how to implement the proposed modelling framework in cases where there is an analytic solution to the system of ODEs or where an ODE solver is required to obtain a numerical solution. We focus on the use of a Bayesian modelling approach, but the proposed methodology can also be coupled with maximum likelihood estimation. A simulation study is presented to illustrate the performance of these models and the interplay of sample size and censoring. Two case studies using real data are presented to illustrate the use of the proposed approach and to highlight the interpretability of the corresponding models. We conclude with a discussion on potential extensions of our work and strategies to include covariates into our framework. Although we focus on examples on Medical Statistics, the proposed framework is applicable in any context where the interest lies on estimating and interpreting the dynamics hazard function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05205v5</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. A. Christen, F. J. Rubio</dc:creator>
    </item>
    <item>
      <title>Bayesian Deep Generative Models for Replicated Networks with Multiscale Overlapping Clusters</title>
      <link>https://arxiv.org/abs/2405.20936</link>
      <description>arXiv:2405.20936v2 Announce Type: replace 
Abstract: Our interest is in replicated network data with multiple networks observed across the same set of nodes. Examples include brain connection networks, in which nodes corresponds to brain regions and replicates to different individuals, and ecological networks, in which nodes correspond to species and replicates to samples collected at different locations and/or times. Our goal is to infer a hierarchical structure of the nodes at a population level, while performing multi-resolution clustering of the individual replicates. In brain connectomics, the focus is on inferring common relationships among the brain regions, while characterizing inter-individual variability in an easily interpretable manner. To accomplish this, we propose a Bayesian hierarchical model, while providing theoretical support in terms of identifiability and posterior consistency, and design efficient methods for posterior computation. We provide novel technical tools for proving model identifiability, which are of independent interest. Our simulations and application to brain connectome data provide support for the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20936v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuren Zhou, Yuqi Gu, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Effect estimation in the presence of a misclassified binary mediator</title>
      <link>https://arxiv.org/abs/2407.06970</link>
      <description>arXiv:2407.06970v2 Announce Type: replace 
Abstract: Mediation analyses allow researchers to quantify the effect of an exposure variable on an outcome variable through a mediator variable. If a binary mediator variable is misclassified, the resulting analysis can be severely biased. Misclassification is especially difficult to deal with when it is differential and when there are no gold standard labels available. Previous work has addressed this problem using a sensitivity analysis framework or by assuming that misclassification rates are known. We leverage a variable related to the misclassification mechanism to recover unbiased parameter estimates without using gold standard labels. The proposed methods require the reasonable assumption that the sum of the sensitivity and specificity is greater than 1. Three correction methods are presented: (1) an ordinary least squares correction for Normal outcome models, (2) a multi-step predictive value weighting method, and (3) a seamless expectation-maximization algorithm. We apply our misclassification correction strategies to investigate the mediating role of gestational hypertension on the association between maternal age and pre-term birth.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06970v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kimberly A. Hochstedler Webb, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>A simple extension of Azadkia &amp; Chatterjee's rank correlation to multi-response vectors</title>
      <link>https://arxiv.org/abs/2212.01621</link>
      <description>arXiv:2212.01621v3 Announce Type: replace-cross 
Abstract: Recently, Chatterjee (2023) recognized the lack of a direct generalization of his rank correlation $\xi$ in Azadkia and Chatterjee (2021) to a multi-dimensional response vector. As a natural solution to this problem, we here propose an extension of $\xi$ that is applicable to a set of $q \geq 1$ response variables, where our approach builds upon converting the original vector-valued problem into a univariate problem and then applying the rank correlation $\xi$ to it. Our novel measure $T$ quantifies the scale-invariant extent of functional dependence of a response vector $\mathbf{Y} = (Y_1,\dots,Y_q)$ on predictor variables $\mathbf{X} = (X_1, \dots,X_p)$, characterizes independence of $\mathbf{X}$ and $\mathbf{Y}$ as well as perfect dependence of $\mathbf{Y}$ on $\mathbf{X}$ and hence fulfills all the characteristics of a measure of predictability. Aiming at maximum interpretability, we provide various invariance results for $T$ as well as a closed-form expression in multivariate normal models. Building upon the graph-based estimator for $\xi$ in Azadkia and Chatterjee (2021), we obtain a non-parametric, strongly consistent estimator for $T$ and show its asymptotic normality. Based on this estimator, we develop a model-free and dependence-based feature ranking and forward feature selection for multiple-outcome data. Simulation results and real case studies illustrate $T$'s broad applicability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01621v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan Ansari, Sebastian Fuchs</dc:creator>
    </item>
    <item>
      <title>Sparse and geometry-aware generalisation of the mutual information for joint discriminative clustering and feature selection</title>
      <link>https://arxiv.org/abs/2302.03391</link>
      <description>arXiv:2302.03391v2 Announce Type: replace-cross 
Abstract: Feature selection in clustering is a hard task which involves simultaneously the discovery of relevant clusters as well as relevant variables with respect to these clusters. While feature selection algorithms are often model-based through optimised model selection or strong assumptions on the data distribution, we introduce a discriminative clustering model trying to maximise a geometry-aware generalisation of the mutual information called GEMINI with a simple l1 penalty: the Sparse GEMINI. This algorithm avoids the burden of combinatorial feature subset exploration and is easily scalable to high-dimensional data and large amounts of samples while only designing a discriminative clustering model. We demonstrate the performances of Sparse GEMINI on synthetic datasets and large-scale datasets. Our results show that Sparse GEMINI is a competitive algorithm and has the ability to select relevant subsets of variables with respect to the clustering without using relevance criteria or prior hypotheses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03391v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Louis Ohl, Pierre-Alexandre Mattei, Charles Bouveyron, Micka\"el Leclercq, Arnaud Droit, Fr\'ed\'eric Precioso</dc:creator>
    </item>
    <item>
      <title>Physics-Informed Priors with Application to Boundary Layer Velocity</title>
      <link>https://arxiv.org/abs/2311.12978</link>
      <description>arXiv:2311.12978v2 Announce Type: replace-cross 
Abstract: One of the most popular recent areas of machine learning predicates the use of neural networks augmented by information about the underlying process in the form of Partial Differential Equations (PDEs). These physics-informed neural networks are obtained by penalizing the inference with a PDE, and have been cast as a minimization problem currently lacking a formal approach to quantify the uncertainty. In this work, we propose a novel model-based framework which regards the PDE as a prior information of a deep Bayesian neural network. The prior is calibrated without data to resemble the PDE solution in the prior mean, while our degree in confidence on the PDE with respect to the data is expressed in terms of the prior variance. The information embedded in the PDE is then propagated to the posterior yielding physics-informed forecasts with uncertainty quantification. We apply our approach to a simulated viscous fluid and to experimentally-obtained turbulent boundary layer velocity in a water tunnel using an appropriately simplified Navier-Stokes equation. Our approach requires very few observations to produce physically-consistent forecasts as opposed to non-physical forecasts stemming from non-informed priors, thereby allowing forecasting complex systems where some amount of data as well as some contextual knowledge is available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12978v2</guid>
      <category>physics.flu-dyn</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Menicali, David H. Richter, Stefano Castruccio</dc:creator>
    </item>
    <item>
      <title>Scalable Spatiotemporal Prediction with Bayesian Neural Fields</title>
      <link>https://arxiv.org/abs/2403.07657</link>
      <description>arXiv:2403.07657v2 Announce Type: replace-cross 
Abstract: Spatiotemporal datasets, which consist of spatially-referenced time series, are ubiquitous in many scientific and business-intelligence applications, such as air pollution monitoring, disease tracking, and cloud-demand forecasting. As modern datasets continue to increase in size and complexity, there is a growing need for new statistical methods that are flexible enough to capture complex spatiotemporal dynamics and scalable enough to handle large prediction problems. This work presents the Bayesian Neural Field (BayesNF), a domain-general statistical model for inferring rich probability distributions over a spatiotemporal domain, which can be used for data-analysis tasks including forecasting, interpolation, and variography. BayesNF integrates a novel deep neural network architecture for high-capacity function estimation with hierarchical Bayesian inference for robust uncertainty quantification. By defining the prior through a sequence of smooth differentiable transforms, posterior inference is conducted on large-scale data using variationally learned surrogates trained via stochastic gradient descent. We evaluate BayesNF against prominent statistical and machine-learning baselines, showing considerable improvements on diverse prediction problems from climate and public health datasets that contain tens to hundreds of thousands of measurements. The paper is accompanied with an open-source software package (https://github.com/google/bayesnf) that is easy-to-use and compatible with modern GPU and TPU accelerators on the JAX machine learning platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.07657v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feras Saad, Jacob Burnim, Colin Carroll, Brian Patton, Urs K\"oster, Rif A. Saurous, Matthew Hoffman</dc:creator>
    </item>
    <item>
      <title>Hidden Markov models with an unknown number of states and a repulsive prior on the state parameters</title>
      <link>https://arxiv.org/abs/2407.10869</link>
      <description>arXiv:2407.10869v2 Announce Type: replace-cross 
Abstract: Hidden Markov models (HMMs) offer a robust and efficient framework for analyzing time series data, modelling both the underlying latent state progression over time and the observation process, conditional on the latent state. However, a critical challenge lies in determining the appropriate number of underlying states, often unknown in practice. In this paper, we employ a Bayesian framework, treating the number of states as a random variable and employing reversible jump Markov chain Monte Carlo to sample from the posterior distributions of all parameters, including the number of states. Additionally, we introduce repulsive priors for the state parameters in HMMs, and hence avoid overfitting issues and promote parsimonious models with dissimilar state components. We perform an extensive simulation study comparing performance of models with independent and repulsive prior distributions on the state parameters, and demonstrate our proposed framework on two ecological case studies: GPS tracking data on muskox in Antarctica and acoustic data on Cape gannets in South Africa. Our results highlight how our framework effectively explores the model space, defined by models with different latent state dimensions, while leading to latent states that are distinguished better and hence are more interpretable, enabling better understanding of complex dynamic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10869v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Rotous, Alex Diana, Alessio Farcomeni, Eleni Matechou, Andr\'ea Thiebault</dc:creator>
    </item>
  </channel>
</rss>
