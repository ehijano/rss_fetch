<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 02:42:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Restless Multi-Process Multi-Armed Bandits with Applications to Self-Driving Microscopies</title>
      <link>https://arxiv.org/abs/2512.14930</link>
      <description>arXiv:2512.14930v1 Announce Type: new 
Abstract: High-content screening microscopy generates large amounts of live-cell imaging data, yet its potential remains constrained by the inability to determine when and where to image most effectively. Optimally balancing acquisition time, computational capacity, and photobleaching budgets across thousands of dynamically evolving regions of interest remains an open challenge, further complicated by limited field-of-view adjustments and sensor sensitivity. Existing approaches either rely on static sampling or heuristics that neglect the dynamic evolution of biological processes, leading to inefficiencies and missed events. Here, we introduce the restless multi-process multi-armed bandit (RMPMAB), a new decision-theoretic framework in which each experimental region is modeled not as a single process but as an ensemble of Markov chains, thereby capturing the inherent heterogeneity of biological systems such as asynchronous cell cycles and heterogeneous drug responses. Building upon this foundation, we derive closed-form expressions for transient and asymptotic behaviors of aggregated processes, and design scalable Whittle index policies with sub-linear complexity in the number of imaging regions. Through both simulations and a real biological live-cell imaging dataset, we show that our approach achieves substantial improvements in throughput under resource constraints. Notably, our algorithm outperforms Thomson Sampling, Bayesian UCB, epsilon-Greedy, and Round Robin by reducing cumulative regret by more than 37% in simulations and capturing 93% more biologically relevant events in live imaging experiments, underscoring its potential for transformative smart microscopy. Beyond improving experimental efficiency, the RMPMAB framework unifies stochastic decision theory with optimal autonomous microscopy control, offering a principled approach to accelerate discovery across multidisciplinary sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14930v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaume Anguera Peris, Songtao Cheng, Hanzhao Zhang, Wei Ouyang, Joakim Jald\'en</dc:creator>
    </item>
    <item>
      <title>Early CRAB-like Biomarker Signatures Reveal a Preclinical Susceptibility Continuum for Multiple Myeloma</title>
      <link>https://arxiv.org/abs/2512.15056</link>
      <description>arXiv:2512.15056v1 Announce Type: new 
Abstract: Multiple myeloma (MM) evolves over decades, yet robust tools for identifying individuals at risk long before clinical onset remain limited. Using data from 378,930 UK Biobank participants, we systematically characterized the longitudinal dynamics and predictive value of routinely measured "CRAB-like" biomarkers, including hematologic indices, protein metabolism markers, renal function, and serum calcium. Across multivariable models, biomarkers reflecting anemia and protein imbalance (including hemoglobin, red blood cell indices, total protein, albumin, and the albumin/globulin ratio) showed strong and consistent associations with future MM, independent of demographic, lifestyle, clinical, and genetic risk factors. These markers displayed pronounced non-linear dose-response relationships and contributed substantially to 5- and 10-year MM risk discrimination, with the C-index improving from 0.66 to 0.76. Longitudinal analyses revealed progressive shifts in red cell morphology and protein metabolism profiles up to a decade before diagnosis, supporting the existence of a preclinical susceptibility continuum detectable in the general population. Our findings suggest that subtle yet quantifiable deviations in common laboratory tests reflect early microenvironmental changes that precede malignant plasma cell expansion, offering opportunities for risk stratification and targeted surveillance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15056v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Bingjie Li, Jiadai Xu, Yiqing Sun, Peng Liu, Zhigang Yao</dc:creator>
    </item>
    <item>
      <title>A Blind Source Separation Framework to Monitor Sectoral Power Demand from Grid-Scale Load Measurements</title>
      <link>https://arxiv.org/abs/2512.15232</link>
      <description>arXiv:2512.15232v1 Announce Type: new 
Abstract: As we are moving towards decentralized power systems dominated by intermittent electricity generation from renewable energy sources, demand-side flexibility is becoming a critical issue. In this context, it is essential to understand the composition of electricity demand at various scales of the power grid. At the regional or national scale, there is however little visibility on the relative contributions of different consumer categories, due to the complexity and costs of collecting end-users consumption data. To address this issue, we propose a blind source separation framework based on a constrained variant of non-negative matrix factorization to monitor the consumption of residential, services and industrial sectors at high frequency from aggregate high-voltage grid load measurements. Applying the method to Italy's national load curve between 2021 and 2023, we reconstruct accurate hourly consumption profiles for each sector. Results reveal that both households and services daily consumption behaviors are driven by two distinct regimes related to the season and day type whereas industrial demand follows a single, stable daily profile. Besides, the monthly consumption estimates of each sector derived from the disaggregated load are found to closely align with sample-based indices and be more precise than forecasting approaches based on these indices for real-time monitoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15232v1</guid>
      <category>stat.AP</category>
      <category>eess.SP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillaume Koechlin, Filippo Bovera, Elena Degli Innocenti, Barbara Santini, Alessandro Venturi, Simona Vazio, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>Cyclists route choice modeling from trip duration data in urban areas</title>
      <link>https://arxiv.org/abs/2512.15257</link>
      <description>arXiv:2512.15257v1 Announce Type: new 
Abstract: The lack of GPS data limits the ability to reconstruct the actual routes taken by cyclists in urban areas. This article introduces an inference method based solely on trip durations and origin-destination pairs from bike-sharing system (BSS) users. Travel time distributions are modeled using log-normal mixture models, allowing us to identify the presence of distinct behaviors. The approach is applied to 3.8 million trips recorded in 2022 in the Toulouse metropolitan area, with observed durations compared against travel times estimated by OpenStreetMap (OSM). Results show that, for many station pairs, trip durations align closely with the fastest route suggested by OSM, reflecting a dominant and routine practice. In other cases, mixture models reveal more heterogeneous behaviors, including longer trips, detours, or intermediate stops. This approach highlights both the stability and diversity of cycling practices, providing a robust tool for usage analysis in data-limited contexts, and offering new insights into urban mobility dynamics without relying on spatially explicit data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15257v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bertrand Jouve (LISST), Paul Rochet (OPTIM), Mohamadou Salifou (LISST)</dc:creator>
    </item>
    <item>
      <title>Change detection with adaptive sampling for binary responses</title>
      <link>https://arxiv.org/abs/2512.15507</link>
      <description>arXiv:2512.15507v1 Announce Type: new 
Abstract: We propose using an adaptive sampling method to detect changes for a system with multiple lines. The adaptive sampling utilizes the information in responses to learn on which line is more likely to have a change thus allocating more units to the line. The learning process is formatted as a Markov decision process by integrating sampling information with likelihood ratio for changes to define rewards and the optimal sampling is approximated by using the Bellman operator iteratively based on the average reward criterion. We demonstrate the performance of the proposed method for binary responses using the exact distribution method for adaptive sampling. Our numeric results show that the adaptive sampling samples more often the line that has a change and the statistical power to detect a change is better than those with the equal randomization for sample sizes of 20 or higher. When sample sizes increase or the difference between out-of-control and in-control probabilities increases, the adaptive sampling allocates higher proportion of units averagely to the line with a change and the statistical power to detect a change increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15507v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yanqing Yi, Su-Fen Yang</dc:creator>
    </item>
    <item>
      <title>A Statistical Framework for Spatial Boundary Estimation and Change Detection: Application to the Sahel Sahara Climate Transition</title>
      <link>https://arxiv.org/abs/2512.15650</link>
      <description>arXiv:2512.15650v1 Announce Type: new 
Abstract: Spatial boundaries, such as ecological transitions or climatic regime interfaces, capture steep environmental gradients, and shifts in their structure can signal emerging environmental changes. Quantifying uncertainty in spatial boundary locations and formally testing for temporal shifts remains challenging, especially when boundaries are derived from noisy, gridded environmental data. We present a unified framework that combines heteroskedastic Gaussian process (GP) regression with a scaled Maximum Absolute Difference (MAD) Global Envelope Test (GET) to estimate spatial boundary curves and assess whether they evolve over time. The heteroskedastic GP provides a flexible probabilistic reconstruction of boundary lines, capturing spatially varying mean structure and location specific variability, while the test offers a rigorous hypothesis testing tool for detecting departures from expected boundary behaviors. Simulation studies show that the proposed method achieves the correct size under the null and high power for detecting local boundary shifts. Applying our framework to the Sahel Sahara transition zone, using annual Koppen Trewartha climate classifications from 1960 to 1989, we find no statistically significant decade scale changes in the arid and semi arid or semi arid and non arid interfaces. However, the method successfully identifies localized boundary shifts during the extreme drought years of 1983 and 1984, consistent with climate studies documenting regional anomalies in these interfaces during that period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15650v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stephen Tivenan, Indranil Sahoo, Yanjun Qian</dc:creator>
    </item>
    <item>
      <title>Layer-2 Adoption and Ethereum Mainnet Congestion: Regime-Aware Causal Evidence Across London, the Merge, and Dencun (2021-2024)</title>
      <link>https://arxiv.org/abs/2512.14724</link>
      <description>arXiv:2512.14724v1 Announce Type: cross 
Abstract: Do Ethereum's Layer-2 (L2) rollups actually decongest the Layer-1 (L1) mainnet once protocol upgrades and demand are held constant? Using a 1245-day daily panel from August 5, 2021 to December 31, 2024 that spans the London, Merge, and Dencun upgrades, we link Ethereum fee and congestion metrics to L2 user activity, macro-demand proxies, and targeted event indicators. We estimate a regime-aware error-correction model that treats posting-clean L2 user share as a continuous treatment. Over the pre-Dencun (London+Merge) window, a 10 percentage point increase in L2 adoption lowers median base fees by about 13% -- roughly 5 Gwei at pre-Dencun levels -- and deviations from the long-run relation decay with an 11-day half-life. Block utilization and a scarcity index show similar congestion relief. After Dencun, L2 adoption is already high and treatment support narrows, so blob-era estimates are statistically imprecise and we treat them as exploratory. The pre-Dencun window therefore delivers the first cross-regime causal estimate of how aggregate L2 adoption decongests Ethereum, together with a reusable template for monitoring rollup-centric scaling strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14724v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aysajan Eziz</dc:creator>
    </item>
    <item>
      <title>Evaluating Weather Forecasts from a Decision Maker's Perspective</title>
      <link>https://arxiv.org/abs/2512.14779</link>
      <description>arXiv:2512.14779v1 Announce Type: cross 
Abstract: Standard weather forecast evaluations focus on the forecaster's perspective and on a statistical assessment comparing forecasts and observations. In practice, however, forecasts are used to make decisions, so it seems natural to take the decision-maker's perspective and quantify the value of a forecast by its ability to improve decision-making. Decision calibration provides a novel framework for evaluating forecast performance at the decision level rather than the forecast level. We evaluate decision calibration to compare Machine Learning and classical numerical weather prediction models on various weather-dependent decision tasks. We find that model performance at the forecast level does not reliably translate to performance in downstream decision-making: some performance differences only become apparent at the decision level, and model rankings can change among different decision tasks. Our results confirm that typical forecast evaluations are insufficient for selecting the optimal forecast model for a specific decision task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14779v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kornelius Raeth, Nicole Ludwig</dc:creator>
    </item>
    <item>
      <title>Bayesian Latent Class Regression and Variable Selection with Applications to Sleep Patterns Data</title>
      <link>https://arxiv.org/abs/2512.14903</link>
      <description>arXiv:2512.14903v1 Announce Type: cross 
Abstract: Sleep difficulties in children are heterogeneous in presentation, yet conventional assessment tools like the Children's Sleep Habits Questionnaire (CSHQ) reduce this complexity to a single cumulative score, obscuring distinct patterns of sleep disturbance that require different interventions. Latent Class Regression (LCR) models offer a principled approach to identify subgroups with shared sleep behaviour profiles whilst incorporating predictors of group membership, but Bayesian inference for these models has been hindered by computational challenges and the absence of variable selection methods. We propose a fully Bayesian framework for LCR that uses P\'olya-Gamma data augmentation, enabling efficient sampling of regression coefficients. We extend this framework to include variable selection for both predictors and item responses: predictor variable selection via latent inclusion indicators and item selection through a partially collapsed approach. Through simulation studies, we show that the proposed methods yield accurate parameter estimates, resolve identifiability issues arising in full models and successfully identify informative predictors and items while excluding noise variables. Applying this methodology to CSHQ data from 148 children reveals distinct latent subgroups with different sleep behaviour profiles, anxious nighttime sleepers, short/light sleepers and those with more pervasive sleep problems, with each carrying distinct implications for intervention. Results also highlight the predictive role of Autism Spectrum Disorder diagnosis in subgroup membership. These findings demonstrate the limitations of conventional CSHQ scoring and illustrate the benefits of a probabilistic subgroup-based approach as an alternative for understanding paediatric sleep difficulties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.14903v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Heaney, Olive Healy, Jason Wyse, Arthur White</dc:creator>
    </item>
    <item>
      <title>Data-driven controlled subgroup selection in clinical trials</title>
      <link>https://arxiv.org/abs/2512.15676</link>
      <description>arXiv:2512.15676v1 Announce Type: cross 
Abstract: Subgroup selection in clinical trials is essential for identifying patient groups that react differently to a treatment, thereby enabling personalised medicine. In particular, subgroup selection can identify patient groups that respond particularly well to a treatment or that encounter adverse events more often. However, this is a post-selection inference problem, which may pose challenges for traditional techniques used for subgroup analysis, such as increased Type I error rates and potential biases from data-driven subgroup identification. In this paper, we present two methods for subgroup selection in regression problems: one based on generalised linear modelling and another on isotonic regression. We demonstrate how these methods can be used for data-driven subgroup identification in the analysis of clinical trials, focusing on two distinct tasks: identifying patient groups that are safe from manifesting adverse events and identifying patient groups with high treatment effect, while controlling for Type I error in both cases. A thorough simulation study is conducted to evaluate the strengths and weaknesses of each method, providing detailed insight into the sensitivity of the Type I error rate control to modelling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15676v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuel M. M\"uller, Bj\"orn Bornkamp, Frank Bretz, Timothy I. Cannings, Wei Liu, Henry W. J. Reeve, Richard J. Samworth, Nikolaos Sfikas, Fang Wan, Konstantinos Sechidis</dc:creator>
    </item>
    <item>
      <title>3D Bivariate Spatial Modelling of Argo Ocean Temperature and Salinity Profiles</title>
      <link>https://arxiv.org/abs/2210.11611</link>
      <description>arXiv:2210.11611v3 Announce Type: replace 
Abstract: Variables within the global oceans can reveal the impacts of a warming climate, as the oceans absorb huge amounts of solar energy. Understanding the joint spatial distribution of key ocean variables is therefore essential. In this paper, we investigate the spatial dependence structure between ocean temperature and salinity using Argo observations and construct a bivariate spatial model covering from the surface through the ocean interior. We develop a flexible class of multivariate nonstationary covariance models defined in 3-dimensional (3D) space (longitude $\times$ latitude $\times$ depth) that allow the variances and correlations to vary with depth, capturing the ocean's vertical structure. These models describe the joint spatial distribution of the two variables while incorporating the underlying vertical structure of the ocean. We apply this framework to Argo temperature and salinity data and address the computational challenges of large data volumes through the Vecchia approximation. Our results show that the proposed bivariate covariance model effectively represents the complex vertical cross-covariance structure of the processes and their first- and second-order differences, whereas classical bivariate models, including the bivariate Mat\'{e}rn, poorly fit the empirical cross-covariance structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.11611v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Lai Salvana, Jian Cao, Mikyoung Jun</dc:creator>
    </item>
    <item>
      <title>Predicting Power Grid Failures Using Self-Organized Criticality: A Case Study of the Texas Grid 2014-2022</title>
      <link>https://arxiv.org/abs/2504.10675</link>
      <description>arXiv:2504.10675v2 Announce Type: replace 
Abstract: This study develops a novel predictive framework for power grid vulnerability based on the statistical signatures of Self-Organized Criticality (SOC). By analyzing the evolution of the power law critical exponents in outage size distributions from the Texas grid during 2014-2022, we demonstrate the method's ability for forecasting system-wide vulnerability to catastrophic failures. Our results reveal a systematic decline in the critical exponent from 1.45 in 2018 to 0.95 in 2020, followed by a drop below the theoretical critical threshold ($\alpha$ = 1) to 0.62 in 2021, coinciding precisely with the catastrophic February 2021 power crisis. Such predictive signal emerged 6-12 months before the crisis. By monitoring critical exponent transitions through subcritical and supercritical regimes, we provide quantitative early warning capabilities for catastrophic infrastructure failures, with significant implications for grid resilience planning, risk assessment, and emergency preparedness in increasingly stressed power systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10675v2</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mary Lai O. Salva\~na, Gregory L. Tangonan</dc:creator>
    </item>
    <item>
      <title>Nonparametric Linear Discriminant Analysis for High Dimensional Matrix-Valued Data</title>
      <link>https://arxiv.org/abs/2507.19028</link>
      <description>arXiv:2507.19028v3 Announce Type: replace-cross 
Abstract: This paper addresses classification problems with matrix-valued data, which commonly arise in applications such as neuroimaging and signal processing. Building on the assumption that the data from each class follows a matrix normal distribution, we propose a novel extension of Fisher's Linear Discriminant Analysis (LDA) tailored for matrix-valued observations. To effectively capture structural information while maintaining estimation flexibility, we adopt a nonparametric empirical Bayes framework based on Nonparametric Maximum Likelihood Estimation (NPMLE), applied to vectorized and scaled matrices. The NPMLE method has been shown to provide robust, flexible, and accurate estimates for vector-valued data with various structures in the mean vector or covariance matrix. By leveraging its strengths, our method is effectively generalized to the matrix setting, thereby improving classification performance. Through extensive simulation studies and real data applications, including electroencephalography (EEG) and magnetic resonance imaging (MRI) analysis, we demonstrate that the proposed method tends to outperform existing approaches across a variety of data structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.19028v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seungyeon Oh, Seongoh Park, Hoyoung Park</dc:creator>
    </item>
    <item>
      <title>A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance</title>
      <link>https://arxiv.org/abs/2512.13446</link>
      <description>arXiv:2512.13446v2 Announce Type: replace-cross 
Abstract: Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13446v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Murat Yaslioglu</dc:creator>
    </item>
    <item>
      <title>A Fair, Flexible, Zero-Waste Digital Electricity Market: A First-Principles Approach Combining Automatic Market Making, Holarchic Architectures and Shapley Theory</title>
      <link>https://arxiv.org/abs/2512.13871</link>
      <description>arXiv:2512.13871v2 Announce Type: replace-cross 
Abstract: This thesis presents a fundamental rethink of electricity market design at the wholesale and balancing layers. Rather than treating markets as static spot clearing mechanisms, it reframes them as a continuously online, event driven dynamical control system: a two sided marketplace operating directly on grid physics.
  Existing energy only, capacity augmented, and zonal market designs are shown to admit no shock robust Nash equilibrium under realistic uncertainty, instead relying on price caps, uplift, and regulatory intervention to preserve solvency and security. In response, the thesis develops a holarchic Automatic Market Maker (AMM) in which prices are bounded, exogenous control signals derived from physical tightness rather than emergent equilibrium outcomes.
  The AMM generalises nodal and zonal pricing through nested scarcity layers, from node to cluster to zone to region to system, such that participant facing prices inherit from the tightest binding constraint. Nodal and zonal pricing therefore emerge as special cases of a unified scarcity propagation rule.
  Beyond pricing, the AMM functions as a scarcity aware control system and a digitally enforceable rulebook for fair access and proportional allocation under shortage. Fuel costs are recovered through pay as bid energy dispatch consistent with merit order, while non fuel operating and capital costs are allocated according to adequacy, flexibility, and locational contribution.
  Large scale simulations demonstrate bounded input bounded output stability, controllable procurement costs, zero structural waste, and improved distributional outcomes. The architecture is climate aligned and policy configurable, but requires a managed transition and new operational tools for system operators and market participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13871v2</guid>
      <category>eess.SY</category>
      <category>cs.HC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaun Sweeney, Robert Shorten, Mark O'Malley</dc:creator>
    </item>
  </channel>
</rss>
