<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 02:38:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Min-Max Correlation Clustering via Neighborhood Similarity</title>
      <link>https://arxiv.org/abs/2502.12519</link>
      <description>arXiv:2502.12519v1 Announce Type: new 
Abstract: We present an efficient algorithm for the min-max correlation clustering problem. The input is a complete graph where edges are labeled as either positive $(+)$ or negative $(-)$, and the objective is to find a clustering that minimizes the $\ell_{\infty}$-norm of the disagreement vector over all vertices.
  We resolve this problem with an efficient $(3 + \epsilon)$-approximation algorithm that runs in nearly linear time, $\tilde{O}(|E^+|)$, where $|E^+|$ denotes the number of positive edges. This improves upon the previous best-known approximation guarantee of 4 by Heidrich, Irmai, and Andres, whose algorithm runs in $O(|V|^2 + |V| D^2)$ time, where $|V|$ is the number of nodes and $D$ is the maximum degree in the graph.
  Furthermore, we extend our algorithm to the massively parallel computation (MPC) model and the semi-streaming model. In the MPC model, our algorithm runs on machines with memory sublinear in the number of nodes and takes $O(1)$ rounds. In the streaming model, our algorithm requires only $\tilde{O}(|V|)$ space, where $|V|$ is the number of vertices in the graph.
  Our algorithms are purely combinatorial. They are based on a novel structural observation about the optimal min-max instance, which enables the construction of a $(3 + \epsilon)$-approximation algorithm using $O(|E^+|)$ neighborhood similarity queries. By leveraging random projection, we further show these queries can be computed in nearly linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12519v1</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nairen Cao, Steven Roche, Hsin-Hao Su</dc:creator>
    </item>
    <item>
      <title>Maximizing Value in Challenge the Champ Tournaments</title>
      <link>https://arxiv.org/abs/2502.12569</link>
      <description>arXiv:2502.12569v1 Announce Type: new 
Abstract: A tournament is a method to decide the winner in a competition, and describes the overall sequence in which matches between the players are held. While deciding a worthy winner is the primary goal of a tournament, a close second is to maximize the value generated for the matches played, with value for a match measured either in terms of tickets sold, television viewership, advertising revenue, or other means. Tournament organizers often seed the players -- i.e., decide which matches are played -- to increase this value.
  We study the value maximization objective in a particular tournament format called Challenge the Champ. This is a simple tournament format where an ordering of the players is decided. The first player in this order is the initial champion. The remaining players in order challenge the current champion; if a challenger wins, she replaces the current champion. We model the outcome of a match between two players using a complete directed graph, called a strength graph, with each player represented as a vertex, and the direction of an edge indicating the winner in a match. The value-maximization objective has been recently explored for knockout tournaments when the strength graph is a directed acyclic graph (DAG).
  We extend the investigation to Challenge the Champ tournaments and general strength graphs. We study different representations of the value of each match, and completely characterize the computational complexity of the problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12569v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.MA</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Umang Bhaskar, Juhi Chaudhary, Palash Dey</dc:creator>
    </item>
    <item>
      <title>Revisiting Token Sliding on Chordal Graphs</title>
      <link>https://arxiv.org/abs/2502.12749</link>
      <description>arXiv:2502.12749v1 Announce Type: new 
Abstract: In this article, we revisit the complexity of the reconfiguration of independent sets under the token sliding rule on chordal graphs. In the \textsc{Token Sliding-Connectivity} problem, the input is a graph $G$ and an integer $k$, and the objective is to determine whether the reconfiguration graph $TS_k(G)$ of $G$ is connected. The vertices of $TS_k(G)$ are $k$-independent sets of $G$, and two vertices are adjacent if and only if one can transform one of the two corresponding independent sets into the other by sliding a vertex (also called a \emph{token}) along an edge. Bonamy and Bousquet [WG'17] proved that the \textsc{Token Sliding-Connectivity} problem is polynomial-time solvable on interval graphs but \NP-hard on split graphs. In light of these two results, the authors asked: can we decide the connectivity of $TS_k(G)$ in polynomial time for chordal graphs with \emph{maximum clique-tree degree} $d$? We answer this question in the negative and prove that the problem is \para-\NP-hard when parameterized by $d$. More precisely, the problem is \NP-hard even when $d = 4$. We then study the parameterized complexity of the problem for a larger parameter called \emph{leafage} and prove that the problem is \co-\W[1]-hard. We prove similar results for a closely related problem called \textsc{Token Sliding-Reachability}. In this problem, the input is a graph $G$ with two of its $k$-independent sets $I$ and $J$, and the objective is to determine whether there is a sequence of valid token sliding moves that transform $I$ into $J$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12749v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rajat Adak, Saraswati Girish Nanoti, Prafullkumar Tale</dc:creator>
    </item>
    <item>
      <title>Finding Maximum Weight 2-Packing Sets on Arbitrary Graphs</title>
      <link>https://arxiv.org/abs/2502.12856</link>
      <description>arXiv:2502.12856v1 Announce Type: new 
Abstract: A 2-packing set for an undirected, weighted graph G=(V,E,w) is a subset S of the vertices V such that any two vertices are not adjacent and have no common neighbors. The Maximum Weight 2-Packing Set problem that asks for a 2-packing set of maximum weight is
  NP-hard. Next to 13 novel data reduction rules for this problem, we develop two new approaches to solve this problem on arbitrary graphs. First, we introduce a preprocessing routine that exploits the close relation of 2-packing sets to independent sets. This makes well-studied independent set solvers usable for the Maximum Weight 2-Packing Set problem. Second, we propose an iterative reduce-and-peel approach that utilizes the new data reductions.
  Our experiments show that our preprocessing routine gives speedups of multiple orders of magnitude, while also improving solution quality, and memory consumption compared to a naive transformation to independent set instances. Furthermore, it solves 44 % of the instances tested to optimality.
  Our heuristic can keep up with the best-performing maximum weight independent set solvers combined with our preprocessing routine.
  Additionally, our heuristic can find the best solution quality on the biggest instances in our data set, outperforming all other approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12856v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jannick Borowitz, Ernestine Gro{\ss}mann, Christian Schulz</dc:creator>
    </item>
    <item>
      <title>On the Complexity of Minimising the Moving Distance for Dispersing Objects</title>
      <link>https://arxiv.org/abs/2502.12903</link>
      <description>arXiv:2502.12903v1 Announce Type: new 
Abstract: We study Geometric Graph Edit Distance (GGED), a graph-editing model to compute the minimum edit distance of intersection graphs that uses moving objects as an edit operation. We first show an $O(n\log n)$-time algorithm that minimises the total moving distance to disperse unit intervals. This algorithm is applied to render a given unit interval graph (i) edgeless, (ii) acyclic and (iii) $k$-clique-free. We next show that GGED becomes strongly NP-hard when rendering a weighted interval graph (i) edgeless, (ii) acyclic and (iii) $k$-clique-free. Lastly, we prove that minimising the maximum moving distance for rendering a unit disk graph edgeless is strongly NP-hard over the $L_1$ and $L_2$ distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12903v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicol\'as Honorato-Droguett, Kazuhiro Kurita, Tesshu Hanaka, Hirotaka Ono</dc:creator>
    </item>
    <item>
      <title>Approximate Tree Completion and Learning-Augmented Algorithms for Metric Minimum Spanning Trees</title>
      <link>https://arxiv.org/abs/2502.12993</link>
      <description>arXiv:2502.12993v1 Announce Type: new 
Abstract: Finding a minimum spanning tree (MST) for $n$ points in an arbitrary metric space is a fundamental primitive for hierarchical clustering and many other ML tasks, but this takes $\Omega(n^2)$ time to even approximate. We introduce a framework for metric MSTs that first (1) finds a forest of disconnected components using practical heuristics, and then (2) finds a small weight set of edges to connect disjoint components of the forest into a spanning tree. We prove that optimally solving the second step still takes $\Omega(n^2)$ time, but we provide a subquadratic 2.62-approximation algorithm. In the spirit of learning-augmented algorithms, we then show that if the forest found in step (1) overlaps with an optimal MST, we can approximate the original MST problem in subquadratic time, where the approximation factor depends on a measure of overlap. In practice, we find nearly optimal spanning trees for a wide range of metrics, while being orders of magnitude faster than exact algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12993v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nate Veldt, Thomas Stanley, Benjamin W. Priest, Trevor Steil, Keita Iwabuchi, T. S. Jayram, Geoffrey Sanders</dc:creator>
    </item>
    <item>
      <title>Edge-Colored Clustering in Hypergraphs: Beyond Minimizing Unsatisfied Edges</title>
      <link>https://arxiv.org/abs/2502.13000</link>
      <description>arXiv:2502.13000v1 Announce Type: new 
Abstract: We consider a framework for clustering edge-colored hypergraphs, where the goal is to cluster (equivalently, to color) objects based on the primary type of multiway interactions they participate in. One well-studied objective is to color nodes to minimize the number of unsatisfied hyperedges -- those containing one or more nodes whose color does not match the hyperedge color. We motivate and present advances for several directions that extend beyond this minimization problem. We first provide new algorithms for maximizing satisfied edges, which is the same at optimality but is much more challenging to approximate, with all prior work restricted to graphs. We develop the first approximation algorithm for hypergraphs, and then refine it to improve the best-known approximation factor for graphs. We then introduce new objective functions that incorporate notions of balance and fairness, and provide new hardness results, approximations, and fixed-parameter tractability results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13000v1</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alex Crane, Thomas Stanley, Blair D. Sullivan, Nate Veldt</dc:creator>
    </item>
    <item>
      <title>Smoothed Analysis of Dynamic Graph Algorithms</title>
      <link>https://arxiv.org/abs/2502.13007</link>
      <description>arXiv:2502.13007v1 Announce Type: new 
Abstract: Recent years have seen significant progress in the study of dynamic graph algorithms, and most notably, the introduction of strong lower bound techniques for them (e.g., Henzinger, Krinninger, Nanongkai and Saranurak, STOC 2015; Larsen and Yu, FOCS 2023). As worst-case analysis (adversarial inputs) may lead to the necessity of high running times, a natural question arises: in which cases are high running times really necessary, and in which cases these inputs merely manifest unique pathological cases?
  Early attempts to tackle this question were made by Nikoletseas, Reif, Spirakis and Yung (ICALP 1995) and by Alberts and Henzinger (Algorithmica 1998), who considered models with very little adversarial control over the inputs, and showed fast algorithms exist for them. The question was then overlooked for decades, until Henzinger, Lincoln and Saha (SODA 2022) recently addressed uniformly random inputs, and presented algorithms and impossibility results for several subgraph counting problems.
  To tackle the above question more thoroughly, we employ smoothed analysis, a celebrated framework introduced by Spielman and Teng (J. ACM, 2004). An input is proposed by an adversary but then a noisy version of it is processed by the algorithm instead. Parameterized by the amount of adversarial control, this input model fully interpolates between worst-case inputs and a uniformly random input. Doing so, we extend impossibility results for some problems to the smoothed model with only a minor quantitative loss. That is, we show that partially-adversarial inputs suffice to impose high running times for certain problems. In contrast, we show that other problems become easy even with the slightest amount of noise. In addition, we study the interplay between the adversary and the noise, leading to three natural models of smoothed inputs, for which we show a hierarchy of increasing complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13007v1</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Uri Meir, Ami Paz</dc:creator>
    </item>
    <item>
      <title>Computational-Statistical Tradeoffs at the Next-Token Prediction Barrier: Autoregressive and Imitation Learning under Misspecification</title>
      <link>https://arxiv.org/abs/2502.12465</link>
      <description>arXiv:2502.12465v1 Announce Type: cross 
Abstract: Next-token prediction with the logarithmic loss is a cornerstone of autoregressive sequence modeling, but, in practice, suffers from error amplification, where errors in the model compound and generation quality degrades as sequence length $H$ increases. From a theoretical perspective, this phenomenon should not appear in well-specified settings, and, indeed, a growing body of empirical work hypothesizes that misspecification, where the learner is not sufficiently expressive to represent the target distribution, may be the root cause. Under misspecification -- where the goal is to learn as well as the best-in-class model up to a multiplicative approximation factor $C\geq 1$ -- we confirm that $C$ indeed grows with $H$ for next-token prediction, lending theoretical support to this empirical hypothesis. We then ask whether this mode of error amplification is avoidable algorithmically, computationally, or information-theoretically, and uncover inherent computational-statistical tradeoffs. We show:
  (1) Information-theoretically, one can avoid error amplification and achieve $C=O(1)$.
  (2) Next-token prediction can be made robust so as to achieve $C=\tilde O(H)$, representing moderate error amplification, but this is an inherent barrier: any next-token prediction-style objective must suffer $C=\Omega(H)$.
  (3) For the natural testbed of autoregressive linear models, no computationally efficient algorithm can achieve sub-polynomial approximation factor $C=e^{(\log H)^{1-\Omega(1)}}$; however, at least for binary token spaces, one can smoothly trade compute for statistical power and improve on $C=\Omega(H)$ in sub-exponential time.
  Our results have consequences in the more general setting of imitation learning, where the widely-used behavior cloning algorithm generalizes next-token prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12465v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dhruv Rohatgi, Adam Block, Audrey Huang, Akshay Krishnamurthy, Dylan J. Foster</dc:creator>
    </item>
    <item>
      <title>GPU Memory Usage Optimization for Backward Propagation in Deep Network Training</title>
      <link>https://arxiv.org/abs/2502.12499</link>
      <description>arXiv:2502.12499v1 Announce Type: cross 
Abstract: In modern Deep Learning, it has been a trend to design larger Deep Neural Networks (DNNs) for the execution of more complex tasks and better accuracy. On the other hand, Convolutional Neural Networks (CNNs) have become the standard method for most of computer vision tasks. However, the memory allocation for the intermediate data in convolution layers can cause severe memory pressure during model training. Many solutions have been proposed to resolve the problem. Besides hardware-dependent solutions, a general methodology rematerialization can reduce GPU memory usage by trading computation for memory efficiently. The idea is to select a set of intermediate results during the forward phase as checkpoints, and only save them in memory to reduce memory usage. The backward phase recomputes the intermediate data from the closest checkpoints in memory as needed. This recomputation increases execution time but saves memory by not storing all intermediate results in memory during the forward phase. In this paper, we will focus on efficiently finding the optimal checkpoint subset to achieve the least peak memory usage during the model training. We first describe the theoretical background of the training of a neural network using mathematical equations. We use these equations to identify all essential data required during both forward and backward phases to compute the gradient of weights of the model. We first identify the checkpoint selection problem and propose a dynamic programming algorithm with time complexity O(n3) to solve the problem of finding the optimal checkpoint subset. With extensive experiments, we formulate a more accurate description of the problem using our theoretical analysis and revise the objective function based on the tracing, and propose an O(n)-time algorithm for finding the optimal checkpoint subset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12499v1</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.jpdc.2025.105053</arxiv:DOI>
      <dc:creator>Ding-Yong Hong, Tzu-Hsien Tsai, Ning Wang, Pangfeng Liu, Jan-Jan Wu</dc:creator>
    </item>
    <item>
      <title>Generalized De Bruijn Words, Invertible Necklaces, and the Burrows-Wheeler Transform</title>
      <link>https://arxiv.org/abs/2502.12844</link>
      <description>arXiv:2502.12844v1 Announce Type: cross 
Abstract: We define generalized de Bruijn words, as those words having a Burrows--Wheeler transform that is a concatenation of permutations of the alphabet. We show how to interpret generalized de Bruijn words in terms of Hamiltonian cycles in the generalized de Bruijn graphs introduced in the early '80s in the context of network design. When the size of the alphabet is a prime, we give relations between generalized de Bruijn words, normal bases of finite fields, invertible circulant matrices, and Reutenauer groups. In particular, we highlight a correspondence between binary de Bruijn words of order $d+1$, binary necklaces of length $2^{d}$ having an odd number of $1$s, invertible BWT matrices of size $2^{d}\times 2^{d}$, and normal bases of the finite field $\mathbb{F}_{2^{2^{d}}}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12844v1</guid>
      <category>math.CO</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <category>cs.FL</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriele Fici, Est\'eban Gabory</dc:creator>
    </item>
    <item>
      <title>Improving Algorithmic Efficiency using Cryptography</title>
      <link>https://arxiv.org/abs/2502.13065</link>
      <description>arXiv:2502.13065v1 Announce Type: cross 
Abstract: Cryptographic primitives have been used for various non-cryptographic objectives, such as eliminating or reducing randomness and interaction. We show how to use cryptography to improve the time complexity of solving computational problems. Specifically, we show that under standard cryptographic assumptions, we can design algorithms that are asymptotically faster than existing ones while maintaining correctness.
  As a concrete demonstration, we construct a distribution of trapdoored matrices with the following properties: (a) computationally bounded adversaries cannot distinguish a random matrix from one drawn from this distribution, and (b) given a secret key, we can multiply such a n-by-n matrix with any vector in near-linear (in n) time. We provide constructions both over finite fields and the reals. This enables a broad speedup technique: any algorithm relying on a random matrix - such as those using various notions of dimensionality reduction - can replace it with a matrix from our distribution, achieving computational speedups while preserving correctness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13065v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vinod Vaikuntanathan, Or Zamir</dc:creator>
    </item>
    <item>
      <title>Polynomial kernels for edge modification problems towards block and strictly chordal graphs</title>
      <link>https://arxiv.org/abs/2201.13140</link>
      <description>arXiv:2201.13140v4 Announce Type: replace 
Abstract: We consider edge modification problems towards block and strictly chordal graphs, where one is given an undirected graph $G = (V,E)$ and an integer $k \in \mathbb{N}$ and seeks to edit (add or delete) at most $k$ edges from $G$ to obtain a block graph or a strictly chordal graph. The completion and deletion variants of these problems are defined similarly by only allowing edge additions for the former and only edge deletions for the latter. Block graphs are a well-studied class of graphs and admit several characterizations, e.g. they are diamond-free chordal graphs. Strictly chordal graphs, also referred to as block duplicate graphs, are a natural generalization of block graphs where one can add true twins of cut-vertices. Strictly chordal graphs are exactly dart and gem-free chordal graphs. We prove the NP-completeness for most variants of these problems and provide $O(k^2)$ vertex-kernels for Block Graph Editing and Block Graph Deletion, $O(k^3)$ vertex-kernels for Strictly Chordal Completion and Strictly Chordal Deletion and a $O(k^4)$ vertex-kernel for Strictly Chordal Editing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.13140v4</guid>
      <category>cs.DS</category>
      <category>cs.CC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ma\"el Dumas, Anthony Perez, Mathis Rocton, Ioan Todinca</dc:creator>
    </item>
    <item>
      <title>Fast and Efficient Matching Algorithm with Deadline Instances</title>
      <link>https://arxiv.org/abs/2305.08353</link>
      <description>arXiv:2305.08353v3 Announce Type: replace 
Abstract: The online weighted matching problem is a fundamental problem in machine learning due to its numerous applications. Despite many efforts in this area, existing algorithms are either too slow or don't take $\mathrm{deadline}$ (the longest time a node can be matched) into account. In this paper, we introduce a market model with $\mathrm{deadline}$ first. Next, we present our two optimized algorithms (\textsc{FastGreedy} and \textsc{FastPostponedGreedy}) and offer theoretical proof of the time complexity and correctness of our algorithms. In \textsc{FastGreedy} algorithm, we have already known if a node is a buyer or a seller. But in \textsc{FastPostponedGreedy} algorithm, the status of each node is unknown at first. Then, we generalize a sketching matrix to run the original and our algorithms on both real data sets and synthetic data sets. Let $\epsilon \in (0,0.1)$ denote the relative error of the real weight of each edge. The competitive ratio of original \textsc{Greedy} and \textsc{PostponedGreedy} is $\frac{1}{2}$ and $\frac{1}{4}$ respectively. Based on these two original algorithms, we proposed \textsc{FastGreedy} and \textsc{FastPostponedGreedy} algorithms and the competitive ratio of them is $\frac{1 - \epsilon}{2}$ and $\frac{1 - \epsilon}{4}$ respectively. At the same time, our algorithms run faster than the original two algorithms. Given $n$ nodes in $\mathbb{R} ^ d$, we decrease the time complexity from $O(nd)$ to $\widetilde{O}(\epsilon^{-2} \cdot (n + d))$, where for any function $f$, we use $\widetilde{O}(f)$ to denote $f \cdot \mathrm{poly}(\log f)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.08353v3</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhao Song, Weixin Wang, Chenbo Yin, Junze Yin</dc:creator>
    </item>
    <item>
      <title>Holey graphs: very large Betti numbers are testable</title>
      <link>https://arxiv.org/abs/2401.06109</link>
      <description>arXiv:2401.06109v2 Announce Type: replace 
Abstract: We show that the graph property of having a (very) large $k$-th Betti number $\beta_k$ for constant $k$ is testable with a constant number of queries in the dense graph model. More specifically, we consider a clique complex defined by an underlying graph and prove that for any $\varepsilon&gt;0$, there exists $\delta(\varepsilon,k)&gt;0$ such that testing whether $\beta_k \geq (1-\delta) d_k$ for $\delta \leq \delta(\varepsilon,k)$ reduces to tolerantly testing $(k+2)$-clique-freeness, which is known to be testable. This complements a result by Elek (2010) showing that Betti numbers are testable in the bounded-degree model. Our result combines the Euler characteristic, matroid theory and the graph removal lemma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06109v2</guid>
      <category>cs.DS</category>
      <category>cs.DM</category>
      <category>math.CO</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-82697-9_22</arxiv:DOI>
      <dc:creator>D\'aniel Szab\'o, Simon Apers</dc:creator>
    </item>
    <item>
      <title>Reducing Matroid Optimization to Basis Search</title>
      <link>https://arxiv.org/abs/2408.04118</link>
      <description>arXiv:2408.04118v4 Announce Type: replace 
Abstract: Much energy has been devoted to developing a matroid's computational properties, yet parallel algorithm design for matroid optimization seems less understood. Specifically, the current state of the art is a folklore reduction from optimization to the search based on methods originating in [KUW88]. However, while this reduction adds only constant overhead in terms of \emph{adaptive complexity}, it imposes a high cost in \emph{query complexity}. In response, we present a new reduction from optimization to search within the class of \emph{binary matroids} which, when $n$ and $r$ take the size of the ground set and matroid rank respectively, implies a novel optimization algorithm terminating in $\mathcal{O}(\sqrt{n}\cdot\log r)$ parallel rounds using only $\mathcal{O}(rn\cdot\log r)$ independence queries. This is a significant improvement in query complexity when the matroid is sparse, meaning $r \ll n$, while trading off only a logarithmic factor of the rank in the adaptive complexity. At a technical level, our method begins by observing that a basis is optimal if and only if it is the set of points of minimum weight in any cocircuit. Importantly, this certificate reveals that simultaneous tests for \emph{local optimality} in cocircuits is a general paradigm for parallel matroid optimization. By combining this idea with connections between bases and cocircuits we obtain our reduction, whose efficiency follows by analyzing the lattice of flats. A primary goal of our study is initiating a finer understanding of parallel matroid optimization. And so, since many of our techniques begin with observations about general matroids and their flats, we hope that our efforts aid the future design of parallel matroid algorithms and applications of lattice theory thereof.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.04118v4</guid>
      <category>cs.DS</category>
      <category>cs.DC</category>
      <category>cs.DM</category>
      <category>math.OC</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Streit, Vijay K. Garg</dc:creator>
    </item>
    <item>
      <title>Subexponential Parameterized Algorithms for Hitting Subgraphs</title>
      <link>https://arxiv.org/abs/2409.04786</link>
      <description>arXiv:2409.04786v2 Announce Type: replace 
Abstract: For a finite set $\mathcal{F}$ of graphs, the $\mathcal{F}$-Hitting problem aims to compute, for a given graph $G$ (taken from some graph class $\mathcal{G}$) of $n$ vertices (and $m$ edges) and a parameter $k\in\mathbb{N}$, a set $S$ of vertices in $G$ such that $|S|\leq k$ and $G-S$ does not contain any subgraph isomorphic to a graph in $\mathcal{F}$. As a generic problem, $\mathcal{F}$-Hitting subsumes many fundamental vertex-deletion problems that are well-studied in the literature. The $\mathcal{F}$-Hitting problem admits a simple branching algorithm with running time $2^{O(k)}\cdot n^{O(1)}$, while it cannot be solved in $2^{o(k)}\cdot n^{O(1)}$ time on general graphs assuming the ETH.
  In this paper, we establish a general framework to design subexponential parameterized algorithms for the $\mathcal{F}$-Hitting problem on a broad family of graph classes. Specifically, our framework yields algorithms that solve $\mathcal{F}$-Hitting with running time $2^{O(k^c)}\cdot n+O(m)$ for a constant $c&lt;1$ on any graph class $\mathcal{G}$ that admits balanced separators whose size is (strongly) sublinear in the number of vertices and polynomial in the size of a maximum clique. Examples include all graph classes of polynomial expansion and many important classes of geometric intersection graphs. Our algorithms also apply to the \textit{weighted} version of $\mathcal{F}$-Hitting, where each vertex of $G$ has a weight and the goal is to compute the set $S$ with a minimum weight that satisfies the desired conditions.
  The core of our framework is an intricate subexponential branching algorithm that reduces an instance of $\mathcal{F}$-Hitting (on the aforementioned graph classes) to $2^{O(k^c)}$ general hitting-set instances, where the Gaifman graph of each instance has treewidth $O(k^c)$, for some constant $c&lt;1$ depending on $\mathcal{F}$ and the graph class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04786v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Lokshtanov, Fahad Panolan, Saket Saurabh, Jie Xue, Meirav Zehavi</dc:creator>
    </item>
    <item>
      <title>Cheesemap: A High-Performance Point-Indexing Data Structure for Neighbor Search in LiDAR Data</title>
      <link>https://arxiv.org/abs/2502.11602</link>
      <description>arXiv:2502.11602v2 Announce Type: replace 
Abstract: Point cloud data, as the representation of three-dimensional spatial information, is a fundamental piece of information in various domains where indexing and querying these point clouds efficiently is crucial for tasks such as object recognition, autonomous navigation, and environmental modeling. In this paper, we present a comprehensive comparative analysis of various data structures combined with neighboring search methods across different types of point clouds. Additionally, we introduce a novel data structure, cheesemap, to handle 3D LiDAR point clouds. Exploring the sparsity and irregularity in the distribution of points, there are three flavors of the cheesemap: dense, sparse, and mixed. Results show that the cheesemap can outperform state-of-the-art data structures in terms of execution time per query, particularly for ALS (Aerial Laser Scanning) point clouds. Memory consumption is also minimal, especially in the sparse and mixed representations, making the cheesemap a suitable choice for applications involving three-dimensional point clouds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.11602v2</guid>
      <category>cs.DS</category>
      <category>cs.PF</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ruben Laso, Miguel Yermo</dc:creator>
    </item>
    <item>
      <title>Elfs, trees and quantum walks</title>
      <link>https://arxiv.org/abs/2211.16379</link>
      <description>arXiv:2211.16379v2 Announce Type: replace-cross 
Abstract: We study an elementary Markov process on graphs based on electric flow sampling (elfs). The elfs process repeatedly samples from an electric flow on a graph. While the sinks of the flow are fixed, the source is updated using the electric flow sample, and the process ends when it hits a sink vertex.
  We argue that this process naturally connects to many key quantities of interest. E.g., we describe a random walk coupling which implies that the elfs process has the same arrival distribution as a random walk. We also analyze the electric hitting time, which is the expected time before the process hits a sink vertex. As our main technical contribution, we show that the electric hitting time on trees is logarithmic in the graph size and weights.
  The initial motivation behind the elfs process is that quantum walks can sample from electric flows, and they can hence implement this process very naturally. This yields a quantum walk algorithm for sampling from the random walk arrival distribution, which has widespread applications. It complements the existing line of quantum walk search algorithms which only return an element from the sink, but yield no insight in the distribution of the returned element. By our bound on the electric hitting time on trees, the quantum walk algorithm on trees requires quadratically fewer steps than the random walk hitting time, up to polylog factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.16379v2</guid>
      <category>quant-ph</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simon Apers, Stephen Piddock</dc:creator>
    </item>
    <item>
      <title>1-in-3 vs. Not-All-Equal: Dichotomy of a broken promise</title>
      <link>https://arxiv.org/abs/2302.03456</link>
      <description>arXiv:2302.03456v3 Announce Type: replace-cross 
Abstract: The 1-in-3 and Not-All-Equal satisfiability problems for Boolean CNF formulas are two well-known NP-hard problems. In contrast, the promise 1-in-3 vs. Not-All-Equal problem can be solved in polynomial time. In the present work, we investigate this constraint satisfaction problem in a regime where the promise is weakened from either side by a rainbow-free structure, and establish a complexity dichotomy for the resulting class of computational problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03456v3</guid>
      <category>cs.CC</category>
      <category>cs.DM</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Ciardo, Marcin Kozik, Andrei Krokhin, Tamio-Vesa Nakajima, Stanislav \v{Z}ivn\'y</dc:creator>
    </item>
    <item>
      <title>On estimating the trace of quantum state powers</title>
      <link>https://arxiv.org/abs/2410.13559</link>
      <description>arXiv:2410.13559v2 Announce Type: replace-cross 
Abstract: We investigate the computational complexity of estimating the trace of quantum state powers $\text{tr}(\rho^q)$ for an $n$-qubit mixed quantum state $\rho$, given its state-preparation circuit of size $\text{poly}(n)$. This quantity is closely related to and often interchangeable with the Tsallis entropy $\text{S}_q(\rho) = \frac{1-\text{tr}(\rho^q)}{q-1}$, where $q = 1$ corresponds to the von Neumann entropy. For any non-integer $q \geq 1 + \Omega(1)$, we provide a quantum estimator for $\text{S}_q(\rho)$ with time complexity $\text{poly}(n)$, exponentially improving the prior best results of $\exp(n)$ due to Acharya, Issa, Shende, and Wagner (ISIT 2019), Wang, Guan, Liu, Zhang, and Ying (TIT 2024), and Wang, Zhang, and Li (TIT 2024), and Wang and Zhang (ESA 2024). Our speedup is achieved by introducing efficiently computable uniform approximations of positive power functions into quantum singular value transformation.
  Our quantum algorithm reveals a sharp phase transition between the case of $q=1$ and constant $q&gt;1$ in the computational complexity of the Quantum $q$-Tsallis Entropy Difference Problem (TsallisQED$_q$), particularly deciding whether the difference $\text{S}_q(\rho_0) - \text{S}_q(\rho_1)$ is at least $0.001$ or at most $-0.001$:
  - For any $1+\Omega(1) \leq q \leq 2$, TsallisQED$_q$ is $\mathsf{BQP}$-complete, which implies that Purity Estimation is also $\mathsf{BQP}$-complete.
  - For any $1 \leq q \leq 1 + \frac{1}{n-1}$, TsallisQED$_q$ is $\mathsf{QSZK}$-hard, leading to hardness of approximating the von Neumann entropy because $\text{S}_q(\rho) \leq \text{S}(\rho)$, as long as $\mathsf{BQP} \subsetneq \mathsf{QSZK}$.
  The hardness results are derived from reductions based on new inequalities for the quantum $q$-Jensen-(Shannon-)Tsallis divergence with $1\leq q \leq 2$, which are of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13559v2</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1137/1.9781611978322.28</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 2025 Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), pp. 947-993, 2025</arxiv:journal_reference>
      <dc:creator>Yupan Liu, Qisheng Wang</dc:creator>
    </item>
    <item>
      <title>Weighted Envy Freeness With Bounded Subsidies</title>
      <link>https://arxiv.org/abs/2411.12696</link>
      <description>arXiv:2411.12696v3 Announce Type: replace-cross 
Abstract: We explore solutions for fairly allocating indivisible items among agents assigned weights representing their entitlements. Our fairness goal is weighted-envy-freeness (WEF), where each agent deems their allocated portion relative to their entitlement at least as favorable as any other's relative to their own. In many cases, achieving WEF necessitates monetary transfers, which can be modeled as third-party subsidies. The goal is to attain WEF with bounded subsidies.
  Previous work in the unweighted setting of subsidies relied on basic characterizations of EF that fail in the weighted settings. This makes our new setting challenging and theoretically intriguing. We present polynomial-time algorithms that compute WEF-able allocations with an upper bound on the subsidy per agent in three distinct additive valuation scenarios: (1) general, (2) identical, and (3) binary. When all weights are equal, our bounds reduce to the bounds derived in the literature for the unweighted setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12696v3</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <pubDate>Wed, 19 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noga Klein Elmalem, Rica Gonen, Erel Segal-Halevi</dc:creator>
    </item>
  </channel>
</rss>
