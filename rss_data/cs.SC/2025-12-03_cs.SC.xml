<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SC</link>
    <description>cs.SC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 03 Dec 2025 05:01:55 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Model-Based Diagnosis with Multiple Observations: A Unified Approach for C Software and Boolean Circuits</title>
      <link>https://arxiv.org/abs/2512.02898</link>
      <description>arXiv:2512.02898v1 Announce Type: cross 
Abstract: Debugging is one of the most time-consuming and expensive tasks in software development and circuit design. Several formula-based fault localisation (FBFL) methods have been proposed, but they fail to guarantee a set of diagnoses across all failing tests or may produce redundant diagnoses that are not subset-minimal, particularly for programs/circuits with multiple faults.
  This paper introduces CFaults, a novel fault localisation tool for C software and Boolean circuits with multiple faults. CFaults leverages Model-Based Diagnosis (MBD) with multiple observations and aggregates all failing test cases into a unified Maximum Satisfiability (MaxSAT) formula. Consequently, our method guarantees consistency across observations and simplifies the fault localisation procedure. Experimental results on three benchmark sets, two of C programs, TCAS and C-Pack-IPAs, and one of Boolean circuits, ISCAS85, show that CFaults is faster at localising faults in C software than other FBFL approaches such as BugAssist, SNIPER, and HSD. On the ISCAS85 benchmark, CFaults is generally slower than HSD; however, it localises faults in only 6% fewer circuits, demonstrating that it remains competitive in this domain. Furthermore, CFaults produces only subset-minimal diagnoses of faulty statements, whereas the other approaches tend to enumerate redundant diagnoses (e.g., BugAssist and SNIPER).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.02898v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LO</category>
      <category>cs.SC</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pedro Orvalho, Marta Kwiatkowska, Mikol\'a\v{s} Janota, Vasco Manquinho</dc:creator>
    </item>
    <item>
      <title>Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks</title>
      <link>https://arxiv.org/abs/2410.17498</link>
      <description>arXiv:2410.17498v2 Announce Type: replace-cross 
Abstract: Large Language Models (LLMs) have demonstrated impressive abilities in symbol processing through in-context learning (ICL). This success flies in the face of decades of critiques asserting that artificial neural networks cannot master abstract symbol manipulation. We seek to understand the mechanisms that can enable robust symbol processing in transformer networks, illuminating both the unanticipated success, and the significant limitations, of transformers in symbol processing. Borrowing insights from symbolic AI and cognitive science on the power of Production System architectures, we develop a high-level Production System Language, PSL, that allows us to write symbolic programs to do complex, abstract symbol processing, and create compilers that precisely implement PSL programs in transformer networks which are, by construction, 100% mechanistically interpretable. The work is driven by study of a purely abstract (semantics-free) symbolic task that we develop, Templatic Generation (TGT). Although developed through study of TGT, PSL is, we demonstrate, highly general: it is Turing Universal. The new type of transformer architecture that we compile from PSL programs suggests a number of paths for enhancing transformers' capabilities at symbol processing. We note, however, that the work we report addresses computability, and not learnability, by transformer networks.
  Note: The first section provides an extended synopsis of the entire paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17498v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.NE</category>
      <category>cs.SC</category>
      <pubDate>Wed, 03 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1613/jair.1.17469</arxiv:DOI>
      <arxiv:journal_reference>Paul Smolensky, Roland Fernandez, Zhenghao Herbert Zhou, Mattia Opper, Adam Davies, and Jianfeng Gao. 2025. Mechanisms of Symbol Processing for In-Context Learning in Transformer Networks. Journal of Artificial Intelligence Research, 84(23)</arxiv:journal_reference>
      <dc:creator>Paul Smolensky, Roland Fernandez, Zhenghao Herbert Zhou, Mattia Opper, Adam Davies, Jianfeng Gao</dc:creator>
    </item>
  </channel>
</rss>
