<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 28 Oct 2024 04:01:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimal Doubling Thresholds in Backgammon-like Stochastic Games</title>
      <link>https://arxiv.org/abs/2410.19178</link>
      <description>arXiv:2410.19178v1 Announce Type: new 
Abstract: We study variants of a stochastic game inspired by backgammon where players may propose to double the stake, with the game state dictated by a one-dimensional random walk. Our variants allow for different numbers of proposals and different multipliers to the stake. We determine the optimal game state for proposing and accepting, giving analytic solutions in many variants. We also introduce a 3-player generalization of the game and prove basic results about its behavior, in addition to providing a simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19178v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoru Ju, Daniel Leifer, Steven J. Miller, Sooraj A. Padmanabhan, Chenyang Sun, Luke Tichi, Benjamin Tocher, Kiley Wallace</dc:creator>
    </item>
    <item>
      <title>Stochastic dynamic programming under recursive Epstein-Zin preferences</title>
      <link>https://arxiv.org/abs/2410.19181</link>
      <description>arXiv:2410.19181v1 Announce Type: new 
Abstract: This paper treats of discrete-time Markov decision processes with recursive utility defined using a non-linear aggregator of Epstein-Zin type with the Kreps-Porteus certainty equivalent operator. According to the classification introduced by Marinacci and Montrucchio, the aggregators that we consider are of Thompson type. We focus on the existence and uniqueness of a solution to the Bellman equation. If the action spaces of an agent are singletons, the problem concerns the existence and uniqueness of a recursive utility in the sense of Koopmans. The models of this type were recently studied by Ren and Stachurski who applied Du's fixed point theorem for increasing and convex or concave operators acting on an ordered Banach space. We show that, in two cases considered by Ren and Stachurski, the existence and uniqueness of solution to the Bellman equation follows from the Banach contraction mapping principle. We allow the per-period utilities in the models to be unbounded. Therefore, we work with the weighted supremum norm. Since we apply the Banach fixed point theorem for contraction mappings acting on a standard complete metric space, we need not assume any boundary conditions, which are present when the Thompson metric or Du's theorem are used. Moreover, our results give better bounds for the geometric convergence of the value iteration algorithm leading to the solution of the Bellman equation than those obtained by Du's fixed point theorem. Some illustrative examples are given. This paper also contains also new results on the existence and uniqueness of solutions to the Bellman equation in two cases of parameters in the Epsten-Zin aggregator, where the application of Du's theorem is not possible. Our results are based on a relatively simple reformulation of the problem exploiting the structure of Epstein-Zin preferences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19181v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anna Ja\'skiewicz, Andrzej S. Nowak</dc:creator>
    </item>
    <item>
      <title>Approximate Projections onto the Positive Semidefinite Cone Using Randomization</title>
      <link>https://arxiv.org/abs/2410.19208</link>
      <description>arXiv:2410.19208v1 Announce Type: new 
Abstract: This paper presents two novel algorithms for approximately projecting symmetric matrices onto the Positive Semidefinite (PSD) cone using Randomized Numerical Linear Algebra (RNLA). Classical PSD projection methods rely on full-rank deterministic eigen-decomposition, which can be computationally prohibitive for large-scale problems. Our approach leverages RNLA to construct low-rank matrix approximations before projection, significantly reducing the required numerical resources. The first algorithm utilizes random sampling to generate a low-rank approximation, followed by a standard eigen-decomposition on this smaller matrix. The second algorithm enhances this process by introducing a scaling approach that aligns the leading-order singular values with the positive eigenvalues, ensuring that the low-rank approximation captures the essential information about the positive eigenvalues for PSD projection. Both methods offer a trade-off between accuracy and computational speed, supported by probabilistic error bounds. To further demonstrate the practical benefits of our approach, we integrate the randomized projection methods into a first-order Semi-Definite Programming (SDP) solver. Numerical experiments, including those on SDPs derived from Sum-of-Squares (SOS) programming problems, validate the effectiveness of our method, especially for problems that are infeasible with traditional deterministic methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19208v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Morgan Jones, James Anderson</dc:creator>
    </item>
    <item>
      <title>On the Trade-Off Between Distributional Belief and Ambiguity: Conservatism, Finite-Sample Guarantees, and Asymptotic Properties</title>
      <link>https://arxiv.org/abs/2410.19234</link>
      <description>arXiv:2410.19234v1 Announce Type: new 
Abstract: We propose and analyze a new data-driven trade-off (TRO) approach for modeling uncertainty that serves as a middle ground between the optimistic approach, which adopts a distributional belief, and the pessimistic distributionally robust optimization approach, which hedges against distributional ambiguity. We equip the TRO model with a TRO ambiguity set characterized by a size parameter controlling the level of optimism and a shape parameter representing distributional ambiguity. We first show that constructing the TRO ambiguity set using a general star-shaped shape parameter with the empirical distribution as its star center is necessary and sufficient to guarantee the hierarchical structure of the sequence of TRO ambiguity sets. Then, we analyze the properties of the TRO model, including quantifying conservatism, quantifying bias and generalization error, and establishing asymptotic properties. Specifically, we show that the TRO model could generate a spectrum of decisions, ranging from optimistic to conservative decisions. Additionally, we show that it could produce an unbiased estimator of the true optimal value. Furthermore, we establish the almost-sure convergence of the optimal value and the set of optimal solutions of the TRO model to their true counterparts. We exemplify our theoretical results using an inventory control problem and a portfolio optimization problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19234v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Man Yiu Tsang, Karmel S. Shehadeh</dc:creator>
    </item>
    <item>
      <title>Multiple Regression for Matrix and Vector Predictors: Models, Theory, Algorithms, and Beyond</title>
      <link>https://arxiv.org/abs/2410.19264</link>
      <description>arXiv:2410.19264v1 Announce Type: new 
Abstract: Matrix regression plays an important role in modern data analysis due to its ability to handle complex relationships involving both matrix and vector variables. We propose a class of regularized regression models capable of predicting both matrix and vector variables, accommodating various regularization techniques tailored to the inherent structures of the data. We establish the consistency of our estimator when penalizing the nuclear norm of the matrix variable and the $\ell_1$ norm of the vector variable. To tackle the general regularized regression model, we propose a unified framework based on an efficient preconditioned proximal point algorithm. Numerical experiments demonstrate the superior estimation and prediction accuracy of our proposed estimator, as well as the efficiency of our algorithm compared to the state-of-the-art solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19264v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meixia Lin, Ziyang Zeng, Yangjing Zhang</dc:creator>
    </item>
    <item>
      <title>Fully First-Order Methods for Decentralized Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2410.19319</link>
      <description>arXiv:2410.19319v1 Announce Type: new 
Abstract: This paper focuses on decentralized stochastic bilevel optimization (DSBO) where agents only communicate with their neighbors. We propose Decentralized Stochastic Gradient Descent and Ascent with Gradient Tracking (DSGDA-GT), a novel algorithm that only requires first-order oracles that are much cheaper than second-order oracles widely adopted in existing works. We further provide a finite-time convergence analysis showing that for $n$ agents collaboratively solving the DSBO problem, the sample complexity of finding an $\epsilon$-stationary point in our algorithm is $\mathcal{O}(n^{-1}\epsilon^{-7})$, which matches the currently best-known results of the single-agent counterpart with linear speedup. The numerical experiments demonstrate both the communication and training efficiency of our algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19319v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Xiaoyu Wang, Xuxing Chen, Shiqian Ma, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>On pairs of complementary GJ pivoting transforming skew-symmetric matrices</title>
      <link>https://arxiv.org/abs/2410.19350</link>
      <description>arXiv:2410.19350v1 Announce Type: new 
Abstract: This article describes certain ratios that attend pairs of complementary Gauss-Jordan pivotings transforming skew-symmetric matrices. Our interest in those ratios was motivated by a need to prove a crucial Claim stated in a recently proposed strongly polynomial-time algorithm for the general LP problem. That Claim is proved in this article and, as a consequence of this proof, a compact implementation of the strongly polynomial-time algorithm is suggested.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19350v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Samuel Awoniyi</dc:creator>
    </item>
    <item>
      <title>Accelerated optimization algorithms and ordinary differential equations: the convex non Euclidean case</title>
      <link>https://arxiv.org/abs/2410.19380</link>
      <description>arXiv:2410.19380v1 Announce Type: new 
Abstract: We study the connections between ordinary differential equations and optimization algorithms in a non-Euclidean setting. We propose a novel accelerated algorithm for minimising convex functions over a convex constrained set. This algorithm is a natural generalization of Nesterov's accelerated gradient descent method to the non-Euclidean setting and can be interpreted as an additive Runge-Kutta algorithm. The algorithm can also be derived as a numerical discretization of the ODE appearing in Krichene et al. (2015a). We use Lyapunov functions to establish convergence rates for the ODE and show that the discretizations considered achieve acceleration beyond the setting studied in Krichene et al. (2015a). Finally, we discuss how the proposed algorithm connects to various equations and algorithms in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19380v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul Dobson, Jesus Mar\'ia Sanz-Serna, Konstantinos C. Zygalakis</dc:creator>
    </item>
    <item>
      <title>A stochastic method of moving asymptotes for topology optimization under uncertainty</title>
      <link>https://arxiv.org/abs/2410.19428</link>
      <description>arXiv:2410.19428v1 Announce Type: new 
Abstract: Topology optimization under uncertainty or reliability-based topology optimization is usually numerically very expensive. This is mainly due to the fact that an accurate evaluation of the probabilistic model requires the system to be simulated for a large number of varying parameters. Traditional gradient-based optimization schemes thus face the difficulty that reasonable accuracy and numerical efficiency often seem mutually exclusive. In this work, we propose a stochastic optimization technique to tackle this problem. To be precise, we combine the well-known method of moving asymptotes (MMA) with a stochastic sample-based integration strategy. By adaptively recombining gradient information from previous steps, we obtain a noisy gradient estimator that is asymptotically correct, i.e., the approximation error vanishes over the course of iterations. As a consequence, the resulting stochastic method of moving asymptotes (sMMA) allows us to solve chance constraint topology optimization problems for a fraction of the cost compared to traditional approaches from literature. To demonstrate the efficiency of sMMA, we analyze structural optimization problems in two and three dimensions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19428v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lukas Pflug, Michael Stingl, Andrian Uihlein</dc:creator>
    </item>
    <item>
      <title>A Distributed Time-Varying Optimization Approach Based on an Event-Triggered Scheme</title>
      <link>https://arxiv.org/abs/2410.19458</link>
      <description>arXiv:2410.19458v1 Announce Type: new 
Abstract: In this paper, we present an event-triggered distributed optimization approach including a distributed controller to solve a class of distributed time-varying optimization problems (DTOP). The proposed approach is developed within a distributed neurodynamic (DND) framework that not only optimizes the global objective function in real-time, but also ensures that the states of the agents converge to consensus. This work stands out from existing methods in two key aspects. First, the distributed controller enables the agents to communicate only at designed instants rather than continuously by an event-triggered scheme, which reduces the energy required for agent communication. Second, by incorporating an integral mode technique, the event-triggered distributed controller avoids computing the inverse of the Hessian of each local objective function, thereby reducing computational costs. Finally, an example of battery charging problem is provided to demonstrate the effectiveness of the proposed event-triggered distributed optimization approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19458v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haojin Li, Xiaodong Cheng, Peter van Heijster, Sitian Qin</dc:creator>
    </item>
    <item>
      <title>Observer-based output feedback for an age-structured SIRD model</title>
      <link>https://arxiv.org/abs/2410.19481</link>
      <description>arXiv:2410.19481v1 Announce Type: new 
Abstract: An age-structured Susceptible-Infected-Recovered-Deceased (SIRD) epidemic model is considered. The aim of this paper is to design an observer-based output feedback control law, representing an immunization process, typically vaccination, intended to decrease the peak of infected individuals in the population. At first, well-posedness and stability of the system in open-loop are investigated. Then, to obtain the observer-based output feedback law, a state feedback law is designed by using a normal form. Conditions to ensure stability are established. However, due to physical constraints, this law needs to be adapted. Therefore, a constrained state-feedback law is implemented. This law is designed to fulfill the physical constraints while having good properties (Lipschitz for instance), needed for the last part of the article. Finally, an observer-based output feedback law is obtained using high-gain observer. At each step of the design, convergence properties are obtained. Finally, numerical simulations are performed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19481v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Candy Sonveaux, Christophe Prieur, Gildas Besan\c{c}on, Joseph J. Winkin</dc:creator>
    </item>
    <item>
      <title>A Trust Region Proximal Gradient Method for Nonlinear Multi-objective Optimization Problems</title>
      <link>https://arxiv.org/abs/2410.19502</link>
      <description>arXiv:2410.19502v1 Announce Type: new 
Abstract: In this paper, a globally convergent trust region proximal gradient method is developed for composite multi-objective optimization problems where each objective function can be represented as the sum of a smooth function and a nonsmooth function. The proposed method is free from any kind of priori chosen parameters or ordering information of objective functions. At every iteration of the proposed method, a sub problem is solved to find a suitable direction. This sub problem uses a quadratic approximation of each smooth function and a trust region constraint. An update formula for trust region radius is introduce in this paper. A sequence is generated using descent directions. It is justified that under some mild assumptions every accumulation point of this sequence is a critical point. The proposed method is verified and compared with some existing methods using a set of problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19502v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Abu Talhamainuddin Ansary</dc:creator>
    </item>
    <item>
      <title>Optimization with First Order Algorithms</title>
      <link>https://arxiv.org/abs/2410.19506</link>
      <description>arXiv:2410.19506v1 Announce Type: new 
Abstract: These notes focus on the minimization of convex functionals using first-order optimization methods, which are fundamental in many areas of applied mathematics and engineering. The primary goal of this document is to introduce and analyze the most classical first-order optimization algorithms. We aim to provide readers with both a practical and theoretical understanding in how and why these algorithms converge to minimizers of convex functions. The main algorithms covered in these notes include gradient descent, Forward-Backward splitting, Douglas-Rachford splitting, the Alternating Direction Method of Multipliers (ADMM), and Primal-Dual algorithms. All these algorithms fall into the class of first order methods, as they only involve gradients and subdifferentials, that are first order derivatives of the functions to optimize. For each method, we provide convergence theorems, with precise assumptions and conditions under which the convergence holds, accompanied by complete proofs. Beyond convex optimization, the final part of this manuscript extends the analysis to nonconvex problems, where we discuss the convergence behavior of these same first-order methods under broader assumptions. To contextualize the theory, we also include a selection of practical examples illustrating how these algorithms are applied in different image processing problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19506v1</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Dossal, Samuel Hurault, Nicolas Papadakis</dc:creator>
    </item>
    <item>
      <title>Improving Stochastic Cubic Newton with Momentum</title>
      <link>https://arxiv.org/abs/2410.19644</link>
      <description>arXiv:2410.19644v1 Announce Type: new 
Abstract: We study stochastic second-order methods for solving general non-convex optimization problems. We propose using a special version of momentum to stabilize the stochastic gradient and Hessian estimates in Newton's method. We show that momentum provably improves the variance of stochastic estimates and allows the method to converge for any noise level. Using the cubic regularization technique, we prove a global convergence rate for our method on general non-convex problems to a second-order stationary point, even when using only a single stochastic data sample per iteration. This starkly contrasts with all existing stochastic second-order methods for non-convex problems, which typically require large batches. Therefore, we are the first to demonstrate global convergence for batches of arbitrary size in the non-convex case for the Stochastic Cubic Newton. Additionally, we show improved speed on convex stochastic problems for our regularized Newton methods with momentum.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19644v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>El Mahdi Chayti, Nikita Doikov, Martin Jaggi</dc:creator>
    </item>
    <item>
      <title>On a Geometric Interpretation Of the Subset Sum Problem</title>
      <link>https://arxiv.org/abs/2410.19024</link>
      <description>arXiv:2410.19024v1 Announce Type: cross 
Abstract: For $S \in \mathbb{N}^n$ and $T \in \mathbb{N}$, the Subset Sum Problem (SSP) $\exists^? x \in \{0,1\}^n $ such that $S^T\cdot x = T$ can be interpreted as the problem of deciding whether the intersection of the positive unit hypercube $Q_n = [0,1]^n$ with the hyperplane $S^T\cdot \left(x - \frac{S}{\|S\|^2 }\cdot T \right) = 0$ contains at least a vertex. In this paper, we give an algorithm of complexity $\mathcal{O}\left( \frac{1}{\epsilon}\cdot n^b \right)$, for some absolute constant $b$, which either proves that there are no vertices in a slab of thickness $\epsilon$ either finds a vertex in the slab of thickness $4\cdot \epsilon$. It is shown that any vertex $P$ in a slab of thickness $\epsilon$ meets $\left| \frac{S^T\cdot P}{T} - 1 \right| \leq \epsilon$, therefore making the proposed algorithm a FPTAS for the SSP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19024v1</guid>
      <category>cs.CG</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marius Costandin</dc:creator>
    </item>
    <item>
      <title>State-Dependent Linear Utility Functions for Monetary Returns</title>
      <link>https://arxiv.org/abs/2410.19030</link>
      <description>arXiv:2410.19030v1 Announce Type: cross 
Abstract: We present a theory of expected utility with state-dependent linear utility function for monetary returns, that includes results on first order stochastic dominance, mean-preserving spread, increasing-concave linear utility profiles and risk aversion. As an application of the expected utility theory developed here, we analyze the contract that a monopolist would offer in an insurance market that allowed for partial coverage of loss. We also define a utility function for monetary returns that in a certain sense reconciles state-dependent constant average utility of money with loss aversion and the Friedman-Savage hypothesis. As an immediate consequence of such a utility function, we obtain a profile of state-dependent linear utility functions for monetary returns, where states of nature correspond to mutually disjoint intervals in which monetary gains and losses may occur.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19030v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <category>math.OC</category>
      <category>q-fin.PM</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Somdeb Lahiri</dc:creator>
    </item>
    <item>
      <title>Gradient Descent Efficiency Index</title>
      <link>https://arxiv.org/abs/2410.19448</link>
      <description>arXiv:2410.19448v1 Announce Type: cross 
Abstract: Gradient descent is a widely used iterative algorithm for finding local minima in multivariate functions. However, the final iterations often either overshoot the minima or make minimal progress, making it challenging to determine an optimal stopping point. This study introduces a new efficiency metric, Ek, designed to quantify the effectiveness of each iteration. The proposed metric accounts for both the relative change in error and the stability of the loss function across iterations. This measure is particularly valuable in resource-constrained environments, where costs are closely tied to training time. Experimental validation across multiple datasets and models demonstrates that Ek provides valuable insights into the convergence behavior of gradient descent, complementing traditional performance metrics. The index has the potential to guide more informed decisions in the selection and tuning of optimization algorithms in machine learning applications and be used to compare the "effectiveness" of models relative to each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19448v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aviral Dhingra</dc:creator>
    </item>
    <item>
      <title>Super Gradient Descent: Global Optimization requires Global Gradient</title>
      <link>https://arxiv.org/abs/2410.19706</link>
      <description>arXiv:2410.19706v1 Announce Type: cross 
Abstract: Global minimization is a fundamental challenge in optimization, especially in machine learning, where finding the global minimum of a function directly impacts model performance and convergence. This report introduces a novel optimization method that we called Super Gradient Descent, designed specifically for one-dimensional functions, guaranteeing convergence to the global minimum for any k-Lipschitz function defined on a closed interval [a, b]. Our approach addresses the limitations of traditional optimization algorithms, which often get trapped in local minima. In particular, we introduce the concept of global gradient which offers a robust solution for precise and well-guided global optimization. By focusing on the global minimization problem, this work bridges a critical gap in optimization theory, offering new insights and practical advancements in different optimization problems in particular Machine Learning problems like line search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19706v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Seifeddine Achour</dc:creator>
    </item>
    <item>
      <title>An acceleration technique for methods for finding the nearest point in a polytope and computing the distance between two polytopes</title>
      <link>https://arxiv.org/abs/2205.04553</link>
      <description>arXiv:2205.04553v5 Announce Type: replace 
Abstract: We present a simple and efficient acceleration technique for an arbitrary method for computing the Euclidean projection of a point onto a convex polytope, defined as the convex hull of a finite number of points, in the case when the number of points in the polytope is much greater than the dimension of the space. The technique consists in applying any given method to a "small" subpolytope of the original polytope and gradually shifting it, till the projection of the given point onto the subpolytope coincides with its projection onto the original polytope. The results of numerical experiments demonstrate the high efficiency of the proposed acceleration technique. In particular, they show that the reduction of computation time increases with an increase of the number of points in the polytope and is proportional to this number for some methods. In the second part of the paper, we also discuss a straightforward extension of the proposed acceleration technique to the case of arbitrary methods for computing the distance between two convex polytopes, defined as the convex hulls of finite sets of points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2205.04553v5</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. V. Dolgopolik</dc:creator>
    </item>
    <item>
      <title>Fixed-Point Automatic Differentiation of Forward--Backward Splitting Algorithms for Partly Smooth Functions</title>
      <link>https://arxiv.org/abs/2208.03107</link>
      <description>arXiv:2208.03107v3 Announce Type: replace 
Abstract: A large class of non-smooth practical optimization problems can be written as minimization of a sum of smooth and partly smooth functions. We examine such structured problems which also depend on a parameter vector and study the problem of differentiating its solution mapping with respect to the parameter which has far reaching applications in sensitivity analysis and parameter learning problems. Under partial smoothness and other mild assumptions, we apply Implicit (ID) and Automatic Differentiation (AD) to the fixed-point iterations of proximal splitting algorithms. We show that AD of the sequence generated by these algorithms converges (linearly under further assumptions) to the derivative of the solution mapping. For a variant of automatic differentiation, which we call Fixed-Point Automatic Differentiation (FPAD), we remedy the memory overhead problem of the Reverse Mode AD and moreover provide faster convergence theoretically. We numerically illustrate the convergence and convergence rates of AD and FPAD on Lasso and Group Lasso problems and demonstrate the working of FPAD on prototypical image denoising problems by learning the regularization term.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03107v3</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheheryar Mehmood, Peter Ochs</dc:creator>
    </item>
    <item>
      <title>Contest for system observability as an infinitely repeated game</title>
      <link>https://arxiv.org/abs/2306.13570</link>
      <description>arXiv:2306.13570v2 Announce Type: replace 
Abstract: This paper studies a system security problem in the context of observability based on a two-person non-cooperative infinitely repeated game. Both the attacker and the defender have means to modify the dimension of the unobservable subspace, which is set as the value function. Utilizing tools from geometric control, we construct the best response sets considering one-step and two-step optimality respectively to maximize or minimize the value function. We find that the best response sets are not single-valued maps, resulting in a variety of game outcomes. In addition, we establish a necessary and sufficient condition for Nash equilibrium. The game analysis reveals that the defender plays a pivotal role in steering the game towards equilibrium, while the attacker can disrupt the equilibrium by sacrificing immediate gains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13570v2</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yueyue Xu, Panpan Zhou, Lin Wang, Zhixin Liu, Xiaoming Hu</dc:creator>
    </item>
    <item>
      <title>Higher-order Lie bracket approximation and averaging of control-affine systems with application to extremum seeking</title>
      <link>https://arxiv.org/abs/2310.07092</link>
      <description>arXiv:2310.07092v4 Announce Type: replace 
Abstract: This paper provides a rigorous derivation for what is known in the literature as the Lie bracket approximation of control-affine systems in a sequential framework for higher-orders. In fact, by using chronological calculus, we show that said Lie bracket approximations are themselves higher-order averaging terms. Hence, this paper bridges both averaging and approximation theories of control-affine systems. In particular, the Lie bracket approximation of order ($n$) turns out to be a higher-order averaging of order ($n+1$). The derivation and formulation provided in this paper can be directly reduced to the first and second-order Lie bracket approximations available in the literature. However, we do not need to make many of the unproven assumptions provided in the literature and show that they are in fact natural corollaries from our work. Moreover, we use our results to show that important and useful information about control-affine extremum seeking systems can be obtained and used for performance improvement, including faster convergence of very simple structures. We provide multiple numerical simulations to demonstrate both the conceptual elements of this work as well as the significance of our results on extremum seeking with comparison against the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07092v4</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sameer Pokhrel, Sameh A. Eisa</dc:creator>
    </item>
    <item>
      <title>The cosine measure relative to a subspace</title>
      <link>https://arxiv.org/abs/2401.09609</link>
      <description>arXiv:2401.09609v2 Announce Type: replace 
Abstract: The cosine measure was introduced in 2003 to quantify the richness of a finite positive spanning sets of directions in the context of derivative-free directional methods. A positive spanning set is a set of vectors whose nonnegative linear combinations span the whole space. The present work extends the definition of cosine measure. In particular, the paper studies cosine measures relative to a subspace, and proposes a deterministic algorithm to compute it. The paper also studies the situation in which the set of vectors is infinite. The extended definition of the cosine measure might be useful for subspace decomposition methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.09609v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Charles Audet, Warren Hare, Gabriel Jarry-Bolduc</dc:creator>
    </item>
    <item>
      <title>On the saddle point of a zero-sum stopper vs. singular-controller game</title>
      <link>https://arxiv.org/abs/2401.17719</link>
      <description>arXiv:2401.17719v3 Announce Type: replace 
Abstract: We construct a saddle point in a class of zero-sum games between a stopper and a singular-controller. The underlying dynamics is a one-dimensional, time-homogeneous, singularly controlled diffusion taking values either on $\mathbb{R}$ or on $[0,\infty)$. The games are set on a finite-time horizon, thus leading to analytical problems in the form of parabolic variational inequalities with gradient and obstacle constraints. The saddle point is characterised in terms of two moving boundaries: an optimal stopping boundary and an optimal control boundary. These boundaries allow us to construct an optimal stopping time for the stopper and an optimal control for the singular-controller. Our method relies on a new link between the value function of the game and the value function of an auxiliary optimal stopping problem with absorption. We show that the smooth-fit condition at the stopper's optimal boundary (in the game), translates into an absorption condition in the auxiliary problem. This is somewhat in contrast with results obtained in problems of singular control with absorption and it highlights the key role of smooth-fit in this context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17719v3</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrea Bovo, Tiziano De Angelis</dc:creator>
    </item>
    <item>
      <title>Global approximate controllability of quantum systems by form perturbations and applications</title>
      <link>https://arxiv.org/abs/2402.02955</link>
      <description>arXiv:2402.02955v2 Announce Type: replace 
Abstract: We provide sufficient conditions for the approximate controllability of infinite-dimensional quantum control systems corresponding to form perturbations of the drift Hamiltonian modulated by a control function. We rely on previous results on controllability of quantum bilinear control systems and obtain a priori $L^1$-bounds of the controls for generic initial and target states. We apply a stability result for the non-autonomous Schr\"odinger equation to extend the results to systems defined by form perturbations, including singular perturbations. As an application of our results, we prove approximate controllability of a quantum particle in a one-dimensional box with a point-interaction with tuneable strength at the centre of the box.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.02955v2</guid>
      <category>math.OC</category>
      <category>math-ph</category>
      <category>math.FA</category>
      <category>math.MP</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aitor Balmaseda, Davide Lonigro, Juan Manuel P\'erez-Pardo</dc:creator>
    </item>
    <item>
      <title>A Decentralized Shotgun Approach for Team Deception</title>
      <link>https://arxiv.org/abs/2406.17160</link>
      <description>arXiv:2406.17160v3 Announce Type: replace 
Abstract: Deception is helpful for agents masking their intentions from an observer. We consider a team of agents deceiving their supervisor. The supervisor defines nominal behavior for the agents via reference policies, but the agents share an alternate task that they can only achieve by deviating from these references. As such, the agents use deceptive policies to complete the task while ensuring that their behaviors remain plausible to the supervisor. We propose a setting with centralized deceptive policy synthesis and decentralized execution. We model each agent with a Markov decision process and constrain the agents' deceptive policies so that, with high probability, at least one agent achieves the task. We then provide an algorithm to synthesize deceptive policies that ensure the deviations of all agents are small by minimizing the worst Kullback-Leibler divergence between any agent's deceptive and reference policies. Thanks to decentralization, this algorithm scales linearly with the number of agents and also facilitates the efficient synthesis of reference policies. We then explore a more general version of the deceptive policy synthesis problem. In particular, we consider a supervisor who selects a subset of agents to eliminate based on the agents' behaviors. We give algorithms to synthesize deceptive policies so that, after the supervisor eliminates some agents, the remaining agents complete the task with high probability. We demonstrate the developed methods in a package delivery example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17160v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-74835-6_9</arxiv:DOI>
      <arxiv:journal_reference>In: Decision and Game Theory for Security. GameSec 2024. Lecture Notes in Computer Science, vol. 14908, pp. 177-197. Springer, Cham (2025)</arxiv:journal_reference>
      <dc:creator>Caleb Probine, Mustafa O. Karabag, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>On finite termination of quasi-Newton methods on quadratic problems</title>
      <link>https://arxiv.org/abs/2407.03072</link>
      <description>arXiv:2407.03072v2 Announce Type: replace 
Abstract: Quasi-Newton methods form an important class of methods for solving nonlinear optimization problems. In such methods, first order information is used to approximate the second derivative. The aim is to mimic the fast convergence that can be guaranteed by Newton-based methods. In the best case, quasi-Newton methods will far outperform steepest descent and other first order methods, without the computational cost of calculating the exact second derivative. These convergence guarantees hold locally, which follows closely from the fact that, if the objective function is strongly convex, it can be approximated well by a quadratic function close to the solution. Understanding the performance of quasi-Newton methods on quadratic problems with a symmetric positive definite Hessian is therefore of vital importance. In the classic case, an approximation of the Hessian is updated at every iteration and exact line search is used. It is well known that the algorithm terminates finitely, even when the Hessian approximation is memoryless, i.e. requires only the most recent information. In this paper, we explore the possibilities in which reliance on exact line search and dependence on conjugate search directions can be relaxed, while preserving finite termination properties of quasi-Newton methods on quadratic problems. We show that it suffices to create a memoryless quasi-Newton matrix based on two vectors to give ability to compute a Newton direction within a finite number of iterations, independent of step lengths. It is unnecessary for the quasi-Newton approximation to act as the Hessian on the full space.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03072v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aban Ansari-\"Onnestam, Anders Forsgren</dc:creator>
    </item>
    <item>
      <title>Small-Time Local Controllability of the multi-input bilinear Schr\"odinger equation thanks to a quadratic term</title>
      <link>https://arxiv.org/abs/2407.07446</link>
      <description>arXiv:2407.07446v3 Announce Type: replace 
Abstract: The goal of this article is to contribute to a better understanding of the relations between the exact controllability of nonlinear PDEs and the control theory for ODEs based on Lie brackets, through a study of the Schr\"odinger PDE with bilinear control. We focus on the small-time local controllability (STLC) around an equilibrium, when the linearized system is not controllable. We study the second order term in the Taylor expansion of the state, with respect to the control. For scalar-input ODEs, quadratic terms never recover controllability: they induce signed drifts in the dynamics. Thus proving STLC requires to go at least to the third order. Similar results were proved for the bilinear Schr\"odinger PDE with scalar-input controls. In this article, we study the case of multi-input systems. We clarify among the quadratic Lie brackets, those that allow to recover STLC: they are bilinear with respect to two different controls. For ODEs, our result is a consequence of Sussman's sufficient condition $S(\theta)$ (when focused on quadratic terms), but we propose a new proof, designed to prepare an easier transfer to PDEs. This proof relies on a representation formula of the state inspired by the Magnus formula. By adapting it, we prove a new STLC result for the bilinear Schr\"odinger PDE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07446v3</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Th\'eo Gherdaoui (IRMAR, ENS Rennes)</dc:creator>
    </item>
    <item>
      <title>Distributed Computing for Huge-Scale Linear Programming</title>
      <link>https://arxiv.org/abs/2408.06204</link>
      <description>arXiv:2408.06204v4 Announce Type: replace 
Abstract: This study develops an algorithm for distributed computing of linear programming problems of huge-scales. Global consensus with single common variable, multiblocks, and augmented Lagrangian are adopted. The consensus is used to partition the constraints of equality and inequality into multi-consensus blocks, and the subblocks of each consensus block are employed to partition the primal variables into $M$ sets of disjoint subvectors. The block-coordinate Gauss-Seidel method, the proximal point method, and ADMM are used to update the primal variables, and descent models used to update the dual. Convergence of the algorithm to optimal solution is shown and the rate of convergence of the augmented Lagrangian, of $O(1/k)$ is obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.06204v4</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luoyi Tao</dc:creator>
    </item>
    <item>
      <title>SymILO: A Symmetry-Aware Learning Framework for Integer Linear Optimization</title>
      <link>https://arxiv.org/abs/2409.19678</link>
      <description>arXiv:2409.19678v2 Announce Type: replace 
Abstract: Integer linear programs (ILPs) are commonly employed to model diverse practical problems such as scheduling and planning. Recently, machine learning techniques have been utilized to solve ILPs. A straightforward idea is to train a model via supervised learning, with an ILP as the input and an optimal solution as the label. An ILP is symmetric if its variables can be permuted without changing the problem structure, resulting in numerous equivalent and optimal solutions. Randomly selecting an optimal solution as the label can introduce variability in the training data, which may hinder the model from learning stable patterns. In this work, we incorporate the intrinsic symmetry of ILPs and propose a novel training framework called SymILO. Specifically, we modify the learning task by introducing solution permutation along with neural network weights as learnable parameters and then design an alternating algorithm to jointly optimize the loss function. We conduct extensive experiments on ILPs involving different symmetries and the computational results demonstrate that our symmetry-aware approach significantly outperforms three existing methods -- achieving $50.3\%$, $66.5\%$, and $45.4\%$ average improvements, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19678v2</guid>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qian Chen, Tianjian Zhang, Linxin Yang, Qingyu Han, Akang Wang, Ruoyu Sun, Xiaodong Luo, Tsung-Hui Chang</dc:creator>
    </item>
    <item>
      <title>Congestion and Penalization in Optimal Transport</title>
      <link>https://arxiv.org/abs/2410.07363</link>
      <description>arXiv:2410.07363v3 Announce Type: replace 
Abstract: In this paper we introduce two novel models derived from the discrete optimal transport problem. The first model extends the traditional transport problem by adding a quadratic congestion factor directly into the cost function, while the second model replaces conventional constraints with weighted penalization terms. We present theoretical results, for the characterization of interior and corner solution for some specific cases, and we perform smooth comparative statics analysis. We also propose an $O((N+L)(NL)^2)$ algorithm for computing the optimal plan for the penalized model. Additionally, in the annex, we provide some examples that illustrate the main results of the paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.07363v3</guid>
      <category>math.OC</category>
      <category>econ.TH</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcelo Gallardo, Manuel Loaiza, Jorge Ch\'avez</dc:creator>
    </item>
    <item>
      <title>Time-Varying Convex Optimization with $O(n)$ Computational Complexity</title>
      <link>https://arxiv.org/abs/2410.15009</link>
      <description>arXiv:2410.15009v2 Announce Type: replace 
Abstract: In this article, we consider the problem of unconstrained time-varying convex optimization, where the cost function changes with time. We provide an in-depth technical analysis of the problem and argue why freezing the cost at each time step and taking finite steps toward the minimizer is not the best tracking solution for this problem. We propose a set of algorithms that by taking into account the temporal variation of the cost aim to reduce the tracking error of the time-varying minimizer of the problem. The main contribution of our work is that our proposed algorithms only require the first-order derivatives of the cost function with respect to the decision variable. This approach significantly reduces computational cost compared to the existing algorithms, which use the inverse of the Hessian of the cost. Specifically, the proposed algorithms reduce the computational cost from $O(n^3)$ to $O(n)$ per timestep, where $n$ is the size of the decision variable. Avoiding the inverse of the Hessian also makes our algorithms applicable to non-convex optimization problems. We refer to these algorithms as $O(n)$-algorithms. These $O(n)$-algorithms are designed to solve the problem for different scenarios based on the available temporal information about the cost. We illustrate our results through various examples, including the solution of a model predictive control problem framed as a convex optimization problem with a streaming time-varying cost function.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15009v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Rostami, S. S. Kia</dc:creator>
    </item>
    <item>
      <title>Automatic Differentiation of Optimization Algorithms with Time-Varying Updates</title>
      <link>https://arxiv.org/abs/2410.15923</link>
      <description>arXiv:2410.15923v2 Announce Type: replace 
Abstract: Numerous Optimization Algorithms have a time-varying update rule thanks to, for instance, a changing step size, momentum parameter or, Hessian approximation. In this paper, we apply unrolled or automatic differentiation to a time-varying iterative process and provide convergence (rate) guarantees for the resulting derivative iterates. We adapt these convergence results and apply them to proximal gradient descent with variable step size and FISTA when solving partly smooth problems. We confirm our findings numerically by solving $\ell_1$ and $\ell_2$-regularized linear and logisitc regression respectively. Our theoretical and numerical results show that the convergence rate of the algorithm is reflected in its derivative iterates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.15923v2</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sheheryar Mehmood, Peter Ochs</dc:creator>
    </item>
    <item>
      <title>Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects</title>
      <link>https://arxiv.org/abs/2208.04883</link>
      <description>arXiv:2208.04883v4 Announce Type: replace-cross 
Abstract: Interstellar objects (ISOs) are likely representatives of primitive materials invaluable in understanding exoplanetary star systems. Due to their poorly constrained orbits with generally high inclinations and relative velocities, however, exploring ISOs with conventional human-in-the-loop approaches is significantly challenging. This paper presents Neural-Rendezvous -- a deep learning-based guidance and control framework for encountering fast-moving objects, including ISOs, robustly, accurately, and autonomously in real time. It uses pointwise minimum norm tracking control on top of a guidance policy modeled by a spectrally-normalized deep neural network, where its hyperparameters are tuned with a loss function directly penalizing the MPC state trajectory tracking error. We show that Neural-Rendezvous provides a high probability exponential bound on the expected spacecraft delivery error, the proof of which leverages stochastic incremental stability analysis. In particular, it is used to construct a non-negative function with a supermartingale property, explicitly accounting for the ISO state uncertainty and the local nature of nonlinear state estimation guarantees. In numerical simulations, Neural-Rendezvous is demonstrated to satisfy the expected error bound for 100 ISO candidates. This performance is also empirically validated using our spacecraft simulator and in high-conflict and distributed UAV swarm reconfiguration with up to 20 UAVs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.04883v4</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.2514/1.G007671</arxiv:DOI>
      <dc:creator>Hiroyasu Tsukamoto, Soon-Jo Chung, Yashwanth Kumar Nakka, Benjamin Donitz, Declan Mages, Michel Ingham</dc:creator>
    </item>
    <item>
      <title>Bayan Algorithm: Detecting Communities in Networks Through Exact and Approximate Optimization of Modularity</title>
      <link>https://arxiv.org/abs/2209.04562</link>
      <description>arXiv:2209.04562v5 Announce Type: replace-cross 
Abstract: Community detection is a classic network problem with extensive applications in various fields. Its most common method is using modularity maximization heuristics which rarely return an optimal partition or anything similar. Partitions with globally optimal modularity are difficult to compute, and therefore have been underexplored. Using structurally diverse networks, we compare 30 community detection methods including our proposed algorithm that offers optimality and approximation guarantees: the Bayan algorithm. Unlike existing methods, Bayan globally maximizes modularity or approximates it within a factor. Our results show the distinctive accuracy and stability of maximum-modularity partitions in retrieving planted partitions at rates higher than most alternatives for a wide range of parameter settings in two standard benchmarks. Compared to the partitions from 29 other algorithms, maximum-modularity partitions have the best medians for description length, coverage, performance, average conductance, and well clusteredness. These advantages come at the cost of additional computations which Bayan makes possible for small networks (networks that have up to 3000 edges in their largest connected component). Bayan is several times faster than using open-source and commercial solvers for modularity maximization, making it capable of finding optimal partitions for instances that cannot be optimized by any other existing method. Our results point to a few well performing algorithms, among which Bayan stands out as the most reliable method for small networks. A Python implementation of the Bayan algorithm (bayanpy) is publicly available through the package installer for Python.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.04562v5</guid>
      <category>cs.SI</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevE.110.044315</arxiv:DOI>
      <arxiv:journal_reference>Physical Review E 110 (2024), 044315. Issue 4</arxiv:journal_reference>
      <dc:creator>Samin Aref, Mahdi Mostajabdaveh, Hriday Chheda</dc:creator>
    </item>
    <item>
      <title>Global stability and optimal control in a single-strain dengue model with fractional-order transmission and recovery process</title>
      <link>https://arxiv.org/abs/2402.11974</link>
      <description>arXiv:2402.11974v2 Announce Type: replace-cross 
Abstract: The current manuscript introduce a single-strain dengue model developed from stochastic processes incorporating fractional order transmission and recovery. The fractional derivative has been introduced within the context of transmission and recovery process, displaying characteristics similar to tempered fractional ($TF$) derivatives. It has been established that under certain condition, a function's $TF$ derivatives are proportional to the function itself. Applying the following observation, we examined stability of several steady-state solutions, such as disease-free and endemic states, in light of this newly formulated model, using the reproduction number (R_0). In addition, the precise range of epidemiological parameters for the fractional order model was determined by calibrating weekly registered dengue incidence in the San Juan municipality of Puerto Rico, from April 9, 2010, to April 2, 2011. We performed a global sensitivity analysis method to measure the influence of key model parameters (along with the fractional-order coefficient) on total dengue cases and the basic reproduction number (R_0) using a Monte Carlo-based partial rank correlation coefficient (PRCC). Moreover, we formulated a fractional-order model with fractional control to asses the effectiveness of different interventions, such as reduction the recruitment rate of mosquito breeding, controlling adult vector, and providing individual protection. Also, we established the existence of a solution for the fractional-order optimal control problem. Finally, the numerical experiment illustrates that, policymakers should place importance on the fractional order transmission and recovery parameters that capture the underline mechanisms of disease along with reducing the spread of dengue cases, carried out through the implementation of two vector controls.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.11974v2</guid>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tahajuddin Sk, Kaushik Bal, Santosh Biswas, Tridip Sardar</dc:creator>
    </item>
    <item>
      <title>Computing the Bias of Constant-step Stochastic Approximation with Markovian Noise</title>
      <link>https://arxiv.org/abs/2405.14285</link>
      <description>arXiv:2405.14285v2 Announce Type: replace-cross 
Abstract: We study stochastic approximation algorithms with Markovian noise and constant step-size $\alpha$. We develop a method based on infinitesimal generator comparisons to study the bias of the algorithm, which is the expected difference between $\theta_n$ -- the value at iteration $n$ -- and $\theta^*$ -- the unique equilibrium of the corresponding ODE. We show that, under some smoothness conditions, this bias is of order $O(\alpha)$. Furthermore, we show that the time-averaged bias is equal to $\alpha V + O(\alpha^2)$, where $V$ is a constant characterized by a Lyapunov equation, showing that $\mathbb{E}[\bar{\theta}_n] \approx \theta^*+V\alpha + O(\alpha^2)$, where $\bar{\theta}_n=(1/n)\sum_{k=1}^n\theta_k$ is the Polyak-Ruppert average. We also show that $\bar{\theta}_n$ converges with high probability around $\theta^*+\alpha V$. We illustrate how to combine this with Richardson-Romberg extrapolation to derive an iterative scheme with a bias of order $O(\alpha^2)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14285v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sebastian Allmeier, Nicolas Gast</dc:creator>
    </item>
    <item>
      <title>A constrained optimization approach to improve robustness of neural networks</title>
      <link>https://arxiv.org/abs/2409.13770</link>
      <description>arXiv:2409.13770v2 Announce Type: replace-cross 
Abstract: In this paper, we present a novel nonlinear programming-based approach to fine-tune pre-trained neural networks to improve robustness against adversarial attacks while maintaining high accuracy on clean data. Our method introduces adversary-correction constraints to ensure correct classification of adversarial data and minimizes changes to the model parameters. We propose an efficient cutting-plane-based algorithm to iteratively solve the large-scale nonconvex optimization problem by approximating the feasible region through polyhedral cuts and balancing between robustness and accuracy. Computational experiments on standard datasets such as MNIST and CIFAR10 demonstrate that the proposed approach significantly improves robustness, even with a very small set of adversarial data, while maintaining minimal impact on accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13770v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shudian Zhao, Jan Kronqvist</dc:creator>
    </item>
    <item>
      <title>Loss Landscape Characterization of Neural Networks without Over-Parametrization</title>
      <link>https://arxiv.org/abs/2410.12455</link>
      <description>arXiv:2410.12455v3 Announce Type: replace-cross 
Abstract: Optimization methods play a crucial role in modern machine learning, powering the remarkable empirical achievements of deep learning models. These successes are even more remarkable given the complex non-convex nature of the loss landscape of these models. Yet, ensuring the convergence of optimization methods requires specific structural conditions on the objective function that are rarely satisfied in practice. One prominent example is the widely recognized Polyak-Lojasiewicz (PL) inequality, which has gained considerable attention in recent years. However, validating such assumptions for deep neural networks entails substantial and often impractical levels of over-parametrization. In order to address this limitation, we propose a novel class of functions that can characterize the loss landscape of modern deep models without requiring extensive over-parametrization and can also include saddle points. Crucially, we prove that gradient-based optimizers possess theoretical guarantees of convergence under this assumption. Finally, we validate the soundness of our new function class through both theoretical analysis and empirical experimentation across a diverse range of deep learning models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12455v3</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ML</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Rustem Islamov, Niccol\`o Ajroldi, Antonio Orvieto, Aurelien Lucchi</dc:creator>
    </item>
    <item>
      <title>Comparative Analysis of Indicators for Multiobjective Diversity Optimization</title>
      <link>https://arxiv.org/abs/2410.18900</link>
      <description>arXiv:2410.18900v2 Announce Type: replace-cross 
Abstract: Indicator-based (multiobjective) diversity optimization aims at finding a set of near (Pareto-)optimal solutions that maximizes a diversity indicator, where diversity is typically interpreted as the number of essentially different solutions. Whereas, in the first diversity-oriented evolutionary multiobjective optimization algorithm, the NOAH algorithm by Ulrich and Thiele, the Solow Polasky Diversity (also related to Magnitude) served as a metric, other diversity indicators might be considered, such as the parameter-free Max-Min Diversity, and the Riesz s-Energy, which features uniformly distributed solution sets. In this paper, focusing on multiobjective diversity optimization, we discuss different diversity indicators from the perspective of indicator-based evolutionary algorithms (IBEA) with multiple objectives. We examine theoretical, computational, and practical properties of these indicators, such as monotonicity in species, twinning, monotonicity in distance, strict monotonicity in distance, uniformity of maximizing point sets, computational effort for a set of size~n, single-point contributions, subset selection, and submodularity. We present new theorems -- including a proof of the NP-hardness of the Riesz s-Energy Subset Selection Problem -- and consolidate existing results from the literature. In the second part, we apply these indicators in the NOAH algorithm and analyze search dynamics through an example. We examine how optimizing with one indicator affects the performance of others and propose NOAH adaptations specific to the Max-Min indicator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18900v2</guid>
      <category>cs.NE</category>
      <category>math.OC</category>
      <pubDate>Mon, 28 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ksenia Pereverdieva, Andr\'e Deutz, Tessa Ezendam, Thomas B\"ack, H\`erm Hofmeyer, Michael T. M. Emmerich</dc:creator>
    </item>
  </channel>
</rss>
