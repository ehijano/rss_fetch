<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DS updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DS</link>
    <description>cs.DS updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DS" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 14 May 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Tensor Sketch: Fast and Scalable Polynomial Kernel Approximation</title>
      <link>https://arxiv.org/abs/2505.08146</link>
      <description>arXiv:2505.08146v1 Announce Type: new 
Abstract: Approximation of non-linear kernels using random feature maps has become a powerful technique for scaling kernel methods to large datasets. We propose \textit{Tensor Sketch}, an efficient random feature map for approximating polynomial kernels. Given $n$ training samples in $\R^d$ Tensor Sketch computes low-dimensional embeddings in $\R^D$ in time $\BO{n(d+D \log{D})}$ making it well-suited for high-dimensional and large-scale settings. We provide theoretical guarantees on the approximation error, ensuring the fidelity of the resulting kernel function estimates. We also discuss extensions and highlight applications where Tensor Sketch serves as a central computational tool.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08146v1</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ninh Pham, Rasmus Pagh</dc:creator>
    </item>
    <item>
      <title>Uniform Universal Sets, Splitters, and Bisectors</title>
      <link>https://arxiv.org/abs/2505.08308</link>
      <description>arXiv:2505.08308v1 Announce Type: new 
Abstract: Given a subset of size $k$ of a very large universe a randomized way to find this subset could consist of deleting half of the universe and then searching the remaining part. With a probability of $2^{-k}$ one will succeed. By probability amplification, a randomized algorithm needs about $2^k$ rounds until it succeeds. We construct bisectors that derandomize this process and have size~$2^{k+o(k)}$. One application is derandomization of reductions between average case complexity classes. We also construct uniform $(n,k)$-universal sets that generalize universal sets in such a way that they are bisectors at the same time. This construction needs only linear time and produces families of asymptotically optimal size without using advanced combinatorial constructions as subroutines, which previous families did, but are basedmainly on modulo functions and refined brute force search.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08308v1</guid>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elisabet Burjons, Peter Rossmanith</dc:creator>
    </item>
    <item>
      <title>A k-swap Local Search for Makespan Scheduling</title>
      <link>https://arxiv.org/abs/2401.05956</link>
      <description>arXiv:2401.05956v2 Announce Type: replace 
Abstract: Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains. In this paper, we address the problem of scheduling a set of jobs on identical parallel machines with the objective of makespan minimization. For this problem, we consider a local search neighborhood, called $k$-swap, which is a generalized version of the widely-used swap and jump neighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines. First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm. Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood. For $k \geq 3$, we provide an exponential lower bound regardless of the number of machines, and for $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound for the case of having two machines. Finally, we conduct computational experiments on various families of instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.05956v2</guid>
      <category>cs.DS</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Lars Rohwedder, Ashkan Safari, Tjark Vredeveld</dc:creator>
    </item>
    <item>
      <title>A Quantum Constraint Generation Framework for Binary Linear Programs</title>
      <link>https://arxiv.org/abs/2503.21222</link>
      <description>arXiv:2503.21222v2 Announce Type: replace 
Abstract: We propose a new approach to utilize quantum computers for binary linear programming (BLP), which can be extended to general integer linear programs (ILP). Quantum optimization algorithms, hybrid or quantum-only, are currently general purpose, standalone solvers for ILP. However, to consider them practically useful, we expect them to overperform the current state of the art classical solvers. That expectation is unfair to quantum algorithms: in classical ILP solvers, after many decades of evolution, many different algorithms work together as a robust machine to get the best result. This is the approach we would like to follow now with our quantum 'solver' solutions. In this study we wrap any suitable quantum optimization algorithm into a quantum informed classical constraint generation framework. First we relax our problem by dropping all constraints and encode it into an Ising Hamiltonian for the quantum optimization subroutine. Then, by sampling from the solution state of the subroutine, we obtain information about constraint violations in the initial problem, from which we decide which coupling terms we need to introduce to the Hamiltonian. The coupling terms correspond to the constraints of the initial binary linear program. Then we optimize over the new Hamiltonian again, until we reach a feasible solution, or other stopping conditions hold. Since one can decide how many constraints they add to the Hamiltonian in a single step, our algorithm is at least as efficient as the (hybrid) quantum optimization algorithm it wraps. We support our claim with results on small scale minimum cost exact cover problem instances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21222v2</guid>
      <category>cs.DS</category>
      <category>quant-ph</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andr\'as Cz\'egel, Bogl\'arka G. -T\'oth</dc:creator>
    </item>
    <item>
      <title>Computing High-dimensional Confidence Sets for Arbitrary Distributions</title>
      <link>https://arxiv.org/abs/2504.02723</link>
      <description>arXiv:2504.02723v2 Announce Type: replace 
Abstract: We study the problem of learning a high-density region of an arbitrary distribution over $\mathbb{R}^d$. Given a target coverage parameter $\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \subset \mathbb{R}^d$ such that $S$ achieves $\delta$ coverage of $D$, i.e., $\mathbb{P}_{y \sim D} \left[ y \in S \right] \ge \delta$, and the volume of $S$ is as small as possible. This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation.
  In the most general setting, this problem is statistically intractable, so we restrict our attention to competing with sets from a concept class $C$ with bounded VC-dimension. An algorithm is competitive with class $C$ if, given samples from an arbitrary distribution $D$, it outputs in polynomial time a set that achieves $\delta$ coverage of $D$, and whose volume is competitive with the smallest set in $C$ with the required coverage $\delta$. This problem is computationally challenging even in the basic setting when $C$ is the set of all Euclidean balls. Existing algorithms based on coresets find in polynomial time a ball whose volume is $\exp(\tilde{O}( d/ \log d))$-factor competitive with the volume of the best ball.
  Our main result is an algorithm that finds a confidence set whose volume is $\exp(\tilde{O}(d^{1/2}))$ factor competitive with the optimal ball having the desired coverage. The algorithm is improper (it outputs an ellipsoid). Combined with our computational intractability result for proper learning balls within an $\exp(\tilde{O}(d^{1-o(1)}))$ approximation factor in volume, our results provide an interesting separation between proper and (improper) learning of confidence sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.02723v2</guid>
      <category>cs.DS</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chao Gao, Liren Shan, Vaidehi Srinivas, Aravindan Vijayaraghavan</dc:creator>
    </item>
    <item>
      <title>On the Space Complexity of Online Convolution</title>
      <link>https://arxiv.org/abs/2505.00181</link>
      <description>arXiv:2505.00181v2 Announce Type: replace-cross 
Abstract: We study a discrete convolution streaming problem. An input arrives as a stream of numbers $z = (z_0,z_1,z_2,\ldots)$, and at time $t$ our goal is to output $(Tz)_t$ where $T$ is a lower-triangular Toeplitz matrix. We focus on space complexity; the algorithm can store a buffer of $\beta(t)$ numbers in order to achieve this goal.
  We characterize space complexity when algorithms perform continuous operations. The matrix $T$ corresponds to a generating function $G(x)$. If $G(x)$ is rational of degree $d$, then it is known that the space complexity is at most $O(d)$. We prove a corresponding lower bound; the space complexity is at least $\Omega(d)$. In addition, for irrational $G(x)$, we prove that the space complexity is infinite. We also provide finite-time guarantees. For example, for the generating function $\frac{1}{\sqrt{1-x}}$ that was studied in various previous works in the context of differentially private continual counting, we prove a sharp lower bound on the space complexity; at time $t$, it is at least $\Omega(t)$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00181v2</guid>
      <category>cs.CC</category>
      <category>cs.DS</category>
      <pubDate>Wed, 14 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joel Daniel Andersson, Amir Yehudayoff</dc:creator>
    </item>
  </channel>
</rss>
