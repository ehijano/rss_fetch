<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Dec 2024 03:45:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>An adaptive approximate Bayesian computation MCMC with Global-Local proposals</title>
      <link>https://arxiv.org/abs/2412.15644</link>
      <description>arXiv:2412.15644v1 Announce Type: new 
Abstract: In this paper, we address the challenge of Markov Chain Monte Carlo algorithms within the Approximate Bayesian Computation framework, which often get trapped in local optima due to their inherent local exploration mechanism. We propose a novel Global-Local ABC-MCMC algorithm that combines the "exploration" capabilities of global proposals with the "exploitation" finesse of local proposals. By integrating iterative importance resampling into the likelihood-free framework, we establish an effective global proposal distribution. We select the optimum mixture of global and local moves based on a unit cost version of expected squared jumped distance via sequential optimization. Furthermore, we propose two adaptive schemes: The first involves a normalizing flow-based probabilistic distribution learning model to iteratively improve the proposal for importance sampling, and the second focuses on optimizing the efficiency of the local sampler by utilizing Langevin dynamics and common random numbers. We numerically demonstrate that our method improves sampling efficiency and achieve more reliable convergence for complex posteriors. A software package implementing this method is available at https://github.com/caofff/GL-ABC-MCMC.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15644v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuefei Cao, Shijia Wang, Yongdao Zhou</dc:creator>
    </item>
    <item>
      <title>Lecture Notes on High Dimensional Linear Regression</title>
      <link>https://arxiv.org/abs/2412.15633</link>
      <description>arXiv:2412.15633v1 Announce Type: cross 
Abstract: These lecture notes cover advanced topics in linear regression, with an in-depth exploration of the existence, uniqueness, relations, computation, and non-asymptotic properties of the most prominent estimators in this setting. The covered estimators include least squares, ridgeless, ridge, and lasso. The content follows a proposition-proof structure, making it suitable for students seeking a formal and rigorous understanding of the statistical theory underlying machine learning methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.15633v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alberto Quaini</dc:creator>
    </item>
    <item>
      <title>A Bayesian prevalence-incidence mixture model for screening outcomes with misclassification</title>
      <link>https://arxiv.org/abs/2412.16065</link>
      <description>arXiv:2412.16065v1 Announce Type: cross 
Abstract: We propose BayesPIM, a Bayesian prevalence-incidence mixture model for estimating time- and covariate-dependent disease incidence from screening and surveillance data. The method is particularly suited to settings where some individuals may have the disease at baseline, baseline tests may be missing or incomplete, and the screening test has imperfect sensitivity. Building on the existing PIMixture framework, which assumes perfect sensitivity, BayesPIM accommodates uncertain test accuracy by incorporating informative priors. By including covariates, the model can quantify heterogeneity in disease risk, thereby informing personalized screening strategies. We motivate the model using data from high-risk familial colorectal cancer (CRC) surveillance through colonoscopy, where adenomas - precursors of CRC - may already be present at baseline and remain undetected due to imperfect test sensitivity. We show that conditioning incidence and prevalence estimates on covariates explains substantial heterogeneity in adenoma risk. Using a Metropolis-within-Gibbs sampler and data augmentation, BayesPIM robustly recovers incidence times while handling latent prevalence. Informative priors on the test sensitivity stabilize estimation and mitigate non-convergence issues. Model fit can be assessed using information criteria and validated against a non-parametric estimator. In this way, BayesPIM enhances estimation accuracy and supports the development of more effective, patient-centered screening policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.16065v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Klausch, Birgit I. Lissenberg-Witte, Veerle M. Coup\'e</dc:creator>
    </item>
    <item>
      <title>SHAP zero Explains Genomic Models with Near-zero Marginal Cost for Future Queried Sequences</title>
      <link>https://arxiv.org/abs/2410.19236</link>
      <description>arXiv:2410.19236v2 Announce Type: replace-cross 
Abstract: With the rapid growth of large-scale machine learning models in genomics, Shapley values have emerged as a popular method for model explanations due to their theoretical guarantees. While Shapley values explain model predictions locally for an individual input query sequence, extracting biological knowledge requires global explanation across thousands of input sequences. This demands exponential model evaluations per sequence, resulting in significant computational cost and carbon footprint. Herein, we develop SHAP zero, a method that estimates Shapley values and interactions with a near-zero marginal cost for future queried sequences after paying a one-time fee for model sketching. SHAP zero achieves this by establishing a surprisingly underexplored connection between the Shapley values and interactions and the Fourier transform of the model. Explaining two genomic models, one trained to predict guide RNA binding and the other to predict DNA repair outcome, we demonstrate that SHAP zero achieves orders of magnitude reduction in amortized computational cost compared to state-of-the-art algorithms, revealing almost all predictive motifs -- a finding previously inaccessible due to the combinatorial space of possible interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19236v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Darin Tsui, Aryan Musharaf, Yigit Efe Erginbas, Justin Singh Kang, Amirali Aghazadeh</dc:creator>
    </item>
  </channel>
</rss>
