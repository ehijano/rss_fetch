<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NA updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NA</link>
    <description>cs.NA updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NA" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 02:52:59 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stochastic trace estimation for parameter-dependent matrices applied to spectral density approximation</title>
      <link>https://arxiv.org/abs/2502.18626</link>
      <description>arXiv:2502.18626v1 Announce Type: new 
Abstract: Stochastic trace estimation is a well-established tool for approximating the trace of a large symmetric matrix $\mathbf{B}$. Several applications involve a matrix that depends continuously on a parameter $t \in [a,b]$, and require trace estimates of $\mathbf{B}(t)$ for many values of $t$. This is, for example, the case when approximating the spectral density of a matrix. Approximating the trace separately for each matrix $\mathbf{B}(t_1), \dots, \mathbf{B}(t_m)$ clearly incurs redundancies and a cost that scales linearly with $m$. To address this issue, we propose and analyze modifications for three stochastic trace estimators, the Girard-Hutchinson, Nystr\"om, and Nystr\"om++ estimators. Our modification uses \emph{constant} randomization across different values of $t$, that is, every matrix $\mathbf{B}(t_1), \dots, \mathbf{B}(t_m)$ is multiplied with the \emph{same} set of random vectors. When combined with Chebyshev approximation in $t$, the use of such constant random matrices allows one to reuse matrix-vector products across different values of $t$, leading to significant cost reduction. Our analysis shows that the loss of stochastic independence across different $t$ does not lead to deterioration. In particular, we show that $\mathcal{O}(\varepsilon^{-1})$ random matrix-vector products suffice to ensure an error of $\varepsilon &gt; 0$ for Nystr\"om++, independent of low-rank properties of $\mathbf{B}(t)$. We discuss in detail how the combination of Nystr\"om++ with Chebyshev approximation applies to spectral density estimation and provide an analysis of the resulting method. This improves various aspects of an existing stochastic estimator for spectral density estimation. Several numerical experiments from electronic structure interaction, statistical thermodynamics, and neural network optimization validate our findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18626v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Matti, Haoze He, Daniel Kressner, Hei Yin Lam</dc:creator>
    </item>
    <item>
      <title>An NEPv Approach for Feature Selection via Orthogonal OCCA with the (2,1)-norm Regularization</title>
      <link>https://arxiv.org/abs/2502.18633</link>
      <description>arXiv:2502.18633v1 Announce Type: new 
Abstract: A novel feature selection model via orthogonal canonical correlation analysis with the $(2,1)$-norm regularization is proposed, and the model is solved by a practical NEPv approach (nonlinear eigenvalue problem with eigenvector dependency), yielding a feature selection method named OCCA-FS. It is proved that OCCA-FS always produces a sequence of approximations with monotonic objective values and is globally convergent. Extensive numerical experiments are performed to compare OCCA-FS against existing feature selection methods. The numerical results demonstrate that OCCA-FS produces superior classification performance and often comes out on the top among all feature selection methods in comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18633v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Li Wang, Lei-Hong Zhang, Ren-Cang Li</dc:creator>
    </item>
    <item>
      <title>Convergence of random splitting method for the Allen-Cahn equation in a background flow</title>
      <link>https://arxiv.org/abs/2502.18849</link>
      <description>arXiv:2502.18849v1 Announce Type: new 
Abstract: We study in this paper the convergence of the random splitting method for Allen-Cahn equation in a background flow that plays as a simplified model for phase separation in multiphase flows. The model does not own the gradient flow structure as the usual Allen-Cahn equation does, and the random splitting method is advantageous due to its simplicity and better convergence rate. Though the random splitting is a classical method, the analysis of the convergence is not straightforward for this model due to the nonlinearity and unboundedness of the operators. We obtain uniform estimates of various Sobolev norms of the numerical solutions and the stability of the model. Based on the Sobolev estimates, the local trunction errors are then rigorously obtained. We then prove that the random operator splitting has an expected single run error with order $1.5$ and a bias with order $2$. Numerical experiments are then performed to confirm our theoretic findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18849v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lei Li, Chen Wang</dc:creator>
    </item>
    <item>
      <title>Formulation and Analysis of Blended Atomistic to Higher-Order Continuum Coupling Methods for Crystalline Defects</title>
      <link>https://arxiv.org/abs/2502.18854</link>
      <description>arXiv:2502.18854v1 Announce Type: new 
Abstract: Concurrent multiscale methods play an important role in modeling and simulating materials with defects, aiming to achieve the balance between accuracy and efficiency. Atomistic-to-continuum (a/c) coupling methods, a typical class of concurrent multiscale methods, link atomic-scale simulations with continuum mechanics. Existing a/c methods adopt the classic second-order Cauchy-Born approximation as the continuum mechanics model. In this work, we employ a higher-order Cauchy-Born model to study the potential accuracy improvement of the coupling scheme. In particular, we develop an energy-based blended atomistic to higher-order continuum method and present a rigorous a priori error analysis. We show that the overall accuracy of the energy-based blended method is not actually improved due the coupling interface error which is of lower order and may not be improved. On the contrast, higher order accuracy is achieved by the force-based blended atomistic to higher-order continuum method. Our theoretical results are demonstrated by a detailed numerical study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18854v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Junfeng Lu, Hao Wang, Yangshuai Wang</dc:creator>
    </item>
    <item>
      <title>Application of quasi-Monte Carlo in Mine Countermeasure Simulations with a Stochastic Optimal Control Framework</title>
      <link>https://arxiv.org/abs/2502.18997</link>
      <description>arXiv:2502.18997v1 Announce Type: new 
Abstract: Modelling and simulating mine countermeasures search missions performed by autonomous vehicles equipped with a sensor capable of detecting mines at sea is a challenging endeavour. The output of our stochastic optimal control implementation consists of an optimal trajectory in a square domain for the autonomous vehicle such that the total mission time is minimized for a given residual risk of not detecting sea mines. We model this risk as an expected value integral. We found that upon completion of the simulation, the user requested residual risk is usually not satisfied. We solved this by implementing a relaxation strategy which consists of incrementally increasing the square search domain. We then combined this strategy with different quasi-Monte Carlo schemes used for solving the integral. We found that using a Rank-1 Lattice scheme yields a speedup up to a factor two with respect to the Monte Carlo scheme. We also present an implementation which allows us to compute a trajectory in a convex quadrilateral domain, as opposed to a square domain, and combine it with our relaxation strategy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18997v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Philippe Blondeel, Filip Van Utterbeeck, Ben Lauwens</dc:creator>
    </item>
    <item>
      <title>On conservative, stable boundary and coupling conditions for diffusion equations I -- The conservation property for explicit schemes</title>
      <link>https://arxiv.org/abs/2502.19003</link>
      <description>arXiv:2502.19003v1 Announce Type: new 
Abstract: This paper introduces improved numerical techniques for addressing numerical boundary and interface coupling conditions in the context of diffusion equations in cellular biophysics or heat conduction problems in fluid-structure interactions. Our primary focus is on two critical numerical aspects related to coupling conditions: the preservation of the conservation property and ensuring stability. Notably, a key oversight in some existing literature on coupling methods is the neglect of upholding the conservation property within the overall scheme. This oversight forms the central theme of the initial part of our research. As a first step, we limited ourselves to explicit schemes on uniform grids. Implicit schemes and the consideration of varying mesh sizes at the interface will be reserved for a subsequent paper \cite{CMW3}. Another paper \cite{CMW2} will address the issue of stability.
  We examine these schemes from the perspective of finite differences, including finite elements, following the application of a nodal quadrature rule. Additionally, we explore a finite volume-based scheme involving cells and flux considerations. Our analysis reveals that discrete boundary and flux coupling conditions uphold the conservation property in distinct ways in nodal-based and cell-based schemes. The coupling conditions under investigation encompass well-known approaches such as Dirichlet-Neumann coupling, heat flux coupling, and specific channel and pumping flux conditions drawn from the field of biophysics. The theoretical findings pertaining to the conservation property are corroborated through computations across a range of test cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19003v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taj Munir, Nagaiah Chamakuri, Gerald Warnecke</dc:creator>
    </item>
    <item>
      <title>A pressure- and Reynolds-semi-robust space-time DG method for the incompressible Navier-Stokes equations</title>
      <link>https://arxiv.org/abs/2502.19035</link>
      <description>arXiv:2502.19035v1 Announce Type: new 
Abstract: We carry out a stability and convergence analysis of a fully discrete scheme for the time-dependent Navier-Stokes equations resulting from combining an $H(\mathrm{div}, \Omega)$-conforming discontinuous Galerkin spatial discretization, and a discontinuous Galerkin time stepping scheme. Such a scheme is proven to be pressure robust and Reynolds semi-robust. Standard techniques can be used to analyze only the case of lowest-order approximations in time. Therefore, we use some nonstandard test functions to prove existence of discrete solutions, unconditional stability, and quasi-optimal convergence rates for any degree of approximation in time. In particular, a continuous dependence of the discrete solution on the data of the problem, and quasi-optimal convergence rates for low and high Reynolds numbers are proven in an energy norm including the term $L^{\infty}(0, T; L^2(\Omega)^d)$ for the velocity. Some numerical experiments validating our theoretical results are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19035v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L. Beir\~ao da Veiga, F. Dassi, S. G\'omez</dc:creator>
    </item>
    <item>
      <title>Geometric Ergodicity and Optimal Error Estimates for a Class of Novel Tamed Schemes to Super-linear Stochastic PDEs</title>
      <link>https://arxiv.org/abs/2502.19117</link>
      <description>arXiv:2502.19117v1 Announce Type: new 
Abstract: We construct a class of novel tamed schemes that can preserve the original Lyapunov functional for super-linear stochastic PDEs (SPDEs), including the stochastic Allen--Cahn equation, driven by multiplicative or additive noise, and provide a rigorous analysis of their long-time unconditional stability. We also show that the corresponding Galerkin-based fully discrete tamed schemes inherit the geometric ergodicity of the SPDEs and establish their convergence towards the SPDEs with optimal strong rates in both the multiplicative and additive noise cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19117v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhihui Liu, Jie Shen</dc:creator>
    </item>
    <item>
      <title>Reduced order models for time-dependent problems using the Laplace transform</title>
      <link>https://arxiv.org/abs/2502.19120</link>
      <description>arXiv:2502.19120v1 Announce Type: new 
Abstract: We propose a reduced basis method to solve time-dependent partial differential equations based on the Laplace transform. Unlike traditional approaches, we start by applying said transform to the evolution problem, yielding a time-independent boundary value problem that depends on the complex Laplace parameter. First, in an offline stage, we appropriately sample the Laplace parameter and solve the collection of problems using the finite element method. Next, we apply a Proper Orthogonal Decomposition (POD) to this collection of solutions in order to obtain a reduced basis that is of dimension much smaller than that of the original solution space. This reduced basis, in turn, is then used to solve the evolution problem using any suitable time-stepping method. A key insight to justify the formulation of the method resorts to Hardy spaces of analytic functions. By applying the widely-known Paley-Wiener theorem we can then define an isometry between the solution of the time-dependent problem and its Laplace transform. As a consequence of this result, one may conclude that computing a POD with samples taken in the Laplace domain produces an exponentially accurate reduced basis for the time-dependent problem. Numerical experiments characterizing the performance of the method, in terms of accuracy and speed-up, are included for a variety of relevant time-evolution equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19120v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ricardo Reyes</dc:creator>
    </item>
    <item>
      <title>JS-type and Z-type weights for fourth-order central-upwind weighted essentially non-oscillatory schemes</title>
      <link>https://arxiv.org/abs/2502.19221</link>
      <description>arXiv:2502.19221v2 Announce Type: new 
Abstract: The central-upwind weighted essentially non-oscillatory (WENO) scheme introduces the downwind substencil to reconstruct the numerical flux, where the smoothness indicator for the downwind substencil is of critical importance in maintaining high order in smooth regions and preserving the essentially nonoscillatory behavior in shock capturing. In this study, we design the smoothness indicator for the downwind substencil by simply summing up all local smoothness indicators and taking the average, which includes the regularity information of the whole stencil. Accordingly the JS-type and Z-type nonlinear weights, based on simple local smoothness indicators, are developed for the fourth-order central-upwind WENO scheme. The accuracy, robustness, and high-resolution properties of our proposed schemes are demonstrated in a variety of one- and two-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19221v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiaxi Gu, Xinjuan Chen, Kwanghyuk Park, Jae-Hun Jung</dc:creator>
    </item>
    <item>
      <title>Second order in time finite element schemes for curve shortening flow and curve diffusion</title>
      <link>https://arxiv.org/abs/2502.19277</link>
      <description>arXiv:2502.19277v1 Announce Type: new 
Abstract: We prove optimal error bounds for a second order in time finite element approximation of curve shortening flow in possibly higher codimension. In addition, we introduce a second order in time method for curve diffusion. Both schemes are based on variational formulations of strictly parabolic systems of partial differential equations. In each time step only two linear systems need to be solved. Numerical experiments demonstrate second order convergence as well as asymptotic equidistribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19277v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Klaus Deckelnick, Robert N\"urnberg</dc:creator>
    </item>
    <item>
      <title>PhysicsSolver: Transformer-Enhanced Physics-Informed Neural Networks for Forward and Forecasting Problems in Partial Differential Equations</title>
      <link>https://arxiv.org/abs/2502.19290</link>
      <description>arXiv:2502.19290v1 Announce Type: new 
Abstract: Time-dependent partial differential equations are a significant class of equations that describe the evolution of various physical phenomena over time. One of the open problems in scientific computing is predicting the behaviour of the solution outside the given temporal region. Most traditional numerical methods are applied to a given time-space region and can only accurately approximate the solution of the given region. To address this problem, many deep learning-based methods, basically data-driven and data-free approaches, have been developed to solve these problems. However, most data-driven methods require a large amount of data, which consumes significant computational resources and fails to utilize all the necessary information embedded underlying the partial differential equations (PDEs). Moreover, data-free approaches such as Physics-Informed Neural Networks (PINNs) may not be that ideal in practice, as traditional PINNs, which primarily rely on multilayer perceptrons (MLPs) and convolutional neural networks (CNNs), tend to overlook the crucial temporal dependencies inherent in real-world physical systems. We propose a method denoted as \textbf{PhysicsSolver} that merges the strengths of two approaches: data-free methods that can learn the intrinsic properties of physical systems without using data, and data-driven methods, which are effective at making predictions. Extensive numerical experiments have demonstrated the efficiency and robustness of our proposed method. We provide the code at \href{https://github.com/PhysicsSolver/PhysicsSolver}{https://github.com/PhysicsSolver}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19290v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenyi Zhu, Yuchen Huang, Liu Liu</dc:creator>
    </item>
    <item>
      <title>Error estimates for viscous Burgers' equation using deep learning method</title>
      <link>https://arxiv.org/abs/2502.19392</link>
      <description>arXiv:2502.19392v1 Announce Type: new 
Abstract: The articles focuses on error estimates as well as stability analysis of deep learning methods for stationary and non-stationary viscous Burgers equation in two and three dimensions. The local well-posedness of homogeneous boundary value problem for non-stationary viscous Burgers equation is established by using semigroup techniques and fixed point arguments. By considering a suitable approximate problem and deriving appropriate energy estimates, we prove the existence of a unique strong solution. Additionally, we extend our analysis to the global well-posedness of the non-homogeneous problem. For both the stationary and non-stationary cases, we derive explicit error estimates in suitable Lebesgue and Sobolev norms by optimizing a loss function in a Deep Neural Network approximation of the solution with fixed complexity. Finally, numerical results on prototype systems are presented to illustrate the derived error estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19392v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wasim Akram, Sagar Gautam, Deepanshu Verma, Manil T. Mohan</dc:creator>
    </item>
    <item>
      <title>Multigrid methods for total variation</title>
      <link>https://arxiv.org/abs/2502.18659</link>
      <description>arXiv:2502.18659v1 Announce Type: cross 
Abstract: Based on a nonsmooth coherence condition, we construct and prove the convergence of a forward-backward splitting method that alternates between steps on a fine and a coarse grid. Our focus is a total variation regularised inverse imaging problems, specifically, their dual problems, for which we develop in detail the relevant coarse-grid problems. We demonstrate the performance of our method on total variation denoising and magnetic resonance imaging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18659v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>eess.IV</category>
      <category>math.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felipe Guerra, Tuomo Valkonen</dc:creator>
    </item>
    <item>
      <title>Algorithmic approaches to avoiding bad local minima in nonconvex inconsistent feasibility</title>
      <link>https://arxiv.org/abs/2502.19052</link>
      <description>arXiv:2502.19052v1 Announce Type: cross 
Abstract: The main challenge of nonconvex optimization is to find a global optimum, or at least to avoid ``bad'' local minima and meaningless stationary points. We study here the extent to which algorithms, as opposed to optimization models and regularization, can be tuned to accomplish this goal. The model we consider is a nonconvex, inconsistent feasibility problem with many local minima, where these are points at which the gaps between the sets are smallest on neighborhoods of these points. The algorithms that we compare are all projection-based algorithms, specifically cyclic projections, the cyclic relaxed Douglas-Rachford algorithm, and relaxed Douglas-Rachford splitting on the product space. The local convergence and fixed points of these algorithms have already been characterized in pervious theoretical studies. We demonstrate the theory for these algorithms in the context of orbital tomographic imaging from angle-resolved photon emission spectroscopy (ARPES) measurements, both synthetically generated and experimental. Our results show that, while the cyclic projections and cyclic relaxed Douglas-Rachford algorithms generally converge the fastest, the method of relaxed Douglas-Rachford splitting on the product space does move away from bad local minima of the other two algorithms, settling eventually on clusters of local minima corresponding to globally optimal critical points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19052v1</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thi Lan Dinh, Wiebke Bennecke, G. S. Matthijs Jansen, D. Russell Luke, Stefan Mathias</dc:creator>
    </item>
    <item>
      <title>A Nonlinear Extension of the Variable Projection (VarPro) Method for NURBS-based Conformal Surface Flattening</title>
      <link>https://arxiv.org/abs/2502.19088</link>
      <description>arXiv:2502.19088v1 Announce Type: cross 
Abstract: In the field of computer graphics, conformal surface flattening has been widely studied for tasks such as texture mapping, geometry processing, and mesh generation. Typically, existing methods aim to flatten a given input geometry while preserving conformality as much as possible, meaning the result is only as conformal as possible. By contrast, this study focuses on surfaces that can be flattened conformally without singularities, making the process a coupled problem: the input (or target) surface must be recursively refined while its flattening is computed.
  Although the uniformization theorem or the Riemann mapping theorem guarantees the existence of a conformal flattening for any simply connected, orientable surface, those theorems permit singularities in the flattening. If singularities are not allowed, only a special class of surfaces can be conformally flattened-though many practical surfaces do fall into this class.
  To address this, we develop a NURBS-based approach in which both the input surface and its flattening are refined in tandem, ensuring mutual conformality. Because NURBS surfaces cannot represent singularities, the resulting pair of surfaces is naturally singularity-free. Our work is inspired by the form-finding method by [Miki and Mitchell 2022, 2024], which solves bilinear PDEs by iteratively refining two surfaces together. Building on their demonstration of the effectiveness of variable projection (VarPro), we adopt a similar strategy: VarPro alternates between a linear projection and a nonlinear iteration, leveraging a partially linear (separable) problem structure. However, since our conformal condition separates into two nonlinear subproblems, we introduce a nonlinear extension of VarPro. Although this significantly increases computational cost, the quality of the results is noteworthy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19088v1</guid>
      <category>cs.CG</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Masaaki Miki</dc:creator>
    </item>
    <item>
      <title>Chebyshev HOPGD with sparse grid sampling for parameterized linear systems</title>
      <link>https://arxiv.org/abs/2309.14178</link>
      <description>arXiv:2309.14178v3 Announce Type: replace 
Abstract: We consider approximating solutions to parameterized linear systems of the form $A(\mu_1,\mu_2) x(\mu_1,\mu_2) = b$. Here the matrix $A(\mu_1,\mu_2) \in \mathbb{R}^{n \times n}$ is nonsingular, large, and sparse and depends nonlinearly on the parameters. Specifically, the system arises from a discretization of a partial differential equation and $x(\mu_1,\mu_2) \in \mathbb{R}^n$, $b \in \mathbb{R}^n$. The treatment of linear systems with nonlinear dependence on a single parameter has been well-studied, and robust methods combining companion linearization, Krylov subspace methods, and Chebyshev interpolation have enabled fast solution for multiple parameter values at the cost of a single iteration.
  Solution of systems depending nonlinearly on multiple parameters is more challenging. This work overcomes those additional challenges by combining companion linearization, the Krylov subspace method preconditioned bi-conjugate gradient (BiCG), and a decomposition of a tensor matrix of precomputed solutions, called snapshots. This produces a reduced order model of $x(\mu_1,\mu_2)$, and this model can be evaluated inexpensively for many values of the parameters. An interpolation of the model is used to produce approximations on the entire parameter space. In addition this method can be used to solve a parameter estimation problem.
  This approach allows us to achieve similar computational savings as for the one-parameter case; we can solve for many parameter pairs at the cost of many fewer applications of an efficient iterative method. The technique is presented for dependence on two parameters, but the strategy can be extended to more parameters using the same approach. Numerical examples of a parameterized Helmholtz equation show the competitiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.14178v3</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siobh\'an Correnty, Melina A. Freitag, Kirk M. Soodhalter</dc:creator>
    </item>
    <item>
      <title>Integrating Additive Multigrid with Multipreconditioned Conjugate Gradient Method</title>
      <link>https://arxiv.org/abs/2402.12833</link>
      <description>arXiv:2402.12833v2 Announce Type: replace 
Abstract: Due to its optimal complexity, the multigrid (MG) method is one of the most popular approaches for solving large-scale linear systems arising from the discretization of partial differential equations. However, the parallel implementation of standard MG methods, which are inherently multiplicative, suffers from increasing communication complexity. In such cases, the additive variants of MG methods provide a good alternative due to their inherently parallel nature, although they exhibit slower convergence. This work combines the additive multigrid method with the multipreconditioned conjugate gradient (MPCG) method. In the proposed approach, the MPCG method employs the corrections from the different levels of the MG hierarchy as separate preconditioned search directions. In this approach, the MPCG method updates the current iterate by using the linear combination of the preconditioned search directions, where the optimal coefficients for the linear combination are computed by exploiting the energy norm minimization of the CG method. The idea behind our approach is to combine the $A$-conjugacy of the search directions of the MPCG method and the quasi $H_1$-orthogonality of the corrections from the MG hierarchy. In the numerical section, we study the performance of the proposed method compared to the standard additive and multiplicative MG methods used as preconditioners for the CG method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12833v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hardik Kothari, Maria Giuseppina Chiara Nestola, Marco Favino, Rolf Krause</dc:creator>
    </item>
    <item>
      <title>Backward errors for multiple eigenpairs in structured and unstructured nonlinear eigenvalue problems</title>
      <link>https://arxiv.org/abs/2405.06327</link>
      <description>arXiv:2405.06327v2 Announce Type: replace 
Abstract: Given a nonlinear matrix-valued function $F(\lambda)$ and approximate eigenpairs $(\lambda_i, v_i)$, we discuss how to determine the smallest perturbation $\delta F$ such that $[F + \delta F](\lambda_i) v_i = 0$; we call the distance between the $F$ and $F + \delta F$ the backward error for this set of approximate eigenpairs. We focus on the case where $F(\lambda)$ is given as a linear combination of scalar functions multiplying matrix coefficients $F_i$, and the perturbation is done on the matrix coefficients. We provide inexpensive upper bounds, and a way to accurately compute the backward error by means of direct computations or through Riemannian optimization. We also discuss how the backward error can be determined when the $F_i$ have particular structures (such as symmetry, sparsity, or low-rank), and the perturbations are required to preserve them. For special cases (such as for symmetric coefficients), explicit and inexpensive formulas to compute the $\delta F_i$ are also given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06327v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Miryam Gnazzo, Leonardo Robol</dc:creator>
    </item>
    <item>
      <title>Low-rank approximation of parameter-dependent matrices via CUR decomposition</title>
      <link>https://arxiv.org/abs/2408.05595</link>
      <description>arXiv:2408.05595v2 Announce Type: replace 
Abstract: A low-rank approximation of a parameter-dependent matrix $A(t)$ is an important task in the computational sciences appearing for example in dynamical systems and compression of a series of images. In this work, we introduce AdaCUR, an efficient algorithm for computing a low-rank approximation of parameter-dependent matrices via CUR decompositions. The key idea for this algorithm is that for nearby parameter values, the column and row indices for the CUR decomposition can often be reused. AdaCUR is rank-adaptive, provides error control, and has complexity that compares favorably against existing methods. A faster algorithm which we call FastAdaCUR that prioritizes speed over accuracy is also given, which is rank-adaptive and has complexity which is at most linear in the number of rows or columns, but without error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05595v2</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taejun Park, Yuji Nakatsukasa</dc:creator>
    </item>
    <item>
      <title>Mathematical Introduction to Deep Learning: Methods, Implementations, and Theory</title>
      <link>https://arxiv.org/abs/2310.20360</link>
      <description>arXiv:2310.20360v2 Announce Type: replace-cross 
Abstract: This book aims to provide an introduction to the topic of deep learning algorithms. We review essential components of deep learning algorithms in full mathematical detail including different artificial neural network (ANN) architectures (such as fully-connected feedforward ANNs, convolutional ANNs, recurrent ANNs, residual ANNs, and ANNs with batch normalization) and different optimization algorithms (such as the basic stochastic gradient descent (SGD) method, accelerated methods, and adaptive methods). We also cover several theoretical aspects of deep learning algorithms such as approximation capacities of ANNs (including a calculus for ANNs), optimization theory (including Kurdyka-{\L}ojasiewicz inequalities), and generalization errors. In the last part of the book some deep learning approximation methods for PDEs are reviewed including physics-informed neural networks (PINNs) and deep Galerkin methods. We hope that this book will be useful for students and scientists who do not yet have any background in deep learning at all and would like to gain a solid foundation as well as for practitioners who would like to obtain a firmer mathematical understanding of the objects and methods considered in deep learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.20360v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arnulf Jentzen, Benno Kuckuck, Philippe von Wurstemberger</dc:creator>
    </item>
    <item>
      <title>Distributed Stochastic Optimization of a Neural Representation Network for Time-Space Tomography Reconstruction</title>
      <link>https://arxiv.org/abs/2404.19075</link>
      <description>arXiv:2404.19075v2 Announce Type: replace-cross 
Abstract: 4D time-space reconstruction of dynamic events or deforming objects using X-ray computed tomography (CT) is an important inverse problem in non-destructive evaluation. Conventional back-projection based reconstruction methods assume that the object remains static for the duration of several tens or hundreds of X-ray projection measurement images (reconstruction of consecutive limited-angle CT scans). However, this is an unrealistic assumption for many in-situ experiments that causes spurious artifacts and inaccurate morphological reconstructions of the object. To solve this problem, we propose to perform a 4D time-space reconstruction using a distributed implicit neural representation (DINR) network that is trained using a novel distributed stochastic training algorithm. Our DINR network learns to reconstruct the object at its output by iterative optimization of its network parameters such that the measured projection images best match the output of the CT forward measurement model. We use a forward measurement model that is a function of the DINR outputs at a sparsely sampled set of continuous valued 4D object coordinates. Unlike previous neural representation architectures that forward and back propagate through dense voxel grids that sample the object's entire time-space coordinates, we only propagate through the DINR at a small subset of object coordinates in each iteration resulting in an order-of-magnitude reduction in memory and compute for training. DINR leverages distributed computation across several compute nodes and GPUs to produce high-fidelity 4D time-space reconstructions. We use both simulated parallel-beam and experimental cone-beam X-ray CT datasets to demonstrate the superior performance of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19075v2</guid>
      <category>eess.IV</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>K. Aditya Mohan, Massimiliano Ferrucci, Chuck Divin, Garrett A. Stevenson, Hyojin Kim</dc:creator>
    </item>
    <item>
      <title>Parametric Sensitivity Analysis for Models of Reaction Networks within Interacting Compartments</title>
      <link>https://arxiv.org/abs/2408.09208</link>
      <description>arXiv:2408.09208v2 Announce Type: replace-cross 
Abstract: Models of reaction networks within interacting compartments (RNIC) are a generalization of stochastic reaction networks. It is most natural to think of the interacting compartments as "cells" that can appear, degrade, split, and even merge, with each cell containing an evolving copy of the underlying stochastic reaction network. Such models have a number of parameters, including those associated with the internal chemical model and those associated with the compartment interactions, and it is natural to want efficient computational methods for the numerical estimation of sensitivities of model statistics with respect to these parameters. Motivated by the extensive work on computational methods for parametric sensitivity analysis in the context of stochastic reaction networks over the past few decades, we provide a number of methods in the basic RNIC setting. Provided methods include the (unbiased) Girsanov transformation method (also called the Likelihood Ratio method) and a number of coupling methods for the implementation of finite differences, each motivated by methods from previous work related to stochastic reaction networks. We provide several numerical examples comparing the various methods in the new setting. We find that the relative performance of each method is in line with its analog in the "standard" stochastic reaction network setting. We have made all of the Matlab code used to implement the various methods freely available for download.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09208v2</guid>
      <category>q-bio.MN</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <category>q-bio.QM</category>
      <pubDate>Thu, 27 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David F. Anderson, Aidan S. Howells</dc:creator>
    </item>
  </channel>
</rss>
