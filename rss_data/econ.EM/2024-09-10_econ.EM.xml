<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 11 Sep 2024 01:47:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Lee Bounds with Multilayered Sample Selection</title>
      <link>https://arxiv.org/abs/2409.04589</link>
      <description>arXiv:2409.04589v1 Announce Type: new 
Abstract: This paper investigates the causal effect of job training on wage rates in the presence of firm heterogeneity. When training affects worker sorting to firms, sample selection is no longer binary but is "multilayered". This paper extends the canonical Heckman (1979) sample selection model - which assumes selection is binary - to a setting where it is multilayered, and shows that in this setting Lee bounds set identifies a total effect that combines a weighted-average of the causal effect of job training on wage rates across firms with a weighted-average of the contrast in wages between different firms for a fixed level of training. Thus, Lee bounds set identifies a policy-relevant estimand only when firms pay homogeneous wages and/or when job training does not affect worker sorting across firms. We derive sharp closed-form bounds for the causal effect of job training on wage rates at each firm which leverage information on firm-specific wages. We illustrate our partial identification approach with an empirical application to the Job Corps Study. Results show that while conventional Lee bounds are strictly positive, our within-firm bounds include 0 showing that canonical Lee bounds may be capturing a pure sorting effect of job training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04589v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kory Kroft, Ismael Mourifi\'e, Atom Vayalinkal</dc:creator>
    </item>
    <item>
      <title>Improving the Finite Sample Performance of Double/Debiased Machine Learning with Propensity Score Calibration</title>
      <link>https://arxiv.org/abs/2409.04874</link>
      <description>arXiv:2409.04874v1 Announce Type: new 
Abstract: Machine learning techniques are widely used for estimating causal effects. Double/debiased machine learning (DML) (Chernozhukov et al., 2018) uses a double-robust score function that relies on the prediction of nuisance functions, such as the propensity score, which is the probability of treatment assignment conditional on covariates. Estimators relying on double-robust score functions are highly sensitive to errors in propensity score predictions. Machine learners increase the severity of this problem as they tend to over- or underestimate these probabilities. Several calibration approaches have been proposed to improve probabilistic forecasts of machine learners. This paper investigates the use of probability calibration approaches within the DML framework. Simulation results demonstrate that calibrating propensity scores may significantly reduces the root mean squared error of DML estimates of the average treatment effect in finite samples. We showcase it in an empirical example and provide conditions under which calibration does not alter the asymptotic properties of the DML estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04874v1</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniele Ballinari, Nora Bearth</dc:creator>
    </item>
    <item>
      <title>DEPLOYERS: An agent based modeling tool for multi country real world data</title>
      <link>https://arxiv.org/abs/2409.04876</link>
      <description>arXiv:2409.04876v1 Announce Type: new 
Abstract: We present recent progress in the design and development of DEPLOYERS, an agent-based macroeconomics modeling (ABM) framework, capable to deploy and simulate a full economic system (individual workers, goods and services firms, government, central and private banks, financial market, external sectors) whose structure and activity analysis reproduce the desired calibration data, that can be, for example a Social Accounting Matrix (SAM) or a Supply-Use Table (SUT) or an Input-Output Table (IOT).Here we extend our previous work to a multi-country version and show an example using data from a 46-countries 64-sectors FIGARO Inter-Country IOT. The simulation of each country runs on a separate thread or CPU core to simulate the activity of one step (month, week, or day) and then interacts (updates imports, exports, transfer) with that country's foreign partners, and proceeds to the next step. This interaction can be chosen to be aggregated (a single row and column IO account) or disaggregated (64 rows and columns) with each partner. A typical run simulates thousands of individuals and firms engaged in their monthly activity and then records the results, much like a survey of the country's economic system. This data can then be subjected to, for example, an Input-Output analysis to find out the sources of observed stylized effects as a function of time in the detailed and realistic modeling environment that can be easily implemented in an ABM framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04876v1</guid>
      <category>econ.EM</category>
      <category>q-fin.CP</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Jaraiz, Ruth Pinacho</dc:creator>
    </item>
    <item>
      <title>Difference-in-Differences with Multiple Events</title>
      <link>https://arxiv.org/abs/2409.05184</link>
      <description>arXiv:2409.05184v1 Announce Type: new 
Abstract: Confounding events with correlated timing violate the parallel trends assumption in Difference-in-Differences (DiD) designs. I show that the standard staggered DiD estimator is biased in the presence of confounding events. Identification can be achieved with units not yet treated by either event as controls and a double DiD design using variation in treatment timing. I apply this method to examine the effect of states' staggered minimum wage raise on teen employment from 2010 to 2020. The Medicaid expansion under the ACA confounded the raises, leading to a spurious negative estimate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05184v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin-Tung Tsai</dc:creator>
    </item>
    <item>
      <title>The Surprising Robustness of Partial Least Squares</title>
      <link>https://arxiv.org/abs/2409.05713</link>
      <description>arXiv:2409.05713v1 Announce Type: new 
Abstract: Partial least squares (PLS) is a simple factorisation method that works well with high dimensional problems in which the number of observations is limited given the number of independent variables. In this article, we show that PLS can perform better than ordinary least squares (OLS), least absolute shrinkage and selection operator (LASSO) and ridge regression in forecasting quarterly gross domestic product (GDP) growth, covering the period from 2000 to 2023. In fact, through dimension reduction, PLS proved to be effective in lowering the out-of-sample forecasting error, specially since 2020. For the period 2000-2019, the four methods produce similar results, suggesting that PLS is a valid regularisation technique like LASSO or ridge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05713v1</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao B. Assun\c{c}\~ao, Pedro Afonso Fernandes</dc:creator>
    </item>
    <item>
      <title>Bellwether Trades: Characteristics of Trades influential in Predicting Future Price Movements in Markets</title>
      <link>https://arxiv.org/abs/2409.05192</link>
      <description>arXiv:2409.05192v1 Announce Type: cross 
Abstract: In this study, we leverage powerful non-linear machine learning methods to identify the characteristics of trades that contain valuable information. First, we demonstrate the effectiveness of our optimized neural network predictor in accurately predicting future market movements. Then, we utilize the information from this successful neural network predictor to pinpoint the individual trades within each data point (trading window) that had the most impact on the optimized neural network's prediction of future price movements. This approach helps us uncover important insights about the heterogeneity in information content provided by trades of different sizes, venues, trading contexts, and over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05192v1</guid>
      <category>q-fin.TR</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Tejas Ramdas, Martin T. Wells</dc:creator>
    </item>
    <item>
      <title>Uniform Estimation and Inference for Nonparametric Partitioning-Based M-Estimators</title>
      <link>https://arxiv.org/abs/2409.05715</link>
      <description>arXiv:2409.05715v1 Announce Type: cross 
Abstract: This paper presents uniform estimation and inference theory for a large class of nonparametric partitioning-based M-estimators. The main theoretical results include: (i) uniform consistency for convex and non-convex objective functions; (ii) optimal uniform Bahadur representations; (iii) optimal uniform (and mean square) convergence rates; (iv) valid strong approximations and feasible uniform inference methods; and (v) extensions to functional transformations of underlying estimators. Uniformity is established over both the evaluation point of the nonparametric functional parameter and a Euclidean parameter indexing the class of loss functions. The results also account explicitly for the smoothness degree of the loss function (if any), and allow for a possibly non-identity (inverse) link function. We illustrate the main theoretical and methodological results with four substantive applications: quantile regression, distribution regression, $L_p$ regression, and Logistic regression; many other possibly non-smooth, nonlinear, generalized, robust M-estimation settings are covered by our theoretical results. We provide detailed comparisons with the existing literature and demonstrate substantive improvements: we achieve the best (in some cases optimal) known results under improved (in some cases minimal) requirements in terms of regularity conditions and side rate restrictions. The supplemental appendix reports other technical results that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05715v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Yingjie Feng, Boris Shigida</dc:creator>
    </item>
    <item>
      <title>Enhancing Preference-based Linear Bandits via Human Response Time</title>
      <link>https://arxiv.org/abs/2409.05798</link>
      <description>arXiv:2409.05798v1 Announce Type: cross 
Abstract: Binary human choice feedback is widely used in interactive preference learning for its simplicity, but it provides limited information about preference strength. To overcome this limitation, we leverage human response times, which inversely correlate with preference strength, as complementary information. Our work integrates the EZ-diffusion model, which jointly models human choices and response times, into preference-based linear bandits. We introduce a computationally efficient utility estimator that reformulates the utility estimation problem using both choices and response times as a linear regression problem. Theoretical and empirical comparisons with traditional choice-only estimators reveal that for queries with strong preferences ("easy" queries), choices alone provide limited information, while response times offer valuable complementary information about preference strength. As a result, incorporating response times makes easy queries more useful. We demonstrate this advantage in the fixed-budget best-arm identification problem, with simulations based on three real-world datasets, consistently showing accelerated learning when response times are incorporated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.05798v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shen Li, Yuyang Zhang, Zhaolin Ren, Claire Liang, Na Li, Julie A. Shah</dc:creator>
    </item>
    <item>
      <title>Cross-Sectional Dynamics Under Network Structure: Theory and Macroeconomic Applications</title>
      <link>https://arxiv.org/abs/2211.13610</link>
      <description>arXiv:2211.13610v4 Announce Type: replace 
Abstract: Many environments in economics feature a cross-section of units linked by bilateral ties. I develop a framework for studying dynamics of cross-sectional variables that exploits this network structure. The Network-VAR (NVAR) is a vector autoregression in which innovations transmit cross-sectionally via bilateral links and which can accommodate rich patterns of how network effects of higher order accumulate over time. It can be used to estimate dynamic network effects, with the network given or inferred from dynamic cross-correlations in the data. It also offers a dimensionality-reduction technique for modeling high-dimensional (cross-sectional) processes, owing to networks' ability to summarize complex relations among variables (units) by relatively few bilateral links. In a first application, consistent with an RBC economy with lagged input-output conversion, I estimate how sectoral productivity shocks transmit along supply chains and affect sectoral prices in the US economy. In a second application, I forecast monthly industrial production growth across 44 countries by assuming and estimating a network underlying the dynamics. This reduces out-of-sample mean squared errors by up to 23% relative to a factor model, consistent with an equivalence result I derive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13610v4</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marko Mlikota</dc:creator>
    </item>
    <item>
      <title>Double Machine Learning for Static Panel Models with Fixed Effects</title>
      <link>https://arxiv.org/abs/2312.08174</link>
      <description>arXiv:2312.08174v4 Announce Type: replace 
Abstract: Recent advances in causal inference have seen the development of methods which make use of the predictive power of machine learning algorithms. In this paper, we use these algorithms to approximate high-dimensional and non-linear nuisance functions of the confounders and double machine learning (DML) to make inferences about the effects of policy interventions from panel data. We propose new estimators by extending correlated random effects, within-group and first-difference estimation for linear models to an extension of Robinson (1988)'s partially linear regression model to static panel data models with individual fixed effects and unspecified non-linear confounding effects. We provide an illustrative example of DML for observational panel data showing the impact of the introduction of the minimum wage on voting behaviour in the UK.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08174v4</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Paul Clarke, Annalivia Polselli</dc:creator>
    </item>
    <item>
      <title>When is IV identification agnostic about outcomes?</title>
      <link>https://arxiv.org/abs/2406.02835</link>
      <description>arXiv:2406.02835v2 Announce Type: replace 
Abstract: Many identification results in instrumental variables (IV) models hold without requiring any restrictions on the distribution of potential outcomes, or how those outcomes are correlated with selection behavior. This enables IV models to allow for arbitrary heterogeneity in treatment effects and the possibility of selection on gains in the outcome. I call this type of identification result "outcome-agnostic", and provide a necessary and sufficient condition for treatment effects to be point identified in an outcome-agnostic manner when the instruments and treatments take a finite number of values. The condition generalizes the well-known LATE monotonicity assumption, and unifies a wide variety of other known IV identification results. The result also yields a brute-force approach to revealing all selection models that allow for point identification of treatment effects, and then enumerating all of the identified parameters within each selection model. Though computationally intensive, the search uncovers several new IV identification results even in simple settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.02835v2</guid>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Leonard Goff</dc:creator>
    </item>
    <item>
      <title>Extract Mechanisms from Heterogeneous Effects: Identification Strategy for Mediation Analysis</title>
      <link>https://arxiv.org/abs/2403.04131</link>
      <description>arXiv:2403.04131v4 Announce Type: replace-cross 
Abstract: Understanding causal mechanisms is crucial for explaining and generalizing empirical phenomena. Causal mediation analysis offers statistical techniques to quantify mediation effects. However, current methods often require multiple ignorability assumptions or sophisticated research designs. In this paper, we introduce a novel identification strategy that enables the simultaneous identification and estimation of treatment and mediation effects. This strategy is based on a new decomposition of total treatment effects and explores heterogeneous treatment effects. Monte Carlo simulations demonstrate that the method is more accurate and precise across various scenarios. To illustrate the efficiency and efficacy of our method, we apply it to estimate the causal mediation effects in two studies with distinct data structures, focusing on common pool resource governance and voting information. Additionally, we have developed statistical software to facilitate the implementation of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04131v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiawei Fu</dc:creator>
    </item>
    <item>
      <title>Transfer Learning for Spatial Autoregressive Models with Application to U.S. Presidential Election Prediction</title>
      <link>https://arxiv.org/abs/2405.15600</link>
      <description>arXiv:2405.15600v2 Announce Type: replace-cross 
Abstract: It is important to incorporate spatial geographic information into U.S. presidential election analysis, especially for swing states. The state-level analysis also faces significant challenges of limited spatial data availability. To address the challenges of spatial dependence and small sample sizes in predicting U.S. presidential election results using spatially dependent data, we propose a novel transfer learning framework within the SAR model, called as tranSAR. Classical SAR model estimation often loses accuracy with small target data samples. Our framework enhances estimation and prediction by leveraging information from similar source data. We introduce a two-stage algorithm, consisting of a transferring stage and a debiasing stage, to estimate parameters and establish theoretical convergence rates for the estimators. Additionally, if the informative source data are unknown, we propose a transferable source detection algorithm using spatial residual bootstrap to maintain spatial dependence and derive its detection consistency. Simulation studies show our algorithm substantially improves the classical two-stage least squares estimator. We demonstrate our method's effectiveness in predicting outcomes in U.S. presidential swing states, where it outperforms traditional methods. In addition, our tranSAR model predicts that the Democratic party will win the 2024 U.S. presidential election.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.15600v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hao Zeng, Wei Zhong, Xingbai Xu</dc:creator>
    </item>
    <item>
      <title>Weak-instrument-robust subvector inference in instrumental variables regression: A subvector Lagrange multiplier test and properties of subvector Anderson-Rubin confidence sets</title>
      <link>https://arxiv.org/abs/2407.15256</link>
      <description>arXiv:2407.15256v2 Announce Type: replace-cross 
Abstract: We propose a weak-instrument-robust subvector Lagrange multiplier test for instrumental variables regression. We show that it is asymptotically size-correct under a technical condition. This is the first weak-instrument-robust subvector test for instrumental variables regression to recover the degrees of freedom of the commonly used non-weak-instrument-robust Wald test. Additionally, we provide a closed-form solution for subvector confidence sets obtained by inverting the subvector Anderson-Rubin test. We show that they are centered around a k-class estimator. Also, we show that the subvector confidence sets for single coefficients of the causal parameter are jointly bounded if and only if Anderson's likelihood-ratio test rejects the hypothesis that the first-stage regression parameter is of reduced rank, that is, that the causal parameter is not identified. Finally, we show that if a confidence set obtained by inverting the Anderson-Rubin test is bounded and nonempty, it is equal to a Wald-based confidence set with a data-dependent confidence level. We explicitly compute this Wald-based confidence test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15256v2</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.TH</category>
      <pubDate>Tue, 10 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malte Londschien, Peter B\"uhlmann</dc:creator>
    </item>
  </channel>
</rss>
