<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Apr 2025 01:43:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A Novel Strategy for Detecting Multiple Mediators in High-Dimensional Mediation Models</title>
      <link>https://arxiv.org/abs/2504.11550</link>
      <description>arXiv:2504.11550v1 Announce Type: new 
Abstract: This article presents a novel methodology for detecting multiple biomarkers in high-dimensional mediation models by utilizing a modified Least Absolute Shrinkage and Selection Operator (LASSO) alongside Pathway LASSO. This approach effectively addresses the problem of overestimating direct effects, which can result in the inaccurate identification of mediators with nonzero indirect effects. To mitigate this overestimation and improve the true positive rate for detecting mediators, two constraints on the $L_1$-norm penalty are introduced. The proposed methodology's effectiveness is demonstrated through extensive simulations across various scenarios, highlighting its robustness and reliability under different conditions. Furthermore, a procedure for selecting an optimal threshold for dimension reduction using sure independence screening is introduced, enhancing the accuracy of true biomarker detection and yielding a final model that is both robust and well-suited for real-world applications. To illustrate the practical utility of this methodology, the results are applied to a study dataset involving patients with internalizing psychopathology, showcasing its applicability in clinical settings. Overall, this methodology signifies a substantial advancement in biomarker detection within high-dimensional mediation models, offering promising implications for both research and clinical practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11550v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pei-Shan Yen, Soumya Sahu, Debarghya Nandi, Zhaoliang Zhou, Olusola Ajilore, Dulal Bhaumik</dc:creator>
    </item>
    <item>
      <title>Mapping Multivariate Phenotypes in the Presence of Missing Observations for Family-Based Data</title>
      <link>https://arxiv.org/abs/2504.11579</link>
      <description>arXiv:2504.11579v1 Announce Type: new 
Abstract: Clinical end-point traits are often characterized by quantitative or qualitative precursors and it has been argued that it may be statistically a more powerful strategy to analyze these precursor traits to decipher the genetic architecture of the underlying complex end-point trait. While association methods for both quantitative and qualitative traits have been extensively developed to analyze population level data, development of such methods are of current research interest for family-level data that pose additional challenges of incorporation of correlation of trait values within a family. Haldar and Ghosh (2015) developed a test which is Statistical equivalent of the classical TDT for quantitative traits and multivariate phenotypes. The model does not require a priori assumptions on the probability distributions of the phenotypes. However, it may often arise in practice that data on the phenotype of interest may not be available for all offspring in a nuclear family. In this study, we explore methodologies to estimate missing phenotypes conditioned on the available ones and carry out the transmission-based test for association on the 'complete' data. We consider three types of phenotypes: continuous, count and categorical. For a missing continuous phenotype, the trait value is estimated using a conditional normal model. For a missing count phenotypes, the trait value is estimated using a conditional Poisson model. For a missing categorical phenotype, the risk of the phenotype status is estimated using a conditional logistic model. We shall carry out simulations under a wide spectrum of genetic models and assess the effect of the proposed imputation strategy on the power of the association test vis-\`a-vis the the ideal situation with no missing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11579v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Saurabh Ghosh</dc:creator>
    </item>
    <item>
      <title>Kernel-based Method for Detecting Structural Break in Distribution of Functional Data</title>
      <link>https://arxiv.org/abs/2504.11583</link>
      <description>arXiv:2504.11583v1 Announce Type: new 
Abstract: We propose a novel method to detect and date structural breaks in the entire distribution of functional data. Theoretical guarantees are developed for our procedure under fewer assumptions than in the existing work. In particular, we establish the asymptotic null distribution of the test statistic, which enables us to test the null hypothesis at a certain significance level. Additionally, the limiting distribution of the estimated structural break date is developed under two situations of the break size: fixed and shrinking towards 0 at a specified rate. We further propose a unified bootstrap procedure to construct a confidence interval for the true structural break date for these two situations. These theoretical results are justified through comprehensive simulation studies in finite samples. We apply the proposed method to two real-world examples: Australian temperature data for detecting structural beaks and Canadian weather data for goodness of fit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11583v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peijun Sang, Bing Li</dc:creator>
    </item>
    <item>
      <title>Statistical Modeling of Combinatorial Response Data</title>
      <link>https://arxiv.org/abs/2504.11630</link>
      <description>arXiv:2504.11630v1 Announce Type: new 
Abstract: In categorical data analysis, there is rich literature for modeling binary and polychotomous responses. However, existing methods are inadequate for handling combinatorial responses, where each response is an array of integers subject to additional constraints. Such data are increasingly common in modern applications, such as surveys collected under skip logic, event propagation on a network, and observed matching in ecology. Ignoring the combinatorial structure in the response data may lead to biased estimation and prediction. The fundamental challenge for modeling these integer-vector data is the lack of a link function that connects a linear or functional predictor with a probability respecting the combinatorial constraints. In this paper, we propose a novel augmented likelihood, in which a combinatorial response can be viewed as a deterministic transform of a continuous latent variable. We specify the transform as the maximizer of integer linear program, and characterize useful properties such as dual thresholding representation. When taking a Bayesian approach and considering a multivariate normal distribution for the latent variable, our method becomes a direct generalization to the celebrated probit data augmentation, and enjoys straightforward computation via Gibbs sampler. We provide theoretical justification for the proposed method at an interesting intersection between duality and probability distribution and develop useful sufficient conditions that guarantee the applicability of our method. We demonstrate the effectiveness of our method through simulation studies and a real data application on modeling the formation of seasonal matching between waterfowl.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11630v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Yu Zheng, Malay Ghosh, Leo Duan</dc:creator>
    </item>
    <item>
      <title>Scalable Efficient Inference in Complex Surveys through Targeted Resampling of Weights</title>
      <link>https://arxiv.org/abs/2504.11636</link>
      <description>arXiv:2504.11636v1 Announce Type: new 
Abstract: Survey data often arises from complex sampling designs, such as stratified or multistage sampling, with unequal inclusion probabilities. When sampling is informative, traditional inference methods yield biased estimators and poor coverage. Classical pseudo-likelihood based methods provide accurate asymptotic inference but lack finite-sample uncertainty quantification and the ability to integrate prior information. Existing Bayesian approaches, like the Bayesian pseudo-posterior estimator and weighted Bayesian bootstrap, have limitations; the former struggles with uncertainty quantification, while the latter is computationally intensive and sensitive to bootstrap replicates. To address these challenges, we propose the Survey-adjusted Weighted Likelihood Bootstrap (S-WLB), which resamples weights from a carefully chosen distribution centered around the underlying sampling weights. S-WLB is computationally efficient, theoretically consistent, and delivers finite-sample uncertainty intervals which are proven to be asymptotically valid. We demonstrate its performance through simulations and applications to nationally representative survey datasets like NHANES and NSDUH.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11636v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Snigdha Das, Dipankar Bandyopadhyay, Debdeep Pati</dc:creator>
    </item>
    <item>
      <title>A cautionary note for plasmode simulation studies in the setting of causal inference</title>
      <link>https://arxiv.org/abs/2504.11740</link>
      <description>arXiv:2504.11740v1 Announce Type: new 
Abstract: Plasmode simulation has become an important tool for evaluating the operating characteristics of different statistical methods in complex settings, such as pharmacoepidemiological studies of treatment effectiveness using electronic health records (EHR) data. These studies provide insight into how estimator performance is impacted by challenges including rare events, small sample size, etc., that can indicate which among a set of methods performs best in a real-world dataset. Plasmode simulation combines data resampled from a real-world dataset with synthetic data to generate a known truth for an estimand in realistic data. There are different potential plasmode strategies currently in use. We compare two popular plasmode simulation frameworks. We provide numerical evidence and a theoretical result, which shows that one of these frameworks can cause certain estimators to incorrectly appear overly biased with lower than nominal confidence interval coverage. Detailed simulation studies using both synthetic and real-world EHR data demonstrate that these pitfalls remain at large sample sizes and when analyzing data from a randomized controlled trial. We conclude with guidance for the choice of a plasmode simulation approach that maintains good theoretical properties to allow a fair evaluation of statistical methods while also maintaining the desired similarity to real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11740v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pamela A Shaw, Susan Gruber, Brian D. Williamson, Rishi Desai, Susan M. Shortreed, Chloe Krakauer, Jennifer C. Nelson, Mark J. van der Laan</dc:creator>
    </item>
    <item>
      <title>Bringing closure to FDR control: beating the e-Benjamini-Hochberg procedure</title>
      <link>https://arxiv.org/abs/2504.11759</link>
      <description>arXiv:2504.11759v1 Announce Type: new 
Abstract: False discovery rate (FDR) has been a key metric for error control in multiple hypothesis testing, and many methods have developed for FDR control across a diverse cross-section of settings and applications. We develop a closure principle for all FDR controlling procedures, i.e., we provide a characterization based on e-values for all admissible FDR controlling procedures. We leverage this idea to formulate the closed eBH procedure, a (usually strict) improvement over the eBH procedure for FDR control when provided with e-values. We demonstrate the practical performance of closed eBH in simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11759v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyu Xu, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Post-selection Inference in Regression Models for Group Testing Data</title>
      <link>https://arxiv.org/abs/2504.11767</link>
      <description>arXiv:2504.11767v1 Announce Type: new 
Abstract: We develop methodology for valid inference after variable selection in logistic regression when the responses are partially observed, that is, when one observes a set of error-prone testing outcomes instead of the true values of the responses. Aiming at selecting important covariates while accounting for missing information in the response data, we apply the expectation-maximization algorithm to compute maximum likelihood estimators subject to LASSO penalization. Subsequent to variable selection, we make inferences on the selected covariate effects by extending post-selection inference methodology based on the polyhedral lemma. Empirical evidence from our extensive simulation study suggests that our post-selection inference results are more reliable than those from naive inference methods that use the same data to perform variable selection and inference without adjusting for variable selection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11767v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujae101</arxiv:DOI>
      <arxiv:journal_reference>Biometrics. 2024 Jul 1;80(3):ujae101</arxiv:journal_reference>
      <dc:creator>Qinyan Shen, Karl Gregory, Xianzheng Huang</dc:creator>
    </item>
    <item>
      <title>Non-centering for discrete-valued state transition models: an application to ESBL-producing E. coli transmission in Malawi</title>
      <link>https://arxiv.org/abs/2504.11836</link>
      <description>arXiv:2504.11836v1 Announce Type: new 
Abstract: Infectious disease transmission is often modelled by discrete-valued stochastic state-transition processes. Due to a lack of complete data, Bayesian inference for these models often relies on data-augmentation techniques. These techniques are often inefficient or time consuming to implement. We introduce a novel data-augmentation Markov chain Monte Carlo method for discrete-time individual-based epidemic models, which we call the Rippler algorithm. This method uses the transmission model in the proposal step of the Metropolis-Hastings algorithm, rather than in the accept-reject step. We test the Rippler algorithm on simulated data and apply it to data on extended-spectrum beta-lactamase (ESBL)-producing E. coli collected in Blantyre, Malawi. We compare the Rippler algorithm to two other commonly used Bayesian inference methods for partially observed epidemic data, and find that it has a good balance between mixing speed and computational complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11836v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Neill, Rebecca Lester, Winnie Bakali, Gareth Roberts, Nicholas Feasey, Lloyd A. C. Chapman, Chris Jewell</dc:creator>
    </item>
    <item>
      <title>Proximal Inference on Population Intervention Indirect Effect</title>
      <link>https://arxiv.org/abs/2504.11848</link>
      <description>arXiv:2504.11848v1 Announce Type: new 
Abstract: The population intervention indirect effect (PIIE) is a novel mediation effect representing the indirect component of the population intervention effect. Unlike traditional mediation measures, such as the natural indirect effect, the PIIE holds particular relevance in observational studies involving unethical exposures, when hypothetical interventions that impose harmful exposures are inappropriate. Although prior research has identified PIIE under unmeasured confounders between exposure and outcome, it has not fully addressed the confounding that affects the mediator. This study extends the PIIE identification to settings where unmeasured confounders influence exposure-outcome, exposure-mediator, and mediator-outcome relationships. Specifically, we leverage observed covariates as proxy variables for unmeasured confounders, constructing three proximal identification frameworks. Additionally, we characterize the semiparametric efficiency bound and develop multiply robust and locally efficient estimators. To handle high-dimensional nuisance parameters, we propose a debiased machine learning approach that achieves $\sqrt{n}$-consistency and asymptotic normality to estimate the true PIIE values, even when the machine learning estimators for the nuisance functions do not converge at $\sqrt{n}$-rate. In simulations, our estimators demonstrate higher confidence interval coverage rates than conventional methods across various model misspecifications. In a real data application, our approaches reveal an indirect effect of alcohol consumption on depression risk mediated by depersonalization symptoms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11848v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Bai, Yifan Cui, Baoluo Sun</dc:creator>
    </item>
    <item>
      <title>Testing of tempered fractional Brownian motions</title>
      <link>https://arxiv.org/abs/2504.11906</link>
      <description>arXiv:2504.11906v1 Announce Type: new 
Abstract: We propose here a testing methodology based on the autocovariance, detrended moving average, and time-averaged mean-squared displacement statistics for tempered fractional Brownian motions (TFBMs) which are related to the notions of semi-long range dependence and transient anomalous diffusion. In this framework, we consider three types of TFBMs: two with a tempering factor incorporated into their moving-average representation, and one with a tempering parameter added to the autocorrelation formula. We illustrate their dynamics with the use of quantile lines. Using the proposed methodology, we provide a comprehensive power analysis of the tests. It appears that the tests allow distinguishing between the tempered processes with different Hurst parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11906v1</guid>
      <category>stat.ME</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katarzyna Macioszek, Farzad Sabzikar, Krzysztof Burnecki</dc:creator>
    </item>
    <item>
      <title>Semiparametric Causal Discovery and Inference with Invalid Instruments</title>
      <link>https://arxiv.org/abs/2504.12085</link>
      <description>arXiv:2504.12085v1 Announce Type: new 
Abstract: Learning causal relationships among a set of variables, as encoded by a directed acyclic graph, from observational data is complicated by the presence of unobserved confounders. Instrumental variables (IVs) are a popular remedy for this issue, but most existing methods either assume the validity of all IVs or postulate a specific form of relationship, such as a linear model, between the primary variables and the IVs. To overcome these limitations, we introduce a partially linear structural equation model for causal discovery and inference that accommodates potentially invalid IVs and allows for general dependence of the primary variables on the IVs. We establish identification under this semiparametric model by constructing surrogate valid IVs, and develop a finite-sample procedure for estimating the causal structures and effects. Theoretically, we show that our procedure consistently learns the causal structures, yields asymptotically normal estimates, and effectively controls the false discovery rate in edge recovery. Simulation studies demonstrate the superiority of our method over existing competitors, and an application to inferring gene regulatory networks in Alzheimer's disease illustrates its usefulness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12085v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jing Zou, Wei Li, Wei Lin</dc:creator>
    </item>
    <item>
      <title>Bayesian random-effects meta-analysis of aggregate data on clinical events</title>
      <link>https://arxiv.org/abs/2504.12214</link>
      <description>arXiv:2504.12214v1 Announce Type: new 
Abstract: To appreciate intervention effects on rare events, meta-analysis techniques are commonly applied in order to assess the accumulated evidence. When it comes to adverse effects in clinical trials, these are often most adequately handled using survival methods. A common-effect model that is able to process data in commonly quoted formats in terms of hazard ratios has been proposed for this purpose by Holzhauer (Stat. Med. 2017; 36(5):723-737). In order to accommodate potential heterogeneity between studies, we have extended the model by Holzhauer to a random-effects approach. The Bayesian model is described in detail, and applications to realistic data sets are discussed along with sensitivity analyses and Monte Carlo simulations to support the conclusions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12214v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian R\"over, Qiong Wu, Anja Loos, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Trend Filtered Mixture of Experts for Automated Gating of High-Frequency Flow Cytometry Data</title>
      <link>https://arxiv.org/abs/2504.12287</link>
      <description>arXiv:2504.12287v1 Announce Type: new 
Abstract: Ocean microbes are critical to both ocean ecosystems and the global climate. Flow cytometry, which measures cell optical properties in fluid samples, is routinely used in oceanographic research. Despite decades of accumulated data, identifying key microbial populations (a process known as ``gating'') remains a significant analytical challenge. To address this, we focus on gating multidimensional, high-frequency flow cytometry data collected {\it continuously} on board oceanographic research vessels, capturing time- and space-wise variations in the dynamic ocean. Our paper proposes a novel mixture-of-experts model in which both the gating function and the experts are given by trend filtering. The model leverages two key assumptions: (1) Each snapshot of flow cytometry data is a mixture of multivariate Gaussians and (2) the parameters of these Gaussians vary smoothly over time. Our method uses regularization and a constraint to ensure smoothness and that cluster means match biologically distinct microbe types. We demonstrate, using flow cytometry data from the North Pacific Ocean, that our proposed model accurately matches human-annotated gating and corrects significant errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12287v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sangwon Hyun, Tim Coleman, Francois Ribalet, Jacob Bien</dc:creator>
    </item>
    <item>
      <title>The underlap coefficient as a measure of a biomarker's discriminatory ability</title>
      <link>https://arxiv.org/abs/2504.12288</link>
      <description>arXiv:2504.12288v2 Announce Type: new 
Abstract: The first step in evaluating a potential diagnostic biomarker is to examine the variation in its values across different disease groups. In a three-class disease setting, the volume under the receiver operating characteristic surface and the three-class Youden index are commonly used summary measures of a biomarker's discriminatory ability. However, these measures rely on a stochastic ordering assumption for the distributions of biomarker outcomes across the three groups. This assumption can be restrictive, particularly when covariates are involved, and its violation may lead to incorrect conclusions about a biomarker's ability to distinguish between the three disease classes. Even when a stochastic ordering exists, the order may vary across different biomarkers in discovery studies involving dozens or even thousands of candidate biomarkers, complicating automated ranking. To address these challenges and complement existing measures, we propose the underlap coefficient, a novel summary index of a biomarker's ability to distinguish between three (or more) disease groups, and study its properties. Additionally, we introduce Bayesian nonparametric estimators for both the unconditional underlap coefficient and its covariate-specific counterpart. These estimators are broadly applicable to a wide range of biomarkers and populations. A simulation study reveals a good performance of the proposed estimators across a range of conceivable scenarios. We illustrate the proposed approach through an application to an Alzheimer's disease (AD) dataset aimed to assess how four potential AD biomarkers distinguish between individuals with normal cognition, mild impairment, and dementia, and how and if age and gender impact this discriminatory ability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.12288v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaoxi Zhang (for the Alzheimer's Disease Neuroimaging Initiative), Vanda Inacio (for the Alzheimer's Disease Neuroimaging Initiative), Miguel de Carvalho (for the Alzheimer's Disease Neuroimaging Initiative)</dc:creator>
    </item>
    <item>
      <title>In silico clinical trials in drug development: a systematic review</title>
      <link>https://arxiv.org/abs/2503.08746</link>
      <description>arXiv:2503.08746v2 Announce Type: cross 
Abstract: In the context of clinical research, computational models have received increasing attention over the past decades. In this systematic review, we aimed to provide an overview of the role of so-called in silico clinical trials (ISCTs) in medical applications. Exemplary for the broad field of clinical medicine, we focused on in silico (IS) methods applied in drug development, sometimes also referred to as model informed drug development (MIDD). We searched PubMed and ClinicalTrials.gov for published articles and registered clinical trials related to ISCTs. We identified 202 articles and 48 trials, and of these, 76 articles and 19 trials were directly linked to drug development. We extracted information from all 202 articles and 48 clinical trials and conducted a more detailed review of the methods used in the 76 articles that are connected to drug development. Regarding application, most articles and trials focused on cancer and imaging related research while rare and pediatric diseases were only addressed in 18 and 4 studies, respectively. While some models were informed combining mechanistic knowledge with clinical or preclinical (in-vivo or in-vitro) data, the majority of models were fully data-driven, illustrating that clinical data is a crucial part in the process of generating synthetic data in ISCTs. Regarding reproducibility, a more detailed analysis revealed that only 24% (18 out of 76) of the articles provided an open-source implementation of the applied models, and in only 20% of the articles the generated synthetic data were publicly available. Despite the widely raised interest, we also found that it is still uncommon for ISCTs to be part of a registered clinical trial and their application is restricted to specific diseases leaving potential benefits of ISCTs not fully exploited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08746v2</guid>
      <category>q-bio.QM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohua Chen, Lucia Chantal Schneider, Christian R\"over, Emmanuelle Comets, Markus Christian Elze, Andrew Hooker, Joanna IntHout, Anne-Sophie Jannot, Daria Julkowska, Yanis Mimouni, Marina Savelieva, Nigel Stallard, Moreno Ursino, Marc Vandemeulebroecke, Sebastian Weber, Martin Posch, Sarah Zohar, Tim Friede</dc:creator>
    </item>
    <item>
      <title>Towards Interpretable Deep Generative Models via Causal Representation Learning</title>
      <link>https://arxiv.org/abs/2504.11609</link>
      <description>arXiv:2504.11609v1 Announce Type: cross 
Abstract: Recent developments in generative artificial intelligence (AI) rely on machine learning techniques such as deep learning and generative modeling to achieve state-of-the-art performance across wide-ranging domains. These methods' surprising performance is due in part to their ability to learn implicit "representations'' of complex, multi-modal data. Unfortunately, deep neural networks are notoriously black boxes that obscure these representations, making them difficult to interpret or analyze. To resolve these difficulties, one approach is to build new interpretable neural network models from the ground up. This is the goal of the emerging field of causal representation learning (CRL) that uses causality as a vector for building flexible, interpretable, and transferable generative AI. CRL can be seen as a culmination of three intrinsically statistical problems: (i) latent variable models such as factor analysis; (ii) causal graphical models with latent variables; and (iii) nonparametric statistics and deep learning. This paper reviews recent progress in CRL from a statistical perspective, focusing on connections to classical models and statistical and causal identifiablity results. This review also highlights key application areas, implementation strategies, and open statistical questions in CRL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.11609v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gemma E. Moran, Bryon Aragam</dc:creator>
    </item>
    <item>
      <title>Scalable simulation-based inference for implicitly defined models using a metamodel for Monte Carlo log-likelihood estimator</title>
      <link>https://arxiv.org/abs/2311.09446</link>
      <description>arXiv:2311.09446v3 Announce Type: replace 
Abstract: Models implicitly defined through a random simulator of a process have become widely used in scientific and industrial applications in recent years. However, simulation-based inference methods for such implicit models, like approximate Bayesian computation (ABC), often scale poorly as data size increases. We develop a scalable inference method for implicitly defined models using a metamodel for the Monte Carlo log-likelihood estimator derived from simulations. This metamodel characterizes both statistical and simulation-based randomness in the distribution of the log-likelihood estimator across different parameter values. Our metamodel-based method quantifies uncertainty in parameter estimation in a principled manner, leveraging the local asymptotic normality of the mean function of the log-likelihood estimator. We apply this method to construct accurate confidence intervals for parameters of partially observed Markov process models where the Monte Carlo log-likelihood estimator is obtained using the bootstrap particle filter. We numerically demonstrate that our method enables accurate and highly scalable parameter inference across several examples, including a mechanistic compartment model for infectious diseases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.09446v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joonha Park</dc:creator>
    </item>
    <item>
      <title>W-Kernel and Its Principal Space for Frequentist Evaluation of Bayesian Estimators</title>
      <link>https://arxiv.org/abs/2311.13017</link>
      <description>arXiv:2311.13017v5 Announce Type: replace 
Abstract: Evaluating the variability of posterior estimates is a key aspect of Bayesian model assessment. In this study, we focus on the posterior covariance matrix W, defined using the log-likelihood of each observation. Previous studies have examined the role of the principal space of W in Bayesian sensitivity analysis, notably MacEachern and Peruggia (2002) and Thomas et al. (2018). In this work, we show that the principal space of W is also relevant for frequentist evaluation, using the recently proposed Bayesian infinitesimal jackknife (IJ) approximation Giordano and Broderick (2023) as a key tool. We next consider the relationship between the matrix W and the Fisher kernel. We show that the Fisher kernel can be regarded as an approximation to W; the matrix W, in itself, can be interpreted as a reproducing kernel, which we refer to as the W-kernel. Based on this connection, we examine the dual relationship between the W-kernel formulation in the data space and the classical asymptotic formulation in the parameter space. These ideas suggest a form of Bayesian-frequentist duality that emerges through the dual structure of kernel PCA, where posterior and frequentist covariances serve as inner products in their respective spaces. As an application, we consider an approximate bootstrap of posterior means based on posterior samples generated by MCMC. We show that the projection onto the principal space of W facilitates frequentist evaluation, particularly of the higher-order term in this procedure. In one of the appendices, we introduce incomplete Cholesky decomposition as an efficient method for computing the principal space of W and discuss the related concept of representative subsets of the observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13017v5</guid>
      <category>stat.ME</category>
      <category>cond-mat.stat-mech</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukito Iba</dc:creator>
    </item>
    <item>
      <title>A Stability Framework for Parameter Selection in the Minimum Covariance Determinant Problem</title>
      <link>https://arxiv.org/abs/2401.14359</link>
      <description>arXiv:2401.14359v4 Announce Type: replace 
Abstract: The Minimum Covariance Determinant (MCD) method is a widely adopted tool for robust estimation and outlier detection. In this paper, we introduce MCD model selection based on the notion of stability. Our best subset method leverages prior best practices such as statistical depths for initialization and concentration steps for subset refinement. Our contribution lies in constructing a bootstrap procedure to estimate the instability of the best subset algorithm. The instability path offers insights into a dataset's inlier/outlier structure and facilitates suitable choice of the subset size. We rigorously benchmark the proposed framework against existing MCD variants and illustrate its practical utility on several real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14359v4</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiang Heng, Hui Shen, Kenneth Lange</dc:creator>
    </item>
    <item>
      <title>Group-Weighted Conformal Prediction</title>
      <link>https://arxiv.org/abs/2401.17452</link>
      <description>arXiv:2401.17452v4 Announce Type: replace 
Abstract: Conformal prediction (CP) is a method for constructing a prediction interval around the output of a fitted model, whose validity does not rely on the model being correct--the CP interval offers a coverage guarantee that is distribution-free, but relies on the training data being drawn from the same distribution as the test data. A recent variant, weighted conformal prediction (WCP), reweights the method to allow for covariate shift between the training and test distributions. However, WCP requires knowledge of the nature of the covariate shift-specifically,the likelihood ratio between the test and training covariate distributions. In practice, since this likelihood ratio is estimated rather than known exactly, the coverage guarantee may degrade due to the estimation error. In this paper, we consider a special scenario where observations belong to a finite number of groups, and these groups determine the covariate shift between the training and test distributions-for instance, this may arise if the training set is collected via stratified sampling. Our results demonstrate that in this special case, the predictive coverage guarantees of WCP can be drastically improved beyond the bounds given by existing estimation error bounds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.17452v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aabesh Bhattacharyya, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>A Bayesian Estimator of Sample Size</title>
      <link>https://arxiv.org/abs/2404.07923</link>
      <description>arXiv:2404.07923v3 Announce Type: replace 
Abstract: We consider a Bayesian estimator of sample size (BESS) and an application to oncology dose optimization clinical trials. BESS is built upon three pillars, Sample size, Evidence from observed data, and Confidence in posterior inference. It uses a simple logic of "given the evidence from data, a specific sample size can achieve a degree of confidence in the posterior inference." The key distinction between BESS and standard sample size estimation (SSE) is that SSE, typically based on Frequentist inference, specifies the true parameters values in its calculation while BESS assumes possible outcome from the observed data. As a result, the calibration of the sample size is not based on type I or type II error rates, but on posterior probabilities. We demonstrate that BESS leads to a more interpretable statement for investigators, and can easily accommodates prior information as well as sample size re-estimation. We explore its performance in comparison to the standard SSE and demonstrate its usage through a case study of oncology optimization trial. BESS can be applied to general hypothesis tests. An R tool is available at https://ccte.uchicago.edu/BESS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.07923v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dehua Bi, Yuan Ji</dc:creator>
    </item>
    <item>
      <title>Optimal Sequential Procedure for Early Detection of Multiple Side Effects</title>
      <link>https://arxiv.org/abs/2405.08759</link>
      <description>arXiv:2405.08759v2 Announce Type: replace 
Abstract: In this paper, we propose an optimal sequential procedure for the early detection of potential side effects resulting from the administration of some treatment (e.g. a vaccine, say). The results presented here extend previous results obtained in Wang and Boukai (2024) who study the single side effect case to the case of two (or more) side effects. While the sequential procedure we employ, simultaneously monitors several of the treatment's side effects, the $(\alpha, \beta)$-optimal test we propose does not require any information about the inter-correlation between these potential side effects. However, in all of the subsequent analyses, including the derivations of the exact expressions of the Average Sample Number (ASN), the Power function, and the properties of the post-test (or post-detection) estimators, we accounted specifically, for the correlation between the potential side effects. In the real-life application (such as post-marketing surveillance), the number of available observations is large enough to justify asymptotic analyses of the sequential procedure (testing and post-detection estimation) properties. Accordingly, we also derive the consistency and asymptotic normality of our post-test estimators; results which enable us to also provide (asymptotic, post-detection) confidence intervals for the probabilities of various side-effects. Moreover, to compare two specific side effects, their relative risk plays an important role. We derive the distribution of the estimated relative risk in the asymptotic framework to provide appropriate inference. To illustrate the theoretical results presented, we provide two detailed examples based on the data of side effects on COVID-19 vaccine collected in Nigeria (see Nigeria (see Ilori et al. (2022)).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08759v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiayue Wang, Ben Boukai</dc:creator>
    </item>
    <item>
      <title>Detecting dependence structure: visualization and inference</title>
      <link>https://arxiv.org/abs/2410.05858</link>
      <description>arXiv:2410.05858v2 Announce Type: replace 
Abstract: Identifying dependency between two random variables is a fundamental problem. The clear interpretability and ability of a procedure to provide information on the form of possible dependence is particularly important when exploring dependencies. In this paper, we introduce a novel method that employs a new estimator of the quantile dependence function and pertinent local acceptance regions. This leads to an insightful visualisation and a rigorous evaluation of the underlying dependence structure. We also propose a test of independence of two random variables, pertinent to this new estimator. Our procedures are based on ranks, and we derive a finite-sample theory that guarantees the inferential validity of our solutions at any given sample size. The procedures are simple to implement and computationally efficient. The large sample consistency of the proposed test is also proved. We show that, in terms of power, the new test is one of the best statistics for independence testing when considering a wide range of alternative models. Finally, we demonstrate the use of our approach to visualise dependence structure and to detect local departures from independence through analysing some real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.05858v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bogdan \'Cmiel, Teresa Ledwina</dc:creator>
    </item>
    <item>
      <title>Nonlocal prior mixture-based Bayesian wavelet regression</title>
      <link>https://arxiv.org/abs/2501.18134</link>
      <description>arXiv:2501.18134v2 Announce Type: replace 
Abstract: We propose a novel Bayesian wavelet regression approach using a three-component spike-and-slab prior for wavelet coefficients, combining a point mass at zero, a moment (MOM) prior, and an inverse moment (IMOM) prior. This flexible prior supports small and large coefficients differently, offering advantages for highly dispersed data where wavelet coefficients span multiple scales. The IMOM prior's heavy tails capture large coefficients, while the MOM prior is better suited for smaller non-zero coefficients. Further, our method introduces innovative hyperparameter specifications for mixture probabilities and scale parameters, including generalized logit, hyperbolic secant, and generalized normal decay for probabilities, and double exponential decay for scaling. Hyperparameters are estimated via an empirical Bayes approach, enabling posterior inference tailored to the data. Extensive simulations demonstrate significant performance gains over two-component wavelet methods. Applications to electroencephalography and noisy audio data illustrate the method's utility in capturing complex signal characteristics. We implement our method in an R package NLPwavelet.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18134v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nilotpal Sanyal</dc:creator>
    </item>
    <item>
      <title>On Bessel's Correction -- Unbiased Sample Variance, the "Bariance," and a Novel Runtime-Optimized Unbiased Sample Variance Estimator</title>
      <link>https://arxiv.org/abs/2503.22333</link>
      <description>arXiv:2503.22333v2 Announce Type: replace 
Abstract: Bessel's correction adjusts the denominator in the sample variance formula from n to n - 1 to produce an unbiased estimator for the population variance. This paper includes rigorous derivations, geometric interpretations, and visualizations. It then introduces the concept of 'bariance', an alternative pairwise distances intuition of sample dispersion without an arithmetic mean. Finally, we address practical concerns raised in Rosenthal's article advocating the use of n-based estimates from a more holistic MSE-based viewpoint for pedagogical reasons and in certain practical contexts. Finally, the empirical part using simulation reveals that the run-time of estimating population variance can be significantly shortened when using an algebraically optimized bariance approach using scalar sums to estimate an unbiased variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22333v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.32388/3GJGNA</arxiv:DOI>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Multivariate Causal Effects: a Bayesian Causal Regression Factor Model</title>
      <link>https://arxiv.org/abs/2504.03480</link>
      <description>arXiv:2504.03480v2 Announce Type: replace 
Abstract: The impact of wildfire smoke on air quality is a growing concern, contributing to air pollution through a complex mixture of chemical species with important implications for public health. While previous studies have primarily focused on its association with total particulate matter (PM2.5), the causal relationship between wildfire smoke and the chemical composition of PM2.5 remains largely unexplored. Exposure to these chemical mixtures plays a critical role in shaping public health, yet capturing their relationships requires advanced statistical methods capable of modeling the complex dependencies among chemical species. To fill this gap, we propose a Bayesian causal regression factor model that estimates the multivariate causal effects of wildfire smoke on the concentration of 27 chemical species in PM2.5 across the United States. Our approach introduces two key innovations: (i) a causal inference framework for multivariate potential outcomes, and (ii) a novel Bayesian factor model that employs a probit stick-breaking process as prior for treatment-specific factor scores. By focusing on factor scores, our method addresses the missing data challenge common in causal inference and enables a flexible, data-driven characterization of the latent factor structure, which is crucial to capture the complex correlation among multivariate outcomes. Through Monte Carlo simulations, we show the model's accuracy in estimating the causal effects in multivariate outcomes and characterizing the treatment-specific latent structure. Finally, we apply our method to US air quality data, estimating the causal effect of wildfire smoke on 27 chemical species in PM2.5, providing a deeper understanding of their interdependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.03480v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Jenna Landy, Corwin Zigler, Giovanni Parmigiani, Roberta De Vito</dc:creator>
    </item>
    <item>
      <title>The ecological forecast limit revisited: Potential, actual and relative system predictability</title>
      <link>https://arxiv.org/abs/2412.00753</link>
      <description>arXiv:2412.00753v2 Announce Type: replace-cross 
Abstract: Ecological forecasts are model-based statements about currently unknown ecosystem states in time or space. For a model forecast to be useful to inform decision makers, model validation and verification determine adequateness. The measure of forecast goodness that can be translated into a limit up to which a forecast is acceptable is known as the 'forecast limit'. While verification in weather forecasting follows strict criteria with established metrics and forecast limits, assessments of ecological forecasting models still remain experiment-specific, and forecast limits are rarely reported. As such, users of ecological forecasts remain uninformed of how far into the future statements can be trusted. In this work, we synthesise existing approaches to define empirical forecast limits in a unified framework for assessing ecological predictability and offer recipes for their computation. We distinguish the model's potential and absolute forecast limit, and show how a benchmark model can help determine its relative forecast limit. The approaches are demonstrated with three case studies from population, ecosystem, and Earth system research. We found that forecast limits can be computed with three requirements: A verification reference, a scoring function, and a predictive error tolerance. Within our framework, forecast limits are defined for practically any ecological forecast and support research on ecological predictability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.00753v2</guid>
      <category>stat.AP</category>
      <category>physics.data-an</category>
      <category>q-bio.PE</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marieke Wesselkamp, Jakob Albrecht, Ewan Pinnington, William J. Castillo, Florian Pappenberger, Carsten F. Dormann</dc:creator>
    </item>
    <item>
      <title>Recovering Unobserved Network Links from Aggregated Relational Data: Discussions on Bayesian Latent Surface Modeling and Penalized Regression</title>
      <link>https://arxiv.org/abs/2501.10675</link>
      <description>arXiv:2501.10675v2 Announce Type: replace-cross 
Abstract: Accurate network data are essential in fields such as economics, sociology, and computer science. Aggregated Relational Data (ARD) provides a way to capture network structures using partial data. This article compares two main frameworks for recovering network links from ARD: Bayesian Latent Surface Modeling (BLSM) and Frequentist Penalized Regression (FPR). Using simulation studies and real-world applications, we evaluate their theoretical properties, computational efficiency, and practical utility in domains like financial risk assessment and epidemiology. Key findings emphasize the importance of trait design, privacy considerations, and hybrid modeling approaches to improve scalability and robustness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.10675v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yen-hsuan Tseng</dc:creator>
    </item>
    <item>
      <title>E-TRIALS: Empowering Data-Driven Decisions to Enhance Computer-Based Learning Platforms</title>
      <link>https://arxiv.org/abs/2502.10545</link>
      <description>arXiv:2502.10545v2 Announce Type: replace-cross 
Abstract: Computer-based learning platforms (CBLPs) have become a common medium in schools, transforming how students learn and interact with educational content. However, researchers still lack adequate tools to address the diverse set of challenges that students face in these environments. In this paper, we introduce \textbf{Ed-Tech Research Infrastructure to Advance Learning Sciences (E-TRIALS)}, a free tool developed by ASSISTments to help researchers conduct randomized controlled trials in the realm of learning sciences. We describe its features, the types of experiments it supports, and how it can address critical research questions. We showcase E-TRIALS' capabilities through two real-world interventions. Finally, we evaluate the efficacy of interventions using three average treatment effect (ATE) estimators. Student's t-test, regression, and Leave-One-Out Potential outcomes (LOOP). The results demonstrate that the unbiased LOOP estimator can achieve greater precision by adjusting for baseline covariates compared to the Student's t test. Our work demonstrates the potential of E-TRIALS to advance research and contribute to the development of more effective, inclusive, and adaptive CBLP. The code used for this work is available at https://osf.io/xp6ch/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10545v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abubakir Siedahmed, Yanping Pei, Adam C Sales, Neil T Heffernan, Johann Gagnon-Bartsch, Di Zhang, Brendan A. Schuetze, Allison Zengilowski</dc:creator>
    </item>
    <item>
      <title>Relaxing the Markov Requirements on Reinforcement Learning Under Weak Partial Ignorability</title>
      <link>https://arxiv.org/abs/2504.07722</link>
      <description>arXiv:2504.07722v2 Announce Type: replace-cross 
Abstract: Incomplete data, confounding effects, and violations of the Markov property are interrelated problems which are ubiquitous in Reinforcement Learning applications. We introduce the concept of ``partial ignorabilty" and leverage it to establish a novel convergence theorem for adaptive Reinforcement Learning. This theoretical result relaxes the Markov assumption on the stochastic process underlying conventional $Q$-learning, deploying a generalized form of the Robbins-Monro stochastic approximation theorem to establish optimality. This result has clear downstream implications for most active subfields of Reinforcement Learning, with clear paths for extension to the field of Causal Inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07722v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>MaryLena Bleile</dc:creator>
    </item>
  </channel>
</rss>
