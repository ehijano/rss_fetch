<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Nov 2025 05:01:42 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Integrating Spatial and Temporal Effects in Seat-Belt Compliance Assessment with Telematics Data</title>
      <link>https://arxiv.org/abs/2511.20712</link>
      <description>arXiv:2511.20712v1 Announce Type: new 
Abstract: Seat belt use remains one of the most effective measures for reducing vehicle occupant fatalities and injuries. Yet, seat-belt compliance across different locales demands far more granular data than traditional, roadside surveys can provide. These surveys are spatially sparse, temporally intermittent, and costly to administer, often providing coarse-grained snapshots insufficient for capturing dynamic behavioral patterns or localized disparities. Telematics data emerges as a transformative alternative, offering continuous, high-resolution driver event records, such as seat belt latch status, across vast geographic areas. This granular and scalable data enables the application of advanced spatiotemporal models that more accurately reflect the complex interactions driving seatbelt use. This study utilizes telematics data to generate county-level seat belt compliance metrics for Iowa in 2022, employing a suite of beta-regression models that incorporate spatial and temporal random effects. The study findings demonstrate that models including both spatial and temporal components outperform those with spatial or temporal effects alone, underscoring the importance of jointly accounting for geographic clustering and temporal dynamics. Among explanatory variables, vehicle miles traveled (VMT) and per capita income emerge as significant predictors of compliance rates. The significant spatial and temporal effects highlight that telematics-based granular data substantially enhances model fit and inference quality. The results demonstrate that integrating granular telematics data within sophisticated spatiotemporal frameworks significantly improves inference, providing policymakers with precise insights for targeted interventions and advancing traffic safety research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20712v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ashutosh Dumka, Raghupathi Kandiboina, Skylar Knickerbocker, Neal Hawkins, Jonathan Wood, Anuj Sharma</dc:creator>
    </item>
    <item>
      <title>Exploropleth: exploratory analysis of data binning methods in choropleth maps</title>
      <link>https://arxiv.org/abs/2511.20655</link>
      <description>arXiv:2511.20655v1 Announce Type: cross 
Abstract: When creating choropleth maps, mapmakers often bin (i.e., group, classify) quantitative data values into groups to help show that certain areas fall within a similar range of values. For instance, a mapmaker may divide counties into groups of high, middle, and low life expectancy (measured in years). It is well known that different binning methods (e.g., natural breaks, quantile) yield different groupings, meaning the same data can be presented differently depending on how it is divided into bins. To help guide a wide variety of users, we present a new, open source, web-based, geospatial visualization tool, Exploropleth, that lets users interact with a catalog of established data binning methods, and subsequently compare, customize, and export custom maps. This tool advances the state of the art by providing multiple binning methods in one view and supporting administrative unit reclassification on-the-fly. We interviewed 16 cartographers and geographic information systems (GIS) experts from 13 government organizations, non-government organizations (NGOs), and federal agencies who identified opportunities to integrate Exploropleth into their existing mapmaking workflow, and found that the tool has potential to educate students as well as mapmakers with varying levels of experience. Exploropleth is open-source and publicly available at https://exploropleth.github.io.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20655v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Arpit Narechania, Alex Endert, Clio Andris</dc:creator>
    </item>
    <item>
      <title>Conformal Safety Monitoring for Flight Testing: A Case Study in Data-Driven Safety Learning</title>
      <link>https://arxiv.org/abs/2511.20811</link>
      <description>arXiv:2511.20811v1 Announce Type: cross 
Abstract: We develop a data-driven approach for runtime safety monitoring in flight testing, where pilots perform maneuvers on aircraft with uncertain parameters. Because safety violations can arise unexpectedly as a result of these uncertainties, pilots need clear, preemptive criteria to abort the maneuver in advance of safety violation. To solve this problem, we use offline stochastic trajectory simulation to learn a calibrated statistical model of the short-term safety risk facing pilots. We use flight testing as a motivating example for data-driven learning/monitoring of safety due to its inherent safety risk, uncertainty, and human-interaction. However, our approach consists of three broadly-applicable components: a model to predict future state from recent observations, a nearest neighbor model to classify the safety of the predicted state, and classifier calibration via conformal prediction. We evaluate our method on a flight dynamics model with uncertain parameters, demonstrating its ability to reliably identify unsafe scenarios, match theoretical guarantees, and outperform baseline approaches in preemptive classification of risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20811v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aaron O. Feldman, D. Isaiah Harp, Joseph Duncan, Mac Schwager</dc:creator>
    </item>
    <item>
      <title>A statistical framework for comparing epidemic forests</title>
      <link>https://arxiv.org/abs/2511.20819</link>
      <description>arXiv:2511.20819v1 Announce Type: cross 
Abstract: Inferring who infected whom in an outbreak is essential for characterising transmission dynamics and guiding public health interventions. However, this task is challenging due to limited surveillance data and the complexity of immunological and social interactions. Instead of a single definitive transmission tree, epidemiologists often consider multiple plausible trees forming \textit{epidemic forests}. Various inference methods and assumptions can yield different epidemic forests, yet no formal test exists to assess whether these differences are statistically significant. We propose such a framework using a chi-square test and permutational multivariate analysis of variance (PERMANOVA). We assessed each method's ability to distinguish simulated epidemic forests generated under different offspring distributions. While both methods achieved perfect specificity for forests with 100+ trees, PERMANOVA consistently outperformed the chi-square test in sensitivity across all epidemic and forest sizes. Implemented in the R package \textit{mixtree}, we provide the first statistical framework to robustly compare epidemic forests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.20819v1</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cyril Geismar, Peter J. White, Anne Cori, Thibaut Jombar</dc:creator>
    </item>
    <item>
      <title>How to Correctly Report LLM-as-a-Judge Evaluations</title>
      <link>https://arxiv.org/abs/2511.21140</link>
      <description>arXiv:2511.21140v1 Announce Type: cross 
Abstract: Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21140v1</guid>
      <category>cs.LG</category>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Chungpa Lee, Thomas Zeng, Jongwon Jeong, Jy-yong Sohn, Kangwook Lee</dc:creator>
    </item>
    <item>
      <title>Portfolio Optimization via Transfer Learning</title>
      <link>https://arxiv.org/abs/2511.21221</link>
      <description>arXiv:2511.21221v1 Announce Type: cross 
Abstract: Recognizing that asset markets generally exhibit shared informational characteristics, we develop a portfolio strategy based on transfer learning that leverages cross-market information to enhance the investment performance in the market of interest by forward validation. Our strategy asymptotically identifies and utilizes the informative datasets, selectively incorporating valid information while discarding the misleading information. This enables our strategy to achieve the maximum Sharpe ratio asymptotically. The promising performance is demonstrated by numerical studies and case studies of two portfolios: one consisting of stocks dual-listed in A-shares and H-shares, and another comprising equities from various industries of the United States.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21221v1</guid>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kexin Wang, Xiaomeng Zhang, Xinyu Zhang</dc:creator>
    </item>
    <item>
      <title>Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling</title>
      <link>https://arxiv.org/abs/2511.21636</link>
      <description>arXiv:2511.21636v1 Announce Type: cross 
Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21636v1</guid>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Peter S. Hovmand, Kari O'Donnell, Callie Ogland-Hand, Brian Biroscak, Douglas D. Gunzler</dc:creator>
    </item>
    <item>
      <title>Some statistical aspects of the Covid-19 response</title>
      <link>https://arxiv.org/abs/2409.06473</link>
      <description>arXiv:2409.06473v3 Announce Type: replace 
Abstract: This paper discusses some statistical aspects of the U.K. Covid-19 pandemic response, focussing particularly on cases where we believe that a statistically questionable approach or presentation has had a substantial impact on public perception, or government policy, or both. We discuss the presentation of statistics relating to Covid risk, and the risk of the response measures, arguing that biases tended to operate in opposite directions, overplaying Covid risk and underplaying the response risks. We also discuss some issues around presentation of life loss data, excess deaths and the use of case data. The consequences of neglect of most individual variability from epidemic models, alongside the consequences of some other statistically important omissions are also covered. Finally the evidence for full stay at home lockdowns having been necessary to reverse waves of infection is examined, with new analyses provided for a number of European countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06473v3</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssa/qnaf092</arxiv:DOI>
      <dc:creator>Simon N. Wood, Ernst C. Wit, Paul M. McKeigue, Danshu Hu, Beth Flood, Lauren Corcoran, Thea Abou Jawad</dc:creator>
    </item>
    <item>
      <title>Estimating Interpretable Heterogeneous Treatment Effect with Causal Subgroup Discovery in Survival Outcomes</title>
      <link>https://arxiv.org/abs/2409.19241</link>
      <description>arXiv:2409.19241v4 Announce Type: replace-cross 
Abstract: Estimating heterogeneous treatment effect (HTE) for survival outcomes has gained increasing attention, as it captures the variation in treatment efficacy across patients or subgroups in delaying disease progression. However, most existing methods focus on post-hoc subgroup identification rather than simultaneously estimating HTE and selecting relevant subgroups. In this paper, we propose an interpretable HTE estimation framework that integrates three meta-learners that simultaneously estimate CATE for survival outcomes and identify predictive subgroups. We evaluated the performance of our method through comprehensive simulation studies across various randomized clinical trial (RCT) settings. Additionally, we demonstrated its application in a large RCT for age-related macular degeneration (AMD), a polygenic progressive eye disease, to estimate the HTE of an antioxidant and mineral supplement on time-to-AMD progression and to identify genetics-based subgroups with enhanced treatment effects. Our method offers a direct interpretation of the estimated HTE and provides evidence to support precision healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.19241v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Na Bo, Ying Ding</dc:creator>
    </item>
    <item>
      <title>Robust Causal Inference for EHR-based Studies of Point Exposures with Missingness in Eligibility Criteria</title>
      <link>https://arxiv.org/abs/2504.16230</link>
      <description>arXiv:2504.16230v2 Announce Type: replace-cross 
Abstract: Missingness in variables that define study eligibility criteria is a seldom addressed challenge in electronic health record (EHR)-based settings. It is typically the case that patients with incomplete eligibility information are excluded from analysis without consideration of (implicit) assumptions that are being made, leaving study conclusions subject to potential selection bias. In an effort to ascertain eligibility for more patients, researchers may look back further in time prior to study baseline, and in using outdated values of eligibility-defining covariates may inappropriately be including individuals who, unbeknownst to the researcher, fail to meet eligibility at baseline. To the best of our knowledge, however, very little work has been done to mitigate these concerns. We propose a robust and efficient estimator of the causal average treatment effect on the treated, defined in the study eligible population, in cohort studies where eligibility-defining covariates are missing at random. The approach facilitates the use of flexible machine-learning strategies for component nuisance functions while maintaining appropriate convergence rates for valid asymptotic inference. This method is directly motivated by, and applied throughout to EHR data from Kaiser Permanente to analyze differences between two common bariatric surgical interventions for long-term weight and glycemic outcomes among a cohort of severely obese patients with type II diabetes mellitus.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.16230v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luke Benz, Rajarshi Mukherjee, Rui Wang, David Arterburn, Heidi Fischer, Catherine Lee, Susan M. Shortreed, Sebastien Haneuse, Alexander W. Levis</dc:creator>
    </item>
    <item>
      <title>Multivariate Spatio-temporal Modelling for Completing Cancer Registries and Forecasting Incidence</title>
      <link>https://arxiv.org/abs/2507.21714</link>
      <description>arXiv:2507.21714v2 Announce Type: replace-cross 
Abstract: Cancer data, particularly cancer incidence and mortality, are fundamental to understand the cancer burden, to set targets for cancer control and to evaluate the evolution of the implementation of a cancer control policy. However, the complexity of data collection, classification, validation and processing result in cancer incidence figures often lagging two to three years behind the calendar year. In response, national or regional population-based cancer registries (PBCRs) are increasingly interested in methods for forecasting cancer incidence. However, in many countries there is an additional difficulty in projecting cancer incidence as regional registries are usually not established in the same year and therefore cancer incidence data series between different regions of a country are not harmonised over time. This study addresses the challenge of forecasting cancer incidence with incomplete data at both regional and national levels. To achieve this, we propose the use of multivariate spatio-temporal shared component models that jointly model mortality data and available cancer incidence data. We evaluate the performance of these multivariate models using lung cancer incidence data and the corresponding number of deaths reported in England for the period 2001-2019. Model performance was assessed using different predictive measures to select the best model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.21714v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2025.100944</arxiv:DOI>
      <arxiv:journal_reference>Retegui, G., Etxeberria, J., &amp; Ugarte, M. D. (2026). Multivariate spatio-temporal modelling for completing cancer registries and forecasting incidence. Spatial Statistics, 71</arxiv:journal_reference>
      <dc:creator>Garazi Retegui, Jaione Etxeberria, Mar\'ia Dolores Ugarte</dc:creator>
    </item>
    <item>
      <title>"Rich-Get-Richer"? Platform Attention and Earnings Inequality using Patreon Earnings Data</title>
      <link>https://arxiv.org/abs/2509.26523</link>
      <description>arXiv:2509.26523v2 Announce Type: replace-cross 
Abstract: Using monthly Patreon earnings, we quantify how platform attention algorithms shape earnings concentration across creator economies. Patreon is a tool for creators to monetize additional content from loyal subscribers but offers little native distribution, so its earnings proxy well for the attention creators capture on external platforms (Instagram, Twitch, YouTube, Twitter/X, Facebook, and ``Patreon-only''). Fitting power-law tails to test for a highly unequal earnings distribution, we have three key findings. First, across years and platforms the earnings tail and distribution exhibits a Pareto exponent around $\alpha \approx 2$, closer to concentrated capital income than to labor income and consistent with a compounding, ``rich-get-richer'' dynamic (Barabasi and Albert 1999). Second, when algorithms tilt more attention toward the top, the gains are drawn disproportionately from the creator ``middle class''. Third, over time, creator inequality across social media platforms converge toward similarly heavy-tailed (and increasingly concentrated) distributions, plausibly as algorithmic recommendations rises in importance relative to user-filtered content via the social graph. While our Patreon-sourced data represents a small subset of total creator earnings on these platforms, it provides unique insight into the cross-platform algorithmic effects on earnings concentration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.26523v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 27 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilan Strauss, Jangho Yang, Mariana Mazzucato</dc:creator>
    </item>
  </channel>
</rss>
