<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Jun 2025 04:00:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Stop Chasing the C-index: This Is How We Should Evaluate Our Survival Models</title>
      <link>https://arxiv.org/abs/2506.02075</link>
      <description>arXiv:2506.02075v1 Announce Type: new 
Abstract: We argue that many survival analysis and time-to-event models are incorrectly evaluated. First, we survey many examples of evaluation approaches in the literature and find that most rely on concordance (C-index). However, the C-index only measures a model's discriminative ability and does not assess other important aspects, such as the accuracy of the time-to-event predictions or the calibration of the model's probabilistic estimates. Next, we present a set of key desiderata for choosing the right evaluation metric and discuss their pros and cons. These are tailored to the challenges in survival analysis, such as sensitivity to miscalibration and various censoring assumptions. We hypothesize that the current development of survival metrics conforms to a double-helix ladder, and that model validity and metric validity must stand on the same rung of the assumption ladder. Finally, we discuss the appropriate methods for evaluating a survival model in practice and summarize various viewpoints opposing our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02075v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christian Marius Lillelund, Shi-ang Qi, Russell Greiner, Christian Fischer Pedersen</dc:creator>
    </item>
    <item>
      <title>Function-on-function Differential Regression</title>
      <link>https://arxiv.org/abs/2506.02363</link>
      <description>arXiv:2506.02363v1 Announce Type: new 
Abstract: Function-on-function regression has been a topic of substantial interest due to its broad applicability, where the relation between functional predictor and response is concerned. In this article, we propose a new framework for modeling the regression mapping that extends beyond integral type, motivated by the prevalence of physical phenomena governed by differential relations, which is referred to as function-on-function differential regression. However, a key challenge lies in representing the differential regression operator, unlike functions that can be expressed by expansions. As a main contribution, we introduce a new notion of model identification involving differential operators, defined through their action on functions. Based on this action-aware identification, we are able to develop a regularization method for estimation using operator reproducing kernel Hilbert spaces. Then a goodness-of-fit test is constructed, which facilitates model checking for differential regression relations. We establish a Bahadur representation for the regression estimator with various theoretical implications, such as the minimax optimality of the proposed estimator, and the validity and consistency of the proposed test. To illustrate the effectiveness of our method, we conduct simulation studies and an application to a real data example on the thermodynamic energy equation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02363v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tongyu Li, Fang Yao</dc:creator>
    </item>
    <item>
      <title>Joint Modeling for Learning Decision-Making Dynamics in Behavioral Experiments</title>
      <link>https://arxiv.org/abs/2506.02394</link>
      <description>arXiv:2506.02394v1 Announce Type: new 
Abstract: Major depressive disorder (MDD), a leading cause of disability and mortality, is associated with reward-processing abnormalities and concentration issues. Motivated by the probabilistic reward task from the Establishing Moderators and Biosignatures of Antidepressant Response in Clinical Care (EMBARC) study, we propose a novel framework that integrates the reinforcement learning (RL) model and drift-diffusion model (DDM) to jointly analyze reward-based decision-making with response times. To account for emerging evidence suggesting that decision-making may alternate between multiple interleaved strategies, we model latent state switching using a hidden Markov model (HMM). In the ''engaged'' state, decisions follow an RL-DDM, simultaneously capturing reward processing, decision dynamics, and temporal structure. In contrast, in the ''lapsed'' state, decision-making is modeled using a simplified DDM, where specific parameters are fixed to approximate random guessing with equal probability. The proposed method is implemented using a computationally efficient generalized expectation-maximization algorithm with forward-backward procedures. Through extensive numerical studies, we demonstrate that our proposed method outperforms competing approaches under various reward-generating distributions, both with and without strategy switching. When applied to the EMBARC study, our framework reveals that MDD patients exhibit lower overall engagement than healthy controls and experience longer decision times when they do engage. Additionally, we show that neuroimaging measures of brain activities are associated with decision-making characteristics in the ''engaged'' state but not in the ''lapsed'' state, providing evidence of brain-behavioral association specific to the ''engaged'' state.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02394v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuan Bian, Xingche Guo, Yuanjia Wang</dc:creator>
    </item>
    <item>
      <title>Testing for large-dimensional covariance matrix under differential privacy</title>
      <link>https://arxiv.org/abs/2506.02410</link>
      <description>arXiv:2506.02410v1 Announce Type: new 
Abstract: The increasing prevalence of high-dimensional data across various applications has raised significant privacy concerns in statistical inference. In this paper, we propose a differentially private integrated statistic for testing large-dimensional covariance structures, enabling accurate statistical insights while safeguarding privacy. First, we analyze the global sensitivity of sample eigenvalues for sub-Gaussian populations, where our method bypasses the commonly assumed boundedness of data covariates. For sufficiently large sample size, the privatized statistic guarantees privacy with high probability. Furthermore, when the ratio of dimension to sample size, $d/n \to y \in (0, \infty)$, the privatized test is asymptotically distribution-free with well-known critical values, and detects the local alternative hypotheses distinct from the null at the fastest rate of $1/\sqrt{n}$. Extensive numerical studies on synthetic and real data showcase the validity and powerfulness of our proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02410v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiwei Sang, Yicheng Zeng, Xuehu Zhu, Shurong Zheng</dc:creator>
    </item>
    <item>
      <title>Prenatal phthalate exposures and adiposity outcomes trajectories: a multivariate Bayesian factor regression approach</title>
      <link>https://arxiv.org/abs/2506.02518</link>
      <description>arXiv:2506.02518v1 Announce Type: new 
Abstract: We aim to assess the longitudinal effects of prenatal exposure to phthalates on the risk of childhood obesity in children aged 4 to 7, with potential time-varying and sex-specific effects. Multiple body-composition-related outcomes, such as BMI z-score, fat mass percentage, and waist circumference, are available in the data. Existing chemical mixture analyses often look at these outcomes individually due to the limited availability of multivariate models for mixture exposures. We propose a multivariate Bayesian factor regression that handles multicollinearity in chemical exposures and borrows information across highly correlated outcomes to improve estimation efficiency. We demonstrate the proposed method's utility through simulation studies and an analysis of data from the Mount Sinai Children's Environmental Health Study. We find the associations between prenatal phthalate exposures and adiposity outcomes in male children to be negative at early ages but to become positive as the children get older.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02518v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Phuc H. Nguyen, Stephanie M. Engel, Amy H. Herring</dc:creator>
    </item>
    <item>
      <title>Variable Selection in Functional Linear Cox Model</title>
      <link>https://arxiv.org/abs/2506.02524</link>
      <description>arXiv:2506.02524v1 Announce Type: new 
Abstract: Modern biomedical studies frequently collect complex, high-dimensional physiological signals using wearables and sensors along with time-to-event outcomes, making efficient variable selection methods crucial for interpretation and improving the accuracy of survival models. We propose a novel variable selection method for a functional linear Cox model with multiple functional and scalar covariates measured at baseline. We utilize a spline-based semiparametric estimation approach for the functional coefficients and a group minimax concave type penalty (MCP), which effectively integrates smoothness and sparsity into the estimation of functional coefficients. An efficient group descent algorithm is used for optimization, and an automated procedure is provided to select optimal values of the smoothing and sparsity parameters. Through simulation studies, we demonstrate the method's ability to perform accurate variable selection and estimation. The method is applied to 2003-06 cohort of the National Health and Nutrition Examination Survey (NHANES) data, identifying the key temporally varying distributional patterns of physical activity and demographic predictors related to all-cause mortality. Our analysis sheds light on the intricate association between daily distributional patterns of physical activity and all-cause mortality among older US adults.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02524v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanzhen Yue, Stella Self, Yichao Wu, Jiajia Zhang, Rahul Ghosal</dc:creator>
    </item>
    <item>
      <title>Generalized Labeled Multi-Bernoulli Filters and Multitarget-Correlation Models</title>
      <link>https://arxiv.org/abs/2506.02772</link>
      <description>arXiv:2506.02772v1 Announce Type: new 
Abstract: The generalized labeled multi-Bernoulli (GLMB) filter is a theoretically rigorous Bayes-optimal multitarget tracking algorithm with computationally tractable implementations, based on labeled random finite set (LRFS) theory. It presumes that multitarget populations can be approximated using GLMB multitarget probability density functions (p.d.f.'s), which consist of weighted hypotheses regarding the current target-states. A special case of the GLMB p.d.f.-the LMB p.d.f.-presumes that the targets are statistically independent. This paper demonstrates that a) GLMB p.d.f.'s can be interpreted as straightforward generalizations of LMB p.d.f.'s to statistically correlated target populations, given an implicit presumption of "simple labeled correlation" (SLC) models of multitarget correlation; b) the GLMB filter can be reformulated as a SLC-GLMB filter; and c) SLC models seem primarily appropriate for target clusters consisting of small numbers of closely-spaced targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02772v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ronald Mahler</dc:creator>
    </item>
    <item>
      <title>Localized Functional Principal Component Analysis Based on Covariance Structure</title>
      <link>https://arxiv.org/abs/2506.02836</link>
      <description>arXiv:2506.02836v1 Announce Type: new 
Abstract: Functional principal component analysis (FPCA) is a widely used technique in functional data analysis for identifying the primary sources of variation in a sample of random curves. The eigenfunctions obtained from standard FPCA typically have non-zero support across the entire domain. In applications, however, it is often desirable to analyze eigenfunctions that are non-zero only on specific portions of the original domain-and exhibit zero regions when little is contributed to a specific direction of variability-allowing for easier interpretability. Our method identifies sparse characteristics of the underlying stochastic process and derives localized eigenfunctions by mirroring these characteristics without explicitly enforcing sparsity. Specifically, we decompose the stochastic process into uncorrelated sub-processes, each supported on disjoint intervals. Applying FPCA to these sub-processes yields localized eigenfunctions that are naturally orthogonal. In contrast, approaches that enforce localization through penalization must additionally impose orthogonality. Moreover, these approaches can suffer from over-regularization, resulting in eigenfunctions and eigenvalues that deviate from the inherent structure of their population counterparts, potentially misrepresenting data characteristics. Our approach avoids these issues by preserving the inherent structure of the data. Moreover, since the sub-processes have disjoint supports, the eigenvalues associated to the localized eigenfunctions allow for assessing the importance of each sub-processes in terms of its contribution to the total explained variance. We illustrate the effectiveness of our method through simulations and real data applications. Supplementary material for this article is available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02836v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria Laura Battagliola, Jan O. Bauer</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Inference for Adaptive Experiments</title>
      <link>https://arxiv.org/abs/2506.02881</link>
      <description>arXiv:2506.02881v1 Announce Type: new 
Abstract: Multi-arm bandit experimental designs are increasingly being adopted over standard randomized trials due to their potential to improve outcomes for study participants, enable faster identification of the best-performing options, and/or enhance the precision of estimating key parameters. Current approaches for inference after adaptive sampling either rely on asymptotic normality under restricted experiment designs or underpowered martingale concentration inequalities that lead to weak power in practice. To bypass these limitations, we propose a simulation-based approach for conducting hypothesis tests and constructing confidence intervals for arm specific means and their differences. Our simulation-based approach uses positively biased nuisances to generate additional trajectories of the experiment, which we call \textit{simulation with optimism}. Using these simulations, we characterize the distribution potentially non-normal sample mean test statistic to conduct inference. We provide guarantees for (i) asymptotic type I error control, (ii) convergence of our confidence intervals, and (iii) asymptotic strong consistency of our estimator over a wide variety of common bandit designs. Our empirical results show that our approach achieves the desired coverage while reducing confidence interval widths by up to 50%, with drastic improvements for arms not targeted by the design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02881v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian M Cho, Aur\'elien Bibaut, Nathan Kallus</dc:creator>
    </item>
    <item>
      <title>Power Enhancement of Permutation-Augmented Partial-Correlation Tests via Fixed-Row Permutations</title>
      <link>https://arxiv.org/abs/2506.02906</link>
      <description>arXiv:2506.02906v1 Announce Type: new 
Abstract: Permutation-based partial-correlation tests guarantee finite-sample Type I error control under any fixed design and exchangeable noise, yet their power can collapse when the permutation-augmented design aligns too closely with the covariate of interest. We remedy this by fixing a design-driven subset of rows and permuting only the remainder. The fixed rows are chosen by a greedy algorithm that maximizes a lower bound on power. This strategy reduces covariate-permutation collinearity while preserving worst-case Type I error control. Simulations confirm that this refinement maintains nominal size and delivers substantial power gains over original unrestricted permutations, especially in high-collinearity regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02906v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tianyi Wang, Guanghui Wang, Zhaojun Wang, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Incorporating Correlated Nugget Effects in Multivariate Spatial Models: An Application to Argo Ocean Data</title>
      <link>https://arxiv.org/abs/2506.03042</link>
      <description>arXiv:2506.03042v1 Announce Type: new 
Abstract: Accurate analysis of global oceanographic data, such as temperature and salinity profiles from the Argo program, requires geostatistical models capable of capturing complex spatial dependencies. This study introduces Gaussian and non-Gaussian hierarchical multivariate Mat\'ern-SPDE models with correlated nugget effects to account for small-scale variability and measurement error correlations. Using simulations and Argo data, we demonstrate that incorporating correlated nugget effects significantly improves the accuracy of parameter estimation and spatial prediction in both Gaussian and non-Gaussian multivariate spatial processes. When applied to global ocean temperature and salinity data, our model yields lower correlation estimates between fields compared to models that assume independent noise. This suggests that traditional models may overestimate the underlying field correlation. By separating these effects, our approach captures fine-scale oceanic patterns more effectively. These findings show the importance of relaxing the assumption of independent measurement errors in multivariate hierarchical models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03042v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damilya Saduakhas, David Bolin, Xiaotian Jin, Alexandre B. Simas, Jonas Wallin</dc:creator>
    </item>
    <item>
      <title>An IPCW Adjusted Win Statistics Approach in Clinical Trials Incorporating Equivalence Margins to Define Ties</title>
      <link>https://arxiv.org/abs/2506.03050</link>
      <description>arXiv:2506.03050v1 Announce Type: new 
Abstract: In clinical trials, multiple outcomes of different priorities commonly occur as the patient's response may not be adequately characterized by a single outcome. Win statistics are appealing summary measures for between-group difference at more than one endpoint. When defining the result of pairwise comparisons of a time-to-event endpoint, it is desirable to allow ties to account for incomplete follow-up and not clinically meaningful difference in endpoints of interest. In this paper, we propose a class of win statistics for time-to-event endpoints with a user-specified equivalence margin. These win statistics are identifiable in the presence of right-censoring and do not depend on the censoring distribution. We then develop estimation and inference procedures for the proposed win statistics based on inverse-probability-of-censoring {weighting} (IPCW) adjustment to handle right-censoring. We conduct extensive simulations to investigate the operational characteristics of the proposed procedure in the finite sample setting. A real oncology trial is used to illustrate the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03050v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ying Cui, Bo Huang, Gaohong Dong, Ryuji Uozumi, Lu Tian</dc:creator>
    </item>
    <item>
      <title>Constructing Evidence-Based Tailoring Variables for Adaptive Interventions</title>
      <link>https://arxiv.org/abs/2506.03054</link>
      <description>arXiv:2506.03054v1 Announce Type: new 
Abstract: Background: An adaptive intervention (ADI) uses individual information in order to select treatment, to improve effectiveness while reducing cost and burden. ADIs require tailoring variables: person- and potentially time-specific information used to decide whether and how to deliver treatment. Specifying a tailoring variable for an intervention requires specifying what to measure, when to measure it, when to make the resulting decisions, and what cutoffs should be used in making those decisions. This involves tradeoffs between specificity versus sensitivity, and between waiting for sufficient information versus intervening quickly. These questions are causal and prescriptive (what should be done and when), not merely predictive (what would happen if current conditions persist).
  Purpose: There is little specific guidance in the literature on how to empirically choose tailoring variables, including cutoffs, measurement times, and decision times. Methods: We review possible approaches for comparing potential tailoring variables and propose a framework for systematically developing tailoring variables.
  Results: Although secondary observational data can be used to select tailoring variables, additional assumptions are needed. A specifically designed randomized experiment for optimization purposes (an optimization randomized clinical trial or ORCT), in the form of a multi-arm randomized trial, sequential multiple assignment randomized trial, a factorial experiment, or hybrid among them, may provide a more direct way to answer these questions.
  Conclusions: Using randomization directly to inform tailoring variables would provide the most direct causal evidence, but designing a trial to compare both tailoring variables and treatments adds complexity; further methodological research is warranted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03054v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>John J. Dziak, Inbal Nahum-Shani</dc:creator>
    </item>
    <item>
      <title>Synergy-Informed Design of Platform Trials for Combination Therapies</title>
      <link>https://arxiv.org/abs/2506.03086</link>
      <description>arXiv:2506.03086v1 Announce Type: new 
Abstract: Combination drug therapies hold significant promise for enhancing treatment efficacy, particularly in fields such as oncology, immunotherapy, and infectious diseases. However, designing clinical trials for these regimens poses unique statistical challenges due to multiple hypothesis testing, shared control groups, and overlapping treatment components that induce complex correlation structures. In this paper, we develop a novel statistical framework tailored for early-phase translational combination therapy trials, with a focus on platform trial designs. Our methodology introduces a generalized Dunnett's procedure that controls false positive rates by accounting for the correlations between treatment arms. Additionally, we propose strategies for power analysis and sample size optimization that leverage preclinical data to estimate effect sizes, synergy parameters, and inter-arm correlations. Simulation studies demonstrate that our approach not only controls various false positive metrics under diverse trial scenarios but also informs optimal allocation ratios to maximize power. A real-data application further illustrates the integration of translational preclinical insights into the clinical trial design process. An open-source R package is provided to support the application of our methods in practice. Overall, our framework offers statistically rigorous guidance for the design of early-phase combination therapy trials, aiming to enhance the efficiency of the bench-to-bedside transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03086v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nan Miles Xi, Man Mandy Jin, Lin Wang, Xin Huang</dc:creator>
    </item>
    <item>
      <title>Two-Phase Treatment with Noncompliance: Identifying the Cumulative Average Treatment Effect via Multisite Instrumental Variables</title>
      <link>https://arxiv.org/abs/2506.03104</link>
      <description>arXiv:2506.03104v1 Announce Type: new 
Abstract: In evaluating a multi-phase intervention, the cumulative average treatment effect (ATE) is often the causal estimand of key interest. Yet some individuals who do not respond well to the Phase-I treatment may subsequently display noncompliant behaviors. However, noncompliance tends to be constrained by the stochastic availability of slots under the alternative treatment condition in Phase II, which makes the notion of the "complier average treatment effect" problematic. Moreover, the Phase-I treatment is expected to affect an individual's potential outcomes through additional pathways that violate the exclusion restriction. Extending an instrumental variable (IV) strategy for multisite trials, we clarify conditions for identifying the cumulative ATE of a two-phase treatment by employing the random assignment of the Phase-I treatment as the IV. Our strategy relaxes the exclusion restriction and the sequential ignorability in their conventional forms. We evaluate the performance of the new strategy through simulations. Reanalyzing data from the Tennessee class size study in which students and teachers were assigned at random to either a small or a regular class in kindergarten (Phase I) yet noncompliance occurred in Grade 1 (Phase II), we estimate the cumulative ATE of receiving two years of instruction in a small class versus a regular class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03104v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guanglei Hong, Xu Qin, Zhengyan Xu, Fan Yang</dc:creator>
    </item>
    <item>
      <title>Party Ideologies and Political Polarization-Driven Conflicts: A Study of the Global South</title>
      <link>https://arxiv.org/abs/2506.02004</link>
      <description>arXiv:2506.02004v1 Announce Type: cross 
Abstract: Post-World War II armed conflicts have often been viewed with higher scrutiny in order to avoid a full-scale global war. This scrutiny has led to the establishment of determinants of war such as poverty, inequalities, literacy, and many more. There is a gap that exists in probing countries in the Global South for political party fragmentation and examining ideology-driven polarization's effect on armed conflicts. This paper fills this gap by asking the question: How does political identity-induced polarization affect conflicts in the Global South region? Polarization indices are created based on socially relevant issues and party stances from the V-Party Dataset. Along with control variables, they are tested against the response variables conflict frequency and conflict severity created from the UCDP (Uppsala Conflict Data Program). Through Chow's test, Regional Structural Breaks are found between regions when accounting for polarization-conflict dynamics. A multilevel mixed effects modelling approach is used to create region-specific models to find what types of polarization affect conflict in different geographies and their adherence to normative current developments. The paper highlights that vulnerable regions of the world are prone to higher polarization-induced violence. Modelling estimates indicate polarization of party credo on Minority Rights, Rejection of Political Violence, Religious Principles, and Political Pluralism are strong proponents of cultivated violence. The Global South's inhibitions and slow progress towards development are caused by hindrances from armed conflicts; this paper's results show self-inflicted political instability and fragmentation's influence on these events, making the case for urgency in addressing and building inter-group homogeneity and tolerance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02004v1</guid>
      <category>physics.soc-ph</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shreyansh Padarha</dc:creator>
    </item>
    <item>
      <title>Evaluating the Unseen Capabilities: How Many Theorems Do LLMs Know?</title>
      <link>https://arxiv.org/abs/2506.02058</link>
      <description>arXiv:2506.02058v1 Announce Type: cross 
Abstract: Accurate evaluation of large language models (LLMs) is crucial for understanding their capabilities and guiding their development. However, current evaluations often inconsistently reflect the actual capacities of these models. In this paper, we demonstrate that one of many contributing factors to this \textit{evaluation crisis} is the oversight of unseen knowledge -- information encoded by LLMs but not directly observed or not yet observed during evaluations. We introduce KnowSum, a statistical framework designed to provide a more comprehensive assessment by quantifying the unseen knowledge for a class of evaluation tasks. KnowSum estimates the unobserved portion by extrapolating from the appearance frequencies of observed knowledge instances. We demonstrate the effectiveness and utility of KnowSum across three critical applications: estimating total knowledge, evaluating information retrieval effectiveness, and measuring output diversity. Our experiments reveal that a substantial volume of knowledge is omitted when relying solely on observed LLM performance. Importantly, KnowSum yields significantly different comparative rankings for several common LLMs based on their internal knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02058v1</guid>
      <category>cs.CL</category>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Li, Jiayi Xin, Qi Long, Weijie J. Su</dc:creator>
    </item>
    <item>
      <title>Learning Treatment Representations for Downstream Instrumental Variable Regression</title>
      <link>https://arxiv.org/abs/2506.02200</link>
      <description>arXiv:2506.02200v1 Announce Type: cross 
Abstract: Traditional instrumental variable (IV) estimators face a fundamental constraint: they can only accommodate as many endogenous treatment variables as available instruments. This limitation becomes particularly challenging in settings where the treatment is presented in a high-dimensional and unstructured manner (e.g. descriptions of patient treatment pathways in a hospital). In such settings, researchers typically resort to applying unsupervised dimension reduction techniques to learn a low-dimensional treatment representation prior to implementing IV regression analysis. We show that such methods can suffer from substantial omitted variable bias due to implicit regularization in the representation learning step. We propose a novel approach to construct treatment representations by explicitly incorporating instrumental variables during the representation learning process. Our approach provides a framework for handling high-dimensional endogenous variables with limited instruments. We demonstrate both theoretically and empirically that fitting IV models on these instrument-informed representations ensures identification of directions that optimize outcome prediction. Our experiments show that our proposed methodology improves upon the conventional two-stage approaches that perform dimension reduction without incorporating instrument information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02200v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiangyi Lin, Hui Lan, Vasilis Syrgkanis</dc:creator>
    </item>
    <item>
      <title>Assumption-free stability for ranking problems</title>
      <link>https://arxiv.org/abs/2506.02257</link>
      <description>arXiv:2506.02257v1 Announce Type: cross 
Abstract: In this work, we consider ranking problems among a finite set of candidates: for instance, selecting the top-$k$ items among a larger list of candidates or obtaining the full ranking of all items in the set. These problems are often unstable, in the sense that estimating a ranking from noisy data can exhibit high sensitivity to small perturbations. Concretely, if we use data to provide a score for each item (say, by aggregating preference data over a sample of users), then for two items with similar scores, small fluctuations in the data can alter the relative ranking of those items. Many existing theoretical results for ranking problems assume a separation condition to avoid this challenge, but real-world data often contains items whose scores are approximately tied, limiting the applicability of existing theory. To address this gap, we develop a new algorithmic stability framework for ranking problems, and propose two novel ranking operators for achieving stable ranking: the \emph{inflated top-$k$} for the top-$k$ selection problem and the \emph{inflated full ranking} for ranking the full list. To enable stability, each method allows for expressing some uncertainty in the output. For both of these two problems, our proposed methods provide guaranteed stability, with no assumptions on data distributions and no dependence on the total number of candidates to be ranked. Experiments on real-world data confirm that the proposed methods offer stability without compromising the informativeness of the output.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.02257v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiting Liang, Jake A. Soloff, Rina Foygel Barber, Rebecca Willett</dc:creator>
    </item>
    <item>
      <title>On the Need to Align Intent and Implementation in Uncertainty Quantification for Machine Learning</title>
      <link>https://arxiv.org/abs/2506.03037</link>
      <description>arXiv:2506.03037v1 Announce Type: cross 
Abstract: Quantifying uncertainties for machine learning (ML) models is a foundational challenge in modern data analysis. This challenge is compounded by at least two key aspects of the field: (a) inconsistent terminology surrounding uncertainty and estimation across disciplines, and (b) the varying technical requirements for establishing trustworthy uncertainties in diverse problem contexts. In this position paper, we aim to clarify the depth of these challenges by identifying these inconsistencies and articulating how different contexts impose distinct epistemic demands. We examine the current landscape of estimation targets (e.g., prediction, inference, simulation-based inference), uncertainty constructs (e.g., frequentist, Bayesian, fiducial), and the approaches used to map between them. Drawing on the literature, we highlight and explain examples of problematic mappings. To help address these issues, we advocate for standards that promote alignment between the \textit{intent} and \textit{implementation} of uncertainty quantification (UQ) approaches. We discuss several axes of trustworthiness that are necessary (if not sufficient) for reliable UQ in ML models, and show how these axes can inform the design and evaluation of uncertainty-aware ML systems. Our practical recommendations focus on scientific ML, offering illustrative cases and use scenarios, particularly in the context of simulation-based inference (SBI).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03037v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shubhendu Trivedi, Brian D. Nord</dc:creator>
    </item>
    <item>
      <title>Iterative Methods for Full-Scale Gaussian Process Approximations for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2405.14492</link>
      <description>arXiv:2405.14492v3 Announce Type: replace 
Abstract: Gaussian processes are flexible probabilistic regression models which are widely used in statistics and machine learning. However, a drawback is their limited scalability to large data sets. To alleviate this, full-scale approximations (FSAs) combine predictive process methods and covariance tapering, thus approximating both global and local structures. We show how iterative methods can be used to reduce computational costs in calculating likelihoods, gradients, and predictive distributions with FSAs. In particular, we introduce a novel preconditioner and show theoretically and empirically that it accelerates the conjugate gradient method's convergence speed and mitigates its sensitivity with respect to the FSA parameters and the eigenvalue structure of the original covariance matrix, and we demonstrate empirically that it outperforms a state-of-the-art pivoted Cholesky preconditioner. Furthermore, we introduce an accurate and fast way to calculate predictive variances using stochastic simulation and iterative methods. In addition, we show how our newly proposed FITC preconditioner can also be used in iterative methods for Vecchia approximations. In our experiments, it outperforms existing state-of-the-art preconditioners for Vecchia approximations. All methods are implemented in a free C++ software library with high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14492v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tim Gyger, Reinhard Furrer, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>How to build your latent Markov model -- the role of time and space</title>
      <link>https://arxiv.org/abs/2406.19157</link>
      <description>arXiv:2406.19157v4 Announce Type: replace 
Abstract: Statistical models that involve latent Markovian state processes have become immensely popular tools for analysing time series and other sequential data. However, the plethora of model formulations, the inconsistent use of terminology, and the various inferential approaches and software packages can be overwhelming to practitioners, especially when they are new to this area. With this review-like paper, we thus aim to provide guidance for both statisticians and practitioners working with latent Markov models by offering a unifying view on what otherwise are often considered separate model classes, from hidden Markov models over state-space models to Markov-modulated Poisson processes. In particular, we provide a roadmap for identifying a suitable latent Markov model formulation given the data to be analysed. Furthermore, we emphasise that it is key to applied work with any of these model classes to understand how recursive techniques exploiting the models' dependence structure can be used for inference. The R package LaMa adapts this unified view and provides an easy-to-use framework for very fast (C++ based) numerical maximum likelihood estimation of any of the models discussed in this paper, allowing users to tailor a latent Markov model to their data using a Lego-type approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19157v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sina Mews, Jan-Ole Koslik, Roland Langrock</dc:creator>
    </item>
    <item>
      <title>On the Selection Stability of Stability Selection and Its Applications</title>
      <link>https://arxiv.org/abs/2411.09097</link>
      <description>arXiv:2411.09097v3 Announce Type: replace 
Abstract: Stability selection is a widely adopted resampling-based framework for high-dimensional variable selection. This paper seeks to broaden the use of an established stability estimator to evaluate the overall stability of the stability selection results, moving beyond single-variable analysis. We suggest that the stability estimator offers two advantages: it can serve as a reference to reflect the robustness of the results obtained, and it can help identify a Pareto optimal regularization value to improve stability. By determining the regularization value, we calibrate key stability selection parameters, namely, the decision-making threshold and the expected number of falsely selected variables, within established theoretical bounds. In addition, the convergence of stability values over successive sub-samples sheds light on the required number of sub-samples addressing a notable gap in prior studies. The \texttt{stabplot} R package is developed to facilitate the use of the methodology featured in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09097v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Nouraie, Samuel Muller</dc:creator>
    </item>
    <item>
      <title>Quadratic Form based Multiple Contrast Tests for Comparison of Group Means</title>
      <link>https://arxiv.org/abs/2411.10121</link>
      <description>arXiv:2411.10121v2 Announce Type: replace 
Abstract: Comparing the mean vectors across different groups is a cornerstone in the realm of multivariate statistics, with quadratic forms commonly serving as test statistics. However, when the overall hypothesis is rejected, identifying specific vector components or determining the groups among which differences exist requires additional investigations. Conversely, employing multiple contrast tests (MCT) allows conclusions about which components or groups contribute to these differences. However, they come with a trade-off, as MCT lose some benefits inherent to quadratic forms. In this paper, we combine both approaches to get a quadratic form based multiple contrast test that leverages the advantages of both. To understand its theoretical properties, we investigate its asymptotic distribution in a semiparametric model. We thereby focus on two common quadratic forms - the Wald-type statistic and the Anova-type statistic - although our findings are applicable to any quadratic form.
  Furthermore, we employ Monte-Carlo and resampling techniques to enhance the test's performance in small sample scenarios. Through an extensive simulation study, we assess the performance of our proposed tests against existing alternatives, highlighting their advantages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10121v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paavo Sattler, Markus Pauly, Merle Munko</dc:creator>
    </item>
    <item>
      <title>Kernel-based estimators for functional causal effects</title>
      <link>https://arxiv.org/abs/2503.05024</link>
      <description>arXiv:2503.05024v4 Announce Type: replace 
Abstract: We propose causal effect estimators based on empirical Fr\'{e}chet means and operator-valued kernels, tailored to functional data spaces. These methods address the challenges of high-dimensionality, sequential ordering, and model complexity while preserving robustness to treatment misspecification. Using structural assumptions, we obtain compact representations of potential outcomes, enabling scalable estimation of causal effects over time and across covariates. We provide both theoretical, regarding the consistency of functional causal effects, as well as empirical comparison of a range of proposed causal effect estimators.
  Applications to binary treatment settings with functional outcomes illustrate the framework's utility in biomedical monitoring, where outcomes exhibit complex temporal dynamics. Our estimators accommodate scenarios with registered covariates and outcomes, aligning them to the Fr\'{e}chet means, as well as cases requiring higher-order representations to capture intricate covariate-outcome interactions. These advancements extend causal inference to dynamic and non-linear domains, offering new tools for understanding complex treatment effects in functional data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.05024v4</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yordan P. Raykov, Hengrui Luo, Justin D. Strait, Wasiur R. KhudaBukhsh</dc:creator>
    </item>
    <item>
      <title>Do Large Language Models (Really) Need Statistical Foundations?</title>
      <link>https://arxiv.org/abs/2505.19145</link>
      <description>arXiv:2505.19145v2 Announce Type: replace 
Abstract: Large language models (LLMs) represent a new paradigm for processing unstructured data, with applications across an unprecedented range of domains. In this paper, we address, through two arguments, whether the development and application of LLMs would genuinely benefit from foundational contributions from the statistics discipline. First, we argue affirmatively, beginning with the observation that LLMs are inherently statistical models due to their profound data dependency and stochastic generation processes, where statistical insights are naturally essential for handling variability and uncertainty. Second, we argue that the persistent black-box nature of LLMs -- stemming from their immense scale, architectural complexity, and development practices often prioritizing empirical performance over theoretical interpretability -- renders closed-form or purely mechanistic analyses generally intractable, thereby necessitating statistical approaches due to their flexibility and often demonstrated effectiveness. To substantiate these arguments, the paper outlines several research areas -- including alignment, watermarking, uncertainty quantification, evaluation, and data mixture optimization -- where statistical methodologies are critically needed and are already beginning to make valuable contributions. We conclude with a discussion suggesting that statistical research concerning LLMs will likely form a diverse ``mosaic'' of specialized topics rather than deriving from a single unifying theory, and highlighting the importance of timely engagement by our statistics community in LLM research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19145v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>Learning to Simulate: Generative Metamodeling via Quantile Regression</title>
      <link>https://arxiv.org/abs/2311.17797</link>
      <description>arXiv:2311.17797v3 Announce Type: replace-cross 
Abstract: Stochastic simulation models effectively capture complex system dynamics but are often too slow for real-time decision-making. Traditional metamodeling techniques learn relationships between simulator inputs and a single output summary statistic, such as the mean or median. These techniques enable real-time predictions without additional simulations. However, they require prior selection of one appropriate output summary statistic, limiting their flexibility in practical applications. We propose a new concept: generative metamodeling. It aims to construct a "fast simulator of the simulator," generating random outputs significantly faster than the original simulator while preserving approximately equal conditional distributions. Generative metamodels enable rapid generation of numerous random outputs upon input specification, facilitating immediate computation of any summary statistic for real-time decision-making. We introduce a new algorithm, quantile-regression-based generative metamodeling (QRGMM), and establish its distributional convergence and convergence rate. Extensive numerical experiments demonstrate QRGMM's efficacy compared to other state-of-the-art generative algorithms in practical real-time decision-making scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17797v3</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>L. Jeff Hong, Yanxi Hou, Qingkai Zhang, Xiaowei Zhang</dc:creator>
    </item>
    <item>
      <title>Self-Organizing State-Space Models with Artificial Dynamics</title>
      <link>https://arxiv.org/abs/2409.08928</link>
      <description>arXiv:2409.08928v5 Announce Type: replace-cross 
Abstract: We consider the problem of performing parameter and state inference in a state-space model (SSM) parametrized by a static parameter $\theta$. A popular idea to address this problem consists of incorporating $\theta$ in the state of the system and allowing its time evolution, modelled as a Markov chain $(\theta_t)_{t\geq 1}$. This proxy model defines a so-called self-organizing SSM (SO-SSM) to which one may apply standard particle filters. However, the practical implementation of this idea in a theoretically justified manner has remained an open problem until now. In this paper we fill this gap and in particular show that theoretically consistent SO-SSMs can be defined such that $\|\mathrm{Var}(\theta_{t+1}|\theta_{t})\|\rightarrow 0$ slowly as $t\rightarrow\infty$. This, in turn, leads to particle filter algorithms for online inference in SSMs which we find to be robust in simulation. We also develop constructions of $(\theta_t)_{t\geq 1}$ and associated theoretical guarantees tailored to the application of SO-SSMs to maximum likelihood estimation in SSMs, leading to novel iterated filtering algorithms. The algorithms developed in this work have the advantage of being simple to implement and to require minimal tuning to perform well.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08928v5</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuan Chen, Mathieu Gerber, Christophe Andrieu, Randal Douc</dc:creator>
    </item>
    <item>
      <title>Theoretical Foundations of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2411.11824</link>
      <description>arXiv:2411.11824v3 Announce Type: replace-cross 
Abstract: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.
  The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11824v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>De-Biasing Structure Function Estimates From Sparse Time Series of the Solar Wind: A Data-Driven Approach</title>
      <link>https://arxiv.org/abs/2412.10053</link>
      <description>arXiv:2412.10053v2 Announce Type: replace-cross 
Abstract: Structure functions, which represent the moments of the increments of a stochastic process, are essential complementary statistics to power spectra for analysing the self-similar behaviour of a time series. However, many real-world environmental datasets, such as those collected by spacecraft monitoring the solar wind, contain gaps, which inevitably corrupt the statistics. The nature of this corruption for structure functions remains poorly understood - indeed, often overlooked. Here we simulate gaps in a large set of magnetic field intervals from Parker Solar Probe in order to characterize the behaviour of the structure function of a sparse time series of solar wind turbulence. We quantify the resultant error with regards to the overall shape of the structure function, and its slope in the inertial range. Noting the consistent underestimation of the true curve when using linear interpolation, we demonstrate the ability of an empirical correction factor to de-bias these estimates. This correction, "learnt" from the data from a single spacecraft, is shown to generalize well to data from a solar wind regime elsewhere in the heliosphere, producing smaller errors, on average, for missing fractions &gt;25%. Given this success, we apply the correction to gap-affected Voyager intervals from the inner heliosheath and local interstellar medium, obtaining spectral indices similar to those from previous studies. This work provides a tool for future studies of fragmented solar wind time series, such as those from Voyager, MAVEN, and OMNI, as well as sparsely-sampled astrophysical and geophysical processes more generally.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.10053v2</guid>
      <category>astro-ph.SR</category>
      <category>physics.space-ph</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 04 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3847/1538-4357/addc6a 10.3847/1538-4357/addc6a 10.3847/1538-4357/addc6a</arxiv:DOI>
      <dc:creator>Daniel Wrench, Tulasi N. Parashar</dc:creator>
    </item>
  </channel>
</rss>
