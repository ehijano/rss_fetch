<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 25 Oct 2024 04:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Assessment of Developmental Dysgraphia Utilising a Display Tablet</title>
      <link>https://arxiv.org/abs/2410.18230</link>
      <description>arXiv:2410.18230v1 Announce Type: new 
Abstract: Even though the computerised assessment of developmental dysgraphia (DD) based on online handwriting processing has increasing popularity, most of the solutions are based on a setup, where a child writes on a paper fixed to a digitizing tablet that is connected to a computer. Although this approach enables the standard way of writing using an inking pen, it is difficult to be administered by children themselves. The main goal of this study is thus to explore, whether the quantitative analysis of online handwriting recorded via a display screen tablet could sufficiently support the assessment of DD as well. For the purpose of this study, we enrolled 144 children (attending the 3rd and 4th class of a primary school), whose handwriting proficiency was assessed by a special education counsellor, and who assessed themselves by the Handwriting Proficiency Screening Questionnaires for Children (HPSQ C). Using machine learning models based on a gradient-boosting algorithm, we were able to support the DD diagnosis with up to 83.6% accuracy. The HPSQ C total score was estimated with a minimum error equal to 10.34 %. Children with DD spent significantly higher time in-air, they had a higher number of pen elevations, a bigger height of on-surface strokes, a lower in-air tempo, and a higher variation in the angular velocity. Although this study shows a promising impact of DD assessment via display tablets, it also accents the fact that modelling of subjective scores is challenging and a complex and data-driven quantification of DD manifestations is needed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18230v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-45461-5_2</arxiv:DOI>
      <arxiv:journal_reference>IGS 2023. Lecture Notes in Computer Science, vol 14285, pp.21-35</arxiv:journal_reference>
      <dc:creator>Jiri Mekyska, Zoltan Galaz, Katarina Safarova, Vojtech Zvoncak, Lukas Cunek, Tomas Urbanek, Jana Marie Havigerova, Jirina Bednarova, Jan Mucha, Michal Gavenciak, Zdenek Smekal, Marcos Faundez-Zanuy</dc:creator>
    </item>
    <item>
      <title>A Pilot Study on Clinician-AI Collaboration in Diagnosing Depression from Speech</title>
      <link>https://arxiv.org/abs/2410.18297</link>
      <description>arXiv:2410.18297v1 Announce Type: new 
Abstract: This study investigates clinicians' perceptions and attitudes toward an assistive artificial intelligence (AI) system that employs a speech-based explainable ML algorithm for detecting depression. The AI system detects depression from vowel-based spectrotemporal variations of speech and generates explanations through explainable AI (XAI) methods. It further provides decisions and explanations at various temporal granularities, including utterance groups, individual utterances, and within each utterance. A small-scale user study was conducted to evaluate users' perceived usability of the system, trust in the system, and perceptions of design factors associated with several elements of the system. Quantitative and qualitative analysis of the collected data indicates both positive and negative aspects that influence clinicians' perception toward the AI. Results from quantitative analysis indicate that providing more AI explanations enhances user trust but also increases system complexity. Qualitative analysis indicates the potential of integrating such systems into the current diagnostic and screening workflow, but also highlights existing limitations including clinicians' reduced familiarity with AI/ML systems and the need for user-friendly and intuitive visualizations of speech information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18297v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kexin Feng, Theodora Chaspari</dc:creator>
    </item>
    <item>
      <title>CAMeleon: Interactively Exploring Craft Workflows in CAD</title>
      <link>https://arxiv.org/abs/2410.18299</link>
      <description>arXiv:2410.18299v1 Announce Type: new 
Abstract: Designers of physical objects make assumptions on the material and fabrication workflow early in the design process. Recovering from bad assumptions is hard, because the design and resulting CAD model are locked-in to those assumptions. We present CAMeleon, a software tool to interactively explore fabrication workflows at any stage of the CAD process.
  CAMeleon's modular architecture allows users to execute their design with different workflows, and preview results. Users can freely explore alternative workflows. CAMeleon's architecture, can be extended with new workflows, increasing the richness of workflows available.
  We implemented five fabrication workflows in CAMeleon. We demonstrate CAMeleon's extensibility through collaboration with six craftsmen whose workflows we replicated. We also implemented workflows of three papers and reflect on the process of these nine extensions. A usability study (n=12) showed that CAMeleon allowed participants to explore and select workflows for their design which they did not know before.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18299v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuo Feng, Yifan Shan, Xuening Wang, Ritik Batra, Thijs Roumen</dc:creator>
    </item>
    <item>
      <title>When Group Spirit Meets Personal Journeys: Exploring Motivational Dynamics and Design Opportunities in Group Therapy</title>
      <link>https://arxiv.org/abs/2410.18329</link>
      <description>arXiv:2410.18329v1 Announce Type: new 
Abstract: Psychotherapy, such as cognitive-behavioral therapy (CBT), is effective in treating various mental disorders. Technology-facilitated mental health therapy improves client engagement through methods like digitization or gamification. However, these innovations largely cater to individual therapy, ignoring the potential of group therapy-a treatment for multiple clients concurrently, which enables individual clients to receive various perspectives in the treatment process and also addresses the scarcity of healthcare practitioners to reduce costs. Notwithstanding its cost-effectiveness and unique social dynamics that foster peer learning and community support, group therapy, such as group CBT, faces the issue of attrition. While existing medical work has developed guidelines for therapists, such as establishing leadership and empathy to facilitate group therapy, understanding about the interactions between each stakeholder is still missing. To bridge this gap, this study examined a group CBT program called the Serigaya Methamphetamine Relapse Prevention Program (SMARPP) as a case study to understand stakeholder coordination and communication, along with factors promoting and hindering continuous engagement in group therapy. In-depth interviews with eight facilitators and six former clients from SMARPP revealed the motivators and demotivators for facilitator-facilitator, client-client, and facilitator-client communications. Our investigation uncovers the presence of discernible conflicts between clients' intrapersonal motivation as well as interpersonal motivation in the context of group therapy through the lens of self-determination theory. We discuss insights and research opportunities for the HCI community to mediate such tension and enhance stakeholder communication in future technology-assisted group therapy settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18329v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shixian Geng, Ginshi Shimojima, Chi-Lan Yang, Zefan Sramek, Shunpei Norihama, Ayumi Takano, Simo Hosio, Koji Yatani</dc:creator>
    </item>
    <item>
      <title>The Senses Considered as One Perceptual System</title>
      <link>https://arxiv.org/abs/2410.18502</link>
      <description>arXiv:2410.18502v1 Announce Type: new 
Abstract: J. J. Gibson (1966) rejected many classical assumptions about perception but retained 1 that dates back to classical antiquity: the assumption of separate senses. We suggest that Gibson's retention of this assumption compromised his novel concept of perceptual systems. We argue that lawful, 1:1 specification of the animal--environment interaction, which is necessary for perception to be direct, cannot exist in individual forms of ambient energy, such as light, or sound. We argue that specification exists exclusively in emergent, higher order patterns that extend across different forms of ambient energy. These emergent, higher order patterns constitute the global array. If specification exists exclusively in the global array, then direct perception cannot be based upon detection of patterns that are confined to individual forms of ambient energy and, therefore, Gibson's argument for the existence of several distinct perceptual systems cannot be correct. We argue that the senses function as a single, irreducible perceptual system that is sensitive exclusively to patterns in the global array. That is, rather than distinct perceptual systems there exists only 1 perceptual system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18502v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1080/10407413.2017.1331116</arxiv:DOI>
      <arxiv:journal_reference>Ecological Psychology, 2017, James J. Gibson's 1966 Book: 50 Years Later - Part 2, 29 (3), pp.165-197</arxiv:journal_reference>
      <dc:creator>Thomas Stoffregen (UMN, APAL), Bruno Mantel (CERREV, UNICAEN UFR STAPS, UNICAEN), Beno\^it G. Bardy (EuroMov, UM)</dc:creator>
    </item>
    <item>
      <title>TangibleChannel: An Innovative Data Physicalization System for Visual Channel Education</title>
      <link>https://arxiv.org/abs/2410.18810</link>
      <description>arXiv:2410.18810v1 Announce Type: new 
Abstract: In this paper, we provide an overview of our attempts to harness data physicalizations as pedagogical tools for enhancing the understanding of visual channels. We first elaborate the research goals that we have crafted for the physicalization prototype, shedding light on the key principles that guided our design choices. Then we detail the materials and datasets we employed for nine channels on our physicalization prototype. A preliminary pilot study is followed to validate its effectiveness. In the end, we present our upcoming research initiatives, including a comparative study for assessing the usability of the physicalization system. In general, the main purpose of our work is to stimulate a wider engagement among visualization educators and researchers, encouraging them to delve into the potentialities of data physicalization as an innovative addition to contemporary teaching methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18810v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Siqi Xie, Yu Liu, Lingyun Yu</dc:creator>
    </item>
    <item>
      <title>Expanding AI Awareness Through Everyday Interactions with AI: A Reflective Journal Study</title>
      <link>https://arxiv.org/abs/2410.18845</link>
      <description>arXiv:2410.18845v1 Announce Type: new 
Abstract: As the application of AI continues to expand, students in technology programs are poised to be both producers and users of the technologies. They are also positioned to engage with AI applications within and outside the classroom. While focusing on the curriculum when examining students' AI knowledge is common, extending this connection to students' everyday interactions with AI provides a more complete picture of their learning. In this paper, we explore student's awareness and engagement with AI in the context of school and their daily lives. Over six weeks, 22 undergraduate students participated in a reflective journal study and submitted a weekly journal entry about their interactions with AI. The participants were recruited from a technology and society course that focuses on the implications of technology on people, communities, and processes. In their weekly journal entries, participants reflected on interactions with AI on campus (coursework, advertises campus events, or seminars) and beyond (social media, news, or conversations with friends and family). The journal prompts were designed to help them think through what they had read, watched, or been told and reflect on the development of their own perspectives, knowledge, and literacy on the topic. Overall, students described nine categories of interactions: coursework, news and current events, using software and applications, university events, social media related to their work, personal discussions with friends and family, interacting with content, and gaming. Students reported that completing the diaries allowed them time for reflection and made them more aware of the presence of AI in their daily lives and of its potential benefits and drawbacks. This research contributes to the ongoing work on AI awareness and literacy by bringing in perspectives from beyond a formal educational context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18845v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ashish Hingle, Aditya Johri</dc:creator>
    </item>
    <item>
      <title>Intention Is All You Need</title>
      <link>https://arxiv.org/abs/2410.18851</link>
      <description>arXiv:2410.18851v1 Announce Type: new 
Abstract: Among the many narratives of the transformative power of Generative AI is one that sees in the world a latent nation of programmers who need to wield nothing but intentions and natural language to render their ideas in software. In this paper, this outlook is problematised in two ways. First, it is observed that generative AI is not a neutral vehicle of intention. Multiple recent studies paint a picture of the "mechanised convergence" phenomenon, namely, that generative AI has a homogenising effect on intention. Second, it is observed that the formation of intention itself is immensely challenging. Constraints, materiality, and resistance can offer paths to design metaphors for intentional tools. Finally, existentialist approaches to intention are discussed and possible implications for programming are proposed in the form of a speculative, illustrative set of intentional programming practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18851v1</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Advait Sarkar</dc:creator>
    </item>
    <item>
      <title>Swarm manipulation: An efficient and accurate technique for multi-object manipulation in virtual reality</title>
      <link>https://arxiv.org/abs/2410.18924</link>
      <description>arXiv:2410.18924v1 Announce Type: new 
Abstract: The theory of swarm control shows promise for controlling multiple objects, however, scalability is hindered by cost constraints, such as hardware and infrastructure. Virtual Reality (VR) can overcome these limitations, but research on swarm interaction in VR is limited. This paper introduces a novel Swarm Manipulation interaction technique and compares it with two baseline techniques: Virtual Hand and Controller (ray-casting). We evaluated these techniques in a user study ($N$ = 12) in three tasks (selection, rotation, and resizing) across five conditions. Our results indicate that Swarm Manipulation yielded superior performance, with significantly faster speeds in most conditions across the three tasks. It notably reduced resizing size deviations but introduced a trade-off between speed and accuracy in the rotation task. Additionally, we conducted a follow-up user study ($N$ = 6) using Swarm Manipulation in two complex VR scenarios and obtained insights through semi-structured interviews, shedding light on optimized swarm control mechanisms and perceptual changes induced by this interaction paradigm. These results demonstrate the potential of the Swarm Manipulation technique to enhance the usability and user experience in VR compared to conventional manipulation techniques. In future studies, we aim to understand and improve swarm interaction via internal swarm particle cooperation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18924v1</guid>
      <category>cs.HC</category>
      <category>cs.RO</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang Li, Jin-Du Wang, John J. Dudley, Per Ola Kristensson</dc:creator>
    </item>
    <item>
      <title>TextureMeDefect: LLM-based Defect Texture Generation for Railway Components on Mobile Devices</title>
      <link>https://arxiv.org/abs/2410.18085</link>
      <description>arXiv:2410.18085v1 Announce Type: cross 
Abstract: Texture image generation has been studied for various applications, including gaming and entertainment. However, context-specific realistic texture generation for industrial applications, such as generating defect textures on railway components, remains unexplored. A mobile-friendly, LLM-based tool that generates fine-grained defect characteristics offers a solution to the challenge of understanding the impact of defects from actual occurrences. We introduce TextureMeDefect, an innovative tool leveraging an LLM-based AI-Inferencing engine. The tool allows users to create realistic defect textures interactively on images of railway components taken with smartphones or tablets. We conducted a multifaceted evaluation to assess the relevance of the generated texture, time, and cost in using this tool on iOS and Android platforms. We also analyzed the software usability score (SUS) across three scenarios. TextureMeDefect outperformed traditional image generation tools by generating meaningful textures faster, showcasing the potential of AI-driven mobile applications on consumer-grade devices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18085v1</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahatara Ferdousi, M. Anwar Hossain, Abdulmotaleb El Saddik</dc:creator>
    </item>
    <item>
      <title>Human-Agent Coordination in Games under Incomplete Information via Multi-Step Intent</title>
      <link>https://arxiv.org/abs/2410.18242</link>
      <description>arXiv:2410.18242v1 Announce Type: cross 
Abstract: Strategic coordination between autonomous agents and human partners under incomplete information can be modeled as turn-based cooperative games. We extend a turn-based game under incomplete information, the shared-control game, to allow players to take multiple actions per turn rather than a single action. The extension enables the use of multi-step intent, which we hypothesize will improve performance in long-horizon tasks. To synthesize cooperative policies for the agent in this extended game, we propose an approach featuring a memory module for a running probabilistic belief of the environment dynamics and an online planning algorithm called IntentMCTS. This algorithm strategically selects the next action by leveraging any communicated multi-step intent via reward augmentation while considering the current belief. Agent-to-agent simulations in the Gnomes at Night testbed demonstrate that IntentMCTS requires fewer steps and control switches than baseline methods. A human-agent user study corroborates these findings, showing an 18.52% higher success rate compared to the heuristic baseline and a 5.56% improvement over the single-step prior work. Participants also report lower cognitive load, frustration, and higher satisfaction with the IntentMCTS agent partner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18242v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shenghui Chen, Ruihan Zhao, Sandeep Chinchali, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>UGotMe: An Embodied System for Affective Human-Robot Interaction</title>
      <link>https://arxiv.org/abs/2410.18373</link>
      <description>arXiv:2410.18373v1 Announce Type: cross 
Abstract: Equipping humanoid robots with the capability to understand emotional states of human interactants and express emotions appropriately according to situations is essential for affective human-robot interaction. However, enabling current vision-aware multimodal emotion recognition models for affective human-robot interaction in the real-world raises embodiment challenges: addressing the environmental noise issue and meeting real-time requirements. First, in multiparty conversation scenarios, the noises inherited in the visual observation of the robot, which may come from either 1) distracting objects in the scene or 2) inactive speakers appearing in the field of view of the robot, hinder the models from extracting emotional cues from vision inputs. Secondly, realtime response, a desired feature for an interactive system, is also challenging to achieve. To tackle both challenges, we introduce an affective human-robot interaction system called UGotMe designed specifically for multiparty conversations. Two denoising strategies are proposed and incorporated into the system to solve the first issue. Specifically, to filter out distracting objects in the scene, we propose extracting face images of the speakers from the raw images and introduce a customized active face extraction strategy to rule out inactive speakers. As for the second issue, we employ efficient data transmission from the robot to the local server to improve realtime response capability. We deploy UGotMe on a human robot named Ameca to validate its real-time inference capabilities in practical scenarios. Videos demonstrating real-world deployment are available at https://pi3-141592653.github.io/UGotMe/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18373v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peizhen Li, Longbing Cao, Xiao-Ming Wu, Xiaohan Yu, Runze Yang</dc:creator>
    </item>
    <item>
      <title>VoxelKeypointFusion: Generalizable Multi-View Multi-Person Pose Estimation</title>
      <link>https://arxiv.org/abs/2410.18723</link>
      <description>arXiv:2410.18723v1 Announce Type: cross 
Abstract: In the rapidly evolving field of computer vision, the task of accurately estimating the poses of multiple individuals from various viewpoints presents a formidable challenge, especially if the estimations should be reliable as well. This work presents an extensive evaluation of the generalization capabilities of multi-view multi-person pose estimators to unseen datasets and presents a new algorithm with strong performance in this task. It also studies the improvements by additionally using depth information. Since the new approach can not only generalize well to unseen datasets, but also to different keypoints, the first multi-view multi-person whole-body estimator is presented. To support further research on those topics, all of the work is publicly accessible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18723v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Bermuth, Alexander Poeppel, Wolfgang Reif</dc:creator>
    </item>
    <item>
      <title>Breaking Down the Barriers: Investigating Non-Expert User Experiences in Robotic Teleoperation in UK and Japan</title>
      <link>https://arxiv.org/abs/2410.18727</link>
      <description>arXiv:2410.18727v1 Announce Type: cross 
Abstract: Robots are being created each year with the goal of integrating them into our daily lives. As such, there is an interest in research in evaluating the trust of humans toward robots. In addition, teleoperating robotic arms can be challenging for non-experts. In order to reduce the strain put on the user, we created TELESIM, a modular and plug-and-play framework that enables direct teleoperation of any robotic arm using a digital twin as the interface between users and the robotic system. However, analysis of the strain put on the user and its ability to trust robots was omitted. This paper addresses these omissions by presenting the additional results of our user survey of 37 participants carried out in UK. In addition, we present the results of an additional user survey, under similar conditions performed in Japan, with the goal of addressing the limitations of our previous approach, by interfacing a VR controller with a UR5e. Our experimental results show that the UR5e has a higher number of towers built. Additionally, the UR5e gives the least amount of cognitive stress, while the combination of Senseglove and UR3 gives the user the highest physical strain and causes the user to feel more frustrated. Finally, Japanese seems more trusting towards robots than British.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18727v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florent P Audonnet, Andrew Hamilton, Yakiyasu Domae, Ixchel G Ramirez-Alpizar, Gerardo Aragon-Camarasa</dc:creator>
    </item>
    <item>
      <title>Exploring the Universe with SNAD: Anomaly Detection in Astronomy</title>
      <link>https://arxiv.org/abs/2410.18875</link>
      <description>arXiv:2410.18875v1 Announce Type: cross 
Abstract: SNAD is an international project with a primary focus on detecting astronomical anomalies within large-scale surveys, using active learning and other machine learning algorithms. The work carried out by SNAD not only contributes to the discovery and classification of various astronomical phenomena but also enhances our understanding and implementation of machine learning techniques within the field of astrophysics. This paper provides a review of the SNAD project and summarizes the advancements and achievements made by the team over several years.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18875v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-67826-4_15</arxiv:DOI>
      <arxiv:journal_reference>In: Baixeries, J., Ignatov, D.I., Kuznetsov, S.O., Stupnikov, S. (eds) Data Analytics and Management in Data Intensive Domains. DAMDID/RCDL 2023. Communications in Computer and Information Science, vol 2086. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Alina A. Volnova, Patrick D. Aleo, Anastasia Lavrukhina, Etienne Russeil, Timofey Semenikhin, Emmanuel Gangler, Emille E. O. Ishida, Matwey V. Kornilov, Vladimir Korolev, Konstantin Malanchev, Maria V. Pruzhinskaya, Sreevarsha Sreejith</dc:creator>
    </item>
    <item>
      <title>Guiding Empowerment Model: Liberating Neurodiversity in Online Higher Education</title>
      <link>https://arxiv.org/abs/2410.18876</link>
      <description>arXiv:2410.18876v1 Announce Type: cross 
Abstract: In this innovative practice full paper, we address the equity gap for neurodivergent and situationally limited learners by identifying the spectrum of dynamic factors that impact learning and function. Educators have shown a growing interest in identifying learners' cognitive abilities and learning preferences to measure their impact on academic achievement. Often institutions employ one-size-fits-all approaches leaving the burden on disabled students to self-advocate or tolerate inadequate support. Emerging frameworks guide neurodivergent learners through instructional approaches, such as online education. However, these frameworks fail to address holistic environmental needs or recommend technology interventions, particularly for those with undisclosed learning or developmental disabilities and situational limitations. In this article, we integrate a neurodivergent perspective through secondary research of around 100 articles to introduce a Guiding Empowerment Model involving key cognitive and situational factors that contextualize day-to-day experiences affecting learner ability. We synthesize three sample student profiles that highlight user problems in functioning. We use this model to evaluate sample learning platform features and other supportive technology solutions. The proposed approach augments frameworks such as Universal Design for Learning to consider factors including various sensory processing differences, social connection challenges, and environmental limitations. We suggest that by applying the mode through technology-enabled features such as customizable task management, guided varied content access, and guided multi-modal collaboration, major learning barriers of neurodivergent and situationally limited learners will be removed to activate the successful pursuit of their academic goals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18876v1</guid>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hannah Beaux, Pegah Karimi, Otilia Pop, Rob Clark</dc:creator>
    </item>
    <item>
      <title>An intelligent sociotechnical systems (iSTS) framework: Enabling a hierarchical human-centered AI (hHCAI) approach</title>
      <link>https://arxiv.org/abs/2401.03223</link>
      <description>arXiv:2401.03223v5 Announce Type: replace 
Abstract: While artificial intelligence (AI) offers significant benefits, it also has negatively impacted humans and society. A human-centered AI (HCAI) approach has been proposed to address these issues. However, current HCAI practices have shown limited contributions due to a lack of sociotechnical thinking. To overcome these challenges, we conducted a literature review and comparative analysis of sociotechnical characteristics with respect to AI. Then, we propose updated sociotechnical systems (STS) design principles. Based on these findings, this paper introduces an intelligent sociotechnical systems (iSTS) framework to extend traditional STS theory and meet the demands with respect to AI. The iSTS framework emphasizes human-centered joint optimization across individual, organizational, ecosystem, and societal levels. The paper further integrates iSTS with current HCAI practices, proposing a hierarchical HCAI (hHCAI) approach. This hHCAI approach offers a structured approach to address challenges in HCAI practices from a broader sociotechnical perspective. Finally, we provide recommendations for future iSTS and hHCAI work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03223v5</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Wei Xu, Zaifeng Gao</dc:creator>
    </item>
    <item>
      <title>Trickery: Exploring a Serious Game Approach to Raise Awareness of Deceptive Patterns</title>
      <link>https://arxiv.org/abs/2401.06247</link>
      <description>arXiv:2401.06247v3 Announce Type: replace 
Abstract: Deceptive patterns are often used in interface design to manipulate users into taking actions they would not otherwise take, such as consenting to excessive data collection. We present Trickery, a narrative serious game that incorporates seven gamified deceptive patterns. We designed the game as a potential mechanism for raising awareness of, and increasing resistance to, deceptive patterns through direct consequences of player actions. We conducted an explorative gameplay study to examine player behavior when confronted with the game Trickery. In addition, we conducted an online survey to shed light on the perceived helpfulness of our gamified deceptive patterns. Our results reveal different player motivations and driving forces that players used to justify their behavior when confronted with deceptive patterns in the Trickery game. In addition, we identified several influencing factors that need to be considered when adapting deceptive patterns into gameplay. Overall, the approach appears to be a promising solution for increasing user understanding and awareness of deceptive patterns.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.06247v3</guid>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kirill Kronhardt, Kevin Rolfes, Jens Gerken</dc:creator>
    </item>
    <item>
      <title>Investigating Labeler Bias in Face Annotation for Machine Learning</title>
      <link>https://arxiv.org/abs/2301.09902</link>
      <description>arXiv:2301.09902v3 Announce Type: replace-cross 
Abstract: In a world increasingly reliant on artificial intelligence, it is more important than ever to consider the ethical implications of artificial intelligence on humanity. One key under-explored challenge is labeler bias, which can create inherently biased datasets for training and subsequently lead to inaccurate or unfair decisions in healthcare, employment, education, and law enforcement. Hence, we conducted a study to investigate and measure the existence of labeler bias using images of people from different ethnicities and sexes in a labeling task. Our results show that participants possess stereotypes that influence their decision-making process and that labeler demographics impact assigned labels. We also discuss how labeler bias influences datasets and, subsequently, the models trained on them. Overall, a high degree of transparency must be maintained throughout the entire artificial intelligence training process to identify and correct biases in the data as early as possible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.09902v3</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.3233/FAIA240191</arxiv:DOI>
      <arxiv:journal_reference>Frontiers in Artificial Intelligence and Applications (2024) 145-162</arxiv:journal_reference>
      <dc:creator>Luke Haliburton, Sinksar Ghebremedhin, Robin Welsch, Albrecht Schmidt, Sven Mayer</dc:creator>
    </item>
    <item>
      <title>RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation</title>
      <link>https://arxiv.org/abs/2410.11722</link>
      <description>arXiv:2410.11722v2 Announce Type: replace-cross 
Abstract: The emergence of Segment Anything (SAM) sparked research interest in the field of interactive segmentation, especially in the context of image editing tasks and speeding up data annotation. Unlike common semantic segmentation, interactive segmentation methods allow users to directly influence their output through prompts (e.g. clicks). However, click patterns in real-world interactive segmentation scenarios remain largely unexplored. Most methods rely on the assumption that users would click in the center of the largest erroneous area. Nevertheless, recent studies show that this is not always the case. Thus, methods may have poor performance in real-world deployment despite high metrics in a baseline benchmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing study of click patterns in an interactive segmentation scenario and collected 475K real-user clicks. Drawing on ideas from saliency tasks, we develop a clickability model that enables sampling clicks, which closely resemble actual user inputs. Using our model and dataset, we propose RClicks benchmark for a comprehensive comparison of existing interactive segmentation methods on realistic clicks. Specifically, we evaluate not only the average quality of methods, but also the robustness w.r.t. click patterns. According to our benchmark, in real-world usage interactive segmentation models may perform worse than it has been reported in the baseline benchmark, and most of the methods are not robust. We believe that RClicks is a significant step towards creating interactive segmentation methods that provide the best user experience in real-world cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11722v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 25 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anton Antonov, Andrey Moskalenko, Denis Shepelev, Alexander Krapukhin, Konstantin Soshin, Anton Konushin, Vlad Shakhuro</dc:creator>
    </item>
  </channel>
</rss>
