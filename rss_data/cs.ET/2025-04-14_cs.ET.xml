<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.ET updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.ET</link>
    <description>cs.ET updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.ET" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Apr 2025 03:10:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>All-in-Memory Stochastic Computing using ReRAM</title>
      <link>https://arxiv.org/abs/2504.08340</link>
      <description>arXiv:2504.08340v1 Announce Type: new 
Abstract: As the demand for efficient, low-power computing in embedded and edge devices grows, traditional computing methods are becoming less effective for handling complex tasks. Stochastic computing (SC) offers a promising alternative by approximating complex arithmetic operations, such as addition and multiplication, using simple bitwise operations, like majority or AND, on random bit-streams. While SC operations are inherently fault-tolerant, their accuracy largely depends on the length and quality of the stochastic bit-streams (SBS). These bit-streams are typically generated by CMOS-based stochastic bit-stream generators that consume over 80% of the SC system's power and area. Current SC solutions focus on optimizing the logic gates but often neglect the high cost of moving the bit-streams between memory and processor. This work leverages the physics of emerging ReRAM devices to implement the entire SC flow in place: (1) generating low-cost true random numbers and SBSs, (2) conducting SC operations, and (3) converting SBSs back to binary. Considering the low reliability of ReRAM cells, we demonstrate how SC's robustness to errors copes with ReRAM's variability. Our evaluation shows significant improvements in throughput (1.39x, 2.16x) and energy consumption (1.15x, 2.8x) over state-of-the-art (CMOS- and ReRAM-based) solutions, respectively, with an average image quality drop of 5% across multiple SBS lengths and image processing tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08340v1</guid>
      <category>cs.ET</category>
      <category>cs.AR</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Paulo C. de Lima, Mehran Shoushtari Moghadam, Sercan Aygun, Jeronimo Castrillon, M. Hassan Najafi, Asif Ali Khan</dc:creator>
    </item>
    <item>
      <title>CICV5G: A 5G Communication Delay Dataset for PnC in Cloud-based Intelligent Connected Vehicles</title>
      <link>https://arxiv.org/abs/2504.08255</link>
      <description>arXiv:2504.08255v1 Announce Type: cross 
Abstract: Cloud-based intelligent connected vehicles (CICVs) leverage cloud computing and vehicle-to-everything (V2X) to enable efficient information exchange and cooperative control. However, communication delay is a critical factor in vehicle-cloud interactions, potentially deteriorating the planning and control (PnC) performance of CICVs. To explore whether the new generation of communication technology, 5G, can support the PnC of CICVs, we present CICV5G, a publicly available 5G communication delay dataset for the PnC of CICVs. This dataset offers real-time delay variations across diverse traffic environments, velocity, data transmission frequencies, and network conditions. It contains over 300,000 records, with each record consists of the network performance indicators (e.g., cell ID, reference signal received power, and signal-to-noise ratio) and PnC related data (e.g., position). Based on the CICV5G, we compare the performance of CICVs with that of autonomous vehicles and examine how delay impacts the PnC of CICVs. The object of this dataset is to support research in developing more accurate communication models and to provide a valuable reference for scheme development and network deployment for CICVs. To ensure that the research community can benefit from this work, our dataset and accompanying code are made publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08255v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinrui Zhang, Peizhi Zhang, Junpeng Huang, Haojie Feng, Yining Ma, Feng Shen, Lu Xiong</dc:creator>
    </item>
    <item>
      <title>Application of machine learning models to predict the relationship between air pollution, ecosystem degradation, and health disparities and lung cancer in Vietnam</title>
      <link>https://arxiv.org/abs/2504.08651</link>
      <description>arXiv:2504.08651v1 Announce Type: cross 
Abstract: Lung cancer is one of the major causes of death worldwide, and Vietnam is not an exception. This disease is the second most common type of cancer globally and the second most common cause of death in Vietnam, just after liver cancer, with 23,797 fatal cases and 26,262 new cases, or 14.4% of the disease in 2020. Recently, with rising disease rates in Vietnam causing a huge public health burden, lung cancer continues to hold the top position in attention and care. Especially together with climate change, under a variety of types of pollution, deforestation, and modern lifestyles, lung cancer risks are on red alert, particularly in Vietnam. To understand more about the severe disease sources in Vietnam from a diversity of key factors, including environmental features and the current health state, with a particular emphasis on Vietnam's distinct socioeconomic and ecological context, we utilize large datasets such as patient health records and environmental indicators containing necessary information, such as deforestation rate, green cover rate, air pollution, and lung cancer risks, that is collected from well-known governmental sharing websites. Then, we process and connect them and apply analytical methods (heatmap, information gain, p-value, spearman correlation) to determine causal correlations influencing lung cancer risks. Moreover, we deploy machine learning (ML) models (Decision Tree, Random Forest, Support Vector Machine, K-mean clustering) to discover cancer risk patterns. Our experimental results, leveraged by the aforementioned ML models to identify the disease patterns, are promising, particularly, the models as Random Forest, SVM, and PCA are working well on the datasets and give high accuracy (99%), however, the K means clustering has very low accuracy (10%) and does not fit the datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08651v1</guid>
      <category>cs.LG</category>
      <category>cs.ET</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Ngoc Hong Tran, Lan Kim Vien, Ngoc-Thao Thi Le</dc:creator>
    </item>
    <item>
      <title>Quantum Large Language Model Fine-Tuning</title>
      <link>https://arxiv.org/abs/2504.08732</link>
      <description>arXiv:2504.08732v1 Announce Type: cross 
Abstract: We introduce a hybrid quantum-classical deep learning architecture for large language model fine-tuning. The classical portion of the architecture is a sentence transformer that is powerful enough to display significant accuracy for complex tasks such as sentiment prediction. The quantum portion of the architecture consists of parameterized quantum circuits that utilize long-range connections between qubits.
  We analyze the performance of the hybrid models for various settings of hyperparameters, including the number of qubits, the depth of the quantum circuits, learning rate, number of re-uploading steps, etc. Based on a screening study of main effects, we show an overall improvement in prediction accuracy over a comparable classical baseline, with a trend of increasing accuracy with number of qubits. We observe up to $3.14\%$ improvements in accuracy over classical architectures of comparable model size, within the set of hyperparameters probed in this study.
  We demonstrate the contribution of each module in our architecture through ablation studies. Our studies are based on finite shot-counts and include simulations based on noisy quantum gates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08732v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sang Hyub Kim, Jonathan Mei, Claudio Girotto, Masako Yamada, Martin Roetteler</dc:creator>
    </item>
    <item>
      <title>EDA-Q: Electronic Design Automation for Superconducting Quantum Chip</title>
      <link>https://arxiv.org/abs/2502.15386</link>
      <description>arXiv:2502.15386v4 Announce Type: replace 
Abstract: Electronic Design Automation (EDA) plays a crucial role in classical chip design and significantly influences the development of quantum chip design. However, traditional EDA tools cannot be directly applied to quantum chip design due to vast differences compared to the classical realm. Several EDA products tailored for quantum chip design currently exist, yet they only cover partial stages of the quantum chip design process instead of offering a fully comprehensive solution. Additionally, they often encounter issues such as limited automation, steep learning curves, challenges in integrating with actual fabrication processes, and difficulties in expanding functionality. To address these issues, we developed a full-stack EDA tool specifically for quantum chip design, called EDA-Q. The design workflow incorporates functionalities present in existing quantum EDA tools while supplementing critical design stages such as device mapping and fabrication process mapping, which users expect. EDA-Q utilizes a unique architecture to achieve exceptional scalability and flexibility. The integrated design mode guarantees algorithm compatibility with different chip components, while employing a specialized interactive processing mode to offer users a straightforward and adaptable command interface. Application examples demonstrate that EDA-Q significantly reduces chip design cycles, enhances automation levels, and decreases the time required for manual intervention. Multiple rounds of testing on the designed chip have validated the effectiveness of EDA-Q in practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.15386v4</guid>
      <category>cs.ET</category>
      <category>quant-ph</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bo Zhao, Zhihang Li, Xiaohan Yu, Benzheng Yuan, Chaojie Zhang, Yimin Gao, Weilong Wang, Qing Mu, Shuya Wang, Huihui Sun, Tian Yang, Mengfan Zhang, Chuanbing Han, Peng Xu, Wenqing Wang, Zheng Shan</dc:creator>
    </item>
    <item>
      <title>The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2408.12622</link>
      <description>arXiv:2408.12622v2 Announce Type: replace-cross 
Abstract: The risks posed by Artificial Intelligence (AI) are of considerable concern to academics, auditors, policymakers, AI companies, and the public. However, a lack of shared understanding of AI risks can impede our ability to comprehensively discuss, research, and react to them. This paper addresses this gap by creating an AI Risk Repository to serve as a common frame of reference. This comprises a living database of 777 risks extracted from 43 taxonomies, which can be filtered based on two overarching taxonomies and easily accessed, modified, and updated via our website and online spreadsheets. We construct our Repository with a systematic review of taxonomies and other structured classifications of AI risk followed by an expert consultation. We develop our taxonomies of AI risk using a best-fit framework synthesis. Our high-level Causal Taxonomy of AI Risks classifies each risk by its causal factors (1) Entity: Human, AI; (2) Intentionality: Intentional, Unintentional; and (3) Timing: Pre-deployment; Post-deployment. Our mid-level Domain Taxonomy of AI Risks classifies risks into seven AI risk domains: (1) Discrimination &amp; toxicity, (2) Privacy &amp; security, (3) Misinformation, (4) Malicious actors &amp; misuse, (5) Human-computer interaction, (6) Socioeconomic &amp; environmental, and (7) AI system safety, failures, &amp; limitations. These are further divided into 23 subdomains. The AI Risk Repository is, to our knowledge, the first attempt to rigorously curate, analyze, and extract AI risk frameworks into a publicly accessible, comprehensive, extensible, and categorized risk database. This creates a foundation for a more coordinated, coherent, and complete approach to defining, auditing, and managing the risks posed by AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.12622v2</guid>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Slattery, Alexander K. Saeri, Emily A. C. Grundy, Jess Graham, Michael Noetel, Risto Uuk, James Dao, Soroush Pour, Stephen Casper, Neil Thompson</dc:creator>
    </item>
    <item>
      <title>Transfer of Knowledge through Reverse Annealing: A Preliminary Analysis of the Benefits and What to Share</title>
      <link>https://arxiv.org/abs/2501.15865</link>
      <description>arXiv:2501.15865v2 Announce Type: replace-cross 
Abstract: Being immersed in the NISQ-era, current quantum annealers present limitations for solving optimization problems efficiently. To mitigate these limitations, D-Wave Systems developed a mechanism called Reverse Annealing, a specific type of quantum annealing designed to perform local refinement of good states found elsewhere. Despite the research activity around Reverse Annealing, none has theorized about the possible benefits related to the transfer of knowledge under this paradigm. This work moves in that direction and is driven by experimentation focused on answering two key research questions: i) is reverse annealing a paradigm that can benefit from knowledge transfer between similar problems? and ii) can we infer the characteristics that an input solution should meet to help increase the probability of success? To properly guide the tests in this paper, the well-known Knapsack Problem has been chosen for benchmarking purposes, using a total of 34 instances composed of 14 and 16 items.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15865v2</guid>
      <category>quant-ph</category>
      <category>cs.AI</category>
      <category>cs.ET</category>
      <pubDate>Mon, 14 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eneko Osaba, Esther Villar-Rodriguez</dc:creator>
    </item>
  </channel>
</rss>
