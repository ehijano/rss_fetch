<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.soc-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.soc-ph</link>
    <description>physics.soc-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.soc-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Feb 2025 02:45:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Digital Urban Twin Enabling Interactive Pollution Predictions and Enhanced Planning</title>
      <link>https://arxiv.org/abs/2502.13746</link>
      <description>arXiv:2502.13746v1 Announce Type: new 
Abstract: Digital twin (DT) technology is increasingly used in urban planning, leveraging real-time data integration for environmental monitoring. This paper presents an urban-focused DT that combines computational fluid dynamics simulations with live meteorological data to analyze pollution dispersion. Addressing the health impacts of pollutants like particulate matter and nitrogen dioxide, the DT provides real-time updates on air quality, wind speed, and direction. Using OpenStreetMaps XML-based data, the model distinguishes between porous elements like trees and solid structures, enhancing urban flow analysis. The framework employs the lattice Boltzmann method (LBM) within the open-source software OpenLB to simulate pollution transport. Nitrogen dioxide and particulate matter concentrations are estimated based on traffic and building emissions, enabling hot-spot identification. The DT was used from November 7 to 23, 2024, with hourly updates, capturing pollution trends influenced by wind patterns. Results show that alternating east-west winds during this period create a dynamic pollution distribution, identifying critical residential exposure areas. This work contributes a novel DT framework that integrates real-time meteorological data, OpenStreetMap-based geometry, and high-fidelity LBM simulations for urban wind and pollution modeling. Unlike existing DTs, which focus on structural monitoring or large-scale environmental factors, this approach enables fine-grained, dynamic analyses of urban airflow and pollution dispersion. By allowing interactive modifications to urban geometry and continuous data updates, the DT serves as a powerful tool for adaptive urban planning, supporting evidence-based policy making to improve air quality and public health.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13746v1</guid>
      <category>physics.soc-ph</category>
      <category>physics.flu-dyn</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Teutscher, Fedor Bukreev, Adrian Kummerlaender, Stephan Simonis, Peter Baechler, Ashkan Rezaee, Mariusz Hermansdorfer, Mathias J. Krause</dc:creator>
    </item>
    <item>
      <title>Emergence of Scale-Free Traffic Jams in Highway Networks: A Probabilistic Approach</title>
      <link>https://arxiv.org/abs/2502.13944</link>
      <description>arXiv:2502.13944v1 Announce Type: new 
Abstract: Traffic congestion continues to escalate with urbanization and socioeconomic development, necessitating advanced modeling to understand and mitigate its impacts. In large-scale networks, traffic congestion can be studied using cascade models, where congestion not only impacts isolated segments, but also propagates through the network in a domino-like fashion. One metric for understanding these impacts is congestion cost, which is typically defined as the additional travel time caused by traffic jams. Recent data suggests that congestion cost exhibits a universal scale-free-tailed behavior. However, the mechanism driving this phenomenon is not yet well understood. To address this gap, we propose a stochastic cascade model of traffic congestion. We show that traffic congestion cost is driven by the scale-free distribution of traffic intensities. This arises from the catastrophe principle, implying that severe congestion is likely caused by disproportionately large traffic originating from a single location. We also show that the scale-free nature of congestion cost is robust to various congestion propagation rules, explaining the universal scaling observed in empirical data. These findings provide a new perspective in understanding the fundamental drivers of traffic congestion and offer a unifying framework for studying congestion phenomena across diverse traffic networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13944v1</guid>
      <category>physics.soc-ph</category>
      <category>math.PR</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Agnieszka Janicka, Fiona Sloothaak, Maria Vlasiou, Bert Zwart</dc:creator>
    </item>
    <item>
      <title>Evidence of Replica Symmetry Breaking under the Nishimori conditions in epidemic inference on graphs</title>
      <link>https://arxiv.org/abs/2502.13249</link>
      <description>arXiv:2502.13249v1 Announce Type: cross 
Abstract: In Bayesian inference, computing the posterior distribution from the data is typically a non-trivial problem, which usually requires approximations such as mean-field approaches or numerical methods, like the Monte Carlo Markov Chain. Being a high-dimensional distribution over a set of correlated variables, the posterior distribution can undergo the notorious replica symmetry breaking transition. When it happens, several mean-field methods and virtually every Monte Carlo scheme can not provide a reasonable approximation to the posterior and its marginals. Replica symmetry is believed to be guaranteed whenever the data is generated with known prior and likelihood distributions, namely under the so-called Nishimori conditions. In this paper, we break this belief, by providing a counter-example showing that, under the Nishimori conditions, replica symmetry breaking arises. Introducing a simple, geometrical model that can be thought of as a patient zero retrieval problem in a highly infectious regime of the epidemic Susceptible-Infectious model, we show that under the Nishimori conditions, there is evidence of replica symmetry breaking. We achieve this result by computing the instability of the replica symmetric cavity method toward the one step replica symmetry broken phase. The origin of this phenomenon -- replica symmetry breaking under the Nishimori conditions -- is likely due to the correlated disorder appearing in the epidemic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13249v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfredo Braunstein, Louise Budzynski, Matteo Mariani, Federico Ricci-Tersenghi</dc:creator>
    </item>
    <item>
      <title>Community Notes Moderate Engagement With and Diffusion of False Information Online</title>
      <link>https://arxiv.org/abs/2502.13322</link>
      <description>arXiv:2502.13322v1 Announce Type: cross 
Abstract: Social networks scaffold the diffusion of information on social media. Much attention has been given to the spread of true vs. false content on online social platforms, including the structural differences between their diffusion patterns. However, much less is known about how platform interventions on false content alter the engagement with and diffusion of such content. In this work, we estimate the causal effects of Community Notes, a novel fact-checking feature adopted by X (formerly Twitter) to solicit and vet crowd-sourced fact-checking notes for false content. We gather detailed time series data for 40,074 posts for which notes have been proposed and use synthetic control methods to estimate a range of counterfactual outcomes. We find that attaching fact-checking notes significantly reduces the engagement with and diffusion of false content. We estimate that, on average, the notes resulted in reductions of 45.7% in reposts, 43.5% in likes, 22.9% in replies, and 14.0% in views after being attached. Over the posts' entire lifespans, these reductions amount to 11.4% fewer reposts, 13.0% fewer likes, 7.3% fewer replies, and 5.7% fewer views on average. In reducing reposts, we observe that diffusion cascades for fact-checked content are less deep, but not less broad, than synthetic control estimates for non-fact-checked content with similar reach. This structural difference contrasts notably with differences between false vs. true content diffusion itself, where false information diffuses farther, but with structural patterns that are otherwise indistinguishable from those of true information, conditional on reach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13322v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Slaughter, Axel Peytavin, Johan Ugander, Martin Saveski</dc:creator>
    </item>
    <item>
      <title>Environmental Influences on Collaboration Network Evolution: A Historical Analysis</title>
      <link>https://arxiv.org/abs/2502.13607</link>
      <description>arXiv:2502.13607v1 Announce Type: cross 
Abstract: We analysed two large collaboration networks -- the Microsoft Academic Graph (1800-2020) and Internet Movie Database (1900-2020) -- to quantify network responses to major historical events. Our analysis revealed four properties of network-environment interaction. First, historical events can influence network evolution, with effects persisting far longer than previously recognised; the academic network showed 45\% declines during World Wars and 90\% growth during La Belle Epoque. Second, node and edge processes exhibited different environmental sensitivities; while node addition/removal tracked historical events, edge formation maintained stable statistical properties even during major disruptions. Third, different collaboration networks showed distinct response patterns; academic networks displayed sharp disruptions and rapid recoveries, while entertainment networks showed gradual changes and greater resilience. Fourth, both networks developed increasing resilience. Our results provide new insights for modelling network evolution and managing collaborative systems during periods of external disruption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.13607v1</guid>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter R Williams, Zhan Chen</dc:creator>
    </item>
    <item>
      <title>Optimal ambition in business, politics and life</title>
      <link>https://arxiv.org/abs/2502.10500</link>
      <description>arXiv:2502.10500v2 Announce Type: replace 
Abstract: When should we be satisfied and when should we look for greener pastures? When is the perfect the enemy of the good? This type of question arises in many different contexts, including business, politics, resource exploitation, and our personal lives. Folk intuition suggests that people should aim for above-average results, but overreaching can lead to failure. Here, we mathematically formalize this intuition and relate it to empirical research across diverse domains. We model a search for strategies that have uncertain rewards over a fixed time period. The agent (i.e. searcher) knows the statistical distribution of rewards across strategies. At each time step, the agent either is satisfied and sticks with their current strategy or continues searching. We prove that the agent's optimal satisfaction threshold is both finite and strictly larger than the mean of available rewards. Compared to the optimal threshold, being too ambitious has a higher expected cost than being too cautious, implying that uncertainty over the reward distribution should motivate caution. The optimal satisfaction threshold becomes larger if the search time is longer, or if the reward distribution is rugged (i.e., has low autocorrelation) or left skewed. Using upward social comparison to assess the reward landscape biases agents towards never being satisfied, which decreases their expected performance substantially. We discuss how these insights can be applied empirically, using examples from entrepreneurship, economic policy, political campaigns, online dating, and college admissions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.10500v2</guid>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ekaterina Landgren, Ryan E. Langendorf, Matthew G. Burgess</dc:creator>
    </item>
    <item>
      <title>Hands-on STEM Learning Experiences using Digital Technologies</title>
      <link>https://arxiv.org/abs/2408.00781</link>
      <description>arXiv:2408.00781v2 Announce Type: replace-cross 
Abstract: The facilitation of STEM education can be enhanced by the provision of opportunities for learners to gain a better understanding of science through the utilization of tangible and visual examples. The objective of this work is to present an account of our experiences and activities carried out in Italian schools with this novel approach. The selection of projects and experiences discussed --in which students develop a range of core competencies such as collaboration, creativity, critical thinking, experimentation, prototyping, communication and problem-solving; include tangible complex 3D printed structures, large micro-controller board replicas and the visualization of wind dynamics and tiny invisible elementary particles among others. These hands-on experiences demonstrate the benefits on the use of digital fabrication technologies implemented within a FabLab for STEM learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.00781v2</guid>
      <category>physics.ed-ph</category>
      <category>cs.CV</category>
      <category>cs.ET</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Gaia Fior, Carlo Fonda, Enrique Canessa</dc:creator>
    </item>
    <item>
      <title>Fast algorithms to improve fair information access in networks</title>
      <link>https://arxiv.org/abs/2409.03127</link>
      <description>arXiv:2409.03127v2 Announce Type: replace-cross 
Abstract: We consider the problem of selecting $k$ seed nodes in a network to maximize the minimum probability of activation under an independent cascade beginning at these seeds. The motivation is to promote fairness by ensuring that even the least advantaged members of the network have good access to information. Our problem can be viewed as a variant of the classic influence maximization objective, but it appears somewhat more difficult to solve: only heuristics are known. Moreover, the scalability of these methods is sharply constrained by the need to repeatedly estimate access probabilities.
  We design and evaluate a suite of $10$ new scalable algorithms which crucially do not require probability estimation. To facilitate comparison with the state-of-the-art, we make three more contributions which may be of broader interest. We introduce a principled method of selecting a pairwise information transmission parameter used in experimental evaluations, as well as a new performance metric which allows for comparison of algorithms across a range of values for the parameter $k$. Finally, we provide a new benchmark corpus of $174$ networks drawn from $6$ domains. Our algorithms retain most of the performance of the state-of-the-art while reducing running time by orders of magnitude. Specifically, a meta-learner approach is on average only $20\%$ less effective than the state-of-the-art on held-out data, but about $75-130$ times faster. Further, the meta-learner's performance exceeds the state-of the-art on about $20\%$ of networks, and the magnitude of its running time advantage is maintained on much larger networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03127v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dennis Robert Windham, Caroline J. Wendt, Alex Crane, Madelyn J Warr, Freda Shi, Sorelle A. Friedler, Blair D. Sullivan, Aaron Clauset</dc:creator>
    </item>
    <item>
      <title>The Gerontocratization of Science: How hypergrowth reshapes knowledge circulation</title>
      <link>https://arxiv.org/abs/2410.00788</link>
      <description>arXiv:2410.00788v2 Announce Type: replace-cross 
Abstract: Scientific literature has been growing exponentially for decades, with publications from the last twenty years now comprising 60% of all academic output. While the impact of information overload on news and social-media consumption is well-documented, its consequences on scientific progress remain understudied. Here, we investigate how this rapid expansion affects the circulation and exploitation of scientific ideas. Unlike other cultural domains, science is experiencing a decline in the proportion of highly influential papers and a slower turnover in its canons. This results in the disproportionate persistence of established works, a phenomenon we term the ``gerontocratization of science''. To test whether hypergrowth drives this trend, we develop a generative citation model that incorporates random discovery, cumulative advantage, and exponential growth of the scientific literature. Our findings reveal that as scientific output expands exponentially, gerontocratization emerges and intensifies, reducing the influence of new research. Recognizing and understanding this mechanism is crucial for developing targeted strategies to sustain intellectual dynamism and ensure a balanced and healthy renewal of scientific knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00788v2</guid>
      <category>cs.DL</category>
      <category>physics.soc-ph</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antoine Houssard, Floriana Gargiulo, Tommaso Venturini, Paola Tubaro, Gabriele Di Bona</dc:creator>
    </item>
  </channel>
</rss>
