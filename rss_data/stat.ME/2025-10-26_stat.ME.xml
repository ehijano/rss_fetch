<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 27 Oct 2025 04:00:29 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Handling Missing Responses under Cluster Dependence with Applications to Language Model Evaluation</title>
      <link>https://arxiv.org/abs/2510.20928</link>
      <description>arXiv:2510.20928v1 Announce Type: new 
Abstract: Human annotations play a crucial role in evaluating the performance of GenAI models. Two common challenges in practice, however, are missing annotations (the response variable of interest) and cluster dependence among human-AI interactions (e.g., questions asked by the same user may be highly correlated). Reliable inference must address both these issues to achieve unbiased estimation and appropriately quantify uncertainty when estimating average scores from human annotations. In this paper, we analyze the doubly robust estimator, a widely used method in missing data analysis and causal inference, applied to this setting and establish novel theoretical properties under cluster dependence. We further illustrate our findings through simulations and a real-world conversation quality dataset. Our theoretical and empirical results underscore the importance of incorporating cluster dependence in missing response problems to perform valid statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20928v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhenghao Zeng, David Arbour, Avi Feller, Ishita Dasgupta, Atanu R Sinha, Edward H. Kennedy</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of flexible Heckman selection models using Hamiltonian Monte Carlo</title>
      <link>https://arxiv.org/abs/2510.20942</link>
      <description>arXiv:2510.20942v1 Announce Type: new 
Abstract: The Heckman selection model is widely used in econometric analysis and other social sciences to address sample selection bias in data modeling. A common assumption in Heckman selection models is that the error terms follow an independent bivariate normal distribution. However, real-world data often deviates from this assumption, exhibiting heavy-tailed behavior, which can lead to inconsistent estimates if not properly addressed. In this paper, we propose a Bayesian analysis of Heckman selection models that replace the Gaussian assumption with well-known members of the class of scale mixture of normal distributions, such as the Student's-t and contaminated normal distributions. For these complex structures, Stan's default No-U-Turn sampler is utilized to obtain posterior simulations. Through extensive simulation studies, we compare the performance of the Heckman selection models with normal, Student's-t and contaminated normal distributions. We also demonstrate the broad applicability of this methodology by applying it to medical care and labor supply data. The proposed algorithms are implemented in the R package HeckmanStan.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20942v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Heeju Lim, Victor E. Lachos, Victor H. Lachos</dc:creator>
    </item>
    <item>
      <title>Autocorrelation Test under Frequent Mean Shifts</title>
      <link>https://arxiv.org/abs/2510.21047</link>
      <description>arXiv:2510.21047v1 Announce Type: new 
Abstract: Testing for the presence of autocorrelation is a fundamental problem in time series analysis. Classical methods such as the Box-Pierce test rely on the assumption of stationarity, necessitating the removal of non-stationary components such as trends or shifts in the mean prior to application. However, this is not always practical, particularly when the mean structure is complex, such as being piecewise constant with frequent shifts. In this work, we propose a new inferential framework for autocorrelation in time series data under frequent mean shifts. In particular, we introduce a Shift-Immune Portmanteau (SIP) test that reliably tests for autocorrelation and is robust against mean shifts. We illustrate an application of our method to nanopore sequencing data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21047v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyang Liu, Ning Hao, Yue Selena Niu, Han Xiao, Hongxu Ding</dc:creator>
    </item>
    <item>
      <title>Sensitivity Analysis when Generalizing Causal Effects from Multiple Studies to a Target Population: Motivation from the ECHO Program</title>
      <link>https://arxiv.org/abs/2510.21116</link>
      <description>arXiv:2510.21116v1 Announce Type: new 
Abstract: Unobserved effect modifiers can induce bias when generalizing causal effect estimates to target populations. In this work, we extend a sensitivity analysis framework assessing the robustness of study results to unobserved effect modification that adapts to various generalizability scenarios, including multiple (conditionally) randomized trials, observational studies, or combinations thereof. This framework is interpretable and does not rely on distributional or functional assumptions about unknown parameters. We demonstrate how to leverage the multi-study setting to detect violation of the generalizability assumption through hypothesis testing, showing with simulations that the proposed test achieves high power under real-world sample sizes. Finally, we apply our sensitivity analysis framework to analyze the generalized effect estimate of secondhand smoke exposure on birth weight using cohort sites from the Environmental influences on Child Health Outcomes (ECHO) study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21116v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bolun Liu, Trang Quynh Nguyen, Elizabeth A. Stuart, Bryan Lau, Amii M. Kress, Michael R. Elliott, Kyle R. Busse, Ellen C. Caniglia, Yajnaseni Chakraborti, Amy J. Elliott, James E. Gern, Alison E. Hipwell, Catherine J. Karr, Kaja Z. LeWinn, Li Luo, Hans-Georg M\"uller, Sunni L. Mumford, Ruby H. N. Nguyen, Emily Oken, Janet L. Peacock, Enrique F. Schisterman, Arjun Sondhi, Rosalind J. Wright, Yidong Zhou, Elizabeth L. Ogburn</dc:creator>
    </item>
    <item>
      <title>Leveraging semantic similarity for experimentation with AI-generated treatments</title>
      <link>https://arxiv.org/abs/2510.21119</link>
      <description>arXiv:2510.21119v1 Announce Type: new 
Abstract: Large Language Models (LLMs) enable a new form of digital experimentation where treatments combine human and model-generated content in increasingly sophisticated ways. The main methodological challenge in this setting is representing these high-dimensional treatments without losing their semantic meaning or rendering analysis intractable. Here, we address this problem by focusing on learning low-dimensional representations that capture the underlying structure of such treatments. These representations enable downstream applications such as guiding generative models to produce meaningful treatment variants and facilitating adaptive assignment in online experiments. We propose double kernel representation learning, which models the causal effect through the inner product of kernel-based representations of treatments and user covariates. We develop an alternating-minimization algorithm that learns these representations efficiently from data and provides convergence guarantees under a low-rank factor model. As an application of this framework, we introduce an adaptive design strategy for online experimentation and demonstrate the method's effectiveness through numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21119v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Lei Shi, David Arbour, Raghavendra Addanki, Ritwik Sinha, Avi Feller</dc:creator>
    </item>
    <item>
      <title>Expectation-propagation for Bayesian empirical likelihood inference</title>
      <link>https://arxiv.org/abs/2510.21174</link>
      <description>arXiv:2510.21174v1 Announce Type: new 
Abstract: Bayesian inference typically relies on specifying a parametric model that approximates the data-generating process. However, misspecified models can yield poor convergence rates and unreliable posterior calibration. Bayesian empirical likelihood offers a semi-parametric alternative by replacing the parametric likelihood with a profile empirical likelihood defined through moment constraints, thereby avoiding explicit distributional assumptions. Despite these advantages, Bayesian empirical likelihood faces substantial computational challenges, including the need to solve a constrained optimization problem for each likelihood evaluation and difficulties with non-convex posterior support, particularly in small-sample settings. This paper introduces a variational approach based on expectation-propagation to approximate the Bayesian empirical-likelihood posterior, balancing computational cost and accuracy without altering the target posterior via adjustments such as pseudo-observations. Empirically, we show that our approach can achieve a superior cost-accuracy trade-off relative to existing methods, including Hamiltonian Monte Carlo and variational Bayes. Theoretically, we show that the approximation and the Bayesian empirical-likelihood posterior are asymptotically equivalent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21174v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kenyon Ng, Weichang Yu, Howard D. Bondell</dc:creator>
    </item>
    <item>
      <title>Forecast reconciliation with non-linear constraints</title>
      <link>https://arxiv.org/abs/2510.21249</link>
      <description>arXiv:2510.21249v1 Announce Type: new 
Abstract: Methods for forecasting time series adhering to linear constraints have seen notable development in recent years, especially with the advent of forecast reconciliation. This paper extends forecast reconciliation to the open question of non-linearly constrained time series. Non-linear constraints can emerge with variables that are formed as ratios such as mortality rates and unemployment rates. On the methodological side, Non-linearly Constrained Reconciliation (NLCR) is proposed. This algorithm adjusts forecasts that fail to meet non-linear constraints, in a way that ensures the new forecasts meet the constraints. The NLCR method is a projection onto a non-linear surface, formulated as a constrained optimisation problem. On the theoretical side, optimisation methods are again used, this time to derive sufficient conditions for when the NLCR methodology is guaranteed to improve forecast accuracy. Finally on the empirical side, NLCR is applied to two datasets from demography and economics and shown to significantly improve forecast accuracy relative to relevant benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21249v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniele Girolimetto, Anastasios Panagiotelis, Tommaso Di Fonzo, Han Li</dc:creator>
    </item>
    <item>
      <title>A Comparison for Non-Specialists of Workflow Steps and Similarity of Factor Rankings for Several Global Sensitivity Analysis Methods</title>
      <link>https://arxiv.org/abs/2510.21579</link>
      <description>arXiv:2510.21579v1 Announce Type: new 
Abstract: Global sensitivity analysis (GSA) is a recommended step in the use of computer simulation models. GSA quantifies the relative importance of model inputs on outputs (Factor Ranking), identifies inputs that could be fixed, thus simplifying model calibration (Factor Fixing), and pinpointing areas for future data collection (Factor Prioritization). Given the wide variety of GSA methods, choosing between methods can be challenging for non-GSA experts. Issues include workflow steps and complexity, interpretation of GSA outputs, and the degree of similarity between methods in Factor Ranking. We conducted a study of both widely and less commonly used GSA methods applied to three simulators of differing complexity. All methods share common issues around implementation with specification of parameter ranges particularly critical. Similarities in Factor Rankings were generally high based on Kendall's W. Sobol' first order and total sensitivity indices were easy to interpret and informative with regression trees providing additional insight into interactions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21579v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ken Newman, Shaini Naha, Leah Jackson-Blake, Cairistiona Topp, Miriam Glendell, Adam Butler</dc:creator>
    </item>
    <item>
      <title>MECfda: An R Package for Bias Correction Due to Measurement Error in Functional and Scalar Covariates in Scalar-on-Function Regression Models</title>
      <link>https://arxiv.org/abs/2510.21661</link>
      <description>arXiv:2510.21661v1 Announce Type: new 
Abstract: Functional data analysis (FDA) deals with high-resolution data recorded over a continuum, such as time, space or frequency. Device-based assessments of physical activity or sleep are objective yet still prone to measurement error. We present MECfda, an R package that (i) fits scalar-on-function, generalized scalar-on-function, and functional quantile regression models, and (ii) provides bias-corrected estimation when functional covariates are measured with error. By unifying these tools under a consistent syntax, MECfda enables robust inference for FDA applications that involve noisy functional data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21661v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heyang Ji, Carmen Tekwe</dc:creator>
    </item>
    <item>
      <title>Optimal weighted tests for replication studies and the two-trials rule</title>
      <link>https://arxiv.org/abs/2510.21708</link>
      <description>arXiv:2510.21708v1 Announce Type: new 
Abstract: Replication studies for scientific research are an important part of ensuring the reliability and integrity of experimental findings. In the context of clinical trials, the concept of replication has been formalised by the 'two-trials' rule, where two pivotal studies are required to show positive results before a drug can be approved. In experiments testing multiple hypotheses simultaneously, control of the overall familywise error rate (FWER) is additionally required in many contexts. The well-known Bonferroni procedure controls the FWER, and a natural extension is to introduce weights into this procedure to reflect the a-priori importance of hypotheses or to maximise some measure of the overall power of the experiment. In this paper, we consider analysing a replication study using an optimal weighted Bonferroni procedure, with the weights based on the results of the original study that is being replicated and the optimality criterion being to maximise the disjunctive power of the trial (the power to reject at least one non-null hypothesis). We show that using the proposed procedure can lead to a substantial increase in the disjunctive power of the replication study, and is robust to changes in the effect sizes between the two studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21708v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David S. Robertson, Thomas Jaki</dc:creator>
    </item>
    <item>
      <title>A Multiscale Approach for Enhancing Weak Signal Detection</title>
      <link>https://arxiv.org/abs/2510.20828</link>
      <description>arXiv:2510.20828v1 Announce Type: cross 
Abstract: Stochastic resonance (SR), a phenomenon originally introduced in climate modeling, enhances signal detection by leveraging optimal noise levels within non-linear systems. Traditional SR techniques, mainly based on single-threshold detectors, are limited to signals whose behavior does not depend on time. Often large amounts of noise are needed to detect weak signals, which can distort complex signal characteristics. To address these limitations, this study explores multi-threshold systems and the application of SR in multiscale applications using wavelet transforms. In the multiscale domain signals can be analyzed at different levels of resolution to better understand the underlying dynamics.
  We propose a double-threshold detection system that integrates two single-threshold detectors to enhance weak signal detection. We evaluate it both in the original data domain and in the multiscale domain using simulated and real-world signals and compare its performance with existing methods.
  Experimental results demonstrate that, in the original data domain, the proposed double-threshold detector significantly improves weak signal detection compared to conventional single-threshold approaches. Its performance is further improved in the frequency domain, requiring lower noise levels while outperforming existing detection systems. This study advances SR-based detection methodologies by introducing a robust approach to weak signal identification, with potential applications in various disciplines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20828v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dixon Vimalajeewa, Ursula U. Muller, Brani Vidakovic</dc:creator>
    </item>
    <item>
      <title>To MCMC or not to MCMC: Evaluating non-MCMC methods for Bayesian penalized regression</title>
      <link>https://arxiv.org/abs/2510.20947</link>
      <description>arXiv:2510.20947v1 Announce Type: cross 
Abstract: Markov Chain Monte Carlo (MCMC) sampling is computationally expensive, especially for complex models. Alternative methods make simplifying assumptions about the posterior to reduce computational burden, but their impact on predictive performance remains unclear. This paper compares MCMC and non-MCMC methods for high-dimensional penalized regression, examining when computational shortcuts are justified for prediction tasks.
  We conduct a comprehensive simulation study using high-dimensional tabular data, then validate findings with empirical datasets featuring both continuous and binary outcomes. An in-depth analysis of one dataset provides a step-by-step tutorial implementing various algorithms in R.
  Our results show that mean-field variational inference consistently performs comparably to MCMC methods. In simulations, mean-field VI exhibited 3-90\% higher MSE across scenarios while reducing runtime by 7-30x compared to Hamiltonian Monte Carlo. Empirical datasets revealed dramatic speed-ups (100-400x) in some cases with similar or superior predictive performance. However, performance varied: some cases showed over 100x MSE increases with only 30x speed-ups, highlighting the context-dependent nature of these trade-offs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20947v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian D. van Leeuwen, Sara van Erp</dc:creator>
    </item>
    <item>
      <title>Doubly-Regressing Approach for Subgroup Fairness</title>
      <link>https://arxiv.org/abs/2510.21091</link>
      <description>arXiv:2510.21091v1 Announce Type: cross 
Abstract: Algorithmic fairness is a socially crucial topic in real-world applications of AI.
  Among many notions of fairness, subgroup fairness is widely studied when multiple sensitive attributes (e.g., gender, race, age) are present.
  However, as the number of sensitive attributes grows, the number of subgroups increases accordingly, creating heavy computational burdens and data sparsity problem (subgroups with too small sizes).
  In this paper, we develop a novel learning algorithm for subgroup fairness which resolves these issues by focusing on subgroups with sufficient sample sizes as well as marginal fairness (fairness for each sensitive attribute).
  To this end, we formalize a notion of subgroup-subset fairness and introduce a corresponding distributional fairness measure called the supremum Integral Probability Metric (supIPM).
  Building on this formulation, we propose the Doubly Regressing Adversarial learning for subgroup Fairness (DRAF) algorithm, which reduces a surrogate fairness gap for supIPM with much less computation than directly reducing supIPM.
  Theoretically, we prove that the proposed surrogate fairness gap is an upper bound of supIPM.
  Empirically, we show that the DRAF algorithm outperforms baseline methods in benchmark datasets, specifically when the number of sensitive attributes is large so that many subgroups are very small.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21091v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kyungseon Lee, Kunwoong Kim, Jihu Lee, Dongyoon Yang, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Instance-Adaptive Hypothesis Tests with Heterogeneous Agents</title>
      <link>https://arxiv.org/abs/2510.21178</link>
      <description>arXiv:2510.21178v1 Announce Type: cross 
Abstract: We study hypothesis testing over a heterogeneous population of strategic agents with private information. Any single test applied uniformly across the population yields statistical error that is sub-optimal relative to the performance of an oracle given access to the private information. We show how it is possible to design menus of statistical contracts that pair type-optimal tests with payoff structures, inducing agents to self-select according to their private information. This separating menu elicits agent types and enables the principal to match the oracle performance even without a priori knowledge of the agent type. Our main result fully characterizes the collection of all separating menus that are instance-adaptive, matching oracle performance for an arbitrary population of heterogeneous agents. We identify designs where information elicitation is essentially costless, requiring negligible additional expense relative to a single-test benchmark, while improving statistical performance. Our work establishes a connection between proper scoring rules and menu design, showing how the structure of the hypothesis test constrains the elicitable information. Numerical examples illustrate the geometry of separating menus and the improvements they deliver in error trade-offs. Overall, our results connect statistical decision theory with mechanism design, demonstrating how heterogeneity and strategic participation can be harnessed to improve efficiency in hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.21178v1</guid>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Flora C. Shi, Martin J. Wainwright, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Spectral embedding and the latent geometry of multipartite networks</title>
      <link>https://arxiv.org/abs/2202.03945</link>
      <description>arXiv:2202.03945v3 Announce Type: replace 
Abstract: Spectral embedding finds vector representations of the nodes of a network, based on the eigenvectors of a properly constructed matrix, and has found applications throughout science and technology. Many networks are multipartite, meaning that they contain nodes of fundamentally different types, e.g. drugs, diseases and proteins, and edges are only observed between nodes of different types. When the network is multipartite, this paper demonstrates that the node representations obtained via spectral embedding lie near type-specific low-dimensional subspaces of a higher-dimensional ambient space. For this reason we propose a follow-on step after spectral embedding, to recover node representations in their intrinsic rather than ambient dimension, proving uniform consistency under a low-rank, inhomogeneous random graph model. We demonstrate the performance of our procedure on a large 6-partite biomedical network relevant for drug discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.03945v3</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexander Modell, Ian Gallagher, Joshua Cape, Patrick Rubin-Delanchy</dc:creator>
    </item>
    <item>
      <title>A machine learning approach based on survival analysis for IBNR frequencies in non-life reserving</title>
      <link>https://arxiv.org/abs/2312.14549</link>
      <description>arXiv:2312.14549v3 Announce Type: replace 
Abstract: We introduce new approaches for forecasting IBNR (Incurred But Not Reported) frequencies by leveraging individual claims data, which includes accident date, reporting delay, and possibly additional features for every reported claim. A key element of our proposal involves computing development factors, which may be influenced by both the accident date and other features. These development factors serve as the basis for predictions. While we assume close to continuous observations of accident date and reporting delay, the development factors can be expressed at any level of granularity, such as months, quarters, or year and predictions across different granularity levels exhibit coherence. The calculation of development factors relies on the estimation of a hazard function in reverse development time, and we present three distinct methods for estimating this function: the Cox proportional hazard model, a feed-forward neural network, and eXtreme gradient boosting. In all three cases, estimation is based on the same partial likelihood that accommodates left truncation and ties in the data. While the first case is a semi-parametric model that assumes in parts a log linear structure, the two machine learning approaches only assume that the baseline and the other factors are multiplicatively separable. Through an extensive simulation study and real-world data application, our approach demonstrates promising results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14549v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Munir Hiabu, Emil Hofman, Gabriele Pittarello</dc:creator>
    </item>
    <item>
      <title>Objective Bayesian FDR</title>
      <link>https://arxiv.org/abs/2404.00256</link>
      <description>arXiv:2404.00256v2 Announce Type: replace 
Abstract: Here, we develop an objective Bayesian analysis for large-scale datasets. When Bayesian analysis is applied to large-scale datasets, the cut point that provides the posterior probability is usually determined following customs. In this work, we propose setting the cut point in an objective manner, which is determined so as to match the posterior null number with the estimated true null number. The posterior probability obtained using an objective cut point is relatively similar to the real false discovery rate (FDR), which facilitates control of the FDR level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.00256v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiko Hayashi</dc:creator>
    </item>
    <item>
      <title>Outlier-Robust Bayesian Multivariate Analysis with Correlation-Intact Sandwich Mixture</title>
      <link>https://arxiv.org/abs/2508.18004</link>
      <description>arXiv:2508.18004v2 Announce Type: replace 
Abstract: Handling outliers is a fundamental challenge in multivariate data analysis because outliers may distort the structures of correlation or conditional independence. Although robust Bayesian inference has been extensively studied in univariate settings, theoretical results ensuring posterior robustness in multivariate models are scarce. We propose a novel scale mixture of multivariate normals called correlation-intact sandwich mixtures, in which the scale parameters are real values and follow an unfolded log-Pareto distribution. Our theoretical results on posterior robustness in multivariate settings emphasize that the use of a symmetric, super heavy-tailed distribution for scale parameters is essential for achieving posterior robustness against element-wise contamination. The posterior inference for the proposed model is feasible using the developed efficient Gibbs sampling algorithm. The superiority of the proposed method was further illustrated further in simulation and empirical studies using graphical models and multivariate regression in the presence of complex outlier structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.18004v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Discrete Fourier Transform versus Discrete Chi-Square Method</title>
      <link>https://arxiv.org/abs/2509.01540</link>
      <description>arXiv:2509.01540v2 Announce Type: replace 
Abstract: We compare two time series analysis methods, the Discrete Fourier Transform (DFT) and our Discrete Chi-square Method (DCM).
  DCM is designed for detecting many signals superimposed on an unknown trend. The solution for the non-linear DCM model is an ill-posed problem. The backbone of DCM is the Gauss-Markov theorem that the least squares fit is the best unbiased estimator for linear regression models. DCM is a simple numerical time series analysis method that performs a massive number of linear least squares fits. Hence, the data spacing, even or uneven, is irrelevant. We show that our numerical solution for the DCM model fulfils the three conditions of a well-posed problem: existence, uniqueness and stability.The Fisher-test is used to identify the best DCM model from all alternative tested DCM models. The correct DCM model must also pass our Predictivity-test. Our analyses of seven different simulated data samples expose the weaknesses of DFT and the efficiency of DCM. The DCM signal and trend detection depend only on the sample size and the accuracy of data. DCM is an ideal forecasting method because the time span of observations is irrelevant. If the Gauss-Markov theorem is valid, DCM can not fail. We recommend fast sampling of large high quality datasets and the analysis of those datasets using numerical DCM parallel computation Python code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01540v2</guid>
      <category>stat.ME</category>
      <category>astro-ph.IM</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lauri Jetsu</dc:creator>
    </item>
    <item>
      <title>Ensembled Direct Multi-Step forecasting methodology with comparison on macroeconomic and financial data</title>
      <link>https://arxiv.org/abs/2509.13945</link>
      <description>arXiv:2509.13945v2 Announce Type: replace 
Abstract: Accurate forecasts of macroeconomic and financial data, such as GDP, CPI, unemployment rates, and stock indices, are crucial for the success of countries, businesses, and investors, resulting in a constant demand for reliable forecasting models. This research introduces a novel methodology for time series forecasting that combines Ensemble technique with a Direct Multi-Step (DMS) forecasting procedure. This Ensembled Direct Multi-Step (EDMS) approach not only leverages the strengths of both techniques but also capitalizes on their synergy. The ensemble models in the methodology were selected based on performance, complexity, and computational resource requirements, encompassing a full spectrum of model complexities, from simple Linear and Polynomial Regression to medium-complexity ETS and complex LSTM model. Models were weighted based on their performances. In the DMS procedure we limit retraining to one- and five- year/month forecasts for economic and financial data respectively. Standard Iterative Multi-Step (IMS) procedure is employed for other horizons, effectively reducing computational demands while maintaining satisfactory results. The proposed methodology is benchmarked against Ensemble technique conventionally applied to IMS-generated forecasts, utilizing several publicly available macroeconomic and financial datasets. Results demonstrate a significant performance improvement with EDMS methodology, averaging a 33.32% enhancement across the analysed datasets, and sometimes improvement reaching above 60%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13945v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz M. {\L}api\'nski, Krzysztof Zi\'o{\l}kowski</dc:creator>
    </item>
    <item>
      <title>Iterative Data Curation with Theoretical Guarantees</title>
      <link>https://arxiv.org/abs/2510.11428</link>
      <description>arXiv:2510.11428v2 Announce Type: replace 
Abstract: In recent years, more and more large data sets have become available. Data accuracy, the absence of verifiable errors in data, is crucial for these large materials to enable high-quality research, downstream applications, and model training. This results in the problem of how to curate or improve data accuracy in such large and growing data, especially when the data is too large for manual curation to be feasible. This paper presents a unified procedure for iterative and continuous improvement of data sets. We provide theoretical guarantees that data accuracy tests speed up error reduction and, most importantly, that the proposed approach will, asymptotically, eliminate all errors in data with probability one. We corroborate the theoretical results with simulations and a real-world use case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11428v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>V\"ain\"o Yrj\"an\"ainen, Johan Jonasson, M{\aa}ns Magnusson</dc:creator>
    </item>
    <item>
      <title>Regret Distribution in Stochastic Bandits: Optimal Trade-off between Expectation and Tail Risk</title>
      <link>https://arxiv.org/abs/2304.04341</link>
      <description>arXiv:2304.04341v2 Announce Type: replace-cross 
Abstract: We study the optimal trade-off between expectation and tail risk for regret distribution in the stochastic multi-armed bandit model. We fully characterize the interplay among three desired properties for policy design: worst-case optimality, instance-dependent consistency, and light-tailed risk. New policies are proposed to characterize the optimal regret tail probability for any regret threshold. In particular, we discover an intrinsic gap of the optimal tail rate depending on whether the time horizon $T$ is known a priori or not. Interestingly, when it comes to the purely worst-case scenario, this gap disappears. Our results reveal insights on how to design policies that balance between efficiency and safety, and highlight extra insights on policy robustness with regard to policy hyper-parameters and model mis-specification. We also conduct a simulation study to validate our theoretical insights and provide practical amendment to our policies. Finally, we discuss extensions of our results to (i) general sub-exponential environments and (ii) general stochastic linear bandits. Furthermore, we find that a special case of our policy design surprisingly coincides with what was adopted in AlphaGo Monte Carlo Tree Search. Our theory provides high-level insights to why their engineered solution is successful and should be advocated in complex decision-making environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.04341v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>David Simchi-Levi, Zeyu Zheng, Feng Zhu</dc:creator>
    </item>
    <item>
      <title>Smooth Sailing: Lipschitz-Driven Uncertainty Quantification for Spatial Association</title>
      <link>https://arxiv.org/abs/2502.06067</link>
      <description>arXiv:2502.06067v3 Announce Type: replace-cross 
Abstract: Estimating associations between spatial covariates and responses - rather than merely predicting responses - is central to environmental science, epidemiology, and economics. For instance, public health officials might be interested in whether air pollution has a strictly positive association with a health outcome, and the magnitude of any effect. Standard machine learning methods often provide accurate predictions but offer limited insight into covariate-response relationships. And we show that existing methods for constructing confidence (or credible) intervals for associations can fail to provide nominal coverage in the face of model misspecification and nonrandom locations - despite both being essentially always present in spatial problems. We introduce a method that constructs valid frequentist confidence intervals for associations in spatial settings. Our method requires minimal assumptions beyond a form of spatial smoothness and a homoskedastic Gaussian error assumption. In particular, we do not require model correctness or covariate overlap between training and target locations. Our approach is the first to guarantee nominal coverage in this setting and outperforms existing techniques in both real and simulated experiments. Our confidence intervals are valid in finite samples when the noise of the Gaussian error is known, and we provide an asymptotically consistent estimation procedure for this noise variance when it is unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06067v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David R. Burt, Renato Berlinghieri, Stephen Bates, Tamara Broderick</dc:creator>
    </item>
    <item>
      <title>CLT and Edgeworth Expansion for m-out-of-n Bootstrap Estimators of The Studentized Median</title>
      <link>https://arxiv.org/abs/2505.11725</link>
      <description>arXiv:2505.11725v2 Announce Type: replace-cross 
Abstract: The m-out-of-n bootstrap, originally proposed by Bickel, Gotze, and Zwet (1992), approximates the distribution of a statistic by repeatedly drawing m subsamples (with m much smaller than n) without replacement from an original sample of size n. It is now routinely used for robust inference with heavy-tailed data, bandwidth selection, and other large-sample applications. Despite its broad applicability across econometrics, biostatistics, and machine learning, rigorous parameter-free guarantees for the soundness of the m-out-of-n bootstrap when estimating sample quantiles have remained elusive.
  This paper establishes such guarantees by analyzing the estimator of sample quantiles obtained from m-out-of-n resampling of a dataset of size n. We first prove a central limit theorem for a fully data-driven version of the estimator that holds under a mild moment condition and involves no unknown nuisance parameters. We then show that the moment assumption is essentially tight by constructing a counter-example in which the CLT fails. Strengthening the assumptions slightly, we derive an Edgeworth expansion that provides exact convergence rates and, as a corollary, a Berry Esseen bound on the bootstrap approximation error. Finally, we illustrate the scope of our results by deriving parameter-free asymptotic distributions for practical statistics, including the quantiles for random walk Metropolis-Hastings and the rewards of ergodic Markov decision processes, thereby demonstrating the usefulness of our theory in modern estimation and learning tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11725v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Imon Banerjee, Sayak Chakrabarty</dc:creator>
    </item>
    <item>
      <title>STACI: Spatio-Temporal Aleatoric Conformal Inference</title>
      <link>https://arxiv.org/abs/2505.21658</link>
      <description>arXiv:2505.21658v2 Announce Type: replace-cross 
Abstract: Fitting Gaussian Processes (GPs) provides interpretable aleatoric uncertainty quantification for estimation of spatio-temporal fields. Spatio-temporal deep learning models, while scalable, typically assume a simplistic independent covariance matrix for the response, failing to capture the underlying correlation structure. However, spatio-temporal GPs suffer from issues of scalability and various forms of approximation bias resulting from restrictive assumptions of the covariance kernel function. We propose STACI, a novel framework consisting of a variational Bayesian neural network approximation of non-stationary spatio-temporal GP along with a novel spatio-temporal conformal inference algorithm. STACI is highly scalable, taking advantage of GPU training capabilities for neural network models, and provides statistically valid prediction intervals for uncertainty quantification. STACI outperforms competing GPs and deep methods in accurately approximating spatio-temporal processes and we show it easily scales to datasets with millions of observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.21658v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brandon R. Feng, David Keetae Park, Xihaier Luo, Arantxa Urdangarin, Shinjae Yoo, Brian J. Reich</dc:creator>
    </item>
    <item>
      <title>Knowledge Distillation of Uncertainty using Deep Latent Factor Model</title>
      <link>https://arxiv.org/abs/2510.19290</link>
      <description>arXiv:2510.19290v2 Announce Type: replace-cross 
Abstract: Deep ensembles deliver state-of-the-art, reliable uncertainty quantification, but their heavy computational and memory requirements hinder their practical deployments to real applications such as on-device AI. Knowledge distillation compresses an ensemble into small student models, but existing techniques struggle to preserve uncertainty partly because reducing the size of DNNs typically results in variation reduction. To resolve this limitation, we introduce a new method of distribution distillation (i.e. compressing a teacher ensemble into a student distribution instead of a student ensemble) called Gaussian distillation, which estimates the distribution of a teacher ensemble through a special Gaussian process called the deep latent factor model (DLF) by treating each member of the teacher ensemble as a realization of a certain stochastic process. The mean and covariance functions in the DLF model are estimated stably by using the expectation-maximization (EM) algorithm. By using multiple benchmark datasets, we demonstrate that the proposed Gaussian distillation outperforms existing baselines. In addition, we illustrate that Gaussian distillation works well for fine-tuning of language models and distribution shift problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19290v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sehyun Park, Jongjin Lee, Yunseop Shin, Ilsang Ohn, Yongdai Kim</dc:creator>
    </item>
    <item>
      <title>Testing Most Influential Sets</title>
      <link>https://arxiv.org/abs/2510.20372</link>
      <description>arXiv:2510.20372v2 Announce Type: replace-cross 
Abstract: Small subsets of data with disproportionate influence on model outcomes can have dramatic impacts on conclusions, with a few data points sometimes overturning key findings. While recent work has developed methods to identify these most influential sets, no formal theory exists to determine when their influence reflects genuine problems rather than natural sampling variation. We address this gap by developing a principled framework for assessing the statistical significance of most influential sets. Our theoretical results characterize the extreme value distributions of maximal influence and enable rigorous hypothesis tests for excessive influence, replacing current ad-hoc sensitivity checks. We demonstrate the practical value of our approach through applications across economics, biology, and machine learning benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20372v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lucas Darius Konrad, Nikolas Kuschnig</dc:creator>
    </item>
    <item>
      <title>Incomplete U-Statistics of Equireplicate Designs: Berry-Esseen Bound and Efficient Construction</title>
      <link>https://arxiv.org/abs/2510.20755</link>
      <description>arXiv:2510.20755v2 Announce Type: replace-cross 
Abstract: U-statistics are a fundamental class of estimators that generalize the sample mean and underpin much of nonparametric statistics. Although extensively studied in both statistics and probability, key challenges remain: their high computational cost - addressed partly through incomplete U-statistics - and their non-standard asymptotic behavior in the degenerate case, which typically requires resampling methods for hypothesis testing. This paper presents a novel perspective on U-statistics, grounded in hypergraph theory and combinatorial designs. Our approach bypasses the traditional Hoeffding decomposition, the main analytical tool in this literature but one highly sensitive to degeneracy. By characterizing the dependence structure of a U-statistic, we derive a Berry-Esseen bound valid for incomplete U-statistics of deterministic designs, yielding conditions under which Gaussian limiting distributions can be established even in degenerate cases and when the order diverges. We also introduce efficient algorithms to construct incomplete U-statistics of equireplicate designs, a subclass of deterministic designs that, in certain cases, achieve minimum variance. Finally, we apply our framework to kernel-based tests that use Maximum Mean Discrepancy (MMD) and Hilbert-Schmidt Independence Criterion. In a real data example with the CIFAR-10 dataset, our permutation-free MMD test delivers substantial computational gains while retaining power and type I error control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20755v2</guid>
      <category>math.ST</category>
      <category>math.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 27 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cesare Miglioli, Jordan Awan</dc:creator>
    </item>
  </channel>
</rss>
