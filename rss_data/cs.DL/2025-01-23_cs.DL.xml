<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Jan 2025 02:40:23 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Preserving Culinary Traditions. A Crowdsourced Digital Collection of Cookbooks</title>
      <link>https://arxiv.org/abs/2501.12786</link>
      <description>arXiv:2501.12786v1 Announce Type: new 
Abstract: Recipes of popular origin and handwritten cookbooks are often overlooked by scholars. Rag\`u is a pilot project that tries to fill in this gap by gathering and digitising a collection of cookbooks belonging to the Italian traditional cuisine, and making it accessible via a digital platform. The project aims at contributing to two research lines: a) to identify agile methods for publishing data in a low-cost crowdsourcing project, and b) to devise an effective storytelling journey for the Rag\`u project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12786v1</guid>
      <category>cs.DL</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.6092/unibo/amsacta/7927</arxiv:DOI>
      <arxiv:journal_reference>Proceedings del XIII Convegno Annuale AIUCD2024, A cura di: Di Silvestro, Antonio ; Spampinato, Daria (2024). ISBN 978-88-942535-8-0</arxiv:journal_reference>
      <dc:creator>Giulia Renda, Giulia Manganelli, Mila Fumini, Marilena Daquino</dc:creator>
    </item>
    <item>
      <title>A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering</title>
      <link>https://arxiv.org/abs/2501.12728</link>
      <description>arXiv:2501.12728v1 Announce Type: cross 
Abstract: Context: Empirical Software Engineering (ESE) drives innovation in SE through qualitative and quantitative studies. However, concerns about the correct application of empirical methodologies have existed since the 2006 Dagstuhl seminar on SE. Objective: To analyze three decades of SE research, identify mistakes in statistical methods, and evaluate experts' ability to detect and address these issues. Methods: We conducted a literature survey of ~27,000 empirical studies, using LLMs to classify statistical methodologies as adequate or inadequate. Additionally, we selected 30 primary studies and held a workshop with 33 ESE experts to assess their ability to identify and resolve statistical issues. Results: Significant statistical issues were found in the primary studies, and experts showed limited ability to detect and correct these methodological problems, raising concerns about the broader ESE community's proficiency in this area. Conclusions. Despite our study's eventual limitations, its results shed light on recurring issues from promoting information copy-and-paste from past authors' works and the continuous publication of inadequate approaches that promote dubious results and jeopardize the spread of the correct statistical strategies among researchers. Besides, it justifies further investigation into empirical rigor in software engineering to expose these recurring issues and establish a framework for reassessing our field's foundation of statistical methodology application. Therefore, this work calls for critically rethinking and reforming data analysis in empirical software engineering, paving the way for our work soon.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12728v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matteo Esposito, Mikel Robredo, Murali Sridharan, Guilherme Horta Travassos, Rafael Pe\~naloza, Valentina Lenarduzzi</dc:creator>
    </item>
    <item>
      <title>Ontology-Enhanced Educational Annotation Activities</title>
      <link>https://arxiv.org/abs/2501.12943</link>
      <description>arXiv:2501.12943v1 Announce Type: cross 
Abstract: Information and communications technology and technology-enhanced learning have unquestionably transformed traditional teaching-learning processes and are positioned as key factors to promote quality education, one of the basic sustainable development goals of the 2030 agenda. Document annotation, which was traditionally carried out with pencil and paper and currently benefits from digital document annotation tools, is a representative example of this transformation. Using document annotation tools, students can enrich the documents with annotations that highlight the most relevant aspects of these documents. As the conceptual complexity of the learning domain increases, the annotation of the documents may require comprehensive domain knowledge and an expert analysis capability that students usually lack. Consequently, a proliferation of irrelevant, incorrect, and/or poorly decontextualized annotations may appear, while other relevant aspects are completely ignored by the students. The main hypothesis proposed by this paper is that the use of a guiding annotation ontology in the annotation activities is a keystone aspect to alleviate these shortcomings. Consequently, comprehension is improved, exhaustive content analysis is promoted, and meta-reflective thinking is developed. To test this hypothesis, we describe our own annotation tool, \@note, which fully implements this ontology-enhanced annotation paradigm, and we provide experimental evidence about how \@note can improve academic performance via a pilot study concerning critical literary annotation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12943v1</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.3390/su11164455</arxiv:DOI>
      <arxiv:journal_reference>Sustainability, 2019</arxiv:journal_reference>
      <dc:creator>Joaqu\'i Gayoso-Cabada, Mar\'ia Goicoechea-de-Jorge, Mercedes G\'omez-Albarr\'an, Amelia Sanz-Cabrerizo, Antonio Sarasa-Cabezuelo, Jos\'e-Luis Sierra</dc:creator>
    </item>
    <item>
      <title>DASCH: Bringing 100+ Years of Photographic Data into the 21st Century and Beyond</title>
      <link>https://arxiv.org/abs/2501.12977</link>
      <description>arXiv:2501.12977v1 Announce Type: cross 
Abstract: The Harvard College Observatory was the preeminent astronomical data center of the early 20th century: it gathered and archived an enormous collection of glass photographic plates that became, and remains, the largest in the world. For nearly twenty years DASCH (Digital Access to a Sky Century @ Harvard) actively digitized this library using a one-of-a kind plate scanner. In early 2024, after 470,000 scans, the DASCH project finished. Now, this unique analog dataset can be integrated into 21st-century, digital analyses. The key DASCH data products include ~200 TB of plate images, ~16 TB of calibrated light curves, and a variety of supporting metadata and calibration outputs. Virtually every part of the sky is covered by thousands of DASCH images with a time baseline spanning more than 100 years; most stars brighter than B ~ 15 have hundreds or thousands of detections. DASCH Data Release 7, issued in late 2024, represents the culmination of the DASCH scanning project.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.12977v1</guid>
      <category>astro-ph.IM</category>
      <category>cs.DL</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter K. G. Williams (Center for Astrophysics | Harvard &amp; Smithsonian)</dc:creator>
    </item>
    <item>
      <title>CLOCR-C: Context Leveraging OCR Correction with Pre-trained Language Models</title>
      <link>https://arxiv.org/abs/2408.17428</link>
      <description>arXiv:2408.17428v2 Announce Type: replace-cross 
Abstract: The digitisation of historical print media archives is crucial for increasing accessibility to contemporary records. However, the process of Optical Character Recognition (OCR) used to convert physical records to digital text is prone to errors, particularly in the case of newspapers and periodicals due to their complex layouts. This paper introduces Context Leveraging OCR Correction (CLOCR-C), which utilises the infilling and context-adaptive abilities of transformer-based language models (LMs) to improve OCR quality. The study aims to determine if LMs can perform post-OCR correction, improve downstream NLP tasks, and the value of providing the socio-cultural context as part of the correction process. Experiments were conducted using seven LMs on three datasets: the 19th Century Serials Edition (NCSE) and two datasets from the Overproof collection. The results demonstrate that some LMs can significantly reduce error rates, with the top-performing model achieving over a 60\% reduction in character error rate on the NCSE dataset. The OCR improvements extend to downstream tasks, such as Named Entity Recognition, with increased Cosine Named Entity Similarity. Furthermore, the study shows that providing socio-cultural context in the prompts improves performance, while misleading prompts lower performance. In addition to the findings, this study releases a dataset of 91 transcribed articles from the NCSE, containing a total of 40 thousand words, to support further research in this area. The findings suggest that CLOCR-C is a promising approach for enhancing the quality of existing digital archives by leveraging the socio-cultural information embedded in the LMs and the text requiring correction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.17428v2</guid>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <pubDate>Thu, 23 Jan 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Bourne</dc:creator>
    </item>
  </channel>
</rss>
