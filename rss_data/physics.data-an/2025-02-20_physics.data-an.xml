<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Feb 2025 05:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Abstraction requires breadth: a renormalisation group approach</title>
      <link>https://arxiv.org/abs/2407.01656</link>
      <description>arXiv:2407.01656v3 Announce Type: replace-cross 
Abstract: Abstraction is the process of extracting the essential features from raw data while ignoring irrelevant details. This is similar to the process of focusing on large-scale properties, systematically removing irrelevant small-scale details, implemented in the renormalisation group of statistical physics. This analogy is suggestive because the fixed points of the renormalisation group offer an ideal candidate of a truly abstract -- i.e. data independent -- representation. It has been observed that abstraction emerges with depth in neural networks. Deep layers of neural network capture abstract characteristics of data, such as "cat-ness" or "dog-ness" in images, by combining the lower level features encoded in shallow layers (e.g. edges). Yet we argue that depth alone is not enough to develop truly abstract representations. We advocate that the level of abstraction crucially depends on how broad the training set is. We address the issue within a renormalisation group approach where a representation is expanded to encompass a broader set of data. We take the unique fixed point of this transformation -- the Hierarchical Feature Model -- as a candidate for an abstract representation. This theoretical picture is tested in numerical experiments based on Deep Belief Networks trained on data of different breadth. These show that representations in deep layers of neural networks approach the Hierarchical Feature Model as the data gets broader, in agreement with theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.01656v3</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Carlo Orientale Caputo, Elias Seiffert, Matteo Marsili</dc:creator>
    </item>
    <item>
      <title>A Novel Constrained Sampling Method for Efficient Exploration in Materials and Chemical Mixture Design</title>
      <link>https://arxiv.org/abs/2407.16567</link>
      <description>arXiv:2407.16567v3 Announce Type: replace-cross 
Abstract: Efficient exploration of multicomponent material composition spaces is often limited by time and financial constraints, particularly when mixture and synthesis constraints exist. Traditional methods like Latin hypercube sampling (LHS) struggle with constrained problems especially in high dimensions, while emerging approaches like Bayesian optimization (BO) face challenges in early-stage exploration. This article introduces ConstrAined Sequential laTin hypeRcube sampling methOd (CASTRO), an open-source tool designed to address these challenges. CASTRO is optimized for uniform sampling in constrained small- to moderate-dimensional spaces, with scalability to higher dimensions through future adaptations. CASTRO uses a divide-and-conquer strategy to decompose problems into parallel subproblems, improving efficiency and scalability. It effectively handles equality-mixture constraints, ensuring comprehensive design space coverage and leveraging LHS and LHS with multidimensional uniformity (LHSMDU). It also integrates prior experimental knowledge, making it well-suited for efficient exploration within limited budgets. Validation through two material design case studies, a four-dimensional problem with near-uniform distributions and a nine-dimensional problem with additional synthesis constraints, demonstrates CASTRO's effectiveness in exploring constrained design spaces for materials science, pharmaceuticals and chemicals. The software and case studies are available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16567v3</guid>
      <category>stat.CO</category>
      <category>cond-mat.mtrl-sci</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Christina Schenk, Maciej Haranczyk</dc:creator>
    </item>
    <item>
      <title>NEAR: A Training-Free Pre-Estimator of Machine Learning Model Performance</title>
      <link>https://arxiv.org/abs/2408.08776</link>
      <description>arXiv:2408.08776v2 Announce Type: replace-cross 
Abstract: Artificial neural networks have been shown to be state-of-the-art machine learning models in a wide variety of applications, including natural language processing and image recognition. However, building a performant neural network is a laborious task and requires substantial computing power. Neural Architecture Search (NAS) addresses this issue by an automatic selection of the optimal network from a set of potential candidates. While many NAS methods still require training of (some) neural networks, zero-cost proxies promise to identify the optimal network without training. In this work, we propose the zero-cost proxy \textit{Network Expressivity by Activation Rank} (NEAR). It is based on the effective rank of the pre- and post-activation matrix, i.e., the values of a neural network layer before and after applying its activation function. We demonstrate the cutting-edge correlation between this network score and the model accuracy on NAS-Bench-101 and NATS-Bench-SSS/TSS. In addition, we present a simple approach to estimate the optimal layer sizes in multi-layer perceptrons. Furthermore, we show that this score can be utilized to select hyperparameters such as the activation function and the neural network weight initialization scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08776v2</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>physics.chem-ph</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>13th International Conference on Learning Representations, ICLR 2025, Singapore</arxiv:journal_reference>
      <dc:creator>Raphael T. Husistein, Markus Reiher, Marco Eckhoff</dc:creator>
    </item>
    <item>
      <title>An a posteriori data-driven method for phase-averaged optical measurements</title>
      <link>https://arxiv.org/abs/2502.12369</link>
      <description>arXiv:2502.12369v2 Announce Type: replace-cross 
Abstract: Phase-averaging is a fundamental approach for investigating periodic and non-stationary phenomena. In fluid dynamics, these can be generated by rotating blades such as propellers/turbines or by pulsed jets. Traditional phase-averaging approaches often rely on synchronized data acquisition systems, which might require high-speed cameras, light sources, and precise delay generators and encoders, making them expensive and sometimes unfeasible. This work proposes an a posteriori data-driven approach that reconstructs phase information from randomly acquired uncorrelated photographic frames (snapshots) using the ISOMAP algorithm. The technique enables accurate reordering of snapshots in the phase space and subsequent computation of the phase-averaged flow field without the need for synchronization. The framework was validated through numerical simulations and experimental fluid dynamics datasets from an optical setup featuring single- and multi-propeller configurations. The results demonstrate that the proposed method effectively captures the periodic flow characteristics while addressing the challenges related to synchronization and hardware limitations. Furthermore, the ability to apply this technique to archival datasets extends its applicability to a wide range of experimental fluid dynamics studies. This approach provides a scalable and cost-effective alternative to traditional methods for the analysis of periodic phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12369v2</guid>
      <category>physics.flu-dyn</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 20 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrico Amico, Sara Montagner, Jacopo Serpieri, Gioacchino Cafiero</dc:creator>
    </item>
  </channel>
</rss>
