<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jan 2026 05:01:49 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Marked point processes intensity estimation using sparse group Lasso method applied to locations of lucrative and cooperative banks in mainland France</title>
      <link>https://arxiv.org/abs/2512.23772</link>
      <description>arXiv:2512.23772v1 Announce Type: new 
Abstract: In this paper, we model the locations of five major banks in mainland France, two lucrative and three cooperative institutions based on socio-economic considerations. Locations of banks are collected using web scrapping and constitute a bivariate spatial point process for which we estimate nonparametrically summary functions (intensity, Ripley and cross-Ripley's K functions). This shows that the pattern is highly inhomogenenous and exhibits a clustering effect especially at small scales, and thus a significant departure to the bivariate (inhomogeneous) Poisson point process is pointed out. We also collect socio-economic datasets (at the living area level) from INSEE and propose a parametric modelling of the intensity function using these covariates. We propose a group-penalized bivariate composite likelihood method to estimate the model parameters, and we establish its asymptotic properties. The application of the methodology to the banking dataset provides new insights into the specificity of the cooperative model within the sector, particularly in relation to the theories of institutional isomorphism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23772v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Am\'elie Artis (PACTE), Achmad Choiruddin (SVH), Jean-Fran\c{c}ois Coeurjolly (SVH), Fr\'ed\'erique Letu\'e (SVH)</dc:creator>
    </item>
    <item>
      <title>A Fuzzy Approach for Randomized Confidence Intervals</title>
      <link>https://arxiv.org/abs/2512.23866</link>
      <description>arXiv:2512.23866v1 Announce Type: new 
Abstract: We propose randomized confidence intervals based on the Neyman-Pearson lemma, in order to make them more broadly applicable to distributions that do not satisfy regularity conditions. This is achieved by using the definition of fuzzy confidence intervals. These intervals are compared with methods described in the literature for well-known distributions such as normal, binomial, and Poisson. The results show that in high-variance situations, the new intervals provide better performance. Furthermore, through these intervals, it is possible to compute a lower bound for the expected length, demonstrating that they achieve the minimal maximum expected length for a Bernoulli trial observation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23866v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Carlos Henrique Trigo Nasser Felix, Nancy Lopes Garcia, Alex Rodrigo dos Santos Sousa</dc:creator>
    </item>
    <item>
      <title>Completing and studentising Spearman's correlation in the presence of ties</title>
      <link>https://arxiv.org/abs/2512.23993</link>
      <description>arXiv:2512.23993v1 Announce Type: new 
Abstract: Non-parametric correlation coefficients have been widely used for analysing arbitrary random variables upon common populations, when requiring an explicit error distribution to be known is an unacceptable assumption. We examine an \(\ell_{2}\) representation of a correlation coefficient (Emond and Mason, 2002) from the perspective of a statistical estimator upon random variables, and verify a number of interesting and highly desirable mathematical properties, mathematically similar to the Whitney embedding of a Hilbert space into the \(\ell_{2}\)-norm space. In particular, we show here that, in comparison to the traditional Spearman (1904) \(\rho\), the proposed Kemeny \(\rho_{\kappa}\) correlation coefficient satisfies Gauss-Markov conditions in the presence or absence of ties, thereby allowing both discrete and continuous marginal random variables. We also prove under standard regularity conditions a number of desirable scenarios, including the construction of a null hypothesis distribution which is Student-t distributed, parallel to standard practice with Pearson's r, but without requiring either continuous random variables nor particular Gaussian errors. Simulations in particular focus upon highly kurtotic data, with highly nominal empirical coverage consistent with theoretical expectation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23993v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>Least Square Estimation: SDEs Perturbed by L\'evy Noise with Sparse Sample Paths</title>
      <link>https://arxiv.org/abs/2512.24005</link>
      <description>arXiv:2512.24005v1 Announce Type: new 
Abstract: This article investigates the least squares estimators (LSE) for the unknown parameters in stochastic differential equations (SDEs) that are affected by L\'evy noise, particularly when the sample paths are sparse. Specifically, given $n$ sparsely observed curves related to this model, we derive the least squares estimators for the unknown parameters: the drift coefficient, the diffusion coefficient, and the jump-diffusion coefficient. We also establish the asymptotic rate of convergence for the proposed LSE estimators. Additionally, in the supplementary materials, the proposed methodology is applied to a benchmark dataset of functional data/curves, and a small simulation study is conducted to illustrate the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24005v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brijesh Kumar Jha, Subhra Sankar Dhar, Akash Ashirbad Panda</dc:creator>
    </item>
    <item>
      <title>An exact unbiased semi-parametric maximum quasi-likelihood framework which is complete in the presence of ties</title>
      <link>https://arxiv.org/abs/2512.24009</link>
      <description>arXiv:2512.24009v1 Announce Type: new 
Abstract: This paper introduces a novel quasi-likelihood extension of the generalised Kendall \(\tau_{a}\) estimator, together with an extension of the Kemeny metric and its associated covariance and correlation forms. The central contribution is to show that the U-statistic structure of the proposed coefficient \(\tau_{\kappa}\) naturally induces a quasi-maximum likelihood estimation (QMLE) framework, yielding consistent Wald and likelihood ratio test statistics. The development builds on the uncentred correlation inner-product (Hilbert space) formulation of Emond and Mason (2002) and resolves the associated sub-Gaussian likelihood optimisation problem under the \(\ell_{2}\)-norm via an Edgeworth expansion of higher-order moments. The Kemeny covariance coefficient \(\tau_{\kappa}\) is derived within a novel likelihood framework for pairwise comparison-continuous random variables, enabling direct inference on population-level correlation between ranked or weakly ordered datasets. Unlike existing approaches that focus on marginal or pairwise summaries, the proposed framework supports sample-observed weak orderings and accommodates ties without information loss. Drawing parallels with Thurstone's Case V latent ordering model, we derive a quasi-likelihood-based tie model with analytic standard errors, generalising classical U-statistics. The framework applies to general continuous and discrete random variables and establishes formal equivalence to Bradley-Terry and Thurstone models, yielding a uniquely identified linear representation with both analytic and likelihood-based estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24009v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>A Robust Persistent Homology : Trimming Approach</title>
      <link>https://arxiv.org/abs/2512.24222</link>
      <description>arXiv:2512.24222v1 Announce Type: new 
Abstract: This article studies the robust version of persistent homology based on trimming methodology to capture the geometric feature through support of the data in presence of outliers. Precisely speaking, the proposed methodology works when the outliers lie outside the main data cloud as well as inside the data cloud. In the course of theoretical study, it is established that the Bottleneck distance between the proposed robust version of persistent homology and its population analogue can be made arbitrary small with a certain rate for a sufficiently large sample size. The practicability of the methodology is shown for various simulated data and bench mark real data associated with cellular biology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24222v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tuhin Subhra Mahato, Subhra Sankar Dhar</dc:creator>
    </item>
    <item>
      <title>Valid and Efficient Two-Stage Latent Subgroup Analysis with Observational Data</title>
      <link>https://arxiv.org/abs/2512.24223</link>
      <description>arXiv:2512.24223v1 Announce Type: new 
Abstract: Subgroup analysis evaluates treatment effects across multiple sub-populations. When subgroups are defined by latent memberships inferred from imperfect measurements, the analysis typically involves two inter-connected models, a latent class model and a subgroup outcome model. The classical one-stage framework, which models the joint distribution of the two models, may be infeasible with observational data containing many confounders. The two-stage framework, which first estimates the latent class model and then performs subgroup analysis using estimated latent memberships, can accommodate potential confounders but may suffer from bias issues due to misclassification of latent subgroup memberships. This paper focuses on latent subgroups inferred from binary item responses and addresses when and how a valid two-stage latent subgroup analysis can be made with observational data. We investigate the maximum misclassification rate that a valid two-stage framework can tolerate. Introducing a spectral method perspective, we propose a two-stage approach to achieve the desired misclassification rate with the blessing of many item responses. Our method accommodates high-dimensional confounders, is computationally efficient and robust to noninformative items. In observational studies, our methods lead to consistent estimation and valid inference on latent subgroup effects. We demonstrate its merit through simulation studies and an application to educational assessment data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24223v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanhui Luo, Xinzhou Guo, Yuqi Gu</dc:creator>
    </item>
    <item>
      <title>A Novel Approach for Data Integration with Multiple Heterogeneous Data Sources</title>
      <link>https://arxiv.org/abs/2512.24342</link>
      <description>arXiv:2512.24342v1 Announce Type: new 
Abstract: The integration of data from multiple sources is increasingly used to achieve larger sample sizes and enhance population diversity. Our previous work established that, under random sampling from the same underlying population, integrating large incomplete datasets with summary-level data produces unbiased parameter estimates. In this study, we develop a novel statistical framework that enables the integration of summary-level data with information from heterogeneous data sources by leveraging auxiliary information. The proposed approach estimates study-specific sampling weights using this auxiliary information and calibrates the estimating equations to obtain the full set of model parameters. We evaluate the performance of the proposed method through simulation studies under various sampling designs and illustrate its application by reanalyzing U.S. cancer registry data combined with summary-level odds ratio estimates for selected colorectal cancer (CRC) risk factors, while relaxing the random sampling assumption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24342v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Farimah Shamsi, Andriy Derkach</dc:creator>
    </item>
    <item>
      <title>Bayesian inference for functional extreme events defined via partially unobserved processes</title>
      <link>https://arxiv.org/abs/2512.24356</link>
      <description>arXiv:2512.24356v1 Announce Type: new 
Abstract: In order to describe the extremal behaviour of some stochastic process $X$, approaches from univariate extreme value theory are typically generalized to the spatial domain. In particular, generalized peaks-over-threshold approaches allow for the consideration of single extreme events. These can be flexibly defined as exceedances of a risk functional $r$, such as a spatial average, applied to $X$. Inference for the resulting limit process, the so-called $r$-Pareto process, requires the evaluation of $r(X)$ and thus the knowledge of the whole process $X$. In many practical applications, however, observations of $X$ are only available at scattered sites. To overcome this issue, we propose a two-step MCMC-algorithm in a Bayesian framework. In a first step, we sample from $X$ conditionally on the observations in order to evaluate which observations lead to $r$-exceedances. In a second step, we use these exceedances to sample from the posterior distribution of the parameters of the limiting $r$-Pareto process. Alternating these steps results in a full Bayesian model for the extremes of $X$. We show that, under appropriate assumptions, the probability of classifying an observation as $r$-exceedance in the first step converges to the desired probability. Furthermore, given the first step, the distribution of the Markov chain constructed in the second step converges to the posterior distribution of interest. The procedure is compared to the Bayesian version of the standard procedure in a simulation study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24356v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Thannheimer, Marco Oesting</dc:creator>
    </item>
    <item>
      <title>Geometric criteria for identifying extremal dependence and flexible modeling via additive mixtures</title>
      <link>https://arxiv.org/abs/2512.24392</link>
      <description>arXiv:2512.24392v1 Announce Type: new 
Abstract: The framework of geometric extremes is based on the convergence of scaled sample clouds onto a limit set, characterized by a gauge function, with the shape of the limit set determining extremal dependence structures. While it is known that a blunt limit set implies asymptotic independence, the absence of bluntness can be linked to both asymptotic dependence and independence. Focusing on the bivariate case, under a truncated gamma modeling assumption with bounded angular density, we show that a ``pointy'' limit set implies asymptotic dependence, thus offering practical geometric criteria for identifying extremal dependence classes. Suitable models for the gauge function offer the ability to capture asymptotically independent or dependent data structures, without requiring prior knowledge of the true extremal dependence structure. The geometric approach thus offers a simple alternative to various parametric copula models that have been developed for this purpose in recent years. We consider two types of additively mixed gauge functions that provide a smooth interpolation between asymptotic dependence and asymptotic independence. We derive their explicit forms, explore their properties, and establish connections to the developed geometric criteria. Through a simulation study, we evaluate the effectiveness of the geometric approach with additively mixed gauge functions, comparing its performance to existing methodologies that account for both asymptotic dependence and asymptotic independence. The methodology is computationally efficient and yields reliable performance across various extremal dependence scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24392v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeongjin Lee, Jennifer Wadsworth</dc:creator>
    </item>
    <item>
      <title>Demystifying Proximal Causal Inference</title>
      <link>https://arxiv.org/abs/2512.24413</link>
      <description>arXiv:2512.24413v1 Announce Type: new 
Abstract: Proximal causal inference (PCI) has emerged as a promising framework for identifying and estimating causal effects in the presence of unobserved confounders. While many traditional causal inference methods rely on the assumption of no unobserved confounding, this assumption is likely often violated. PCI mitigates this challenge by relying on an alternative set of assumptions regarding the relationships between treatment, outcome, and auxiliary variables that serve as proxies for unmeasured confounders. We review existing identification results, discuss the assumptions necessary for valid causal effect estimation via PCI, and compare different PCI estimation methods. We offer practical guidance on operationalizing PCI, with a focus on selecting and evaluating proxy variables using domain knowledge, measurement error perspectives, and negative control analogies. Through conceptual examples, we demonstrate tensions in proxy selection and discuss the importance of clearly defining the unobserved confounding mechanism. By bridging formal results with applied considerations, this work aims to demystify PCI, encourage thoughtful use in practice, and identify open directions for methodological development and empirical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24413v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Grace V. Ringlein, Trang Quynh Nguyen, Peter P. Zandi, Elizabeth A. Stuart, Harsh Parikh</dc:creator>
    </item>
    <item>
      <title>Exact finite mixture representations for species sampling processes</title>
      <link>https://arxiv.org/abs/2512.24414</link>
      <description>arXiv:2512.24414v1 Announce Type: new 
Abstract: Random probability measures, together with their constructions, representations, and associated algorithms, play a central role in modern Bayesian inference. A key class is that of proper species sampling processes, which offer a relatively simple yet versatile framework that extends naturally to non-exchangeable settings. We revisit this class from a computational perspective and show that they admit exact finite mixture representations. In particular, we prove that any proper species sampling process can be written, at the prior level, as a finite mixture with a latent truncation variable and reweighted atoms, while preserving its distributional features exactly. These finite formulations can be used as drop-in replacements in Bayesian mixture models, recasting posterior computation in terms of familiar finite-mixture machinery. This yields straightforward MCMC implementations and tractable expressions, while avoiding ad hoc truncations and model-specific constructions. The resulting representation preserves the full generality of the original infinite-dimensional priors while enabling practical gains in algorithm design and implementation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24414v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rams\'es H. Mena, Christos Merkatas, Theodoros Nicoleris, Carlos E. Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Model-Assisted Bayesian Estimators of Transparent Population Level Summary Measures for Ordinal Outcomes in Randomized Controlled Trials</title>
      <link>https://arxiv.org/abs/2512.24442</link>
      <description>arXiv:2512.24442v1 Announce Type: new 
Abstract: In randomized controlled trials, ordinal outcomes typically improve statistical efficiency over binary outcomes. The treatment effect on an ordinal outcome is usually described by the odds ratio from a proportional odds model, but this summary measure lacks transparency with respect to its emphasis on the components of the ordinal outcome when proportional odds is violated. We propose various summary measures for ordinal outcomes that are fully transparent in this regard, including 'weighted geometric mean' odds ratios and relative risks, and 'weighted mean' risk differences. We also develop and evaluate efficient model-assisted Bayesian estimators for these population level summary measures based on non-proportional odds models that facilitate covariate adjustment with marginalization via the Bayesian bootstrap. We propose a weighting scheme that engenders appealing invariance properties, including to whether the ordinal outcome is ordered from best to worst versus worst to best. Using computer simulation, we show that comparative testing based on the proposed population level summary measures performs well relative to the conventional proportional odds approach. We also report an analysis of the COVID-OUT trial, which exhibits evidence of non-proportional odds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24442v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lindsey E. Turner, Carolyn T. Bramante, Thomas A. Murray</dc:creator>
    </item>
    <item>
      <title>Robust reduced rank regression under heavy-tailed noise and missing data via non-convex penalization</title>
      <link>https://arxiv.org/abs/2512.24450</link>
      <description>arXiv:2512.24450v1 Announce Type: new 
Abstract: Reduced rank regression (RRR) is a fundamental tool for modeling multiple responses through low-dimensional latent structures, offering both interpretability and strong predictive performance in high-dimensional settings. Classical RRR methods, however, typically rely on squared loss and Gaussian noise assumptions, rendering them sensitive to heavy-tailed errors, outliers, and data contamination. Moreover, the presence of missing data--common in modern applications--further complicates reliable low-rank estimation. In this paper, we propose a robust reduced rank regression framework that simultaneously addresses heavy-tailed noise, outliers, and missing data. Our approach combines a robust Huber loss with nonconvex spectral regularization, specifically the minimax concave penalty (MCP) and smoothly clipped absolute deviation (SCAD). Unlike convex nuclear-norm regularization, the proposed nonconvex penalties alleviate excessive shrinkage and enable more accurate recovery of the underlying low-rank structure. The method also accommodates missing data in the response matrix without requiring imputation. We develop an efficient proximal gradient algorithm based on alternating updates and tailored spectral thresholding. Extensive simulation studies demonstrate that the proposed methods substantially outperform nuclear-norm-based and non-robust alternatives under heavy-tailed noise and contamination. An application to cancer cell line data set further illustrates the practical advantages of the proposed robust RRR framework.
  Our method is implemented in the R package rrpackrobust available at https://github.com/tienmt/rrpackrobust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24450v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>The Tien Mai</dc:creator>
    </item>
    <item>
      <title>Power Analysis is Essential: High-Powered Tests Suggest Minimal to No Effect of Rounded Shapes on Click-Through Rates</title>
      <link>https://arxiv.org/abs/2512.24521</link>
      <description>arXiv:2512.24521v1 Announce Type: new 
Abstract: Underpowered studies (below 50%) suffer from the winner's curse: a statistically significant result must exaggerate the true treatment effect to meet the significance threshold. A study by Dipayan Biswas, Annika Abell, and Roger Chacko published in the Journal of Consumer Research (2023) reported that in an A/B test simply rounding the corners of square buttons increased the online click-through rate by 55% (p-value 0.037)$\unicode{x2014}$a striking finding with potentially wide-ranging implications for the digital industry that is seeking to enhance consumer engagement. Drawing on our experience with tens of thousands of A/B tests, many involving similar user interface modifications, we found this dramatic claim implausibly large. To evaluate the claim, we conducted three high-powered A/B tests, each involving over two thousand times more users than the original study. All three experiments yielded effect size estimates that were approximately two orders of magnitude smaller than initially reported, with 95% confidence intervals that include zero, that is, not statistically significant at the 0.05 level. Two additional independent replications by Evidoo found similarly small effects. These findings underscore the critical importance of power analysis and experimental design to increase trust and reproducibility of results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24521v1</guid>
      <category>stat.ME</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ron Kohavi, Jakub Linowski, Lukas Vermeer, Fabrice Boisseranc, Joachim Furuseth, Andrew Gelman, Guido Imbens, Ravikiran Rajagopal</dc:creator>
    </item>
    <item>
      <title>Multiple Testing of One-Sided Hypotheses with Conservative $p$-values</title>
      <link>https://arxiv.org/abs/2512.24588</link>
      <description>arXiv:2512.24588v1 Announce Type: new 
Abstract: We study a large-scale one-sided multiple testing problem in which test statistics follow normal distributions with unit variance, and the goal is to identify signals with positive mean effects. A common approach is to compute $p$-values under the assumption that all null means are exactly zero and then apply standard multiple testing procedures such as the Benjamini--Hochberg (BH) or Storey--BH method. However, because the null hypothesis is composite, some null means may be strictly negative. In this case, the resulting $p$-values are conservative, leading to a substantial loss of power. Existing methods address this issue by modifying the multiple testing procedure itself, for example through conditioning strategies or discarding rules. In contrast, we focus on correcting the $p$-values so that they are exact under the null. Specifically, we estimate the marginal null distribution of the test statistics within an empirical Bayes framework and construct refined $p$-values based on this estimated distribution. These refined $p$-values can then be directly used in standard multiple testing procedures without modification. Extensive simulation studies show that the proposed method substantially improves power when $p$-values are conservative, while achieving comparable performance to existing methods when $p$-values are exact. An application to phosphorylation data further demonstrates the practical effectiveness of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24588v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangok Seo, Johan Lim, Hyungwon Choi, Jaesik Jeong</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes Method for Large Scale Multiple Testing with Heteroscedastic Errors</title>
      <link>https://arxiv.org/abs/2512.24611</link>
      <description>arXiv:2512.24611v1 Announce Type: new 
Abstract: In this paper, we address the normal mean inference problem, which involves testing multiple means of normal random variables with heteroscedastic variances. Most existing empirical Bayes methods for this setting are developed under restrictive assumptions, such as the scaled inverse-chi-squared prior for variances and unimodality for the non-null mean distribution. However, when either of these assumptions is violated, these methods often fail to control the false discovery rate (FDR) at the target level or suffer from a substantial loss of power. To overcome these limitations, we propose a new empirical Bayes method, gg-Mix, which assumes only independence between the normal means and variances, without imposing any structural restrictions on their distributions. We thoroughly evaluate the FDR control and power of gg-Mix through extensive numerical studies and demonstrate its superior performance compared to existing methods. Finally, we apply gg-Mix to three real data examples to further illustrate the practical advantages of our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24611v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kwangok Seo, Johan Lim, Kaiwen Wang, Dohwan Park, Shota Katayama, Xinlei Wang</dc:creator>
    </item>
    <item>
      <title>Quasi-Maximum Likelihood Estimation for a Genuinely Unbalanced Dynamic Network Panel Data Model</title>
      <link>https://arxiv.org/abs/2512.24748</link>
      <description>arXiv:2512.24748v1 Announce Type: new 
Abstract: This paper develops a quasi-maximum likelihood estimator for genuinely unbalanced dynamic network panel data models with individual fixed effects. We propose a model that accommodates contemporaneous and lagged network spillovers, temporal dependence, and a listing effect that activates upon a unit's first appearance in the panel. We establish the consistency of the QMLE as both $N$ and $T$ go to infinity, derive its asymptotic distribution, and identify an asymptotic bias arising from incidental parameters when $N$ is asymptotically large relative to $T$. Based on the asymptotic bias expression, we propose a bias-corrected estimator that is asymptotically unbiased and normally distributed under appropriate regularity conditions. Monte Carlo experiments examine the finite sample performance of the bias-corrected estimator across different criteria, including bias, RMSE, coverage probability, and the normality of the estimator. The empirical application to Airbnb listings from New Zealand and New York City reveals region-specific patterns in spatial and temporal price transmission, illustrating the importance of modeling genuine unbalancedness in dynamic network settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24748v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhijian Wang, Xingbai Xu, Tuo Liu</dc:creator>
    </item>
    <item>
      <title>Modewise Additive Factor Model for Matrix Time Series</title>
      <link>https://arxiv.org/abs/2512.25025</link>
      <description>arXiv:2512.25025v1 Announce Type: new 
Abstract: We introduce a Modewise Additive Factor Model (MAFM) for matrix-valued time series that captures row-specific and column-specific latent effects through an additive structure, offering greater flexibility than multiplicative frameworks such as Tucker and CP factor models. In MAFM, each observation decomposes into a row-factor component, a column-factor component, and noise, allowing distinct sources of variation along different modes to be modeled separately. We develop a computationally efficient two-stage estimation procedure: Modewise Inner-product Eigendecomposition (MINE) for initialization, followed by Complement-Projected Alternating Subspace Estimation (COMPAS) for iterative refinement. The key methodological innovation is that orthogonal complement projections completely eliminate cross-modal interference when estimating each loading space. We establish convergence rates for the estimated factor loading matrices under proper conditions. We further derive asymptotic distributions for the loading matrix estimators and develop consistent covariance estimators, yielding a data-driven inference framework that enables confidence interval construction and hypothesis testing. As a technical contribution of independent interest, we establish matrix Bernstein inequalities for quadratic forms of dependent matrix time series. Numerical experiments on synthetic and real data demonstrate the advantages of the proposed method over existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25025v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Elynn Chen, Yuefeng Han, Jiayu Li, Ke Xu</dc:creator>
    </item>
    <item>
      <title>Bayesian Elastic Net Regression with Structured Prior Dependence</title>
      <link>https://arxiv.org/abs/2512.25045</link>
      <description>arXiv:2512.25045v1 Announce Type: new 
Abstract: Many regularization priors for Bayesian regression assume the regression coefficients are a priori independent. In particular this is the case for standard Bayesian treatments of the lasso and the elastic net. While independence may be reasonable in some data-analytic settings, incorporating dependence in these prior distributions provides greater modeling flexibility. This paper introduces the orthant normal distribution in its general form and shows how it can be used to structure prior dependence in the Bayesian elastic net regression model. An L1-regularized version of Zellner's g prior is introduced as a special case, creating a new link between the literature on penalized optimization and an important class of regression priors. Computation is challenging due to an intractable normalizing constant in the prior. We avoid this issue by modifying slightly a standard prior of convenience for the hyperparameters in such a way to enable simple and fast Gibbs sampling of the posterior distribution. The benefit of including structured prior dependence in the Bayesian elastic net regression model is demonstrated through simulation and a near-infrared spectroscopy data example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25045v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christopher M. Hans, Ningyi Liu</dc:creator>
    </item>
    <item>
      <title>Sequential Bayesian parameter-state estimation in dynamical systems with noisy and incomplete observations via a variational framework</title>
      <link>https://arxiv.org/abs/2512.25056</link>
      <description>arXiv:2512.25056v1 Announce Type: new 
Abstract: Online joint estimation of unknown parameters and states in a dynamical system with uncertainty quantification is crucial in many applications. For example, digital twins dynamically update their knowledge of model parameters and states to support prediction and decision-making. Reliability and computational speed are vital for DTs. Online parameter-state estimation ensures computational efficiency, while uncertainty quantification is essential for making reliable predictions and decisions. In parameter-state estimation, the joint distribution of the state and model parameters conditioned on the data, termed the joint posterior, provides accurate uncertainty quantification. Because the joint posterior is generally intractable to compute, this paper presents an online variational inference framework to compute its approximation at each time step. The approximation is factorized into a marginal distribution over the model parameters and a state distribution conditioned on the parameters. This factorization enables recursive updates through a two-stage procedure: first, the parameter posterior is approximated via variational inference; second, the state distribution conditioned on the parameters is computed using Gaussian filtering based on the estimated parameter posterior. The algorithmic design is supported by a theorem establishing upper bounds on the joint posterior approximation error. Numerical experiments demonstrate that the proposed method (i) matches the performance of the joint particle filter in low-dimensional problems, accurately inferring both unobserved states and unknown parameters of dynamical and observation models; (ii) remains robust under noisy, partial observations and model discrepancies in a chaotic Lorenz 96 system; and (iii) scales effectively to a high-dimensional convection-diffusion system, where it outperforms the joint ensemble Kalman filter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25056v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Liliang Wang, Alex Gorodetsky</dc:creator>
    </item>
    <item>
      <title>New Exam Security Questions in the AI Era: Comparing AI-Generated Item Similarity Between Naive and Detail-Guided Prompting Approaches</title>
      <link>https://arxiv.org/abs/2512.23729</link>
      <description>arXiv:2512.23729v1 Announce Type: cross 
Abstract: Large language models (LLMs) have emerged as powerful tools for generating domain-specific multiple-choice questions (MCQs), offering efficiency gains for certification boards but raising new concerns about examination security. This study investigated whether LLM-generated items created with proprietary guidance differ meaningfully from those generated using only publicly available resources. Four representative clinical activities from the American Board of Family Medicine (ABFM) blueprint were mapped to corresponding Entrustable Professional Activities (EPAs), and three LLMs (GPT-4o, Claude 4 Sonnet, Gemini 2.5 Flash) produced items under a naive strategy using only public EPA descriptors, while GPT-4o additionally produced items under a guided strategy that incorporated proprietary blueprints, item-writing guidelines, and exemplar items, yielding 160 total items. Question stems and options were encoded using PubMedBERT and BioBERT, and intra- and inter-strategy cosine similarity coefficients were calculated. Results showed high internal consistency within each prompting strategy, while cross-strategy similarity was lower overall. However, several domain model pairs, particularly in narrowly defined areas such as viral pneumonia and hypertension, exceeded the 0.65 threshold, indicating convergence between naive and guided pipelines. These findings suggest that while proprietary resources impart distinctiveness, LLMs prompted only with public information can still generate items closely resembling guided outputs in constrained clinical domains, thereby heightening risks of item exposure. Safeguarding the integrity of high stakes examinations will require human-first, AI-assisted item development, strict separation of formative and summative item pools, and systematic similarity surveillance to balance innovation with security.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23729v1</guid>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ting Wang, Caroline Prendergast, Susan Lottridge</dc:creator>
    </item>
    <item>
      <title>TabMixNN: A Unified Deep Learning Framework for Structural Mixed Effects Modeling on Tabular Data</title>
      <link>https://arxiv.org/abs/2512.23787</link>
      <description>arXiv:2512.23787v1 Announce Type: cross 
Abstract: We present TabMixNN, a flexible PyTorch-based deep learning framework that synthesizes classical mixed-effects modeling with modern neural network architectures for tabular data analysis. TabMixNN addresses the growing need for methods that can handle hierarchical data structures while supporting diverse outcome types including regression, classification, and multitask learning. The framework implements a modular three-stage architecture: (1) a mixed-effects encoder with variational random effects and flexible covariance structures, (2) backbone architectures including Generalized Structural Equation Models (GSEM) and spatial-temporal manifold networks, and (3) outcome-specific prediction heads supporting multiple outcome families. Key innovations include an R-style formula interface for accessibility, support for directed acyclic graph (DAG) constraints for causal structure learning, Stochastic Partial Differential Equation (SPDE) kernels for spatial modeling, and comprehensive interpretability tools including SHAP values and variance decomposition. We demonstrate the framework's flexibility through applications to longitudinal data analysis, genomic prediction, and spatial-temporal modeling. TabMixNN provides a unified interface for researchers to leverage deep learning while maintaining the interpretability and theoretical grounding of classical mixed-effects models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23787v1</guid>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Deniz Akdemir</dc:creator>
    </item>
    <item>
      <title>Statistical Guarantees in the Search for Less Discriminatory Algorithms</title>
      <link>https://arxiv.org/abs/2512.23943</link>
      <description>arXiv:2512.23943v1 Announce Type: cross 
Abstract: Recent scholarship has argued that firms building data-driven decision systems in high-stakes domains like employment, credit, and housing should search for "less discriminatory algorithms" (LDAs) (Black et al., 2024). That is, for a given decision problem, firms considering deploying a model should make a good-faith effort to find equally performant models with lower disparate impact across social groups. Evidence from the literature on model multiplicity shows that randomness in training pipelines can lead to multiple models with the same performance, but meaningful variations in disparate impact. This suggests that developers can find LDAs simply by randomly retraining models. Firms cannot continue retraining forever, though, which raises the question: What constitutes a good-faith effort? In this paper, we formalize LDA search via model multiplicity as an optimal stopping problem, where a model developer with limited information wants to produce strong evidence that they have sufficiently explored the space of models. Our primary contribution is an adaptive stopping algorithm that yields a high-probability upper bound on the gains achievable from a continued search, allowing the developer to certify (e.g., to a court) that their search was sufficient. We provide a framework under which developers can impose stronger assumptions about the distribution of models, yielding correspondingly stronger bounds. We validate the method on real-world credit, employment and housing datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.23943v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chris Hays, Ben Laufer, Solon Barocas, Manish Raghavan</dc:creator>
    </item>
    <item>
      <title>Colorful Pinball: Density-Weighted Quantile Regression for Conditional Guarantee of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2512.24139</link>
      <description>arXiv:2512.24139v1 Announce Type: cross 
Abstract: While conformal prediction provides robust marginal coverage guarantees, achieving reliable conditional coverage for specific inputs remains challenging. Although exact distribution-free conditional coverage is impossible with finite samples, recent work has focused on improving the conditional coverage of standard conformal procedures. Distinct from approaches that target relaxed notions of conditional coverage, we directly minimize the mean squared error of conditional coverage by refining the quantile regression components that underpin many conformal methods. Leveraging a Taylor expansion, we derive a sharp surrogate objective for quantile regression: a density-weighted pinball loss, where the weights are given by the conditional density of the conformity score evaluated at the true quantile. We propose a three-headed quantile network that estimates these weights via finite differences using auxiliary quantile levels at \(1-\alpha \pm \delta\), subsequently fine-tuning the central quantile by optimizing the weighted loss. We provide a theoretical analysis with exact non-asymptotic guarantees characterizing the resulting excess risk. Extensive experiments on diverse high-dimensional real-world datasets demonstrate remarkable improvements in conditional coverage performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24139v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>HOLOGRAPH: Active Causal Discovery via Sheaf-Theoretic Alignment of Large Language Model Priors</title>
      <link>https://arxiv.org/abs/2512.24478</link>
      <description>arXiv:2512.24478v1 Announce Type: cross 
Abstract: Causal discovery from observational data remains fundamentally limited by identifiability constraints. Recent work has explored leveraging Large Language Models (LLMs) as sources of prior causal knowledge, but existing approaches rely on heuristic integration that lacks theoretical grounding. We introduce HOLOGRAPH, a framework that formalizes LLM-guided causal discovery through sheaf theory--representing local causal beliefs as sections of a presheaf over variable subsets. Our key insight is that coherent global causal structure corresponds to the existence of a global section, while topological obstructions manifest as non-vanishing sheaf cohomology. We propose the Algebraic Latent Projection to handle hidden confounders and Natural Gradient Descent on the belief manifold for principled optimization. Experiments on synthetic and real-world benchmarks demonstrate that HOLOGRAPH provides rigorous mathematical foundations while achieving competitive performance on causal discovery tasks with 50-100 variables. Our sheaf-theoretic analysis reveals that while Identity, Transitivity, and Gluing axioms are satisfied to numerical precision (&lt;10^{-6}), the Locality axiom fails for larger graphs, suggesting fundamental non-local coupling in latent variable projections. Code is available at [https://github.com/hyunjun1121/holograph](https://github.com/hyunjun1121/holograph).</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24478v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyunjun Kim</dc:creator>
    </item>
    <item>
      <title>Improving the stability of the covariance-controlled adaptive Langevin thermostat for large-scale Bayesian sampling</title>
      <link>https://arxiv.org/abs/2512.24515</link>
      <description>arXiv:2512.24515v1 Announce Type: cross 
Abstract: Stochastic gradient Langevin dynamics and its variants approximate the likelihood of an entire dataset, via random (and typically much smaller) subsets, in the setting of Bayesian sampling. Due to the (often substantial) improvement of the computational efficiency, they have been widely used in large-scale machine learning applications. It has been demonstrated that the so-called covariance-controlled adaptive Langevin (CCAdL) thermostat, which incorporates an additional term involving the covariance matrix of the noisy force, outperforms popular alternative methods. A moving average is used in CCAdL to estimate the covariance matrix of the noisy force, in which case the covariance matrix will converge to a constant matrix in long-time limit. Moreover, it appears in our numerical experiments that the use of a moving average could reduce the stability of the numerical integrators, thereby limiting the largest usable stepsize. In this article, we propose a modified CCAdL (i.e., mCCAdL) thermostat that uses the scaling part of the scaling and squaring method together with a truncated Taylor series approximation to the exponential to numerically approximate the exact solution to the subsystem involving the additional term proposed in CCAdL. We also propose a symmetric splitting method for mCCAdL, instead of an Euler-type discretisation used in the original CCAdL thermostat. We demonstrate in our numerical experiments that the newly proposed mCCAdL thermostat achieves a substantial improvement in the numerical stability over the original CCAdL thermostat, while significantly outperforming popular alternative stochastic gradient methods in terms of the numerical accuracy for large-scale machine learning applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24515v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiani Wei, Xiaocheng Shang</dc:creator>
    </item>
    <item>
      <title>Generalized Poisson Matrix Factorization for Overdispersed Count Data</title>
      <link>https://arxiv.org/abs/2512.24604</link>
      <description>arXiv:2512.24604v1 Announce Type: cross 
Abstract: Non-negative matrix factorization (NMF) is widely used as a feature extraction technique for matrices with non-negative entries, such as image data, purchase histories, and other types of count data. In NMF, a non-negative matrix is decomposed into the product of two non-negative matrices, and the approximation accuracy is evaluated by a loss function. If the Kullback-Leibler divergence is chosen as the loss function, the estimation coincides with maximum likelihood under the assumption that the data entries are distributed according to a Poisson distribution. To address overdispersion, negative binomial matrix factorization has recently been proposed as an extension of the Poisson-based model. However, the negative binomial distribution often generates an excessive number of zeros, which limits its expressive capacity. In this study, we propose a non-negative matrix factorization based on the generalized Poisson distribution, which can flexibly accommodate overdispersion, and we introduce a maximum likelihood approach for parameter estimation. This methodology provides a more versatile framework than existing models, thereby extending the applicability of NMF to a broader class of count data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.24604v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ryo Ohashi, Hiroyasu Abe, Fumitake Sakaori</dc:creator>
    </item>
    <item>
      <title>Testing Monotonicity in a Finite Population</title>
      <link>https://arxiv.org/abs/2512.25032</link>
      <description>arXiv:2512.25032v1 Announce Type: cross 
Abstract: We consider the extent to which we can learn from a completely randomized experiment whether everyone has treatment effects that are weakly of the same sign, a condition we call monotonicity. From a classical sampling perspective, it is well-known that monotonicity is untestable. By contrast, we show from the design-based perspective -- in which the units in the population are fixed and only treatment assignment is stochastic -- that the distribution of treatment effects in the finite population (and hence whether monotonicity holds) is formally identified. We argue, however, that the usual definition of identification is unnatural in the design-based setting because it imagines knowing the distribution of outcomes over different treatment assignments for the same units. We thus evaluate the informativeness of the data by the extent to which it enables frequentist testing and Bayesian updating. We show that frequentist tests can have nontrivial power against some alternatives, but power is generically limited. Likewise, we show that there exist (non-degenerate) Bayesian priors that never update about whether monotonicity holds. We conclude that, despite the formal identification result, the ability to learn about monotonicity from data in practice is severely limited.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25032v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiafeng Chen, Jonathan Roth, Jann Spiess</dc:creator>
    </item>
    <item>
      <title>Compound Estimation for Binomials</title>
      <link>https://arxiv.org/abs/2512.25042</link>
      <description>arXiv:2512.25042v1 Announce Type: cross 
Abstract: Many applications involve estimating the mean of multiple binomial outcomes as a common problem -- assessing intergenerational mobility of census tracts, estimating prevalence of infectious diseases across countries, and measuring click-through rates for different demographic groups. The most standard approach is to report the plain average of each outcome. Despite simplicity, the estimates are noisy when the sample sizes or mean parameters are small. In contrast, the Empirical Bayes (EB) methods are able to boost the average accuracy by borrowing information across tasks. Nevertheless, the EB methods require a Bayesian model where the parameters are sampled from a prior distribution which, unlike the commonly-studied Gaussian case, is unidentified due to discreteness of binomial measurements. Even if the prior distribution is known, the computation is difficult when the sample sizes are heterogeneous as there is no simple joint conjugate prior for the sample size and mean parameter.
  In this paper, we consider the compound decision framework which treats the sample size and mean parameters as fixed quantities. We develop an approximate Stein's Unbiased Risk Estimator (SURE) for the average mean squared error given any class of estimators. For a class of machine learning-assisted linear shrinkage estimators, we establish asymptotic optimality, regret bounds, and valid inference. Unlike existing work, we work with the binomials directly without resorting to Gaussian approximations. This allows us to work with small sample sizes and/or mean parameters in both one-sample and two-sample settings. We demonstrate our approach using three datasets on firm discrimination, education outcomes, and innovation rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.25042v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yan Chen, Lihua Lei</dc:creator>
    </item>
    <item>
      <title>Hypothesis testing for partial tail correlation in multivariate extremes</title>
      <link>https://arxiv.org/abs/2210.02048</link>
      <description>arXiv:2210.02048v3 Announce Type: replace 
Abstract: Statistical modeling of high dimensional extremes remains challenging and has generally been limited to moderate dimensions. Understanding structural relationships among variables at their extreme levels is crucial both for constructing simplified models and for identifying sparsity in extremal dependence. In this paper, we introduce the notion of partial tail correlation to characterize structural relationships between pairs of variables in their tails. To this end, we propose a tail regression approach for nonnegative regularly varying random vectors and define partial tail correlation based on the regression residuals. Using an extreme analogue of the covariance matrix, we show that the resulting regression coefficients and partial tail correlations take the same form as in classical non-extreme settings. For inference, we develop a hypothesis test to explore sparsity in extremal dependence structures, and demonstrate its effectiveness through simulations and an application to the Danube river network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.02048v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mihyun Kim, Jeongjin Lee</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimates of Parameters in Generalized Gamma Distribution with SeLF Algorithm</title>
      <link>https://arxiv.org/abs/2306.16419</link>
      <description>arXiv:2306.16419v2 Announce Type: replace 
Abstract: This undergraduate thesis focuses on calculating maximum likelihood estimates of parameters in the generalized Gamma distribution using the SeLF algorithm. As an extension of the Gamma distribution, the generalized Gamma distribution can better fit real data and has been widely applied. The research begins by exploring the definition of the generalized Gamma distribution and its similarities and differences from the traditional Gamma distribution. Then, the SeLF and US algorithms are discussed in detail. The SeLF algorithm is a new algorithm based on the Minorization-Maximization algorithm, which can obtain the local optimal solution with few iterations, with the advantages of fast computation, high accuracy, and good convergence. The US algorithm is a method for finding the zeros of a function, which stands at a higher level than the SeLF algorithm and can improve the convergence speed and stability. This thesis proposes a method for calculating maximum likelihood estimates of the parameters in the generalized Gamma distribution using the SeLF and US algorithms, and presents the practical implementation of the algorithms, as well as simulations and data analysis to evaluate the performance of the proposed methods. The results demonstrate that the SeLF algorithm can achieve more stable and accurate estimates of the parameters in the generalized Gamma distribution more quickly, compared to traditional Newton's method, which can be useful in various applications. This thesis provides a comprehensive and in-depth exploration of the generalized Gamma distribution and the SeLF algorithm, and proposes a new method for calculating maximum likelihood estimates of parameters, contributing to the development of statistical methods for parameter estimation in complex models. The proposed method in this thesis has important practical significance and application value for solving practical problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.16419v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufei Cai</dc:creator>
    </item>
    <item>
      <title>Studentising Kendall's Tau: U-Statistic Estimators and Bias Correction for a Generalised Rank Variance-Covariance framework</title>
      <link>https://arxiv.org/abs/2307.10973</link>
      <description>arXiv:2307.10973v2 Announce Type: replace 
Abstract: Kemeny (1959) introduced a topologically complete metric space to study ordinal random variables, particularly in the context of Condorcet's paradox and the measurability of ties. Building on this, Emond &amp; Mason (2002) reformulated Kemeny's framework into a rank correlation coefficient by embedding the metric space into a Hilbert structure. This transformation enables the analysis of data under weak order-preserving transformations (monotonically non-decreasing) within a linear probabilistic framework. However, the statistical properties of this rank correlation estimator, such as bias, estimation variance, and Type I error rates, have not been thoroughly evaluated.
  In this paper, we derive and prove a complete U-statistic estimator in the presence of ties for Kemeny's \(\tau_{\kappa}\), addressing the positive bias introduced by tied ranks. We also introduce a consistent population standard error estimator. The null distribution of the test statistic is shown to follow a \(t_{(N-2)}\)-distribution. Simulation results demonstrate that the proposed method outperforms Kendall's \(\tau_{b}\), offering a more accurate and robust measure of ordinal association which is topologically complete upon standard linear models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10973v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Landon Hurley</dc:creator>
    </item>
    <item>
      <title>Extremile scalar-on-function regression</title>
      <link>https://arxiv.org/abs/2405.20817</link>
      <description>arXiv:2405.20817v2 Announce Type: replace 
Abstract: Extremiles provide a generalization of quantiles which are not only robust, but also have an intrinsic link with extreme value theory. This paper introduces an extremile regression model tailored for functional covariate spaces. The estimation procedure turns out to be a weighted version of local linear scalar-on-function regression, where now a double kernel approach plays a crucial role. Asymptotic expressions for the bias and variance are established, applicable to both decreasing bandwidth sequences and automatically selected bandwidths. The methodology is then investigated in detail through a simulation study. Furthermore, we illustrate the method's applicability with an analysis of the Berkeley Growth data, showcasing its performance in a real-world functional data setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.20817v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Laura Battagliola, Martin Bladt</dc:creator>
    </item>
    <item>
      <title>Inference at the data's edge: Gaussian processes for modeling and inference under model-dependency, poor overlap, and extrapolation</title>
      <link>https://arxiv.org/abs/2407.10442</link>
      <description>arXiv:2407.10442v2 Announce Type: replace 
Abstract: Many inferential tasks involve fitting models to observed data and predicting outcomes at new covariate values, requiring interpolation or extrapolation. Conventional methods select a single best-fitting model, discarding fits that were similarly plausible in-sample but would yield sharply different predictions out-of-sample. Gaussian Processes (GPs) offer a principled alternative. Rather than committing to one conditional expectation function, GPs deliver a posterior distribution over outcomes at any covariate value. This posterior effectively retains the range of models consistent with the data, widening uncertainty intervals where extrapolation magnifies divergence. In this way, the GP's uncertainty estimates reflect the implications of extrapolation on our predictions, helping to tame the "dangers of extreme counterfactuals" (King &amp; Zeng, 2006). The approach requires (i) specifying a covariance function linking outcome similarity to covariate similarity, and (ii) assuming Gaussian noise around the conditional expectation. We provide an accessible introduction to GPs with emphasis on this property, along with a simple, automated procedure for hyperparameter selection implemented in the R package gpss. We illustrate the value of GPs for capturing counterfactual uncertainty in three settings: (i) treatment effect estimation with poor overlap, (ii) interrupted time series requiring extrapolation beyond pre-intervention data, and (iii) regression discontinuity designs where estimates hinge on boundary behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.10442v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soonhong Cho, Doeun Kim, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>A Practical Guide to Estimating Conditional Marginal Effects: Modern Approaches</title>
      <link>https://arxiv.org/abs/2504.01355</link>
      <description>arXiv:2504.01355v2 Announce Type: replace 
Abstract: This Element offers a practical guide to estimating conditional marginal effects-how treatment effects vary with a moderating variable-using modern statistical methods. Commonly used approaches, such as linear interaction models, often suffer from unclarified estimands, limited overlap, and restrictive functional forms. This guide begins by clearly defining the estimand and presenting the main identification results. It then reviews and improves upon existing solutions, such as the semiparametric kernel estimator, and introduces robust estimation strategies, including augmented inverse propensity score weighting with Lasso selection (AIPW-Lasso) and double machine learning (DML) with modern algorithms. Each method is evaluated through simulations and empirical examples, with practical recommendations tailored to sample size and research context. All tools are implemented in the accompanying \texttt{interflex} package for \texttt{R}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.01355v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiehan Liu, Ziyi Liu, Yiqing Xu</dc:creator>
    </item>
    <item>
      <title>Measuring the Impact of Missingness in Traffic Stop Data</title>
      <link>https://arxiv.org/abs/2505.18281</link>
      <description>arXiv:2505.18281v2 Announce Type: replace 
Abstract: In this article we explore the data available through the Stanford Open Policing Project. The data consist of information on millions of traffic stops across close to 100 different cities and highway patrols. Using a variety of metrics, we identify that the data is not missing completely at random. Furthermore, we develop ways of quantifying and visualizing missingness trends for different variables across the datasets. We follow up by performing a sensitivity analysis to extend work done on the outcome test as well as to extend work done on sharp bounds on the average treatment effect. We demonstrate that bias calculations can fundamentally shift depending on the assumptions made about the observations for which the race variable has not been recorded. We suggest ways that our missingness sensitivity analysis can be extended to myriad different contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.18281v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Saatvik Kher, Johanna Hardin</dc:creator>
    </item>
    <item>
      <title>Bayesian spatio-temporal weighted regression for integrating missing and misaligned environmental data</title>
      <link>https://arxiv.org/abs/2511.02149</link>
      <description>arXiv:2511.02149v2 Announce Type: replace 
Abstract: Estimating environmental exposures from multi-source data is central to public health research and policy. Integrating data from satellite products and ground monitors are increasingly used to produce exposure surfaces. However, spatio-temporal misalignment often induced from missing data introduces substantial uncertainty and reduces predictive accuracy. We propose a Bayesian weighted predictor regression framework that models spatio-temporal relationships when predictors are observed on irregular supports or have substantial missing data, and are not concurrent with the outcome. The key feature of our model is a spatio-temporal kernel that aggregates the predictor over local space-time neighborhoods, built directly into the likelihood, eliminating any separate gap-filling or forced data alignment stage. We introduce a numerical approximation using a Voronoi-based spatial quadrature combined with irregular temporal increments for estimation under data missingness and misalignment. We showed that misspecification of the spatial and temporal lags induced bias in the mean and parameter estimates, indicating the need for principled parameter selection. Simulation studies confirmed these findings, where careful tuning was critical to control bias and achieve accurate prediction, while the proposed quadrature performed well under severe missingness. As an illustrative application, we estimated fine particulate matter (PM$_{2.5}$) in northern California using satellite-derived aerosol optical depth (AOD) and wildfire smoke plume indicators. Relative to a traditional collocated linear model, our approach improved out-of-sample predictive performance, reduced uncertainty, and yielded robust temporal predictions and spatial surface estimation. Our framework is extensible to additional spatio-temporally varying covariates and other kernel families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02149v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yovna Junglee, Vianey Leos Barajas, Meredith Franklin</dc:creator>
    </item>
    <item>
      <title>Estimation and Inference for Causal Explainability</title>
      <link>https://arxiv.org/abs/2512.20219</link>
      <description>arXiv:2512.20219v4 Announce Type: replace 
Abstract: Understanding how much each variable contributes to an outcome is a central question across disciplines. A causal view of explainability is favorable for its ability in uncovering underlying mechanisms and generalizing to new contexts. Based on a family of causal explainability quantities, we develop methods for their estimation and inference. In particular, we construct a one-step correction estimator using semi-parametric efficiency theory, which explicitly leverages the independence structure of variables to reduce the asymptotic variance. For a null hypothesis on the boundary, i.e., zero explainability, we show its equivalence to Fisher's sharp null, which motivates a randomization-based inference procedure. Finally, we illustrate the empirical efficacy of our approach through simulations as well as an immigration experiment dataset, where we investigate how features and their interactions shape public opinion toward admitting immigrants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.20219v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Weihan Zhang, Zijun Gao</dc:creator>
    </item>
    <item>
      <title>Modeling Spatio-Temporal Transport: From Rigid Advection to Realistic Dynamics</title>
      <link>https://arxiv.org/abs/2303.02756</link>
      <description>arXiv:2303.02756v4 Announce Type: replace-cross 
Abstract: Stochastic models for spatio-temporal transport face a critical trade-off between physical realism and interpretability. The advection model with a single constant velocity is interpretable but physically limited by its perfect correlation over time. This work aims to bridge the gap between this simple framework and its physically realistic extensions. Our guiding principle is to introduce a spatial correlation structure that vanishes over time. To achieve this, we present two distinct approaches. The first constructs complex velocity structures, either through superpositions of advection components or by allowing the velocity to vary locally. The second is a spectral technique that replaces the singular spectrum of rigid advection with a more flexible form, introducing temporal decorrelation controlled by parameters. We accompany these models with efficient simulation algorithms and demonstrate their success in replicating complex dynamics, such as tropical cyclones and the solutions of partial differential equations. Finally, we illustrate the practical utility of the proposed framework by comparing its simulations to real-world precipitation data from Hurricane Florence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.02756v4</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Maria Laura Battagliola, Sofia Charlotta Olhede</dc:creator>
    </item>
    <item>
      <title>Are Ensembles Getting Better all the Time?</title>
      <link>https://arxiv.org/abs/2311.17885</link>
      <description>arXiv:2311.17885v3 Announce Type: replace-cross 
Abstract: Ensemble methods combine the predictions of several base models. We study whether or not including more models always improves their average performance. This question depends on the kind of ensemble considered, as well as the predictive metric chosen. We focus on situations where all members of the ensemble are a priori expected to perform equally well, which is the case of several popular methods such as random forests or deep ensembles. In this setting, we show that ensembles are getting better all the time if, and only if, the considered loss function is convex. More precisely, in that case, the loss of the ensemble is a decreasing function of the number of models. When the loss function is nonconvex, we show a series of results that can be summarised as: ensembles of good models keep getting better, and ensembles of bad models keep getting worse. To this end, we prove a new result on the monotonicity of tail probabilities that may be of independent interest. We illustrate our results on a medical problem (diagnosing melanomas using neural nets) and a "wisdom of crowds" experiment (guessing the ratings of upcoming movies).</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.17885v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, vol. 26 (201), 1-46, 2025</arxiv:journal_reference>
      <dc:creator>Pierre-Alexandre Mattei, Damien Garreau</dc:creator>
    </item>
    <item>
      <title>Subtype-Aware Registration of Longitudinal Electronic Health Records</title>
      <link>https://arxiv.org/abs/2501.07336</link>
      <description>arXiv:2501.07336v2 Announce Type: replace-cross 
Abstract: Electronic Health Records (EHRs) contain extensive patient information that can inform downstream clinical decisions, such as mortality prediction, disease phenotyping, and disease onset prediction. A key challenge in EHR data analysis is the temporal gap between when a condition is first recorded and its actual onset time. Such timeline misalignment can lead to artificially distinct biomarker trends among patients with similar disease progression, undermining the reliability of downstream analyses and complicating tasks such as disease subtyping and outcome prediction. To address this challenge, we provide a subtype-aware timeline registration method that leverages data projection and discrete optimization to correct timeline misalignment. Through simulation and real-world data analyses, we demonstrate that the proposed method effectively aligns distorted observed records with the true disease progression patterns, enhancing subtyping clarity and improving performance in downstream clinical analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07336v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xin Gai, Shiyi Jiang, Anru R. Zhang</dc:creator>
    </item>
    <item>
      <title>Human- vs. AI-generated tests: dimensionality and information accuracy in latent trait evaluation</title>
      <link>https://arxiv.org/abs/2510.24739</link>
      <description>arXiv:2510.24739v2 Announce Type: replace-cross 
Abstract: Artificial Intelligence (AI) and large language models (LLMs) are increasingly used in social and psychological research. Among potential applications, LLMs can be used to generate, customise, or adapt measurement instruments. This study presents a preliminary investigation of AI-generated questionnaires by comparing two ChatGPT-based adaptations of the Body Awareness Questionnaire (BAQ) with the validated human-developed version. The AI instruments were designed with different levels of explicitness in content and instructions on construct facets, and their psychometric properties were assessed using a Bayesian Graded Response Model. Results show that although surface wording between AI and original items was similar, differences emerged in dimensionality and in the distribution of item and test information across latent traits. These findings illustrate the importance of applying statistical measures of accuracy to ensure the validity and interpretability of AI-driven tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.24739v2</guid>
      <category>cs.HC</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mario Angelelli, Morena Oliva, Serena Arima, Enrico Ciavolino</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Routing Protocol in Vehicular Opportunistic Networks: A Dynamic Cluster-based Routing Using Deep Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2511.19026</link>
      <description>arXiv:2511.19026v3 Announce Type: replace-cross 
Abstract: Opportunistic Networks (OppNets) employ the Store-Carry-Forward (SCF) paradigm to maintain communication during intermittent connectivity. However, routing performance suffers due to dynamic topology changes, unpredictable contact patterns, and resource constraints including limited energy and buffer capacity. These challenges compromise delivery reliability, increase latency, and reduce node longevity in highly dynamic environments. This paper proposes Cluster-based Routing using Deep Reinforcement Learning (CR-DRL), an adaptive routing approach that integrates an Actor-Critic learning framework with a heuristic function. CR-DRL enables real-time optimal relay selection and dynamic cluster overlap adjustment to maintain connectivity while minimizing redundant transmissions and enhancing routing efficiency. Simulation results demonstrate significant improvements over state-of-the-art baselines. CR-DRL extends node lifetimes by up to 21%, overall energy use is reduced by 17%, and nodes remain active for 15% longer. Communication performance also improves, with up to 10% higher delivery ratio, 28.5% lower delay, 7% higher throughput, and data requiring 30% fewer transmission steps across the network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.19026v3</guid>
      <category>cs.NI</category>
      <category>stat.ME</category>
      <pubDate>Thu, 01 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Meisam Sahrifi Sani, Saeid Iranmanesh, Raad Raad, Faisel Tubbal</dc:creator>
    </item>
  </channel>
</rss>
