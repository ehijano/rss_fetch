<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.QM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.QM</link>
    <description>q-bio.QM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.QM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 09 Apr 2024 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 09 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>GENEVIC: GENetic data Exploration and Visualization via Intelligent interactive Console</title>
      <link>https://arxiv.org/abs/2404.04299</link>
      <description>arXiv:2404.04299v1 Announce Type: new 
Abstract: Summary: The vast generation of genetic data poses a significant challenge in efficiently uncovering valuable knowledge. Introducing GENEVIC, an AI-driven chat framework that tackles this challenge by bridging the gap between genetic data generation and biomedical knowledge discovery. Leveraging generative AI, notably ChatGPT, it serves as a biologist's 'copilot'. It automates the analysis, retrieval, and visualization of customized domain-specific genetic information, and integrates functionalities to generate protein interaction networks, enrich gene sets, and search scientific literature from PubMed, Google Scholar, and arXiv, making it a comprehensive tool for biomedical research. In its pilot phase, GENEVIC is assessed using a curated database that ranks genetic variants associated with Alzheimer's disease, schizophrenia, and cognition, based on their effect weights from the Polygenic Score Catalog, thus enabling researchers to prioritize genetic variants in complex diseases. GENEVIC's operation is user-friendly, accessible without any specialized training, secured by Azure OpenAI's HIPAA-compliant infrastructure, and evaluated for its efficacy through real-time query testing. As a prototype, GENEVIC is set to advance genetic research, enabling informed biomedical decisions.
  Availability and implementation: GENEVIC is publicly accessible at https://genevic-anath2024.streamlit.app. The underlying code is open-source and available via GitHub at https://github.com/anath2110/GENEVIC.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04299v1</guid>
      <category>q-bio.QM</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Anindita Nath (Center for Precision Health, McWilliams School of Biomedical Informatics, UT Health Houston, TX), Savannah Mwesigwa (Center for Precision Health, McWilliams School of Biomedical Informatics, UT Health Houston, TX), Yulin Dai (Center for Precision Health, McWilliams School of Biomedical Informatics, UT Health Houston, TX), Xiaoqian Jiang (Department of Health Data Science and Artificial Intelligence, McWilliams School of Biomedical Informatics, UT Health Houston, TX), Zhongming Zhao (Center for Precision Health, McWilliams School of Biomedical Informatics, UT Health Houston, TX, MD Anderson Cancer Center, UTHealth Graduate School of Biomedical Sciences, Houston, TX)</dc:creator>
    </item>
    <item>
      <title>System and Method to Determine ME/CFS and Long COVID Disease Severity Using a Wearable Sensor</title>
      <link>https://arxiv.org/abs/2404.04345</link>
      <description>arXiv:2404.04345v1 Announce Type: new 
Abstract: Objective: We present a simple parameter, calculated from a single wearable sensor, that can be used to objectively measure disease severity in people with myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS) or Long COVID. We call this parameter UpTime. Methods: Prior research has shown that the amount of time a person spends upright, defined as lower legs vertical with feet on the floor, correlates strongly with ME/CFS disease severity. We use a single commercial inertial measurement unit (IMU) attached to the ankle to calculate the percentage of time each day that a person spends upright (i.e., UpTime) and number of Steps/Day. As Long COVID shares symptoms with ME/CFS, we also apply this method to determine Long COVID disease severity. We performed a trial with 55 subjects broken into three cohorts, healthy controls, ME/CFS, and Long COVID. Subjects wore the IMU on their ankle for a period of 7 days. UpTime and Steps/Day were calculated each day and results compared between cohorts. Results: UpTime effectively distinguishes between healthy controls and subjects diagnosed with ME/CFS ($\mathbf{p = 0.00004}$) and between healthy controls and subjects diagnosed with Long COVID ($\mathbf{p = 0.01185}$). Steps/Day did distinguish between controls and subjects with ME/CFS ($\mathbf{p = 0.01}$) but did not distinguish between controls and subjects with Long COVID ($\mathbf{p = 0.3}$). Conclusion: UpTime is an objective measure of ME/CFS and Long COVID severity. UpTime can be used as an objective outcome measure in clinical research and treatment trials. Significance: Objective assessment of ME/CFS and Long COVID disease severity using UpTime could spur development of treatments by enabling the effect of those treatments to be easily measured.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04345v1</guid>
      <category>q-bio.QM</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Sun, Suzanne D. Vernon, Shad Roundy</dc:creator>
    </item>
    <item>
      <title>Convolutional Neural Network Transformer (CNNT) for Fluorescence Microscopy image Denoising with Improved Generalization and Fast Adaptation</title>
      <link>https://arxiv.org/abs/2404.04726</link>
      <description>arXiv:2404.04726v1 Announce Type: new 
Abstract: Deep neural networks have been applied to improve the image quality of fluorescence microscopy imaging. Previous methods are based on convolutional neural networks (CNNs) which generally require more time-consuming training of separate models for each new imaging experiment, impairing the applicability and generalization. Once the model is trained (typically with tens to hundreds of image pairs) it can then be used to enhance new images that are like the training data. In this study, we proposed a novel imaging-transformer based model, Convolutional Neural Network Transformer (CNNT), to outperform the CNN networks for image denoising. In our scheme we have trained a single CNNT based backbone model from pairwise high-low SNR images for one type of fluorescence microscope (instance structured illumination, iSim). Fast adaption to new applications was achieved by fine-tuning the backbone on only 5-10 sample pairs per new experiment. Results show the CNNT backbone and fine-tuning scheme significantly reduces the training time and improves the image quality, outperformed training separate models using CNN approaches such as - RCAN and Noise2Fast. Here we show three examples of the efficacy of this approach on denoising wide-field, two-photon and confocal fluorescence data. In the confocal experiment, which is a 5 by 5 tiled acquisition, the fine-tuned CNNT model reduces the scan time form one hour to eight minutes, with improved quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04726v1</guid>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Azaan Rehman, Alexander Zhovmer, Ryo Sato, Yosuke Mukoyama, Jiji Chen, Alberto Rissone, Rosa Puertollano, Harshad Vishwasrao, Hari Shroff, Christian A. Combs, Hui Xue</dc:creator>
    </item>
    <item>
      <title>A probabilistic model of relapse in drug addiction</title>
      <link>https://arxiv.org/abs/2404.04755</link>
      <description>arXiv:2404.04755v1 Announce Type: new 
Abstract: More than 60% of individuals recovering from substance use disorder relapse within one year. Some will resume drug consumption even after decades of abstinence. The cognitive and psychological mechanisms that lead to relapse are not completely understood, but stressful life experiences and external stimuli that are associated with past drug-taking are known to play a primary role. Stressors and cues elicit memories of drug-induced euphoria and the expectation of relief from current anxiety, igniting an intense craving to use again; positive experiences and supportive environments may mitigate relapse. We present a mathematical model of relapse in drug addiction that draws on known psychiatric concepts such as the "positive activation; negative activation" paradigm and the "peak-end" rule to construct a relapse rate that depends on external factors (intensity and timing of life events) and individual traits (mental responses to these events). We analyze which combinations and ordering of stressors, cues, and positive events lead to the largest relapse probability and propose interventions to minimize the likelihood of relapse. We find that the best protective factor is exposure to a mild, yet continuous, source of contentment, rather than large, episodic jolts of happiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04755v1</guid>
      <category>q-bio.QM</category>
      <category>q-bio.NC</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sayun Mao, Tom Chou, Maria D'Orsogna</dc:creator>
    </item>
    <item>
      <title>DeepLINK-T: deep learning inference for time series data using knockoffs and LSTM</title>
      <link>https://arxiv.org/abs/2404.04317</link>
      <description>arXiv:2404.04317v1 Announce Type: cross 
Abstract: High-dimensional longitudinal time series data is prevalent across various real-world applications. Many such applications can be modeled as regression problems with high-dimensional time series covariates. Deep learning has been a popular and powerful tool for fitting these regression models. Yet, the development of interpretable and reproducible deep-learning models is challenging and remains underexplored. This study introduces a novel method, Deep Learning Inference using Knockoffs for Time series data (DeepLINK-T), focusing on the selection of significant time series variables in regression while controlling the false discovery rate (FDR) at a predetermined level. DeepLINK-T combines deep learning with knockoff inference to control FDR in feature selection for time series models, accommodating a wide variety of feature distributions. It addresses dependencies across time and features by leveraging a time-varying latent factor structure in time series covariates. Three key ingredients for DeepLINK-T are 1) a Long Short-Term Memory (LSTM) autoencoder for generating time series knockoff variables, 2) an LSTM prediction network using both original and knockoff variables, and 3) the application of the knockoffs framework for variable selection with FDR control. Extensive simulation studies have been conducted to evaluate DeepLINK-T's performance, showing its capability to control FDR effectively while demonstrating superior feature selection power for high-dimensional longitudinal time series data compared to its non-time series counterpart. DeepLINK-T is further applied to three metagenomic data sets, validating its practical utility and effectiveness, and underscoring its potential in real-world applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04317v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenxuan Zuo, Zifan Zhu, Yuxuan Du, Yi-Chun Yeh, Jed A. Fuhrman, Jinchi Lv, Yingying Fan, Fengzhu Sun</dc:creator>
    </item>
    <item>
      <title>BOLD v4: A Centralized Bioinformatics Platform for DNA-based Biodiversity Data</title>
      <link>https://arxiv.org/abs/2404.05696</link>
      <description>arXiv:2404.05696v1 Announce Type: cross 
Abstract: BOLD, the Barcode of Life Data System, supports the acquisition, storage, validation, analysis, and publication of DNA barcodes, activities requiring the integration of molecular, morphological, and distributional data. Its pivotal role in curating the reference library of DNA barcodes, coupled with its data management and analysis capabilities, make it a central resource for biodiversity science. It enables rapid, accurate identification of specimens and also reveals patterns of genetic diversity and evolutionary relationships among taxa. Launched in 2005, BOLD has become an increasingly powerful tool for advancing understanding of planetary biodiversity. It currently hosts 17 million specimen records and 14 million barcodes that provide coverage for more than a million species from every continent and ocean. The platform has the long-term goal of providing a consistent, accurate system for identifying all species of eukaryotes. BOLD's integrated analytical tools, full data lifecycle support, and secure collaboration framework distinguish it from other biodiversity platforms. BOLD v4 brought enhanced data management and analysis capabilities as well as novel functionality for data dissemination and publication. Its next version will include features to strengthen its utility to the research community, governments, industry, and society-at-large.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05696v1</guid>
      <category>cs.DB</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sujeevan Ratnasingham, Catherine Wei, Dean Chan, Jireh Agda, Josh Agda, Liliana Ballesteros-Mejia, Hamza Ait Boutou, Zak Mohammad El Bastami, Eddie Ma, Ramya Manjunath, Dana Rea, Chris Ho, Angela Telfer, Jaclyn McKeowan, Miduna Rahulan, Claudia Steinke, Justin Dorsheimer, Megan Milton, Paul D. N. Hebert</dc:creator>
    </item>
    <item>
      <title>Tensor-based Multimodal Learning for Prediction of Pulmonary Arterial Wedge Pressure from Cardiac MRI</title>
      <link>https://arxiv.org/abs/2303.07540</link>
      <description>arXiv:2303.07540v2 Announce Type: replace-cross 
Abstract: Heart failure is a serious and life-threatening condition that can lead to elevated pressure in the left ventricle. Pulmonary Arterial Wedge Pressure (PAWP) is an important surrogate marker indicating high pressure in the left ventricle. PAWP is determined by Right Heart Catheterization (RHC) but it is an invasive procedure. A non-invasive method is useful in quickly identifying high-risk patients from a large population. In this work, we develop a tensor learning-based pipeline for identifying PAWP from multimodal cardiac Magnetic Resonance Imaging (MRI). This pipeline extracts spatial and temporal features from high-dimensional scans. For quality control, we incorporate an epistemic uncertainty-based binning strategy to identify poor-quality training samples. To improve the performance, we learn complementary information by integrating features from multimodal data: cardiac MRI with short-axis and four-chamber views, and Electronic Health Records. The experimental analysis on a large cohort of $1346$ subjects who underwent the RHC procedure for PAWP estimation indicates that the proposed pipeline has a diagnostic value and can produce promising performance with significant improvement over the baseline in clinical practice (i.e., $\Delta$AUC $=0.10$, $\Delta$Accuracy $=0.06$, and $\Delta$MCC $=0.39$). The decision curve analysis further confirms the clinical utility of our method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.07540v2</guid>
      <category>cs.LG</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-43990-2_20</arxiv:DOI>
      <dc:creator>Prasun C. Tripathi, Mohammod N. I. Suvon, Lawrence Schobs, Shuo Zhou, Samer Alabed, Andrew J. Swift, Haiping Lu</dc:creator>
    </item>
    <item>
      <title>SegmentAnything helps microscopy images based automatic and quantitative organoid detection and analysis</title>
      <link>https://arxiv.org/abs/2309.04190</link>
      <description>arXiv:2309.04190v4 Announce Type: replace-cross 
Abstract: Organoids are self-organized 3D cell clusters that closely mimic the architecture and function of in vivo tissues and organs. Quantification of organoid morphology helps in studying organ development, drug discovery, and toxicity assessment. Recent microscopy techniques provide a potent tool to acquire organoid morphology features, but manual image analysis remains a labor and time-intensive process. Thus, this paper proposes a comprehensive pipeline for microscopy analysis that leverages the SegmentAnything to precisely demarcate individual organoids. Additionally, we introduce a set of morphological properties, including perimeter, area, radius, non-smoothness, and non-circularity, allowing researchers to analyze the organoid structures quantitatively and automatically. To validate the effectiveness of our approach, we conducted tests on bright-field images of human induced pluripotent stem cells (iPSCs) derived neural-epithelial (NE) organoids. The results obtained from our automatic pipeline closely align with manual organoid detection and measurement, showcasing the capability of our proposed method in accelerating organoids morphology analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.04190v4</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaodan Xing, Chunling Tang, Yunzhe Guo, Nicholas Kurniawan, Guang Yang</dc:creator>
    </item>
    <item>
      <title>On the Feasibility of Deep Learning Classification from Raw Signal Data in Radiology, Ultrasonography and Electrophysiology</title>
      <link>https://arxiv.org/abs/2402.16165</link>
      <description>arXiv:2402.16165v2 Announce Type: replace-cross 
Abstract: Medical imaging is a very useful tool in healthcare, various technologies being employed to non-invasively peek inside the human body. Deep learning with neural networks in radiology was welcome - albeit cautiously - by the radiologist community. Most of the currently deployed or researched deep learning solutions are applied on already generated images of medical scans, use the neural networks to aid in the generation of such images, or use them for identifying specific substance markers in spectrographs. This paper's author posits that if the neural networks were trained directly on the raw signals from the scanning machines, they would gain access to more nuanced information than from the already processed images, hence the training - and later, the inferences - would become more accurate. The paper presents the main current applications of deep learning in radiography, ultrasonography, and electrophysiology, and discusses whether the proposed neural network training directly on raw signals is feasible.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16165v2</guid>
      <category>eess.SY</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Szilard Enyedi</dc:creator>
    </item>
  </channel>
</rss>
