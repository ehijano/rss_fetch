<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 12 Jul 2024 01:41:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 11 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Asymptotic locations of bounded and unbounded eigenvalues of sample correlation matrices of certain factor models -- application to a components retention rule</title>
      <link>https://arxiv.org/abs/2407.07282</link>
      <description>arXiv:2407.07282v1 Announce Type: new 
Abstract: Let the dimension $N$ of data and the sample size $T$ tend to $\infty$ with $N/T \to c &gt; 0$. The spectral properties of a sample correlation matrix $\mathbf{C}$ and a sample covariance matrix $\mathbf{S}$ are asymptotically equal whenever the population correlation matrix $\mathbf{R}$ is bounded (El Karoui 2009). We demonstrate this also for general linear models for unbounded $\mathbf{R}$, by examining the behavior of the singular values of multiplicatively perturbed matrices. By this, we establish: Given a factor model of an idiosyncratic noise variance $\sigma^2$ and a rank-$r$ factor loading matrix $\mathbf{L}$ which rows all have common Euclidean norm $L$. Then, the $k$th largest eigenvalues $\lambda_k$ $(1\le k\le N)$ of $\mathbf{C}$ satisfy almost surely: (1) $\lambda_r$ diverges, (2) $\lambda_k/s_k^2\to1/(L^2 + \sigma^2)$ $(1 \le k \le r)$ for the $k$th largest singular value $s_k$ of $\mathbf{L}$, and (3) $\lambda_{r + 1}\to(1-\rho)(1+\sqrt{c})^2$ for $\rho := L^2/(L^2 + \sigma^2)$. Whenever $s_r$ is much larger than $\sqrt{\log N}$, then broken-stick rule (Frontier 1976, Jackson 1993), which estimates $\mathrm{rank}\, \mathbf{L}$ by a random partition (Holst 1980) of $[0,\,1]$, tends to $r$ (a.s.). We also provide a natural factor model where the rule tends to "essential rank" of $\mathbf{L}$ (a.s.) which is smaller than $\mathrm{rank}\, \mathbf{L}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07282v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yohji Akama, Peng Tian</dc:creator>
    </item>
    <item>
      <title>Geometric quantile-based measures of multivariate distributional characteristics</title>
      <link>https://arxiv.org/abs/2407.07297</link>
      <description>arXiv:2407.07297v1 Announce Type: new 
Abstract: Several new geometric quantile-based measures for multivariate dispersion, skewness, kurtosis, and spherical asymmetry are defined. These measures differ from existing measures, which use volumes and are easy to calculate. Some theoretical justification is given, followed by experiments illustrating that they are reasonable measures of these distributional characteristics and computing confidence regions with the desired coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07297v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ha-Young Shin, Hee-Seok Oh</dc:creator>
    </item>
    <item>
      <title>Viscosity estimation for 2D pipe flows I. Construction, consistency, asymptotic normality</title>
      <link>https://arxiv.org/abs/2407.07369</link>
      <description>arXiv:2407.07369v1 Announce Type: new 
Abstract: We consider the motion of incompressible viscous fluid in a rectangle, imposing the periodicity condition in one direction and the no-slip boundary condition in the other. Assuming that the flow is subject to an external random force, white in time and regular in space, we construct an estimator for the viscosity using only observations of the enstrophy. The goal of the paper is to prove that the estimator is strongly consistent and asymptotically normal. The proof of consistency is based on the explicit formula for the estimator and some bounds for trajectories, while that of asymptotic normality uses in addition mixing properties of the Navier-Stokes flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07369v1</guid>
      <category>math.ST</category>
      <category>math.AP</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thi Hien Nguyen, Armen Shirikyan</dc:creator>
    </item>
    <item>
      <title>Granulometric Smoothing on Manifolds</title>
      <link>https://arxiv.org/abs/2407.07559</link>
      <description>arXiv:2407.07559v1 Announce Type: new 
Abstract: Given a random sample from a density function supported on a manifold M, a new method for the estimating highest density regions of the underlying population is introduced. The new proposal is based on the empirical version of the opening operator from mathematical morphology combined with a preliminary estimator of density function. This results in an estimator that is easy-to-compute since it simply consists of a list of carefully selected centers and a radius. The new estimator is shown to be consistent, and its convergence rate in terms of the Hausdorff distance are provided. All consistency results are established uniformly on the level of the set and for any Riemannian manifold M satisfying mild assumptions. The applicability of the procedure is shown by means of some illustrative examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07559v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Diego Bol\'on, Rosa M. Crujeiras, Alberto Rodr\'iguez-Casal</dc:creator>
    </item>
    <item>
      <title>A Simple Note on the Basic Properties of Subgaussian Random Variables</title>
      <link>https://arxiv.org/abs/2407.07348</link>
      <description>arXiv:2407.07348v1 Announce Type: cross 
Abstract: This note provides a basic description of subgaussianity, by defining $(\sigma, \rho)$-subgaussian random variables $X$ ($\sigma&gt;0, \rho&gt;0$) as those satisfying $\mathbb{E}(\exp(\lambda X))\leq \rho\exp(\frac{1}{2}\sigma^2\lambda^2)$ for any $\lambda\in\mathbb{R}$. The introduction of the parameter $\rho$ may be particularly useful for those seeking to refine bounds, or align results from different sources, in the analysis of stochastic processes and concentration inequalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07348v1</guid>
      <category>math.PR</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Li</dc:creator>
    </item>
    <item>
      <title>Pessimism Meets Risk: Risk-Sensitive Offline Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2407.07631</link>
      <description>arXiv:2407.07631v1 Announce Type: cross 
Abstract: We study risk-sensitive reinforcement learning (RL), a crucial field due to its ability to enhance decision-making in scenarios where it is essential to manage uncertainty and minimize potential adverse outcomes. Particularly, our work focuses on applying the entropic risk measure to RL problems. While existing literature primarily investigates the online setting, there remains a large gap in understanding how to efficiently derive a near-optimal policy based on this risk measure using only a pre-collected dataset. We center on the linear Markov Decision Process (MDP) setting, a well-regarded theoretical framework that has yet to be examined from a risk-sensitive standpoint. In response, we introduce two provably sample-efficient algorithms. We begin by presenting a risk-sensitive pessimistic value iteration algorithm, offering a tight analysis by leveraging the structure of the risk-sensitive performance measure. To further improve the obtained bounds, we propose another pessimistic algorithm that utilizes variance information and reference-advantage decomposition, effectively improving both the dependence on the space dimension $d$ and the risk-sensitivity factor. To the best of our knowledge, we obtain the first provably efficient risk-sensitive offline RL algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07631v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dake Zhang, Boxiang Lyu, Shuang Qiu, Mladen Kolar, Tong Zhang</dc:creator>
    </item>
    <item>
      <title>High-dimensional Covariance Estimation by Pairwise Likelihood Truncation</title>
      <link>https://arxiv.org/abs/2407.07717</link>
      <description>arXiv:2407.07717v1 Announce Type: cross 
Abstract: Pairwise likelihood offers a useful approximation to the full likelihood function for covariance estimation in high-dimensional context. It simplifies high-dimensional dependencies by combining marginal bivariate likelihood objects, thereby making estimation more manageable. In certain models, including the Gaussian model, both pairwise and full likelihoods are known to be maximized by the same parameter values, thus retaining optimal statistical efficiency, when the number of variables is fixed. Leveraging this insight, we introduce the estimation of sparse high-dimensional covariance matrices by maximizing a truncated version of the pairwise likelihood function, which focuses on pairwise terms corresponding to nonzero covariance elements. To achieve a meaningful truncation, we propose to minimize the discrepancy between pairwise and full likelihood scores plus an L1-penalty discouraging the inclusion of uninformative terms. Differently from other regularization approaches, our method selects whole pairwise likelihood objects rather than individual covariance parameters, thus retaining the inherent unbiasedness of the pairwise likelihood estimating equations. This selection procedure is shown to have the selection consistency property as the covariance dimension increases exponentially fast. As a result, the implied pairwise likelihood estimator is consistent and converges to the oracle maximum likelihood estimator that assumes knowledge of nonzero covariance entries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07717v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alessandro Casa, Davide Ferrari, Zhendong Huang</dc:creator>
    </item>
    <item>
      <title>Analysis of Langevin Monte Carlo from Poincar\'e to Log-Sobolev</title>
      <link>https://arxiv.org/abs/2112.12662</link>
      <description>arXiv:2112.12662v2 Announce Type: replace 
Abstract: Classically, the continuous-time Langevin diffusion converges exponentially fast to its stationary distribution $\pi$ under the sole assumption that $\pi$ satisfies a Poincar\'e inequality. Using this fact to provide guarantees for the discrete-time Langevin Monte Carlo (LMC) algorithm, however, is considerably more challenging due to the need for working with chi-squared or R\'enyi divergences, and prior works have largely focused on strongly log-concave targets. In this work, we provide the first convergence guarantees for LMC assuming that $\pi$ satisfies either a Lata\l{}a--Oleszkiewicz or modified log-Sobolev inequality, which interpolates between the Poincar\'e and log-Sobolev settings. Unlike prior works, our results allow for weak smoothness and do not require convexity or dissipativity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.12662v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sinho Chewi, Murat A. Erdogdu, Mufan Bill Li, Ruoqi Shen, Matthew Zhang</dc:creator>
    </item>
    <item>
      <title>Learning linear operators: Infinite-dimensional regression as a well-behaved non-compact inverse problem</title>
      <link>https://arxiv.org/abs/2211.08875</link>
      <description>arXiv:2211.08875v3 Announce Type: replace 
Abstract: We consider the problem of learning a linear operator $\theta$ between two Hilbert spaces from empirical observations, which we interpret as least squares regression in infinite dimensions. We show that this goal can be reformulated as an inverse problem for $\theta$ with the feature that its forward operator is generally non-compact (even if $\theta$ is assumed to be compact or of $p$-Schatten class). However, we prove that, in terms of spectral properties and regularisation theory, this inverse problem is equivalent to the known compact inverse problem associated with scalar response regression.
  Our framework allows for the elegant derivation of dimension-free rates for generic learning algorithms under H\"older-type source conditions. The proofs rely on the combination of techniques from kernel regression with recent results on concentration of measure for sub-exponential Hilbertian random variables. The obtained rates hold for a variety of practically-relevant scenarios in functional regression as well as nonlinear regression with operator-valued kernels and match those of classical kernel regression with scalar response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.08875v3</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mattes Mollenhauer, Nicole M\"ucke, T. J. Sullivan</dc:creator>
    </item>
    <item>
      <title>Censoring heavy-tail count distributions for parameter estimation with an application to stable distributions</title>
      <link>https://arxiv.org/abs/2212.11697</link>
      <description>arXiv:2212.11697v3 Announce Type: replace 
Abstract: A new approach based on censoring and moment criterion is introduced for parameter estimation of count distributions when the probability generating function is available even though a closed form of the probability mass function and/or finite moments do not exist.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.11697v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spl.2023.109903</arxiv:DOI>
      <dc:creator>Antonio Di Noia, Marzia Marcheselli, Caterina Pisani, Luca Pratelli</dc:creator>
    </item>
    <item>
      <title>A new over-dispersed count model</title>
      <link>https://arxiv.org/abs/2301.01480</link>
      <description>arXiv:2301.01480v3 Announce Type: replace-cross 
Abstract: A new two-parameter discrete distribution, namely the PoiG distribution is derived by the convolution of a Poisson variate and an independently distributed geometric random variable. This distribution generalizes both the Poisson and geometric distributions and can be used for modelling over-dispersed as well as equi-dispersed count data. A number of important statistical properties of the proposed count model, such as the probability generating function, the moment generating function, the moments, the survival function and the hazard rate function. Monotonic properties are studied, such as the log concavity and the stochastic ordering are also investigated in detail. Method of moment and the maximum likelihood estimators of the parameters of the proposed model are presented. It is envisaged that the proposed distribution may prove to be useful for the practitioners for modelling over-dispersed count data compared to its closest competitors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.01480v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anupama Nandi, Subrata Chakraborty, Aniket Biswas</dc:creator>
    </item>
    <item>
      <title>Testing for sparse idiosyncratic components in factor-augmented regression models</title>
      <link>https://arxiv.org/abs/2307.13364</link>
      <description>arXiv:2307.13364v4 Announce Type: replace-cross 
Abstract: We propose a novel bootstrap test of a dense model, namely factor regression, against a sparse plus dense alternative augmenting model with sparse idiosyncratic components. The asymptotic properties of the test are established under time series dependence and polynomial tails. We outline a data-driven rule to select the tuning parameter and prove its theoretical validity. In simulation experiments, our procedure exhibits high power against sparse alternatives and low power against dense deviations from the null. Moreover, we apply our test to various datasets in macroeconomics and finance and often reject the null. This suggests the presence of sparsity -- on top of a dense model -- in commonly studied economic applications. The R package FAS implements our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.13364v4</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jad Beyhum, Jonas Striaukas</dc:creator>
    </item>
    <item>
      <title>Beyond the Calibration Point: Mechanism Comparison in Differential Privacy</title>
      <link>https://arxiv.org/abs/2406.08918</link>
      <description>arXiv:2406.08918v2 Announce Type: replace-cross 
Abstract: In differentially private (DP) machine learning, the privacy guarantees of DP mechanisms are often reported and compared on the basis of a single $(\varepsilon, \delta)$-pair. This practice overlooks that DP guarantees can vary substantially even between mechanisms sharing a given $(\varepsilon, \delta)$, and potentially introduces privacy vulnerabilities which can remain undetected. This motivates the need for robust, rigorous methods for comparing DP guarantees in such cases. Here, we introduce the $\Delta$-divergence between mechanisms which quantifies the worst-case excess privacy vulnerability of choosing one mechanism over another in terms of $(\varepsilon, \delta)$, $f$-DP and in terms of a newly presented Bayesian interpretation. Moreover, as a generalisation of the Blackwell theorem, it is endowed with strong decision-theoretic foundations. Through application examples, we show that our techniques can facilitate informed decision-making and reveal gaps in the current understanding of privacy risks, as current practices in DP-SGD often result in choosing mechanisms with high excess privacy vulnerabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08918v2</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Georgios Kaissis, Stefan Kolek, Borja Balle, Jamie Hayes, Daniel Rueckert</dc:creator>
    </item>
  </channel>
</rss>
