<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 03:19:45 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving the performance of Bandwidth Efficient Acknowledgement based Multicast (BEAM) protocol in VANET for Urban environment</title>
      <link>https://arxiv.org/abs/2412.18792</link>
      <description>arXiv:2412.18792v1 Announce Type: new 
Abstract: Vehicular Ad-hoc Network (VANET) is a subset of Mobile Ad-hoc Network (MANET) enabling communication between vehicles for safety, traffic updates, entertainment, and data sharing. Due to the high mobility in VANETs, routing messages to their final destination is challenging. Various protocols, such as broadcasting, multicasting, and geo-casting, are used to disseminate data. Multicasting protocols are effective in conserving bandwidth. One such protocol, Bandwidth Efficient Acknowledgment Based Multicasting Protocol (BEAM), improves VANET performance by minimizing in-network message transactions, particularly in emergencies. However, BEAM may cause multi-car collisions due to the absence of vehicle-to-vehicle (V2V) communication. To address this, we propose an algorithm that incorporates clustering, grouping vehicles based on predefined metrics like density, velocity, and location. Clustering controls VANET topology dynamics, enhancing stability and reducing communication barriers and RSU installation costs. The proposed approach uses static and mobile agents for communication between vehicles and RSUs. In multicast groups, RSUs manage communication, while cluster heads (CHs) handle non-multicast groups. Agents decide cluster size and select primary and secondary cluster heads based on vehicle speed and connectivity. A backup cluster head prevents re-clustering during high mobility, extending cluster life and reducing routing overhead. Simulation results demonstrate improved performance in terms of throughput, PDR, and end-to-end delay compared to BEAM</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18792v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alehegn Minale Chanie (Dr), Dawit Kflie (Dr), Getamesay Haile</dc:creator>
    </item>
    <item>
      <title>Digital Twin Enhanced Deep Reinforcement Learning for Intelligent Omni-Surface Configurations in MU-MIMO Systems</title>
      <link>https://arxiv.org/abs/2412.18856</link>
      <description>arXiv:2412.18856v1 Announce Type: new 
Abstract: Intelligent omni-surface (IOS) is a promising technique to enhance the capacity of wireless networks, by reflecting and refracting the incident signal simultaneously. Traditional IOS configuration schemes, relying on all sub-channels' channel state information and user equipments' mobility, are difficult to implement in complex realistic systems. Existing works attempt to address this issue employing deep reinforcement learning (DRL), but this method requires a lot of trial-and-error interactions with the external environment for efficient results and thus cannot satisfy the real-time decision-making. To enable model-free and real-time IOS control, this paper puts forth a new framework that integrates DRL and digital twins. DeepIOS, a DRL based IOS configuration scheme with the goal of maximizing the sum data rate, is first developed to jointly optimize the phase-shift and amplitude of IOS in multi-user multiple-input-multiple-output systems. Thereafter, to further reduce the computational complexity, DeepIOS introduces an action branch architecture, which separately decides two optimization variables in parallel. Finally, a digital twin module is constructed through supervised learning as a pre-verification platform for DeepIOS, such that the decision-making's real-time can be guaranteed. The formulated framework is a closed-loop system, in which the physical space provides data to establish and calibrate the digital space, while the digital space generates experience samples for DeepIOS training and sends the trained parameters to the IOS controller for configurations. Numerical results show that compared with random and MAB schemes, the proposed framework attains a higher data rate and is more robust to different settings. Furthermore, the action branch architecture reduces DeepIOS's computational complexity, and the digital twin module improves the convergence speed and run-time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18856v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaowen Ye, Xianghao Yu, Liqun Fu</dc:creator>
    </item>
    <item>
      <title>VINEVI: A Virtualized Network Vision Architecture for Smart Monitoring of Heterogeneous Applications and Infrastructures</title>
      <link>https://arxiv.org/abs/2412.19226</link>
      <description>arXiv:2412.19226v1 Announce Type: new 
Abstract: Monitoring heterogeneous infrastructures and applications is essential to cope with user requirements properly, but it still lacks enhancements. The well-known state-of-the-art methods and tools do not support seamless monitoring of bare-metal, low-cost infrastructures, neither hosted nor virtualized services with fine-grained details. This work proposes VIrtualized NEtwork VIsion architecture (VINEVI), an intelligent method for seamless monitoring heterogeneous infrastructures and applications. The VINEVI architecture advances state of the art with a node-embedded traffic classification agent placing physical and virtualized infrastructures enabling real-time traffic classification. VINEVI combines this real-time traffic classification with well-known tools such as Prometheus and Victoria Metrics to monitor the entire stack from the hardware to the virtualized applications. Experimental results showcased that VINEVI architecture allowed seamless heterogeneous infrastructure monitoring with a higher level of detail beyond literature. Also, our node-embedded real-time Internet traffic classifier evolved with flexibility the methods with monitoring heterogeneous infrastructures seamlessly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19226v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-030-99584-3_46</arxiv:DOI>
      <arxiv:journal_reference>International Conference on Advanced Information Networking and Applications (AINA-2022)</arxiv:journal_reference>
      <dc:creator>Rodrigo Moreira, Hugo G. V. O. da Cunha, Larissa F. Rodrigues Moreira, Fl\'avio de Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>6Diffusion: IPv6 Target Generation Using a Diffusion Model with Global-Local Attention Mechanisms for Internet-wide IPv6 Scanning</title>
      <link>https://arxiv.org/abs/2412.19243</link>
      <description>arXiv:2412.19243v1 Announce Type: new 
Abstract: Due to the vast address space of IPv6, the brute-force scanning methods originally applicable to IPv4 are no longer suitable for proactive scanning of IPv6. The recently proposed target generation algorithms have a low hit rate for existing IPv6 target generation algorithms, primarily because they do not accurately fit the distribution patterns of active IPv6 addresses. This paper introduces a diffusion model-based IPv6 target generation algorithm called 6Diffusion. 6Diffusion first maps addresses to vector space for language modeling, adds noise to active IPv6 addresses in the forward process, diffusing them throughout the entire IPv6 address space, and then performs a reverse process to gradually denoise and recover to active IPv6 addresses. We use the DDIM sampler to increase the speed of generating candidate sets. At the same time, we introduce the GLF-MSA (Global-Local Fusion Multi-Head Self-Attention) mechanism to adapt to the top-down global allocation pattern of IPv6 addresses and the local characteristics of IPv6 address segments, thus better learning the deep-level features of active IPv6 addresses. Experimental results show that compared to existing methods, 6Diffusion can generate higher quality candidate sets and outperforms state-of-the-art target generation algorithms across multiple metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19243v1</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nabo He, DanDan Li, Xiaohong Huang</dc:creator>
    </item>
    <item>
      <title>Improving the network traffic classification using the Packet Vision approach</title>
      <link>https://arxiv.org/abs/2412.19360</link>
      <description>arXiv:2412.19360v1 Announce Type: new 
Abstract: The network traffic classification allows improving the management, and the network services offer taking into account the kind of application. The future network architectures, mainly mobile networks, foresee intelligent mechanisms in their architectural frameworks to deliver application-aware network requirements. The potential of convolutional neural networks capabilities, widely exploited in several contexts, can be used in network traffic classification. Thus, it is necessary to develop methods based on the content of packets transforming it into a suitable input for CNN technologies. Hence, we implemented and evaluated the Packet Vision, a method capable of building images from packets raw-data, considering both header and payload. Our approach excels those found in state-of-the-art by delivering security and privacy by transforming the raw-data packet into images. Therefore, we built a dataset with four traffic classes evaluating the performance of three CNNs architectures: AlexNet, ResNet-18, and SqueezeNet. Experiments showcase the Packet Vision combined with CNNs applicability and suitability as a promising approach to deliver outstanding performance in classifying network traffic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19360v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.CV</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5753/wvc.2020.13496</arxiv:DOI>
      <arxiv:journal_reference>WORKSHOP DE VIS\~AO COMPUTACIONAL (WVC) 2020</arxiv:journal_reference>
      <dc:creator>Rodrigo Moreira, Larissa Ferreira Rodrigues, Pedro Frosi Rosa, Fl\'avio de Oliveira Silva</dc:creator>
    </item>
    <item>
      <title>An Overview of Machine Learning-Driven Resource Allocation in IoT Networks</title>
      <link>https://arxiv.org/abs/2412.19478</link>
      <description>arXiv:2412.19478v1 Announce Type: new 
Abstract: In the wake of disruptive IoT technologies generating massive amounts of diverse data, Machine Learning (ML) will play a crucial role in bringing intelligence to Internet of Things (IoT) networks. This paper provides a comprehensive analysis of the current state of resource allocation within IoT networks, focusing specifically on two key categories: Low-Power IoT Networks and Mobile IoT Networks. We delve into the resource allocation strategies that are crucial for optimizing network performance and energy efficiency in these environments. Furthermore, the paper explores the transformative role of Machine Learning (ML), Deep Learning (DL), and Reinforcement Learning (RL) in enhancing IoT functionalities. We highlight a range of applications and use cases where these advanced technologies can significantly improve decision-making and optimization processes. In addition to the opportunities presented by ML, DL, and RL, we also address the potential challenges that organizations may face when implementing these technologies in IoT settings. These challenges include crucial accuracy, low flexibility and adaptability, and high computational cost, etc. Finally, the paper identifies promising avenues for future research, emphasizing the need for innovative solutions to overcome existing hurdles and improve the integration of ML, DL, and RL into IoT networks. By providing this holistic perspective, we aim to contribute to the ongoing discourse on resource allocation strategies and the application of intelligent technologies in the IoT landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19478v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhengdong Li</dc:creator>
    </item>
    <item>
      <title>Retrieval-augmented Generation for GenAI-enabled Semantic Communications</title>
      <link>https://arxiv.org/abs/2412.19494</link>
      <description>arXiv:2412.19494v1 Announce Type: new 
Abstract: Semantic communication (SemCom) is an emerging paradigm aiming at transmitting only task-relevant semantic information to the receiver, which can significantly improve communication efficiency. Recent advancements in generative artificial intelligence (GenAI) have empowered GenAI-enabled SemCom (GenSemCom) to further expand its potential in various applications. However, current GenSemCom systems still face challenges such as semantic inconsistency, limited adaptability to diverse tasks and dynamic environments, and the inability to leverage insights from past transmission. Motivated by the success of retrieval-augmented generation (RAG) in the domain of GenAI, this paper explores the integration of RAG in GenSemCom systems. Specifically, we first provide a comprehensive review of existing GenSemCom systems and the fundamentals of RAG techniques. We then discuss how RAG can be integrated into GenSemCom. Following this, we conduct a case study on semantic image transmission using an RAG-enabled diffusion-based SemCom system, demonstrating the effectiveness of the proposed integration. Finally, we outline future directions for advancing RAG-enabled GenSemCom systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19494v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shunpu Tang, Ruichen Zhang, Yuxuan Yan, Qianqian Yang, Dusit Niyato, Xianbin Wang, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>Performance Evaluation of IoT LoRa Networks on Mars Through ns-3 Simulations</title>
      <link>https://arxiv.org/abs/2412.19549</link>
      <description>arXiv:2412.19549v1 Announce Type: new 
Abstract: In recent years, there has been a significant surge of interest in Mars exploration, driven by the planet's potential for human settlement and its proximity to Earth. In this paper, we explore the performance of the LoRaWAN technology on Mars, to study whether commercial off-the-shelf IoT products, designed and developed on Earth, can be deployed on the Martian surface. We use the ns-3 simulator to model various environmental conditions, primarily focusing on the Free Space Path Loss (FSPL) and the impact of Martian dust storms. Simulation results are given with respect to Earth, as a function of the distance, packet size, offered traffic, and the impact of Mars' atmospheric perturbations. We show that LoRaWAN can be a viable communication solution on Mars, although the performance is heavily affected by the extreme Martian environment over long distances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19549v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Manuele Favero, Alessandro Canova, Marco Giordani, Michele Zorzi</dc:creator>
    </item>
    <item>
      <title>Adaptive Context-Aware Multi-Path Transmission Control for VR/AR Content: A Deep Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2412.19737</link>
      <description>arXiv:2412.19737v1 Announce Type: new 
Abstract: This paper introduces the Adaptive Context-Aware Multi-Path Transmission Control Protocol (ACMPTCP), an efficient approach designed to optimize the performance of Multi-Path Transmission Control Protocol (MPTCP) for data-intensive applications such as augmented and virtual reality (AR/VR) streaming. ACMPTCP addresses the limitations of conventional MPTCP by leveraging deep reinforcement learning (DRL) for agile end-to-end path management and optimal bandwidth allocation, facilitating path realignment across diverse network environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19737v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shakil Ahmed, Saifur Rahman Sabuj, Ashfaq Khokhar</dc:creator>
    </item>
    <item>
      <title>Detection and classification of DDoS flooding attacks by machine learning method</title>
      <link>https://arxiv.org/abs/2412.18990</link>
      <description>arXiv:2412.18990v1 Announce Type: cross 
Abstract: This study focuses on a method for detecting and classifying distributed denial of service (DDoS) attacks, such as SYN Flooding, ACK Flooding, HTTP Flooding, and UDP Flooding, using neural networks. Machine learning, particularly neural networks, is highly effective in detecting malicious traffic. A dataset containing normal traffic and various DDoS attacks was used to train a neural network model with a 24-106-5 architecture. The model achieved high Accuracy (99.35%), Precision (99.32%), Recall (99.54%), and F-score (0.99) in the classification task. All major attack types were correctly identified. The model was also further tested in the lab using virtual infrastructures to generate normal and DDoS traffic. The results showed that the model can accurately classify attacks under near-real-world conditions, demonstrating 95.05% accuracy and balanced F-score scores for all attack types. This confirms that neural networks are an effective tool for detecting DDoS attacks in modern information security systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18990v1</guid>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of the 1st International Workshop on Bioinformatics and Applied Information Technologies (BAIT 2024), Zboriv, Ukraine, October 02-04, 2024</arxiv:journal_reference>
      <dc:creator>Dmytro Tymoshchuk, Oleh Yasniy, Mykola Mytnyk, Nataliya Zagorodna, Vitaliy Tymoshchuk</dc:creator>
    </item>
    <item>
      <title>Empowering the Edge Intelligence by Air-Ground Integrated Federated Learning</title>
      <link>https://arxiv.org/abs/2007.13054</link>
      <description>arXiv:2007.13054v2 Announce Type: replace 
Abstract: Ubiquitous intelligence has been widely recognized as a critical vision of the future sixth generation (6G) networks, which implies the intelligence over the whole network from the core to the edge including end devices. Nevertheless, fulfilling such vision, particularly the intelligence at the edge, is extremely challenging, due to the limited resources of edge devices as well as the ubiquitous coverage envisioned by 6G. To empower the edge intelligence, in this article, we propose a novel framework called AGIFL (Air-Ground Integrated Federated Learning), which organically integrates air-ground integrated networks and federated learning (FL). In the AGIFL, leveraging the flexible on-demand 3D deployment of aerial nodes such as unmanned aerial vehicles (UAVs), all the nodes can collaboratively train an effective learning model by FL. We also conduct a case study to evaluate the effect of two different deployment schemes of the UAV over the learning and network performance. Last but not the least, we highlight several technical challenges and future research directions in the AGIFL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.13054v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuben Qu, Chao Dong, Jianchao Zheng, Haipeng Dai, Fan Wu, Song Guo, Alagan Anpalagan</dc:creator>
    </item>
    <item>
      <title>Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven Digital Twins in Industrial Cyber-Physical Systems</title>
      <link>https://arxiv.org/abs/2408.01173</link>
      <description>arXiv:2408.01173v2 Announce Type: replace 
Abstract: Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern manufacturing and industries. By digitizing data throughout product life cycles, Digital Twins (DTs) in ICPSs enable a shift from current industrial infrastructures to intelligent and adaptive infrastructures. Thanks to data process capability, Generative Artificial Intelligence (GenAI) can drive the construction and update of DTs to improve predictive accuracy and prepare for diverse smart manufacturing. However, mechanisms that leverage Industrial Internet of Things (IIoT) devices to share sensing data for DT construction are susceptible to adverse selection problems. In this paper, we first develop a GenAI-driven DT architecture in ICPSs. To address the adverse selection problem caused by information asymmetry, we propose a contract theory model and develop a sustainable diffusion-based soft actor-critic algorithm to identify the optimal feasible contract. Specifically, we leverage dynamic structured pruning techniques to reduce parameter numbers of actor networks, allowing sustainability and efficient implementation of the proposed algorithm. Numerical results demonstrate the effectiveness of the proposed scheme and the algorithm, enabling efficient DT construction and updates to monitor and manage ICPSs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01173v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinbo Wen, Jiawen Kang, Dusit Niyato, Yang Zhang, Shiwen Mao</dc:creator>
    </item>
    <item>
      <title>A Large-Scale IPv6-Based Measurement of the Starlink Network</title>
      <link>https://arxiv.org/abs/2412.18243</link>
      <description>arXiv:2412.18243v2 Announce Type: replace 
Abstract: Low Earth Orbit (LEO) satellite networks have attracted considerable attention for their ability to deliver global, low-latency broadband Internet services. In this paper, we present a large-scale measurement study of the Starlink network, the largest LEO satellite constellation to date. We begin by proposing an efficient method for discovering active Starlink user routers, identifying approximately 3.2 million IPv6 addresses across 102 countries and 123 regions-representing, to the best of our knowledge, the most complete list of Starlink user routers' active IPv6 addresses. Based on the discovered user routers, we map the Starlink backbone network, which consists of 33 Points of Presence (PoPs) and 70 connections between them. Furthermore, we conduct a detailed statistical analysis of active Starlink users and PoPs. Finally, we summarize the IPv6 address assignment strategy adopted by the Starlink network. The dataset of the backbone network is publicly available at https://ki3.org.cn/#/starlink-network.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18243v2</guid>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingsen Wang, Xiaohui Zhang, Shuai Wang, Li Chen, Jinwei Zhao, Jianping Pan, Dan Li, Yong Jiang</dc:creator>
    </item>
    <item>
      <title>Asynchronous Telegate and Teledata Protocols for Distributed Quantum Computing</title>
      <link>https://arxiv.org/abs/2407.14987</link>
      <description>arXiv:2407.14987v2 Announce Type: replace-cross 
Abstract: The cost of distributed quantum operations such as the telegate and teledata protocols is high due to latencies from distributing entangled photons and classical information. This paper proposes an extension to the telegate and teledata protocols to allow for asynchronous classical communication which hides the cost of distributed quantum operations. We then discuss the benefits and limitations of these asynchronous protocols and propose a potential way to improve these asynchronous protocols using nonunitary operators. Finally, a quantum network card is described as an example of how asynchronous quantum operations might be used.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14987v2</guid>
      <category>quant-ph</category>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jacob Peckham, Dwight Makaroff, Steven Rayan</dc:creator>
    </item>
    <item>
      <title>AI Flow</title>
      <link>https://arxiv.org/abs/2411.12469</link>
      <description>arXiv:2411.12469v2 Announce Type: replace-cross 
Abstract: Recent advancements in large language models (LLMs) and their multimodal variants have led to remarkable progress across various domains, demonstrating impressive capabilities and unprecedented potential. In the era of ubiquitous connectivity, leveraging communication networks to distribute intelligence is a transformative concept, envisioning AI-powered services accessible at the network edge. However, pushing large models from the cloud to resource-constrained environments faces critical challenges. Model inference on low-end devices leads to excessive latency and performance bottlenecks, while raw data transmission over limited bandwidth networks causes high communication overhead. This article presents AI Flow, a framework that streamlines the inference process by jointly leveraging the heterogeneous resources available across devices, edge nodes, and cloud servers, making intelligence flow across networks. To facilitate cooperation among multiple computational nodes, the proposed framework explores a paradigm shift in the design of communication network systems from transmitting information flow to intelligence flow, where the goal of communications is task-oriented and folded into the inference process. Experimental results demonstrate the effectiveness of the proposed framework through an image captioning use case, showcasing the ability to reduce response latency while maintaining high-quality captions. This article serves as a position paper for identifying the motivation, challenges, and principles of AI Flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.12469v2</guid>
      <category>eess.SP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Mon, 30 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiawei Shao, Xuelong Li</dc:creator>
    </item>
  </channel>
</rss>
