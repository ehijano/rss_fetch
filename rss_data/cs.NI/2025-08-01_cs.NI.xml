<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Aug 2025 04:00:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>PRIME: Pseudo-Random Integrated Multi-Part Entropy for Adaptive Packet Spraying in AI/ML Data centers</title>
      <link>https://arxiv.org/abs/2507.23012</link>
      <description>arXiv:2507.23012v1 Announce Type: new 
Abstract: Large-scale distributed training in production data centers place significant demands on network infrastructure. In particular, significant load balancing challenges arise when processing AI/ML workloads, consisting of low-entropy, bursty and long-lived flows. Existing solutions designed for Ethernet, such as Equal-Cost Multi-Path (ECMP) struggle to maintain high network utilization. While major industry players (e.g., Ultra Ethernet Consortium) and parts of academia have proposed packet spraying to enhance AI/ML workload performance, we argue that existing packet spraying solutions lead to buffer inflation over time, negatively affecting network performance. Specifically, when ACK coalescing is used, these solutions lead to stale information, degrading network performance. Additionally, in asymmetric network conditions- such as mix of ordered an unordered traffic, or link degradation and failures- existing packet spraying solutions often lead to increased tail latency. In this paper, we present the design and evaluation of PRIME, a pseudo-randomized round-robin approach to packet spraying that considers the network topology to optimize load distribution and performance. PRIME uses congestion as an indicator to re-balance the load. To this extent, PRIME takes into account various congestion signals, accounting for congestion severity, and their decay times to avoid network hotspots. We extensively evaluated PRIME using large-scale production-level simulator. Our results indicate that, compared to existing solutions, PRIME leads to up to 15% improvement for permutation traffic and up to 27% improvement in network degradation scenarios</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23012v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ashkan Sobhani, Sogand Sadrhaghighi, Xingjun Chu</dc:creator>
    </item>
    <item>
      <title>InterfO-RAN: Real-Time In-band Cellular Uplink Interference Detection with GPU-Accelerated dApps</title>
      <link>https://arxiv.org/abs/2507.23177</link>
      <description>arXiv:2507.23177v1 Announce Type: new 
Abstract: Ultra-dense fifth generation (5G) and beyond networks leverage spectrum sharing and frequency reuse to enhance throughput, but face unpredictable in-band uplink (UL) interference challenges that significantly degrade Signal to Interference plus Noise Ratio (SINR) at affected Next Generation Node Bases (gNBs). This is particularly problematic at cell edges, where overlapping regions force User Equipments (UEs) to increase transmit power, and in directional millimeter wave systems, where beamforming sidelobes can create unexpected interference. The resulting signal degradation disrupts protocol operations, including scheduling and resource allocation, by distorting quality indicators like Reference Signal Received Power (RSRP) and Received Signal Strength Indicator (RSSI), and can compromise critical functions such as channel state reporting and Hybrid Automatic Repeat Request (HARQ) acknowledgments. To address this problem, this article introduces InterfO-RAN, a real-time programmable solution that leverages a Convolutional Neural Network (CNN) to process In-phase and Quadrature (I/Q) samples in the gNB physical layer, detecting in-band interference with accuracy exceeding 91% in under 650 us. InterfO-RAN represents the first O-RAN dApp accelerated on Graphics Processing Unit (GPU), coexisting with the 5G NR physical layer processing of NVIDIA Aerial. Deployed in an end-to-end private 5G network with commercial Radio Units (RUs) and smartphones, our solution was trained and tested on more than 7 million NR UL slots collected from real-world environments, demonstrating robust interference detection capabilities essential for maintaining network performance in dense deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23177v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Neagin Neasamoni Santhi, Davide Villa, Michele Polese, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>Optimal Packetization Towards Low Latency in Random Access Networks (extended version)</title>
      <link>https://arxiv.org/abs/2507.23286</link>
      <description>arXiv:2507.23286v1 Announce Type: new 
Abstract: As the demand for low-latency services grows, ensuring the delay performance of random access (RA) networks has become a priority. Existing studies on the queueing delay performance of the Aloha model universally treat packets as atomic transmission units, focusing primarily on delay measured in time slots. However, the impact of packetization on queueing delay has been consistently overlooked, particularly for the mean queueing delay measured in seconds, which serves as a more precise and practically relevant performance metric than its slot-based counterpart. Here, packetization refers to the process of determining the number of bits assembled into a packet. To optimize queueing delay from the perspective of packetization, this paper establishes the mathematical relationship between packetization and mean queueing delay in seconds for both connection-free and connection-based Aloha schemes, and explores the optimal packetization strategy to minimize this delay. We identify the optimal mean queueing delay and its corresponding packet size via numerical methods, and further analyze the influence of various network parameters. We further use simulations to investigate the similar impact of packetization on jitter of queueing delay. We then apply our analysis to re-evaluate the complex trade-off between the connection-free and connection-based schemes through the new perspective of packetization. Furthermore, recognizing that an analysis of the queueing delay performance for RA-SDT in NTN scenarios, especially from a packetization perspective, also remains an unexplored area, we apply the analysis to this scenario as a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23286v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zihong Li, Anshan Yuan, Xinghua Sun</dc:creator>
    </item>
    <item>
      <title>FAST-LoRa: An Efficient Simulation Framework for Evaluating LoRaWAN Networks and Transmission Parameter Strategies</title>
      <link>https://arxiv.org/abs/2507.23342</link>
      <description>arXiv:2507.23342v1 Announce Type: new 
Abstract: The Internet of Things (IoT) has transformed many industries, and LoRaWAN (Long Range Wide Area Network), built on LoRa (Long Range) technology, has become a crucial solution for enabling scalable, low-cost, and energy-efficient communication in wide-area networks. Simulation tools are essential for optimizing the transmission parameters and, therefore, the energy efficiency and performance of LoRaWAN networks. While existing simulation frameworks accurately replicate real-world scenarios by including multiple layers of communication protocols, they often imply significant computational overhead and simulation times. To address this issue, this paper introduces FAST-LoRa, a novel simulation framework designed to enable fast and efficient evaluation of LoRaWAN networks and selection of transmission parameters. FAST-LoRa streamlines computation by relying on analytical models without complex packet-level simulations and implementing gateway reception using efficient matrix operations. Rather than aiming to replace discrete-event simulators, FAST-LoRa is intended as a lightweight and accurate approximation tool for evaluating transmission parameter strategies in scenarios with stable traffic patterns and uplink-focused communications. In our evaluation, we compare FAST-LoRa with a well-established simulator using multiple network configurations with varying numbers of end devices and gateways. The results show that FAST-LoRa achieves similar accuracy in estimating key network metrics, even in complex scenarios with interference and multi-gateway reception, with a Mean Absolute Error (MAE) of 0.940 $\times 10^{-2}$ for the Packet Delivery Ratio (PDR) and 0.040 bits/mJ for Energy Efficiency (EE), while significantly reducing computational time by up to three orders of magnitude.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23342v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laura Acosta Garc\'ia, Juan Aznar Poveda, Fabian Margreiter, Antonio-Javier Garc\'ia S\'anchez, Joan Garc\'ia Haro, Thomas Fahringer, Jos\'e Lorente L\'opez, Jos\'e-V\'ictor Rodr\'iguez</dc:creator>
    </item>
    <item>
      <title>Dual-Mode Wireless Devices for Adaptive Pull and Push-Based Communication</title>
      <link>https://arxiv.org/abs/2507.23421</link>
      <description>arXiv:2507.23421v1 Announce Type: new 
Abstract: This paper introduces a dual-mode communication framework for wireless devices that integrates query-driven (pull) and event-driven (push) transmissions within a unified time-frame structure. Devices typically respond to information requests in pull mode, but if an anomaly is detected, they preempt the regular response to report the critical condition. Additionally, push-based communication is used to proactively send critical data without waiting for a request. This adaptive approach ensures timely, context-aware, and efficient data delivery across different network conditions. To achieve high energy efficiency, we incorporate a wake-up radio mechanism and we design a tailored medium access control (MAC) protocol that supports data traffic belonging to the different communication classes. A comprehensive system-level analysis is conducted, accounting for the wake-up control operation and evaluating three key performance metrics: the success probability of anomaly reports (push traffic), the success probability of query responses (pull traffic) and the total energy consumption. Numerical results characterize the system's behavior and highlight the inherent trade-off in success probabilities between push- and pull-based traffic as a function of allocated communication resources. Our analysis demonstrates that the proposed approach reduces energy consumption by up to 30% compared to a traditional approach, while maintaining reliable support for both communication paradigms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23421v1</guid>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Cavallero, Fabio Saggese, Junya Shiraishi, Israel Leyva-Mayorga, Shashi Raj Pandey, Chiara Buratti, Petar Popovski</dc:creator>
    </item>
    <item>
      <title>From Timestamps to Versions: Version AoI in Single- and Multi-Hop Networks</title>
      <link>https://arxiv.org/abs/2507.23433</link>
      <description>arXiv:2507.23433v1 Announce Type: new 
Abstract: Timely and informative data dissemination in communication networks is essential for enhancing system performance and energy efficiency, as it reduces the transmission of outdated or redundant data. Timeliness metrics, such as Age of Information (AoI), effectively quantify data freshness; however, these metrics fail to account for the intrinsic informativeness of the content itself. To address this limitation, content-based metrics have been proposed that combine both timeliness and informativeness. Nevertheless, existing studies have predominantly focused on evaluating average metric values, leaving the complete distribution-particularly in multi-hop network scenarios-largely unexplored. In this paper, we provide a comprehensive analysis of the stationary distribution of the Version Age of Information (VAoI), a content-based metric, under various scheduling policies, including randomized stationary, uniform, and threshold-based policies, with transmission constraints in single-hop and multi-hop networks. We derive closed-form expressions for the stationary distribution and average VAoI under these scheduling approaches. Furthermore, for threshold-based scheduling, we analytically determine the optimal threshold value that minimizes VAoI and derive the corresponding optimal VAoI in closed form. Numerical evaluations verify our analytical findings, providing valuable insights into leveraging VAoI in the design of efficient communication networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23433v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erfan Delfani, Nikolaos Pappas</dc:creator>
    </item>
    <item>
      <title>Networked Physical Computing: A New Paradigm for Effective Task Completion via Hypergraph Aided Trusted Task-Resource Matching</title>
      <link>https://arxiv.org/abs/2507.23556</link>
      <description>arXiv:2507.23556v1 Announce Type: new 
Abstract: Due to the diverse physical attributes of computing resources and tasks, developing effective mechanisms to facilitate task and resource matching in complex connected systems for value-oriented task completion has become increasingly challenging. To address the challenge, this paper proposes a networked physical computing system that integrates the physical attributes of computing resources and tasks as well as task-specific trust relationships among devices to enable value-driven task completion. Specifically, we propose a state-of-the-art hypergraph-aided trusted task-resource matching (TTR-matching) framework to achieve the envisioned physical computing. First, a task-specific trusted physical resource hypergraph is defined, which integrates task-specific trust, the physical attributes of resources, and task types. This enables accurate modeling of device collaboration dependencies under specific task types. Next, a task hypergraph is generated to associate the task initiator with the physical attributes of the corresponding tasks. Based on these two hypergraphs, a hypergraph matching algorithm is designed to facilitate task-specific trusted collaborator selection and accurate task-resource matching for value-maximizing task completion. Extensive experimental results demonstrate that the proposed TTR-matching framework outperforms comparison algorithms in identifying task-specific trustworthy collaborators and maximizing the average value of task completion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.23556v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Published in IEEE Transactions on Network Science and Engineering, 2025</arxiv:journal_reference>
      <dc:creator>Botao Zhu, Xianbin Wang</dc:creator>
    </item>
    <item>
      <title>Optimal and Near-Optimal Adaptive Vector Quantization</title>
      <link>https://arxiv.org/abs/2402.03158</link>
      <description>arXiv:2402.03158v2 Announce Type: replace-cross 
Abstract: Quantization is a fundamental optimization for many machine-learning use cases, including compressing gradients, model weights and activations, and datasets. The most accurate form of quantization is \emph{adaptive}, where the error is minimized with respect to a given input, rather than optimizing for the worst case. However, optimal adaptive quantization methods are considered infeasible in terms of both their runtime and memory requirements.
  We revisit the Adaptive Vector Quantization (AVQ) problem and present algorithms that find optimal solutions with asymptotically improved time and space complexity. We also present an even faster near-optimal algorithm for large inputs. Our experiments show our algorithms may open the door to using AVQ more extensively in a variety of machine learning applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.03158v2</guid>
      <category>cs.LG</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ran Ben-Basat, Yaniv Ben-Itzhak, Michael Mitzenmacher, Shay Vargaftik</dc:creator>
    </item>
    <item>
      <title>Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks</title>
      <link>https://arxiv.org/abs/2504.10403</link>
      <description>arXiv:2504.10403v3 Announce Type: replace-cross 
Abstract: Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10403v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan Zhu, Jingyang Zhu, Ting Wang, Yuanming Shi, Chunxiao Jiang, Khaled Ben Letaief</dc:creator>
    </item>
    <item>
      <title>Jelly: a Fast and Convenient RDF Serialization Format</title>
      <link>https://arxiv.org/abs/2506.11298</link>
      <description>arXiv:2506.11298v2 Announce Type: replace-cross 
Abstract: Existing RDF serialization formats such as Turtle, N-Quads, and JSON-LD are widely used for communication and storage in knowledge graph and Semantic Web applications. However, they suffer from limitations in performance, compression ratio, and lack of native support for RDF streams. To address these shortcomings, we introduce Jelly, a fast and convenient binary serialization format for RDF data that supports both batch and streaming use cases. Jelly is designed to maximize serialization throughput, reduce file size with lightweight streaming compression, and minimize compute resource usage. Built on Protocol Buffers, Jelly is easy to integrate with modern programming languages and RDF libraries. To maximize reusability, Jelly has an open protocol specification, open-source implementations in Java and Python integrated with popular RDF libraries, and a versatile command-line tool. To illustrate its usefulness, we outline concrete use cases where Jelly can provide tangible benefits. We consider that by combining practical usability with state-of-the-art efficiency, Jelly is an important contribution to the Semantic Web tool stack.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.11298v2</guid>
      <category>cs.DB</category>
      <category>cs.NI</category>
      <pubDate>Fri, 01 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piotr Sowinski, Karolina Bogacka, Anastasiya Danilenka, Nikita Kozlov</dc:creator>
    </item>
  </channel>
</rss>
