<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Aug 2025 04:00:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>OddEEC: A New Sketch Technique for Error Estimating Coding</title>
      <link>https://arxiv.org/abs/2508.11842</link>
      <description>arXiv:2508.11842v1 Announce Type: new 
Abstract: Error estimating coding (EEC) is a standard technique for estimating the number of bit errors during packet transmission over wireless networks. In this paper, we propose OddEEC, a novel EEC scheme. OddEEC is a nontrivial adaptation of a data sketching technique named Odd Sketch to EEC, addressing new challenges therein by its bit sampling technique and maximum likelihood estimator. Our experiments show that OddEEC overall achieves comparable estimation accuracy as competing schemes such as gEEC and mEEC, with much smaller decoding complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11842v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huayi Wang, Jingfan Meng, Jun Xu</dc:creator>
    </item>
    <item>
      <title>Bandit-Based Charging with Beamforming for Mobile Wireless-Powered IoT Systems</title>
      <link>https://arxiv.org/abs/2508.11971</link>
      <description>arXiv:2508.11971v1 Announce Type: new 
Abstract: Wireless power transfer (WPT) is increasingly used to sustain Internet-of-Things (IoT) systems by wirelessly charging embedded devices. Mobile chargers further enhance scalability in wireless-powered IoT (WP-IoT) networks, but pose new challenges due to dynamic channel conditions and limited energy budgets. Most existing works overlook such dynamics or ignore real-time constraints on charging schedules. This paper presents a bandit-based charging framework for WP-IoT systems using mobile chargers with practical beamforming capabilities and real-time charging constraints. We explicitly consider time-varying channel state information (CSI) and impose a strict charging deadline in each round, which reflects the hard real-time constraint from the charger's limited battery capacity. We formulate a temporal-spatial charging policy that jointly determines the charging locations, durations, and beamforming configurations. Area discretization enables polynomial-time enumeration with constant approximation bounds. We then propose two online bandit algorithms for both stationary and non-stationary unknown channel state scenarios with bounded regrets. Our extensive experimental results validate that the proposed algorithms can rapidly approach the theoretical upper bound while effectively tracking the dynamic channel states for adaptive adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11971v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chenchen Fu, Zining Zhou, Xiaoxing Qiu, Sujunjie Sun, Weiwei Wu, Song Han</dc:creator>
    </item>
    <item>
      <title>TailO-RAN: O-RAN Control on Scheduler Parameters to Tailor RAN Performance</title>
      <link>https://arxiv.org/abs/2508.12112</link>
      <description>arXiv:2508.12112v1 Announce Type: new 
Abstract: The traditional black-box and monolithic approach to Radio Access Networks (RANs) has heavily limited flexibility and innovation. The Open RAN paradigm, and the architecture proposed by the O-RAN ALLIANCE, aim to address these limitations via openness, virtualization and network intelligence. In this work, first we propose a novel, programmable scheduler design for Open RAN Distributed Units (DUs) that can guarantee minimum throughput levels to User Equipments (UEs) via configurable weights. Then, we propose an O-RAN xApp that reconfigures the scheduler's weights dynamically based on the joint Complementary Cumulative Distribution Function (CCDF) of reported throughput values. We demonstrate the effectiveness of our approach by considering the problem of asset tracking in 5G-powered Industrial Internet of Things (IIoT) where uplink video transmissions from a set of cameras are used to detect and track assets via computer vision algorithms. We implement our programmable scheduler on the OpenAirInterface (OAI) 5G protocol stack, and test the effectiveness of our xApp control by deploying it on the O-RAN Software Community (OSC) near-RT RAN Intelligent Controller (RIC) and controlling a 5G RAN instantiated on the Colosseum Open RAN digital twin. Our experimental results demonstrate that our approach enhances the success percentage of meeting throughput requirements by 33% compared to a reference scheduler. Moreover, in the asset tracking use case, we show that the xApp improves the detection accuracy, i.e., the F1 score, by up to 37.04%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12112v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolo Longhi, Salvatore D'Oro, Leonardo Bonati, Michele Polese, Roberto Verdone, Tommaso Melodia</dc:creator>
    </item>
    <item>
      <title>An Efficient and Adaptive Framework for Achieving Underwater High-performance Maintenance Networks</title>
      <link>https://arxiv.org/abs/2508.12661</link>
      <description>arXiv:2508.12661v1 Announce Type: new 
Abstract: With the development of space-air-ground-aqua integrated networks (SAGAIN), high-speed and reliable network services are accessible at any time and any location. However, the long propagation delay and limited network capacity of underwater communication networks (UCN) negatively impact the service quality of SAGAIN. To address this issue, this paper presents U-HPNF, a hierarchical framework designed to achieve a high-performance network with self-management, self-configuration, and self-optimization capabilities. U-HPNF leverages the sensing and decision-making capabilities of deep reinforcement learning (DRL) to manage limited resources in UCNs, including communication bandwidth, computational resources, and energy supplies. Additionally, we incorporate federated learning (FL) to iteratively optimize the decision-making model, thereby reducing communication overhead and protecting the privacy of node observation information. By deploying digital twins (DT) at both the intelligent sink layer and aggregation layer, U-HPNF can mimic numerous network scenarios and adapt to varying network QoS requirements. Through a three-tier network design with two-levels DT, U-HPNF provides an AI-native high-performance underwater network. Numerical results demonstrate that the proposed U-HPNF framework can effectively optimize network performance across various situations and adapt to changing QoS requirements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12661v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-981-96-2767-7_39</arxiv:DOI>
      <dc:creator>Yu Gou, Tong Zhang, Jun Liu, Zhongyang Qi, Dezhi Zheng</dc:creator>
    </item>
    <item>
      <title>Game-Theoretic and Reinforcement Learning-Based Cluster Head Selection for Energy-Efficient Wireless Sensor Network</title>
      <link>https://arxiv.org/abs/2508.12707</link>
      <description>arXiv:2508.12707v1 Announce Type: new 
Abstract: Energy in Wireless Sensor Networks (WSNs) is critical to network lifetime and data delivery. However, the primary impediment to the durability and dependability of these sensor nodes is their short battery life. Currently, power-saving algorithms such as clustering and routing algorithms have improved energy efficiency in standard protocols. This paper proposes a clustering-based routing approach for creating an adaptive, energy-efficient mechanism. Our system employs a multi-step clustering strategy to select dynamic cluster heads (CH) with optimal energy distribution. We use Game Theory (GT) and Reinforcement Learning (RL) to optimize resource utilization. Modeling the network as a multi-agent RL problem using GT principles allows for self-clustering while optimizing sensor lifetime and energy balance. The proposed AI-powered CH-Finding algorithm improves network efficiency by preventing premature energy depletion in specific nodes while also ensuring uniform energy usage across the network. Our solution enables controlled power consumption, resulting in a deterministic network lifetime. This predictability lowers maintenance costs by reducing the need for node replacement. Furthermore, our proposed method prevents sensor nodes from disconnecting from the network by designating the sensor with the highest charge as an intermediary and using single-hop routing. This approach improves the energy efficiency and stability of Wireless Sensor Network (WSN) deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12707v1</guid>
      <category>cs.NI</category>
      <category>cs.MA</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehrshad Eskandarpour, Saba Pirahmadian, Parham Soltani, Hossein Soleimani</dc:creator>
    </item>
    <item>
      <title>Towards Nomadic 6G Communication Networks: Implications on Architecture, Standardization, and Regulatory Aspects</title>
      <link>https://arxiv.org/abs/2508.12710</link>
      <description>arXiv:2508.12710v1 Announce Type: new 
Abstract: The emergence of nomadic mobile communication networks for sixth-generation (6G) introduces a paradigm shift in how network infrastructure is conceptualized, deployed, and operated. Unlike traditional fixed systems, Nomadic Networks (NNs) consist of mobile and self-organizing nodes that provide radio infrastructure capabilities in motion. This paper explores the architectural implications of such systems, with a particular focus on the design and evolution of network interfaces. We analyze the requirements for inter-node communication, service discovery, and control delegation in dynamic environments. Furthermore, we examine the regulatory and licensing challenges that arise when infrastructure elements traverse jurisdictional boundaries. Based on current 6G visions and relevant research, we identify limitations in existing architectures and propose a set of interface principles tailored to nomadicity. By synthesizing findings from mobile, non-terrestrial, and organic network domains, this work contributes to the architectural foundation for future nomadic 6G communication systems and outlines directions for interface standardization in decentralized, mobile infrastructures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12710v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Daniel Lindenschmitt, Marcos Rates Crippa, Hans D. Schotten</dc:creator>
    </item>
    <item>
      <title>Cooperative Sensing-Assisted Predictive Beam Tracking for MIMO-OFDM Networked ISAC Systems</title>
      <link>https://arxiv.org/abs/2508.12723</link>
      <description>arXiv:2508.12723v1 Announce Type: new 
Abstract: This paper studies a multiple-input multiple-output (MIMO) orthogonal frequency division multiplexing (OFDM) networked integrated sensing and communication (ISAC) system, in which multiple base stations (BSs) perform beam tracking to communicate with a mobile device. In particular, we focus on the beam tracking over a number of tracking time slots (TTSs) and suppose that these BSs operate at non-overlapping frequency bands to avoid the severe inter-cell interference. Under this setup, we propose a new cooperative sensing-assisted predictive beam tracking design. In each TTS, the BSs use echo signals to cooperatively track the mobile device as a sensing target, and continuously adjust the beam directions to follow the device for enhancing the performance for both communication and sensing. First, we propose a cooperative sensing design to track the device, in which the BSs first employ the two-dimensional discrete Fourier transform (2D-DFT) technique to perform local target estimation, and then use the extended Kalman filter (EKF) method to fuse their individual measurement results for predicting the target parameters. Next, based on the predicted results, we obtain the achievable rate for communication and the predicted conditional Cram\'er-Rao lower bound (PC-CRLB) for target parameters estimation in the next TTS, as a function of the beamforming vectors. Accordingly, we formulate the predictive beamforming design problem, with the objective of maximizing the achievable communication rate in the following TTS, while satisfying the PC-CRLB requirement for sensing. To address the resulting non-convex problem, we first propose a semi-definite relaxation (SDR)-based algorithm to obtain the optimal solution, and then develop an alternative penalty-based algorithm to get a high-quality low-complexity solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12723v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Yang, Zhiqing Wei, Jie Xu, Huici Wu, Zhiyong Feng</dc:creator>
    </item>
    <item>
      <title>Some optimization possibilities in data plane programming</title>
      <link>https://arxiv.org/abs/2508.12767</link>
      <description>arXiv:2508.12767v1 Announce Type: new 
Abstract: Software-defined networking (SDN) technology aims to create a highly flexible network by decoupling control plane and the data plane and programming them independently. There has been a lot of research on improving and optimizing the control plane, and data plane programming is a relatively new concept, so study on it is one of the hot topics for researchers. At the 2019 Dagstuhl Seminar, well-known scientists on computer networking discussed challenges and problems in the field of data plane programming that need to be addressed over the next 10 years. Based on this seminar issues and papers review, we suggested some possible solutions which are for optimizing data plane to improve packet processing performance and link utilization. The suggestions include (i) enriching data plane language with asynchronous external function, (ii) compression based on payload size, (iii) in-network caching for fast packet processing, and (iv) offloading external functions to an additional thread, virtual machine (VM) or server, etc. In addition, we implemented some of these in the P4 data plane language to illustrate the practicality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12767v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Altangerel Gereltsetseg, Tejfel M\'at\'e</dc:creator>
    </item>
    <item>
      <title>SDAP-based QoS Flow Multiplexing Support in Simu5G for 5G NR Simulation</title>
      <link>https://arxiv.org/abs/2508.12785</link>
      <description>arXiv:2508.12785v1 Announce Type: new 
Abstract: The Service Data Adaptation Protocol (SDAP) plays a central role in 5G New Radio (NR), acting as a bridge between the core and radio networks, by enabling QoS Flow multiplexing over shared Data Radio Bearers (DRBs). However, most 5G simulation frameworks, including the popular OMNet++-based Simu5G, lack SDAP support, limiting their ability to model realistic QoS behavior. This paper presents a modular, standardscompliant SDAP extension for Simu5G. The implementation includes core elements such as QoS Flow Identifer (QFI) flow tagging, SDAP header insertion/removal, and configurable logical DRB mapping. The proposed design supports multi-QFI simulation scenarios and enables researchers to model differentiated QoS flows and flowaware scheduling policies. Validation results confirm correct SDAP behavior and pave the way for advanced 5G simulations involving per-flow isolation, latency-sensitive traffic, and industrial QoS profiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12785v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohamed Seliem, Utz Roedig, Cormac Sreenan, Dirk Pesch</dc:creator>
    </item>
    <item>
      <title>RoTO: Robust Topology Obfuscation Against Tomography Inference Attacks</title>
      <link>https://arxiv.org/abs/2508.12852</link>
      <description>arXiv:2508.12852v1 Announce Type: new 
Abstract: Tomography inference attacks aim to reconstruct network topology by analyzing end-to-end probe delays. Existing defenses mitigate these attacks by manipulating probe delays to mislead inference, but rely on two strong assumptions: (i) probe packets can be perfectly detected and altered, and (ii) attackers use known, fixed inference algorithms. These assumptions often break in practice, leading to degraded defense performance under detection errors or adaptive adversaries. We present RoTO, a robust topology obfuscation scheme that eliminates both assumptions by modeling uncertainty in attacker-observed delays through a distributional formulation. RoTO casts the defense objective as a min-max optimization problem that maximizes expected topological distortion across this uncertainty set, without relying on perfect probe control or specific attacker models. To approximate attacker behavior, RoTO leverages graph neural networks for inference simulation and adversarial training. We also derive an upper bound on attacker success probability, and demonstrate that our approach enhances topology obfuscation performance through the optimization of this upper bound. Experimental results show that RoTO outperforms existing defense methods, achieving average improvements of 34% in structural similarity and 42.6% in link distance while maintaining strong robustness and concealment capabilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12852v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengze Du, Heng Xu, Zhiwei Yu, Ying Zhou, Zili Meng, Jialong Li</dc:creator>
    </item>
    <item>
      <title>REACH: Reinforcement Learning for Efficient Allocation in Community and Heterogeneous Networks</title>
      <link>https://arxiv.org/abs/2508.12857</link>
      <description>arXiv:2508.12857v1 Announce Type: new 
Abstract: Community GPU platforms are emerging as a cost-effective and democratized alternative to centralized GPU clusters for AI workloads, aggregating idle consumer GPUs from globally distributed and heterogeneous environments. However, their extreme hardware/software diversity, volatile availability, and variable network conditions render traditional schedulers ineffective, leading to suboptimal task completion. In this work, we present REACH (Reinforcement Learning for Efficient Allocation in Community and Heterogeneous Networks), a Transformer-based reinforcement learning framework that redefines task scheduling as a sequence scoring problem to balance performance, reliability, cost, and network efficiency. By modeling both global GPU states and task requirements, REACH learns to adaptively co-locate computation with data, prioritize critical jobs, and mitigate the impact of unreliable resources. Extensive simulation results show that REACH improves task completion rates by up to 17%, more than doubles the success rate for high-priority tasks, and reduces bandwidth penalties by over 80% compared to state-of-the-art baselines. Stress tests further demonstrate its robustness to GPU churn and network congestion, while scalability experiments confirm its effectiveness in large-scale, high-contention scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12857v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiwei Yu, Chengze Du, Heng Xu, Ying Zhou, Bo Liu, Jialong Li</dc:creator>
    </item>
    <item>
      <title>Neural Gaussian Radio Fields for Channel Estimation</title>
      <link>https://arxiv.org/abs/2508.11668</link>
      <description>arXiv:2508.11668v1 Announce Type: cross 
Abstract: Accurate channel state information (CSI) remains the most critical bottleneck in modern wireless networks, with pilot overhead consuming up to 11-21% of transmission bandwidth, increasing latency by 20-40% in massive MIMO systems, and reducing potential spectral efficiency by over 53%. Traditional estimation techniques fundamentally fail under mobility, with feedback delays as small as 4 ms causing 50% throughput degradation at even modest speeds (30 km/h). We present neural Gaussian radio fields (nGRF), a novel framework that leverages explicit 3D Gaussian primitives to synthesize complex channel matrices accurately and efficiently. Unlike NeRF-based approaches that rely on slow implicit representations or existing Gaussian splatting methods that use non-physical 2D projections, nGRF performs direct 3D electromagnetic field aggregation, with each Gaussian acting as a localized radio modulator. nGRF demonstrates superior performance across diverse environments: in indoor scenarios, it achieves a 10.9$\times$ higher prediction SNR than state of the art methods while reducing inference latency from 242 ms to just 1.1 ms (a 220$\times$ speedup). For large-scale outdoor environments, where existing approaches fail to function, nGRF achieves an SNR of 26.2 dB. Moreover, nGRF requires only 0.011 measurements per cubic foot compared to 0.2-178.1 for existing methods, thereby reducing data collection burden by 18$\times$. Training time is similarly reduced from hours to minutes (a 180$\times$ reduction), enabling rapid adaptation to dynamic environments. The code and datasets are available at: https://github.com/anonym-auth/n-grf</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11668v1</guid>
      <category>eess.SP</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muhammad Umer, Muhammad Ahmed Mohsin, Ahsan Bilal, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Assessing User Privacy Leakage in Synthetic Packet Traces: An Attack-Grounded Approach</title>
      <link>https://arxiv.org/abs/2508.11742</link>
      <description>arXiv:2508.11742v1 Announce Type: cross 
Abstract: Current synthetic traffic generators (SynNetGens) promise privacy but lack comprehensive guarantees or empirical validation, even as their fidelity steadily improves. We introduce the first attack-grounded benchmark for assessing the privacy of SynNetGens directly from the traffic they produce. We frame privacy as membership inference at the traffic-source level--a realistic and actionable threat for data holders. To this end, we present TraceBleed, the first attack that exploits behavioral fingerprints across flows using contrastive learning and temporal chunking, outperforming prior membership inference baselines by 172%. Our large-scale study across GAN-, diffusion-, and GPT-based SynNetGens uncovers critical insights: (i) SynNetGens leak user-level information; (ii) differential privacy either fails to stop these attacks or severely degrades fidelity; and (iii) sharing more synthetic data amplifies leakage by 59% on average. Finally, we introduce TracePatch, the first SynNetGen-agnostic defense that combines adversarial ML with SMT constraints to mitigate leakage while preserving fidelity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11742v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Minhao Jin, Hongyu He, Maria Apostolaki</dc:creator>
    </item>
    <item>
      <title>Securing Sideways: Thwarting Lateral Movement by Implementing Active Directory Tiering</title>
      <link>https://arxiv.org/abs/2508.11812</link>
      <description>arXiv:2508.11812v1 Announce Type: cross 
Abstract: The advancement of computing equipment and the advances in services over the Internet has allowed corporations, higher education, and many other organizations to pursue the shared computing network environment. A requirement for shared computing environments is a centralized identity system to authenticate and authorize user access. An organization's digital identity plane is a prime target for cyber threat actors. When compromised, identities can be exploited to steal credentials, create unauthorized accounts, and manipulate permissions-enabling attackers to gain control of the network and undermine its confidentiality, availability, and integrity. Cybercrime losses reached a record of 16.6 B in the United States in 2024. For organizations using Microsoft software, Active Directory is the on-premises identity system of choice. In this article, we examine the challenge of security compromises in Active Directory (AD) environments and present effective strategies to prevent credential theft and limit lateral movement by threat actors. Our proposed approaches aim to confine the movement of compromised credentials, preventing significant privilege escalation and theft. We argue that through our illustration of real-world scenarios, tiering can halt lateral movement and advanced cyber-attacks, thus reducing ransom escalation. Our work bridges a gap in existing literature by combining technical guidelines with theoretical arguments in support of tiering, positioning it as a vital component of modern cybersecurity strategy even though it cannot function in isolation. As the hardware advances and the cloud sourced services along with AI is advancing with unprecedented speed, we think it is important for security experts and the business to work together and start designing and developing software and frameworks to classify devices automatically and accurately within the tiered structure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11812v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tyler Schroder, Sohee Kim Park</dc:creator>
    </item>
    <item>
      <title>Stochastic Modeling of Filtration with Sieving in Graded Pore Networks</title>
      <link>https://arxiv.org/abs/2508.11820</link>
      <description>arXiv:2508.11820v1 Announce Type: cross 
Abstract: We model filtration of a feed solution, containing both small and large foulant particles, by a membrane filter. The membrane interior is modeled as a network of pores, allowing for the simultaneous adsorption of small particles and sieving of large particles, two fouling mechanisms typically observed during the early stages of commercial filtration applications. In our model, first-principles continuum partial differential equations model transport of the small particles and adsorptive fouling in each pore, while sieving particles are assumed to follow a discrete Poisson arrival process with a biased random walk through the pore network. Our goals are to understand the relative influences of each fouling mode and highlight the effect of their coupling on the performance of filters with a pore-size gradient (specifically, we consider a banded filter with different pore sizes in each band). Our results suggest that, due to the discrete nature of pore blockage, sieving alters qualitatively the rate of the flux decline. Moreover, the difference between sieving particle sizes and the initial pore size (radius) in each band plays a crucial role in indicating the onset and disappearance of sieving-adsorption competition. Lastly, we demonstrate a phase transition in the filter lifetime as the arrival frequency of sieving particles increases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11820v1</guid>
      <category>physics.flu-dyn</category>
      <category>cs.NA</category>
      <category>cs.NI</category>
      <category>math.NA</category>
      <category>math.PR</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binan Gu, Pejman Sanaei, Lou Kondic, Linda J. Cummings</dc:creator>
    </item>
    <item>
      <title>On Balancing Sparsity with Reliable Connectivity in Distributed Network Design with Random K-out Graphs</title>
      <link>https://arxiv.org/abs/2508.11863</link>
      <description>arXiv:2508.11863v1 Announce Type: cross 
Abstract: In several applications in distributed systems, an important design criterion is ensuring that the network is sparse, i.e., does not contain too many edges, while achieving reliable connectivity. Sparsity ensures communication overhead remains low, while reliable connectivity is tied to reliable communication and inference on decentralized data reservoirs and computational resources. A class of network models called random K-out graphs appear widely as a heuristic to balance connectivity and sparsity, especially in settings with limited trust, e.g., privacy-preserving aggregation of networked data in which networks are deployed. However, several questions remain regarding how to choose network parameters in response to different operational requirements, including the need to go beyond asymptotic results and the ability to model the stochastic and adversarial environments. To address this gap, we present theorems to inform the choice of network parameters that guarantee reliable connectivity in regimes where nodes can be finite or unreliable. We first derive upper and lower bounds for probability of connectivity in random K-out graphs when the number of nodes is finite. Next, we analyze the property of r-robustness, a stronger notion than connectivity that enables resilient consensus in the presence of malicious nodes. Finally, motivated by aggregation mechanisms based on pairwise masking, we model and analyze the impact of a subset of adversarial nodes, modeled as deletions, on connectivity and giant component size - metrics that are closely tied to privacy guarantees. Together, our results pave the way for end-to-end performance guarantees for a suite of algorithms for reliable inference on networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.11863v1</guid>
      <category>cs.SI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <category>math.OC</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mansi Sood, Eray Can Elumar, Osman Yagan</dc:creator>
    </item>
    <item>
      <title>ATLAS: AI-Native Receiver Test-and-Measurement by Leveraging AI-Guided Search</title>
      <link>https://arxiv.org/abs/2508.12204</link>
      <description>arXiv:2508.12204v1 Announce Type: cross 
Abstract: Industry adoption of Artificial Intelligence (AI)-native wireless receivers, or even modular, Machine Learning (ML)-aided wireless signal processing blocks, has been slow. The main concern is the lack of explainability of these trained ML models and the significant risks posed to network functionalities in case of failures, especially since (i) testing on every exhaustive case is infeasible and (ii) the data used for model training may not be available. This paper proposes ATLAS, an AI-guided approach that generates a battery of tests for pre-trained AI-native receiver models and benchmarks the performance against a classical receiver architecture. Using gradient-based optimization, it avoids spanning the exhaustive set of all environment and channel conditions; instead, it generates the next test in an online manner to further probe specific configurations that offer the highest risk of failure. We implement and validate our approach by adopting the well-known DeepRx AI-native receiver model as well as a classical receiver using differentiable tensors in NVIDIA's Sionna environment. ATLAS uncovers specific combinations of mobility, channel delay spread, and noise, where fully and partially trained variants of AI-native DeepRx perform suboptimally compared to the classical receivers. Our proposed method reduces the number of tests required per failure found by 19% compared to grid search for a 3-parameters input optimization problem, demonstrating greater efficiency. In contrast, the computational cost of the grid-based approach scales exponentially with the number of variables, making it increasingly impractical for high-dimensional problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12204v1</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mauro Belgiovine, Suyash Pradhan, Johannes Lange, Michael L\"ohning, Kaushik Chowdhury</dc:creator>
    </item>
    <item>
      <title>Cold-RL: Learning Cache Eviction with Offline Reinforcement Learning for NGINX</title>
      <link>https://arxiv.org/abs/2508.12485</link>
      <description>arXiv:2508.12485v1 Announce Type: cross 
Abstract: Web proxies such as NGINX commonly rely on least-recently-used (LRU) eviction, which is size agnostic and can thrash under periodic bursts and mixed object sizes. We introduce Cold-RL, a learned eviction policy for NGINX that replaces LRU's forced-expire path with a dueling Deep Q-Network served by an ONNX sidecar within a strict microsecond budget. On each eviction, Cold-RL samples the K least-recently-used objects, extracts six lightweight features (age, size, hit count, inter-arrival time, remaining TTL, and last origin RTT), and requests a bitmask of victims; a hard timeout of 500 microseconds triggers immediate fallback to native LRU. Policies are trained offline by replaying NGINX access logs through a cache simulator with a simple reward: a retained object earns one point if it is hit again before TTL expiry. We compare against LRU, LFU, size-based, adaptive LRU, and a hybrid baseline on two adversarial workloads. With a 25 MB cache, Cold-RL raises hit ratio from 0.1436 to 0.3538, a 146 percent improvement over the best classical baseline; at 100 MB, from 0.7530 to 0.8675, a 15 percent gain; and at 400 MB it matches classical methods (about 0.918). Inference adds less than 2 percent CPU overhead and keeps 95th percentile eviction latency within budget. To our knowledge, this is the first reinforcement learning eviction policy integrated into NGINX with strict SLOs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12485v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aayush Gupta, Arpit Bhayani</dc:creator>
    </item>
    <item>
      <title>ChamaleoNet: Programmable Passive Probe for Enhanced Visibility on Erroneous Traffic</title>
      <link>https://arxiv.org/abs/2508.12496</link>
      <description>arXiv:2508.12496v1 Announce Type: cross 
Abstract: Traffic visibility remains a key component for management and security operations. Observing unsolicited and erroneous traffic, such as unanswered traffic or errors, is fundamental to detect misconfiguration, temporary failures or attacks. ChamaleoNet transforms any production network into a transparent monitor to let administrators collect unsolicited and erroneous traffic directed to hosts, whether offline or active, hosting a server or a client, protected by a firewall, or unused addresses. ChamaleoNet is programmed to ignore well-formed traffic and collect only erroneous packets, including those generated by misconfigured or infected internal hosts, and those sent by external actors which scan for services. Engineering such a system poses several challenges, from scalability to privacy. Leveraging the SDN paradigm, ChamaleoNet processes the traffic flowing through a campus/corporate network and focuses on erroneous packets only, lowering the pressure on the collection system while respecting privacy regulations by design. ChamaleoNet enables the seamless integration with active deceptive systems like honeypots that can impersonate unused hosts/ports/services and engage with senders. The SDN in-hardware filtering reduces the traffic to the controller by 96%, resulting in a scalable solution, which we offer as open source. Simple analytics unveil internal misconfigured and infected hosts, identify temporary failures, and enhance visibility on external radiation produced by attackers looking for vulnerable services.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12496v1</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhihao Wang, Alessandro Cornacchia, Andrea Bianco, Idilio Drago, Paolo Giaccone, Dingde Jiang, Marco Mellia</dc:creator>
    </item>
    <item>
      <title>SL-ACC: A Communication-Efficient Split Learning Framework with Adaptive Channel-wise Compression</title>
      <link>https://arxiv.org/abs/2508.12984</link>
      <description>arXiv:2508.12984v1 Announce Type: cross 
Abstract: The increasing complexity of neural networks poses a significant barrier to the deployment of distributed machine learning (ML) on resource-constrained devices, such as federated learning (FL). Split learning (SL) offers a promising solution by offloading the primary computing load from edge devices to a server via model partitioning. However, as the number of participating devices increases, the transmission of excessive smashed data (i.e., activations and gradients) becomes a major bottleneck for SL, slowing down the model training. To tackle this challenge, we propose a communication-efficient SL framework, named SL-ACC, which comprises two key components: adaptive channel importance identification (ACII) and channel grouping compression (CGC). ACII first identifies the contribution of each channel in the smashed data to model training using Shannon entropy. Following this, CGC groups the channels based on their entropy and performs group-wise adaptive compression to shrink the transmission volume without compromising training accuracy. Extensive experiments across various datasets validate that our proposed SL-ACC framework takes considerably less time to achieve a target accuracy than state-of-the-art benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.12984v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.NI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zehang Lin, Zheng Lin, Miao Yang, Jianhao Huang, Yuxin Zhang, Zihan Fang, Xia Du, Zhe Chen, Shunzhi Zhu, Wei Ni</dc:creator>
    </item>
    <item>
      <title>INSIGHT: A Survey of In-Network Systems for Intelligent, High-Efficiency AI and Topology Optimization</title>
      <link>https://arxiv.org/abs/2505.24269</link>
      <description>arXiv:2505.24269v2 Announce Type: replace 
Abstract: In-network computation represents a transformative approach to addressing the escalating demands of Artificial Intelligence (AI) workloads on network infrastructure. By leveraging the processing capabilities of network devices such as switches, routers, and Network Interface Cards (NICs), this paradigm enables AI computations to be performed directly within the network fabric, significantly reducing latency, enhancing throughput, and optimizing resource utilization. This paper provides a comprehensive analysis of optimizing in-network computation for AI, exploring the evolution of programmable network architectures, such as Software-Defined Networking (SDN) and Programmable Data Planes (PDPs), and their convergence with AI. It examines methodologies for mapping AI models onto resource-constrained network devices, addressing challenges like limited memory and computational capabilities through efficient algorithm design and model compression techniques. The paper also highlights advancements in distributed learning, particularly in-network aggregation, and the potential of federated learning to enhance privacy and scalability. Frameworks like Planter and Quark are discussed for simplifying development, alongside key applications such as intelligent network monitoring, intrusion detection, traffic management, and Edge AI. Future research directions, including runtime programmability, standardized benchmarks, and new applications paradigms, are proposed to advance this rapidly evolving field. This survey underscores the potential of in-network AI to create intelligent, efficient, and responsive networks capable of meeting the demands of next-generation AI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24269v2</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aleksandr Algazinov, Joydeep Chandra, Matt Laing</dc:creator>
    </item>
    <item>
      <title>Multimodal Remote Inference</title>
      <link>https://arxiv.org/abs/2508.07555</link>
      <description>arXiv:2508.07555v2 Announce Type: replace-cross 
Abstract: We consider a remote inference system with multiple modalities, where a multimodal machine learning (ML) model performs real-time inference using features collected from remote sensors. When sensor observations evolve dynamically over time, fresh features are critical for inference tasks. However, timely delivery of features from all modalities is often infeasible because of limited network resources. Towards this end, in this paper, we study a two-modality scheduling problem that seeks to minimize the ML model's inference error, expressed as a penalty function of the Age of Information (AoI) vector of the two modalities. We develop an index-based threshold policy and prove its optimality. Specifically, the scheduler switches to the other modality once the current modality's index function exceeds a predetermined threshold. We show that both modalities share the same threshold and that the index functions and the threshold can be computed efficiently. Our optimality results hold for general AoI functions (which could be non-monotonic and non-separable) and heterogeneous transmission times across modalities. To demonstrate the importance of considering a task-oriented AoI function, we conduct numerical experiments based on robot state prediction and compare our policy with round-robin and uniform random policies (both are oblivious to the AoI and the inference error).n The results show that our policy reduces inference error by up to 55% compared with these baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.07555v2</guid>
      <category>cs.LG</category>
      <category>cs.IT</category>
      <category>cs.NI</category>
      <category>math.IT</category>
      <pubDate>Tue, 19 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Keyuan Zhang, Yin Sun, Bo Ji</dc:creator>
    </item>
  </channel>
</rss>
