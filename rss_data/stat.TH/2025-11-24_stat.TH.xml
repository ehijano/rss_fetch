<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 24 Nov 2025 05:01:10 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CBMA: Improving conformal prediction through Bayesian model averaging</title>
      <link>https://arxiv.org/abs/2511.16924</link>
      <description>arXiv:2511.16924v1 Announce Type: new 
Abstract: Conformal prediction has emerged as a popular technique for facilitating valid predictive inference across a spectrum of machine learning models, under minimal assumption of exchangeability. Recently, Hoff (2023) showed that full conformal Bayes provides the most efficient prediction sets (smallest by expected volume) among all prediction sets that are valid at the $(1 - \alpha)$ level if the model is correctly specified. However, a critical issue arises when the Bayesian model itself may be mis-specified, resulting in prediction set that might be suboptimal, even though it still enjoys the frequentist coverage guarantee. To address this limitation, we propose an innovative solution that combines Bayesian model averaging (BMA) with conformal prediction. This hybrid not only leverages the strengths of Bayesian conformal prediction but also introduces a layer of robustness through model averaging. Theoretically, we prove that the resulting prediction set will converge to the optimal level of efficiency, if the true model is included among the candidate models. This assurance of optimality, even under potential model uncertainty, provides a significant improvement over existing methods, ensuring more reliable and precise uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16924v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Pankaj Bhagwat, Linglong Kong, Bei Jiang</dc:creator>
    </item>
    <item>
      <title>On a synergistic learning phenomenon in nonparametric domain adaptation</title>
      <link>https://arxiv.org/abs/2511.17009</link>
      <description>arXiv:2511.17009v1 Announce Type: new 
Abstract: Consider nonparametric domain adaptation for regression, which assumes the same conditional distribution of the response given the covariates but different marginal distributions of the covariates. An important goal is to understand how the source data may improve the minimax convergence rate of learning the regression function when the likelihood ratio of the covariate marginal distributions of the target data and the source data are unbounded. A previous work of Pathak et al. (2022) show that the minimax transfer learning rate is simply determined by the faster rate of using either the source or the target data alone. In this paper, we present a new synergistic learning phenomenon (SLP) that the minimax convergence rate based on both data may sometimes be faster (even much faster) than the better rate of convergence based on the source or target data only. The SLP occurs when and only when the target sample size is smaller (in order) than but not too much smaller than the source sample size in relation to the smoothness of the regression function and the nature of the covariate densities of the source and target distributions. Interestingly, the SLP happens in two different ways according to the relationship between the two sample sizes. One is that the target data help alleviate the difficulty in estimating the regression function at points where the density of the source data is close to zero and the other is that the source data (with its larger sample size than that of the target data) help the estimation at points where the density of the source data is not small. Extensions to handle unknown source and target parameters and smoothness of the regression function are also obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17009v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ling Zhou, Yuhong Yang</dc:creator>
    </item>
    <item>
      <title>Differentially private testing for relevant dependencies in high dimensions</title>
      <link>https://arxiv.org/abs/2511.17167</link>
      <description>arXiv:2511.17167v1 Announce Type: new 
Abstract: We investigate the problem of detecting dependencies between the components of a high-dimensional vector. Our approach advances the existing literature in two important respects. First, we consider the problem under privacy constraints. Second, instead of testing whether the coordinates are pairwise independent, we are interested in determining whether certain pairwise associations between the components (such as all pairwise Kendall's $\tau$ coefficients) do not exceed a given threshold in absolute value. Considering hypotheses of this form is motivated by the observation that in the high-dimensional regime, it is rare and perhaps impossible to have a null hypothesis that can be modeled exactly by assuming that all pairwise associations are precisely equal to zero.
  The formulation of the null hypothesis as a composite hypothesis makes the problem of constructing tests already non-standard in the non-private setting. Additionally, under privacy constraints, state of the art procedures rely on permutation approaches that are rendered invalid under a composite null. We propose a novel bootstrap based methodology that is especially powerful in sparse settings, develop theoretical guarantees under mild assumptions and show that the proposed method enjoys good finite sample properties even in the high privacy regime. Additionally, we present applications in medical data that showcase the applicability of our methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17167v1</guid>
      <category>math.ST</category>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrick Bastian, Holger Dette, Martin Dunsche</dc:creator>
    </item>
    <item>
      <title>Properties of stepwise parameter estimation in high-dimensional vine copulas</title>
      <link>https://arxiv.org/abs/2511.17291</link>
      <description>arXiv:2511.17291v1 Announce Type: new 
Abstract: The increasing use of vine copulas in high-dimensional settings, where the number of parameters is often of the same order as the sample size, calls for asymptotic theory beyond the traditional fixed-$p$, large-$n$ framework. We establish consistency and asymptotic normality of the stepwise maximum likelihood estimator for vine copulas when the number of parameters diverges as $n \to \infty$. Our theoretical results cover both parametric and nonparametric estimation of the marginal distributions, as well as truncated vines, and are also applicable to general estimation problems, particularly other sequential procedures. Numerical experiments suggest that the derived assumptions are satisfied if the pair copulas in higher trees converge to independence copulas sufficiently fast. A simulation study substantiates these findings and identifies settings in which estimation becomes challenging. In particular, the vine structure strongly affects estimation accuracy, with D-vines being more difficult to estimate than C-vines, and estimates in Gumbel vines exhibit substantially larger biases than those in Gaussian vines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17291v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jana Gauss, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Background on real and complex elliptically symmetric distributions</title>
      <link>https://arxiv.org/abs/2511.17394</link>
      <description>arXiv:2511.17394v1 Announce Type: new 
Abstract: This chapter presents a short overview of real elliptically symmetric (RES) distributions, complemented by circular complex elliptically symmetric (C-CES) and noncircular CES (NC-CES) distributions as complex representations of RES distributions. These distributions are both an extension of the multivariate Gaussian distribution and a multivariate extension of univariate symmetric distributions. They are equivalently defined through their characteristic functions and their stochastic representations, which naturally follow from the spherically symmetric distributions after affine transformations. Particular attention is paid to the absolutely continuous case and to the subclass of compound Gaussian distributions. Results related to moments, affine transformations, marginal and conditional distributions, and summation stability are also presented. Some well-known instances of RES distributions are provided with their main properties. Finally, the estimation of the symmetry center and scatter matrix is briefly discussed through the sample mean (SM), sample covariance matrix (SCM) estimate, maximum estimate (ML), $M$-estimators, and Tyler's $M$-estimators. Particular attention will be paid to the asymptotic Gaussianity of the $M$-estimators of the scatter matrix. To conclude, some hints about the Slepian-Bangs formula are provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17394v1</guid>
      <category>math.ST</category>
      <category>eess.SP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Pierre Delmas</dc:creator>
    </item>
    <item>
      <title>Approximate Least-Favorable Distributions and Nearly Optimal Tests via Stochastic Mirror Descent</title>
      <link>https://arxiv.org/abs/2511.16925</link>
      <description>arXiv:2511.16925v1 Announce Type: cross 
Abstract: We consider a class of hypothesis testing problems where the null hypothesis postulates $M$ distributions for the observed data, and there is only one possible distribution under the alternative. We show that one can use a stochastic mirror descent routine for convex optimization to provably obtain - after finitely many iterations - both an approximate least-favorable distribution and a nearly optimal test, in a sense we make precise. Our theoretical results yield concrete recommendations about the algorithm's implementation, including its initial condition, its step size, and the number of iterations. Importantly, our suggested algorithm can be viewed as a slight variation of the algorithm suggested by Elliott, M\"uller, and Watson (2015), whose theoretical performance guarantees are unknown.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16925v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andr\'es Aradillas Fern\'andez, Jos\'e Blanchet, Jos\'e Luis Montiel Olea, Chen Qiu, J\"org Stoye, Lezhi Tan</dc:creator>
    </item>
    <item>
      <title>Gradient flow for deep equilibrium single-index models</title>
      <link>https://arxiv.org/abs/2511.16976</link>
      <description>arXiv:2511.16976v1 Announce Type: cross 
Abstract: Deep equilibrium models (DEQs) have recently emerged as a powerful paradigm for training infinitely deep weight-tied neural networks that achieve state of the art performance across many modern machine learning tasks. Despite their practical success, theoretically understanding the gradient descent dynamics for training DEQs remains an area of active research. In this work, we rigorously study the gradient descent dynamics for DEQs in the simple setting of linear models and single-index models, filling several gaps in the literature. We prove a conservation law for linear DEQs which implies that the parameters remain trapped on spheres during training and use this property to show that gradient flow remains well-conditioned for all time. We then prove linear convergence of gradient descent to a global minimizer for linear DEQs and deep equilibrium single-index models under appropriate initialization and with a sufficiently small step size. Finally, we validate our theoretical findings through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16976v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjit Dandapanthula, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>On treating right-censoring events like treatments</title>
      <link>https://arxiv.org/abs/2511.17379</link>
      <description>arXiv:2511.17379v1 Announce Type: cross 
Abstract: In causal inference literature, potential outcomes are often indexed by the "elimination of all right-censoring events," leading to the perception that such a restriction is necessary for defining well-posed causal estimands. In this paper, we clarify that this restriction is not required: a well-defined estimand can be formulated without indexing on the elimination of such events. Achieving this requires a more precise classification of right-censoring events than has historically been considered, as the nature of these events has direct implications for identification of the target estimand. We provide a framework that distinguishes different types of right-censoring events from a causal perspective, and demonstrate how this framework relates to censoring definitions and assumptions in classical survival analysis literature. By bridging these perspectives, we provide a clearer understanding of how to handle right-censoring events and provide guidance for identifying causal estimands when right-censored events are present.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17379v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lan Wen, Aaron L. Sarvet, Jessica G. Young</dc:creator>
    </item>
    <item>
      <title>Iterating marginalized Bayes maps for likelihood maximization with application to nonlinear panel models</title>
      <link>https://arxiv.org/abs/2511.17438</link>
      <description>arXiv:2511.17438v1 Announce Type: cross 
Abstract: Complex dynamic systems can be investigated by fitting mechanistic stochastic dynamic models to time series data. In this context, commonly used Monte Carlo inference procedures for model selection and parameter estimation quickly become computationally unfeasible as the system dimension grows. The increasing prevalence of panel data, characterized by multiple related time series, therefore necessitates the development of inference algorithms that are effective for this class of high-dimensional mechanistic models. Nonlinear, non-Gaussian mechanistic models are routinely fitted to time series data but seldom to panel data, despite its widespread availability, suggesting that the practical difficulties for existing procedures are prohibitive. We investigate the use of iterated filtering algorithms for this purpose. We introduce a novel algorithm that contains a marginalization step that mitigates issues arising from particle filtering in high dimensions. Our approach enables likelihood-based inference for models that were previously considered intractable, thus broadening the scope of dynamic models available for panel data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17438v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jesse Wheeler, Aaron J. Abkemeier, Edward L. Ionides</dc:creator>
    </item>
    <item>
      <title>Extending the Accelerated Failure Conditionals Model to Location-Scale Families</title>
      <link>https://arxiv.org/abs/2511.17463</link>
      <description>arXiv:2511.17463v1 Announce Type: cross 
Abstract: Arnold and Arvanitis (2020) introduced a novel class of bivariate conditionally specified distributions, in which dependence between two random variables is established by defining the distribution of one variable conditional on the other. This conditioning regime was formulated through survival functions and termed the accelerated failure conditionals model. Subsequently, Lakhani (2025) extended this conditioning framework to encompass distributional families whose marginal densities may exhibit unimodality and skewness, thereby moving beyond families with non-increasing densities. The present study builds on this line of work by proposing a conditional survival specification derived from a location-scale distributional family, where the dependence between $X$ and $Y$ arises not only through the acceleration function but also via a location function. An illustrative example of this new specification is developed using a Weibull marginal for $X$. The resulting models are fully characterized by closed-form expressions for their moments, and simulations are implemented using the Metropolis-Hastings algorithm. Finally, the model is applied to a dataset in which the empirical distribution of $Y$ lies on the real line, demonstrating the models' capacity to accommodate $Y$ marginals defined over $\mathbb{R}$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.17463v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jared N. Lakhani</dc:creator>
    </item>
    <item>
      <title>Conditional Extreme Value Estimation for Dependent Time Series</title>
      <link>https://arxiv.org/abs/2503.22366</link>
      <description>arXiv:2503.22366v2 Announce Type: replace 
Abstract: We study the consistency and weak convergence of the conditional tail function and conditional Hill estimators under broad dependence assumptions for a heavy-tailed response sequence and a covariate sequence. Consistency is established under $\alpha$-mixing, while asymptotic normality follows from $\beta$-mixing and second-order conditions. A key aspect of our approach is its versatile functional formulation in terms of the conditional tail process. Simulations demonstrate its performance across dependence scenarios. We apply our method to extreme event modelling in the oil industry, revealing distinct tail behaviours under varying conditioning values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.22366v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martin Bladt, Laurits Glargaard, Theodor Henningsen</dc:creator>
    </item>
    <item>
      <title>Online selective conformal inference: adaptive scores, convergence rate and optimality</title>
      <link>https://arxiv.org/abs/2508.10336</link>
      <description>arXiv:2508.10336v2 Announce Type: replace 
Abstract: In a supervised online setting, quantifying uncertainty has been proposed in the seminal work of \cite{gibbs2021adaptive}. For any given point-prediction algorithm, their method (ACI) produces a conformal prediction set with an average missed coverage getting close to a pre-specified level $\alpha$ for a long time horizon. We introduce an extended version of this algorithm, called OnlineSCI, allowing the user to additionally select times where such an inference should be made. OnlineSCI encompasses several prominent online selective tasks, such as building prediction intervals for extreme outcomes, classification with abstention, and online testing. While OnlineSCI controls the average missed coverage on the selected in an adversarial setting, our theoretical results also show that it controls the instantaneous error rate (IER) at the selected times, up to a non-asymptotical remainder term. Importantly, our theory covers the case where OnlineSCI updates the point-prediction algorithm at each time step, a property which we refer to as {\it adaptive} capability. We show that the adaptive versions of OnlineSCI can convergence to an optimal solution and provide an explicit convergence rate in each of the aforementioned application cases, under specific mild conditions. Finally, the favorable behavior of OnlineSCI in practice is illustrated by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.10336v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pierre Humbert, Ulysse Gazin, Ruth Heller, Etienne Roquain</dc:creator>
    </item>
    <item>
      <title>On Minimax Estimation Problems for Periodically Correlated Stochastic Processes</title>
      <link>https://arxiv.org/abs/2510.16906</link>
      <description>arXiv:2510.16906v2 Announce Type: replace 
Abstract: The aim of this article is to overview the problem of mean square optimal estimation of linear functionals which depend on unknown values of periodically correlated stochastic process. Estimates are based on observations of this process and noise. These problems are investigated under conditions of spectral certainty and spectral uncertainty. Formulas for calculating the main characteristics (spectral characteristic, mean square error) of the optimal linear estimates of the functionals are proposed. The least favorable spectral densities and the minimax-robust spectral characteristics of optimal estimates of the functionals are presented for given sets of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16906v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iryna Dubovets'ka, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>New Empirical Process Tools and Their Applications to Robust Deep ReLU Networks and Phase Transitions for Nonparametric Regression</title>
      <link>https://arxiv.org/abs/2511.15841</link>
      <description>arXiv:2511.15841v2 Announce Type: replace 
Abstract: This paper introduces new empirical process tools for analyzing a broad class of statistical learning models under heavy-tailed noise and complex function classes. Our primary contribution is the derivation of two Dudley-type maximal inequalities for expected empirical processes that remove restrictive assumptions such as light tails and uniform boundedness of the function class. These inequalities enlarge the scope of empirical process theory available for statistical learning and nonparametric estimation. Exploiting the new bounds, we establish robustness guarantees for deep ReLU network estimators in Huber and quantile regression. In particular, we prove a unified non-asymptotic sub-Gaussian concentration bound that remains valid even under infinite-variance noise and provide a comprehensive analysis of non-asymptotic robustness for deep Huber estimators across all noise regimes. For deep quantile regression, we provide the first non-asymptotic sub-Gaussian bounds without requiring moment assumptions. As an additional application, our framework yields estimation error bounds for nonparametric least-squares estimators that simultaneously accommodate infinite-variance noise, non-Donsker function classes, and approximation error. Moreover, unlike prior approaches based on specialized multiplier processes, our framework extends to broader empirical risk minimization problems, including the nonparametric generalized linear models and the ``set-structured'' models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15841v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizhe Ding, Runze Li, Lingzhou Xue</dc:creator>
    </item>
    <item>
      <title>Minimax Statistical Estimation under Wasserstein Contamination</title>
      <link>https://arxiv.org/abs/2308.01853</link>
      <description>arXiv:2308.01853v3 Announce Type: replace-cross 
Abstract: Contaminations are a key concern in modern statistical learning, as small but systematic perturbations of all datapoints can substantially alter estimation results. Here, we study Wasserstein-$r$ contaminations ($r\ge 1$) in an $\ell_q$ norm ($q\in [1,\infty]$), in which each observation may undergo an adversarial perturbation with bounded cost, complementing the classical Huber model, corresponding to total variation norm, where only a fraction of observations is arbitrarily corrupted. We study both independent and joint (coordinated) contaminations and develop a minimax theory under $\ell_q^r$ losses.
  Our analysis encompasses several fundamental problems: location estimation, linear regression, and pointwise nonparametric density estimation. For joint contaminations in location estimation and for prediction in linear regression, we obtain the exact minimax risk, identify least favorable contaminations, and show that the sample mean and least squares predictor are respectively minimax optimal. For location estimation under independent contaminations, we give sharp upper and lower bounds, including exact minimaxity in the Euclidean Wasserstein contamination case, when $q=r=2$. For pointwise density estimation in any dimension, we derive the optimal rate, showing that it is achieved by kernel density estimation with a bandwidth that is possibly larger than the classical one.
  Our proofs leverage powerful tools from optimal transport developed over the last 20 years, including the dynamic Benamou-Brenier formulation. Taken together, our results suggest that in contrast to the Huber contamination model, for norm-based Wasserstein contaminations, classical estimators may be nearly optimally robust.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.01853v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Chao, Edgar Dobriban</dc:creator>
    </item>
    <item>
      <title>Manifolds with kinks and the asymptotic behavior of the graph Laplacian operator with Gaussian kernel</title>
      <link>https://arxiv.org/abs/2507.07751</link>
      <description>arXiv:2507.07751v3 Announce Type: replace-cross 
Abstract: We introduce manifolds with kinks, a class of manifolds with possibly singular boundary that notably contains manifolds with smooth boundary and corners. We derive the asymptotic behavior of the Graph Laplace operator with Gaussian kernel and its deterministic limit on these spaces as bandwidth goes to zero. We show that this asymptotic behavior is determined by the inward sector of the tangent space and, as special cases, we derive its behavior near interior and singular points. Lastly, we show the validity of our theoretical results using numerical simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07751v3</guid>
      <category>math.DG</category>
      <category>math.SP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Susovan Pal, David Tewodrose</dc:creator>
    </item>
    <item>
      <title>ResCP: Reservoir Conformal Prediction for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2510.05060</link>
      <description>arXiv:2510.05060v2 Announce Type: replace-cross 
Abstract: Conformal prediction offers a powerful framework for building distribution-free prediction intervals for exchangeable data. Existing methods that extend conformal prediction to sequential data rely on fitting a relatively complex model to capture temporal dependencies. However, these methods can fail if the sample size is small and often require expensive retraining when the underlying data distribution changes. To overcome these limitations, we propose Reservoir Conformal Prediction (ResCP), a novel training-free conformal prediction method for time series. Our approach leverages the efficiency and representation learning capabilities of reservoir computing to dynamically reweight conformity scores. In particular, we compute similarity scores among reservoir states and use them to adaptively reweight the observed residuals at each step. With this approach, ResCP enables us to account for local temporal dynamics when modeling the error distribution without compromising computational scalability. We prove that, under reasonable assumptions, ResCP achieves asymptotic conditional coverage, and we empirically demonstrate its effectiveness across diverse forecasting tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05060v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 24 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberto Neglia, Andrea Cini, Michael M. Bronstein, Filippo Maria Bianchi</dc:creator>
    </item>
  </channel>
</rss>
