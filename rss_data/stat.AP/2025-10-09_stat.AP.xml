<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Oct 2025 01:55:11 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Mixed-Methods Analysis of Repression and Mobilization in Bangladesh's July Revolution Using Machine Learning and Statistical Modeling</title>
      <link>https://arxiv.org/abs/2510.06264</link>
      <description>arXiv:2510.06264v1 Announce Type: new 
Abstract: The 2024 July Revolution in Bangladesh represents a landmark event in the study of civil resistance. This study investigates the central paradox of the success of this student-led civilian uprising: how state violence, intended to quell dissent, ultimately fueled the movement's victory. We employ a mixed-methods approach. First, we develop a qualitative narrative of the conflict's timeline to generate specific, testable hypotheses. Then, using a disaggregated, event-level dataset, we employ a multi-method quantitative analysis to dissect the complex relationship between repression and mobilisation. We provide a framework to analyse explosive modern uprisings like the July Revolution. Initial pooled regression models highlight the crucial role of protest momentum in sustaining the movement. To isolate causal effects, we specify a Two-Way Fixed Effects panel model, which provides robust evidence for a direct and statistically significant local suppression backfire effect. Our Vector Autoregression (VAR) analysis provides clear visual evidence of an immediate, nationwide mobilisation in response to increased lethal violence. We further demonstrate that this effect was non-linear. A structural break analysis reveals that the backfire dynamic was statistically insignificant in the conflict's early phase but was triggered by the catalytic moral shock of the first wave of lethal violence, and its visuals circulated around July 16th. A complementary machine learning analysis (XGBoost, out-of-sample R$^{2}$=0.65) corroborates this from a predictive standpoint, identifying "excessive force against protesters" as the single most dominant predictor of nationwide escalation. We conclude that the July Revolution was driven by a contingent, non-linear backfire, triggered by specific catalytic moral shocks and accelerated by the viral reaction to the visual spectacle of state brutality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06264v1</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md. Saiful Bari Siddiqui, Anupam Debashis Roy</dc:creator>
    </item>
    <item>
      <title>Estimating temporary emigration from capture-recapture data in the presence of latent identification</title>
      <link>https://arxiv.org/abs/2510.06755</link>
      <description>arXiv:2510.06755v1 Announce Type: new 
Abstract: Most capture-recapture models assume that individuals either do not emigrate or emigrate permanently from the sampling area during the sampling period. This assumption is violated when individuals temporarily leave the sampling area and return during later capture occasions, which can result in biased or less precise inferences under normal capture-recapture models. Existing temporary emigration models require that individuals are uniquely and correctly identified. To our knowledge, no studies to date have addressed temporary emigration in the presence of latent individual identification, which can arise in many scenarios such as misidentification, data integration, and batch marking. In this paper, we propose a new latent multinomial temporary emigration modelling framework for analysing capture-recapture data with latent identification. The framework is applicable to both closed- and open-population problems, accommodates data with or without individual identification, and flexibly incorporates different emigration processes, including the completely random and Markovian emigration. Through simulations, we demonstrate that model parameters can be reliably estimated in various emigration scenarios. We apply the proposed framework to a real dataset on golden mantella collected using batch marks under Pollock's robust design. The results show that accounting for temporary emigration provides a better fit to the data compared to the previous model without temporary emigration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06755v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Katarina Skopalova, Jafet Osuna, Wei Zhang</dc:creator>
    </item>
    <item>
      <title>Estimating Real Demand Using a Flipped Queueing Model: A Case of Shared Micro-Mobility Services</title>
      <link>https://arxiv.org/abs/2510.07194</link>
      <description>arXiv:2510.07194v1 Announce Type: new 
Abstract: The spatial-temporal imbalance between supply and demand in shared micro-mobility services often leads to observed demand being censored, resulting in incomplete records of the underlying real demand. This phenomenon undermines the reliability of the collected demand data and hampers downstream applications such as demand forecasting, fleet management, and micro-mobility planning. How to accurately estimate the real demand is challenging and has not been well explored in existing studies. In view of this, we contribute to real demand estimation for shared micro-mobility services by proposing an analytical method that rigorously derives the real demand under appropriate assumptions. Rather than directly modeling the intractable relationship between observed demand and real demand, we propose a novel random variable, Generalized Vehicle Survival Time (GVST), which is observable from trip records. The relationship between GVST and real demand is characterized by introducing a flipped queueing model (FQM) that captures the operational dynamics of shared micro-mobility services. Specifically, the distribution of GVST is derived within the FQM, which allows the real demand estimation problem to be transformed into an inverse queueing problem. We analytically derive the real demand in closed form using a one-sided estimation method, and solve the problem by a system of equations in a two-sided estimation method. We validate the proposed methods using synthetic data and conduct empirical analyses using real-world datasets from bike-sharing and shared e-scooter systems. The experimental results show that both the two-sided and one-sided methods outperform benchmark models. In particular, the one-sided approach provides a closed-form solution that delivers acceptable accuracy, constituting a practical rule of thumb for demand-related analytics and decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07194v1</guid>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binyu Yang, Jinxiao Du, Junlin He, Shi An, Wei Ma</dc:creator>
    </item>
    <item>
      <title>Scalable deep fusion of spaceborne lidar and synthetic aperture radar for global forest structural complexity mapping</title>
      <link>https://arxiv.org/abs/2510.06299</link>
      <description>arXiv:2510.06299v1 Announce Type: cross 
Abstract: Forest structural complexity metrics integrate multiple canopy attributes into a single value that reflects habitat quality and ecosystem function. Spaceborne lidar from the Global Ecosystem Dynamics Investigation (GEDI) has enabled mapping of structural complexity in temperate and tropical forests, but its sparse sampling limits continuous high-resolution mapping. We present a scalable, deep learning framework fusing GEDI observations with multimodal Synthetic Aperture Radar (SAR) datasets to produce global, high-resolution (25 m) wall-to-wall maps of forest structural complexity. Our adapted EfficientNetV2 architecture, trained on over 130 million GEDI footprints, achieves high performance (global R2 = 0.82) with fewer than 400,000 parameters, making it an accessible tool that enables researchers to process datasets at any scale without requiring specialized computing infrastructure. The model produces accurate predictions with calibrated uncertainty estimates across biomes and time periods, preserving fine-scale spatial patterns. It has been used to generate a global, multi-temporal dataset of forest structural complexity from 2015 to 2022. Through transfer learning, this framework can be extended to predict additional forest structural variables with minimal computational cost. This approach supports continuous, multi-temporal monitoring of global forest structural dynamics and provides tools for biodiversity conservation and ecosystem management efforts in a changing climate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06299v1</guid>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tiago de Conto, John Armston, Ralph Dubayah</dc:creator>
    </item>
    <item>
      <title>Dynamical Systems Models for Market Evolution: A Mechanistic Alternative to Autoregressive Methods</title>
      <link>https://arxiv.org/abs/2510.06778</link>
      <description>arXiv:2510.06778v1 Announce Type: cross 
Abstract: We present a novel approach to modeling market dynamics using ordinary differential equations that explicitly incorporates product competitiveness and consumer behavior. Our framework treats market segments as interacting populations in a dynamical system analogous to predator-prey models, where competitive advantages drive market share transitions through mechanistic modeling of market flows including new product adoption, refresh cycles, and obsolescence dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06778v1</guid>
      <category>math.DS</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aparna Komarla, Max Hill</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametric Dynamical Clustering of Time Series</title>
      <link>https://arxiv.org/abs/2510.06919</link>
      <description>arXiv:2510.06919v1 Announce Type: cross 
Abstract: We present a method that models the evolution of an unbounded number of time series clusters by switching among an unknown number of regimes with linear dynamics. We develop a Bayesian non-parametric approach using a hierarchical Dirichlet process as a prior on the parameters of a Switching Linear Dynamical System and a Gaussian process prior to model the statistical variations in amplitude and temporal alignment within each cluster. By modeling the evolution of time series patterns, the method avoids unnecessary proliferation of clusters in a principled manner. We perform inference by formulating a variational lower bound for off-line and on-line scenarios, enabling efficient learning through optimization. We illustrate the versatility and effectiveness of the approach through several case studies of electrocardiogram analysis using publicly available databases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.06919v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Adri\'an P\'erez-Herrero, Paulo F\'elix, Jes\'us Presedo, Carl Henrik Ek</dc:creator>
    </item>
    <item>
      <title>On Assessing Overall Survival (OS) in Oncology Studies</title>
      <link>https://arxiv.org/abs/2510.07122</link>
      <description>arXiv:2510.07122v1 Announce Type: cross 
Abstract: In assessing Overall Survival (OS) in oncology studies, it is essential for the efficacy measure to be Logic-respecting, for otherwise patients may be incorrectly targeted. This paper explains, while Time Ratio (TR) is Logic-respecting, Hazard Ratio (HR) is not Logic-respecting. With Time Ratio (TR) being recommended, a smooth transitioning strategy is suggested. The conclusion states: Logicality requires, and Subgroup Mixable Estimation (SME) delivers, an efficacy assessment for the overall population within the range of minimum and maximum efficacy in the subgroups, no matter how outcome is measured, whichever logic-respecting efficacy measure is chosen, the same efficacy assessment regardless of how subgroups are stratified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07122v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jason C. Hsu</dc:creator>
    </item>
    <item>
      <title>Bayesian Portfolio Optimization by Predictive Synthesis</title>
      <link>https://arxiv.org/abs/2510.07180</link>
      <description>arXiv:2510.07180v1 Announce Type: cross 
Abstract: Portfolio optimization is a critical task in investment. Most existing portfolio optimization methods require information on the distribution of returns of the assets that make up the portfolio. However, such distribution information is usually unknown to investors. Various methods have been proposed to estimate distribution information, but their accuracy greatly depends on the uncertainty of the financial markets. Due to this uncertainty, a model that could well predict the distribution information at one point in time may perform less accurately compared to another model at a different time. To solve this problem, we investigate a method for portfolio optimization based on Bayesian predictive synthesis (BPS), one of the Bayesian ensemble methods for meta-learning. We assume that investors have access to multiple asset return prediction models. By using BPS with dynamic linear models to combine these predictions, we can obtain a Bayesian predictive posterior about the mean rewards of assets that accommodate the uncertainty of the financial markets. In this study, we examine how to construct mean-variance portfolios and quantile-based portfolios based on the predicted distribution information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.07180v1</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>q-fin.CP</category>
      <category>q-fin.PM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Masahiro Kato, Kentaro Baba, Hibiki Kaibuchi, Ryo Inokuchi</dc:creator>
    </item>
    <item>
      <title>Causal Feedback Discovery using Convergence Cross Mapping on Sea Ice Data</title>
      <link>https://arxiv.org/abs/2505.09001</link>
      <description>arXiv:2505.09001v4 Announce Type: replace 
Abstract: Identifying causal relationships in climate systems remains challenging due to nonlinear, coupled dynamics that limit the effectiveness of linear and stochastic causal discovery approaches. This study benchmarks Convergence Cross Mapping (CCM) against Granger causality, PCMCI, and VarLiNGAM using both synthetic datasets with ground truth causal links and 41 years of Arctic climate data (1979--2021). Unlike stochastic models that rely on autoregressive residual dependence, CCM leverages Takens' state-space reconstruction and delay-embedding to reconstruct attractor manifolds from time series. Cross mapping between reconstructed manifolds exploits deterministic signatures of causation, enabling the detection of weak and bidirectional causal links that linear models fail to resolve. Results demonstrate that CCM achieves higher specificity and fewer false positives on synthetic benchmarks, while maintaining robustness under observational noise and limited sample lengths. On Arctic data, CCM reveals significant causal interactions between sea ice extent and atmospheric variables like specific humidity, longwave radiation, and surface temperature with a $p$-value of $0.009$, supporting ice-albedo feedbacks and moisture-radiation couplings central to Arctic amplification. In contrast, stochastic approaches miss these nonlinear dependencies or infer spurious causal relations. This work establishes CCM as a robust causal inference tool for nonlinear climate dynamics and provides the first systematic benchmarking framework for method selection in climate research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09001v4</guid>
      <category>stat.AP</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Nji, Seraj Al Mahmud Mostafa, Jianwu Wang</dc:creator>
    </item>
    <item>
      <title>ACT-Tensor: Tensor Completion Framework for Financial Dataset Imputation</title>
      <link>https://arxiv.org/abs/2508.01861</link>
      <description>arXiv:2508.01861v3 Announce Type: replace 
Abstract: Missing data in financial panels presents a critical obstacle, undermining asset-pricing models and reducing the effectiveness of investment strategies. Such panels are often inherently multi-dimensional, spanning firms, time, and financial variables, which adds complexity to the imputation task. Conventional imputation methods often fail by flattening the data's multidimensional structure, struggling with heterogeneous missingness patterns, or overfitting in the face of extreme data sparsity. To address these limitations, we introduce an Adaptive, Cluster-based Temporal smoothing tensor completion framework (ACT-Tensor) tailored for severely and heterogeneously missing multi-dimensional financial data panels. ACT-Tensor incorporates two key innovations: a cluster-based completion module that captures cross-sectional heterogeneity by learning group-specific latent structures; and a temporal smoothing module that proactively removes short-lived noise while preserving slow-moving fundamental trends. Extensive experiments show that ACT-Tensor consistently outperforms state-of-the-art benchmarks in terms of imputation accuracy across a range of missing data regimes, including extreme sparsity scenarios. To assess its practical financial utility, we evaluate the imputed data with an asset-pricing pipeline tailored for tensor-structured financial data. Results show that ACT-Tensor not only reduces pricing errors but also significantly improves risk-adjusted returns of the constructed portfolio. These findings confirm that our method delivers highly accurate and informative imputations, offering substantial value for financial decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01861v3</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3768292.3770408</arxiv:DOI>
      <dc:creator>Junyi Mo, Jiayu Li, Duo Zhang, Elynn Chen</dc:creator>
    </item>
    <item>
      <title>Sampling effects on Lasso estimation of drift functions in high-dimensional diffusion processes</title>
      <link>https://arxiv.org/abs/2408.08638</link>
      <description>arXiv:2408.08638v4 Announce Type: replace-cross 
Abstract: In this paper, we address high-dimensional parametric estimation of the drift function in diffusion models, specifically focusing on a $d$-dimensional ergodic diffusion process observed at discrete time points. We consider both a general linear form for the drift function and the particular case of the Ornstein-Uhlenbeck (OU) process. Assuming sparsity of the parameter vector, we examine the statistical behavior of the Lasso estimator for the unknown parameter. Our primary contribution is the proof of an oracle inequality for the Lasso estimator, which holds on the intersection of three specific sets defined for our analysis. We carefully control the probability of these sets, tackling the central challenge of our study. This approach allows us to derive error bounds for the $l_1$ and $l_2$ norms, assessing the performance of the proposed Lasso estimator. Our results demonstrate that, under certain conditions, the discretization error becomes negligible, enabling us to achieve the same optimal rate of convergence as if the continuous trajectory of the process were observed. We validate our theoretical findings through numerical experiments, which show that the Lasso estimator significantly outperforms the maximum likelihood estimator (MLE) in terms of support recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08638v4</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Amorino, Francisco Pina, Mark Podolskij</dc:creator>
    </item>
    <item>
      <title>Order Determination for Functional Data</title>
      <link>https://arxiv.org/abs/2503.03000</link>
      <description>arXiv:2503.03000v2 Announce Type: replace-cross 
Abstract: Dimension reduction is often necessary in functional data analysis, with functional principal component analysis being one of the most widely used techniques. A key challenge in applying these methods is determining the number of eigen-pairs to retain, a problem known as order determination. When a covariance function admits a finite representation, the challenge becomes estimating the rank of the associated covariance operator. While this problem is straightforward when the full trajectories of functional data are available, in practice, functional data are typically collected discretely and are subject to measurement error contamination. This contamination introduces a ridge to the empirical covariance function, which obscures the true rank of the covariance operator. We propose a novel procedure to identify the true rank of the covariance operator by leveraging the information of eigenvalues and eigenfunctions. By incorporating the nonparametric nature of functional data through smoothing techniques, the method is applicable to functional data collected at random, subject-specific points. Extensive simulation studies demonstrate the excellent performance of our approach across a wide range of settings, outperforming commonly used information-criterion-based methods and maintaining effectiveness even in high-noise scenarios. We further illustrate our method with two real-world data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03000v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chi Zhang, Peijun Sang, Yingli Qin</dc:creator>
    </item>
    <item>
      <title>The Multivariate SEM-PGS Model: Using Polygenic Scores to Investigate Cross-Trait Genetic Nurture and Assortative Mating</title>
      <link>https://arxiv.org/abs/2510.00353</link>
      <description>arXiv:2510.00353v2 Announce Type: replace-cross 
Abstract: Genetic nurture effects and assortative mating (AM) occur across many human behaviors and can bias estimates from traditional genetic models. These influences are typically studied univariately, within the same trait. However, estimation of cross-trait genetic nurture effects and cross-trait AM remains underexplored due to the absence of suitable approaches. To address this, we developed a multivariate extension of the SEM-PGS model for datasets with genotyped and phenotyped parents and offspring, enabling joint estimation of within-trait and cross-trait genetic and environmental influences. By integrating haplotypic polygenic scores (PGS) into a structural equation modeling framework, the model simultaneously estimates same-trait and cross-trait direct effects, genetic nurture, vertical transmission, and assortative mating. We also provide the first formal description of how copaths can be used to model multivariate assortative mating and derive the corresponding parameter expectations in matrix form. Forward-time Monte Carlo simulations under varying conditions of r^2_PGS and N_trio demonstrate that the model yields unbiased estimates of both within-trait and cross-trait effects when assumptions are met. The precision of estimates was adequate with large sample sizes (N_trio &gt; 16k) and improved as PGS predictive power increased. In addition, our simulation results show that failing to model cross-trait effects biases within-trait estimates, underscoring the importance of incorporating cross-trait effects. The multivariate SEM-PGS model offers a powerful and flexible tool for disentangling gene-environment interplay and advancing the understanding of familial influences on human traits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.00353v2</guid>
      <category>q-bio.QM</category>
      <category>q-bio.GN</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xuanyu Lyu, Jared Balbona, Tong Chen, Matthew C. Keller</dc:creator>
    </item>
    <item>
      <title>A new composite Mann-Whitney test for two-sample survival comparisons with right-censored data</title>
      <link>https://arxiv.org/abs/2510.05353</link>
      <description>arXiv:2510.05353v2 Announce Type: replace-cross 
Abstract: A fundamental challenge in comparing two survival distributions with right censored data is the selection of an appropriate nonparametric test, as the power of standard tests like the Log rank and Wilcoxon is highly dependent on the often unknown nature of the alternative hypothesis. This paper introduces a new, distribution free two sample test designed to overcome this limitation. The proposed method is based on a strategic decomposition of the data into uncensored and censored subsets, from which a composite test statistic is constructed as the sum of two independent Mann Whitney statistics. This design allows the test to automatically and inherently adapt to various patterns of difference including early, late, and crossing hazards without requiring pre specified parameters, pre testing, or complex weighting schemes. An extensive Monte Carlo simulation study demonstrates that the proposed test robustly maintains the nominal Type I error rate. Crucially, its power is highly competitive with the optimal traditional tests in standard scenarios and superior in complex settings with crossing survival curves, while also exhibiting remarkable robustness to high levels of censoring. The test power effectively approximates the maximum power achievable by either the Log rank or Wilcoxon tests across a wide range of alternatives, offering a powerful, versatile, and computationally simple tool for survival analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05353v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 09 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abid Hussain, Touqeer Ahmad</dc:creator>
    </item>
  </channel>
</rss>
