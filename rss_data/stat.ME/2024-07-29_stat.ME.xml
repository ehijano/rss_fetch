<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Jul 2024 02:27:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 29 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Shrinking Coarsened Win Ratio and Testing of Composite Endpoint</title>
      <link>https://arxiv.org/abs/2407.18341</link>
      <description>arXiv:2407.18341v1 Announce Type: new 
Abstract: Composite endpoints consisting of both terminal and non-terminal events, such as death and hospitalization, are frequently used as primary endpoints in cardiovascular clinical trials. The Win Ratio method (WR) proposed by Pocock et al. (2012) [1] employs a hierarchical structure to combine fatal and non-fatal events by giving death information an absolute priority, which adversely affects power if the treatment effect is mainly on the non-fatal outcomes. We hereby propose the Shrinking Coarsened Win Ratio method (SCWR) that releases the strict hierarchical structure of the standard WR by adding stages with coarsened thresholds shrinking to zero. A weighted adaptive approach is developed to determine the thresholds in SCWR. This method preserves the good statistical properties of the standard WR and has a greater capacity to detect treatment effects on non-fatal events. We show that SCWR has an overall more favorable performance than WR in our simulation that addresses the influence of follow-up time, the association between events, and the treatment effect levels, as well as a case study based on the Digitalis Investigation Group clinical trial data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18341v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Tassos Kyriakides, Scott Hummel, Fan Li, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Doubly Robust Targeted Estimation of Conditional Average Treatment Effects for Time-to-event Outcomes with Competing Risks</title>
      <link>https://arxiv.org/abs/2407.18389</link>
      <description>arXiv:2407.18389v1 Announce Type: new 
Abstract: In recent years, precision treatment strategy have gained significant attention in medical research, particularly for patient care. We propose a novel framework for estimating conditional average treatment effects (CATE) in time-to-event data with competing risks, using ICU patients with sepsis as an illustrative example. Our approach, based on cumulative incidence functions and targeted maximum likelihood estimation (TMLE), achieves both asymptotic efficiency and double robustness. The primary contribution of this work lies in our derivation of the efficient influence function for the targeted causal parameter, CATE. We established the theoretical proofs for these properties, and subsequently confirmed them through simulations. Our TMLE framework is flexible, accommodating various regression and machine learning models, making it applicable in diverse scenarios. In order to identify variables contributing to treatment effect heterogeneity and to facilitate accurate estimation of CATE, we developed two distinct variable importance measures (VIMs). This work provides a powerful tool for optimizing personalized treatment strategies, furthering the pursuit of precision medicine.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18389v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Runjia Li, Victor B. Talisa, Chung-Chou H. Chang</dc:creator>
    </item>
    <item>
      <title>Accounting for reporting delays in real-time phylodynamic analyses with preferential sampling</title>
      <link>https://arxiv.org/abs/2407.18432</link>
      <description>arXiv:2407.18432v1 Announce Type: new 
Abstract: The COVID-19 pandemic demonstrated that fast and accurate analysis of continually collected infectious disease surveillance data is crucial for situational awareness and policy making. Coalescent-based phylodynamic analysis can use genetic sequences of a pathogen to estimate changes in its effective population size, a measure of genetic diversity. These changes in effective population size can be connected to the changes in the number of infections in the population of interest under certain conditions. Phylodynamics is an important set of tools because its methods are often resilient to the ascertainment biases present in traditional surveillance data (e.g., preferentially testing symptomatic individuals). Unfortunately, it takes weeks or months to sequence and deposit the sampled pathogen genetic sequences into a database, making them available for such analyses. These reporting delays severely decrease precision of phylodynamic methods closer to present time, and for some models can lead to extreme biases. Here we present a method that affords reliable estimation of the effective population size trajectory closer to the time of data collection, allowing for policy decisions to be based on more recent data. Our work uses readily available historic times between sampling and sequencing for a population of interest, and incorporates this information into the sampling model to mitigate the effects of reporting delay in real-time analyses. We illustrate our methodology on simulated data and on SARS-CoV-2 sequences collected in the state of Washington in 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18432v1</guid>
      <category>stat.ME</category>
      <category>q-bio.PE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Catalina M. Medina, Julia A. Palacios, Volodymyr M. Minin</dc:creator>
    </item>
    <item>
      <title>Integration of Structural Equation Modeling and Bayesian Networks in the Context of Causal Inference: A Case Study on Personal Positive Youth Development</title>
      <link>https://arxiv.org/abs/2407.18612</link>
      <description>arXiv:2407.18612v1 Announce Type: new 
Abstract: In this study, the combined use of structural equation modeling (SEM) and Bayesian network modeling (BNM) in causal inference analysis is revisited. The perspective highlights the debate between proponents of using BNM as either an exploratory phase or even as the sole phase in the definition of structural models, and those advocating for SEM as the superior alternative for exploratory analysis. The individual strengths and limitations of SEM and BNM are recognized, but this exploration evaluates the contention between utilizing SEM's robust structural inference capabilities and the dynamic probabilistic modeling offered by BNM. A case study of the work of, \citet{balaguer_2022} in a structural model for personal positive youth development (\textit{PYD}) as a function of positive parenting (\textit{PP}) and perception of the climate and functioning of the school (\textit{CFS}) is presented. The paper at last presents a clear stance on the analytical primacy of SEM in exploratory causal analysis, while acknowledging the potential of BNM in subsequent phases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18612v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Edgar Benitez, Alvaro Balaguer</dc:creator>
    </item>
    <item>
      <title>Ensemble Kalman inversion approximate Bayesian computation</title>
      <link>https://arxiv.org/abs/2407.18721</link>
      <description>arXiv:2407.18721v1 Announce Type: new 
Abstract: Approximate Bayesian computation (ABC) is the most popular approach to inferring parameters in the case where the data model is specified in the form of a simulator. It is not possible to directly implement standard Monte Carlo methods for inference in such a model, due to the likelihood not being available to evaluate pointwise. The main idea of ABC is to perform inference on an alternative model with an approximate likelihood (the ABC likelihood), estimated at each iteration from points simulated from the data model. The central challenge of ABC is then to trade-off bias (introduced by approximating the model) with the variance introduced by estimating the ABC likelihood. Stabilising the variance of the ABC likelihood requires a computational cost that is exponential in the dimension of the data, thus the most common approach to reducing variance is to perform inference conditional on summary statistics. In this paper we introduce a new approach to estimating the ABC likelihood: using iterative ensemble Kalman inversion (IEnKI) (Iglesias, 2016; Iglesias et al., 2018). We first introduce new estimators of the marginal likelihood in the case of a Gaussian data model using the IEnKI output, then show how this may be used in ABC. Performance is illustrated on the Lotka-Volterra model, where we observe substantial improvements over standard ABC and other commonly-used approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18721v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard G Everitt</dc:creator>
    </item>
    <item>
      <title>Robust Estimation of Polychoric Correlation</title>
      <link>https://arxiv.org/abs/2407.18835</link>
      <description>arXiv:2407.18835v1 Announce Type: new 
Abstract: Polychoric correlation is often an important building block in the analysis of rating data, particularly for structural equation models. However, the commonly employed maximum likelihood (ML) estimator is highly susceptible to misspecification of the polychoric correlation model, for instance through violations of latent normality assumptions. We propose a novel estimator that is designed to be robust to partial misspecification of the polychoric model, that is, the model is only misspecified for an unknown fraction of observations, for instance (but not limited to) careless respondents. In contrast to existing literature, our estimator makes no assumption on the type or degree of model misspecification. It furthermore generalizes ML estimation and is consistent as well as asymptotically normally distributed. We demonstrate the robustness and practical usefulness of our estimator in simulation studies and an empirical application on a Big Five administration. In the latter, the polychoric correlation estimates of our estimator and ML differ substantially, which, after further inspection, is likely due to the presence of careless respondents that the estimator helps identify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18835v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Max Welz, Patrick Mair, Andreas Alfons</dc:creator>
    </item>
    <item>
      <title>Simulation Experiment Design for Calibration via Active Learning</title>
      <link>https://arxiv.org/abs/2407.18885</link>
      <description>arXiv:2407.18885v1 Announce Type: new 
Abstract: Simulation models often have parameters as input and return outputs to understand the behavior of complex systems. Calibration is the process of estimating the values of the parameters in a simulation model in light of observed data from the system that is being simulated. When simulation models are expensive, emulators are built with simulation data as a computationally efficient approximation of an expensive model. An emulator then can be used to predict model outputs, instead of repeatedly running an expensive simulation model during the calibration process. Sequential design with an intelligent selection criterion can guide the process of collecting simulation data to build an emulator, making the calibration process more efficient and effective. This article proposes two novel criteria for sequentially acquiring new simulation data in an active learning setting by considering uncertainties on the posterior density of parameters. Analysis of several simulation experiments and real-data simulation experiments from epidemiology demonstrates that proposed approaches result in improved posterior and field predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18885v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>\"Ozge S\"urer</dc:creator>
    </item>
    <item>
      <title>The nph2ph-transform: applications to the statistical analysis of completed clinical trials</title>
      <link>https://arxiv.org/abs/2407.18905</link>
      <description>arXiv:2407.18905v1 Announce Type: new 
Abstract: We present several illustrations from completed clinical trials on a statistical approach that allows us to gain useful insights regarding the time dependency of treatment effects. Our approach leans on a simple proposition: all non-proportional hazards (NPH) models are equivalent to a proportional hazards model. The nph2ph transform brings an NPH model into a PH form. We often find very simple approximations for this transform, enabling us to analyze complex NPH observations as though they had arisen under proportional hazards. Many techniques become available to us, and we use these to understand treatment effects better.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18905v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sean M. Devlin, John O'Quigley</dc:creator>
    </item>
    <item>
      <title>Higher Partials of fStress</title>
      <link>https://arxiv.org/abs/2407.18314</link>
      <description>arXiv:2407.18314v1 Announce Type: cross 
Abstract: We define *fDistances*, which generalize Euclidean distances, squared distances, and log distances. The least squares loss function to fit fDistances to dissimilarity data is *fStress*. We give formulas and R/C code to compute partial derivatives of orders one to four of fStress, relying heavily on the use of Fa\`a di Bruno's chain rule formula for higher derivatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18314v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Jan de Leeuw</dc:creator>
    </item>
    <item>
      <title>Adaptive Contrastive Search: Uncertainty-Guided Decoding for Open-Ended Text Generation</title>
      <link>https://arxiv.org/abs/2407.18698</link>
      <description>arXiv:2407.18698v1 Announce Type: cross 
Abstract: Decoding from the output distributions of large language models to produce high-quality text is a complex challenge in language modeling. Various approaches, such as beam search, sampling with temperature, $k-$sampling, nucleus $p-$sampling, typical decoding, contrastive decoding, and contrastive search, have been proposed to address this problem, aiming to improve coherence, diversity, as well as resemblance to human-generated text. In this study, we introduce adaptive contrastive search, a novel decoding strategy extending contrastive search by incorporating an adaptive degeneration penalty, guided by the estimated uncertainty of the model at each generation step. This strategy is designed to enhance both the creativity and diversity of the language modeling process while at the same time producing coherent and high-quality generated text output. Our findings indicate performance enhancement in both aspects, across different model architectures and datasets, underscoring the effectiveness of our method in text generation tasks. Our code base, datasets, and models are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18698v1</guid>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Esteban Garces Arias, Julian Rodemann, Meimingwei Li, Christian Heumann, Matthias A{\ss}enmacher</dc:creator>
    </item>
    <item>
      <title>Score matching through the roof: linear, nonlinear, and latent variables causal discovery</title>
      <link>https://arxiv.org/abs/2407.18755</link>
      <description>arXiv:2407.18755v1 Announce Type: cross 
Abstract: Causal discovery from observational data holds great promise, but existing methods rely on strong assumptions about the underlying causal structure, often requiring full observability of all relevant variables. We tackle these challenges by leveraging the score function $\nabla \log p(X)$ of observed variables for causal discovery and propose the following contributions. First, we generalize the existing results of identifiability with the score to additive noise models with minimal requirements on the causal mechanisms. Second, we establish conditions for inferring causal relations from the score even in the presence of hidden variables; this result is two-faced: we demonstrate the score's potential as an alternative to conditional independence tests to infer the equivalence class of causal graphs with hidden variables, and we provide the necessary conditions for identifying direct causes in latent variable models. Building on these insights, we propose a flexible algorithm for causal discovery across linear, nonlinear, and latent variable models, which we empirically validate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18755v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Francesco Montagna, Philipp M. Faller, Patrick Bloebaum, Elke Kirschbaum, Francesco Locatello</dc:creator>
    </item>
    <item>
      <title>Group integrative dynamic factor models with application to multiple subject brain connectivity</title>
      <link>https://arxiv.org/abs/2307.15330</link>
      <description>arXiv:2307.15330v4 Announce Type: replace 
Abstract: This work introduces a novel framework for dynamic factor model-based group-level analysis of multiple subjects time series data, called GRoup Integrative DYnamic factor (GRIDY) models. The framework identifies and characterizes inter-subject similarities and differences between two pre-determined groups by considering a combination of group spatial information and individual temporal dynamics. Furthermore, it enables the identification of intra-subject similarities and differences over time by employing different model configurations for each subject. Methodologically, the framework combines a novel principal angle-based rank selection algorithm and a non-iterative integrative analysis framework. Inspired by simultaneous component analysis, this approach also reconstructs identifiable latent factor series with flexible covariance structures. The performance of the GRIDY models is evaluated through simulations conducted under various scenarios. An application is also presented to compare resting-state functional MRI data collected from multiple subjects in autism spectrum disorder and control groups.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15330v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Younghoon Kim, Zachary F. Fisher, Vladas Pipiras</dc:creator>
    </item>
    <item>
      <title>Bayesian Methods for Modeling Cumulative Exposure to Extensive Environmental Health Hazards</title>
      <link>https://arxiv.org/abs/2404.04398</link>
      <description>arXiv:2404.04398v2 Announce Type: replace 
Abstract: Measuring the impact of an environmental point source exposure on the risk of disease, like cancer or childhood asthma, is well-developed. Modeling how an environmental health hazard that is extensive in space, like a wastewater canal, impacts disease risk is not. We propose a novel Bayesian generative semiparametric model for characterizing the cumulative spatial exposure to an environmental health hazard that is not well-represented by a single point in space. The model couples a dose-response model with a log-Gaussian Cox process integrated against a distance kernel with an unknown length-scale. We show that this model is a well-defined Bayesian inverse model, namely that the posterior exists under a Gaussian process prior for the log-intensity of exposure, and that a simple integral approximation adequately controls the computational error. We quantify the finite-sample properties and the computational tractability of the discretization scheme in a simulation study. Finally, we apply the model to survey data on household risk of childhood diarrheal illness from exposure to a system of wastewater canals in Mezquital Valley, Mexico.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04398v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rob Trangucci, Jesse Contreras, Jon Zelner, Joseph N. S. Eisenberg, Yang Chen</dc:creator>
    </item>
    <item>
      <title>A Correlation-induced Finite Difference Estimator</title>
      <link>https://arxiv.org/abs/2405.05638</link>
      <description>arXiv:2405.05638v2 Announce Type: replace 
Abstract: Estimating stochastic gradients is pivotal in fields like service systems within operations research. The classical method for this estimation is the finite difference approximation, which entails generating samples at perturbed inputs. Nonetheless, practical challenges persist in determining the perturbation and obtaining an optimal finite difference estimator in the sense of possessing the smallest mean squared error (MSE). To tackle this problem, we propose a double sample-recycling approach in this paper. Firstly, pilot samples are recycled to estimate the optimal perturbation. Secondly, recycling these pilot samples again and generating new samples at the estimated perturbation, lead to an efficient finite difference estimator. We analyze its bias, variance and MSE. Our analyses demonstrate a reduction in asymptotic variance, and in some cases, a decrease in asymptotic bias, compared to the optimal finite difference estimator. Therefore, our proposed estimator consistently coincides with, or even outperforms the optimal finite difference estimator. In numerical experiments, we apply the estimator in several examples, and numerical results demonstrate its robustness, as well as coincidence with the theory presented, especially in the case of small sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.05638v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Guo Liang, Guangwu Liu, Kun Zhang</dc:creator>
    </item>
    <item>
      <title>Post-selection inference for causal effects after causal discovery</title>
      <link>https://arxiv.org/abs/2405.06763</link>
      <description>arXiv:2405.06763v2 Announce Type: replace 
Abstract: Algorithms for constraint-based causal discovery select graphical causal models among a space of possible candidates (e.g., all directed acyclic graphs) by executing a sequence of conditional independence tests. These may be used to inform the estimation of causal effects (e.g., average treatment effects) when there is uncertainty about which covariates ought to be adjusted for, or which variables act as confounders versus mediators. However, naively using the data twice, for model selection and estimation, would lead to invalid confidence intervals. Moreover, if the selected graph is incorrect, the inferential claims may apply to a selected functional that is distinct from the actual causal effect. We propose an approach to post-selection inference that is based on a resampling and screening procedure, which essentially performs causal discovery multiple times with randomly varying intermediate test statistics. Then, an estimate of the target causal effect and corresponding confidence sets are constructed from a union of individual graph-based estimates and intervals. We show that this construction has asymptotically correct coverage for the true causal effect parameter. Importantly, the guarantee holds for a fixed population-level effect, not a data-dependent or selection-dependent quantity. Most of our exposition focuses on the PC-algorithm for learning directed acyclic graphs and the multivariate Gaussian case for simplicity, but the approach is general and modular, so it may be used with other conditional independence based discovery algorithms and distributional families.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06763v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ting-Hsuan Chang, Zijian Guo, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Adaptive and Efficient Learning with Blockwise Missing and Semi-Supervised Data</title>
      <link>https://arxiv.org/abs/2405.18722</link>
      <description>arXiv:2405.18722v2 Announce Type: replace 
Abstract: Data fusion is an important way to realize powerful and generalizable analyses across multiple sources. However, different capability of data collection across the sources has become a prominent issue in practice. This could result in the blockwise missingness (BM) of covariates troublesome for integration. Meanwhile, the high cost of obtaining gold-standard labels can cause the missingness of response on a large proportion of samples, known as the semi-supervised (SS) problem. In this paper, we consider a challenging scenario confronting both the BM and SS issues, and propose a novel Data-adaptive projecting Estimation approach for data FUsion in the SEmi-supervised setting (DEFUSE). Starting with a complete-data-only estimator, it involves two successive projection steps to reduce its variance without incurring bias. Compared to existing approaches, DEFUSE achieves a two-fold improvement. First, it leverages the BM labeled sample more efficiently through a novel data-adaptive projection approach robust to model misspecification on the missing covariates, leading to better variance reduction. Second, our method further incorporates the large unlabeled sample to enhance the estimation efficiency through imputation and projection. Compared to the previous SS setting with complete covariates, our work reveals a more essential role of the unlabeled sample in the BM setting. These advantages are justified in asymptotic and simulation studies. We also apply DEFUSE for the risk modeling and inference of heart diseases with the MIMIC-III electronic medical record (EMR) data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18722v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiming Li, Xuehan Yang, Ying Wei, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Percolated stochastic block model via EM algorithm and belief propagation with non-backtracking spectra</title>
      <link>https://arxiv.org/abs/2307.16502</link>
      <description>arXiv:2307.16502v5 Announce Type: replace-cross 
Abstract: Whereas Laplacian and modularity based spectral clustering is apt to dense graphs, recent results show that for sparse ones, the non-backtracking spectrum is the best candidate to find assortative clusters of nodes. Here belief propagation in the sparse stochastic block model is derived with arbitrary given model parameters that results in a non-linear system of equations; with linear approximation, the spectrum of the non-backtracking matrix is able to specify the number $k$ of clusters. Then the model parameters themselves can be estimated by the EM algorithm. Bond percolation in the assortative model is considered in the following two senses: the within- and between-cluster edge probabilities decrease with the number of nodes and edges coming into existence in this way are retained with probability $\beta$. As a consequence, the optimal $k$ is the number of the structural real eigenvalues (greater than $\sqrt{c}$, where $c$ is the average degree) of the non-backtracking matrix of the graph. Assuming, these eigenvalues $\mu_1 &gt;\dots &gt; \mu_k$ are distinct, the multiple phase transitions obtained for $\beta$ are $\beta_i =\frac{c}{\mu_i^2}$; further, at $\beta_i$ the number of detectable clusters is $i$, for $i=1,\dots ,k$. Inflation-deflation techniques are also discussed to classify the nodes themselves, which can be the base of the sparse spectral clustering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16502v5</guid>
      <category>math.CO</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marianna Bolla, Daniel Zhou</dc:creator>
    </item>
    <item>
      <title>Using representation balancing to learn conditional-average dose responses from clustered data</title>
      <link>https://arxiv.org/abs/2309.03731</link>
      <description>arXiv:2309.03731v2 Announce Type: replace-cross 
Abstract: Estimating a unit's responses to interventions with an associated dose, the "conditional average dose response" (CADR), is relevant in a variety of domains, from healthcare to business, economics, and beyond. Such a response typically needs to be estimated from observational data, which introduces several challenges. That is why the machine learning (ML) community has proposed several tailored CADR estimators. Yet, the proposal of most of these methods requires strong assumptions on the distribution of data and the assignment of interventions, which go beyond the standard assumptions in causal inference. Whereas previous works have so far focused on smooth shifts in covariate distributions across doses, in this work, we will study estimating CADR from clustered data and where different doses are assigned to different segments of a population. On a novel benchmarking dataset, we show the impacts of clustered data on model performance and propose an estimator, CBRNet, that learns cluster-agnostic and hence dose-agnostic covariate representations through representation balancing for unbiased CADR inference. We run extensive experiments to illustrate the workings of our method and compare it with the state of the art in ML for CADR estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.03731v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Christopher Bockel-Rickermann, Toon Vanderschueren, Jeroen Berrevoets, Tim Verdonck, Wouter Verbeke</dc:creator>
    </item>
    <item>
      <title>HMM for Discovering Decision-Making Dynamics Using Reinforcement Learning Experiments</title>
      <link>https://arxiv.org/abs/2401.13929</link>
      <description>arXiv:2401.13929v2 Announce Type: replace-cross 
Abstract: Major depressive disorder (MDD) presents challenges in diagnosis and treatment due to its complex and heterogeneous nature. Emerging evidence indicates that reward processing abnormalities may serve as a behavioral marker for MDD. To measure reward processing, patients perform computer-based behavioral tasks that involve making choices or responding to stimulants that are associated with different outcomes. Reinforcement learning (RL) models are fitted to extract parameters that measure various aspects of reward processing to characterize how patients make decisions in behavioral tasks. Recent findings suggest the inadequacy of characterizing reward learning solely based on a single RL model; instead, there may be a switching of decision-making processes between multiple strategies. An important scientific question is how the dynamics of learning strategies in decision-making affect the reward learning ability of individuals with MDD. Motivated by the probabilistic reward task (PRT) within the EMBARC study, we propose a novel RL-HMM framework for analyzing reward-based decision-making. Our model accommodates learning strategy switching between two distinct approaches under a hidden Markov model (HMM): subjects making decisions based on the RL model or opting for random choices. We account for continuous RL state space and allow time-varying transition probabilities in the HMM. We introduce a computationally efficient EM algorithm for parameter estimation and employ a nonparametric bootstrap for inference. We apply our approach to the EMBARC study to show that MDD patients are less engaged in RL compared to the healthy controls, and engagement is associated with brain activities in the negative affect circuitry during an emotional conflict task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13929v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingche Guo, Donglin Zeng, Yuanjia Wang</dc:creator>
    </item>
  </channel>
</rss>
