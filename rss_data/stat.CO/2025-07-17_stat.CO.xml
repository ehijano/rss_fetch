<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 18 Jul 2025 01:36:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fast Variational Bayes for Large Spatial Data</title>
      <link>https://arxiv.org/abs/2507.12251</link>
      <description>arXiv:2507.12251v1 Announce Type: new 
Abstract: Recent variational Bayes methods for geospatial regression, proposed as an alternative to computationally expensive Markov chain Monte Carlo (MCMC) sampling, have leveraged Nearest Neighbor Gaussian processes (NNGP) to achieve scalability. Yet, these variational methods remain inferior in accuracy and speed compared to spNNGP, the state-of-the-art MCMC-based software for NNGP. We introduce spVarBayes, a suite of fast variational Bayesian approaches for large-scale geospatial data analysis using NNGP. Our contributions are primarily computational. We replace auto-differentiation with a combination of calculus of variations, closed-form gradient updates, and linear response corrections for improved variance estimation. We also accommodate covariates (fixed effects) in the model and offer inference on the variance parameters. Simulation experiments demonstrate that we achieve comparable accuracy to spNNGP but with reduced computational costs, and considerably outperform existing variational inference methods in terms of both accuracy and speed. Analysis of a large forest canopy height dataset illustrates the practical implementation of proposed methods and shows that the inference results are consistent with those obtained from the MCMC approach. The proposed methods are implemented in publicly available Github R-package spVarBayes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12251v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiafang Song, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>Surrogate modeling for uncertainty quantification in nonlinear dynamics</title>
      <link>https://arxiv.org/abs/2507.12358</link>
      <description>arXiv:2507.12358v1 Announce Type: new 
Abstract: Predicting the behavior of complex systems in engineering often involves significant uncertainty about operating conditions, such as external loads, environmental effects, and manufacturing variability. As a result, uncertainty quantification (UQ) has become a critical tool in modeling-based engineering, providing methods to identify, characterize, and propagate uncertainty through computational models. However, the stochastic nature of UQ typically requires numerous evaluations of these models, which can be computationally expensive and limit the scope of feasible analyses. To address this, surrogate models, i.e., efficient functional approximations trained on a limited set of simulations, have become central in modern UQ practice. This book chapter presents a concise review of surrogate modeling techniques for UQ, with a focus on the particularly challenging task of capturing the full time-dependent response of dynamical systems. It introduces a classification of time-dependent problems based on the complexity of input excitation and discusses corresponding surrogate approaches, including combinations of principal component analysis with polynomial chaos expansions, time warping techniques, and nonlinear autoregressive models with exogenous inputs (NARX models). Each method is illustrated with simple application examples to clarify the underlying ideas and practical use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.12358v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>S. Marelli, S. Sch\"ar, B. Sudret</dc:creator>
    </item>
    <item>
      <title>Model averaging in the space of probability distributions</title>
      <link>https://arxiv.org/abs/2507.11719</link>
      <description>arXiv:2507.11719v1 Announce Type: cross 
Abstract: This work investigates the problem of model averaging in the context of measure-valued data. Specifically, we study aggregation schemes in the space of probability distributions metrized in terms of the Wasserstein distance. The resulting aggregate models, defined via Wasserstein barycenters, are optimally calibrated to empirical data. To enhance model performance, we employ regularization schemes motivated by the standard elastic net penalization, which is shown to consistently yield models enjoying sparsity properties. The consistency properties of the proposed averaging schemes with respect to sample size are rigorously established using the variational framework of $\Gamma$-convergence. The performance of the methods is evaluated through carefully designed synthetic experiments that assess behavior across a range of distributional characteristics and stress conditions. Finally, the proposed approach is applied to a real-world dataset of insurance losses - characterized by heavy-tailed behavior - to estimate the claim size distribution and the associated tail risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11719v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanouil Androulakis, Georgios I. Papayiannis, Athanasios N. Yannacopoulos</dc:creator>
    </item>
    <item>
      <title>Fiducial Matching: Differentially Private Inference for Categorical Data</title>
      <link>https://arxiv.org/abs/2507.11762</link>
      <description>arXiv:2507.11762v1 Announce Type: cross 
Abstract: The task of statistical inference, which includes the building of confidence intervals and tests for parameters and effects of interest to a researcher, is still an open area of investigation in a differentially private (DP) setting. Indeed, in addition to the randomness due to data sampling, DP delivers another source of randomness consisting of the noise added to protect an individual's data from being disclosed to a potential attacker. As a result of this convolution of noises, in many cases it is too complicated to determine the stochastic behavior of the statistics and parameters resulting from a DP procedure. In this work, we contribute to this line of investigation by employing a simulation-based matching approach, solved through tools from the fiducial framework, which aims to replicate the data generation pipeline (including the DP step) and retrieve an approximate distribution of the estimates resulting from this pipeline. For this purpose, we focus on the analysis of categorical (nominal) data that is common in national surveys, for which sensitivity is naturally defined, and on additive privacy mechanisms. We prove the validity of the proposed approach in terms of coverage and highlight its good computational and statistical performance for different inferential tasks in simulated and applied data settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11762v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ogonnaya Michael Romanus, Younes Boulaguiem, Roberto Molinari</dc:creator>
    </item>
    <item>
      <title>Bayesian multivariate models for bounded directional data</title>
      <link>https://arxiv.org/abs/2507.11784</link>
      <description>arXiv:2507.11784v1 Announce Type: cross 
Abstract: In some areas of knowledge there are data representing directions restricted to a specific range of values. Consequently, it is useful to have models for describing variables defined in subsets of the k-dimensional unit sphere. This need has led to the development of models such as the multivariate projected Gamma distribution. However, the proposal of multivariate models whose marginal variables are defined only in sections of the unit circle and with a flexible dependency structure is limited. In this work, we propose constructing multivariate models where each marginal variable is a circular variable defined only in the first quadrant of the unit circle. Our approach is based on the concept of copula functions. The inferences for the proposed models rely on generating samples of the posterior joint density of all parameters involved in the models. This is achieved by applying a conditional approach that allows inferences to be made using a two-stage sampling. The proposed methodology is illustrated with both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.11784v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Montesinos-Vazquez, Gabriel N\'u\~nez-Antonio</dc:creator>
    </item>
    <item>
      <title>Error bounds for particle gradient descent, and extensions of the log-Sobolev and Talagrand inequalities</title>
      <link>https://arxiv.org/abs/2403.02004</link>
      <description>arXiv:2403.02004v3 Announce Type: replace-cross 
Abstract: We prove non-asymptotic error bounds for particle gradient descent (PGD, Kuntz et al., 2023), a recently introduced algorithm for maximum likelihood estimation of large latent variable models obtained by discretizing a gradient flow of the free energy. We begin by showing that the flow converges exponentially fast to the free energy's minimizers for models satisfying a condition that generalizes both the log-Sobolev and the Polyak--{\L}ojasiewicz inequalities (LSI and P{\L}I, respectively). We achieve this by extending a result well-known in the optimal transport literature (that the LSI implies the Talagrand inequality) and its counterpart in the optimization literature (that the P{\L}I implies the so-called quadratic growth condition), and applying the extension to our new setting. We also generalize the Bakry--\'Emery Theorem and show that the LSI/P{\L}I extension holds for models with strongly concave log-likelihoods. For such models, we further control PGD's discretization error and obtain the non-asymptotic error bounds. While we are motivated by the study of PGD, we believe that the inequalities and results we extend may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.02004v3</guid>
      <category>cs.LG</category>
      <category>math.FA</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Journal of Machine Learning Research, 26(103):1-38, 2025</arxiv:journal_reference>
      <dc:creator>Rocco Caprio, Juan Kuntz, Samuel Power, Adam M. Johansen</dc:creator>
    </item>
    <item>
      <title>Modeling the uncertainty on the covariance matrix for probabilistic forecast reconciliation</title>
      <link>https://arxiv.org/abs/2506.19554</link>
      <description>arXiv:2506.19554v2 Announce Type: replace-cross 
Abstract: In forecast reconciliation, the covariance matrix of the base forecasts errors plays a crucial role. Typically, this matrix is estimated, and then treated as known. In contrast, we propose a Bayesian reconciliation model that accounts for the uncertainty in the estimation of the covariance matrix. This leads to a reconciled predictive distribution that follows a multivariate t-distribution, obtained in closed-form, rather than a multivariate Gaussian. We evaluate our method on three tourism-related datasets, including a new publicly available dataset. Empirical results show that our approach consistently improves prediction intervals compared to Gaussian reconciliation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19554v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Thu, 17 Jul 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chiara Carrara, Dario Azzimonti, Giorgio Corani, Lorenzo Zambon</dc:creator>
    </item>
  </channel>
</rss>
