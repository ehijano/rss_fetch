<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.GT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.GT</link>
    <description>cs.GT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.GT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 01 Nov 2024 02:06:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Combinatorial Diffusion Auction Design</title>
      <link>https://arxiv.org/abs/2410.22765</link>
      <description>arXiv:2410.22765v1 Announce Type: new 
Abstract: Diffusion auction design for combinatorial settings is a long-standing challenge. One difficulty is that we cannot directly extend the solutions for simpler settings to combinatorial settings (like extending the Vickrey auction to VCG in the traditional settings). In this paper, we propose a different approach to leverage the diffusion auctions for single-item settings. We design a combinatorial diffusion auction framework which can use any desirable single-item diffusion auction to produce a combinatorial auction to satisfy incentive compatibility (IC), individual rationality (IR), and weak budget balance (WBB).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22765v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuanyu Li, Miao Li, Yuhan Cao, Dengji Zhao</dc:creator>
    </item>
    <item>
      <title>Conflux-PSRO: Effectively Leveraging Collective Advantages in Policy Space Response Oracles</title>
      <link>https://arxiv.org/abs/2410.22776</link>
      <description>arXiv:2410.22776v1 Announce Type: new 
Abstract: Policy Space Response Oracle (PSRO) with policy population construction has been demonstrated as an effective method for approximating Nash Equilibrium (NE) in zero-sum games. Existing studies have attempted to improve diversity in policy space, primarily by incorporating diversity regularization into the Best Response (BR). However, these methods cause the BR to deviate from maximizing rewards, easily resulting in a population that favors diversity over performance, even when diversity is not always necessary. Consequently, exploitability is difficult to reduce until policies are fully explored, especially in complex games. In this paper, we propose Conflux-PSRO, which fully exploits the diversity of the population by adaptively selecting and training policies at state-level. Specifically, Conflux-PSRO identifies useful policies from the existing population and employs a routing policy to select the most appropriate policies at each decision point, while simultaneously training them to enhance their effectiveness. Compared to the single-policy BR of traditional PSRO and its diversity-improved variants, the BR generated by Conflux-PSRO not only leverages the specialized expertise of diverse policies but also synergistically enhances overall performance. Our experiments on various environments demonstrate that Conflux-PSRO significantly improves the utility of BRs and reduces exploitability compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22776v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yucong Huang (School of Software and Microelectronics, Peking University, Beijing, China), Jiesong Lian (Huazhong University of Science &amp; Technology, Wuhan, China), Mingzhi Wang (Institute for Artificial Intelligence, Peking University, Beijing, China), Chengdong Ma (Institute for Artificial Intelligence, Peking University, Beijing, China), Ying Wen (Shanghai Jiao Tong University, Shanghai, China)</dc:creator>
    </item>
    <item>
      <title>Fair Division with Market Values</title>
      <link>https://arxiv.org/abs/2410.23137</link>
      <description>arXiv:2410.23137v1 Announce Type: new 
Abstract: We introduce a model of fair division with market values, where indivisible goods must be partitioned among agents with (additive) subjective valuations, and each good additionally has a market value. The market valuation can be viewed as a separate additive valuation that holds identically across all the agents. We seek allocations that are simultaneously fair with respect to the subjective valuations and with respect to the market valuation.
  We show that an allocation that satisfies stochastically-dominant envy-freeness up to one good (SD-EF1) with respect to both the subjective valuations and the market valuation does not always exist, but the weaker guarantee of EF1 with respect to the subjective valuations along with SD-EF1 with respect to the market valuation can be guaranteed. We also study a number of other guarantees such as Pareto optimality, EFX, and MMS. In addition, we explore non-additive valuations and extend our model to cake-cutting. Along the way, we identify several tantalizing open questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23137v1</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siddharth Barman, Soroush Ebadian, Mohamad Latifian, Nisarg Shah</dc:creator>
    </item>
    <item>
      <title>Carrot and Stick: Eliciting Comparison Data and Beyond</title>
      <link>https://arxiv.org/abs/2410.23243</link>
      <description>arXiv:2410.23243v1 Announce Type: new 
Abstract: Comparison data elicited from people are fundamental to many machine learning tasks, including reinforcement learning from human feedback for large language models and estimating ranking models. They are typically subjective and not directly verifiable. How to truthfully elicit such comparison data from rational individuals? We design peer prediction mechanisms for eliciting comparison data using a bonus-penalty payment. Our design leverages on the strong stochastic transitivity for comparison data to create symmetrically strongly truthful mechanisms such that truth-telling 1) forms a strict Bayesian Nash equilibrium, and 2) yields the highest payment among all symmetric equilibria. Each individual only needs to evaluate one pair of items and report her comparison in our mechanism.
  We further extend the bonus-penalty payment concept to eliciting networked data, designing a symmetrically strongly truthful mechanism when agents' private signals are sampled according to the Ising models. We provide the necessary and sufficient conditions for our bonus-penalty payment to have truth-telling as a strict Bayesian Nash equilibrium. Experiments on two real-world datasets further support our theoretical discoveries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23243v1</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiling Chen, Shi Feng, Fang-Yi Yu</dc:creator>
    </item>
    <item>
      <title>Reactive Synthesis for Expected Impacts</title>
      <link>https://arxiv.org/abs/2410.22760</link>
      <description>arXiv:2410.22760v1 Announce Type: cross 
Abstract: As business processes become increasingly complex,  effectively modeling decision points, their likelihood,  and resource consumption is crucial for optimizing operations.  To address this challenge, this paper introduces a formal  extension of the Business Process Model and Notation (BPMN)  that incorporates choices, probabilities, and impacts,  referred to as BPMN+CPI. This extension is motivated  by the growing emphasis on precise control within  business process management, where carefully  selecting decision pathways in repeated instances  is crucial for conforming to certain standards of multiple resource consumption and environmental impacts.  In this context we deal with the problem of synthesizing a  strategy (if any) that guarantees that the expected impacts on repeated execution of the input process  are below a given threshold.  We show that this problem belongs to   PSPACE complexity class; moreover we provide an effective procedure  for computing a strategy (if present).</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22760v1</guid>
      <category>cs.LO</category>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.409.7</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 409, 2024, pp. 35-52</arxiv:journal_reference>
      <dc:creator>Emanuele Chini (University "La Sapienza" Rome), Pietro Sala (University of Verona), Andrea Simonetti (University of Verona), Omid Zare (University of Verona)</dc:creator>
    </item>
    <item>
      <title>A Game-Theoretic Approach for Security Control Selection</title>
      <link>https://arxiv.org/abs/2410.22762</link>
      <description>arXiv:2410.22762v1 Announce Type: cross 
Abstract: Selecting the combination of security controls that will most effectively protect a system's assets is a difficult task. If the wrong controls are selected, the system may be left vulnerable to cyber-attacks that can impact the confidentiality, integrity and availability of critical data and services. In practical settings, it is not possible to select and implement every control possible. Instead considerations, such as budget, effectiveness, and dependencies among various controls, must be considered to choose a combination of security controls that best achieve a set of system security objectives. In this paper, we propose a game-theoretic approach for selecting effective combinations of security controls based on expected attacker profiles and a set budget. The control selection problem is set up as a two-person zero-sum one-shot game. Valid control combinations for selection are generated using an algebraic formalism to account for dependencies among selected controls. We demonstrate the proposed approach on an illustrative financial system used in government departments under four different scenarios. The results illustrate how a security analyst can use the proposed approach to guide and support decision-making in the control selection activity when developing secure systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22762v1</guid>
      <category>cs.CR</category>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.4204/EPTCS.409.11</arxiv:DOI>
      <arxiv:journal_reference>EPTCS 409, 2024, pp. 103-119</arxiv:journal_reference>
      <dc:creator>Dylan L\'eveill\'e (Department of Systems,Computer Engineering, Carleton University), Jason Jaskolka (Department of Systems,Computer Engineering, Carleton University)</dc:creator>
    </item>
    <item>
      <title>Self-optimization in distributed manufacturing systems using Modular State-based Stackelberg Games</title>
      <link>https://arxiv.org/abs/2410.22912</link>
      <description>arXiv:2410.22912v1 Announce Type: cross 
Abstract: In this study, we introduce Modular State-based Stackelberg Games (Mod-SbSG), a novel game structure developed for distributed self-learning in modular manufacturing systems. Mod-SbSG enhances cooperative decision-making among self-learning agents within production systems by integrating State-based Potential Games (SbPG) with Stackelberg games. This hierarchical structure assigns more important modules of the manufacturing system a first-mover advantage, while less important modules respond optimally to the leaders' decisions. This decision-making process differs from typical multi-agent learning algorithms in manufacturing systems, where decisions are made simultaneously. We provide convergence guarantees for the novel game structure and design learning algorithms to account for the hierarchical game structure. We further analyse the effects of single-leader/multiple-follower and multiple-leader/multiple-follower scenarios within a Mod-SbSG. To assess its effectiveness, we implement and test Mod-SbSG in an industrial control setting using two laboratory-scale testbeds featuring sequential and serial-parallel processes. The proposed approach delivers promising results compared to the vanilla SbPG, which reduces overflow by 97.1%, and in some cases, prevents overflow entirely. Additionally, it decreases power consumption by 5-13% while satisfying the production demand, which significantly improves potential (global objective) values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22912v1</guid>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Steve Yuwono, Ahmar Kamal Hussain, Dorothea Schwung, Andreas Schwung</dc:creator>
    </item>
    <item>
      <title>Dynamic Matching with Post-allocation Service and its Application to Refugee Resettlement</title>
      <link>https://arxiv.org/abs/2410.22992</link>
      <description>arXiv:2410.22992v1 Announce Type: cross 
Abstract: Motivated by our collaboration with a major refugee resettlement agency in the U.S., we study a dynamic matching problem where each new arrival (a refugee case) must be matched immediately and irrevocably to one of the static resources (a location with a fixed annual quota). In addition to consuming the static resource, each case requires post-allocation service from a server, such as a translator. Given the time-consuming nature of service, a server may not be available at a given time, thus we refer to it as a dynamic resource. Upon matching, the case will wait to avail service in a first-come-first-serve manner. Bursty matching to a location may result in undesirable congestion at its corresponding server. Consequently, the central planner (the agency) faces a dynamic matching problem with an objective that combines the matching reward (captured by pair-specific employment outcomes) with the cost for congestion for dynamic resources and over-allocation for the static ones. Motivated by the observed fluctuations in the composition of refugee pools across the years, we design algorithms that do not rely on distributional knowledge constructed based on past years' data. To that end, we develop learning-based algorithms that are asymptotically optimal in certain regimes, easy to interpret, and computationally fast. Our design is based on learning the dual variables of the underlying optimization problem; however, the main challenge lies in the time-varying nature of the dual variables associated with dynamic resources. To overcome this challenge, our theoretical development brings together techniques from Lyapunov analysis, adversarial online learning, and stochastic optimization. On the application side, when tested on real data from our partner agency, our method outperforms existing ones making it a viable candidate for replacing the current practice upon experimentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.22992v1</guid>
      <category>cs.DS</category>
      <category>cs.GT</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kirk Bansak, Soonbong Lee, Vahideh Manshadi, Rad Niazadeh, Elisabeth Paulson</dc:creator>
    </item>
    <item>
      <title>COMAL: A Convergent Meta-Algorithm for Aligning LLMs with General Preferences</title>
      <link>https://arxiv.org/abs/2410.23223</link>
      <description>arXiv:2410.23223v1 Announce Type: cross 
Abstract: Many alignment methods, including reinforcement learning from human feedback (RLHF), rely on the Bradley-Terry reward assumption, which is insufficient to capture the full range of general human preferences. To achieve robust alignment with general preferences, we model the alignment problem as a two-player zero-sum game, where the Nash equilibrium policy guarantees a 50% win rate against any competing policy. However, previous algorithms for finding the Nash policy either diverge or converge to a Nash policy in a modified game, even in a simple synthetic setting, thereby failing to maintain the 50% win rate guarantee against all other policies. We propose a meta-algorithm, Convergent Meta Alignment Algorithm (COMAL), for language model alignment with general preferences, inspired by convergent algorithms in game theory. Theoretically, we prove that our meta-algorithm converges to an exact Nash policy in the last iterate. Additionally, our meta-algorithm is simple and can be integrated with many existing methods designed for RLHF and preference optimization with minimal changes. Experimental results demonstrate the effectiveness of the proposed framework when combined with existing preference policy optimization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23223v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixin Liu, Argyris Oikonomou, Weiqiang Zheng, Yang Cai, Arman Cohan</dc:creator>
    </item>
    <item>
      <title>Proportional Fairness in Non-Centroid Clustering</title>
      <link>https://arxiv.org/abs/2410.23273</link>
      <description>arXiv:2410.23273v1 Announce Type: cross 
Abstract: We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points (agents) that are large and cohesive. Prior work applies this framework to centroid clustering, where the loss of an agent is its distance to the centroid assigned to its cluster. We expand the framework to non-centroid clustering, where the loss of an agent is a function of the other agents in its cluster, by adapting two proportional fairness criteria -- the core and its relaxation, fully justified representation (FJR) -- to this setting.
  We show that the core can be approximated only under structured loss functions, and even then, the best approximation we are able to establish, using an adaptation of the GreedyCapture algorithm developed for centroid clustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a natural loss function. In contrast, we design a new (inefficient) algorithm, GreedyCohesiveClustering, which achieves the relaxation FJR exactly under arbitrary loss functions, and show that the efficient GreedyCapture algorithm achieves a constant approximation of FJR. We also design an efficient auditing algorithm, which estimates the FJR approximation of any given clustering solution up to a constant factor. Our experiments on real data suggest that traditional clustering algorithms are highly unfair, whereas GreedyCapture is considerably fairer and incurs only a modest loss in common clustering objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.23273v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ioannis Caragiannis, Evi Micha, Nisarg Shah</dc:creator>
    </item>
    <item>
      <title>Tight Bounds for The Price of Fairness</title>
      <link>https://arxiv.org/abs/2311.18339</link>
      <description>arXiv:2311.18339v2 Announce Type: replace 
Abstract: A central decision maker (CDM), who seeks an efficient allocation of scarce resources among a finite number of players, often has to incorporate fairness criteria to avoid unfair outcomes. Indeed, the Price of Fairness (POF), a term coined in the seminal work by Bertsimas et al. (2011), refers to the efficiency loss due to the incorporation of fairness criteria into the allocation method. Quantifying the POF would help the CDM strike an appropriate balance between efficiency and fairness. In this paper we improve upon existing results in the literature, by providing tight bounds for the POF for the proportional fairness criterion for any $n$, when the maximum achievable utilities of the players are equal or are not equal. Further, while Bertsimas et al. (2011) have already derived a tight bound for the max-min fairness criterion for the case that all players have equal maximum achievable utilities, we also provide a tight bound in scenarios where these utilities are not equal. For both criteria, we characterize the conditions where the POF reaches its peak and provide the supremum bounds of our bounds over all maximum achievable utility vectors, which are shown to be asymptotically strictly smaller than the supremum of the Bertsimas et al. (2011) bounds. Finally, we investigate the sensitivity of our bounds and the bounds in Bertsimas et al. (2011) for the POF to the variability of the maximum achievable utilities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18339v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifeng Cao, Yichuan Ding, Daniel Granot</dc:creator>
    </item>
    <item>
      <title>Bias Detection Via Signaling</title>
      <link>https://arxiv.org/abs/2405.17694</link>
      <description>arXiv:2405.17694v2 Announce Type: replace 
Abstract: We introduce and study the problem of detecting whether an agent is updating their prior beliefs given new evidence in an optimal way that is Bayesian, or whether they are biased towards their own prior. In our model, biased agents form posterior beliefs that are a convex combination of their prior and the Bayesian posterior, where the more biased an agent is, the closer their posterior is to the prior. Since we often cannot observe the agent's beliefs directly, we take an approach inspired by information design. Specifically, we measure an agent's bias by designing a signaling scheme and observing the actions they take in response to different signals, assuming that they are maximizing their own expected utility; our goal is to detect bias with a minimum number of signals. Our main results include a characterization of scenarios where a single signal suffices and a computationally efficient algorithm to compute optimal signaling schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17694v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiling Chen, Tao Lin, Ariel D. Procaccia, Aaditya Ramdas, Itai Shapira</dc:creator>
    </item>
    <item>
      <title>Intrinsic Robustness of Prophet Inequality to Strategic Reward Signaling</title>
      <link>https://arxiv.org/abs/2409.18269</link>
      <description>arXiv:2409.18269v2 Announce Type: replace 
Abstract: Prophet inequality concerns a basic optimal stopping problem and states that simple threshold stopping policies -- i.e., accepting the first reward larger than a certain threshold -- can achieve tight $\frac{1}{2}$-approximation to the optimal prophet value. Motivated by its economic applications, this paper studies the robustness of this approximation to natural strategic manipulations in which each random reward is associated with a self-interested player who may selectively reveal his realized reward to the searcher in order to maximize his probability of being selected.
  We say a threshold policy is $\alpha$(-strategically)-robust if it (a) achieves the $\alpha$-approximation to the prophet value for strategic players; and (b) meanwhile remains a $\frac{1}{2}$-approximation in the standard non-strategic setting. Starting with a characterization of each player's optimal information revealing strategy, we demonstrate the intrinsic robustness of prophet inequalities to strategic reward signaling through the following results: (1) for arbitrary reward distributions, there is a threshold policy that is $\frac{1-\frac{1}{e}}{2}$-robust, and this ratio is tight; (2) for i.i.d. reward distributions, there is a threshold policy that is $\frac{1}{2}$-robust, which is tight for the setting; and (3) for log-concave (but non-identical) reward distributions, the $\frac{1}{2}$-robustness can also be achieved under certain regularity assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18269v2</guid>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Tang, Haifeng Xu, Ruimin Zhang, Derek Zhu</dc:creator>
    </item>
    <item>
      <title>Bandits with Preference Feedback: A Stackelberg Game Perspective</title>
      <link>https://arxiv.org/abs/2406.16745</link>
      <description>arXiv:2406.16745v2 Announce Type: replace-cross 
Abstract: Bandits with preference feedback present a powerful tool for optimizing unknown target functions when only pairwise comparisons are allowed instead of direct value queries. This model allows for incorporating human feedback into online inference and optimization and has been employed in systems for fine-tuning large language models. The problem is well understood in simplified settings with linear target functions or over finite small domains that limit practical interest. Taking the next step, we consider infinite domains and nonlinear (kernelized) rewards. In this setting, selecting a pair of actions is quite challenging and requires balancing exploration and exploitation at two levels: within the pair, and along the iterations of the algorithm. We propose MAXMINLCB, which emulates this trade-off as a zero-sum Stackelberg game, and chooses action pairs that are informative and yield favorable rewards. MAXMINLCB consistently outperforms existing algorithms and satisfies an anytime-valid rate-optimal regret guarantee. This is due to our novel preference-based confidence sequences for kernelized logistic estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16745v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.GT</category>
      <category>stat.ML</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Barna P\'asztor, Parnian Kassraie, Andreas Krause</dc:creator>
    </item>
    <item>
      <title>Instigating Cooperation among LLM Agents Using Adaptive Information Modulation</title>
      <link>https://arxiv.org/abs/2409.10372</link>
      <description>arXiv:2409.10372v3 Announce Type: replace-cross 
Abstract: This paper introduces a novel framework combining LLM agents as proxies for human strategic behavior with reinforcement learning (RL) to engage these agents in evolving strategic interactions within team environments. Our approach extends traditional agent-based simulations by using strategic LLM agents (SLA) and introducing dynamic and adaptive governance through a pro-social promoting RL agent (PPA) that modulates information access across agents in a network, optimizing social welfare and promoting pro-social behavior. Through validation in iterative games, including the prisoner dilemma, we demonstrate that SLA agents exhibit nuanced strategic adaptations. The PPA agent effectively learns to adjust information transparency, resulting in enhanced cooperation rates. This framework offers significant insights into AI-mediated social dynamics, contributing to the deployment of AI in real-world team settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.10372v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Thu, 31 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qiliang Chen, Sepehr Ilami, Nunzio Lore, Babak Heydari</dc:creator>
    </item>
  </channel>
</rss>
