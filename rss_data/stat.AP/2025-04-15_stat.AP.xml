<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Apr 2025 01:58:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Study on Text Classification for Public Administration</title>
      <link>https://arxiv.org/abs/2504.09111</link>
      <description>arXiv:2504.09111v1 Announce Type: new 
Abstract: In German public administration, there are 45 different offices to which incoming messages need to be distributed. Since these messages are often unstructured, the system has to be based at least partly on message content. For public service no data are given so far and no pretrained model is available. The data we used are conducted by Governikus KG and are of highly different length. To handle those data with standard methods different approaches are known, like normalization or segmentation. However, text classification is highly dependent on the data structure, a study for public administration data is missing at the moment. We conducted such a study analyzing different techniques of classification based on segments, normalization and feature selection. Thereby, we used different methods, this means neural nets, random forest, logistic regression, SVM classifier and SVAE. The comparison shows for the given public service data a classification accuracy of above 80\% can be reached based on cross validation. We further show that normalization is preferable, while the difference to the segmentation approach depends mainly on the choice of algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09111v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Stefanie Schwaar, Franziska Diez, Michael Trebing, Nils Witznick</dc:creator>
    </item>
    <item>
      <title>Quality Control and Structural Reliability -- A Unified Framework for Integrating Conformity Assessment and Partial Safety Factors</title>
      <link>https://arxiv.org/abs/2504.09508</link>
      <description>arXiv:2504.09508v1 Announce Type: new 
Abstract: Ensuring structural reliability remains a core concern in civil engineering, yet the quantitative effects of quality control measures on material variability and safety margins are not fully understood, especially for materials other than reinforced concrete. This study addresses this gap by presenting a probabilistic framework that integrates Bayesian updating, acceptance sampling, and operating characteristic (OC) curves to model conformity assessment as a probabilistic filter. In doing so, it refines prior distributions of key material and execution parameters based on quality control outcomes, linking reductions in the coefficient of variation directly to adjustments in partial safety factors. Applying the framework to a masonry wall example demonstrates how systematic quality control efforts, particularly those targeting parameters with higher importance such as masonry unit strength and execution quality-produce substantial gains in structural reliability. The analysis shows that combined quality control measures can lower the partial safety factor from a baseline of 1.5 to about 1.38, corresponding to an improvement factor of roughly 1.09 and material savings of approximately 8%. Conversely, controlling parameters with negligible influence, such as mortar properties, provides limited benefit. These findings encourage focusing quality control resources on the most influential parameters and integrating results into semi-probabilistic design methods. By offering a transparent, standards-compatible approach, the framework supports the refinement of design guidelines, promotes more efficient resource allocation, and enhances overall structural safety in the built environment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09508v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tammam Bakeer, Wolfram Jaeger</dc:creator>
    </item>
    <item>
      <title>Integrated Bayesian non-parametric spatial modeling for cross-sample identification of spatially variable genes</title>
      <link>https://arxiv.org/abs/2504.09654</link>
      <description>arXiv:2504.09654v1 Announce Type: new 
Abstract: Spatial transcriptomics has revolutionized tissue analysis by simultaneously mapping gene expression, spatial topography, and histological context across consecutive tissue sections, enabling systematic investigation of spatial heterogeneity. The detection of spatially variable (SV) genes-molecular signatures with position-dependent expression-provides critical insights into disease mechanisms spanning oncology, neurology, and cardiovascular research. Current methodologies, however, confront dual constraints: predominant reliance on predefined spatial pattern templates restricts detection of novel complex spatial architectures, and inconsistent sample selection strategies compromise analytical stability and biological interpretability. To overcome these challenges, we propose a novel Bayesian hierarchical framework incorporating non-parametric spatial modeling and across-sample integration. It takes advantage of the non-parametric technique and develops an adaptive spatial process accommodating complex pattern discovery. A novel cross-sample bi-level shrinkage prior is further introduced for robust multi-sample SV gene detection, facilitating more effective information fusion. An efficient variational inference is developed for posterior inference ensuring computational scalability. This architecture synergistically addresses spatial complexity through adaptive pattern learning while maintaining biological interpretability. Comprehensive simulations and empirical analyzes confirm the improved performance of the proposed method in resolving complex spatial expression patterns compared to existing analytical frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09654v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Meng Zhou, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>Peer-to-Peer Basis Risk Management for Renewable Production Parametric Insurance</title>
      <link>https://arxiv.org/abs/2504.09660</link>
      <description>arXiv:2504.09660v1 Announce Type: new 
Abstract: This work presents a framework for peer-to-peer (P2P) basis risk management applied to solar electricity generation. The approach leverages physically based simulation models to estimate the day-ahead production forecasts and the actual realized production at the solar farm level. We quantify the financial loss from mismatches between forecasted and actual production using the outputs of these simulations. The framework then implements a parametric insurance mechanism to mitigate these financial losses and combines it with a P2P market structure to enhance participant risk sharing. By integrating day-ahead forecasts and actual production data with physical modeling, this method provides a comprehensive solution to manage production variability, offering practical insights for improving financial resilience in renewable energy systems. The results highlight the potential of combining parametric insurance with P2P mechanisms to foster reliability and collaboration in renewable energy markets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09660v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fallou Niakh, Alicia Bassi\`ere, Michel Denuit, Christian Robert</dc:creator>
    </item>
    <item>
      <title>Modeling Discrete Coating Degradation Events via Hawkes Processes</title>
      <link>https://arxiv.org/abs/2504.09706</link>
      <description>arXiv:2504.09706v1 Announce Type: new 
Abstract: Forecasting the degradation of coated materials has long been a topic of critical interest in engineering, as it has enormous implications for both system maintenance and sustainable material use. Material degradation is affected by many factors, including the history of corrosion and characteristics of the environment, which can be measured by high-frequency sensors. However, the high volume of data produced by such sensors can inhibit efficient modeling and prediction. To alleviate this issue, we propose novel metrics for representing material degradation, taking the form of discrete degradation events. These events maintain the statistical properties of continuous sensor readings, such as correlation with time to coating failure and coefficient of variation at failure, but are composed of orders of magnitude fewer measurements. To forecast future degradation of the coating system, a marked Hawkes process models the events. We use the forecast of degradation to predict a future time of failure, exhibiting superior performance to the approach based on direct modeling of galvanic corrosion using continuous sensor measurements. While such maintenance is typically done on a regular basis, degradation models can enable informed condition-based maintenance, reducing unnecessary excess maintenance and preventing unexpected failures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09706v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Repasky, Henry Yuchi, Fritz Friedersdorf, Yao Xie</dc:creator>
    </item>
    <item>
      <title>Impact of rainfall risk on rice production: realized volatility in mean model</title>
      <link>https://arxiv.org/abs/2504.10121</link>
      <description>arXiv:2504.10121v1 Announce Type: new 
Abstract: Rural economies are largely dependent upon agriculture, which is greatly determined by climatic conditions such as rainfall. This study aims to forecast agricultural production in Maharashtra, India, which utilises annual data from the year 1962 to 2021. Since rainfall plays a major role with respect to the crop yield, we analyze the impact of rainfall on crop yield using four time series models that includes ARIMA, ARIMAX, GARCH-ARIMA and GARCH-ARIMAX. We take advantage of rainfall as an external regressor to examine if it contributes to the performance of the model. 1-step, 2-step, and 3-step ahead forecasts are obtained and the model performance is assessed using MAE and RMSE. The models are able to more accurately predict when using rainfall as a predictor compared to when solely dependant on historical production trends (more improved outcomes are seen in the ARIMAX and GARCH-ARIMAX models). As such, these findings underscore the need for climate-aware forecasting techniques that provide useful information to policymakers and farmers to aid in agricultural planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10121v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Soham Ghosh, Sujay Mukhoti, Pritee Sharma</dc:creator>
    </item>
    <item>
      <title>Dynamic Topic Analysis in Academic Journals using Convex Non-negative Matrix Factorization Method</title>
      <link>https://arxiv.org/abs/2504.08743</link>
      <description>arXiv:2504.08743v1 Announce Type: cross 
Abstract: With the rapid advancement of large language models, academic topic identification and topic evolution analysis are crucial for enhancing AI's understanding capabilities. Dynamic topic analysis provides a powerful approach to capturing and understanding the temporal evolution of topics in large-scale datasets. This paper presents a two-stage dynamic topic analysis framework that incorporates convex optimization to improve topic consistency, sparsity, and interpretability. In Stage 1, a two-layer non-negative matrix factorization (NMF) model is employed to extract annual topics and identify key terms. In Stage 2, a convex optimization algorithm refines the dynamic topic structure using the convex NMF (cNMF) model, further enhancing topic integration and stability. Applying the proposed method to IEEE journal abstracts from 2004 to 2022 effectively identifies and quantifies emerging research topics, such as COVID-19 and digital twins. By optimizing sparsity differences in the clustering feature space between traditional and emerging research topics, the framework provides deeper insights into topic evolution and ranking analysis. Moreover, the NMF-cNMF model demonstrates superior stability in topic consistency. At sparsity levels of 0.4, 0.6, and 0.9, the proposed approach improves topic ranking stability by 24.51%, 56.60%, and 36.93%, respectively. The source code (to be open after publication) is available at https://github.com/meetyangyang/CDNMF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08743v1</guid>
      <category>cs.IR</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Yang, Tong Zhang, Jian Wu, Lijie Su</dc:creator>
    </item>
    <item>
      <title>DataMap: A Portable Application for Visualizing High-Dimensional Data</title>
      <link>https://arxiv.org/abs/2504.08875</link>
      <description>arXiv:2504.08875v1 Announce Type: cross 
Abstract: Motivation: The visualization and analysis of high-dimensional data are essential in biomedical research. There is a need for secure, scalable, and reproducible tools to facilitate data exploration and interpretation. Results: We introduce DataMap, a browser-based application for visualization of high-dimensional data using heatmaps, principal component analysis (PCA), and t-distributed stochastic neighbor embedding (t-SNE). DataMap runs in the web browser, ensuring data privacy while eliminating the need for installation or a server. The application has an intuitive user interface for data transformation, annotation, and generation of reproducible R code. Availability and Implementation: Freely available as a GitHub page https://gexijin.github.io/datamap/. The source code can be found at https://github.com/gexijin/datamap, and can also be installed as an R package. Contact: Xijin.Ge@sdstate.ed</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.08875v1</guid>
      <category>q-bio.QM</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Xijin Ge</dc:creator>
    </item>
    <item>
      <title>Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems</title>
      <link>https://arxiv.org/abs/2504.09310</link>
      <description>arXiv:2504.09310v1 Announce Type: cross 
Abstract: AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address "what if" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09310v1</guid>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Osvaldo Simeone, Sangwoo Park, Matteo Zecchin</dc:creator>
    </item>
    <item>
      <title>Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization</title>
      <link>https://arxiv.org/abs/2504.09629</link>
      <description>arXiv:2504.09629v1 Announce Type: cross 
Abstract: Layer-wise post-training quantization has emerged as a widely used technique for compressing large language models (LLMs) without retraining. However, recent progress in this line of research is saturating, underscoring the need to revisit its core limitation and explore further improvements. This study identifies a critical bottleneck in existing layer-wise PTQ methods: the accumulation of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this, we propose Quantization Error Propagation (QEP), a lightweight and general framework that enhances layer-wise PTQ by explicitly propagating the quantization error which enable compensating for accumulated quantization errors. Additionally, we introduce a tunable propagation mechanism that allows for control over both propagation strength and computational overhead, making the framework adaptable to various architectures and resource constraints. Empirical evaluation on LLaMA2 models (7B, 13B, 70B) demonstrate that incorporating QEP into standard layer-wise PTQ pipelines outperforms standard PTQ methods. Notably, QEP yields substantial performance improvements under extreme low-bit quantization settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09629v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yamato Arai, Yuma Ichikawa</dc:creator>
    </item>
    <item>
      <title>An Adaptive Multivariate Functional Control Chart</title>
      <link>https://arxiv.org/abs/2504.09684</link>
      <description>arXiv:2504.09684v1 Announce Type: cross 
Abstract: New data acquisition technologies allow one to gather huge amounts of data that are best represented as functional data. In this setting, profile monitoring assesses the stability over time of both univariate and multivariate functional quality characteristics. The detection power of profile monitoring methods could heavily depend on parameter selection criteria, which usually do not take into account any information from the out-of-control (OC) state. This work proposes a new framework, referred to as adaptive multivariate functional control chart (AMFCC), capable of adapting the monitoring of a multivariate functional quality characteristic to the unknown OC distribution, by combining $p$-values of the partial tests corresponding to Hotelling $T^2$-type statistics calculated at different parameter combinations. Through an extensive Monte Carlo simulation study, the performance of AMFCC is compared with methods that have already appeared in the literature. Finally, a case study is presented in which the proposed framework is used to monitor a resistance spot welding process in the automotive industry. AMFCC is implemented in the R package funcharts, available on CRAN.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09684v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabio Centofanti, Antonio Lepore, Biagio Palumbo</dc:creator>
    </item>
    <item>
      <title>Offline Dynamic Inventory and Pricing Strategy: Addressing Censored and Dependent Demand</title>
      <link>https://arxiv.org/abs/2504.09831</link>
      <description>arXiv:2504.09831v1 Announce Type: cross 
Abstract: In this paper, we study the offline sequential feature-based pricing and inventory control problem where the current demand depends on the past demand levels and any demand exceeding the available inventory is lost. Our goal is to leverage the offline dataset, consisting of past prices, ordering quantities, inventory levels, covariates, and censored sales levels, to estimate the optimal pricing and inventory control policy that maximizes long-term profit. While the underlying dynamic without censoring can be modeled by Markov decision process (MDP), the primary obstacle arises from the observed process where demand censoring is present, resulting in missing profit information, the failure of the Markov property, and a non-stationary optimal policy. To overcome these challenges, we first approximate the optimal policy by solving a high-order MDP characterized by the number of consecutive censoring instances, which ultimately boils down to solving a specialized Bellman equation tailored for this problem. Inspired by offline reinforcement learning and survival analysis, we propose two novel data-driven algorithms to solving these Bellman equations and, thus, estimate the optimal policy. Furthermore, we establish finite sample regret bounds to validate the effectiveness of these algorithms. Finally, we conduct numerical experiments to demonstrate the efficacy of our algorithms in estimating the optimal policy. To the best of our knowledge, this is the first data-driven approach to learning optimal pricing and inventory control policies in a sequential decision-making environment characterized by censored and dependent demand. The implementations of the proposed algorithms are available at https://github.com/gundemkorel/Inventory_Pricing_Control</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09831v1</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Korel Gundem, Zhengling Qi</dc:creator>
    </item>
    <item>
      <title>To Buy an Electric Vehicle or Not? A Bayesian Analysis of Consumer Intent in the United States</title>
      <link>https://arxiv.org/abs/2504.09854</link>
      <description>arXiv:2504.09854v1 Announce Type: cross 
Abstract: The adoption of electric vehicles (EVs) is considered critical to achieving climate goals, yet it hinges on consumer interest. This study explores how public intent to purchase EVs relates to four unexamined factors: exposure to EV information, perceptions of EVs' environmental benefits, views on government climate policy, and confidence in future EV infrastructure; while controlling for prior EV ownership, political affiliation, and demographic characteristics (e.g., age, gender, education, and geographic location). We utilize data from three nationally representative opinion polls conducted by the Pew Research Center between 2021 and 2023, and employ Bayesian techniques to estimate the ordinal probit and ordinal quantile models. Results from ordinal probit show that respondents who are well-informed about EVs, perceive them as environmentally beneficial, or are confident in development of charging stations are more likely to express strong interest in buying an EV, with covariate effects--a metric rarely reported in EV research--of 10.2, 15.5, and 19.1 percentage points, respectively. In contrast, those skeptical of government climate initiatives are more likely to express no interest, by more than 10 percentage points. Prior EV ownership exhibits the highest covariate effect (ranging from 19.0 to 23.1 percentage points), and the impact of most demographic variables is consistent with existing studies. The ordinal quantile models demonstrate significant variation in covariate effects across the distribution of EV purchase intent, offering insights beyond the ordinal probit model. This article is the first to use quantile modeling to reveal how covariate effects differ significantly throughout the spectrum of EV purchase intent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09854v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nafisa Lohawala, Mohammad Arshad Rahman</dc:creator>
    </item>
    <item>
      <title>Multi-objective Bayesian Optimization With Mixed-categorical Design Variables for Expensive-to-evaluate Aeronautical Applications</title>
      <link>https://arxiv.org/abs/2504.09930</link>
      <description>arXiv:2504.09930v1 Announce Type: cross 
Abstract: This work aims at developing new methodologies to optimize computational costly complex systems (e.g., aeronautical engineering systems). The proposed surrogate-based method (often called Bayesian optimization) uses adaptive sampling to promote a trade-off between exploration and exploitation. Our in-house implementation, called SEGOMOE, handles a high number of design variables (continuous, discrete or categorical) and nonlinearities by combining mixtures of experts for the objective and/or the constraints. Additionally, the method handles multi-objective optimization settings, as it allows the construction of accurate Pareto fronts with a minimal number of function evaluations. Different infill criteria have been implemented to handle multiple objectives with or without constraints. The effectiveness of the proposed method was tested on practical aeronautical applications within the context of the European Project AGILE 4.0 and demonstrated favorable results. A first example concerns a retrofitting problem where a comparison between two optimizers have been made. A second example introduces hierarchical variables to deal with architecture system in order to design an aircraft family. The third example increases drastically the number of categorical variables as it combines aircraft design, supply chain and manufacturing process. In this article, we show, on three different realistic problems, various aspects of our optimization codes thanks to the diversity of the treated aircraft problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09930v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>AEROBEST 2023. Vol. 1. No. 1. 2023</arxiv:journal_reference>
      <dc:creator>Nathalie Bartoli, Thierry Lefebvre, R\'emi Lafage, Paul Saves, Youssef Diouane, Joseph Morlier, Jasper Bussemaker, Giuseppa Donelli, Joao Marcos Gomes de Mello, Massimo Mandorino, Pierluigi Della Vecchia</dc:creator>
    </item>
    <item>
      <title>An Image is Worth $K$ Topics: A Visual Structural Topic Model with Pretrained Image Embeddings</title>
      <link>https://arxiv.org/abs/2504.10004</link>
      <description>arXiv:2504.10004v1 Announce Type: cross 
Abstract: Political scientists are increasingly interested in analyzing visual content at scale. However, the existing computational toolbox is still in need of methods and models attuned to the specific challenges and goals of social and political inquiry. In this article, we introduce a visual Structural Topic Model (vSTM) that combines pretrained image embeddings with a structural topic model. This has important advantages compared to existing approaches. First, pretrained embeddings allow the model to capture the semantic complexity of images relevant to political contexts. Second, the structural topic model provides the ability to analyze how topics and covariates are related, while maintaining a nuanced representation of images as a mixture of multiple topics. In our empirical application, we show that the vSTM is able to identify topics that are interpretable, coherent, and substantively relevant to the study of online political communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10004v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mat\'ias Piqueras, Alexandra Segerberg, Matteo Magnani, M{\aa}ns Magnusson, Nata\v{s}a Sladoje</dc:creator>
    </item>
    <item>
      <title>Enhancing Predictive Accuracy in Tennis: Integrating Fuzzy Logic and CV-GRNN for Dynamic Match Outcome and Player Momentum Analysis</title>
      <link>https://arxiv.org/abs/2503.21809</link>
      <description>arXiv:2503.21809v2 Announce Type: replace 
Abstract: The predictive analysis of match outcomes and player momentum in professional tennis has long been a subject of scholarly debate. In this paper, we introduce a novel approach to game prediction by combining a multi-level fuzzy evaluation model with a CV-GRNN model. We first identify critical statistical indicators via Principal Component Analysis and then develop a two-tier fuzzy model based on the Wimbledon data. In addition, the results of Pearson Correlation Coefficient indicate that the momentum indicators, such as Player Win Streak and Score Difference, have a strong correlation among them, revealing insightful trends among players transitioning between losing and winning streaks. Subsequently, we refine the CV-GRNN model by incorporating 15 statistically significant indicators, resulting in an increase in accuracy to 86.64% and a decrease in MSE by 49.21%. This consequently strengthens the methodological framework for predicting tennis match outcomes, emphasizing its practical utility and potential for adaptation in various athletic contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21809v2</guid>
      <category>stat.AP</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kechen Li, Jiaming Liu, Zhenyu Wu, Tianbo Ji</dc:creator>
    </item>
    <item>
      <title>Bot Identification in Social Media</title>
      <link>https://arxiv.org/abs/2503.23629</link>
      <description>arXiv:2503.23629v2 Announce Type: replace 
Abstract: Escalating proliferation of inorganic accounts, commonly known as bots, within the digital ecosystem represents an ongoing and multifaceted challenge to online security, trustworthiness, and user experience. These bots, often employed for the dissemination of malicious propaganda and manipulation of public opinion, wield significant influence in social media spheres with far-reaching implications for electoral processes, political campaigns and international conflicts. Swift and accurate identification of inorganic accounts is of paramount importance in mitigating their detrimental effects. This research paper focuses on the identification of such accounts and explores various effective methods for their detection through machine learning techniques. In response to the pervasive presence of bots in the contemporary digital landscape, this study extracts temporal and semantic features from tweet behaviors and proposes a bot detection algorithm utilizing fundamental machine learning approaches, including Support Vector Machines (SVM) and k-means clustering. Furthermore, the research ranks the importance of these extracted features for each detection technique and also provides uncertainty quantification using a distribution free method, called the conformal prediction, thereby contributing to the development of effective strategies for combating the prevalence of inorganic accounts in social media platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.23629v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dhrubajyoti Ghosh, William Boettcher, Rob Johnston, Soumendra Lahiri</dc:creator>
    </item>
    <item>
      <title>Moment-based Density Elicitation with Applications in Probabilistic Loops</title>
      <link>https://arxiv.org/abs/2304.09094</link>
      <description>arXiv:2304.09094v2 Announce Type: replace-cross 
Abstract: We propose the K-series estimation approach for the recovery of unknown univariate and multivariate distributions given knowledge of a finite number of their moments. Our method is directly applicable to the probabilistic analysis of systems that can be represented as probabilistic loops; i.e., algorithms that express and implement non-deterministic processes ranging from robotics to macroeconomics and biology to software and cyber-physical systems. K-series statically approximates the joint and marginal distributions of a vector of continuous random variables updated in a probabilistic non-nested loop with nonlinear assignments given a finite number of moments of the unknown density. Moreover, K-series automatically derives the distribution of the systems' random variables symbolically as a function of the loop iteration. K-series density estimates are accurate, easy and fast to compute. We demonstrate the feasibility and performance of our approach on multiple benchmark examples from the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09094v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>cs.SC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3728648</arxiv:DOI>
      <dc:creator>Andrey Kofnov, Ezio Bartocci, Efstathia Bura</dc:creator>
    </item>
    <item>
      <title>Learning Fair Decisions with Factor Models: Applications to Annuity Pricing</title>
      <link>https://arxiv.org/abs/2412.04663</link>
      <description>arXiv:2412.04663v2 Announce Type: replace-cross 
Abstract: Fairness-aware statistical learning is essential for mitigating discrimination against protected attributes such as gender, race, and ethnicity in data-driven decision-making. This is particularly critical in high-stakes applications like insurance underwriting and annuity pricing, where biased business decisions can have significant financial and social consequences. Factor models are commonly used in these domains for risk assessment and pricing; however, their predictive outputs may inadvertently introduce or amplify bias. To address this, we propose a Fair Decision Model that incorporates fairness regularization to mitigate outcome disparities. Specifically, the model is designed to ensure that expected decision errors are balanced across demographic groups - a criterion we refer to as Decision Error Parity. We apply this framework to annuity pricing based on mortality modelling. An empirical analysis using Australian mortality data demonstrates that the Fair Decision Model can significantly reduce decision error disparity while also improving predictive accuracy compared to benchmark models, including both traditional and fair factor models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.04663v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fei Huang, Junhao Shen, Yanrong Yang, Ran Zhao</dc:creator>
    </item>
    <item>
      <title>Fiducial Confidence Intervals for Agreement Measures Among Raters Under a Generalized Linear Mixed Effects Model</title>
      <link>https://arxiv.org/abs/2503.04117</link>
      <description>arXiv:2503.04117v2 Announce Type: replace-cross 
Abstract: A generalization of the classical concordance correlation coefficient (CCC) is considered under a three-level design where multiple raters rate every subject over time, and each rater is rating every subject multiple times at each measuring time point. The ratings can be discrete or continuous. A methodology is developed for the interval estimation of the CCC based on a suitable linearization of the model along with an adaptation of the fiducial inference approach. The resulting confidence intervals have satisfactory coverage probabilities and shorter expected widths compared to the interval based on Fisher Z-transformation, even under moderate sample sizes. Two real applications available in the literature are discussed. The first application is based on a clinical trial to determine if various treatments are more effective than a placebo for treating knee pain associated with osteoarthritis. The CCC was used to assess agreement among the manual measurements of the joint space widths on plain radiographs by two raters, and the computer-generated measurements of digitalized radiographs. The second example is on a corticospinal tractography, and the CCC was once again applied in order to evaluate the agreement between a well-trained technologist and a neuroradiologist regarding the measurements of fiber number in both the right and left corticospinal tracts. Other relevant applications of our general approach are highlighted in many areas including artificial intelligence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04117v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soumya Sahu, Thomas Mathew, Dulal K. Bhaumik</dc:creator>
    </item>
    <item>
      <title>Causal Networks of Infodemiological Data: Modelling Dermatitis</title>
      <link>https://arxiv.org/abs/2503.08252</link>
      <description>arXiv:2503.08252v2 Announce Type: replace-cross 
Abstract: Environmental and mental conditions are known risk factors for dermatitis and symptoms of skin inflammation, but their interplay is difficult to quantify; epidemiological studies rarely include both, along with possible confounding factors. Infodemiology leverages large online data sets to address this issue, but fusing them produces strong patterns of spatial and temporal correlation, missingness, and heterogeneity.
  In this paper, we design a causal network that correctly models these complex structures in large-scale infodemiological data from Google, EPA, NOAA and US Census (434 US counties, 134 weeks). Our model successfully captures known causal relationships between weather patterns, pollutants, mental conditions, and dermatitis. Key findings reveal that anxiety accounts for 57.4% of explained variance in dermatitis, followed by NO2 (33.9%), while environmental factors show significant mediation effects through mental conditions. The model predicts that reducing PM2.5 emissions by 25% could decrease dermatitis prevalence by 18%. Through statistical validation and causal inference, we provide unprecedented insights into the complex interplay between environmental and mental health factors affecting dermatitis, offering valuable guidance for public health policies and environmental regulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.08252v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marco Scutari, Samir Salah, Delphine Kerob, Jean Krutmann</dc:creator>
    </item>
  </channel>
</rss>
