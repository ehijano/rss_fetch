<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 08 Jul 2024 04:06:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 08 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Under the null of valid specification, pre-tests of valid specification do not distort inference</title>
      <link>https://arxiv.org/abs/2407.03725</link>
      <description>arXiv:2407.03725v1 Announce Type: new 
Abstract: Consider a parameter of interest, which can be consistently estimated under some conditions. Suppose also that we can at least partly test these conditions with specification tests. We consider the common practice of conducting inference on the parameter of interest conditional on not rejecting these tests. We show that if the tested conditions hold, conditional inference is valid but possibly conservative. This holds generally, without imposing any assumption on the asymptotic dependence between the estimator of the parameter of interest and the specification test.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03725v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Cl\'ement de Chaisemartin, Xavier D'Haultf{\oe}uille</dc:creator>
    </item>
    <item>
      <title>Overeducation under different macroeconomic conditions: The case of Spanish university graduates</title>
      <link>https://arxiv.org/abs/2407.04437</link>
      <description>arXiv:2407.04437v1 Announce Type: new 
Abstract: This paper examines the incidence and persistence of overeducation in the early careers of Spanish university graduates. We investigate the role played by the business cycle and field of study and their interaction in shaping both phenomena. We also analyse the relevance of specific types of knowledge and skills as driving factors in reducing overeducation risk. We use data from the Survey on the Labour Insertion of University Graduates (EILU) conducted by the Spanish National Statistics Institute in 2014 and 2019. The survey collects rich information on cohorts that graduated in the 2009/2010 and 2014/2015 academic years during the Great Recession and the subsequent economic recovery, respectively. Our results show, first, the relevance of the economic scenario when graduates enter the labour market. Graduation during a recession increased overeducation risk and persistence. Second, a clear heterogeneous pattern occurs across fields of study, with health sciences graduates displaying better performance in terms of both overeducation incidence and persistence and less impact of the business cycle. Third, we find evidence that some transversal skills (language, IT, management) can help to reduce overeducation risk in the absence of specific knowledge required for the job, thus indicating some kind of compensatory role. Finally, our findings have important policy implications. Overeducation, and more importantly overeducation persistence, imply a non-neglectable misallocation of resources. Therefore, policymakers need to address this issue in the design of education and labour market policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04437v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maite Bl\'azquez Cuesta, Marco A. P\'erez Navarro, Roc\'io S\'anchez-Mangas</dc:creator>
    </item>
    <item>
      <title>Learning control variables and instruments for causal analysis in observational data</title>
      <link>https://arxiv.org/abs/2407.04448</link>
      <description>arXiv:2407.04448v1 Announce Type: new 
Abstract: This study introduces a data-driven, machine learning-based method to detect suitable control variables and instruments for assessing the causal effect of a treatment on an outcome in observational data, if they exist. Our approach tests the joint existence of instruments, which are associated with the treatment but not directly with the outcome (at least conditional on observables), and suitable control variables, conditional on which the treatment is exogenous, and learns the partition of instruments and control variables from the observed data. The detection of sets of instruments and control variables relies on the condition that proper instruments are conditionally independent of the outcome given the treatment and suitable control variables. We establish the consistency of our method for detecting control variables and instruments under certain regularity conditions, investigate the finite sample performance through a simulation study, and provide an empirical application to labor market data from the Job Corps study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04448v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Apfel, Julia Hatamyar, Martin Huber, Jannis Kueck</dc:creator>
    </item>
    <item>
      <title>When can weak latent factors be statistically inferred?</title>
      <link>https://arxiv.org/abs/2407.03616</link>
      <description>arXiv:2407.03616v1 Announce Type: cross 
Abstract: This article establishes a new and comprehensive estimation and inference theory for principal component analysis (PCA) under the weak factor model that allow for cross-sectional dependent idiosyncratic components under nearly minimal the factor strength relative to the noise level or signal-to-noise ratio. Our theory is applicable regardless of the relative growth rate between the cross-sectional dimension $N$ and temporal dimension $T$. This more realistic assumption and noticeable result requires completely new technical device, as the commonly-used leave-one-out trick is no longer applicable to the case with cross-sectional dependence. Another notable advancement of our theory is on PCA inference $ - $ for example, under the regime where $N\asymp T$, we show that the asymptotic normality for the PCA-based estimator holds as long as the signal-to-noise ratio (SNR) grows faster than a polynomial rate of $\log N$. This finding significantly surpasses prior work that required a polynomial rate of $N$. Our theory is entirely non-asymptotic, offering finite-sample characterizations for both the estimation error and the uncertainty level of statistical inference. A notable technical innovation is our closed-form first-order approximation of PCA-based estimator, which paves the way for various statistical tests. Furthermore, we apply our theories to design easy-to-implement statistics for validating whether given factors fall in the linear spans of unknown latent factors, testing structural breaks in the factor loadings for an individual unit, checking whether two units have the same risk exposures, and constructing confidence intervals for systematic risks. Our empirical studies uncover insightful correlations between our test results and economic cycles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.03616v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jianqing Fan, Yuling Yan, Yuheng Zheng</dc:creator>
    </item>
    <item>
      <title>Identification of Average Marginal Effects in Fixed Effects Dynamic Discrete Choice Models</title>
      <link>https://arxiv.org/abs/2107.06141</link>
      <description>arXiv:2107.06141v2 Announce Type: replace 
Abstract: In nonlinear panel data models, fixed effects methods are often criticized because they cannot identify average marginal effects (AMEs) in short panels. The common argument is that identifying AMEs requires knowledge of the distribution of unobserved heterogeneity, but this distribution is not identified in a fixed effects model with a short panel. In this paper, we derive identification results that contradict this argument. In a panel data dynamic logit model, and for $T$ as small as three, we prove the point identification of different AMEs, including causal effects of changes in the lagged dependent variable or the last choice's duration. Our proofs are constructive and provide simple closed-form expressions for the AMEs in terms of probabilities of choice histories. We illustrate our results using Monte Carlo experiments and with an empirical application of a dynamic structural model of consumer brand choice with state dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2107.06141v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Aguirregabiria, Jesus M. Carro</dc:creator>
    </item>
    <item>
      <title>Trimmed Mean Group Estimation of Average Effects in Ultra Short T Panels under Correlated Heterogeneity</title>
      <link>https://arxiv.org/abs/2310.11680</link>
      <description>arXiv:2310.11680v2 Announce Type: replace 
Abstract: The commonly used two-way fixed effects estimator is biased under correlated heterogeneity and can lead to misleading inference. This paper proposes a new trimmed mean group (TMG) estimator which is consistent at the irregular rate of n^{1/3} even if the time dimension of the panel is as small as the number of its regressors. Extensions to panels with time effects are provided, and a Hausman test of correlated heterogeneity is proposed. Small sample properties of the TMG estimator (with and without time effects) are investigated by Monte Carlo experiments and shown to be satisfactory and perform better than other trimmed estimators proposed in the literature. The proposed test of correlated heterogeneity is also shown to have the correct size and satisfactory power. The utility of the TMG approach is illustrated with an empirical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.11680v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. Hashem Pesaran, Liying Yang</dc:creator>
    </item>
    <item>
      <title>A Gaussian smooth transition vector autoregressive model: An application to the macroeconomic effects of severe weather shocks</title>
      <link>https://arxiv.org/abs/2403.14216</link>
      <description>arXiv:2403.14216v2 Announce Type: replace 
Abstract: We introduce a new smooth transition vector autoregressive model with a Gaussian conditional distribution and transition weights that, for a $p$th order model, depend on the full distribution of the preceding $p$ observations. Specifically, the transition weight of each regime increases in its relative weighted likelihood. This data-driven approach facilitates capturing complex switching dynamics, enhancing the identification of gradual regime shifts. In an empirical application to the macroeconomic effects of a severe weather shock, we find that in monthly U.S. data from 1961:1 to 2022:3, the impacts of the shock are stronger in the regime prevailing in the early part of the sample and in certain crisis periods than in the regime dominating the latter part of the sample. This suggests overall adaptation of the U.S. economy to increased severe weather over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14216v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markku Lanne, Savi Virolainen</dc:creator>
    </item>
    <item>
      <title>Fast and simple inner-loop algorithms of static / dynamic BLP estimations</title>
      <link>https://arxiv.org/abs/2404.04494</link>
      <description>arXiv:2404.04494v3 Announce Type: replace 
Abstract: This study investigates computationally efficient inner-loop algorithms for estimating static/dynamic BLP models. It provides the following ideas to reduce the number of inner-loop iterations: (1). Add a term concerning the outside option share in the BLP contraction mapping; (2). Analytically represent mean product utilities as a function of value functions and solve for the value functions (for dynamic BLP); (3-1). Combine the spectral/SQUAREM algorithm; (3-2). Choice of the step sizes. They are independent and easy to implement. This study shows good performance of these methods by numerical experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.04494v3</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Takeshi Fukasawa</dc:creator>
    </item>
    <item>
      <title>Testing the Fairness-Accuracy Improvability of Algorithms</title>
      <link>https://arxiv.org/abs/2405.04816</link>
      <description>arXiv:2405.04816v3 Announce Type: replace 
Abstract: Many organizations use algorithms that have a disparate impact, i.e., the benefits or harms of the algorithm fall disproportionately on certain social groups. Addressing an algorithm's disparate impact can be challenging, however, because it is often unclear whether it is possible to reduce this impact without sacrificing other objectives of the organization, such as accuracy or profit. Establishing the improvability of algorithms with respect to multiple criteria is of both conceptual and practical interest: in many settings, disparate impact that would otherwise be prohibited under US federal law is permissible if it is necessary to achieve a legitimate business interest. The question is how a policy-maker can formally substantiate, or refute, this "necessity" defense. In this paper, we provide an econometric framework for testing the hypothesis that it is possible to improve on the fairness of an algorithm without compromising on other pre-specified objectives. Our proposed test is simple to implement and can be applied under any exogenous constraint on the algorithm space. We establish the large-sample validity and consistency of our test, and illustrate its practical application by evaluating a healthcare algorithm originally considered by Obermeyer et al. (2019). In this application, we reject the null hypothesis that it is not possible to reduce the algorithm's disparate impact without compromising the accuracy of its predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04816v3</guid>
      <category>econ.EM</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eric Auerbach, Annie Liang, Kyohei Okumura, Max Tabord-Meehan</dc:creator>
    </item>
    <item>
      <title>Estimating Treatment Effects under Recommender Interference: A Structured Neural Networks Approach</title>
      <link>https://arxiv.org/abs/2406.14380</link>
      <description>arXiv:2406.14380v3 Announce Type: replace 
Abstract: Recommender systems are essential for content-sharing platforms by curating personalized content. To evaluate updates to recommender systems targeting content creators, platforms frequently rely on creator-side randomized experiments. The treatment effect measures the change in outcomes when a new algorithm is implemented compared to the status quo. We show that the standard difference-in-means estimator can lead to biased estimates due to recommender interference that arises when treated and control creators compete for exposure. We propose a "recommender choice model" that describes which item gets exposed from a pool containing both treated and control items. By combining a structural choice model with neural networks, this framework directly models the interference pathway while accounting for rich viewer-content heterogeneity. We construct a debiased estimator of the treatment effect and prove it is $\sqrt n$-consistent and asymptotically normal with potentially correlated samples. We validate our estimator's empirical performance with a field experiment on Weixin short-video platform. In addition to the standard creator-side experiment, we conduct a costly double-sided randomization design to obtain a benchmark estimate free from interference bias. We show that the proposed estimator yields results comparable to the benchmark, whereas the standard difference-in-means estimator can exhibit significant bias and even produce reversed signs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14380v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruohan Zhan, Shichao Han, Yuchen Hu, Zhenling Jiang</dc:creator>
    </item>
    <item>
      <title>Kernel Ridge Riesz Representers: Generalization, Mis-specification, and the Counterfactual Effective Dimension</title>
      <link>https://arxiv.org/abs/2102.11076</link>
      <description>arXiv:2102.11076v4 Announce Type: replace-cross 
Abstract: Kernel balancing weights provide confidence intervals for average treatment effects, based on the idea of balancing covariates for the treated group and untreated group in feature space, often with ridge regularization. Previous works on the classical kernel ridge balancing weights have certain limitations: (i) not articulating generalization error for the balancing weights, (ii) typically requiring correct specification of features, and (iii) justifying Gaussian approximation for only average effects.
  I interpret kernel balancing weights as kernel ridge Riesz representers (KRRR) and address these limitations via a new characterization of the counterfactual effective dimension. KRRR is an exact generalization of kernel ridge regression and kernel ridge balancing weights. I prove strong properties similar to kernel ridge regression: population $L_2$ rates controlling generalization error, and a standalone closed form solution that can interpolate. The framework relaxes the stringent assumption that the underlying regression model is correctly specified by the features. It extends Gaussian approximation beyond average effects to heterogeneous effects, justifying confidence sets for causal functions. I use KRRR to quantify uncertainty for heterogeneous treatment effects, by age, of 401(k) eligibility on assets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2102.11076v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rahul Singh</dc:creator>
    </item>
    <item>
      <title>Finite Tests from Functional Characterizations</title>
      <link>https://arxiv.org/abs/2208.03737</link>
      <description>arXiv:2208.03737v5 Announce Type: replace-cross 
Abstract: Classically, testing whether decision makers belong to specific preference classes involves two main approaches. The first, known as the functional approach, assumes access to an entire demand function. The second, the revealed preference approach, constructs inequalities to test finite demand data. This paper bridges these methods by using the functional approach to test finite data through preference learnability results. We develop a computationally efficient algorithm that generates tests for choice data based on functional characterizations of preference families. We provide these restrictions for various applications, including homothetic and weakly separable preferences, where the latter's revealed preference characterization is provably NP-Hard. We also address choice under uncertainty, offering tests for betweenness preferences. Lastly, we perform a simulation exercise demonstrating that our tests are effective in finite samples and accurately reject demands not belonging to a specified class.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.03737v5</guid>
      <category>econ.TH</category>
      <category>econ.EM</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Gauthier, Raghav Malhotra, Agustin Troccoli Moretti</dc:creator>
    </item>
    <item>
      <title>Bootstrap-Assisted Inference for Generalized Grenander-type Estimators</title>
      <link>https://arxiv.org/abs/2303.13598</link>
      <description>arXiv:2303.13598v3 Announce Type: replace-cross 
Abstract: Westling and Carone (2020) proposed a framework for studying the large sample distributional properties of generalized Grenander-type estimators, a versatile class of nonparametric estimators of monotone functions. The limiting distribution of those estimators is representable as the left derivative of the greatest convex minorant of a Gaussian process whose monomial mean can be of unknown order (when the degree of flatness of the function of interest is unknown). The standard nonparametric bootstrap is unable to consistently approximate the large sample distribution of the generalized Grenander-type estimators even if the monomial order of the mean is known, making statistical inference a challenging endeavour in applications. To address this inferential problem, we present a bootstrap-assisted inference procedure for generalized Grenander-type estimators. The procedure relies on a carefully crafted, yet automatic, transformation of the estimator. Moreover, our proposed method can be made ``flatness robust'' in the sense that it can be made adaptive to the (possibly unknown) degree of flatness of the function of interest. The method requires only the consistent estimation of a single scalar quantity, for which we propose an automatic procedure based on numerical derivative estimation and the generalized jackknife. Under random sampling, our inference method can be implemented using a computationally attractive exchangeable bootstrap procedure. We illustrate our methods with examples and we also provide a small simulation study. The development of formal results is made possible by some technical results that may be of independent interest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.13598v3</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matias D. Cattaneo, Michael Jansson, Kenichi Nagasawa</dc:creator>
    </item>
  </channel>
</rss>
