<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 03 Feb 2025 05:00:05 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Greedy Stein Variational Gradient Descent: An algorithmic approach for wave prospection problems</title>
      <link>https://arxiv.org/abs/2501.19370</link>
      <description>arXiv:2501.19370v1 Announce Type: new 
Abstract: In this project, we propose a Variational Inference algorithm to approximate posterior distributions. Building on prior methods, we develop the Gradient-Steered Stein Variational Gradient Descent (G-SVGD) approach. This method introduces a novel loss function that combines a weighted gradient and the Evidence Lower Bound (ELBO) to enhance convergence speed and accuracy. The learning rate is determined through a suboptimal minimization of this loss function within a gradient descent framework.
  The G-SVGD method is compared against the standard Stein Variational Gradient Descent (SVGD) approach, employing the ADAM optimizer for learning rate adaptation, as well as the Markov Chain Monte Carlo (MCMC) method. We assess performance in two wave prospection models representing low-contrast and high-contrast subsurface scenarios. To achieve robust numerical approximations in the forward model solver, a five-point operator is employed, while the adjoint method improves accuracy in computing gradients of the log posterior.
  Our findings demonstrate that G-SVGD accelerates convergence and offers improved performance in scenarios where gradient evaluation is computationally expensive. The abstract highlights the algorithm's applicability to wave prospection models and its potential for broader applications in Bayesian inference. Finally, we discuss the benefits and limitations of G-SVGD, emphasizing its contribution to advancing computational efficiency in uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19370v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose L. Varona-Santana, Marcos A. Capistr\'an</dc:creator>
    </item>
    <item>
      <title>Lightspeed Geometric Dataset Distance via Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.18901</link>
      <description>arXiv:2501.18901v1 Announce Type: cross 
Abstract: We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18901v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho</dc:creator>
    </item>
    <item>
      <title>The Physicist's Guide to the HMC</title>
      <link>https://arxiv.org/abs/2501.19130</link>
      <description>arXiv:2501.19130v1 Announce Type: cross 
Abstract: The hybrid Monte Carlo (HMC) algorithm is arguably the most efficient sampling method for general probability distributions of continuous variables. Together with exact Fourier acceleration (EFA) the HMC becomes equivalent to direct sampling for quadratic actions $S(x)=\frac12 x^\mathsf{T} M x$ (i.e. normal distributions $x\sim \mathrm{e}^{-S(x)}$), only perturbatively worse for perturbative deviations of the action from the quadratic case, and it remains viable for arbitrary actions. In this work the most recent improvements of the HMC including EFA and radial updates are collected into a numerical recipe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19130v1</guid>
      <category>hep-lat</category>
      <category>cond-mat.str-el</category>
      <category>physics.comp-ph</category>
      <category>stat.CO</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johann Ostmeyer</dc:creator>
    </item>
    <item>
      <title>Using gradient of Lagrangian function to compute efficient channels for the ideal observer</title>
      <link>https://arxiv.org/abs/2501.19381</link>
      <description>arXiv:2501.19381v1 Announce Type: cross 
Abstract: It is widely accepted that the Bayesian ideal observer (IO) should be used to guide the objective assessment and optimization of medical imaging systems. The IO employs complete task-specific information to compute test statistics for making inference decisions and performs optimally in signal detection tasks. However, the IO test statistic typically depends non-linearly on the image data and cannot be analytically determined. The ideal linear observer, known as the Hotelling observer (HO), can sometimes be used as a surrogate for the IO. However, when image data are high dimensional, HO computation can be difficult. Efficient channels that can extract task-relevant features have been investigated to reduce the dimensionality of image data to approximate IO and HO performance. This work proposes a novel method for generating efficient channels by use of the gradient of a Lagrangian-based loss function that was designed to learn the HO. The generated channels are referred to as the Lagrangian-gradient (L-grad) channels. Numerical studies are conducted that consider binary signal detection tasks involving various backgrounds and signals. It is demonstrated that channelized HO (CHO) using L-grad channels can produce significantly better signal detection performance compared to the CHO using PLS channels. Moreover, it is shown that the proposed L-grad method can achieve significantly lower computation time compared to the PLS method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.19381v1</guid>
      <category>eess.SP</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 03 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weimin Zhou</dc:creator>
    </item>
  </channel>
</rss>
