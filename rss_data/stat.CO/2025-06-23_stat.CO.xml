<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Jun 2025 02:20:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Understanding uncertainty in Bayesian cluster analysis</title>
      <link>https://arxiv.org/abs/2506.16295</link>
      <description>arXiv:2506.16295v1 Announce Type: new 
Abstract: The Bayesian approach to clustering is often appreciated for its ability to provide uncertainty in the partition structure. However, summarizing the posterior distribution over the clustering structure can be challenging, due the discrete, unordered nature and massive dimension of the space. While recent advancements provide a single clustering estimate to represent the posterior, this ignores uncertainty and may even be unrepresentative in instances where the posterior is multimodal. To enhance our understanding of uncertainty, we propose a WASserstein Approximation for Bayesian clusterIng (WASABI), which summarizes the posterior samples with not one, but multiple clustering estimates, each corresponding to a different part of the space of partitions that receives substantial posterior mass. Specifically, we find such clustering estimates by approximating the posterior distribution in a Wasserstein distance sense, equipped with a suitable metric on the partition space. An interesting byproduct is that a locally optimal solution to this problem can be found using a k-medoids-like algorithm on the partition space to divide the posterior samples into different groups, each represented by one of the clustering estimates. Using both synthetic and real datasets, we show that our proposal helps to improve the understanding of uncertainty, particularly when the data clusters are not well separated or when the employed model is misspecified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16295v1</guid>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cecilia Balocchi, Sara Wade</dc:creator>
    </item>
    <item>
      <title>Quasi-Monte Carlo with one categorical variable</title>
      <link>https://arxiv.org/abs/2506.16582</link>
      <description>arXiv:2506.16582v1 Announce Type: new 
Abstract: We study randomized quasi-Monte Carlo (RQMC) estimation of a multivariate integral where one of the variables takes only a finite number of values. This problem arises when the variable of integration is drawn from a mixture distribution as is common in importance sampling and also arises in some recent work on transport maps. We find that when integration error decreases at an RQMC rate that it is then beneficial to oversample the smallest mixture components instead of using a proportional allocation. We also find that for the most accurate RQMC sampling methods, it is advantageous to arrange that our $n=2^m$ randomized Sobol' points split into subsample sizes that are also powers of~$2$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16582v1</guid>
      <category>stat.CO</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Valerie N. P. Ho, Art B. Owen, Zexin Pan</dc:creator>
    </item>
    <item>
      <title>Summary Statistics of Large-scale Model Outputs for Observation-corrected Outputs</title>
      <link>https://arxiv.org/abs/2506.15845</link>
      <description>arXiv:2506.15845v1 Announce Type: cross 
Abstract: Physics-based models capture broad spatial and temporal dynamics, but often suffer from biases and numerical approximations, while observations capture localized variability but are sparse. Integrating these complementary data modalities is important to improving the accuracy and reliability of model outputs. Meanwhile, physics-based models typically generate large outputs that are challenging to manipulate. In this paper, we propose Sig-PCA, a space-time framework that integrates summary statistics from model outputs with localized observations via a neural network (NN). By leveraging reduced-order representations from physics-based models and integrating them with observational data, our approach corrects model outputs, while allowing to work with dimensionally-reduced quantities hence with smaller NNs. This framework highlights the synergy between observational data and statistical summaries of model outputs, and effectively combines multisource data by preserving essential statistical information. We demonstrate our approach on two datasets (surface temperature and surface wind) with different statistical properties and different ratios of model to observational data. Our method corrects model outputs to align closely with the observational data, specifically enabling to correct probability distributions and space-time correlation structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.15845v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atlanta Chakraborty, Julie Bessac</dc:creator>
    </item>
    <item>
      <title>Leveraging Optimal Transport for Distributed Two-Sample Testing: An Integrated Transportation Distance-based Framework</title>
      <link>https://arxiv.org/abs/2506.16047</link>
      <description>arXiv:2506.16047v1 Announce Type: cross 
Abstract: This paper introduces a novel framework for distributed two-sample testing using the Integrated Transportation Distance (ITD), an extension of the Optimal Transport distance. The approach addresses the challenges of detecting distributional changes in decentralized learning or federated learning environments, where data privacy and heterogeneity are significant concerns. We provide theoretical foundations for the ITD, including convergence properties and asymptotic behavior. A permutation test procedure is proposed for practical implementation in distributed settings, allowing for efficient computation while preserving data privacy. The framework's performance is demonstrated through theoretical power analysis and extensive simulations, showing robust Type I error control and high power across various distributions and dimensions. The results indicate that ITD effectively aggregates information across distributed clients, detecting subtle distributional shifts that might be missed when examining individual clients. This work contributes to the growing field of distributed statistical inference, offering a powerful tool for two-sample testing in modern, decentralized data environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16047v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhengqi Lin, Yan Chen</dc:creator>
    </item>
    <item>
      <title>elicito: A Python Package for Expert Prior Elicitation</title>
      <link>https://arxiv.org/abs/2506.16830</link>
      <description>arXiv:2506.16830v1 Announce Type: cross 
Abstract: Expert prior elicitation plays a critical role in Bayesian analysis by enabling the specification of prior distributions that reflect domain knowledge. However, expert knowledge often refers to observable quantities rather than directly to model parameters, posing a challenge for translating this information into usable priors. We present elicito, a Python package that implements a modular, simulation-based framework for expert prior elicitation. The framework supports both structural and predictive elicitation methods and allows for flexible customization of key components, including the generative model, the form of expert input, prior assumptions (parametric or nonparametric), and loss functions. By structuring the elicitation process into configurable modules, elicito offers transparency, reproducibility, and comparability across elicitation methods. We describe the methodological foundations of the package, its software architecture, and demonstrate its functionality through a case study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16830v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Florence Bockting, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Linear Inverse Problems with Besov Prior: A Randomize-Then-Optimize Method</title>
      <link>https://arxiv.org/abs/2506.16888</link>
      <description>arXiv:2506.16888v1 Announce Type: cross 
Abstract: In this work, we investigate the use of Besov priors in the context of Bayesian inverse problems. The solution to Bayesian inverse problems is the posterior distribution which naturally enables us to interpret the uncertainties. Besov priors are discretization invariant and can promote sparsity in terms of wavelet coefficients. We propose the randomize-then-optimize method to draw samples from the posterior distribution with Besov priors under a general parameter setting and estimate the modes of the posterior distribution. The performance of the proposed method is studied through numerical experiments of a 1D inpainting problem, a 1D deconvolution problem, and a 2D computed tomography problem. Further, we discuss the influence of the choice of the Besov parameters and the wavelet basis in detail, and we compare the proposed method with the state-of-the-art methods. The numerical results suggest that the proposed method is an effective tool for sampling the posterior distribution equipped with general Besov priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.16888v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/s11222-025-10638-2</arxiv:DOI>
      <arxiv:journal_reference>Stat Comput 35, 101 (2025)</arxiv:journal_reference>
      <dc:creator>Andreas Horst, Babak Maboudi Afkham, Yiqiu Dong, Jakob Lemvig</dc:creator>
    </item>
    <item>
      <title>Tuning-Free Coreset Markov Chain Monte Carlo via Hot DoG</title>
      <link>https://arxiv.org/abs/2410.18973</link>
      <description>arXiv:2410.18973v2 Announce Type: replace 
Abstract: A Bayesian coreset is a small, weighted subset of a data set that replaces the full data during inference to reduce computational cost. The state-of-the-art coreset construction algorithm, Coreset Markov chain Monte Carlo (Coreset MCMC), uses draws from an adaptive Markov chain targeting the coreset posterior to train the coreset weights via stochastic gradient optimization. However, the quality of the constructed coreset, and thus the quality of its posterior approximation, is sensitive to the stochastic optimization learning rate. In this work, we propose a learning-rate-free stochastic gradient optimization procedure, Hot-start Distance over Gradient (Hot DoG), for training coreset weights in Coreset MCMC without user tuning effort. We provide a theoretical analysis of the convergence of the coreset weights produced by Hot DoG. We also provide empirical results demonstrate that Hot DoG provides higher quality posterior approximations than other learning-rate-free stochastic gradient methods, and performs competitively to optimally-tuned ADAM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.18973v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Naitong Chen, Jonathan H. Huggins, Trevor Campbell</dc:creator>
    </item>
    <item>
      <title>Fast data inversion for high-dimensional dynamical systems from noisy measurements</title>
      <link>https://arxiv.org/abs/2501.01324</link>
      <description>arXiv:2501.01324v3 Announce Type: replace 
Abstract: In this work, we develop a scalable approach for a flexible latent factor model for high-dimensional dynamical systems. Each latent factor process has its own correlation and variance parameters, and the orthogonal factor loading matrix can be either fixed or estimated. We utilize an orthogonal factor loading matrix that avoids computing the inversion of the posterior covariance matrix at each time of the Kalman filter, and derive closed-form expressions in an expectation-maximization algorithm for parameter estimation, which substantially reduces the computational complexity without approximation. Our study is motivated by inversely estimating slow slip events from geodetic data, such as continuous GPS measurements. Extensive simulated studies illustrate higher accuracy and scalability of our approach compared to alternatives. By applying our method to geodetic measurements in the Cascadia region, our estimated slip better agrees with independently measured seismic data of tremor events. The substantial acceleration from our method enables the use of massive noisy data for geological hazard quantification and other applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01324v3</guid>
      <category>stat.CO</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yizi Lin, Xubo Liu, Paul Segall, Mengyang Gu</dc:creator>
    </item>
    <item>
      <title>fairmetrics: An R package for group fairness evaluation</title>
      <link>https://arxiv.org/abs/2506.06243</link>
      <description>arXiv:2506.06243v2 Announce Type: replace 
Abstract: Fairness is a growing area of machine learning (ML) that focuses on ensuring models do not produce systematically biased outcomes for specific groups, particularly those defined by protected attributes such as race, gender, or age. Evaluating fairness is a critical aspect of ML model development, as biased models can perpetuate structural inequalities. The {fairmetrics} R package offers a user-friendly framework for rigorously evaluating numerous group-based fairness criteria, including metrics based on independence (e.g., statistical parity), separation (e.g., equalized odds), and sufficiency (e.g., predictive parity). Group-based fairness criteria assess whether a model is equally accurate or well-calibrated across a set of predefined groups so that appropriate bias mitigation strategies can be implemented. {fairmetrics} provides both point and interval estimates for multiple metrics through a convenient wrapper function and includes an example dataset derived from the Medical Information Mart for Intensive Care, version II (MIMIC-II) database (Goldberger et al., 2000; Raffa, 2016).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.06243v2</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Benjamin Smith, Jianhui Gao, Jessica Gronsbell</dc:creator>
    </item>
    <item>
      <title>Reversible Markov chains: variational representations and ordering</title>
      <link>https://arxiv.org/abs/1809.01903</link>
      <description>arXiv:1809.01903v2 Announce Type: replace-cross 
Abstract: This pedagogical document explains three variational representations that are useful when comparing the efficiencies of reversible Markov chains: (i) the Dirichlet form and the associated variational representations of the spectral gaps; (ii) a variational representation of the asymptotic variance of an ergodic average; and (iii) the conductance, and the equivalence of a non-zero conductance to a non-zero right spectral gap.</description>
      <guid isPermaLink="false">oai:arXiv.org:1809.01903v2</guid>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chris Sherlock</dc:creator>
    </item>
    <item>
      <title>A more efficient algorithm to compute the Rand Index for change-point problems</title>
      <link>https://arxiv.org/abs/2112.03738</link>
      <description>arXiv:2112.03738v2 Announce Type: replace-cross 
Abstract: We provide a more efficient algorithm for computing the Rand Index when the data cluster comes from a change-point detection problem. Given $N$ data points and two clusterings of size $r$ and $s$, the algorithm runs on $O(r+s)$ time complexity and $O(1)$ memory complexity. The traditional algorithm, in contrast, runs on $O(rs+N)$ time complexity and $O(rs)$ memory complexity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.03738v2</guid>
      <category>cs.DS</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas de Oliveira Prates</dc:creator>
    </item>
    <item>
      <title>A Bayesian approach to functional regression: theory and computation</title>
      <link>https://arxiv.org/abs/2312.14086</link>
      <description>arXiv:2312.14086v3 Announce Type: replace-cross 
Abstract: We propose a novel Bayesian methodology for inference in functional linear and logistic regression models based on the theory of reproducing kernel Hilbert spaces (RKHS's). We introduce general models that build upon the RKHS generated by the covariance function of the underlying stochastic process, and whose formulation includes as particular cases all finite-dimensional models based on linear combinations of marginals of the process, which can collectively be seen as a dense subspace made of simple approximations. By imposing a suitable prior distribution on this dense functional space we can perform data-driven inference via standard Bayes methodology, estimating the posterior distribution through reversible jump Markov chain Monte Carlo methods. In this context, our contribution is two-fold. First, we derive theoretical results that guarantee strong posterior consistency and contraction at an optimal rate under mild conditions. Second, we show that several prediction strategies stemming from our Bayesian procedure are competitive against other usual alternatives in both simulations and real data sets, including a Bayesian-motivated variable selection method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.14086v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Jos\'e R. Berrendero, Antonio Co\'in, Antonio Cuevas</dc:creator>
    </item>
    <item>
      <title>Data-driven modeling and prediction of microglial cell dynamics in the ischemic penumbra</title>
      <link>https://arxiv.org/abs/2404.10915</link>
      <description>arXiv:2404.10915v3 Announce Type: replace-cross 
Abstract: Neuroinflammation immediately follows the onset of ischemic stroke. During this process, microglial cells are activated in and recruited to the tissue surrounding the irreversibly injured infarct core, referred to as the penumbra. Microglial cells can be activated into two distinct phenotypes; however, the dynamics between the detrimental M1 phenotype and beneficial M2 phenotype are not fully understood. Using phenotype-specific cell count data obtained from experimental studies on middle cerebral artery occlusion-induced stroke in mice, we employ sparsity-promoting system identification techniques combined with Bayesian statistical methods for uncertainty quantification to generate continuous and discrete-time predictive models of the M1 and M2 microglial cell dynamics. The resulting data-driven models include constant and linear terms but do not include nonlinear interactions between the cells. Results emphasize an initial M2 dominance followed by a takeover of M1 cells, capture potential long-term dynamics of microglial cells, and suggest a persistent inflammatory response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.10915v3</guid>
      <category>q-bio.CB</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sara Amato, Andrea Arnold</dc:creator>
    </item>
    <item>
      <title>Robust Maximum $L_q$-Likelihood Covariance Estimation for Replicated Spatial Data</title>
      <link>https://arxiv.org/abs/2407.17592</link>
      <description>arXiv:2407.17592v2 Announce Type: replace-cross 
Abstract: Parameter estimation with the maximum $L_q$-likelihood estimator (ML$q$E) is an alternative to the maximum likelihood estimator (MLE) that considers the $q$-th power of the likelihood values for some $q&lt;1$. In this method, extreme values are down-weighted because of their lower likelihood values, which yields robust estimates. In this work, we study the properties of the ML$q$E for spatial data with replicates. We investigate the asymptotic properties of the ML$q$E for Gaussian random fields with a Mat\'ern covariance function, and carry out simulation studies to investigate the numerical performance of the ML$q$E. We show that it can provide more robust and stable estimation results when some of the replicates in the spatial data contain outliers. In addition, we develop a mechanism to find the optimal choice of the hyper-parameter $q$ for the ML$q$E. The robustness of our approach is further verified on a United States precipitation dataset. Compared with other robust methods for spatial data, our proposal is more intuitive and easier to understand, yet it performs well when dealing with datasets containing outliers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17592v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Mon, 23 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.52933/jdssv.v5i4.126</arxiv:DOI>
      <arxiv:journal_reference>Journal of Data Science, Statistics, and Visualisation, 5(4) (2025)</arxiv:journal_reference>
      <dc:creator>Sihan Chen, Joydeep Chowdhury, Marc G. Genton</dc:creator>
    </item>
  </channel>
</rss>
