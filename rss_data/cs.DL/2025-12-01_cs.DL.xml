<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DL updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DL</link>
    <description>cs.DL updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DL" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 01 Dec 2025 05:00:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>The Rapid Growth of AI Foundation Model Usage in Science</title>
      <link>https://arxiv.org/abs/2511.21739</link>
      <description>arXiv:2511.21739v1 Announce Type: new 
Abstract: We present the first large-scale analysis of AI foundation model usage in science - not just citations or keywords. We find that adoption has grown rapidly, at nearly-exponential rates, with the highest uptake in Linguistics, Computer Science, and Engineering. Vision models are the most used foundation models in science, although language models' share is growing. Open-weight models dominate. As AI builders increase the parameter counts of their models, scientists have followed suit but at a much slower rate: in 2013, the median foundation model built was 7.7x larger than the median one adopted in science, by 2024 this had jumped to 26x. We also present suggestive evidence that scientists' use of these smaller models may be limiting them from getting the full benefits of AI-enabled science, as papers that use larger models appear in higher-impact journals and accrue more citations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21739v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ana Tri\v{s}ovi\'c, Alex Fogelson, Janakan Sivaloganathan, Neil Thompson</dc:creator>
    </item>
    <item>
      <title>AI-Augmented Bibliometric Framework: A Paradigm Shift with Agentic AI for Dynamic, Snippet-Based Research Analysis</title>
      <link>https://arxiv.org/abs/2511.21745</link>
      <description>arXiv:2511.21745v1 Announce Type: new 
Abstract: Our paper introduces a generative, multiagent AI framework designed to overcome the rigidity, limited flexibility and technical barriers of current bibliometric tools. The objective is to enable researchers to perform fully dynamic, code-based scientometric analysis using natural language NL instructions, eliminating the need for specialized programming skills while expanding analytical depth. Methodologically, the system integrates four coordinated AI agents: a custom analytics generator, a full-paper retriever, including a Retrieval Augmented Generation RAG based researcher assistant and an automated report generator. User queries are translated into executable Python scripts, run within a sandbox ensuring safety, reproducibility and auditability. The framework supports automated data cleaning, construction of co-authorship and citation networks, temporal analyses, topic modeling, embedding based clustering and synthesis of research gaps. Each analytical session produces an exportable, end to end report. The novelty lies in unifying NL to code scientometrics, multimodal full paper retrieval, agentic exploration and dynamic metric creation in a single adaptive environment, capabilities absent in existing platforms: VOSviewer, Bibliometrix, SciMAT. Unlike static GUI based workflows, the proposed framework supports iterative what if analysis, hybrid indicators and user driven pipeline modification. Results demonstrate that the framework generates valid analysis scripts, retrieves and synthesizes full papers, identifies frontier themes and produces reproducible scientometric outputs. It establishes a new paradigm for accessible, interactive and extensible bibliometric knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21745v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adela Bara, Simona-Vasilica Oprea</dc:creator>
    </item>
    <item>
      <title>Who Owns the Knowledge? Copyright, GenAI, and the Future of Academic Publishing</title>
      <link>https://arxiv.org/abs/2511.21755</link>
      <description>arXiv:2511.21755v1 Announce Type: new 
Abstract: The integration of generative artificial intelligence (GenAI) and large language models (LLMs) into scientific research and higher education presents a paradigm shift, offering revolutionizing opportunities while simultaneously raising profound ethical, legal, and regulatory questions. This study examines the complex intersection of AI and science, with a specific focus on the challenges posed to copyright law and the principles of open science. The author argues that current regulatory frameworks in key jurisdictions like the United States, China, the European Union, and the United Kingdom, while aiming to foster innovation, contain significant gaps, particularly concerning the use of copyrighted works and open science outputs for AI training. Widely adopted licensing mechanisms, such as Creative Commons, fail to adequately address the nuances of AI training, and the pervasive lack of attribution within AI systems fundamentally challenges established notions of originality. This paper issues a call to action, contending that AI training should not be shielded under fair use exceptions. Instead, the author advocates for upholding authors' rights to refuse the use of their works for AI training and proposes that universities assume a leading role in shaping responsible AI governance. The conclusion is that a harmonized international legislative effort is urgently needed to ensure transparency, protect intellectual property, and prevent the emergence of an oligopolistic market structure that could prioritize commercial profit over scientific integrity and equitable knowledge production.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21755v1</guid>
      <category>cs.DL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dmitry Kochetkov</dc:creator>
    </item>
    <item>
      <title>From 'Individual Scientist' to 'Integrated Scientist': The Evolution of Scientific Organizational panels and Their Impact on the Scientific System</title>
      <link>https://arxiv.org/abs/2511.21771</link>
      <description>arXiv:2511.21771v1 Announce Type: new 
Abstract: This article aims to propose and elucidate the analytical concepts of "individual scientist" and "integrated scientist" to depict the fundamental transformation in the modes of scientific research actors throughout the history of science. The "individual scientist" represents an early modern scientific research panel characterized by independence, egalitarian collaboration, and personal recognition, while the "integrated scientist" emerged in the context of "big science," marked by hierarchical teams, division of labor, collaboration, and the concentration of recognition on team leaders. Through historical review and case analysis, this article explores the underlying drivers of this transformation and focuses on its challenges and reconstructions concerning the name-based scientific reward system, aiming to provide a reflective perspective for contemporary scientific governance and research evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21771v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zekai Zhang</dc:creator>
    </item>
    <item>
      <title>Research on Diamond Open Access in the Long Shadow of Science Policy</title>
      <link>https://arxiv.org/abs/2511.22965</link>
      <description>arXiv:2511.22965v1 Announce Type: new 
Abstract: This paper reviews research literature on Diamond Open Access (DOA) journals - sometimes also called Platinum Open Access - that was produced after this journal segment started to become a priority in European research policy around 2020. It contextualizes the current science policy debate, critically examines different understandings of DOA, and reviews studies on the role of such journals in scholarly communication. Most existing research consists of quantitative studies focusing on aspects such as the number of DOA journals, their publication output, the diversity of the landscape in terms of subject areas, languages, publishing entities, indexing in major databases, awareness and perception among scholars, cost analyses, as well as insights into the internal operations of DOA journals. The review shows that research on DOA journals is partly influenced by the science policy discourse in at least two ways: first, through the normativity inherent in that discourse, and second, through the temporality of policy-driven research of practical relevance, which leaves important aspects of the phenomenon understudied. Moreover, research on the DOA journal landscape has implications beyond understanding this particular journal segment, as it also challenges established views of the global system of scholarly communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.22965v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Niels Taubert</dc:creator>
    </item>
    <item>
      <title>ML Researchers Support Openness in Peer Review But Are Concerned About Resubmission Bias</title>
      <link>https://arxiv.org/abs/2511.23439</link>
      <description>arXiv:2511.23439v1 Announce Type: new 
Abstract: Peer-review venues have increasingly adopted open reviewing policies that publicly release anonymized reviews and permit public commenting. Venues have adopted a variety of policies, and there is still ongoing debate about the benefits and drawbacks of decisions. To inform this debate, we surveyed 2,385 reviewers, authors, and other peer-review participants in machine learning to understand their experiences and opinions. Our key findings are:
  (a) Preferences: Over 80% of respondents support releasing reviews for accepted papers and allowing public comments. However, only 27.1% support releasing rejected manuscripts.
  (b) Benefits: Respondents cite improved public understanding (75.3%) and reviewer education (57.8%), increased fairness (56.6%), and stronger incentives for high-quality reviews (48.0%).
  (c) Challenges: The top concern is resubmission bias, where rejection history biases future reviewers (ranked top impact of open reviewing by 41% of respondents, and mentioned in over 50% of free responses). Other challenges include fear of reviewer de-anonymization (33.2%) and potential commenting abuse.
  (d) AI and open peer review: Participants believe open policies deter "AI slop" submissions (71.9%) and AI-generated reviews (38.9%). Respondents are split regarding peer-review venues generating official AI reviews, with 56.0% opposed and 44.0% supportive.
  Finally, we use AI to annotate 4,244 reviews from ICLR (fully open) and NeurIPS (partially open). We find that the fully open venue (ICLR) has higher levels of correctness and completeness than the partially open venue (NeurIPS). The effect size is small for correctness and very small for completeness, and both are statistically significant. We also find that there is no statistically significant difference in the level of substantiation. We release the full dataset at https://github.com/justinpayan/OpenReviewAnalysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.23439v1</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishisht Rao, Justin Payan, Andrew McCallum, Nihar B. Shah</dc:creator>
    </item>
    <item>
      <title>FLAWS: A Benchmark for Error Identification and Localization in Scientific Papers</title>
      <link>https://arxiv.org/abs/2511.21843</link>
      <description>arXiv:2511.21843v1 Announce Type: cross 
Abstract: The identification and localization of errors is a core task in peer review, yet the exponential growth of scientific output has made it increasingly difficult for human reviewers to reliably detect errors given the limited pool of experts. Recent advances in Large Language Models (LLMs) have sparked interest in their potential to support such evaluation tasks, from academic peer review to automated scientific assessment. However, despite the growing use of LLMs in review systems, their capabilities to pinpoint errors remain underexplored. In this work, we introduce Fault Localization Across Writing in Science (FLAWS), an automated benchmark consisting of 713 paper-error pairs designed to evaluate how effectively LLMs detect errors that undermine key claims in research papers. We construct the benchmark by systematically inserting claim-invalidating errors into peer-reviewed papers using LLMs, paired with an automated evaluation metric that measures whether models can identify and localize these errors. Developing such a benchmark presents unique challenges that we overcome: ensuring that the inserted errors are well-defined, challenging, and relevant to the content of the paper, avoiding artifacts that would make identification trivial, and designing a scalable, automated evaluation metric. On the resulting benchmark, we evaluate five frontier LLMs: Claude Sonnet 4.5, DeepSeek Reasoner v3.1, Gemini 2.5 Pro, GPT 5, and Grok 4. Among these, GPT 5 is the top-performing model, achieving 39.1% identification accuracy when k=10, where k is the number of top-ranked error text candidates generated by the LLM.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.21843v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.DL</category>
      <category>cs.LG</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarina Xi, Vishisht Rao, Justin Payan, Nihar B. Shah</dc:creator>
    </item>
    <item>
      <title>Detection of the papermilling behavior</title>
      <link>https://arxiv.org/abs/2405.19872</link>
      <description>arXiv:2405.19872v4 Announce Type: replace 
Abstract: Based on the analysis of the data obtainable from the Web of Science publication and citation database, typical signs of possible papermilling behavior are described, quantified, and illustrated by examples. A MATLAB function is provided for the analysis of the outputs from the Web of Science. A new quantitative indicator -- integrity index, or I-index -- is proposed for using it along with standard bibliographic and scientometric indicators. A case study is presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19872v4</guid>
      <category>cs.DL</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Igor Podlubny</dc:creator>
    </item>
    <item>
      <title>Harvesting Textual and Contrastive Data from the HAL Publication Repository</title>
      <link>https://arxiv.org/abs/2407.20595</link>
      <description>arXiv:2407.20595v3 Announce Type: replace 
Abstract: Authorship attribution in natural language processing traditionally struggles to distinguish genuine stylistic signals from topical confounds. While contrastive learning approaches have addressed this by maximizing semantic overlap between positive pairs, creating large-scale datasets under strict topic constraints remains challenging. We introduce HALvest, a 17-billion-token multilingual corpus harvested from 778k open-access academic papers, and HALvest-Contrastive, a derived dataset designed to isolate stylometric signals through controlled topic variation. Unlike prior work that minimizes lexical overlap, we exploit natural topic drift between papers by the same author, treating residual lexical patterns as authorial fingerprints rather than noise. Comparing lexical baselines (BM25) against neural models trained on unrestricted (topic-rich) versus base (topic-decoupled) triplets, we demonstrate that models trained exclusively on topic-decoupled data achieve superior performance across all test conditions, outperforming both retrieval baselines and models exposed to topic-rich training data. Our analysis reveals that while lexical signals provide substantial performance gains for keyword-driven methods, neural architectures learn robust stylometric representations that plateau with moderate context length, suggesting they capture distributional style beyond surface-level tokens. Both datasets and code are publicly available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20595v3</guid>
      <category>cs.DL</category>
      <category>cs.CL</category>
      <pubDate>Mon, 01 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francis Kulumba, Wissam Antoun, Guillaume Vimont, Laurent Romary</dc:creator>
    </item>
  </channel>
</rss>
