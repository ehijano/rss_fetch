<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.BM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.BM</link>
    <description>q-bio.BM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.BM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 21 Oct 2025 08:40:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>CryoDyna: Multiscale end-to-end modeling of cryo-EM macromolecule dynamics with physics-aware neural network</title>
      <link>https://arxiv.org/abs/2510.16510</link>
      <description>arXiv:2510.16510v1 Announce Type: new 
Abstract: Single-particle cryo-EM has transformed structural biology but still faces challenges in resolving conformational heterogeneity at atomic resolution. Existing cryo-EM heterogeneity analysis methods either lack atomic details or tend to subject to overfitting due to image noise and limited information in single views. To obtain atomic detailed multiple conformations and make full use of particle images of different orientations, we present here CryoDyna, a deep learning framework to infer macromolecular dynamics directly from 2D projections by integrating cross-view attention and multi-scale deformation modeling. Combining coarse-grained MARTINI representation with atomic backmapping, CryoDyna achieves near-atomic interpretation of protein conformational landscapes. Validated on multiple simulated and experimental datasets, CryoDyna demonstrates improved modeling accuracy and robustly recovers multi-scale complex structure changes hidden in the cryo-EM particle stacks. As examples, we generated protein-RNA coordinated motions, resolved dynamics in the unseen region of RAG signal end complex, mapped translocating ribosome states in a one-shot manner, and revealed step-wise closure of a membrane-anchored protein multimer. This work bridges the gap between cryo-EM heterogeneity analysis and atomic-scale structural dynamics, offering a promising tool for exploration of complex biological mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16510v1</guid>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengwei Zhang, Shimian Li, Yihao Niu, Zhen Zhu, Sihao Yuan, Sirui Liu, Yi Qin Gao</dc:creator>
    </item>
    <item>
      <title>Generative AI for Biosciences: Emerging Threats and Roadmap to Biosecurity</title>
      <link>https://arxiv.org/abs/2510.15975</link>
      <description>arXiv:2510.15975v1 Announce Type: cross 
Abstract: The rapid adoption of generative artificial intelligence (GenAI) in the biosciences is transforming biotechnology, medicine, and synthetic biology. Yet this advancement is intrinsically linked to new vulnerabilities, as GenAI lowers the barrier to misuse and introduces novel biosecurity threats, such as generating synthetic viral proteins or toxins. These dual-use risks are often overlooked, as existing safety guardrails remain fragile and can be circumvented through deceptive prompts or jailbreak techniques. In this Perspective, we first outline the current state of GenAI in the biosciences and emerging threat vectors ranging from jailbreak attacks and privacy risks to the dual-use challenges posed by autonomous AI agents. We then examine urgent gaps in regulation and oversight, drawing on insights from 130 expert interviews across academia, government, industry, and policy. A large majority ($\approx 76$\%) expressed concern over AI misuse in biology, and 74\% called for the development of new governance frameworks. Finally, we explore technical pathways to mitigation, advocating a multi-layered approach to GenAI safety. These defenses include rigorous data filtering, alignment with ethical principles during development, and real-time monitoring to block harmful requests. Together, these strategies provide a blueprint for embedding security throughout the GenAI lifecycle. As GenAI becomes integrated into the biosciences, safeguarding this frontier requires an immediate commitment to both adaptive governance and secure-by-design technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.15975v1</guid>
      <category>cs.CR</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zaixi Zhang, Souradip Chakraborty, Amrit Singh Bedi, Emilin Mathew, Varsha Saravanan, Le Cong, Alvaro Velasquez, Sheng Lin-Gibson, Megan Blewett, Dan Hendrycs, Alex John London, Ellen Zhong, Ben Raphael, Jian Ma, Eric Xing, Russ Altman, George Church, Mengdi Wang</dc:creator>
    </item>
    <item>
      <title>Protein Folding with Neural Ordinary Differential Equations</title>
      <link>https://arxiv.org/abs/2510.16253</link>
      <description>arXiv:2510.16253v1 Announce Type: cross 
Abstract: Recent advances in protein structure prediction, such as AlphaFold, have demonstrated the power of deep neural architectures like the Evoformer for capturing complex spatial and evolutionary constraints on protein conformation. However, the depth of the Evoformer, comprising 48 stacked blocks, introduces high computational costs and rigid layerwise discretization. Inspired by Neural Ordinary Differential Equations (Neural ODEs), we propose a continuous-depth formulation of the Evoformer, replacing its 48 discrete blocks with a Neural ODE parameterization that preserves its core attention-based operations. This continuous-time Evoformer achieves constant memory cost (in depth) via the adjoint method, while allowing a principled trade-off between runtime and accuracy through adaptive ODE solvers. Benchmarking on protein structure prediction tasks, we find that the Neural ODE-based Evoformer produces structurally plausible predictions and reliably captures certain secondary structure elements, such as alpha-helices, though it does not fully replicate the accuracy of the original architecture. However, our model achieves this performance using dramatically fewer resources, just 17.5 hours of training on a single GPU, highlighting the promise of continuous-depth models as a lightweight and interpretable alternative for biomolecular modeling. This work opens new directions for efficient and adaptive protein structure prediction frameworks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16253v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <category>stat.ML</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arielle Sanford, Shuo Sun, Christian B. Mendl</dc:creator>
    </item>
    <item>
      <title>Atom-anchored LLMs speak Chemistry: A Retrosynthesis Demonstration</title>
      <link>https://arxiv.org/abs/2510.16590</link>
      <description>arXiv:2510.16590v1 Announce Type: cross 
Abstract: Applications of machine learning in chemistry are often limited by the scarcity and expense of labeled data, restricting traditional supervised methods. In this work, we introduce a framework for molecular reasoning using general-purpose Large Language Models (LLMs) that operates without requiring labeled training data. Our method anchors chain-of-thought reasoning to the molecular structure by using unique atomic identifiers. First, the LLM performs a one-shot task to identify relevant fragments and their associated chemical labels or transformation classes. In an optional second step, this position-aware information is used in a few-shot task with provided class examples to predict the chemical transformation. We apply our framework to single-step retrosynthesis, a task where LLMs have previously underperformed. Across academic benchmarks and expert-validated drug discovery molecules, our work enables LLMs to achieve high success rates in identifying chemically plausible reaction sites ($\geq90\%$), named reaction classes ($\geq40\%$), and final reactants ($\geq74\%$). Beyond solving complex chemical tasks, our work also provides a method to generate theoretically grounded synthetic datasets by mapping chemical knowledge onto the molecular structure and thereby addressing data scarcity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16590v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Kai Hassen, Andrius Bernatavicius, Antonius P. A. Janssen, Mike Preuss, Gerard J. P. van Westen, Djork-Arn\'e Clevert</dc:creator>
    </item>
    <item>
      <title>Accelerated Learning on Large Scale Screens using Generative Library Models</title>
      <link>https://arxiv.org/abs/2510.16612</link>
      <description>arXiv:2510.16612v1 Announce Type: cross 
Abstract: Biological machine learning is often bottlenecked by a lack of scaled data. One promising route to relieving data bottlenecks is through high throughput screens, which can experimentally test the activity of $10^6-10^{12}$ protein sequences in parallel. In this article, we introduce algorithms to optimize high throughput screens for data creation and model training. We focus on the large scale regime, where dataset sizes are limited by the cost of measurement and sequencing. We show that when active sequences are rare, we maximize information gain if we only collect positive examples of active sequences, i.e. $x$ with $y&gt;0$. We can correct for the missing negative examples using a generative model of the library, producing a consistent and efficient estimate of the true $p(y | x)$. We demonstrate this approach in simulation and on a large scale screen of antibodies. Overall, co-design of experiments and inference lets us accelerate learning dramatically.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16612v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Eli N. Weinstein, Andrei Slabodkin, Mattia G. Gollub, Elizabeth B. Wood</dc:creator>
    </item>
    <item>
      <title>A Standardized Benchmark for Machine-Learned Molecular Dynamics using Weighted Ensemble Sampling</title>
      <link>https://arxiv.org/abs/2510.17187</link>
      <description>arXiv:2510.17187v1 Announce Type: cross 
Abstract: The rapid evolution of molecular dynamics (MD) methods, including machine-learned dynamics, has outpaced the development of standardized tools for method validation. Objective comparison between simulation approaches is often hindered by inconsistent evaluation metrics, insufficient sampling of rare conformational states, and the absence of reproducible benchmarks. To address these challenges, we introduce a modular benchmarking framework that systematically evaluates protein MD methods using enhanced sampling analysis. Our approach uses weighted ensemble (WE) sampling via The Weighted Ensemble Simulation Toolkit with Parallelization and Analysis (WESTPA), based on progress coordinates derived from Time-lagged Independent Component Analysis (TICA), enabling fast and efficient exploration of protein conformational space. The framework includes a flexible, lightweight propagator interface that supports arbitrary simulation engines, allowing both classical force fields and machine learning-based models. Additionally, the framework offers a comprehensive evaluation suite capable of computing more than 19 different metrics and visualizations across a variety of domains. We further contribute a dataset of nine diverse proteins, ranging from 10 to 224 residues, that span a variety of folding complexities and topologies. Each protein has been extensively simulated at 300K for one million MD steps per starting point (4 ns). To demonstrate the utility of our framework, we perform validation tests using classic MD simulations with implicit solvent and compare protein conformational sampling using a fully trained versus under-trained CGSchNet model. By standardizing evaluation protocols and enabling direct, reproducible comparisons across MD approaches, our open-source platform lays the groundwork for consistent, rigorous benchmarking across the molecular simulation community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17187v1</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Aghili, Andy Bruce, Daniel Sabo, Sanya Murdeshwar, Kevin Bachelor, Ionut Mistreanu, Ashwin Lokapally, Razvan Marinescu</dc:creator>
    </item>
    <item>
      <title>Protein folding classes -- High-dimensional geometry of amino acid composition space revisited</title>
      <link>https://arxiv.org/abs/2506.01857</link>
      <description>arXiv:2506.01857v5 Announce Type: replace 
Abstract: In this study, the distributions of protein structure classes (or folding types) of experimentally determined structures from a legacy dataset and a comprehensive database (SCOP) are modeled precisely with geometric constructs such as convex polytopes in high-dimensional amino acid composition space. This is a follow-up of a previous non-statistical, geometry-motivated modeling of protein classes with ellipsoidal models, which is superseded presently in three important respects: (1) as a paradigm shift a descriptive 'distribution model' of experimental data is de-coupled from, and serves as the basis for, a possible future predictive 'domain model' generalizable to proteins in the same class for which 3D structures have yet to be determined experimentally, (2) the geometric and analytic characteristics of class distributions are obtained via exact computational geometry calculations, and (3) the full data from a comprehensive database are included in such calculations, eschewing training set selection and biases. In contrast to statistical and machine-learning approaches, the analytical, non-statistical geometry models of protein class distributions demonstrated in this study furnish complete and precise information on their size and relative disposition in the high-dimensional space (vis-\`a-vis any overlaps leading to ambiguity and classification limits). Intended primarily as an accurate and summary description of the complex relationships between amino acid composition and protein classes, and suitably as a basis for predictive modeling where possible, the results suggest that pen-ultimately they may be useful adjuncts for validating sequence-based protein structure predictions and contribute to theoretical and fundamental understanding of secondary structure formation and protein folding, demonstrating the role of high dimensional amino acid composition space in protein studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01857v5</guid>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Boryeu Mao</dc:creator>
    </item>
    <item>
      <title>Reshaping Biomolecular Structure Prediction through Strategic Conformational Exploration with HelixFold-S1</title>
      <link>https://arxiv.org/abs/2507.09251</link>
      <description>arXiv:2507.09251v2 Announce Type: replace 
Abstract: Generating large ensembles of candidate conformations is standard for improving biomolecular structure prediction. Yet aimless sampling is inefficient and costly, producing many redundant conformations with limited diversity, so additional computation often yields little improvement. Here, we present HelixFold-S1, a guided planning approach that strategically targets the most informative regions of conformational space to produce accurate conformations. For each biomolecule, predicted inter-chain contact probabilities serve as a blueprint of the conformational space, guiding computational effort toward higher-probability, low-redundancy contacts that constrain structure generation. Across diverse biomolecular benchmarks, HelixFold-S1 achieves markedly higher structural accuracy than traditional unguided methods while reducing sampling requirements by an order of magnitude. Predicted contact probabilities also provide a rough indicator of prediction difficulty and sampling utility. These results demonstrate that guided planning reshapes conformational exploration and enables more efficient and accurate structural inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09251v2</guid>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lihang Liu, Yang Liu, Xianbin Ye, Shanzhuo Zhang, Yuxin Li, Kunrui Zhu, Yang Xue, Xiaonan Zhang, Xiaomin Fang</dc:creator>
    </item>
    <item>
      <title>Canonicalization of the E value from BLAST similarity search -- dissimilarity measure and distance function for a metric space of protein sequences</title>
      <link>https://arxiv.org/abs/2509.06849</link>
      <description>arXiv:2509.06849v3 Announce Type: replace 
Abstract: Sequence matching algorithms such as BLAST and FASTA have been widely used in searching for evolutionary origin and biological functions of newly discovered nucleic acid and protein sequences. As parts of these search tools, alignment scores and E values are useful indicators of the quality of search results from querying a database of annotated sequences, whereby a high alignment score (and inversely a low E value) reflects significant similarity between the query and the subject (target) sequences. For cross-comparison of results from sufficiently different queries however, the interpretation of alignment score as a similarity measure and E value a dissimilarity measure becomes somewhat nuanced, and prompts herein a judicious distinction of different types of similarity. We show that an adjustment of E value to account for self-matching of query and subject sequences corrects for certain ostensibly anomalous similarity comparisons, resulting in canonical dissimilarity and similarity measures that would be more appropriate for database applications, such as all-on-all sequence alignment or selection of diverse subsets. In actual practice, the canonicalization of E value dissimilarity improves clustering and the diversity of subset selection. While both E value and the canonical E value share positivity and symmetry, two of the four axiomatic properties of a metric space, the canonical E value is also reflexive and meets the condition of triangle inequality, thus itself an appropriate distance function for a metric space of protein sequences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.06849v3</guid>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Boryeu Mao</dc:creator>
    </item>
    <item>
      <title>OneProt: Towards Multi-Modal Protein Foundation Models</title>
      <link>https://arxiv.org/abs/2411.04863</link>
      <description>arXiv:2411.04863v3 Announce Type: replace-cross 
Abstract: Recent advances in Artificial Intelligence have enabled multi-modal systems to model and translate diverse information spaces. Extending beyond text and vision, we introduce OneProt, a multi-modal AI for proteins that integrates structural, sequence, text, and binding site data. Using the ImageBind framework, OneProt aligns the latent spaces of protein modality encoders in a lightweight fine-tuning scheme that focuses on pairwise alignment with sequence data rather than requiring full matches. This novel approach comprises a mix of Graph Neural Networks and transformer architectures. It demonstrates strong performance in retrieval tasks and showcases the efficacy of multi-modal systems in Protein Machine Learning through a broad spectrum of downstream baselines, including enzyme function prediction and binding site analysis. Furthermore, OneProt enables the transfer of representational information from specialized encoders to the sequence encoder, enhancing capabilities for distinguishing evolutionarily related and unrelated sequences and exhibiting representational properties where evolutionarily related proteins align in similar directions within the latent space. In addition, we extensively investigate modality ablations to identify the encoders that contribute most to predictive performance, highlighting the significance of the binding site encoder, which has not been used in similar models previously. This work expands the horizons of multi-modal protein models, paving the way for transformative applications in drug discovery, biocatalytic reaction planning, and protein engineering.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.04863v3</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Klemens Fl\"oge, Srisruthi Udayakumar, Johanna Sommer, Marie Piraud, Stefan Kesselheim, Vincent Fortuin, Stephan G\"unneman, Karel J van der Weg, Holger Gohlke, Erinc Merdivan, Alina Bazarova</dc:creator>
    </item>
    <item>
      <title>GeoRecon: Graph-Level Representation Learning for 3D Molecules via Reconstruction-Based Pretraining</title>
      <link>https://arxiv.org/abs/2506.13174</link>
      <description>arXiv:2506.13174v2 Announce Type: replace-cross 
Abstract: The pretraining-finetuning paradigm has powered major advances in domains such as natural language processing and computer vision, with representative examples including masked language modeling and next-token prediction. In molecular representation learning, however, pretraining tasks remain largely restricted to node-level denoising, which effectively captures local atomic environments but is often insufficient for encoding the global molecular structure critical to graph-level property prediction tasks such as energy estimation and molecular regression. To address this gap, we introduce GeoRecon, a graph-level pretraining framework that shifts the focus from individual atoms to the molecule as an integrated whole. GeoRecon formulates a graph-level reconstruction task: during pretraining, the model is trained to produce an informative graph representation that guides geometry reconstruction while inducing smoother and more transferable latent spaces. This encourages the learning of coherent, global structural features beyond isolated atomic details. Without relying on external supervision, GeoRecon generally improves over backbone baselines on multiple molecular benchmarks including QM9, MD17, MD22, and 3BPA, demonstrating the effectiveness of graph-level reconstruction for holistic and geometry-aware molecular embeddings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.13174v2</guid>
      <category>cs.LG</category>
      <category>q-bio.BM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaoheng Yan, Zian Li, Muhan Zhang</dc:creator>
    </item>
    <item>
      <title>FGBench: A Dataset and Benchmark for Molecular Property Reasoning at Functional Group-Level in Large Language Models</title>
      <link>https://arxiv.org/abs/2508.01055</link>
      <description>arXiv:2508.01055v3 Announce Type: replace-cross 
Abstract: Large language models (LLMs) have gained significant attention in chemistry. However, most existing datasets center on molecular-level property prediction and overlook the role of fine-grained functional group (FG) information. Incorporating FG-level data can provide valuable prior knowledge that links molecular structures with textual descriptions, which can be used to build more interpretable, structure-aware LLMs for reasoning on molecule-related tasks. Moreover, LLMs can learn from such fine-grained information to uncover hidden relationships between specific functional groups and molecular properties, thereby advancing molecular design and drug discovery. Here, we introduce FGBench, a dataset comprising 625K molecular property reasoning problems with functional group information. Functional groups are precisely annotated and localized within the molecule, which ensures the dataset's interoperability thereby facilitating further multimodal applications. FGBench includes both regression and classification tasks on 245 different functional groups across three categories for molecular property reasoning: (1) single functional group impacts, (2) multiple functional group interactions, and (3) direct molecular comparisons. In the benchmark of state-of-the-art LLMs on 7K curated data, the results indicate that current LLMs struggle with FG-level property reasoning, highlighting the need to enhance reasoning capabilities in LLMs for chemistry tasks. We anticipate that the methodology employed in FGBench to construct datasets with functional group-level information will serve as a foundational framework for generating new question-answer pairs, enabling LLMs to better understand fine-grained molecular structure-property relationships. The dataset and evaluation code are available at https://github.com/xuanliugit/FGBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01055v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>q-bio.BM</category>
      <category>q-bio.QM</category>
      <pubDate>Tue, 21 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Xuan Liu, Siru Ouyang, Xianrui Zhong, Jiawei Han, Huimin Zhao</dc:creator>
    </item>
  </channel>
</rss>
