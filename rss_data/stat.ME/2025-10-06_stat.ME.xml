<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 07 Oct 2025 02:46:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Orthogonal Procrustes problem preserves correlations in synthetic data</title>
      <link>https://arxiv.org/abs/2510.02405</link>
      <description>arXiv:2510.02405v1 Announce Type: new 
Abstract: This work introduces the application of the Orthogonal Procrustes problem to the generation of synthetic data. The proposed methodology ensures that the resulting synthetic data preserves important statistical relationships among features, specifically the Pearson correlation. An empirical illustration using a large, real-world, tabular dataset of energy consumption demonstrates the effectiveness of the approach and highlights its potential for application in practical synthetic data generation. Our approach is not meant to replace existing generative models, but rather as a lightweight post-processing step that enforces exact Pearson correlation to an already generated synthetic dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02405v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oussama Ounissi, Nicklas J\"averg\r{a}rd, Adrian Muntean</dc:creator>
    </item>
    <item>
      <title>Bridging the Prediction Error Method and Subspace Identification: A Weighted Null Space Fitting Method</title>
      <link>https://arxiv.org/abs/2510.02529</link>
      <description>arXiv:2510.02529v1 Announce Type: new 
Abstract: Subspace identification methods (SIMs) have proven to be very useful and numerically robust for building state-space models. While most SIMs are consistent, few if any can achieve the efficiency of the maximum likelihood estimate (MLE). Conversely, the prediction error method (PEM) with a quadratic criteria is equivalent to MLE, but it comes with non-convex optimization problems and requires good initialization points. This contribution proposes a weighted null space fitting (WNSF) approach for estimating state-space models, combining some key advantages of the two aforementioned mainstream approaches. It starts with a least-squares estimate of a high-order ARX model, and then a multi-step least-squares procedure reduces the model to a state-space model on canoncial form. It is demonstrated through statistical analysis that when a canonical parameterization is admissible, the proposed method is consistent and asymptotically efficient, thereby making progress on the long-standing open problem about the existence of an asymptotically efficient SIM. Numerical and practical examples are provided to illustrate that the proposed method performs favorable in comparison with SIMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02529v1</guid>
      <category>stat.ME</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiabao He, S. Joe Qin, H\r{a}kan Hjalmarsson</dc:creator>
    </item>
    <item>
      <title>Amortized Bayesian Inference for Spatio-Temporal Extremes: A Copula Factor Model with Autoregression</title>
      <link>https://arxiv.org/abs/2510.02618</link>
      <description>arXiv:2510.02618v1 Announce Type: new 
Abstract: We develop a Bayesian spatio-temporal framework for extreme-value analysis that augments a hierarchical copula model with an autoregressive factor to capture residual temporal dependence in threshold exceedances. The factor can be specified as spatially varying or spatially constant, and the scale parameter incorporates scientifically relevant covariates (e.g., longitude, latitude, altitude), enabling flexible representation of geographic heterogeneity. To avoid the computational burden of the full censored likelihood, we design a Gibbs sampler that embeds amortized neural posterior estimation within each parameter block, yielding scalable inference with full posterior uncertainty for parameters, predictive quantiles, and return levels. Simulation studies demonstrate that the approach improves MCMC mixing and estimation accuracy relative to baseline specifications, particularly when using moderately more complex network architectures, while preserving heavy-tail behavior. We illustrate the methodology with daily precipitation in Guanacaste, Costa Rica, evaluating a suite of nested models and selecting the best-performing factor combination via out-of-sample diagnostics. The chosen specification reveals coherent spatial patterns in multi-year return periods and provides actionable information for infrastructure planning and climate-risk management in a tropical dry region strongly influenced by climatic factors. The proposed Gibbs scheme generalizes to other settings where parameters can be partitioned into inferentially homogeneous blocks and conditionals learned via amortized, likelihood-free methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02618v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos A. Pasquier, Luis A. Barboza</dc:creator>
    </item>
    <item>
      <title>What is in the model? A Comparison of variable selection criteria and model search approaches</title>
      <link>https://arxiv.org/abs/2510.02628</link>
      <description>arXiv:2510.02628v1 Announce Type: new 
Abstract: For many scientific questions, understanding the underlying mechanism is the goal. To help investigators better understand the underlying mechanism, variable selection is a crucial step that permits the identification of the most associated regression variables of interest. A variable selection method consists of model evaluation using an information criterion and a search of the model space. Here, we provide a comprehensive comparison of variable selection methods using performance measures of correct identification rate (CIR), recall, and false discovery rate (FDR). We consider the BIC and AIC for evaluating models, and exhaustive, greedy, LASSO path, and stochastic search approaches for searching the model space; we also consider LASSO using cross validation. We perform simulation studies for linear and generalized linear models that parametrically explore a wide range of realistic sample sizes, effect sizes, and correlations among regression variables. We consider model spaces with a small and larger number of potential regressors. The results show that the exhaustive search BIC and stochastic search BIC outperform the other methods when considering the performance measures on small and large model spaces, respectively. These approaches result in the highest CIR and lowest FDR, which collectively may support long-term efforts towards increasing replicability in research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02628v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shuangshuang Xu, Marco A. R. Ferreira, Allison N. Tegge</dc:creator>
    </item>
    <item>
      <title>Total Robustness in Bayesian Nonlinear Regression for Measurement Error Problems under Model Misspecification</title>
      <link>https://arxiv.org/abs/2510.03131</link>
      <description>arXiv:2510.03131v1 Announce Type: new 
Abstract: Modern regression analyses are often undermined by covariate measurement error, misspecification of the regression model, and misspecification of the measurement error distribution. We present, to the best of our knowledge, the first Bayesian nonparametric framework targeting total robustness that tackles all three challenges in general nonlinear regression. The framework assigns a Dirichlet process prior to the latent covariate-response distribution and updates it with posterior pseudo-samples of the latent covariates, thereby providing the Dirichlet process posterior with observation-informed latent inputs and yielding estimators that minimise the discrepancy between Dirichlet process realisations and the model-induced joint law. This design allows practitioners to (i) encode prior beliefs, (ii) choose between pseudo-sampling latent covariates or working directly with error-prone observations, and (iii) tune the influence of prior and data. We establish generalisation bounds that tighten whenever the prior or pseudo-sample generator aligns with the underlying data generating process, ensuring robustness without sacrificing consistency. A gradient-based algorithm enables efficient computations; simulations and two real-world studies show lower estimation error and reduced estimation sensitivity to misspecification compared to Bayesian and frequentist competitors. The framework, therefore, offers a practical and interpretable paradigm for trustworthy regression when data and models are jointly imperfect.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03131v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mengqi Chen, Charita Dellaporta, Thomas B. Berrett, Theodoros Damoulas</dc:creator>
    </item>
    <item>
      <title>TITE-Safety: Time-to-event Safety Monitoring for Clinical Trials</title>
      <link>https://arxiv.org/abs/2510.03175</link>
      <description>arXiv:2510.03175v1 Announce Type: new 
Abstract: Safety evaluation is an essential component of clinical trials. To protect study participants, these studies often implement safety stopping rules that will halt the trial if an excessive number of toxicity events occur. Existing safety monitoring methods often treat these events as binary outcomes. A strategy that instead handles these as time-to-event endpoints can offer higher power and a reduced time to signal of excess risk, but must manage additional complexities including censoring and competing risks. We propose the TITE-Safety approach for safety monitoring, which incorporates time-to-event information while handling censored observations and competing risks appropriately. This strategy is applied to develop stopping rules using score tests, Bayesian beta-extended binomial models, and sequential probability ratio tests. The operating characteristics of these methods are studied via simulation for common phase 2 and 3 trial scenarios. Across simulation settings, the proposed techniques offer reductions in expected toxicities of 20% or more compared to binary data methods and maintain the type I error rate near the nominal level across various event time distributions. These methods are demonstrated through a redesign of the safety monitoring scheme for BMT CTN 0601, a single arm, phase 2 trial that evaluated bone marrow transplant as treatment for severe sickle cell disease. Our R package "stoppingrule" offers functions to construct and evaluate these stopping rules, providing valuable tools for trial design to investigators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03175v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Michael J. Martens, Qinghua Lian, Brent R. Logan</dc:creator>
    </item>
    <item>
      <title>New M-estimator of the leading principal component</title>
      <link>https://arxiv.org/abs/2510.02799</link>
      <description>arXiv:2510.02799v1 Announce Type: cross 
Abstract: We study the minimization of the non-convex and non-differentiable objective function $v \mapsto \mathrm{E} ( \| X - v \| \| X + v \| - \| X \|^2 )$ in $\mathbb{R}^p$. In particular, we show that its minimizers recover the first principal component direction of elliptically symmetric $X$ under specific conditions. The stringency of these conditions is studied in various scenarios, including a diverging number of variables $p$. We establish the consistency and asymptotic normality of the sample minimizer. We propose a Weiszfeld-type algorithm for optimizing the objective and show that it is guaranteed to converge in a finite number of steps. The results are illustrated with two simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.02799v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joni Virta, Una Radojicic, Marko Voutilainen</dc:creator>
    </item>
    <item>
      <title>Nonparametric Vector Quantile Autoregression</title>
      <link>https://arxiv.org/abs/2510.03166</link>
      <description>arXiv:2510.03166v1 Announce Type: cross 
Abstract: Prediction is a key issue in time series analysis. Just as classical mean regression models, classical autoregressive methods, yielding L$^2$ point-predictions, provide rather poor predictive summaries; a much more informative approach is based on quantile (auto)regression, where the whole distribution of future observations conditional on the past is consistently recovered. Since their introduction by Koenker and Xiao in 2006, autoregressive quantile autoregression methods have become a popular and successful alternative to the traditional L$^2$ ones. Due to the lack of a widely accepted concept of multivariate quantiles, however, quantile autoregression methods so far have been limited to univariate time series. Building upon recent measure-transportation-based concepts of multivariate quantiles, we develop here a nonparametric vector quantile autoregressive approach to the analysis and prediction of (nonlinear as well as linear) multivariate time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03166v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alberto Gonz\'alez-Sanz, Marc Hallin, Yisha Yao</dc:creator>
    </item>
    <item>
      <title>A fast non-reversible sampler for Bayesian finite mixture models</title>
      <link>https://arxiv.org/abs/2510.03226</link>
      <description>arXiv:2510.03226v1 Announce Type: cross 
Abstract: Finite mixtures are a cornerstone of Bayesian modelling, and it is well-known that sampling from the resulting posterior distribution can be a hard task. In particular, popular reversible Markov chain Monte Carlo schemes are often slow to converge when the number of observations $n$ is large. In this paper we introduce a novel and simple non-reversible sampling scheme for Bayesian finite mixture models, which is shown to drastically outperform classical samplers in many scenarios of interest, especially during convergence phase and when components in the mixture have non-negligible overlap. At the theoretical level, we show that the performance of the proposed non-reversible scheme cannot be worse than the standard one, in terms of asymptotic variance, by more than a factor of four; and we provide a scaling limit analysis suggesting that the non-reversible sampler can reduce the convergence time from O$(n^2)$ to O$(n)$. We also discuss why the statistical features of mixture models make them an ideal case for the use of non-reversible discrete samplers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.03226v1</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Filippo Ascolani, Giacomo Zanella</dc:creator>
    </item>
    <item>
      <title>Joint identification of spatially variable genes via a network-assisted Bayesian regularization approach</title>
      <link>https://arxiv.org/abs/2407.05241</link>
      <description>arXiv:2407.05241v2 Announce Type: replace 
Abstract: Identifying genes that display spatial patterns is critical to investigating expression interactions within a spatial context and further dissecting biological understanding of complex mechanistic functionality. Despite the increase in statistical methods designed to identify spatially variable genes, they are mostly based on marginal analysis and share the limitation that the dependence (network) structures among genes are not well accommodated, where a biological process usually involves changes in multiple genes that interact in a complex network. Moreover, the latent cellular composition within spots may introduce confounding variations, negatively affecting identification accuracy. In this study, we develop a novel Bayesian regularization approach for spatial transcriptomic data, with the confounding variations induced by varying cellular distributions effectively corrected. Significantly advancing from the existing studies, a thresholded graph Laplacian regularization is proposed to simultaneously identify spatially variable genes and accommodate the network structure among genes. The proposed method is based on a zero-inflated negative binomial distribution, effectively accommodating the count nature, zero inflation, and overdispersion of spatial transcriptomic data. Extensive simulations and the application to real data demonstrate the competitive performance of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.05241v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mingcong Wu, Yang Li, Shuangge Ma, Mengyun Wu</dc:creator>
    </item>
    <item>
      <title>A Heavily Right Strategy for Statistical Inference with Dependent Studies in Any Dimension</title>
      <link>https://arxiv.org/abs/2501.01065</link>
      <description>arXiv:2501.01065v2 Announce Type: replace 
Abstract: We leverage recent advances in heavy-tail approximations for global hypothesis testing with dependent studies to construct approximate confidence regions without modeling or estimating their dependence structures. A non-rejection region is a confidence region but it may not be convex. Convexity is appealing because it ensures any one-dimensional linear projection of the region is a confidence interval, easy to compute and interpret. We show why convexity fails for nearly all heavy-tail combination tests proposed in recent years, including the influential Cauchy combination test. These insights motivate a \textit{heavily right} strategy: truncating the left half of the Cauchy distribution to obtain the Half-Cauchy combination test. The harmonic mean test also corresponds to a heavily right distribution with a Cauchy-like tail, namely a Pareto distribution with unit power. We prove that both approaches guarantee convexity when individual studies are summarized by Hotelling $T^2$ or $\chi^{2}$ statistics (regardless of the validity of this summary) and provide efficient, \textit{exact} algorithms for implementation. Applying these methods, we develop a divide-and-combine strategy for mean estimation in any dimension and construct simultaneous confidence intervals in a network meta-analysis for treatment effect comparisons across multiple clinical trials. We also present many open problems and conclude with epistemic reflections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.01065v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tianle Liu, Xiao-Li Meng, Natesh S. Pillai</dc:creator>
    </item>
    <item>
      <title>Fusion of heterogeneous data for robust degradation prognostics</title>
      <link>https://arxiv.org/abs/2506.05882</link>
      <description>arXiv:2506.05882v2 Announce Type: replace 
Abstract: Assessing the degradation state of an industrial asset first requires evaluating its current condition and then to project the forecast model trajectory to a predefined prognostic threshold, thereby estimating its remaining useful life (RUL). Depending on the available information, two primary categories of forecasting models may be used: physics-based simulation codes and datadriven (machine learning) approaches. Combining both modelling approaches may enhance prediction robustness, especially with respect to their individual uncertainties. This paper introduces a methodology for fusion of heterogeneous data in degradation prognostics. The proposed approach acts iteratively on a computer model's uncertain input variables by combining kernel-based sensitivity analysis for variable ranking with a Bayesian framework to inform the priors with the heterogeneous data. Additionally, we propose an integration of an aggregate surrogate modeling strategy for computationally expensive degradation simulation codes. The methodology updates the knowledge of the computer code input probabilistic model and reduces the output uncertainty. As an application, we illustrate this methodology on a toy model from crack propagation based on Paris law as well as a complex industrial clogging simulation model for nuclear power plant steam generators, where data is intermittently available over time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05882v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edgar Jaber (EDF R\&amp;D PRISME, CB, LISN), Emmanuel Remy (EDF R\&amp;D PRISME), Vincent Chabridon (EDF R\&amp;D PRISME), Mathilde Mougeot (ENSIIE, CB), Didier Lucor (LISN)</dc:creator>
    </item>
    <item>
      <title>Adaptive Data-Borrowing for Improving Treatment Effect Estimation using External Controls</title>
      <link>https://arxiv.org/abs/2508.03282</link>
      <description>arXiv:2508.03282v2 Announce Type: replace 
Abstract: Randomized controlled trials (RCTs) often exhibit limited inferential efficiency in estimating treatment effects due to small sample sizes. In recent years, the combination of external controls has gained increasing attention as a means of improving the efficiency of RCTs. However, external controls are not always comparable to RCTs, and direct borrowing without careful evaluation can introduce substantial bias and reduce the efficiency of treatment effect estimation. In this paper, we propose a novel influence-based adaptive sample borrowing approach that effectively quantifies the comparability of each sample in the external controls using influence function theory. Given a selected set of borrowed external controls, we further derive a semiparametric efficient estimator under an exchangeability assumption. Recognizing that the exchangeability assumption may not hold for all possible borrowing sets, we conduct a detailed analysis of the asymptotic bias and variance of the proposed estimator under violations of exchangeability. Building on this bias-variance trade-off, we further develop a data-driven approach to select the optimal subset of external controls for borrowing. Extensive simulations and real-world applications demonstrate that the proposed approach significantly enhances treatment effect estimation efficiency in RCTs, outperforming existing approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.03282v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qinwei Yang, Jingyi Li, Peng Wu</dc:creator>
    </item>
    <item>
      <title>Semiparametric Causal Inference for Right-Censored Outcomes with Many Weak Invalid Instruments</title>
      <link>https://arxiv.org/abs/2509.13176</link>
      <description>arXiv:2509.13176v2 Announce Type: replace 
Abstract: We propose a semiparametric framework for causal inference with right-censored survival outcomes and many weak invalid instruments, motivated by Mendelian randomization in biobank studies where classical methods may fail. We adopt an accelerated failure time model and construct a moment condition based on augmented inverse probability of censoring weighting, incorporating both uncensored and censored observations. Under a heteroscedasticity-based condition on the treatment model, we establish point identification of the causal effect despite censoring and invalid instruments. We propose GEL-NOW (Generalized Empirical Likelihood with Non-Neyman Orthogonal and Weak moments) for valid inference under these conditions. A divergent number of Neyman orthogonal nuisance functions is estimated using deep neural networks. A key challenge is that the conditional censoring distribution is a non-Neyman orthogonal nuisance, contributing to the first-order asymptotics of the estimator for the target causal effect parameter. We derive the asymptotic distribution and explicitly incorporate this additional uncertainty into the asymptotic variance formula. We also introduce a censoring-adjusted over-identification test that accounts for this new variance component. Simulation studies and UK Biobank applications demonstrate the method's robustness and practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13176v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qiushi Bu, Wen Su, Xingqiu Zhao, Zhonghua Liu</dc:creator>
    </item>
    <item>
      <title>The Challenges of Hyperparameter Tuning for Accurate Causal Effect Estimation</title>
      <link>https://arxiv.org/abs/2303.01412</link>
      <description>arXiv:2303.01412v2 Announce Type: replace-cross 
Abstract: ML is playing an increasingly crucial role in estimating causal effects of treatments on outcomes from observational data. Many ML methods (`causal estimators') have been proposed for this task. All of these methods, as with any ML approach, require extensive hyperparameter tuning. For non-causal predictive tasks, there is a consensus on the choice of tuning metrics (e.g. mean squared error), making it simple to compare models. However, for causal inference tasks, such a consensus is yet to be reached, making any comparison of causal models difficult. On top of that, there is no ideal metric on which to tune causal estimators, so one must rely on proxies. Furthermore, the fact that model selection in causal inference involves multiple components (causal estimator, ML regressor, hyperparameters, metric), complicates the issue even further. In order to evaluate the importance of each component, we perform an extensive empirical study on their combination. Our experimental setup involves many commonly used causal estimators, regressors (`base learners' henceforth) and metrics applied to four well-known causal inference benchmark datasets. Our results show that hyperparameter tuning increased the probability of reaching state-of-the-art performance in average ($65\% {\rightarrow} 81\%$) and individualised ($50\% {\rightarrow} 57\%$) effect estimation with only commonly used estimators. We also show that the performance of standard metrics can be inconsistent across different scenarios. Our findings highlight the need for further research to establish whether metrics uniformly capable of state-of-the-art performance in causal model evaluation can be found.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.01412v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damian Machlanski, Spyridon Samothrakis, Paul Clarke</dc:creator>
    </item>
    <item>
      <title>A fast, flexible simulation framework for Bayesian adaptive designs -- the R package BATSS</title>
      <link>https://arxiv.org/abs/2410.02050</link>
      <description>arXiv:2410.02050v2 Announce Type: replace-cross 
Abstract: The use of Bayesian adaptive designs for randomised controlled trials has been hindered by the lack of software readily available to statisticians. We have developed a new software package (Bayesian Adaptive Trials Simulator Software - BATSS for the statistical software R, which provides a flexible structure for the fast simulation of Bayesian adaptive designs for clinical trials. We illustrate how the BATSS package can be used to define and evaluate the operating characteristics of Bayesian adaptive designs for various different types of primary outcomes (e.g., those that follow a normal, binary, Poisson or negative binomial distribution) and can incorporate the most common types of adaptations: stopping treatments (or the entire trial) for efficacy or futility, and Bayesian response adaptive randomisation - based on user-defined adaptation rules. Other important features of this highly modular package include: the use of (Integrated Nested) Laplace approximations to compute posterior distributions, parallel processing on a computer or a cluster, customisability, adjustment for covariates and a wide range of available conditional distributions for the response.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02050v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 06 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominique-Laurent Couturier, Rainer Puhr, Stephane Heritier, Thomas Jaki, Elizabeth G Ryan</dc:creator>
    </item>
  </channel>
</rss>
