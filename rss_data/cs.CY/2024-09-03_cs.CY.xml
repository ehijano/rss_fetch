<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Sep 2024 04:02:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Enhancing Trustworthiness and Minimising Bias Issues in Leveraging Social Media Data for Disaster Management Response</title>
      <link>https://arxiv.org/abs/2409.00004</link>
      <description>arXiv:2409.00004v1 Announce Type: new 
Abstract: Disaster events often unfold rapidly, necessitating a swift and effective response. Developing action plans, resource allocation, and resolution of help requests in disaster scenarios is time-consuming and complex since disaster-relevant information is often uncertain. Leveraging real-time data can significantly deal with data uncertainty and enhance disaster response efforts. To deal with real-time data uncertainty, social media appeared as an alternative effective source of real-time data as there has been extensive use of social media during and after the disasters. However, it also brings forth challenges regarding trustworthiness and bias in these data. To fully leverage social media data for disaster management, it becomes crucial to mitigate biases that may arise due to specific disaster types or regional contexts. Additionally, the presence of misinformation within social media data raises concerns about the reliability of data sources, potentially impeding actionable insights and leading to improper resource utilization. To overcome these challenges, our research aimed to investigate how to ensure trustworthiness and address biases in social media data. We aim to investigate and identify the factors that can be used to enhance trustworthiness and minimize bias to make an efficient and scalable disaster management system utilizing real-time social media posts, identify disaster-related keywords, and assess the severity of the disaster. By doing so, the integration of real-time social data can improve the speed and accuracy of disaster management systems</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00004v1</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samia Abid, Bhupesh Kumar Mishra, Dhavalkumar Thakker, Nishikant Mishra</dc:creator>
    </item>
    <item>
      <title>Navigating the sociotechnical labyrinth: Dynamic certification for responsible embodied AI</title>
      <link>https://arxiv.org/abs/2409.00015</link>
      <description>arXiv:2409.00015v1 Announce Type: new 
Abstract: Sociotechnical requirements shape the governance of artificially intelligent (AI) systems. In an era where embodied AI technologies are rapidly reshaping various facets of contemporary society, their inherent dynamic adaptability presents a unique blend of opportunities and challenges. Traditional regulatory mechanisms, often designed for static -- or slower-paced -- technologies, find themselves at a crossroads when faced with the fluid and evolving nature of AI systems. Moreover, typical problems in AI, for example, the frequent opacity and unpredictability of the behaviour of the systems, add additional sociotechnical challenges.
  To address these interconnected issues, we introduce the concept of dynamic certification, an adaptive regulatory framework specifically crafted to keep pace with the continuous evolution of AI systems. The complexity of these challenges requires common progress in multiple domains: technical, socio-governmental, and regulatory. Our proposed transdisciplinary approach is designed to ensure the safe, ethical, and practical deployment of AI systems, aligning them bidirectionally with the real-world contexts in which they operate. By doing so, we aim to bridge the gap between rapid technological advancement and effective regulatory oversight, ensuring that AI systems not only achieve their intended goals but also adhere to ethical standards and societal values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00015v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Georgios Bakirtzis, Andrea Aler Tubella, Andreas Theodorou, David Danks, Ufuk Topcu</dc:creator>
    </item>
    <item>
      <title>Online Advertising is a Regrettable Necessity</title>
      <link>https://arxiv.org/abs/2409.00026</link>
      <description>arXiv:2409.00026v1 Announce Type: new 
Abstract: The exponential growth of the web and its benefits can be attributed largely to its open model where anyone with internet connection can access information on the web for free. This has created unprecedented opportunities for various members of society including the most vulnerable, as recognized by organizations such as the UN. This again can be attributed to online advertising, which has been the main financier to the open web. However, recent trends of paywalling information and services on the web are creating imminent dangers to such open model of the web, inhibiting access for the economically vulnerable, and eventually creating digital segregation. In this paper, we argue that this emerging model lacks sustainability, exacerbates digital divide, and might lead to collapse of online advertising. We revisit the ad-supported open web business model and demonstrate how global users actually pay for the ads they see. Using data on GNI (gross national income) per capita and average paywall access costs, we established a simple income-paywall expenditure gap baseline. With this baseline we show that 135 countries with a total population estimate of 6.56 billion people cannot afford a scenario of a fully paywalled web. We further discuss how a mixed model of the so-called "premium services" creates digital segregation and poses danger to online advertising ecosystem. Finally, we call for further research and policy initiatives to keep the web open and more inclusive with a sustainable business model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00026v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yonas Kassa</dc:creator>
    </item>
    <item>
      <title>The Artificial Intelligence Act: critical overview</title>
      <link>https://arxiv.org/abs/2409.00264</link>
      <description>arXiv:2409.00264v1 Announce Type: new 
Abstract: This article provides a critical overview of the recently approved Artificial Intelligence Act. It starts by presenting the main structure, objectives, and approach of Regulation (EU) 2024/1689. A definition of key concepts follows, and then the material and territorial scope, as well as the timing of application, are analyzed. Although the Regulation does not explicitly set out principles, the main ideas of fairness, accountability, transparency, and equity in AI underly a set of rules of the regulation. This is discussed before looking at the ill-defined set of forbidden AI practices (manipulation and e exploitation of vulnerabilities, social scoring, biometric identification and classification, and predictive policing). It is highlighted that those rules deal with behaviors rather than AI systems. The qualification and regulation of high-risk AI systems are tackled, alongside the obligation of transparency for certain systems, the regulation of general-purpose models, and the rules on certification, supervision, and sanctions. The text concludes that even if the overall framework can be deemed adequate and balanced, the approach is so complex that it risks defeating its own purpose of promoting responsible innovation within the European Union and beyond its borders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00264v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Nuno Sousa e Silva</dc:creator>
    </item>
    <item>
      <title>Anti-woke agenda, gender issues, revisionism and hate speech communities on Brazilian Telegram: from harmful reactionary speech to the crime of glorifying Nazism and Hitler</title>
      <link>https://arxiv.org/abs/2409.00325</link>
      <description>arXiv:2409.00325v1 Announce Type: new 
Abstract: Resistance to progressive policies and hate speech have been consolidating on Brazilian Telegram, with anti-woke communities rejecting diversity and promoting a worldview that sees these social changes as a threat. Therefore, this study aims to address the research question: how are Brazilian conspiracy theory communities on anti-woke agenda, gender issues, revisionism and hate speech topics characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies whose main objective is to understand and characterize Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on arXiv at Cornell University, applying a mirrored method across the seven studies, changing only the thematic object of analysis and providing investigation replicability, including with proprietary and authored codes, adding to the culture of free and open-source software. Regarding the main findings of this study, the following were observed: Anti-woke communities emerge as central forces in the Brazilian conspiracy ecosystem; During crises, mentions of hate speech and revisionism have increased significantly, reflecting polarization; Nazi communities on Telegram propagate extremist ideologies, glorifying Hitler; The interconnectivity between anti-woke, anti-gender and revisionism strengthens the ecosystem of hate; Anti-gender speech facilitates the spread of anti-vaccine disinformation, creating an intersection between health and conspiracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00325v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>A Novel Taxonomy for Navigating and Classifying Synthetic Data in Healthcare Applications</title>
      <link>https://arxiv.org/abs/2409.00701</link>
      <description>arXiv:2409.00701v1 Announce Type: new 
Abstract: Data-driven technologies have improved the efficiency, reliability and effectiveness of healthcare services, but come with an increasing demand for data, which is challenging due to privacy-related constraints on sharing data in healthcare contexts. Synthetic data has recently gained popularity as potential solution, but in the flurry of current research it can be hard to oversee its potential. This paper proposes a novel taxonomy of synthetic data in healthcare to navigate the landscape in terms of three main varieties. Data Proportion comprises different ratios of synthetic data in a dataset and associated pros and cons. Data Modality refers to the different data formats amenable to synthesis and format-specific challenges. Data Transformation concerns improving specific aspects of a dataset like its utility or privacy with synthetic data. Our taxonomy aims to help researchers in the healthcare domain interested in synthetic data to grasp what types of datasets, data modalities, and transformations are possible with synthetic data, and where the challenges and overlaps between the varieties lie.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00701v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bram van Dijk, Saif ul Islam, Jim Achterberg, Hafiz Muhammad Waseem, Parisis Gallos, Gregory Epiphaniou, Carsten Maple, Marcel Haas, Marco Spruit</dc:creator>
    </item>
    <item>
      <title>Who Would Chatbots Vote For? Political Preferences of ChatGPT and Gemini in the 2024 European Union Elections</title>
      <link>https://arxiv.org/abs/2409.00721</link>
      <description>arXiv:2409.00721v1 Announce Type: new 
Abstract: This study examines the political bias of chatbots powered by large language models, namely ChatGPT and Gemini, in the context of the 2024 European Parliament elections. The research focused on the evaluation of political parties represented in the European Parliament across 27 EU Member States by these generative artificial intelligence (AI) systems. The methodology involved daily data collection through standardized prompts on both platforms. The results revealed a stark contrast: while Gemini mostly refused to answer political questions, ChatGPT provided consistent ratings. The analysis showed a significant bias in ChatGPT in favor of left-wing and centrist parties, with the highest ratings for the Greens/European Free Alliance. In contrast, right-wing parties, particularly the Identity and Democracy group, received the lowest ratings. The study identified key factors influencing the ratings, including attitudes toward European integration and perceptions of democratic values. The findings highlight the need for a critical approach to information provided by generative AI systems in a political context and call for more transparency and regulation in this area.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00721v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Haman, Milan \v{S}koln\'ik</dc:creator>
    </item>
    <item>
      <title>Digital Homunculi: Reimagining Democracy Research with Generative Agents</title>
      <link>https://arxiv.org/abs/2409.00826</link>
      <description>arXiv:2409.00826v1 Announce Type: new 
Abstract: The pace of technological change continues to outstrip the evolution of democratic institutions, creating an urgent need for innovative approaches to democratic reform. However, the experimentation bottleneck - characterized by slow speed, high costs, limited scalability, and ethical risks - has long hindered progress in democracy research. This paper proposes a novel solution: employing generative artificial intelligence (GenAI) to create synthetic data through the simulation of digital homunculi, GenAI-powered entities designed to mimic human behavior in social contexts. By enabling rapid, low-risk experimentation with alternative institutional designs, this approach could significantly accelerate democratic innovation. I examine the potential of GenAI-assisted research to mitigate current limitations in democratic experimentation, including the ability to simulate large-scale societal interactions and test complex institutional mechanisms. While acknowledging potential risks such as algorithmic bias, reproducibility challenges, and AI alignment issues, I argue that the benefits of synthetic data are likely to outweigh their drawbacks if implemented with proper caution. To address existing challenges, I propose a range of technical, methodological, and institutional adaptations. The paper concludes with a call for interdisciplinary collaboration in the development and implementation of GenAI-assisted methods in democracy research, highlighting their potential to bridge the gap between democratic theory and practice in an era of rapid technological change.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00826v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Petr Specian</dc:creator>
    </item>
    <item>
      <title>Comprehensive up-to-date impact of the IoMT in healthcare and patients</title>
      <link>https://arxiv.org/abs/2409.01287</link>
      <description>arXiv:2409.01287v1 Announce Type: new 
Abstract: The Internet of Medical Things (IoMT) is a quickly expanding field that intends to develop the features, effectiveness, and availability of healthcare services by applying numerous technologies to gather and diffuse medical data. IoMT devices incorporate wearable sensors, implantable devices, smart home methods, telemedicine policies, and mobile applications. IoMT applications range from chronic disease administration, remote patient monitoring, emergency response, and clinical decision support to health promotion and wellness. This paper aligns on the advantages, defies, and outlook directions of this developing domain. The paper also examines the ethical, legal, and social implications of IoMT, as well as the possible risks and vulnerabilities of the IoMT environment</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01287v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5121/hiij.2024.13301</arxiv:DOI>
      <arxiv:journal_reference>Health Informatics: An International Journal(Hiij) vol. 13, No.3 August 2024</arxiv:journal_reference>
      <dc:creator>Guy. Mouanda</dc:creator>
    </item>
    <item>
      <title>Mutual Benefit: The Case for Sharing Autonomous Vehicle Data with the Public</title>
      <link>https://arxiv.org/abs/2409.01342</link>
      <description>arXiv:2409.01342v1 Announce Type: new 
Abstract: Autonomous driving is a widely researched technology that is frequently tested on public roads. The data generated from these tests represent an essential competitive element for the respective companies moving this technology forward. In this paper, we argue for the normative idea that a part of this data should more explicitly benefit the general public by sharing it through a trusted entity as a form of compensation and control for the communities that are being experimented upon. To support this argument, we highlight what data is available to be shared, make the ethical case for sharing autonomous vehicle data, present case studies in how AV data is currently shared, draw from existing data-sharing platforms from similar transportation industries to make recommendations on how data should be shared and conclude with arguments as to why such data-sharing should be encouraged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01342v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Goedicke, Natalie Chyi, Alexandra Bremers, Stacey Li, James Grimmelmann, Wendy Ju</dc:creator>
    </item>
    <item>
      <title>Human-Centered AI Applications for Canada's Immigration Settlement Sector</title>
      <link>https://arxiv.org/abs/2409.01461</link>
      <description>arXiv:2409.01461v1 Announce Type: new 
Abstract: While AI has been frequently applied in the context of immigration, most of these applications focus on selection and screening, which primarily serve to empower states and authorities, raising concerns due to their understudied reliability and high impact on immigrants' lives. In contrast, this paper emphasizes the potential of AI in Canada's immigration settlement phase, a stage where access to information is crucial and service providers are overburdened. By highlighting the settlement sector as a prime candidate for reliable AI applications, we demonstrate its unique capacity to empower immigrants directly, yet it remains under-explored in AI research. We outline a vision for human-centred and responsible AI solutions that facilitate the integration of newcomers. We call on AI researchers to build upon our work and engage in multidisciplinary research and active collaboration with service providers and government organizations to develop tailored AI tools that are empowering, inclusive and safe.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01461v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isar Nejadgholi, Maryam Molamohammadi, Kimiya Missaghi, Samir Bakhtawar</dc:creator>
    </item>
    <item>
      <title>Lecture Notes from the NaijaCoder Summer Camp</title>
      <link>https://arxiv.org/abs/2409.01499</link>
      <description>arXiv:2409.01499v1 Announce Type: new 
Abstract: The NaijaCoder in-person summer camps are intensive programs for high school and pre-college students in Nigeria. The programs are meant to provide free instruction on the basics of algorithms and computer programming. In 2024, the camps were held in two locations within the country: (i) the Federal Capital Territory (F.C.T.), Abuja; and (ii) Lagos state. Both locations relied on the same set of notes for instructional purposes. We are providing these notes in a publicly-available medium for both students and teachers to review after the main in-person programs are over.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01499v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel Alabi, Joseph Ekpenyong, Alida Monaco</dc:creator>
    </item>
    <item>
      <title>METcross: A framework for short-term forecasting of cross-city metro passenger flow</title>
      <link>https://arxiv.org/abs/2409.01515</link>
      <description>arXiv:2409.01515v1 Announce Type: new 
Abstract: Metro operation management relies on accurate predictions of passenger flow in the future. This study begins by integrating cross-city (including source and target city) knowledge and developing a short-term passenger flow prediction framework (METcross) for the metro. Firstly, we propose a basic framework for modeling cross-city metro passenger flow prediction from the perspectives of data fusion and transfer learning. Secondly, METcross framework is designed to use both static and dynamic covariates as inputs, including economy and weather, that help characterize station passenger flow features. This framework consists of two steps: pre-training on the source city and fine-tuning on the target city. During pre-training, data from the source city trains the feature extraction and passenger flow prediction models. Fine-tuning on the target city involves using the source city's trained model as the initial parameter and fusing the feature embeddings of both cities to obtain the passenger flow prediction results. Finally, we tested the basic prediction framework and METcross framework on the metro networks of Wuxi and Chongqing to experimentally analyze their efficacy. Results indicate that the METcross framework performs better than the basic framework and can reduce the Mean Absolute Error and Root Mean Squared Error by 22.35% and 26.18%, respectively, compared to single-city prediction models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01515v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenbo Lu, Jinhua Xu, Peikun Li, Ting Wang, Yong Zhang</dc:creator>
    </item>
    <item>
      <title>Empirical evidence of Large Language Model's influence on human spoken communication</title>
      <link>https://arxiv.org/abs/2409.01754</link>
      <description>arXiv:2409.01754v1 Announce Type: new 
Abstract: Artificial Intelligence (AI) agents now interact with billions of humans in natural language, thanks to advances in Large Language Models (LLMs) like ChatGPT. This raises the question of whether AI has the potential to shape a fundamental aspect of human culture: the way we speak. Recent analyses revealed that scientific publications already exhibit evidence of AI-specific language. But this evidence is inconclusive, since scientists may simply be using AI to copy-edit their writing. To explore whether AI has influenced human spoken communication, we transcribed and analyzed about 280,000 English-language videos of presentations, talks, and speeches from more than 20,000 YouTube channels of academic institutions. We find a significant shift in the trend of word usage specific to words distinctively associated with ChatGPT following its release. These findings provide the first empirical evidence that humans increasingly imitate LLMs in their spoken language. Our results raise societal and policy-relevant concerns about the potential of AI to unintentionally reduce linguistic diversity, or to be deliberately misused for mass manipulation. They also highlight the need for further investigation into the feedback loops between machine behavior and human culture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01754v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hiromu Yakura, Ezequiel Lopez-Lopez, Levin Brinkmann, Ignacio Serna, Prateek Gupta, Iyad Rahwan</dc:creator>
    </item>
    <item>
      <title>Adoption of smartphones among older adults and the role of perceived threat of cyberattacks</title>
      <link>https://arxiv.org/abs/2409.01771</link>
      <description>arXiv:2409.01771v1 Announce Type: new 
Abstract: Adoption of smartphones by older adults (i.e., 65+ years old) is poorly understood, especially in relation to cybersecurity and cyberthreats. In this study, we focus on the perceived threat of cyberattacks as a potential barrier to smartphone adoption and use among older adults. The study also aims at investigating the differences between users and non-users of smartphones. We conducted a quantitative cross-sectional survey of older adults in Slovenia (N = 535). The results of covariance-based structural equation modeling indicate consistent support for the associations of intention to use (ItU) with perceived usefulness (PU), subjective norm (SN) and attitude toward use (AtU), the association between ease of use (EoU) and PU, the association between hedonic motivation (HM) and AtU, and the association between smartphone technology anxiety (STA) and fear of use (FoU). Even though the negative association between perceived threat (PT) and ItU was significant in the full sample, the non-user and the not aware subsamples, its role in adoption of smartphones among older adults remains puzzling. We uncovered significant positive associations between PT and AtU (except in the not aware subsample), and PT and PU which we could not fully explain in our study. The results of our study provide some insights on how campaigns promoting adoption of smartphones among older adults, workshops, training and informal teaching might be improved.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01771v1</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Patrik Pucer, Bo\v{s}tjan \v{Z}vanut, Simon Vrhovec</dc:creator>
    </item>
    <item>
      <title>DAOs of Collective Intelligence? Unraveling the Complexity of Blockchain Governance in Decentralized Autonomous Organizations</title>
      <link>https://arxiv.org/abs/2409.01823</link>
      <description>arXiv:2409.01823v1 Announce Type: new 
Abstract: Decentralized autonomous organizations (DAOs) have transformed organizational structures by shifting from traditional hierarchical control to decentralized approaches, leveraging blockchain and cryptoeconomics. Despite managing significant funds and building global networks, DAOs face challenges like declining participation, increasing centralization, and inabilities to adapt to changing environments, which stifle innovation. This paper explores DAOs as complex systems and applies complexity science to explain their inefficiencies. In particular, we discuss DAO challenges, their complex nature, and introduce the self-organization mechanisms of collective intelligence, digital democracy, and adaptation. By applying these mechansims to improve DAO design and construction, a practical design framework for DAOs is created. This contribution lays a foundation for future research at the intersection of complexity science and DAOs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01823v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <category>physics.app-ph</category>
      <category>physics.soc-ph</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mark C. Ballandies, Dino Carpentras, Evangelos Pournaras</dc:creator>
    </item>
    <item>
      <title>Observing Context Improves Disparity Estimation when Race is Unobserved</title>
      <link>https://arxiv.org/abs/2409.01984</link>
      <description>arXiv:2409.01984v1 Announce Type: new 
Abstract: In many domains, it is difficult to obtain the race data that is required to estimate racial disparity. To address this problem, practitioners have adopted the use of proxy methods which predict race using non-protected covariates. However, these proxies often yield biased estimates, especially for minority groups, limiting their real-world utility. In this paper, we introduce two new contextual proxy models that advance existing methods by incorporating contextual features in order to improve race estimates. We show that these algorithms demonstrate significant performance improvements in estimating disparities on real-world home loan and voter data. We establish that achieving unbiased disparity estimates with contextual proxies relies on mean-consistency, a calibration-like condition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01984v1</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kweku Kwegyir-Aggrey, Naveen Durvasula, Jennifer Wang, Suresh Venkatasubramanian</dc:creator>
    </item>
    <item>
      <title>The Authentication Gap: Higher Education's Widespread Noncompliance with NIST Digital Identity Guidelines</title>
      <link>https://arxiv.org/abs/2409.00546</link>
      <description>arXiv:2409.00546v1 Announce Type: cross 
Abstract: We examine the authentication practices of a diverse set of 101 colleges and universities in the United States and Canada to determine compliance with five standards in NIST Special Publication 800-63-3 Digital Identity Guidelines. We find widespread noncompliance with standards for password expiration, password composition rules, and knowledge-based authentication. Many institutions still require or recommend noncompliant practices despite years of expert advice and standards to the contrary. Furthermore, we observe that regional and liberal arts colleges have generally lower documented compliance rates than national and global universities, motivating further investment in authentication security at these institutions. These results are a wake-up call that expert cybersecurity recommendations are not sufficiently influencing the policies of higher education institutions, leaving the sector vulnerable to increasingly prevalent ransomware and other cyberattacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00546v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noah Apthorpe, Boen Beavers, Yan Shvartzshnaider, Brett Frischmann</dc:creator>
    </item>
    <item>
      <title>Adapting Physics-Informed Neural Networks for Bifurcation Detection in Ecological Migration Models</title>
      <link>https://arxiv.org/abs/2409.00651</link>
      <description>arXiv:2409.00651v1 Announce Type: cross 
Abstract: In this study, we explore the application of Physics-Informed Neural Networks (PINNs) to the analysis of bifurcation phenomena in ecological migration models. By integrating the fundamental principles of diffusion-advection-reaction equations with deep learning techniques, we address the complexities of species migration dynamics, particularly focusing on the detection and analysis of Hopf bifurcations. Traditional numerical methods for solving partial differential equations (PDEs) often involve intricate calculations and extensive computational resources, which can be restrictive in high-dimensional problems. In contrast, PINNs offer a more flexible and efficient alternative, bypassing the need for grid discretization and allowing for mesh-free solutions. Our approach leverages the DeepXDE framework, which enhances the computational efficiency and applicability of PINNs in solving high-dimensional PDEs. We validate our results against conventional methods and demonstrate that PINNs not only provide accurate bifurcation predictions but also offer deeper insights into the underlying dynamics of diffusion processes. Despite these advantages, the study also identifies challenges such as the high computational costs and the sensitivity of PINN performance to network architecture and hyperparameter settings. Future work will focus on optimizing these algorithms and expanding their application to other complex systems involving bifurcations. The findings from this research have significant implications for the modeling and analysis of ecological systems, providing a powerful tool for predicting and understanding complex dynamical behaviors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.00651v1</guid>
      <category>nlin.CD</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lujie Yin, Xing Lv</dc:creator>
    </item>
    <item>
      <title>Large-scale Urban Facility Location Selection with Knowledge-informed Reinforcement Learning</title>
      <link>https://arxiv.org/abs/2409.01588</link>
      <description>arXiv:2409.01588v1 Announce Type: cross 
Abstract: The facility location problem (FLP) is a classical combinatorial optimization challenge aimed at strategically laying out facilities to maximize their accessibility. In this paper, we propose a reinforcement learning method tailored to solve large-scale urban FLP, capable of producing near-optimal solutions at superfast inference speed. We distill the essential swap operation from local search, and simulate it by intelligently selecting edges on a graph of urban regions, guided by a knowledge-informed graph neural network, thus sidestepping the need for heavy computation of local search. Extensive experiments on four US cities with different geospatial conditions demonstrate that our approach can achieve comparable performance to commercial solvers with less than 5\% accessibility loss, while displaying up to 1000 times speedup. We deploy our model as an online geospatial application at https://huggingface.co/spaces/randommmm/MFLP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01588v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3678717.3691254</arxiv:DOI>
      <dc:creator>Hongyuan Su, Yu Zheng, Jingtao Ding, Depeng Jin, Yong Li</dc:creator>
    </item>
    <item>
      <title>Private Electronic Payments with Self-Custody and Zero-Knowledge Verified Reissuance</title>
      <link>https://arxiv.org/abs/2409.01958</link>
      <description>arXiv:2409.01958v1 Announce Type: cross 
Abstract: This article builds upon the protocol for digital transfers described by Goodell, Toliver, and Nakib, which combines privacy by design for consumers with strong compliance enforcement for recipients of payments and self-validating assets that carry their own verifiable provenance information. We extend the protocol to allow for the verification that reissued assets were created in accordance with rules prohibiting the creation of new assets by anyone but the issuer, without exposing information about the circumstances in which the assets were created that could be used to identify the payer. The modified protocol combines an audit log with zero-knowledge proofs, so that a consumer spending an asset can demonstrate that there exists a valid entry on the audit log that is associated with the asset, without specifying which entry it is. This property is important as a means to allow money to be reissued within the system without the involvement of system operators within the zone of control of the original issuer. Additionally, we identify a key property of privacy-respecting electronic payments, wherein the payer is not required to retain secrets arising from one transaction until the following transaction, and argue that this property is essential to framing security requirements for storage of digital assets and the risk of blackmail or coercion as a way to exfiltrate information about payment history. We claim that the design of our protocol strongly protects the anonymity of payers with respect to their payment transactions, while preventing the creation of assets by any party other than the original issuer without destroying assets of equal value.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.01958v1</guid>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniele Friolo, Geoffrey Goodell, Dann Toliver, Hazem Danny Nakib</dc:creator>
    </item>
    <item>
      <title>The overlooked need for Ethics in Complexity Science: Why it matters</title>
      <link>https://arxiv.org/abs/2409.02002</link>
      <description>arXiv:2409.02002v1 Announce Type: cross 
Abstract: Complexity science, despite its broad scope and potential impact, has not kept pace with fields like artificial intelligence, biotechnology and social sciences in addressing ethical concerns. The field lacks a comprehensive ethical framework, leaving us, as a community, vulnerable to ethical challenges and dilemmas. Other areas have gone through similar experiences and created, with discussions and working groups, their guides, policies and recommendations. Therefore, here we highlight the critical absence of formal guidelines, dedicated ethical committees, and widespread discussions on ethics within the complexity science community. Drawing on insights from the disciplines mentioned earlier, we propose a roadmap to enhance ethical awareness and action. Our recommendations include (i) initiating supportive mechanisms to develop ethical guidelines specific to complex systems research, (ii) creating open-access resources, and (iii) fostering inclusive dialogues to ensure that complexity science can responsibly tackle societal challenges and achieve a more inclusive environment. By initiating this dialogue, we aim to encourage a necessary shift in how ethics is integrated into complexity research, positioning the field to address contemporary challenges more effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02002v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Olumide Adisa, Enio Alterman Blay, Yasaman Asgari, Gabriele Di Bona, Samantha Dies, Ana Maria Jaramillo, Paulo H. Resende, Ana Maria de Sousa Leitao</dc:creator>
    </item>
    <item>
      <title>Pump and Dumps in the Bitcoin Era: Real Time Detection of Cryptocurrency Market Manipulations</title>
      <link>https://arxiv.org/abs/2005.06610</link>
      <description>arXiv:2005.06610v2 Announce Type: replace 
Abstract: In the last years, cryptocurrencies are increasingly popular. Even people who are not experts have started to invest in these securities and nowadays cryptocurrency exchanges process transactions for over 100 billion US dollars per month. However, many cryptocurrencies have low liquidity and therefore they are highly prone to market manipulation schemes. In this paper, we perform an in-depth analysis of pump and dump schemes organized by communities over the Internet. We observe how these communities are organized and how they carry out the fraud. Then, we report on two case studies related to pump and dump groups. Lastly, we introduce an approach to detect the fraud in real time that outperforms the current state of the art, so to help investors stay out of the market when a pump and dump scheme is in action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2005.06610v2</guid>
      <category>cs.CY</category>
      <category>cs.CR</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICCCN49398.2020.9209660</arxiv:DOI>
      <dc:creator>Massimo La Morgia, Alessandro Mei, Francesco Sassi, Julinda Stefa</dc:creator>
    </item>
    <item>
      <title>The Doge of Wall Street: Analysis and Detection of Pump and Dump Cryptocurrency Manipulations</title>
      <link>https://arxiv.org/abs/2105.00733</link>
      <description>arXiv:2105.00733v2 Announce Type: replace 
Abstract: Cryptocurrencies are increasingly popular. Even people who are not experts have started to invest in these assets, and nowadays, cryptocurrency exchanges process transactions for over 100 billion US dollars per month. Despite this, many cryptocurrencies have low liquidity and are highly prone to market manipulation. This paper performs an in-depth analysis of two market manipulations organized by communities over the Internet: The pump and dump and the crowd pump. The pump and dump scheme is a fraud as old as the stock market. Now, it got new vitality in the loosely regulated market of cryptocurrencies. Groups of highly coordinated people systematically arrange this scam, usually on Telegram and Discord. We monitored these groups for more than 3 years detecting around 900 individual events. We report on three case studies related to pump and dump groups. We leverage our unique dataset of the verified pump and dumps to build a machine learning model able to detect a pump and dump in 25 seconds from the moment it starts, achieving the results of 94.5% of F1-score. Then, we move on to the crowd pump, a new phenomenon that hit the news in the first months of 2021, when a Reddit community inflates the price of the GameStop stocks (GME) by over 1,900% on Wall Street, the world's largest stock exchange. Later, other Reddit communities replicate the operation on the cryptocurrency markets. The targets were DogeCoin (DOGE) and Ripple (XRP). We reconstruct how these operations developed and discuss differences and analogies with the standard pump and dump. We believe this study helps understand a widespread phenomenon affecting cryptocurrency markets. The detection algorithms we develop effectively detect these events in real-time and help investors stay out of the market when these frauds are in action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.00733v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3561300</arxiv:DOI>
      <arxiv:journal_reference>ACM Transactions on Internet Technology 23, 1, Article 11 (February 2023), 28 pages</arxiv:journal_reference>
      <dc:creator>Massimo La Morgia, Alessandro Mei, Francesco Sassi, Julinda Stefa</dc:creator>
    </item>
    <item>
      <title>Token Spammers, Rug Pulls, and SniperBots: An Analysis of the Ecosystem of Tokens in Ethereum and in the Binance Smart Chain (BNB)</title>
      <link>https://arxiv.org/abs/2206.08202</link>
      <description>arXiv:2206.08202v3 Announce Type: replace 
Abstract: In this work, we perform a longitudinal analysis of the BNB Smart Chain and Ethereum blockchain from their inception to March 2022. We study the ecosystem of the tokens and liquidity pools, highlighting analogies and differences between the two blockchains. We discover that about 60% of tokens are active for less than one day. Moreover, we find that 1% of addresses create an anomalous number of tokens (between 20% and 25%). We discover that these tokens are used as disposable tokens to perform a particular type of rug pull, which we call 1-day rug pull. We quantify the presence of this operation on both blockchains discovering its prevalence on the BNB Smart Chain. We estimate that 1-day rug pulls generated $240 million in profits. Finally, we present sniper bots, a new kind of trader bot involved in these activities, and we detect their presence and quantify their activity in the rug pull operations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08202v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>https://www.usenix.org/conference/usenixsecurity23/presentation/cernera ISBN: 978-1-939133-37-3 Year:2023</arxiv:journal_reference>
      <dc:creator>Federico Cernera, Massimo La Morgia, Alessandro Mei, Francesco Sassi</dc:creator>
    </item>
    <item>
      <title>A Game of NFTs: Characterizing NFT Wash Trading in the Ethereum Blockchain</title>
      <link>https://arxiv.org/abs/2212.01225</link>
      <description>arXiv:2212.01225v3 Announce Type: replace 
Abstract: The Non-Fungible Token (NFT) market in the Ethereum blockchain experienced explosive growth in 2021, with a monthly trade volume reaching \$6 billion in January 2022. However, concerns have emerged about possible wash trading, a form of market manipulation in which one party repeatedly trades an NFT to inflate its volume artificially. Our research examines the effects of wash trading on the NFT market in Ethereum from the beginning until January 2022, using multiple approaches. We find that wash trading affects 5.66% of all NFT collections, with a total artificial volume of \$3,406,110,774. We look at two ways to profit from wash trading: Artificially increasing the price of the NFT and taking advantage of the token reward systems provided by some marketplaces. Our findings show that exploiting the token reward systems of NFTMs is much more profitable (mean gain of successful operations is \$1.055M on LooksRare), more likely to succeed (more than 80% of operations), and less risky than reselling an NFT at a higher price using wash trading (50% of activities result in a loss). Our research highlights that wash trading is frequent in Ethereum and that NFTMs should implement protective mechanisms to stop such illicit behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.01225v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/ICDCS57875.2023.00018</arxiv:DOI>
      <dc:creator>Massimo La Morgia, Alessandro Mei, Alberto Maria Mongardini, Eugenio Nerio Nemmi</dc:creator>
    </item>
    <item>
      <title>The Cultivated Practices of Text-to-Image Generation</title>
      <link>https://arxiv.org/abs/2306.11393</link>
      <description>arXiv:2306.11393v3 Announce Type: replace 
Abstract: Humankind is entering a novel creative era in which anybody can synthesize digital information using generative artificial intelligence (AI). Text-to-image generation, in particular, has become vastly popular and millions of practitioners produce AI-generated images and AI art online. This chapter first gives an overview of the key developments that enabled a healthy co-creative online ecosystem around text-to-image generation to rapidly emerge, followed by a high-level description of key elements in this ecosystem. A particular focus is placed on prompt engineering, a creative practice that has been embraced by the AI art community. It is then argued that the emerging co-creative ecosystem constitutes an intelligent system on its own - a system that both supports human creativity, but also potentially entraps future generations and limits future development efforts in AI. The chapter discusses the potential risks and dangers of cultivating this co-creative ecosystem, such as the bias inherent in today's training data, potential quality degradation in future image generation systems due to synthetic data becoming common place, and the potential long-term effects of text-to-image generation on people's imagination, ambitions, and development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11393v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-66528-8</arxiv:DOI>
      <dc:creator>Jonas Oppenlaender</dc:creator>
    </item>
    <item>
      <title>The Conspiracy Money Machine: Uncovering Telegram's Conspiracy Channels and their Profit Model</title>
      <link>https://arxiv.org/abs/2310.15977</link>
      <description>arXiv:2310.15977v2 Announce Type: replace 
Abstract: In recent years, major social media platforms have implemented increasingly strict moderation policies, resulting in bans and restrictions on conspiracy theory-related content. To circumvent these restrictions, conspiracy theorists are turning to alternatives, such as Telegram, where they can express and spread their views with fewer limitations. Telegram offers channels, virtual rooms where only administrators can broadcast messages, and a more permissive content policy. These features have created the perfect breeding ground for a complex ecosystem of conspiracy channels. In this paper, we illuminate this ecosystem. First, we propose an approach to detect conspiracy channels. Then, we discover that conspiracy channels can be clustered into four distinct communities comprising over 17,000 channels. Finally, we uncover the "Conspiracy Money Machine," revealing how most conspiracy channels actively seek to profit from their subscribers. We find conspiracy theorists leverage e-commerce platforms to sell questionable products or lucratively promote them through affiliate links. Moreover, we observe that conspiracy channels use donation and crowdfunding platforms to raise funds for their campaigns. We determine that this business involves hundreds of thousands of donors and generates a turnover of almost $66 million.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15977v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vincenzo Imperati, Massimo La Morgia, Alessandro Mei, Alberto Maria Mongardini, Francesco Sassi</dc:creator>
    </item>
    <item>
      <title>An\'alise e modelagem de jogos digitais: relato de uma experi\^encia educacional utilizando metodologias ativas em um grupo multidisciplinar</title>
      <link>https://arxiv.org/abs/2311.14704</link>
      <description>arXiv:2311.14704v2 Announce Type: replace 
Abstract: The traditional teaching of software engineering is focused on technical skills. Active strategies, where students experience content and interact with reality, are effective. The market demands new skills in the digital transformation, dealing with the complexity of modeling businesses and the interconnection between people, systems, and technologies. The transition to active methodologies, such as Problem-Based Learning (PBL), brings real market scenarios into the classroom. This article reports on the experience in the course, presenting concepts and results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.14704v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>V Congresso Brasileiro Interdisciplinar em Ci\^encia e Tecnologia - COBICET 2024</arxiv:journal_reference>
      <dc:creator>David de Oliveira Lemes, Ezequiel Fran\c{c}a dos Santos, Eduardo Romanek, Celso Fujimoto, Adriano Felix Valente</dc:creator>
    </item>
    <item>
      <title>Face Recognition: to Deploy or not to Deploy? A Framework for Assessing the Proportional Use of Face Recognition Systems in Real-World Scenarios</title>
      <link>https://arxiv.org/abs/2402.05731</link>
      <description>arXiv:2402.05731v2 Announce Type: replace 
Abstract: Face recognition (FR) has reached a high technical maturity. However, its use needs to be carefully assessed from an ethical perspective, especially in sensitive scenarios. This is precisely the focus of this paper: the use of FR for the identification of specific subjects in moderately to densely crowded spaces (e.g. public spaces, sports stadiums, train stations) and law enforcement scenarios. In particular, there is a need to consider the trade-off between the need to protect privacy and fundamental rights of citizens as well as their safety. Recent Artificial Intelligence (AI) policies, notably the European AI Act, propose that such FR interventions should be proportionate and deployed only when strictly necessary. Nevertheless, concrete guidelines on how to address the concept of proportional FR intervention are lacking to date. This paper proposes a framework to contribute to assessing whether an FR intervention is proportionate or not for a given context of use in the above mentioned scenarios. It also identifies the main quantitative and qualitative variables relevant to the FR intervention decision (e.g. number of people in the scene, level of harm that the person(s) in search could perpetrate, consequences to individual rights and freedoms) and propose a 2D graphical model making it possible to balance these variables in terms of ethical cost vs security gain. Finally, different FR scenarios inspired by real-world deployments validate the proposed model. The framework is conceived as a simple support tool for decision makers when confronted with the deployment of an FR system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05731v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/FG59268.2024.10581866</arxiv:DOI>
      <dc:creator>Pablo Negri, Isabelle Hupont, Emilia Gomez</dc:creator>
    </item>
    <item>
      <title>A Survey on Responsible Generative AI: What to Generate and What Not</title>
      <link>https://arxiv.org/abs/2404.05783</link>
      <description>arXiv:2404.05783v2 Announce Type: replace 
Abstract: In recent years, generative AI (GenAI), like large language models and text-to-image models, has received significant attention across various domains. However, ensuring the responsible generation of content by these models is crucial for their real-world applicability. This raises an interesting question: What should responsible GenAI generate, and what should it not? To answer the question, this paper investigates the practical responsible requirements of both textual and visual generative models, outlining five key considerations: generating truthful content, avoiding toxic content, refusing harmful instruction, leaking no training data-related content, and ensuring generated content identifiable. Specifically, we review recent advancements and challenges in addressing these requirements. Besides, we discuss and emphasize the importance of responsible GenAI across healthcare, education, finance, and artificial general intelligence domains. Through a unified perspective on both textual and visual generative models, this paper aims to provide insights into practical safety-related issues and further benefit the community in building responsible GenAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.05783v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jindong Gu</dc:creator>
    </item>
    <item>
      <title>Qualitative and quantitative analysis of student's perceptions in the use of generative AI in educational environments</title>
      <link>https://arxiv.org/abs/2405.13487</link>
      <description>arXiv:2405.13487v2 Announce Type: replace 
Abstract: The effective integration of generative artificial intelligence in education is a fundamental aspect to prepare future generations. The objective of this study is to analyze from a quantitative and qualitative point of view the perception of controlled student-IA interaction within the classroom. This analysis includes assessing the ethical implications and everyday use of AI tools, as well as understanding whether AI tools encourage students to pursue STEM careers. Several points for improvement in education are found, such as the challenge of getting teachers to engage with new technologies and adapt their methods in all subjects, not just those related to technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13487v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sergio Altares-L\'opez, Jos\'e M. Bengochea-Guevara, Carlos Ranz, H\'ector Montes, Angela Ribeiro</dc:creator>
    </item>
    <item>
      <title>Unpacking Approaches to Learning and Teaching Machine Learning in K-12 Education: Transparency, Ethics, and Design Activities</title>
      <link>https://arxiv.org/abs/2406.03480</link>
      <description>arXiv:2406.03480v3 Announce Type: replace 
Abstract: In this conceptual paper, we review existing literature on artificial intelligence/machine learning (AI/ML) education to identify three approaches to how learning and teaching ML could be conceptualized. One of them, a data-driven approach, emphasizes providing young people with opportunities to create data sets, train, and test models. A second approach, learning algorithm-driven, prioritizes learning about how the learning algorithms or engines behind how ML models work. In addition, we identify efforts within a third approach that integrates the previous two. In our review, we focus on how the approaches: (1) glassbox and blackbox different aspects of ML, (2) build on learner interests and provide opportunities for designing applications, (3) integrate ethics and justice. In the discussion, we address the challenges and opportunities of current approaches and suggest future directions for the design of learning activities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.03480v3</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>The 19th WiPSCE Conference on Primary and Secondary Computing Education Research 2024</arxiv:journal_reference>
      <dc:creator>Luis Morales-Navarro, Yasmin B. Kafai</dc:creator>
    </item>
    <item>
      <title>61A-Bot Report: AI Assistants in CS1 Save Students Homework Time and Reduce Demands on Staff. (Now What?)</title>
      <link>https://arxiv.org/abs/2406.05600</link>
      <description>arXiv:2406.05600v2 Announce Type: replace 
Abstract: Chatbot interfaces for LLMs enable students to get immediate, interactive help on homework assignments, but even a thoughtfully-designed bot may not serve all pedagogical goals. In this paper, we report on the development and deployment of a GPT-4-based interactive homework assistant ("61A Bot") for students in a large CS1 course; over 2000 students made over 100,000 requests of our bot across two semesters. Our assistant offers one-shot, contextual feedback, primarily through a low-friction "get feedback" prompt within the command-line "autograder" our students already run to test their code. Our Bot wraps student code in a custom prompt that supports our pedagogical goals and avoids providing solutions directly. We discuss our deployment and then analyze the impacts of our Bot on students, primarily through student-reported feedback and tracking of student homework progress. We find reductions in homework-related question rates in our course forum, as well as substantial reductions in homework completion time when our Bot is available. For students in the 50th-80th percentile, these reductions typically exceed 30 minutes per assignment, over 4 standard deviations faster than the mean in prior semesters. Finally, we conclude with a discussion of these observations, the potential impacts on student learning, as well as other potential costs and benefits of AI assistance in CS1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05600v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>J. D. Zamfirescu-Pereira, Laryn Qi, Bj\"orn Hartmann, John DeNero, Narges Norouzi</dc:creator>
    </item>
    <item>
      <title>The Drama Machine: Simulating Character Development with LLM Agents</title>
      <link>https://arxiv.org/abs/2408.01725</link>
      <description>arXiv:2408.01725v2 Announce Type: replace 
Abstract: This paper explores use of multiple large language model (LLM) agents to simulate complex, dynamic characters in dramatic scenarios. We introduce a drama machine framework that coordinates interactions between LLM agents playing different 'Ego' and 'Superego' psychological roles. In roleplay simulations, this design allows intersubjective dialogue and intra-subjective internal monologue to develop in parallel. We apply this framework to two dramatic scenarios - an interview and a detective story - and compare character development with and without the Superego's influence. Though exploratory, results suggest this multi-agent approach can produce more nuanced, adaptive narratives that evolve over a sequence of dialogical turns. We discuss different modalities of LLM-based roleplay and character development, along with what this might mean for conceptualization of AI subjectivity. The paper concludes by considering how this approach opens possibilities for thinking of the roles of internal conflict and social performativity in AI-based simulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.01725v2</guid>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Liam Magee, Vanicka Arora, Gus Gollings, Norma Lam-Saw</dc:creator>
    </item>
    <item>
      <title>Antivax and off-label medication communities on brazilian Telegram: between esotericism as a gateway and the monetization of false miraculous cures</title>
      <link>https://arxiv.org/abs/2408.15308</link>
      <description>arXiv:2408.15308v2 Announce Type: replace 
Abstract: Conspiracy theories, particularly those focused on anti-vaccine narratives and the promotion of off-label medications such as MMS and CDS, have proliferated on Telegram, including in Brazil, finding fertile ground among communities that share esoteric beliefs and distrust towards scientific institutions. In this context, this study seeks to answer how Brazilian conspiracy theory communities on Telegram are characterized and articulated concerning anti-vaccine themes and off-label medications? It is important to highlight that this study is part of a series of seven studies aimed at understanding and characterizing Brazilian conspiracy theory communities on Telegram. This series of seven studies is openly and originally available on the arXiv of Cornell University, applying a mirrored method across all studies, changing only the thematic object of analysis and providing replicable research, including proprietary and original codes developed, contributing to the culture of free and open-source software. Regarding the main findings of this study, it was observed: Themes such as the New World Order and Apocalypse and Survivalism act as significant gateways to anti-vaccine narratives, connecting them to theories of global control; Globalism and New World Order stand out as the main communities receiving invitations from anti-vaccine communities; Occultism and Esotericism emerge as the largest sources of invitations to off-label medication communities, creating a strong connection between esoteric beliefs and the promotion of non-scientific treatments; Anti-vaccine narratives experienced a 290% increase during the COVID-19 pandemic, evidencing a growing interconnectedness with other conspiracy theories; The overlap of themes between anti-vaccine and other conspiracy theories creates an interdependent disinformation network, where different narratives mutually reinforce each other.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15308v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>Climate change denial and anti-science communities on brazilian Telegram: climate disinformation as a gateway to broader conspiracy networks</title>
      <link>https://arxiv.org/abs/2408.15311</link>
      <description>arXiv:2408.15311v2 Announce Type: replace 
Abstract: Conspiracy theories related to climate change denial and anti-science have found fertile ground on Telegram, particularly among Brazilian communities that distrust scientific institutions and oppose global environmental policies. This study seeks to answer the research question: how are Brazilian conspiracy theory communities on climate change and anti-science themes characterized and articulated on Telegram? It is worth noting that this study is part of a series of seven studies aimed at understanding and characterizing Brazilian conspiracy theory communities on Telegram. This series of studies is openly and originally available on arXiv from Cornell University, applying a mirrored method across all seven studies, changing only the thematic focus of analysis, and providing replicable investigation methods, including custom-developed and proprietary codes, contributing to the culture of open-source software. Regarding the main findings of this study, the following observations were made: Climate change denial and anti-science communities interact synergistically, creating a complex network that mutually reinforces disinformation narratives; Apocalyptic themes, such as Apocalypse and Survivalism, act as gateways to climate denial, with 5,057 links directed to these communities; Anti-science communities function as gatekeepers, distributing links evenly to theories such as the New World Order and Globalism, among others; During the COVID-19 pandemic, anti-science discussions experienced a significant peak, driven by vaccine disinformation; The intersection between anti-science narratives and esoteric beliefs reinforces the idea of a supposed alternative truth that challenges science; Since 2022, discussions on climate change have evolved to align with global domination theories; Additionally, the UN's 2030 Agenda is portrayed as part of a global conspiracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.15311v2</guid>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ergon Cugler de Moraes Silva</dc:creator>
    </item>
    <item>
      <title>RLCP: A Reinforcement Learning-based Copyright Protection Method for Text-to-Image Diffusion Model</title>
      <link>https://arxiv.org/abs/2408.16634</link>
      <description>arXiv:2408.16634v2 Announce Type: replace 
Abstract: The increasing sophistication of text-to-image generative models has led to complex challenges in defining and enforcing copyright infringement criteria and protection. Existing methods, such as watermarking and dataset deduplication, fail to provide comprehensive solutions due to the lack of standardized metrics and the inherent complexity of addressing copyright infringement in diffusion models. To deal with these challenges, we propose a Reinforcement Learning-based Copyright Protection(RLCP) method for Text-to-Image Diffusion Model, which minimizes the generation of copyright-infringing content while maintaining the quality of the model-generated dataset. Our approach begins with the introduction of a novel copyright metric grounded in copyright law and court precedents on infringement. We then utilize the Denoising Diffusion Policy Optimization (DDPO) framework to guide the model through a multi-step decision-making process, optimizing it using a reward function that incorporates our proposed copyright metric. Additionally, we employ KL divergence as a regularization term to mitigate some failure modes and stabilize RL fine-tuning. Experiments conducted on 3 mixed datasets of copyright and non-copyright images demonstrate that our approach significantly reduces copyright infringement risk while maintaining image quality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16634v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuan Shi, Jing Yan, Xiaoli Tang, Lingjuan Lyu, Boi Faltings</dc:creator>
    </item>
    <item>
      <title>The Initial Screening Order Problem</title>
      <link>https://arxiv.org/abs/2307.15398</link>
      <description>arXiv:2307.15398v4 Announce Type: replace-cross 
Abstract: We investigate the role of the initial screening order (ISO) in candidate screening tasks, such as employee hiring and academic admissions, in which a screener is tasked with selecting $k$ candidates from a candidate pool. The ISO refers to the order in which the screener searches the candidate pool. Today, it is common for the ISO to be the product of an information access system, such as an online platform or a database query. The ISO has been largely overlooked in the literature, despite its potential impact on the optimality and fairness of the chosen $k$ candidates, especially under a human screener. We define two problem formulations describing the search behavior of the screener under the ISO: the best-$k$, where the screener selects the $k$ best candidates; and the good-$k$, where the screener selects the $k$ first good-enough candidates. To study the impact of the ISO, we introduce a human-like screener and compare it to its algorithmic counterpart, where the human-like screener is conceived to be inconsistent over time due to fatigue. In particular, our analysis shows that the ISO, under a human-like screener solving for the good-$k$ problem, hinders individual fairness despite meeting group level fairness, and hampers the optimality of the selected $k$ candidates. This is due to position bias, where a candidate's evaluation is affected by its position within the ISO. We report extensive simulated experiments exploring the parameters of the best-$k$ and good-$k$ problems for the algorithmic and human-like screeners. The simulation framework is flexible enough to account for multiple screening settings, being an alternative to running real-world candidate screening procedures. This work is motivated by a real-world candidate screening problem studied in collaboration with an European company.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.15398v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jose M. Alvarez, Antonio Mastropietro, Salvatore Ruggieri</dc:creator>
    </item>
    <item>
      <title>GPT has become financially literate: Insights from financial literacy tests of GPT and a preliminary test of how people use it as a source of advice</title>
      <link>https://arxiv.org/abs/2309.00649</link>
      <description>arXiv:2309.00649v2 Announce Type: replace-cross 
Abstract: We assess the ability of GPT -- a large language model -- to serve as a financial robo-advisor for the masses, by using a financial literacy test. Davinci and ChatGPT based on GPT-3.5 score 66% and 65% on the financial literacy test, respectively, compared to a baseline of 33%. However, ChatGPT based on GPT-4 achieves a near-perfect 99% score, pointing to financial literacy becoming an emergent ability of state-of-the-art models. We use the Judge-Advisor System and a savings dilemma to illustrate how researchers might assess advice-utilization from large language models. We also present a number of directions for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.00649v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.frl.2023.104333</arxiv:DOI>
      <arxiv:journal_reference>Finance Research Letters, 2023, 58, 104333</arxiv:journal_reference>
      <dc:creator>Pawe{\l} Niszczota, Sami Abbas</dc:creator>
    </item>
    <item>
      <title>Editing Personality for Large Language Models</title>
      <link>https://arxiv.org/abs/2310.02168</link>
      <description>arXiv:2310.02168v4 Announce Type: replace-cross 
Abstract: This paper introduces an innovative task focused on editing the personality traits of Large Language Models (LLMs). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct PersonalityEdit, a new benchmark dataset to address this task. Drawing on the theory in Social Psychology, we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using GPT-4, generating responses that align with a specified topic and embody the targeted personality trait. We conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can stimulate further annotation in model editing and personality-related research. Code is available at https://github.com/zjunlp/EasyEdit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.02168v4</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shengyu Mao, Xiaohan Wang, Mengru Wang, Yong Jiang, Pengjun Xie, Fei Huang, Ningyu Zhang</dc:creator>
    </item>
    <item>
      <title>On The Fairness Impacts of Hardware Selection in Machine Learning</title>
      <link>https://arxiv.org/abs/2312.03886</link>
      <description>arXiv:2312.03886v2 Announce Type: replace-cross 
Abstract: In the machine learning ecosystem, hardware selection is often regarded as a mere utility, overshadowed by the spotlight on algorithms and data. This oversight is particularly problematic in contexts like ML-as-a-service platforms, where users often lack control over the hardware used for model deployment. How does the choice of hardware impact generalization properties? This paper investigates the influence of hardware on the delicate balance between model performance and fairness. We demonstrate that hardware choices can exacerbate existing disparities, attributing these discrepancies to variations in gradient flows and loss surfaces across different demographic groups. Through both theoretical and empirical analysis, the paper not only identifies the underlying factors but also proposes an effective strategy for mitigating hardware-induced performance imbalances.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.03886v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sree Harsha Nelaturu, Nishaanth Kanna Ravichandran, Cuong Tran, Sara Hooker, Ferdinando Fioretto</dc:creator>
    </item>
    <item>
      <title>Exploring Parent's Needs for Children-Centered AI to Support Preschoolers' Interactive Storytelling and Reading Activities</title>
      <link>https://arxiv.org/abs/2401.13804</link>
      <description>arXiv:2401.13804v2 Announce Type: replace-cross 
Abstract: Interactive storytelling is vital for preschooler development. While children's interactive partners have traditionally been their parents and teachers, recent advances in artificial intelligence (AI) have sparked a surge of AI-based storytelling and reading technologies. As these technologies become increasingly ubiquitous in preschoolers' lives, questions arise regarding how they function in practical storytelling and reading scenarios and, how parents, the most critical stakeholders, experience and perceive these technologies. This paper investigates these questions through a qualitative study with 17 parents of children aged 3-6. Our findings suggest that even though AI-based storytelling and reading technologies provide more immersive and engaging interaction, they still cannot meet parents' expectations due to a series of interactive and algorithmic challenges. We elaborate on these challenges and discuss the possible implications of future AI-based interactive storytelling technologies for preschoolers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13804v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yuling Sun, Jiaju Chen, Bingsheng Yao, Jiali Liu, Dakuo Wang, Xiaojuan Ma, Yuxuan Lu, Ying Xu, Liang He</dc:creator>
    </item>
    <item>
      <title>MM-Soc: Benchmarking Multimodal Large Language Models in Social Media Platforms</title>
      <link>https://arxiv.org/abs/2402.14154</link>
      <description>arXiv:2402.14154v3 Announce Type: replace-cross 
Abstract: Social media platforms are hubs for multimodal information exchange, encompassing text, images, and videos, making it challenging for machines to comprehend the information or emotions associated with interactions in online spaces. Multimodal Large Language Models (MLLMs) have emerged as a promising solution to these challenges, yet they struggle to accurately interpret human emotions and complex content such as misinformation. This paper introduces MM-Soc, a comprehensive benchmark designed to evaluate MLLMs' understanding of multimodal social media content. MM-Soc compiles prominent multimodal datasets and incorporates a novel large-scale YouTube tagging dataset, targeting a range of tasks from misinformation detection, hate speech detection, and social context generation. Through our exhaustive evaluation on ten size-variants of four open-source MLLMs, we have identified significant performance disparities, highlighting the need for advancements in models' social understanding capabilities. Our analysis reveals that, in a zero-shot setting, various types of MLLMs generally exhibit difficulties in handling social media tasks. However, MLLMs demonstrate performance improvements post fine-tuning, suggesting potential pathways for improvement. Our code and data are available at https://github.com/claws-lab/MMSoc.git.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.14154v3</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, Srijan Kumar</dc:creator>
    </item>
    <item>
      <title>Data Authenticity, Consent, &amp; Provenance for AI are all broken: what will it take to fix them?</title>
      <link>https://arxiv.org/abs/2404.12691</link>
      <description>arXiv:2404.12691v2 Announce Type: replace-cross 
Abstract: New capabilities in foundation models are owed in large part to massive, widely-sourced, and under-documented training data collections. Existing practices in data collection have led to challenges in tracing authenticity, verifying consent, preserving privacy, addressing representation and bias, respecting copyright, and overall developing ethical and trustworthy foundation models. In response, regulation is emphasizing the need for training data transparency to understand foundation models' limitations. Based on a large-scale analysis of the foundation model training data landscape and existing solutions, we identify the missing infrastructure to facilitate responsible foundation model development practices. We examine the current shortcomings of common tools for tracing data authenticity, consent, and documentation, and outline how policymakers, developers, and data creators can facilitate responsible foundation model development by adopting universal data provenance standards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.12691v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Proceedings of ICML 2024, in PMLR 235:32711-32725. URL: https://proceedings.mlr.press/v235/longpre24b.html</arxiv:journal_reference>
      <dc:creator>Shayne Longpre, Robert Mahari, Naana Obeng-Marnu, William Brannon, Tobin South, Katy Gero, Sandy Pentland, Jad Kabbara</dc:creator>
    </item>
    <item>
      <title>Fair Mixed Effects Support Vector Machine</title>
      <link>https://arxiv.org/abs/2405.06433</link>
      <description>arXiv:2405.06433v3 Announce Type: replace-cross 
Abstract: To ensure unbiased and ethical automated predictions, fairness must be a core principle in machine learning applications. Fairness in machine learning aims to mitigate biases present in the training data and model imperfections that could lead to discriminatory outcomes. This is achieved by preventing the model from making decisions based on sensitive characteristics like ethnicity or sexual orientation. A fundamental assumption in machine learning is the independence of observations. However, this assumption often does not hold true for data describing social phenomena, where data points are often clustered based. Hence, if the machine learning models do not account for the cluster correlations, the results may be biased. Especially high is the bias in cases where the cluster assignment is correlated to the variable of interest. We present a fair mixed effects support vector machine algorithm that can handle both problems simultaneously. With a reproducible simulation study we demonstrate the impact of clustered data on the quality of fair machine learning predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06433v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jo\~ao Vitor Pamplona, Jan Pablo Burgard</dc:creator>
    </item>
    <item>
      <title>Evaluating Large Language Models on Spatial Tasks: A Multi-Task Benchmarking Study</title>
      <link>https://arxiv.org/abs/2408.14438</link>
      <description>arXiv:2408.14438v3 Announce Type: replace-cross 
Abstract: The advent of large language models such as ChatGPT, Gemini, and others has underscored the importance of evaluating their diverse capabilities, ranging from natural language understanding to code generation. However, their performance on spatial tasks has not been comprehensively assessed. This study addresses this gap by introducing a novel multi-task spatial evaluation dataset, designed to systematically explore and compare the performance of several advanced models on spatial tasks. The dataset encompasses twelve distinct task types, including spatial understanding and path planning, each with verified, accurate answers. We evaluated multiple models, including OpenAI's gpt-3.5-turbo, gpt-4o, and ZhipuAI's glm-4, through a two-phase testing approach. Initially, we conducted zero-shot testing, followed by categorizing the dataset by difficulty and performing prompt tuning tests. Results indicate that gpt-4o achieved the highest overall accuracy in the first phase, with an average of 71.3%. Although moonshot-v1-8k slightly underperformed overall, it surpassed gpt-4o in place name recognition tasks. The study also highlights the impact of prompt strategies on model performance in specific tasks. For example, the Chain-of-Thought (COT) strategy increased gpt-4o's accuracy in path planning from 12.4% to 87.5%, while a one-shot strategy enhanced moonshot-v1-8k's accuracy in mapping tasks from 10.1% to 76.3%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14438v3</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Wed, 04 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liuchang Xu, Shuo Zhao, Qingming Lin, Luyao Chen, Qianqian Luo, Sensen Wu, Xinyue Ye, Hailin Feng, Zhenhong Du</dc:creator>
    </item>
  </channel>
</rss>
