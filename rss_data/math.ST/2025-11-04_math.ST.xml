<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 05 Nov 2025 02:39:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>On Kernels and Covariance Structures in Hilbert Space Gaussian Processes</title>
      <link>https://arxiv.org/abs/2511.00142</link>
      <description>arXiv:2511.00142v1 Announce Type: new 
Abstract: Motivated by practical applications, I present a novel and comprehensive framework for operator-valued positive definite kernels. This framework is applied to both operator theory and stochastic processes. The first application focuses on various dilation constructions within operator theory, while the second pertains to broad classes of stochastic processes. In this context, the authors utilize the results derived from operator-valued kernels to develop new Hilbert space-valued Gaussian processes and to investigate the structures of their covariance configurations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00142v1</guid>
      <category>math.ST</category>
      <category>math.FA</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeed Hashemi Sababe</dc:creator>
    </item>
    <item>
      <title>An LRD spectral test for irregularly discretely observed contaminated functional time series in manifolds</title>
      <link>https://arxiv.org/abs/2511.00518</link>
      <description>arXiv:2511.00518v1 Announce Type: new 
Abstract: A statistical hypothesis test for long range dependence (LRD) in functional time series in manifolds has been formulated in Ruiz-Medina and Crujeiras (2025) in the spectral domain for fully observed functional data. The asymptotic Gaussian distribution of the proposed test statistics, based on the weighted periodogram operator, under the null hypothesis, and the consistency of the test have been derived. In this paper, we analyze the asymptotic properties of this spectral LRD testing procedure, when functional data are contaminated, and discretely observed through random uniform spatial sampling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00518v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>M. D. Ruiz-Medina, R. M. Crujeiras</dc:creator>
    </item>
    <item>
      <title>Stochastic comparisons of finite mixtures with general exponentiated location-scale distributed components</title>
      <link>https://arxiv.org/abs/2511.00791</link>
      <description>arXiv:2511.00791v1 Announce Type: new 
Abstract: In this paper, we study stochastic ordering results between two finite mixtures with single and multiple outliers, assuming subpopulations follow general exponentiated location-scale distributions. For single-outlier mixtures, several sufficient conditions are derived under which the mixture variables are ordered in the usual stochastic, reversed hazard rate, and likelihood ratio orders, using majorization concepts. For multiple-outlier mixtures, results are obtained for the reversed hazard rate, likelihood ratio, and ageing faster orders in reversed hazard rate. Numerical examples and counterexamples are presented to illustrate and support the established theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00791v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raju Bhakta (Barmalzan), Kaushik Gupta (Barmalzan), Ghobad Saadat Kia (Barmalzan), Suchandan Kayal</dc:creator>
    </item>
    <item>
      <title>Persistence-Based Statistics for Detecting Structural Changes in High-Dimensional Point Clouds</title>
      <link>https://arxiv.org/abs/2511.00938</link>
      <description>arXiv:2511.00938v1 Announce Type: new 
Abstract: We study the probabilistic behavior of persistence statistics under distributional variability and propose a novel nonparametric framework for detecting structural changes in high-dimensional random point clouds. We first establish moment bounds and tightness results for classical persistence statistics - total and maximum persistence - under general distributions, with explicit scaling behavior derived for Gaussian mixture models. Building on these theoretical foundations, we introduce a normalized statistic based on persistence landscapes combined with the Jensen-Shannon divergence, and we prove its Holder continuity with respect to perturbations of input point clouds. The resulting measure is stable, scale- and shift-invariant, and suitable for nonparametric inference via permutation testing. A numerical illustration using dynamic attribute vectors from decentralized governance data demonstrates how the proposed method can capture regime shifts and evolving geometric complexity. Our results contribute to the theoretical understanding of random persistence and provide a rigorous statistical foundation for topological change-point detection in complex, high-dimensional systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00938v1</guid>
      <category>math.ST</category>
      <category>math.AT</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toshiyuki Nakayama</dc:creator>
    </item>
    <item>
      <title>Filtering of periodically correlated processes</title>
      <link>https://arxiv.org/abs/2511.00990</link>
      <description>arXiv:2511.00990v1 Announce Type: new 
Abstract: The problem of optimal linear estimation of linear functionals depending on the unknown values of a periodically correlated stochastic process from observations of the process with additive noise is considered. Formulas for calculating the mean square error and the spectral characteristic of the optimal linear estimate of the functionals are proposed in the case where spectral densities are exactly known and in the case where the spectral densities are unknown while a class of admissible spectral densities is given. Formulas that determine the least favorable spectral densities and the minimax (robust) spectral characteristics are proposed for a given class of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00990v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iryna Dubovets'ka, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Study of power series distributions with specified covariances</title>
      <link>https://arxiv.org/abs/2511.01081</link>
      <description>arXiv:2511.01081v1 Announce Type: new 
Abstract: This paper presents a study of power series distributions (PSD) with prescribed covariance characteristics. Such distributions constitute a fundamental class in probability theory and mathematical statistics, as they generalize a wide range of well-known discrete distributions and enable the description of various stochastic phenomena with a predetermined variance structure. The aim of the research is to develop analytical methods for constructing power series distributions with given covariances and to establish the conditions under which a particular function can serve as the covariance of a certain PSD. The paper derives a first-order differential equation for the generating function of the distribution, which determines the relationship between its parameters and the form of the covariance function. It is shown that the choice of an analytical or polynomial covariance completely specifies the structure of the corresponding generating function. The analysis made it possible to construct new families of PSDs that generalize the classical Bernoulli, Poisson, geometric, and other distributions while preserving a given covariance structure. The proposed approach is based on the analytical relationship between the generating function and the covariance function, providing a framework for constructing stochastic models with predefined dispersion properties. The results obtained expand the theoretical framework for describing discrete distributions and open up opportunities for practical applications in statistical estimation, modeling of complex systems, financial processes, machine learning where it is crucial to control the dependence between the mean and the variation. Further research may focus on constructing continuous analogues of such distributions, studying their limiting properties, and applying them to problems of regression and Bayesian analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01081v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Oleksandr Volkov, Yurii Volkov, Nataliia Voinalovych</dc:creator>
    </item>
    <item>
      <title>Nonparametric Least squares estimators for interval censoring</title>
      <link>https://arxiv.org/abs/2511.01103</link>
      <description>arXiv:2511.01103v2 Announce Type: new 
Abstract: The limit distribution of the nonparametric maximum likelihood estimator for interval censored data with more than one observation time per unobservable observation, is still unknown in general. For the so-called separated case, where one has observation times which are at a distance larger than a fixed $\epsilon&gt;0$, the limit distribution was derived in [4]. For the non-separated case there is a conjectured limit distribution, given in [9], Section 5.2 of Part 2. But the findings of the present paper suggest that this conjecture may not hold.
  We prove consistency of a closely related nonparametric isotonic least squares estimator and give a sketch of the proof for a result on its limit distribution. We also provide simulation results to show how the nonparametric MLE and least squares estimator behave in comparison. Moreover, we discuss a simpler least squares estimator that can be computed in one step, but is inferior to the other least squares estimator, since it does not use all information.
  For the simplest model of interval censoring, the current status model, the nonparametric maximum likelihood and least squares estimators are the same. This equivalence breaks down if there are more observation times per unobservable observation. The computations for the simulation of the more complicated interval censoring model were performed by using the iterative convex minorant algorithm. They are provided in the GitHub repository [6].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01103v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piet Groeneboom</dc:creator>
    </item>
    <item>
      <title>Consistent estimation in subcritical birth-and-death processes</title>
      <link>https://arxiv.org/abs/2511.01153</link>
      <description>arXiv:2511.01153v1 Announce Type: new 
Abstract: We investigate parameter estimation in subcritical continuous-time birth-and-death processes with multiple births. We show that the classical maximum likelihood estimators for the model parameters, based on the continuous observation of a single non-extinct trajectory, are not consistent in the usual sense: conditional on survival up to time $t$, they converge as $t \to \infty$ to the corresponding quantities in the associated $Q$-process, namely the process conditioned to survive in the distant future. We develop the first $C$-consistent estimators in this setting, which converge to the true parameter values when conditioning on survival up to time $t$, and establish their asymptotic normality. The analysis relies on spine decompositions and coupling techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01153v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie Hautphenne, Emma Horton</dc:creator>
    </item>
    <item>
      <title>Theoretical analysis of phase-rectified signal averaging (PRSA) algorithm</title>
      <link>https://arxiv.org/abs/2511.01659</link>
      <description>arXiv:2511.01659v1 Announce Type: new 
Abstract: Phase-rectified signal averaging (PRSA) is a widely used algorithm to analyze nonstationary biomedical time series. The method operates by identifying hinge points in the time series according to prescribed rules, extracting segments centered at these points (with overlap permitted), and then averaging the segments. The resulting output is intended to capture the underlying quasi-oscillatory pattern of the signal, which can subsequently serve as input for further scientific analysis. However, a theoretical analysis of PRSA is lacking. In this paper, we investigate PRSA under two settings. First, when the input consists of a superposition of two oscillatory components, $\cos(2\pi t)+A\cos(2\pi (\xi t+\phi))$, where $A&gt;0$, $\xi\in (0,1)$ and $\phi\in [0,1)$, we show that, asymptotically when the sample size $n\to \infty$, the PRSA output takes the form $A'\sin(2\pi t)+B'\sin(2\pi \xi t)$, where $A',B'\neq 0$. Second, when the input is a stationary Gaussian random process, we establish a central limit theorem: under mild regularity conditions, the averaged vector produced by PRSA converges in distribution to a Gaussian random vector as $n\to \infty$ with mean determined by the covariance structure of the random process. These results indicate that caution is warranted when interpreting PRSA outputs for scientific applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01659v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiro Akahori, Joseph Najnudel, Hau-Tieng Wu, Ju-Yi Yen</dc:creator>
    </item>
    <item>
      <title>Selecting valid adjustment sets with uncertain causal graphs</title>
      <link>https://arxiv.org/abs/2511.01662</link>
      <description>arXiv:2511.01662v1 Announce Type: new 
Abstract: Precise knowledge of causal directed acyclic graphs (DAGs) is assumed for standard approaches towards valid adjustment set selection for unbiased estimation, but in practice, the DAG is often inferred from data or expert knowledge, introducing uncertainty. We present techniques to identify valid adjustment sets despite potential errors in the estimated causal graph. Specifically, we assume that only the skeleton of the DAG is known. Under a Bayesian framework, we place a prior on graphs and wish to sample graphs and compute the posterior probability of each set being valid; however, directly doing so is inefficient as the number of sets grows exponentially with the number of nodes in the DAG. We develop theory and techniques so that a limited number of sets are tested while the probability of finding valid adjustment sets remains high. Empirical results demonstrate the effectiveness of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01662v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongyi Hu, St\'ephanie van der Pas</dc:creator>
    </item>
    <item>
      <title>Concentration inequalities for strong laws and laws of the iterated logarithm</title>
      <link>https://arxiv.org/abs/2511.00175</link>
      <description>arXiv:2511.00175v1 Announce Type: cross 
Abstract: We derive concentration inequalities for sums of independent and identically distributed random variables that yield non-asymptotic generalizations of several strong laws of large numbers including some of those due to Kolmogorov [1930], Marcinkiewicz and Zygmund [1937], Chung [1951], Baum and Katz [1965], Ruf, Larsson, Koolen, and Ramdas [2023], and Waudby-Smith, Larsson, and Ramdas [2024]. As applications, we derive non-asymptotic iterated logarithm inequalities in the spirit of Darling and Robbins [1967], as well as pathwise (sometimes described as "game-theoretic") analogues of strong laws and laws of the iterated logarithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00175v1</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Ruf, Ian Waudby-Smith</dc:creator>
    </item>
    <item>
      <title>Residual Balancing for Non-Linear Outcome Models in High Dimensions</title>
      <link>https://arxiv.org/abs/2511.00324</link>
      <description>arXiv:2511.00324v1 Announce Type: cross 
Abstract: We extend the approximate residual balancing (ARB) framework to nonlinear models, answering an open problem posed by Athey et al. (2018). Our approach addresses the challenge of estimating average treatment effects in high-dimensional settings where the outcome follows a generalized linear model. We derive a new bias decomposition for nonlinear models that reveals the need for a second-order correction to account for the curvature of the link function. Based on this insight, we construct balancing weights through an optimization problem that controls for both first and second-order sources of bias. We provide theoretical guarantees for our estimator, establishing its $\sqrt{n}$-consistency and asymptotic normality under standard high-dimensional assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00324v1</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Meza</dc:creator>
    </item>
    <item>
      <title>Correcting the Coverage Bias of Quantile Regression</title>
      <link>https://arxiv.org/abs/2511.00820</link>
      <description>arXiv:2511.00820v1 Announce Type: cross 
Abstract: We develop a collection of methods for adjusting the predictions of quantile regression to ensure coverage. Our methods are model agnostic and can be used to correct for high-dimensional overfitting bias with only minimal assumptions. Theoretical results show that the estimates we develop are consistent and facilitate accurate calibration in the proportional asymptotic regime where the ratio of the dimension of the data and the sample size converges to a constant. This is further confirmed by experiments on both simulated and real data. A key component of our work is a new connection between the leave-one-out coverage and the fitted values of variables appearing in a dual formulation of the quantile regression problem. This facilitates the use of cross-validation in a variety of settings at significantly reduced computational costs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.00820v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Isaac Gibbs, John J. Cherian, Emmanuel J. Cand\`es</dc:creator>
    </item>
    <item>
      <title>Pseudo quantum advantages in perceptron storage capacity</title>
      <link>https://arxiv.org/abs/2511.01028</link>
      <description>arXiv:2511.01028v1 Announce Type: cross 
Abstract: We investigate a generalized quantum perceptron architecture characterized by an oscillating activation function with a tunable frequency ranging from zero to infinity. Employing analytical techniques from statistical mechanics, we derive the optimal storage capacity and demonstrate that the classical result is recovered in the limit of vanishing frequency. As the frequency increases, however, the architecture exhibits enhanced quantum storage capabilities. Notably, this improvement stems solely from the specific form of the activation function and, in principle, could be emulated within a classical framework. Accordingly, we refer to this enhancement as a pseudo quantum advantage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01028v1</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fabio Benatti, Masoud Gharahi, Giovanni Gramegna, Stefano Mancini, Vincenzo Parisi</dc:creator>
    </item>
    <item>
      <title>Stochastic Regret Guarantees for Online Zeroth- and First-Order Bilevel Optimization</title>
      <link>https://arxiv.org/abs/2511.01126</link>
      <description>arXiv:2511.01126v1 Announce Type: cross 
Abstract: Online bilevel optimization (OBO) is a powerful framework for machine learning problems where both outer and inner objectives evolve over time, requiring dynamic updates. Current OBO approaches rely on deterministic \textit{window-smoothed} regret minimization, which may not accurately reflect system performance when functions change rapidly. In this work, we introduce a novel search direction and show that both first- and zeroth-order (ZO) stochastic OBO algorithms leveraging this direction achieve sublinear {stochastic bilevel regret without window smoothing}. Beyond these guarantees, our framework enhances efficiency by: (i) reducing oracle dependence in hypergradient estimation, (ii) updating inner and outer variables alongside the linear system solution, and (iii) employing ZO-based estimation of Hessians, Jacobians, and gradients. Experiments on online parametric loss tuning and black-box adversarial attacks validate our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01126v1</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Parvin Nazari, Bojian Hou, Davoud Ataee Tarzanagh, Li Shen, George Michailidis</dc:creator>
    </item>
    <item>
      <title>A structural equation formulation for general quasi-periodic Gaussian processes</title>
      <link>https://arxiv.org/abs/2511.01151</link>
      <description>arXiv:2511.01151v1 Announce Type: cross 
Abstract: This paper introduces a structural equation formulation that gives rise to a new family of quasi-periodic Gaussian processes, useful to process a broad class of natural and physiological signals. The proposed formulation simplifies generation and forecasting, and provides hyperparameter estimates, which we exploit in a convergent and consistent iterative estimation algorithm. A bootstrap approach for standard error estimation and confidence intervals is also provided. We demonstrate the computational and scaling benefits of the proposed approach on a broad class of problems, including water level tidal analysis, CO$_{2}$ emission data, and sunspot numbers data. By leveraging the structural equations, our method reduces the cost of likelihood evaluations and predictions from $\mathcal{O}(k^2 p^2)$ to $\mathcal{O}(p^2)$, significantly improving scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01151v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Unnati Nigam, Radhendushka Srivastava, Faezeh Marzbanrad, Michael Burke</dc:creator>
    </item>
    <item>
      <title>Stability of the Kim--Milman flow map</title>
      <link>https://arxiv.org/abs/2511.01154</link>
      <description>arXiv:2511.01154v1 Announce Type: cross 
Abstract: In this short note, we characterize stability of the Kim--Milman flow map -- also known as the probability flow ODE -- with respect to variations in the target measure. Rather than the Wasserstein distance, we show that stability holds with respect to the relative Fisher information</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01154v1</guid>
      <category>math.PR</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sinho Chewi, Aram-Alexandre Pooladian, Matthew S. Zhang</dc:creator>
    </item>
    <item>
      <title>On the Variance, Admissibility, and Stability of Empirical Risk Minimization</title>
      <link>https://arxiv.org/abs/2305.18508</link>
      <description>arXiv:2305.18508v2 Announce Type: replace 
Abstract: It is well known that Empirical Risk Minimization (ERM) may attain minimax suboptimal rates in terms of the mean squared error (Birg\'e and Massart, 1993). In this paper, we prove that, under relatively mild assumptions, the suboptimality of ERM must be due to its large bias. Namely, the variance error term of ERM is bounded by the minimax rate. In the fixed design setting, we provide an elementary proof of this result using the probabilistic method. Then, we extend our proof to the random design setting for various models.
  In addition, we provide a simple proof of Chatterjee's admissibility theorem (Chatterjee, 2014, Theorem 1.4), which states that in the fixed design setting, ERM cannot be ruled out as an optimal method, and then we extend this result to the random design setting. We also show that our estimates imply the stability of ERM, complementing the main result of Caponnetto and Rakhlin (2006) for non-Donsker classes. Finally, we highlight the somewhat irregular nature of the loss landscape of ERM in the non-Donsker regime, by showing that functions can be close to ERM, in terms of $L_2$ distance, while still being far from almost-minimizers of the empirical loss.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.18508v2</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gil Kur, Eli Putterman, Alexander Rakhlin</dc:creator>
    </item>
    <item>
      <title>Separation rates for the detection of synchronization of interacting point processes in a mean field frame. Application to neuroscience</title>
      <link>https://arxiv.org/abs/2402.01919</link>
      <description>arXiv:2402.01919v3 Announce Type: replace 
Abstract: Permutation tests have been proposed by Albert et al. (2015) to detect dependence between point processes, modeling in particular spike trains, that is the time occurrences of action potentials emitted by neurons. Our present work focuses on exhibiting a criterion on the separation rate to ensure that the Type II errors of these tests are controlled non asymptotically. This criterion is then discussed in two major models in neuroscience: the jittering Poisson model and Hawkes processes having \(M\) components interacting in a mean field frame and evolving in stationary regime. For both models, we obtain a lower bound of the size \(n\) of the sample necessary to detect the dependency between two neurons.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.01919v3</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Josu\'e Tchouanti, \'Eva L\"ocherbach, Patricia Reynaud-Bouret, Etienne Tanr\'e</dc:creator>
    </item>
    <item>
      <title>On the breakdown point of transport-based quantiles</title>
      <link>https://arxiv.org/abs/2410.16554</link>
      <description>arXiv:2410.16554v2 Announce Type: replace 
Abstract: Recent work has used optimal transport ideas to generalize the notion of (center-outward) quantiles to dimension $d\geq 2$. We study the robustness properties of these transport-based quantiles by deriving their breakdown point, roughly, the smallest amount of contamination required to make these quantiles take arbitrarily aberrant values. We prove that the transport median defined in Chernozhukov et al.~(2017) and Hallin et al.~(2021) has breakdown point of $1/2$. Moreover, a point in the transport depth contour of order $\tau\in [0,1/2]$ has breakdown point of $\tau$. This shows that the multivariate transport depth shares the same breakdown properties as its univariate counterpart. Our proof relies on a general argument connecting the breakdown point of transport maps evaluated at a point to the Tukey depth of that point in the reference measure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16554v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Avella-Medina, Alberto Gonz\'alez-Sanz</dc:creator>
    </item>
    <item>
      <title>Stratified Permutational Berry--Esseen Bounds and Their Applications to Statistics</title>
      <link>https://arxiv.org/abs/2503.13986</link>
      <description>arXiv:2503.13986v2 Announce Type: replace 
Abstract: The stratified linear permutation statistic arises in various statistics problems, including stratified and post-stratified survey sampling, stratified and post-stratified experiments, conditional permutation tests, etc. Although we can derive the Berry--Esseen bounds for the stratified linear permutation statistic based on existing bounds for the non-stratified statistics, those bounds are not sharp, and moreover, this strategy does not work in general settings with heterogeneous strata with varying sizes. We first use Stein's method to obtain a unified stratified permutational Berry--Esseen bound that can accommodate heterogeneous strata. We then apply the bound to various statistics problems, leading to stronger theoretical quantifications and thereby facilitating statistical inference in those problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.13986v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pengfei Tian, Fan Yang, Peng Ding</dc:creator>
    </item>
    <item>
      <title>Testing Random Effects for Binomial Data</title>
      <link>https://arxiv.org/abs/2504.13977</link>
      <description>arXiv:2504.13977v3 Announce Type: replace 
Abstract: In modern scientific research, small-scale studies with limited participants are increasingly common. However, interpreting individual outcomes can be challenging, making it standard practice to combine data across studies using random effects to draw broader scientific conclusions. In this work, we introduce an optimal methodology for assessing the goodness of fit of a reference distribution for the random effects arising from binomial counts. For meta-analyses, we also derive optimal tests to evaluate whether multiple studies are in agreement before pooling the data. In all cases, we prove that the proposed tests optimally distinguish null and alternative hypotheses separated in the 1-Wasserstein distance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13977v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lucas Kania, Larry Wasserman, Sivaraman Balakrishnan</dc:creator>
    </item>
    <item>
      <title>Dynamical mean-field analysis of adaptive Langevin diffusions: Propagation-of-chaos and convergence of the linear response</title>
      <link>https://arxiv.org/abs/2504.15556</link>
      <description>arXiv:2504.15556v2 Announce Type: replace 
Abstract: Motivated by an application to empirical Bayes learning in high-dimensional regression, we study a class of Langevin diffusions in a system with random disorder, where the drift coefficient is driven by a parameter that continuously adapts to the empirical distribution of the realized process up to the current time. The resulting dynamics take the form of a stochastic interacting particle system having both a McKean-Vlasov type interaction and a pairwise interaction defined by the random disorder. We prove a propagation-of-chaos result, showing that in the large system limit over dimension-independent time horizons, the empirical distribution of sample paths of the Langevin process converges to a deterministic limit law that is described by dynamical mean-field theory. This law is characterized by a system of dynamical fixed-point equations for the limit of the drift parameter and for the correlation and response kernels of the limiting dynamics. Using a dynamical cavity argument, we verify that these correlation and response kernels arise as the asymptotic limits of the averaged correlation and linear response functions of single coordinates of the system. These results enable an asymptotic analysis of an empirical Bayes Langevin dynamics procedure for learning an unknown prior parameter in a linear regression model, which we develop in a companion paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15556v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhou Fan, Justin Ko, Bruno Loureiro, Yue M. Lu, Yandi Shen</dc:creator>
    </item>
    <item>
      <title>Dynamical mean-field analysis of adaptive Langevin diffusions: Replica-symmetric fixed point and empirical Bayes</title>
      <link>https://arxiv.org/abs/2504.15558</link>
      <description>arXiv:2504.15558v2 Announce Type: replace 
Abstract: In many applications of statistical estimation via sampling, one may wish to sample from a high-dimensional target distribution that is adaptively evolving to the samples already seen. We study an example of such dynamics, given by a Langevin diffusion for posterior sampling in a Bayesian linear regression model with i.i.d. regression design, whose prior continuously adapts to the Langevin trajectory via a maximum marginal-likelihood scheme. Results of dynamical mean-field theory (DMFT) developed in our companion paper establish a precise high-dimensional asymptotic limit for the joint evolution of the prior parameter and law of the Langevin sample. In this work, we carry out an analysis of the equations that describe this DMFT limit, under conditions of approximate time-translation-invariance which include, in particular, settings where the posterior law satisfies a log-Sobolev inequality. In such settings, we show that this adaptive Langevin trajectory converges on a dimension-independent time horizon to an equilibrium state that is characterized by a system of scalar fixed-point equations, and the associated prior parameter converges to a critical point of a replica-symmetric limit for the model free energy. As a by-product of our analyses, we obtain a new dynamical proof that this replica-symmetric limit for the free energy is exact, in models having a possibly misspecified prior and where a log-Sobolev inequality holds for the posterior law.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.15558v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Zhou Fan, Justin Ko, Bruno Loureiro, Yue M. Lu, Yandi Shen</dc:creator>
    </item>
    <item>
      <title>Batch learning equals online learning in Bayesian supervised learning</title>
      <link>https://arxiv.org/abs/2510.16892</link>
      <description>arXiv:2510.16892v2 Announce Type: replace 
Abstract: Using functoriality of probabilistic morphisms, we prove that sequential and batch Bayesian inversions coincide in supervised learning models with conditionally independent (possibly non-i.i.d.) data \cite{Le2025}. This equivalence holds without domination or discreteness assumptions on sampling operators. We derive a recursive formula for posterior predictive distributions, which reduces to the Kalman filter in Gaussian process regression. For Polish label spaces $\mathcal{Y}$ and arbitrary input sets $\mathcal{X}$, we characterize probability measures on $\mathcal{P}(\mathcal{Y})^{\mathcal{X}}$ via projective systems, generalizing Orbanz \cite{Orbanz2011}. We revisit MacEachern's Dependent Dirichlet Processes (DDP) \cite{MacEachern2000} using copula-based constructions \cite{BJQ2012} and show how to compute posterior predictive distributions in universal Bayesian supervised models with DDP priors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.16892v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>H\^ong V\^an L\^e</dc:creator>
    </item>
    <item>
      <title>Adaptive Algorithms for Infinitely Many-Armed Bandits: A Unified Framework</title>
      <link>https://arxiv.org/abs/2510.27319</link>
      <description>arXiv:2510.27319v2 Announce Type: replace 
Abstract: We consider a bandit problem where the buget is smaller than the number of arms, which may be infinite. In this regime, the usual objective in the literature is to minimize simple regret. To analyze broad classes of distributions with potentially unbounded support, where simple regret may not be well-defined, we take a slightly different approach and seek to maximize the expected simple reward of the recommended arm, providing anytime guarantees. To that end, we introduce a distribution-free algorithm, OSE, that adapts to the distribution of arm means and achieves near-optimal rates for several distribution classes. We characterize the sample complexity through the rank-corrected inverse squared gap function. In particular, we recover known upper bounds and transition regimes for $\alpha$ less or greater than $1/2$ when the quantile function is $\lambda_\eta = 1-\eta^{\alpha}$. We additionally identify new transition regimes depending on the noise level relative to $\alpha$, which we conjecture to be nearly optimal. Additionally, we introduce an enhanced practical version, PROSE, that achieves state-of-the-art empirical performance for the main distribution classes considered in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27319v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emmanuel Pilliat (ENSAI, CREST)</dc:creator>
    </item>
    <item>
      <title>Selecting the Best Optimizing System</title>
      <link>https://arxiv.org/abs/2201.03065</link>
      <description>arXiv:2201.03065v2 Announce Type: replace-cross 
Abstract: We formulate selecting the best optimizing system (SBOS) problems and provide solutions for those problems. In an SBOS problem, a finite number of systems are contenders. Inside each system, a continuous decision variable affects the system's expected performance. An SBOS problem compares different systems based on their expected performances under their own optimally chosen decision to select the best, without advance knowledge of expected performances of the systems nor the optimizing decision inside each system. We design easy-to-implement algorithms that adaptively chooses a system and a choice of decision to evaluate the noisy system performance, sequentially eliminates inferior systems, and eventually recommends a system as the best after spending a user-specified budget. The proposed algorithms integrate the stochastic gradient descent method and the sequential elimination method to simultaneously exploit the structure inside each system and make comparisons across systems. For the proposed algorithms, we prove exponential rates of convergence to zero for the probability of false selection, as the budget grows to infinity. We conduct three numerical examples that represent three practical cases of SBOS problems. Our proposed algorithms demonstrate consistent and stronger performances in terms of the probability of false selection over benchmark algorithms under a range of problem settings and sampling budgets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2201.03065v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nian Si, Yifu Tang, Zeyu Zheng</dc:creator>
    </item>
    <item>
      <title>You Are the Best Reviewer of Your Own Papers: The Isotonic Mechanism</title>
      <link>https://arxiv.org/abs/2206.08149</link>
      <description>arXiv:2206.08149v3 Announce Type: replace-cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) conferences including NeurIPS and ICML have experienced a significant decline in peer review quality in recent years. To address this growing challenge, we introduce the Isotonic Mechanism, a computationally efficient approach to enhancing the accuracy of noisy review scores by incorporating authors' private assessments of their submissions. Under this mechanism, authors with multiple submissions are required to rank their papers in descending order of perceived quality. Subsequently, the raw review scores are calibrated based on this ranking to produce adjusted scores. We prove that authors are incentivized to truthfully report their rankings because doing so maximizes their expected utility, modeled as an additive convex function over the adjusted scores. Moreover, the adjusted scores are shown to be more accurate than the raw scores, with improvements being particularly significant when the noise level is high and the author has many submissions -- a scenario increasingly prevalent at large-scale ML/AI conferences.
  We further investigate whether submission quality information beyond a simple ranking can be truthfully elicited from authors. We establish that a necessary condition for truthful elicitation is that the mechanism be based on pairwise comparisons of the author's submissions. This result underscores the optimality of the Isotonic Mechanism, as it elicits the most fine-grained truthful information among all mechanisms we consider. We then present several extensions, including a demonstration that the mechanism maintains truthfulness even when authors have only partial rather than complete information about their submission quality. Finally, we discuss future research directions, focusing on the practical implementation of the mechanism and the further development of a theoretical framework inspired by our mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08149v3</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>Breakdown points of Fermat-Weber problems under gauge distances</title>
      <link>https://arxiv.org/abs/2306.13424</link>
      <description>arXiv:2306.13424v3 Announce Type: replace-cross 
Abstract: We compute the robustness of Fermat-Weber points with respect to any finite gauge. We show a breakdown point of $1/(1+\sigma)$ where $\sigma$ is the asymmetry measure of the gauge. We obtain quantitative results indicating how far a corrupted Fermat-Weber point can lie from the true value in terms of the original sample and the size of the corrupted part. If the distance from the true value depends only on the original sample, then we call the gauge `uniformly robust.' We show that polyhedral gauges are uniformly robust, but locally strictly convex norms are not, while in dimension 2 any uniform robust gauge is polyhedral.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.13424v3</guid>
      <category>math.MG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrei Com\u{a}neci, Frank Plastria</dc:creator>
    </item>
    <item>
      <title>Bayesian Nonparametrics for Principal Stratification with Continuous Post-Treatment Variables</title>
      <link>https://arxiv.org/abs/2405.17669</link>
      <description>arXiv:2405.17669v3 Announce Type: replace-cross 
Abstract: Principal stratification provides a causal inference framework for investigating treatment effects in the presence of a post-treatment variable. Principal strata play a key role in characterizing the treatment effect by identifying groups of units with the same or similar values for the potential post-treatment variable at all treatment levels. The literature has focused mainly on binary post-treatment variables. Few papers considered continuous post-treatment variables. In the presence of a continuous post-treatment, a challenge is how to identify and characterize meaningful coarsening of the latent principal strata that lead to interpretable principal causal effects. This paper introduces the Confounders-Aware SHared atoms BAyesian mixture (CASBAH), a novel approach for principal stratification with binary treatment and continuous post-treatment variables. CASBAH leverages Bayesian nonparametric priors with an innovative hierarchical structure for the potential post-treatment outcomes that overcomes some of the limitations of previous works. Specifically, the novel features of our method allow for (i) identifying coarsened principal strata through a data-adaptive approach and (ii) providing a comprehensive quantification of the uncertainty surrounding stratum membership. Through Monte Carlo simulations, we show that the proposed methodology performs better than existing methods in characterizing the principal strata and estimating principal effects of the treatment. Finally, CASBAH is applied to a case study in which we estimate the causal effects of US national air quality regulations on pollution levels and health outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.17669v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dafne Zorzetto, Antonio Canale, Fabrizia Mealli, Francesca Dominici, Falco J. Bargagli-Stoffi</dc:creator>
    </item>
    <item>
      <title>Double Descent Meets Out-of-Distribution Detection: Theoretical Insights and Empirical Analysis on the role of model complexity</title>
      <link>https://arxiv.org/abs/2411.02184</link>
      <description>arXiv:2411.02184v3 Announce Type: replace-cross 
Abstract: Out-of-distribution (OOD) detection is essential for ensuring the reliability and safety of machine learning systems. In recent years, it has received increasing attention, particularly through post-hoc detection and training-based methods. In this paper, we focus on post-hoc OOD detection, which enables identifying OOD samples without altering the model's training procedure or objective. Our primary goal is to investigate the relationship between model capacity and its OOD detection performance. Specifically, we aim to answer the following question: Does the Double Descent phenomenon manifest in post-hoc OOD detection? This question is crucial, as it can reveal whether overparameterization, which is already known to benefit generalization, can also enhance OOD detection. Despite the growing interest in these topics by the classic supervised machine learning community, this intersection remains unexplored for OOD detection. We empirically demonstrate that the Double Descent effect does indeed appear in post-hoc OOD detection. Furthermore, we provide theoretical insights to explain why this phenomenon emerges in such setting. Finally, we show that the overparameterized regime does not yield superior results consistently, and we propose a method to identify the optimal regime for OOD detection based on our observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.02184v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mou\"in Ben Ammar, David Brellmann, Arturo Mendoza, Antoine Manzanera, Gianni Franchi</dc:creator>
    </item>
    <item>
      <title>Optimization via Strategic Law of Large Numbers</title>
      <link>https://arxiv.org/abs/2412.05604</link>
      <description>arXiv:2412.05604v4 Announce Type: replace-cross 
Abstract: This paper proposes a unified framework for the global optimization of a continuous function in a bounded rectangular domain. Specifically, we show that: (1) under the optimal strategy for a two-armed decision model, the sample mean converges to a global optimizer under the Strategic Law of Large Numbers, and (2) a sign-based strategy built upon the solution of a parabolic PDE is asymptotically optimal. Motivated by this result, we propose a class of {\bf S}trategic {\bf M}onte {\bf C}arlo {\bf O}ptimization (SMCO) algorithms, which uses a simple strategy that makes coordinate-wise two-armed decisions based on the signs of the partial gradient of the original function being optimized over (without the need of solving PDEs). While this simple strategy is not generally optimal, we show that it is sufficient for our SMCO algorithm to converge to local optimizer(s) from a single starting point, and to global optimizers under a growing set of starting points. Numerical studies demonstrate the suitability of our SMCO algorithms for global optimization, and illustrate the promise of our theoretical framework and practical approach. For a wide range of test functions with challenging optimization landscapes (including ReLU neural networks with square and hinge loss), our SMCO algorithms converge to the global maximum accurately and robustly, using only a small set of starting points (at most 100 for dimensions up to 1000) and a small maximum number of iterations (200). In fact, our algorithms outperform many state-of-the-art global optimizers, as well as local algorithms augmented with the same set of starting points as ours.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.05604v4</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaohong Chen, Zengjing Chen, Wayne Yuan Gao, Xiaodong Yan, Guodong Zhang</dc:creator>
    </item>
    <item>
      <title>A probabilistic view on Riemannian machine learning models for SPD matrices</title>
      <link>https://arxiv.org/abs/2505.02402</link>
      <description>arXiv:2505.02402v2 Announce Type: replace-cross 
Abstract: The goal of this paper is to show how different machine learning tools on the Riemannian manifold $\mathcal{P}_d$ of Symmetric Positive Definite (SPD) matrices can be united under a probabilistic framework. For this, we will need several Gaussian distributions defined on $\mathcal{P}_d$. We will show how popular classifiers on $\mathcal{P}_d$ can be reinterpreted as Bayes Classifiers using these Gaussian distributions. These distributions will also be used for outlier detection and dimension reduction. By showing that those distributions are pervasive in the tools used on $\mathcal{P}_d$, we allow for other machine learning tools to be extended to $\mathcal{P}_d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02402v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Thibault de Surrel, Florian Yger, Fabien Lotte, Sylvain Chevallier</dc:creator>
    </item>
    <item>
      <title>Statistical Analysis of Conditional Group Distributionally Robust Optimization with Cross-Entropy Loss</title>
      <link>https://arxiv.org/abs/2507.09905</link>
      <description>arXiv:2507.09905v2 Announce Type: replace-cross 
Abstract: In multi-source learning with discrete labels, distributional heterogeneity across domains poses a central challenge to developing predictive models that transfer reliably to unseen domains. We study multi-source unsupervised domain adaptation, where labeled data are available from multiple source domains and only unlabeled data are observed from the target domain. To address potential distribution shifts, we propose a novel Conditional Group Distributionally Robust Optimization (CG-DRO) framework that learns a classifier by minimizing the worst-case cross-entropy loss over the convex combinations of the conditional outcome distributions from sources domains. We develop an efficient Mirror Prox algorithm for solving the minimax problem and employ a double machine learning procedure to estimate the risk function, ensuring that errors in nuisance estimation contribute only at higher-order rates. We establish fast statistical convergence rates for the empirical CG-DRO estimator by constructing two surrogate minimax optimization problems that serve as theoretical bridges. A distinguishing challenge for CG-DRO is the emergence of nonstandard asymptotics: the empirical CG-DRO estimator may fail to converge to a standard limiting distribution due to boundary effects and system instability. To address this, we introduce a perturbation-based inference procedure that enables uniformly valid inference, including confidence interval construction and hypothesis testing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09905v2</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zijian Guo, Zhenyu Wang, Yifan Hu, Francis Bach</dc:creator>
    </item>
    <item>
      <title>Provable Generalization Bounds for Deep Neural Networks with Momentum-Adaptive Gradient Dropout</title>
      <link>https://arxiv.org/abs/2510.18410</link>
      <description>arXiv:2510.18410v2 Announce Type: replace-cross 
Abstract: Deep neural networks (DNNs) achieve remarkable performance but often suffer from overfitting due to their high capacity. We introduce Momentum-Adaptive Gradient Dropout (MAGDrop), a novel regularization method that dynamically adjusts dropout rates on activations based on current gradients and accumulated momentum, enhancing stability in non-convex optimization landscapes. To theoretically justify MAGDrop's effectiveness, we derive a non-asymptotic, computable PAC-Bayes generalization bound that accounts for its adaptive nature, achieving up to 29.2\% tighter bounds compared to standard approaches by leveraging momentum-driven perturbation control. Empirically, the activation-based MAGDrop achieves competitive performance on MNIST (99.52\%) and CIFAR-10 (92.03\%), with generalization gaps of 0.48\% and 6.52\%, respectively. We provide fully reproducible code and numerical computation of our bounds to validate our theoretical claims. Our work bridges theoretical insights and practical advancements, offering a robust framework for enhancing DNN generalization, making it suitable for high-stakes applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.18410v2</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 04 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Adeel Safder</dc:creator>
    </item>
  </channel>
</rss>
