<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.OT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.OT</link>
    <description>stat.OT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.OT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Sep 2025 04:09:43 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Oral exams in introductory statistics class with non-native English speakers</title>
      <link>https://arxiv.org/abs/2409.16613</link>
      <description>arXiv:2409.16613v5 Announce Type: replace 
Abstract: Oral exams are a powerful tool to assess student's learning. This is particularly important in introductory statistics classes where students struggle to grasp various topics like the interpretation of probability, $p$-values and more. The challenge of acquiring conceptual understanding is only heightened when students are learning in a second language. In this paper, I share my experience administering oral exams to an introductory statistics class of non-native English speakers at a Japanese university. I explain the context of the university and course, before detailing the exam. Of particular interest is the relationship between exam performance and English proficiency. The results showed little relationship between the two, meaning the exam seemed to truly test student's statistical knowledge rather than their English ability. I close with encouragements and recommendations for practitioners hoping to implement similar oral exams, focusing on the unique difficulties faced by students not learning in their mother tongue.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16613v5</guid>
      <category>stat.OT</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko</dc:creator>
    </item>
    <item>
      <title>On the Ambiguities of Incompatibility in Frequentist Inference</title>
      <link>https://arxiv.org/abs/2509.07147</link>
      <description>arXiv:2509.07147v2 Announce Type: replace 
Abstract: The interpretation of the P-value and its monotone transform s=-log2(p), or S-value, remains debated despite decades of dedicated literature. Within the neo-Fisherian framework, these values are often described as indices of (in)compatibility between the observed data and a set of ideal assumptions (i.e., the statistical model). In this regard, this paper proposes the distinction between two domains: the model domain, where assumptions are taken as perfectly true and every admissible outcome is, by construction, fully compatible with the model; and the real domain, where assumptions may fail and face empirical scrutiny. I argue that, although interpreted through an objective numerical index, any level of incompatibility can arise only in the latter domain, where the epistemic status of the model under examination is uncertain and a genuine conflict between data and hypotheses can therefore occur. The extent to which P- and S-values are taken as indicating incompatibility is a matter of contextual judgment. Within this framework, descriptive approaches serve to quantify the numerical values of P and S; these can be interpreted as indicative of a certain degree (or amount) of incompatibility between data and hypotheses once causal knowledge of the data-generating process and information about the costs and benefits of related decisions become clearer. Although the distinction between the model domain and the real domain may appear merely theoretical or even philosophical, I argue that this perspective is useful for developing a clear mental representation of how statistical estimates should be evaluated in practical settings and applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.07147v2</guid>
      <category>stat.OT</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Rovetta</dc:creator>
    </item>
    <item>
      <title>The effects of retraining on the stability of global models in retail demand forecasting</title>
      <link>https://arxiv.org/abs/2506.05776</link>
      <description>arXiv:2506.05776v2 Announce Type: replace-cross 
Abstract: Forecast stability, that is, the consistency of predictions over time, is essential in business settings where sudden shifts in forecasts can disrupt planning and erode trust in predictive systems. Despite its importance, stability is often overlooked in favor of accuracy. In this study, we evaluate the stability of point and probabilistic forecasts across several retraining scenarios using two large retail demand datasets (M5 and VN1) and ten different global forecasting models. To analyze stability in the probabilistic setting, we propose a new model-agnostic, distribution-free, and scale-free metric that measures probabilistic instability: the Scaled Multi-Quantile Change (SMQC). Furthermore, we also evaluate the effects of retraining on various ensemble configurations based on forecast pooling. The results show that, compared to continuous retraining, less frequent retraining not only preserves but often improves forecast stability, challenging the need for continuous retraining. The study promotes a shift toward stability-aware forecasting practices, proposing a new tool to effectively evaluate forecast stability in probabilistic settings, and offering practical guidelines for building more robust prediction systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05776v2</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Mon, 15 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Zanotti</dc:creator>
    </item>
  </channel>
</rss>
