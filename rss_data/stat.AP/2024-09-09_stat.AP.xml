<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 10 Sep 2024 02:49:13 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Causal effect of the infield shift in the MLB</title>
      <link>https://arxiv.org/abs/2409.03940</link>
      <description>arXiv:2409.03940v1 Announce Type: new 
Abstract: The infield shift has been increasingly used as a defensive strategy in baseball in recent years. Along with the upward trend in its usage, the notoriety of the shift has grown, as it is believed to be responsible for the recent decline in offence. In the 2023 season, Major League Baseball (MLB) implemented a rule change prohibiting the infield shift. However, there has been no systematic analysis of the effectiveness of infield shift to determine if it is a cause of the cooling in offence. We used publicly available data on MLB from 2015-2022 to evaluate the causal effect of the infield shift on the expected runs scored. We employed three methods for drawing causal conclusions from observational data -- nearest neighbour matching, inverse probability of treatment weighting, and instrumental variable analysis -- and evaluated the causal effect in subgroups defined by batter-handedness. The results of all methods showed the shift is effective at preventing runs, but primarily for left-handed batters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03940v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sonia Markes, Linbo Wang, Jessica Gronsbell, Katherine Evans</dc:creator>
    </item>
    <item>
      <title>Privacy risk from synthetic data: practical proposals</title>
      <link>https://arxiv.org/abs/2409.04257</link>
      <description>arXiv:2409.04257v1 Announce Type: new 
Abstract: This paper proposes and compares measures of identity and attribute disclosure risk for synthetic data. Data custodians can use the methods proposed here to inform the decision as to whether to release synthetic versions of confidential data. Different measures are evaluated on two data sets. Insight into the measures is obtained by examining the details of the records identified as posing a disclosure risk. This leads to methods to identify, and possibly exclude, apparently risky records where the identification or attribution would be expected by someone with background knowledge of the data. The methods described are available as part of the \textbf{synthpop} package for \textbf{R}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04257v1</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gillian M Raab</dc:creator>
    </item>
    <item>
      <title>Deep Clustering of Remote Sensing Scenes through Heterogeneous Transfer Learning</title>
      <link>https://arxiv.org/abs/2409.03938</link>
      <description>arXiv:2409.03938v1 Announce Type: cross 
Abstract: This paper proposes a method for unsupervised whole-image clustering of a target dataset of remote sensing scenes with no labels. The method consists of three main steps: (1) finetuning a pretrained deep neural network (DINOv2) on a labelled source remote sensing imagery dataset and using it to extract a feature vector from each image in the target dataset, (2) reducing the dimension of these deep features via manifold projection into a low-dimensional Euclidean space, and (3) clustering the embedded features using a Bayesian nonparametric technique to infer the number and membership of clusters simultaneously. The method takes advantage of heterogeneous transfer learning to cluster unseen data with different feature and label distributions. We demonstrate the performance of this approach outperforming state-of-the-art zero-shot classification methods on several remote sensing scene classification datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.03938v1</guid>
      <category>cs.CV</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Isaac Ray, Alexei Skurikhin</dc:creator>
    </item>
    <item>
      <title>Towards Measuring Sell Side Outcomes in Buy Side Marketplace Experiments using In-Experiment Bipartite Graph</title>
      <link>https://arxiv.org/abs/2409.04174</link>
      <description>arXiv:2409.04174v1 Announce Type: cross 
Abstract: In this study, we evaluate causal inference estimators for online controlled bipartite graph experiments in a real marketplace setting. Our novel contribution is constructing a bipartite graph using in-experiment data, rather than relying on prior knowledge or historical data, the common approach in the literature published to date. We build the bipartite graph from various interactions between buyers and sellers in the marketplace, establishing a novel research direction at the intersection of bipartite experiments and mediation analysis. This approach is crucial for modern marketplaces aiming to evaluate seller-side causal effects in buyer-side experiments, or vice versa. We demonstrate our method using historical buyer-side experiments conducted at Vinted, the largest second-hand marketplace in Europe with over 80M users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04174v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Vaiva Pilkauskait\.e, Jevgenij Gamper, Rasa Gini\=unait\.e, Agne Reklait\.e</dc:creator>
    </item>
    <item>
      <title>Best Linear Unbiased Estimate from Privatized Histograms</title>
      <link>https://arxiv.org/abs/2409.04387</link>
      <description>arXiv:2409.04387v1 Announce Type: cross 
Abstract: In differential privacy (DP) mechanisms, it can be beneficial to release "redundant" outputs, in the sense that a quantity can be estimated by combining different combinations of privatized values. Indeed, this structure is present in the DP 2020 Decennial Census products published by the U.S. Census Bureau. With this structure, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained by combining different values result in the same estimate) and we show that the minimum variance processing is a linear projection. However, standard projection algorithms are too computationally expensive in terms of both memory and execution time for applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. We apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04387v1</guid>
      <category>stat.CO</category>
      <category>cs.CR</category>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jordan Awan, Adam Edwards, Paul Bartholomew, Andrew Sillers</dc:creator>
    </item>
    <item>
      <title>Peer-induced Fairness: A Causal Approach for Algorithmic Fairness Auditing</title>
      <link>https://arxiv.org/abs/2408.02558</link>
      <description>arXiv:2408.02558v4 Announce Type: replace 
Abstract: With the European Union's Artificial Intelligence Act taking effect on 1 August 2024, high-risk AI applications must adhere to stringent transparency and fairness standards. This paper addresses a crucial question: how can we scientifically audit algorithmic fairness? Current methods typically remain at the basic detection stage of auditing, without accounting for more complex scenarios. We propose a novel framework, ``peer-induced fairness'', which combines the strengths of counterfactual fairness and peer comparison strategy, creating a reliable and robust tool for auditing algorithmic fairness. Our framework is universal, adaptable to various domains, and capable of handling different levels of data quality, including skewed distributions. Moreover, it can distinguish whether adverse decisions result from algorithmic discrimination or inherent limitations of the subjects, thereby enhancing transparency. This framework can serve as both a self-assessment tool for AI developers and an external assessment tool for auditors to ensure compliance with the EU AI Act. We demonstrate its utility in small and medium-sized enterprises access to finance, uncovering significant unfairness-41.51% of micro-firms face discrimination compared to non-micro firms. These findings highlight the framework's potential for broader applications in ensuring equitable AI-driven decision-making.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.02558v4</guid>
      <category>stat.AP</category>
      <category>cs.CY</category>
      <category>q-fin.CP</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shiqi Fang, Zexun Chen, Jake Ansell</dc:creator>
    </item>
    <item>
      <title>Colorectal cancer risk mapping through Bayesian Networks</title>
      <link>https://arxiv.org/abs/2408.08618</link>
      <description>arXiv:2408.08618v2 Announce Type: replace 
Abstract: Background and Objective: Only about 14 % of eligible EU citizens finally participate in colorectal cancer (CRC) screening programs despite it being the third most common type of cancer worldwide. The development of CRC risk models can enable predictions to be embedded in decision-support tools facilitating CRC screening and treatment recommendations. This paper develops a predictive model that aids in characterizing CRC risk groups and assessing the influence of a variety of risk factors on the population.
  Methods: A CRC Bayesian Network is learnt by aggregating extensive expert knowledge and data from an observational study and making use of structure learning algorithms to model the relations between variables. The network is then parametrized to characterize these relations in terms of local probability distributions at each of the nodes. It is finally used to predict the risks of developing CRC together with the uncertainty around such predictions.
  Results: A graphical CRC risk mapping tool is developed from the model and used to segment the population into risk subgroups according to variables of interest. Furthermore, the network provides insights on the predictive influence of modifiable risk factors such as alcohol consumption and smoking, and medical conditions such as diabetes or hypertension linked to lifestyles that potentially have an impact on an increased risk of developing CRC.
  Conclusions: CRC is most commonly developed in older individuals. However, some modifiable behavioral factors seem to have a strong predictive influence on its potential risk of development. Modelling these effects facilitates identifying risk groups and targeting influential variables which are subsequently helpful in the design of screening and treatment programs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.08618v2</guid>
      <category>stat.AP</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Daniel Corrales, Alejandro Santos-Lozano, Susana L\'opez-Ortiz, Alejandro Lucia, David R\'ios Insua</dc:creator>
    </item>
    <item>
      <title>Estimation of Over-parameterized Models from an Auto-Modeling Perspective</title>
      <link>https://arxiv.org/abs/2206.01824</link>
      <description>arXiv:2206.01824v4 Announce Type: replace-cross 
Abstract: From a model-building perspective, we propose a paradigm shift for fitting over-parameterized models. Philosophically, the mindset is to fit models to future observations rather than to the observed sample. Technically, given an imputation method to generate future observations, we fit over-parameterized models to these future observations by optimizing an approximation of the desired expected loss function based on its sample counterpart and an adaptive $\textit{duality function}$. The required imputation method is also developed using the same estimation technique with an adaptive $m$-out-of-$n$ bootstrap approach. We illustrate its applications with the many-normal-means problem, $n &lt; p$ linear regression, and neural network-based image classification of MNIST digits. The numerical results demonstrate its superior performance across these diverse applications. While primarily expository, the paper conducts an in-depth investigation into the theoretical aspects of the topic. It concludes with remarks on some open problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.01824v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 09 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Jiang, Chuanhai Liu</dc:creator>
    </item>
  </channel>
</rss>
