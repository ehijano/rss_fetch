<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.IT updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.IT</link>
    <description>cs.IT updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.IT" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 05 Sep 2025 01:26:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>On a class of twisted elliptic curve codes</title>
      <link>https://arxiv.org/abs/2509.03034</link>
      <description>arXiv:2509.03034v1 Announce Type: new 
Abstract: Motivated by the studies of twisted generalized Reed-Solomon (TGRS) codes, we initiate the study of twisted elliptic curve codes (TECCs) in this paper. In particular, we study a class of TECCs with one twist. The parity-check matrices of the TECCs are explicitly given by computing the Weil differentials. Then the sufficient and necessary conditions of self-duality are presented. The minimum distances of the TECCs are also determined. Moreover, examples of MDS, AMDS, self-dual and MDS self-dual TECCs are given. Finally, we calculate the dimensions of the Schur squares of TECCs and show the non-equivalence between TECCs and ECCs/GRS codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03034v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaofeng Liu, Jun Zhang, Fang-Wei Fu</dc:creator>
    </item>
    <item>
      <title>Successive Cancellation Decoding For General Monotone Chain Polar Codes</title>
      <link>https://arxiv.org/abs/2509.03128</link>
      <description>arXiv:2509.03128v1 Announce Type: new 
Abstract: Monotone chain polar codes generalize classical polar codes to multivariate settings, offering a flexible approach for achieving the entire admissible rate region in the distributed lossless coding problem. However, this flexibility also introduces significant challenges for existing successive cancellation (SC) based decoding schemes. Motivated by the need for a general SC decoding solution, we present a comprehensive decoding strategy for monotone chain polar codes that can handle arbitrary numbers of terminals, non-binary alphabets, and decoding along arbitrary monotone chains. Specifically, we formulate the SC decoding task as a series of inference subtasks over the polar transform and propose a computational graph framework based on probability propagation principles. This approach highlights the impact of variable switching during decoding and shows that time complexity varies between $O(N\log{N})$ and $O(N^2)$, depending on the specific chain structure. Moreover, we demonstrate that the widely used $O(N)$ space optimization is not universally applicable to monotone chain polar codes, which prompts us to introduce a constant-time decoder forking strategy based on the proposed logical computation graphs. This strategy enables time-efficient list decoding without relying on $O(N)$-space techniques. Numerical results verify the superior performance of the proposed scheme compared with the classical lazy-copy scheme.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03128v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zichang Ren, Chunhang Zheng, Dou Li, Yuping Zhao</dc:creator>
    </item>
    <item>
      <title>New Bounds for Linear Codes with Applications</title>
      <link>https://arxiv.org/abs/2509.03337</link>
      <description>arXiv:2509.03337v1 Announce Type: new 
Abstract: Bounds on linear codes play a central role in coding theory, as they capture the fundamental trade-off between error-correction capability (minimum distance) and information rate (dimension relative to length). Classical results characterize this trade-off solely in terms of the parameters $n$, $k$, $d$ and $q$. In this work we derive new bounds under the additional assumption that the code contains a nonzero codeword of weight $w$.By combining residual-code techniques with classical results such as the Singleton and Griesmer bounds,we obtain explicit inequalities linking $n$, $k$, $d$, $q$ and $w$. These bounds impose sharper restrictions on admissible codeword weights, particularly those close to the minimum distance or to the code length. Applications include refined constraints on the weights of MDS codes, numerical restrictions on general linear codes, and excluded weight ranges in the weight distribution. Numerical comparisons across standard parameter sets demonstrate that these $w$-aware bounds strictly enlarge known excluded weight ranges and sharpen structural limitations on linear codes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03337v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liren Lin, Guanghui Zhang, Bocong Chen, Hongwei Liu</dc:creator>
    </item>
    <item>
      <title>PoolPy: Flexible Group Testing Design for Large-Scale Screening</title>
      <link>https://arxiv.org/abs/2509.03481</link>
      <description>arXiv:2509.03481v1 Announce Type: new 
Abstract: In large screening campaigns, group testing can greatly reduce the number of tests needed when compared to testing each sample individually. However, choosing and applying an appropriate group testing method remains challenging due to the wide variety in design and performance across methods, and the lack of accessible tools. Here, we present PoolPy, a unified framework for designing and selecting optimal group testing strategies across ten different methods according to user-defined constraints, such as time, cost or sample dilution. By computing over 10,000 group testing designs made available through a web interface, we identified key trade-offs, such as minimizing test number or group size, that define applicability to specific use cases. Overall, we show that no single method is universally optimal, and provide clear indications for method choice on a case-by-case basis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03481v1</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Talamanca, Julian Trouillon</dc:creator>
    </item>
    <item>
      <title>Recall Gabor Communication Theory and Joint Time-Frequency Analysis</title>
      <link>https://arxiv.org/abs/2509.02724</link>
      <description>arXiv:2509.02724v2 Announce Type: cross 
Abstract: In this article, we first briefly recall Gabor's communication theory and then Gabor transform and expansion, and also its connection with joint time frequency analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02724v2</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiang-Gen Xia</dc:creator>
    </item>
    <item>
      <title>minPIC: Towards Optimal Power Allocation in Multi-User Interference Channels</title>
      <link>https://arxiv.org/abs/2509.02797</link>
      <description>arXiv:2509.02797v1 Announce Type: cross 
Abstract: 6G envisions massive cell-free networks with spatially nested multiple access (MAC) and broadcast (BC) channels without centralized coordination. This makes optimal resource allocation across power, subcarriers, and decoding orders crucial for interference channels (ICs), where neither transmitters nor receivers can cooperate. Current orthogonal multiple access (OMA) methods, as well as non-orthogonal (NOMA) and rate-splitting (RSMA) schemes, rely on fixed heuristics for interference management, leading to suboptimal rates, power inefficiency, and scalability issues. This paper proposes a novel minPIC framework for optimal power, subcarrier, and decoding order allocation in general multi-user ICs. Unlike existing methods, minPIC eliminates heuristic SIC order assumptions. Despite the convexity of the IC capacity region, fixing an SIC order induces non-convexity in resource allocation, traditionally requiring heuristic approximations. We instead introduce a dual-variable-guided sorting criterion to identify globally optimal SIC orders, followed by convex optimization with auxiliary log-det constraints, efficiently solved via binary search. We also demonstrate that minPIC could potentially meet the stringent high-rate, low-power targets of immersive XR and other 6G applications. To the best of our knowledge, minPIC is the first algorithmic realisation of the Pareto boundary of the SIC-achievable rate region for Gaussian ICs, opening the door to scalable interference management in cell-free networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02797v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Sagnik Bhattacharya, Abhiram Rao Gorle, John M. Cioffi</dc:creator>
    </item>
    <item>
      <title>Rollout-Based Approximate Dynamic Programming for MDPs with Information-Theoretic Constraints</title>
      <link>https://arxiv.org/abs/2509.02812</link>
      <description>arXiv:2509.02812v1 Announce Type: cross 
Abstract: This paper studies a finite-horizon Markov decision problem with information-theoretic constraints, where the goal is to minimize directed information from the controlled source process to the control process, subject to stage-wise cost constraints, aiming for an optimal control policy. We propose a new way of approximating a solution for this problem, which is known to be formulated as an unconstrained MDP with a continuous information-state using Q-factors. To avoid the computational complexity of discretizing the continuous information-state space, we propose a truncated rollout-based backward-forward approximate dynamic programming (ADP) framework. Our approach consists of two phases: an offline base policy approximation over a shorter time horizon, followed by an online rollout lookahead minimization, both supported by provable convergence guarantees. We supplement our theoretical results with a numerical example where we demonstrate the cost improvement of the rollout method compared to a previously proposed policy approximation method, and the computational complexity observed in executing the offline and online phases for the two methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02812v1</guid>
      <category>eess.SY</category>
      <category>cs.IT</category>
      <category>cs.SY</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zixuan He, Charalambos D. Charalambous, Photios A. Stavrou</dc:creator>
    </item>
    <item>
      <title>Programmable Quantum Matter: Heralding Large Cluster States in Driven Inhomogeneous Spin Ensembles</title>
      <link>https://arxiv.org/abs/2509.02992</link>
      <description>arXiv:2509.02992v1 Announce Type: cross 
Abstract: Atom-like emitters in solids are promising platforms for quantum sensing and information processing, but inhomogeneities in the emitter fine structure complicate quantum control. We present a framework that leverages this diversity to reduce the resources for generating optically heralded spin cluster states across $N_q$ emitters from the conventional order $O(N_q)$ to $O(1)$ in ensembles of $N_q \sim 10$-$100$. An optimized pulse sequence simultaneously corrects pulse-length and detuning errors, achieving single-qubit gate fidelities exceeding $99.99\%$ for errors (normalized relative to the Rabi drive strength) up to 0.3, while maintaining fidelities above $99\%$ for errors as large as 0.4. Applied as a Carr-Purcell-Meiboom-Gill (CPMG) dynamical decoupling protocol to the dominant noise spectrum of silicon-vacancy centers in diamond, it enhances ensemble coherence times by over $7\times$ compared to interleaved bang-bang based CPMG. For state-of-the-art dilution refrigerators, global resonant optimal decoupling across $N_q$ spins sharply reduces heating, addressing the trade-off between the spin coherence and scaling to $N_q \gg 1$. We further introduce a modified single-photon entanglement protocol with an efficient algorithm for deterministic entanglement compilation. Depending on the decoupling time window, our method yields order $O(10^2$-$10^4)$ more entanglement links than bang-bang sequences, with theoretical guarantees of order $\Omega(N_q)$ unique links, improvable by control tuning. Together, these techniques provide scalable tools - including global control, phase denoising, remote entanglement, and compilation - for robust quantum computing architectures with heterogeneous spin ensembles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02992v1</guid>
      <category>quant-ph</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pratyush Anand, Louis Follet, Odiel Hooybergs, Dirk R. Englund</dc:creator>
    </item>
    <item>
      <title>Group-averaged Markov chains: mixing improvement</title>
      <link>https://arxiv.org/abs/2509.02996</link>
      <description>arXiv:2509.02996v1 Announce Type: cross 
Abstract: For Markov kernels $P$ on a general state space $\mathcal{X}$, we introduce a new class of averaged Markov kernels $P_{da}(G,\nu)$ of $P$ induced by a group $G$ that acts on $\mathcal{X}$ and a probability measure $\nu$ on $G \times G$. Notable special cases are the group-orbit average $\overline{P}$, left-average $P_{la}$, right-average $P_{ra}$ and the independent-double-average $(P_{la})_{ra}$. For $\pi$-stationary $P$ in which $\pi$ is invariant with respect to $G$, we show that in general $P_{da}$ enjoys favorable convergence properties than $P$ based on metrics such as spectral gap or asymptotic variance, and within the family of $P_{da}$ the most preferable kernel is in general $(P_{la})_{ra}$. We demonstrate that $P_{la}, P_{ra}, (P_{la})_{ra}$ are comparable in terms of mixing times, which supports the use of $P_{la}, P_{ra}$ in practice as computationally cheaper alternatives over $(P_{la})_{ra}$. These averaged kernels also admit natural geometric interpretations: they emerge as unique projections of $P$ onto specific $G$-invariant structures under the Kullback-Leibler divergence or the Hilbert-Schmidt norm and satisfy Pythagorean identities. On the other hand, in the general case if $\pi$ is not invariant with respect to $G$, we propose and study a technique that we call state-dependent averaging of Markov kernels which generalizes the earlier results to this setting. As examples and applications, this averaging perspective not only allows us to recast state-of-the-art Markov chain samplers such as Hamiltonian Monte Carlo or piecewise-deterministic Markov processes as specific cases of $P_{da}$, but also enables improvements to existing samplers such as Metropolis-Hastings, achieving rapid mixing in some toy models or when $\pi$ is the discrete uniform distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02996v1</guid>
      <category>math.PR</category>
      <category>cs.IT</category>
      <category>math.GR</category>
      <category>math.IT</category>
      <category>stat.CO</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael C. H. Choi, Youjia Wang</dc:creator>
    </item>
    <item>
      <title>Identifiability and minimality bounds of quantum and post-quantum models of classical stochastic processes</title>
      <link>https://arxiv.org/abs/2509.03004</link>
      <description>arXiv:2509.03004v1 Announce Type: cross 
Abstract: To make sense of the world around us, we develop models, constructed to enable us to replicate, describe, and explain the behaviours we see. Focusing on the broad case of sequences of correlated random variables, i.e., classical stochastic processes, we tackle the question of determining whether or not two different models produce the same observable behavior. This is the problem of identifiability. Curiously, the physics of the model need not correspond to the physics of the observations; recent work has shown that it is even advantageous -- in terms of memory and thermal efficiency -- to employ quantum models to generate classical stochastic processes. We resolve the identifiability problem in this regime, providing a means to compare any two models of a classical process, be the models classical, quantum, or `post-quantum', by mapping them to a canonical `generalized' hidden Markov model. Further, this enables us to place (sometimes tight) bounds on the minimal dimension required of a quantum model to generate a given classical stochastic process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03004v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paul M. Riechers, Thomas J. Elliott</dc:creator>
    </item>
    <item>
      <title>PatchNet: A hierarchical approach for neural field-level inference from Quijote Simulations</title>
      <link>https://arxiv.org/abs/2509.03165</link>
      <description>arXiv:2509.03165v1 Announce Type: cross 
Abstract: \textit{What is the cosmological information content of a cubic Gigaparsec of dark matter? } Extracting cosmological information from the non-linear matter distribution has high potential to tighten parameter constraints in the era of next-generation surveys such as Euclid, DESI, and the Vera Rubin Observatory. Traditional approaches relying on summary statistics like the power spectrum and bispectrum, though analytically tractable, fail to capture the full non-Gaussian and non-linear structure of the density field. Simulation-Based Inference (SBI) provides a powerful alternative by learning directly from forward-modeled simulations. In this work, we apply SBI to the \textit{Quijote} dark matter simulations and introduce a hierarchical method that integrates small-scale information from field sub-volumes or \textit{patches} with large-scale statistics such as power spectrum and bispectrum. This hybrid strategy is efficient both computationally and in terms of the amount of training data required. It overcomes the memory limitations associated with full-field training. We show that our approach enhances Fisher information relative to analytical summaries and matches that of a very different approach (wavelet-based statistics), providing evidence that we are estimating the full information content of the dark matter density field at the resolution of $\sim 7.8~\mathrm{Mpc}/h$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03165v1</guid>
      <category>astro-ph.CO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anirban Bairagi, Benjamin Wandelt</dc:creator>
    </item>
    <item>
      <title>Credible Uncertainty Quantification under Noise and System Model Mismatch</title>
      <link>https://arxiv.org/abs/2509.03311</link>
      <description>arXiv:2509.03311v1 Announce Type: cross 
Abstract: State estimators often provide self-assessed uncertainty metrics, such as covariance matrices, whose reliability is critical for downstream tasks. However, these self-assessments can be misleading due to underlying modeling violations like noise or system model mismatch. This letter addresses the problem of estimator credibility by introducing a unified, multi-metric evaluation framework. We construct a compact credibility portfolio that synergistically combines traditional metrics like the Normalized Estimation Error Squared (NEES) and the Noncredibility Index (NCI) with proper scoring rules, namely the Negative Log-Likelihood (NLL) and the Energy Score (ES). Our key contributions are a novel energy distance-based location test to robustly detect system model misspecification and a method that leverages the asymmetric sensitivities of NLL and ES to distinguish optimism covariance scaling from system bias. Monte Carlo simulations across six distinct credibility scenarios demonstrate that our proposed method achieves high classification accuracy (80-100%), drastically outperforming single-metric baselines which consistently fail to provide a complete and correct diagnosis. This framework provides a practical tool for turning patterns of credibility indicators into actionable diagnoses of model deficiencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03311v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Penggao Yan, Li-Ta Hsu</dc:creator>
    </item>
    <item>
      <title>Baseband Model, Cutoff Rate Bounds and Constellation Shaping for Mixed Gaussian-Impulsive Noise</title>
      <link>https://arxiv.org/abs/2509.03333</link>
      <description>arXiv:2509.03333v1 Announce Type: cross 
Abstract: Mixed noise, composed of white Gaussian noise (WGN) and impulsive noise (IN), appears in numerous communication scenarios and can severely degrade system performance. In this paper, we address this issue by optimizing the transmitted constellation under mixed noise based on a theoretical analysis of the cutoff rate (CR). First, starting from the passband model of the mixed noise, we derive its corresponding baseband representation. Due to the complexity of the CR, an exact analytic expression is generally intractable. Therefore, the baseband noise model is employed to obtain closed-form lower and upper bounds of the CR. A piecewise linear approximation is applied to derive efficient bounds by exploiting the algebraic properties of the integral terms. These bounds are then used as criteria to optimize the transmitted constellation points in both geometric and probabilistic distributions. The projected gradient method is employed to solve the optimization problem, and the convergence and properties of the solutions are analyzed. Numerical results demonstrate that the proposed CR bounds are tight and exhibit the expected asymptotic behavior. Furthermore, the optimized constellation scheme achieves a significant rate improvement compared to baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03333v1</guid>
      <category>eess.SP</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianfu Qi, Jun Wang</dc:creator>
    </item>
    <item>
      <title>Information-Theoretic Lower Bounds for Approximating Monomials via Optimal Quantum Tsallis Entropy Estimation</title>
      <link>https://arxiv.org/abs/2509.03496</link>
      <description>arXiv:2509.03496v1 Announce Type: cross 
Abstract: This paper reveals a conceptually new connection from information theory to approximation theory via quantum algorithms for entropy estimation. Specifically, we provide an information-theoretic lower bound $\Omega(\sqrt{n})$ on the approximate degree of the monomial $x^n$, compared to the analytic lower bounds shown in Newman and Rivlin (Aequ. Math. 1976) via Fourier analysis and in Sachdeva and Vishnoi (Found. Trends Theor. Comput. Sci. 2014) via the Markov brothers' inequality. This is done by relating the polynomial approximation of monomials to quantum Tsallis entropy estimation. This further implies a quantum algorithm that estimates to within additive error $\varepsilon$ the Tsallis entropy of integer order $q \geq 2$ of an unknown probability distribution $p$ or an unknown quantum state $\rho$, using $\widetilde \Theta(\frac{1}{\sqrt{q}\varepsilon})$ queries to the quantum oracle that produces a sample from $p$ or prepares a copy of $\rho$, improving the prior best $O(\frac{1}{\varepsilon})$ via the Shift test due to Ekert, Alves, Oi, Horodecki, Horodecki and Kwek (Phys. Rev. Lett. 2002). To the best of our knowledge, this is the first quantum entropy estimator with optimal query complexity (up to polylogarithmic factors) for all parameters simultaneously.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.03496v1</guid>
      <category>quant-ph</category>
      <category>cs.CC</category>
      <category>cs.IT</category>
      <category>math.CA</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qisheng Wang</dc:creator>
    </item>
    <item>
      <title>Region-of-Interest-Guided Deep Joint Source-Channel Coding for Image Transmission</title>
      <link>https://arxiv.org/abs/2506.01269</link>
      <description>arXiv:2506.01269v2 Announce Type: replace 
Abstract: Deep joint source-channel coding (deepJSCC) methods have shown promising improvements in communication performance over wireless networks. However, existing approaches primarily focus on enhancing overall image reconstruction quality, which may not fully align with user experiences, often driven by the quality of regions of interest (ROI). Motivated by this, we propose ROI-guided joint source-channel coding (ROI-JSCC), a novel deepJSCC framework that prioritizes high-quality transmission of ROI. The ROI-JSCC consists of four key components: (1) Image ROI embedding, (2) ROI-guided split processing, (3) ROI-based loss function design, and (4) ROI-adaptive bandwidth allocation. Together, these components allow ROI-JSCC to selectively enhance the ROI reconstruction quality at varying ROI positions while maintaining overall image quality with minimal computational overhead. Experimental results under diverse communication environments demonstrate that ROI-JSCC significantly improves ROI reconstruction quality while maintaining competitive average image quality compared to recent state-of-the-art methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.01269v2</guid>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hansung Choi, Daewon Seo</dc:creator>
    </item>
    <item>
      <title>Symmetric Private Information Retrieval (SPIR) on Graph-Based Replicated Systems</title>
      <link>https://arxiv.org/abs/2507.17736</link>
      <description>arXiv:2507.17736v2 Announce Type: replace 
Abstract: We introduce the problem of symmetric private information retrieval (SPIR) on replicated databases modeled by a simple graph. In this model, each vertex corresponds to a server, and a message is replicated on two servers if and only if there is an edge between them. We consider the setting where the server-side common randomness necessary to accomplish SPIR is also replicated at the servers according to the graph, and we call this as message-specific common randomness. In this setting, we establish a lower bound on the SPIR capacity, i.e., the maximum download rate, for general graphs, by proposing an achievable SPIR scheme. Next, we prove that, for any SPIR scheme to be feasible, the minimum size of message-specific randomness should be equal to the size of a message. Finally, by providing matching upper bounds, we derive the exact SPIR capacity for the class of path and regular graphs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17736v2</guid>
      <category>cs.IT</category>
      <category>cs.CR</category>
      <category>cs.DB</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shreya Meel, Sennur Ulukus</dc:creator>
    </item>
    <item>
      <title>Deep Variational Multivariate Information Bottleneck -- A Framework for Variational Losses</title>
      <link>https://arxiv.org/abs/2310.03311</link>
      <description>arXiv:2310.03311v4 Announce Type: replace-cross 
Abstract: Variational dimensionality reduction methods are widely used for their accuracy, generative capabilities, and robustness. We introduce a unifying framework that generalizes both such as traditional and state-of-the-art methods. The framework is based on an interpretation of the multivariate information bottleneck, trading off the information preserved in an encoder graph (defining what to compress) against that in a decoder graph (defining a generative model for data). Using this approach, we rederive existing methods, including the deep variational information bottleneck, variational autoencoders, and deep multiview information bottleneck. We naturally extend the deep variational CCA (DVCCA) family to beta-DVCCA and introduce a new method, the deep variational symmetric information bottleneck (DVSIB). DSIB, the deterministic limit of DVSIB, connects to modern contrastive learning approaches such as Barlow Twins, among others. We evaluate these methods on Noisy MNIST and Noisy CIFAR-100, showing that algorithms better matched to the structure of the problem like DVSIB and beta-DVCCA produce better latent spaces as measured by classification accuracy, dimensionality of the latent variables, sample efficiency, and consistently outperform other approaches under comparable conditions. Additionally, we benchmark against state-of-the-art models, achieving superior or competitive accuracy. Our results demonstrate that this framework can seamlessly incorporate diverse multi-view representation learning algorithms, providing a foundation for designing novel, problem-specific loss functions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.03311v4</guid>
      <category>cs.LG</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>physics.data-an</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eslam Abdelaleem, Ilya Nemenman, K. Michael Martini</dc:creator>
    </item>
    <item>
      <title>The Broader Landscape of Robustness in Algorithmic Statistics</title>
      <link>https://arxiv.org/abs/2412.02670</link>
      <description>arXiv:2412.02670v2 Announce Type: replace-cross 
Abstract: The last decade has seen a number of advances in computationally efficient algorithms for statistical methods subject to robustness constraints. An estimator may be robust in a number of different ways: to contamination of the dataset, to heavy-tailed data, or in the sense that it preserves privacy of the dataset. We survey recent results in these areas with a focus on the problem of mean estimation, drawing technical and conceptual connections between the various forms of robustness, showing that the same underlying algorithmic ideas lead to computationally efficient estimators in all these settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02670v2</guid>
      <category>stat.ML</category>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gautam Kamath</dc:creator>
    </item>
    <item>
      <title>Learning sparse generalized linear models with binary outcomes via iterative hard thresholding</title>
      <link>https://arxiv.org/abs/2502.18393</link>
      <description>arXiv:2502.18393v2 Announce Type: replace-cross 
Abstract: In statistics, generalized linear models (GLMs) are widely used for modeling data and can expressively capture potential nonlinear dependence of the model's outcomes on its covariates. Within the broad family of GLMs, those with binary outcomes, which include logistic and probit regressions, are motivated by common tasks such as binary classification with (possibly) non-separable data. In addition, in modern machine learning and statistics, data is often high-dimensional yet has a low intrinsic dimension, making sparsity constraints in models another reasonable consideration. In this work, we propose to use and analyze an iterative hard thresholding (projected gradient descent on the ReLU loss) algorithm, called binary iterative hard thresholding (BIHT), for parameter estimation in sparse GLMs with binary outcomes. We establish that BIHT is statistically efficient and converges to the correct solution for parameter estimation in a general class of sparse binary GLMs. Unlike many other methods for learning GLMs, including maximum likelihood estimation, generalized approximate message passing, and GLM-tron (Kakade et al. 2011; Bahmani et al. 2016), BIHT does not require knowledge of the GLM's link function, offering flexibility and generality in allowing the algorithm to learn arbitrary binary GLMs. As two applications, logistic and probit regression are additionally studied. In this regard, it is shown that in logistic regression, the algorithm is in fact statistically optimal in the sense that the order-wise sample complexity matches (up to logarithmic factors) the lower bound obtained previously. To the best of our knowledge, this is the first work achieving statistical optimality for logistic regression in all noise regimes with a computationally efficient algorithm. Moreover, for probit regression, our sample complexity is on the same order as that obtained for logistic regression.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.18393v2</guid>
      <category>math.ST</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Namiko Matsumoto, Arya Mazumdar</dc:creator>
    </item>
    <item>
      <title>Communication Efficient Robotic Mixed Reality with Gaussian Splatting Cross-Layer Optimization</title>
      <link>https://arxiv.org/abs/2508.08624</link>
      <description>arXiv:2508.08624v2 Announce Type: replace-cross 
Abstract: Realizing low-cost communication in robotic mixed reality (RoboMR) systems presents a challenge, due to the necessity of uploading high-resolution images through wireless channels. This paper proposes Gaussian splatting (GS) RoboMR (GSMR), which enables the simulator to opportunistically render a photo-realistic view from the robot's pose by calling ``memory'' from a GS model, thus reducing the need for excessive image uploads. However, the GS model may involve discrepancies compared to the actual environments. To this end, a GS cross-layer optimization (GSCLO) framework is further proposed, which jointly optimizes content switching (i.e., deciding whether to upload image or not) and power allocation (i.e., adjusting to content profiles) across different frames by minimizing a newly derived GSMR loss function. The GSCLO problem is addressed by an accelerated penalty optimization (APO) algorithm that reduces computational complexity by over $10$x compared to traditional branch-and-bound and search algorithms. Moreover, variants of GSCLO are presented to achieve robust, low-power, and multi-robot GSMR. Extensive experiments demonstrate that the proposed GSMR paradigm and GSCLO method achieve significant improvements over existing benchmarks on both wheeled and legged robots in terms of diverse metrics in various scenarios. For the first time, it is found that RoboMR can be achieved with ultra-low communication costs, and mixture of data is useful for enhancing GS performance in dynamic scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.08624v2</guid>
      <category>cs.RO</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <pubDate>Thu, 04 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenxuan Liu, He Li, Zongze Li, Shuai Wang, Wei Xu, Kejiang Ye, Derrick Wing Kwan Ng, Chengzhong Xu</dc:creator>
    </item>
  </channel>
</rss>
