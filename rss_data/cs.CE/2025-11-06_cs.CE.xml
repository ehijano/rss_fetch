<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Nov 2025 02:36:47 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Workday's Approach to Secure and Compliant Cloud ERP Systems</title>
      <link>https://arxiv.org/abs/2511.02856</link>
      <description>arXiv:2511.02856v1 Announce Type: new 
Abstract: Workday's compliance with global standards -- such as GDPR, SOC 2, HIPAA, ISO 27001, and FedRAMP -- shows its ability to best protect critical financial, healthcare, and government data.Automated compliance attributes like audit trails, behavioral analytics, and continuous reporting improve automation of the process and cut down on the manual effort to audit. A comparative review demonstrates enhanced risk management, operational flexibility, and breach mitigation. The paper also discusses potential future solutions with AI, ML and blockchain, to enhance attackdetection and data integrity. Overall, Workday turns out to be a secure, compliant and future-ready ERP solution. The paper also explores emerging trends, including the integration of AI, machine learning, and blockchain technologies to enhance next-generation threat detection and data integrity. The findings position Workday as a reliable, compliant, and future-ready ERP solution, setting a new benchmark for secure enterprise cloud management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02856v1</guid>
      <category>cs.CE</category>
      <category>cs.SE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Monu Sharma</dc:creator>
    </item>
    <item>
      <title>A Conditional Diffusion Model for Building Energy Modeling Workflows</title>
      <link>https://arxiv.org/abs/2511.02930</link>
      <description>arXiv:2511.02930v1 Announce Type: new 
Abstract: Understanding current energy consumption behavior in communities is critical for informing future energy use decisions and enabling efficient energy management. Urban energy models, which are used to simulate these energy use patterns, require large datasets with detailed building characteristics for accurate outcomes. However, such detailed characteristics at the individual building level are often unknown and costly to acquire, or unavailable. Through this work, we propose using a generative modeling approach to generate realistic building attributes to fill in the data gaps and finally provide complete characteristics as inputs to energy models. Our model learns complex, building-level patterns from training on a large-scale residential building stock model containing 2.2 million buildings. We employ a tabular diffusion-based framework that is designed to handle heterogeneous (discrete and continuous) features in tabular building data, such as occupancy, floor area, heating, cooling, and other equipment details. We develop a capability for conditional diffusion, enabling the imputation of missing building characteristics conditioned on known attributes. We conduct a comprehensive validation of our conditional diffusion model, firstly by comparing the generated conditional distributions against the underlying data distribution, and secondly, by performing a case study for a Baltimore residential region, showing the practical utility of our approach. Our work is one of the first to demonstrate the potential of generative modeling to accelerate building energy modeling workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02930v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saumya Sinha, Alexandre Cortiella, Rawad El Kontar, Andrew Glaws, Ryan King, Patrick Emami</dc:creator>
    </item>
    <item>
      <title>A physics-augmented neural network framework for finite strain incompressible viscoelasticity</title>
      <link>https://arxiv.org/abs/2511.02959</link>
      <description>arXiv:2511.02959v1 Announce Type: new 
Abstract: We propose a physics-augmented neural network (PANN) framework for finite strain incompressible viscoelasticity within the generalized standard materials theory. The formulation is based on the multiplicative decomposition of the deformation gradient and enforces unimodularity of the inelastic deformation part throughout the evolution. Invariant-based representations of the free energy and the dual dissipation potential by monotonic and fully input-convex neural networks ensure thermodynamic consistency, objectivity, and material symmetry by construction. The evolution of the internal variables during training is handled by solving the evolution equations using an implicit exponential time integrator. In addition, a trainable gate layer combined with lp regularization automatically identifies the required number of internal variables during training. The PANN is calibrated with synthetic and experimental data, showing excellent agreement for a wide range of deformation rates and different load paths. We also show that the proposed model achieves excellent interpolation as well as plausible and accurate extrapolation behaviors. In addition, we demonstrate consistency of the PANN with linear viscoelasticity by linearization of the full model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02959v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Karl A. Kalina, J\"org Brummund, Markus K\"astner</dc:creator>
    </item>
    <item>
      <title>Hybrid DeepONet Surrogates for Multiphase Flow in Porous Media</title>
      <link>https://arxiv.org/abs/2511.02962</link>
      <description>arXiv:2511.02962v1 Announce Type: new 
Abstract: The solution of partial differential equations (PDEs) plays a central role in numerous applications in science and engineering, particularly those involving multiphase flow in porous media. Complex, nonlinear systems govern these problems and are notoriously computationally intensive, especially in real-world applications and reservoirs. Recent advances in deep learning have spurred the development of data-driven surrogate models that approximate PDE solutions with reduced computational cost. Among these, Neural Operators such as Fourier Neural Operator (FNO) and Deep Operator Networks (DeepONet) have shown strong potential for learning parameter-to-solution mappings, enabling the generalization across families of PDEs. However, both methods face challenges when applied independently to complex porous media flows, including high memory requirements and difficulty handling the time dimension. To address these limitations, this work introduces hybrid neural operator surrogates based on DeepONet models that integrate Fourier Neural Operators, Multi-Layer Perceptrons (MLPs), and Kolmogorov-Arnold Networks (KANs) within their branch and trunk networks. The proposed framework decouples spatial and temporal learning tasks by splitting these structures into the branch and trunk networks, respectively. We evaluate these hybrid models on multiphase flow in porous media problems ranging in complexity from the steady 2D Darcy flow to the 2D and 3D problems belonging to the $10$th Comparative Solution Project from the Society of Petroleum Engineers. Results demonstrate that hybrid schemes achieve accurate surrogate modeling with significantly fewer parameters while maintaining strong predictive performance on large-scale reservoir simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02962v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel S. Santos, Gabriel F. Barros, Amanda C. N. Oliveira, R\^omulo M. Silva, Rodolfo S. M. Freitas, Dakshina M. Valiveti, Xiao-Hui Wu, Fernando A. Rochinha, Alvaro L. G. A. Coutinho</dc:creator>
    </item>
    <item>
      <title>GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes</title>
      <link>https://arxiv.org/abs/2511.03170</link>
      <description>arXiv:2511.03170v1 Announce Type: new 
Abstract: Quantitative structure-activity relationship assumes a smooth relationship between molecular structure and biological activity. However, activity cliffs defined as pairs of structurally similar compounds with large potency differences break this continuity. Recent benchmarks targeting activity cliffs have revealed that classical machine learning models with extended connectivity fingerprints outperform graph neural networks. Our analysis shows that graph embeddings fail to adequately separate structurally similar molecules in the embedding space, making it difficult to distinguish between structurally similar but functionally different molecules. Despite this limitation, molecular graph structures are inherently expressive and attractive, as they preserve molecular topology. To preserve the structural representation of molecules as graphs, we propose a new model, GraphCliff, which integrates short- and long-range information through a gating mechanism. Experimental results demonstrate that GraphCliff consistently improves performance on both non-cliff and cliff compounds. Furthermore, layer-wise node embedding analyses reveal reduced over-smoothing and enhanced discriminative power relative to strong baseline graph models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03170v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hajung Kim, Jueon Park, Junseok Choe, Sheunheun Baek, Hyeon Hwang, Jaewoo Kang</dc:creator>
    </item>
    <item>
      <title>A Theoretical Framework for Environmental Similarity and Vessel Mobility as Coupled Predictors of Marine Invasive Species Pathways</title>
      <link>https://arxiv.org/abs/2511.03499</link>
      <description>arXiv:2511.03499v2 Announce Type: new 
Abstract: Marine invasive species spread through global shipping and generate substantial ecological and economic impacts. Traditional risk assessments require detailed records of ballast water and traffic patterns, which are often incomplete, limiting global coverage. This work advances a theoretical framework that quantifies invasion risk by combining environmental similarity across ports with observed and forecasted maritime mobility. Climate-based feature representations characterize each port's marine conditions, while mobility networks derived from Automatic Identification System data capture vessel flows and potential transfer pathways. Clustering and metric learning reveal climate analogues and enable the estimation of species survival likelihood along shipping routes. A temporal link prediction model captures how traffic patterns may change under shifting environmental conditions. The resulting fusion of environmental similarity and predicted mobility provides exposure estimates at the port and voyage levels, supporting targeted monitoring, routing adjustments, and management interventions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03499v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gabriel Spadon, Vaishnav Vaidheeswaran, Claudio DiBacco</dc:creator>
    </item>
    <item>
      <title>Simulation-Based Validation of an Integrated 4D/5D Digital-Twin Framework for Predictive Construction Control</title>
      <link>https://arxiv.org/abs/2511.03684</link>
      <description>arXiv:2511.03684v1 Announce Type: new 
Abstract: Persistent cost and schedule deviations remain a major challenge in the U.S. construction industry, revealing the limitations of deterministic CPM and static document-based estimating. This study presents an integrated 4D/5D digital-twin framework that couples Building Information Modeling (BIM) with natural-language processing (NLP)-based cost mapping, computer-vision (CV)-driven progress measurement, Bayesian probabilistic CPM updating, and deep-reinforcement-learning (DRL) resource-leveling. A nine-month case implementation on a Dallas-Fort Worth mid-rise project demonstrated measurable gains in accuracy and efficiency: 43% reduction in estimating labor, 6% reduction in overtime, and 30% project-buffer utilization, while maintaining an on-time finish at 128 days within P50-P80 confidence bounds. The digital-twin sandbox also enabled real-time "what-if" forecasting and traceable cost-schedule alignment through a 5D knowledge graph. Findings confirm that integrating AI-based analytics with probabilistic CPM and DRL enhances forecasting precision, transparency, and control resilience. The validated workflow establishes a practical pathway toward predictive, adaptive, and auditable construction management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03684v1</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atena Khoshkonesh, Mohsen Mohammadagha, Navid Ebrahimi</dc:creator>
    </item>
    <item>
      <title>Multi-Region Matrix Interpolation for Dynamic Analysis of Aperiodic Structures under Large Model Parameter Perturbations</title>
      <link>https://arxiv.org/abs/2511.03711</link>
      <description>arXiv:2511.03711v1 Announce Type: new 
Abstract: This work introduces a surrogate-based model for efficiently estimating the frequency response of dynamic mechanical metamaterials, particularly when dealing with large parametric perturbations and aperiodic substructures. The research builds upon a previous matrix interpolation method applied on top of a Craig-Bampton modal reduction, allowing the variations of geometrical features without the need to remesh and recompute Finite Element matrices. This existing procedure has significant limitations since it requires a common modal projection, which inherently restricts the allowable perturbation size of the model parameters, thereby limiting the model parameter space where matrices can be effectively interpolated. The present work offers three contributions: (1) It provides structural dynamic insight into the restrictions imposed by the common modal projection, demonstrating that ill-conditioning can be controlled, (2) it proposes an efficient, sampling-based procedure to identify the non-regular boundaries of the usable region in the model parameter space, and (3) it enhances the surrogate model to accommodate larger model parameter perturbations by proposing a multi-region interpolation strategy. The efficacy of this proposed framework is verified through two illustrative examples. The first example, involving a unit cell with a square plate and circular core, validates the approach for a single well-conditioned projection region. The second example, using a beam-like structure with vibration attenuation bands, demonstrates the true advantage of the multi-region approach, where predictions from traditional Lagrange interpolation deviated significantly with increasing perturbations, while the proposed method maintained high accuracy across different perturbation levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03711v1</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Pereira, R. O. Ruiz</dc:creator>
    </item>
    <item>
      <title>Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks</title>
      <link>https://arxiv.org/abs/2511.02957</link>
      <description>arXiv:2511.02957v1 Announce Type: cross 
Abstract: Pavement infrastructure monitoring is challenged by complex spatial dependencies, changing environmental conditions, and non-linear deterioration across road networks. Traditional Pavement Management Systems (PMS) remain largely reactive, lacking real-time intelligence for failure prevention and optimal maintenance planning. To address this, we propose a unified Digital Twin (DT) and Graph Neural Network (GNN) framework for scalable, data-driven pavement health monitoring and predictive maintenance. Pavement segments and spatial relations are modeled as graph nodes and edges, while real-time UAV, sensor, and LiDAR data stream into the DT. The inductive GNN learns deterioration patterns from graph-structured inputs to forecast distress and enable proactive interventions. Trained on a real-world-inspired dataset with segment attributes and dynamic connectivity, our model achieves an R2 of 0.3798, outperforming baseline regressors and effectively capturing non-linear degradation. We also develop an interactive dashboard and reinforcement learning module for simulation, visualization, and adaptive maintenance planning. This DT-GNN integration enhances forecasting precision and establishes a closed feedback loop for continuous improvement, positioning the approach as a foundation for proactive, intelligent, and sustainable pavement management, with future extensions toward real-world deployment, multi-agent coordination, and smart-city integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02957v1</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>cs.ET</category>
      <category>cs.NE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Mohsin Mahmud Topu, Mahfuz Ahmed Anik, Azmine Toushik Wasi, Md Manjurul Ahsan</dc:creator>
    </item>
    <item>
      <title>Adaptive-Sensorless Monitoring of Shipping Containers</title>
      <link>https://arxiv.org/abs/2511.03022</link>
      <description>arXiv:2511.03022v1 Announce Type: cross 
Abstract: Monitoring the internal temperature and humidity of shipping containers is essential to preventing quality degradation during cargo transportation. Sensorless monitoring -- machine learning models that predict the internal conditions of the containers using exogenous factors -- shows promise as an alternative to monitoring using sensors. However, it does not incorporate telemetry information and correct for systematic errors, causing the predictions to differ significantly from the live data and confusing the users. In this paper, we introduce the residual correction method, a general framework for correcting for systematic biases in sensorless models after observing live telemetry data. We call this class of models ``adaptive-sensorless'' monitoring. We train and evaluate adaptive-sensorless models on the 3.48 million data points -- the largest dataset of container sensor readings ever used in academic research -- and show that they produce consistent improvements over the baseline sensorless models. When evaluated on the holdout set of the simulated data, they achieve average mean absolute errors (MAEs) of 2.24 $\sim$ 2.31$^\circ$C (vs 2.43$^\circ$C by sensorless) for temperature and 5.72 $\sim$ 7.09% for relative humidity (vs 7.99% by sensorless) and average root mean-squared errors (RMSEs) of 3.19 $\sim$ 3.26$^\circ$C for temperature (vs 3.38$^\circ$C by sensorless) and 7.70 $\sim$ 9.12% for relative humidity (vs 10.0% by sensorless). Adaptive-sensorless models enable more accurate cargo monitoring, early risk detection, and less dependence on full connectivity in global shipping.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03022v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lingqing Shen, Chi Heem Wong, Misaki Mito, Arnab Chakrabarti</dc:creator>
    </item>
    <item>
      <title>A Tsallis-Entropy Lens on Genetic Variation</title>
      <link>https://arxiv.org/abs/2511.03063</link>
      <description>arXiv:2511.03063v1 Announce Type: cross 
Abstract: We introduce an information-theoretic generalization of the fixation statistic, the Tsallis-order $q$ F-statistic, $F_q$, which measures the fraction of Tsallis $q$-entropy lost within subpopulations relative to the pooled population. The family nests the classical variance-based fixation index $F_{\textbf{ST}}$ at $q{=}2$ and a Shannon-entropy analogue at $q{=}1$, whose absolute form equals the mutual information between alleles and population labels. By varying $q$, $F_q$ acts as a spectral differentiator that up-weights rare variants at low $q$, while $q{&gt;}1$ increasingly emphasizes common variants, providing a more fine-grained view of differentiation than $F_{\textbf{ST}}$ when allele-frequency spectra are skewed. On real data (865 Oceanian genomes with 1,823,000 sites) and controlled genealogical simulations (seeded from 1,432 founders from HGDP and 1000 Genomes panels, with 322,216 sites), we show that $F_q$ in One-vs-Rest (OVR) and Leave-One-Out (LOO) modes provides clear attribution of which subpopulations drive regional structure, and sensitively timestamps isolation-migration events and founder effects. $F_q$ serves as finer-resolution complement for simulation audits and population-structure summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03063v1</guid>
      <category>cs.IT</category>
      <category>cs.CE</category>
      <category>math.IT</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Margarita Geleta, Daniel Mas Montserrat, Alexander G. Ioannidis</dc:creator>
    </item>
    <item>
      <title>System Identification of a Moored ASV with Recessed Moon Pool via Deterministic and Bayesian Hankel-DMDc</title>
      <link>https://arxiv.org/abs/2511.03482</link>
      <description>arXiv:2511.03482v1 Announce Type: cross 
Abstract: This study addresses the system identification of a small autonomous surface vehicle (ASV) under moored conditions using Hankel dynamic mode decomposition with control (HDMDc) and its Bayesian extension (BHDMDc). Experiments were carried out on a Codevintec CK-14e ASV in the towing tank of CNR-INM, under both irregular and regular head-sea wave conditions. The ASV under investigation features a recessed moon pool, which induces nonlinear responses due to sloshing, thereby increasing the modelling challenge. Data-driven reduced-order models were built from measurements of vessel motions and mooring loads. The HDMDc framework provided accurate deterministic predictions of vessel dynamics, while the Bayesian formulation enabled uncertainty-aware characterization of the model response by accounting for variability in hyperparameter selection. Validation against experimental data demonstrated that both HDMDc and BHDMDc can predict the vessel's response to unseen regular and irregular wave excitations. In conclusion, the study shows that HDMDc-based ROMs are a viable data-driven alternative for system identification, demonstrating for the first time their generalization capability for a sea condition different from the training set, achieving high accuracy in reproducing vessel dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03482v1</guid>
      <category>eess.SY</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Palma, Ivan Santic, Andrea Serani, Lorenzo Minno, Matteo Diez</dc:creator>
    </item>
    <item>
      <title>LiveTradeBench: Seeking Real-World Alpha with Large Language Models</title>
      <link>https://arxiv.org/abs/2511.03628</link>
      <description>arXiv:2511.03628v1 Announce Type: cross 
Abstract: Large language models (LLMs) achieve strong performance across benchmarks--from knowledge quizzes and math reasoning to web-agent tasks--but these tests occur in static settings, lacking real dynamics and uncertainty. Consequently, they evaluate isolated reasoning or problem-solving rather than decision-making under uncertainty. To address this, we introduce LiveTradeBench, a live trading environment for evaluating LLM agents in realistic and evolving markets. LiveTradeBench follows three design principles: (i) Live data streaming of market prices and news, eliminating dependence on offline backtesting and preventing information leakage while capturing real-time uncertainty; (ii) a portfolio-management abstraction that extends control from single-asset actions to multi-asset allocation, integrating risk management and cross-asset reasoning; and (iii) multi-market evaluation across structurally distinct environments--U.S. stocks and Polymarket prediction markets--differing in volatility, liquidity, and information flow. At each step, an agent observes prices, news, and its portfolio, then outputs percentage allocations that balance risk and return. Using LiveTradeBench, we run 50-day live evaluations of 21 LLMs across families. Results show that (1) high LMArena scores do not imply superior trading outcomes; (2) models display distinct portfolio styles reflecting risk appetite and reasoning dynamics; and (3) some LLMs effectively leverage live signals to adapt decisions. These findings expose a gap between static evaluation and real-world competence, motivating benchmarks that test sequential decision making and consistency under live uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03628v1</guid>
      <category>q-fin.TR</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haofei Yu, Fenghai Li, Jiaxuan You</dc:creator>
    </item>
    <item>
      <title>Improving Gene Trees without more data</title>
      <link>https://arxiv.org/abs/2511.03692</link>
      <description>arXiv:2511.03692v1 Announce Type: cross 
Abstract: Estimating species and gene trees from sequence data is challenging. Gene tree estimation is often hampered by low phylogenetic signal in alignments, leading to inaccurate trees. Species tree estimation is complicated by incomplete lineage sorting (ILS), where gene histories differ from the species' history. Summary methods like MP-EST, ASTRAL2, and ASTRID infer species trees from gene trees but suffer when gene tree accuracy is low. To address this, the Statistical Binning (SB) and Weighted Statistical Binning (WSB) pipelines were developed to improve gene tree estimation. However, previous studies only tested these pipelines using multi-locus bootstrapping (MLBS), not the BestML approach.
  This thesis proposes a novel pipeline, WSB+WQMC, which shares design features with the existing WSB+CAML pipeline but has other desirable properties and is statistically consistent under the GTR+MSC model. This study evaluated WSB+WQMC against WSB+CAML using BestML analysis on various simulated datasets. The results confirmed many trends seen in prior MLBS analyses. WSB+WQMC substantially improved gene tree and species tree accuracy (using ASTRAL2 and ASTRID) on most datasets with low, medium, and moderately high ILS levels. In a direct comparison, WSB+WQMC computed less accurate trees than WSB+CAML under certain low and medium ILS conditions. However, WSB+WQMC performed better or at least as accurately as WSB+CAML on all datasets with moderately high and high ILS. It also proved better for estimating gene trees on some medium and low ILS datasets. Thus, WSB+WQMC is a promising alternative to WSB+CAML for phylogenetic estimation, especially in the presence of low phylogenetic signal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.03692v1</guid>
      <category>q-bio.PE</category>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashu Gupta</dc:creator>
    </item>
    <item>
      <title>ChemFM as a Scaling Law Guided Foundation Model Pre-trained on Informative Chemicals</title>
      <link>https://arxiv.org/abs/2410.21422</link>
      <description>arXiv:2410.21422v3 Announce Type: replace 
Abstract: Traditional AI methods often rely on task-specific model designs and training, which constrain both the scalability of model size and generalization across different tasks. Here, we introduce ChemFM, a large foundation model specifically developed for chemicals. By conducting a series of scaling experiments, we identify UniChem as the informative molecular database for pre-training the foundation model. ChemFM comprises 3 billion parameters and is pre-trained on 178 million molecules using self-supervised causal language modeling to extract generalizable molecular representations. This model can be adapted to diverse downstream chemical applications using either full-parameter or parameter-efficient fine-tuning methods. ChemFM consistently outperforms state-of-the-art task-specific AI models across all tested tasks. Notably, it achieves up to 67.48% performance improvement across 34 property prediction benchmarks, up to 33.80% reduction in mean average deviation between conditioned and actual properties of generated molecules in conditional molecular generation tasks, and up to 3.7% top-1 accuracy improvement across 4 reaction prediction datasets. Moreover, ChemFM demonstrates its superior performance in predicting antibiotic activity and cytotoxicity, highlighting its potential to advance the discovery of novel antibiotics. Furthermore, we demonstrate that, as a foundation model, ChemFM exhibits strong data efficiency, requiring significantly fewer labeled training samples to achieve state-of-the-art performance. We anticipate that ChemFM will significantly advance chemistry research by providing a foundation model capable of effectively generalizing across a broad range of tasks with minimal additional training.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21422v3</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Feiyang Cai, Katelin Zacour, Tianyu Zhu, Tzuen-Rong Tzeng, Yongping Duan, Ling Liu, Srikanth Pilla, Gang Li, Feng Luo</dc:creator>
    </item>
    <item>
      <title>A data-driven framework for team selection in Fantasy Premier League</title>
      <link>https://arxiv.org/abs/2505.02170</link>
      <description>arXiv:2505.02170v2 Announce Type: replace 
Abstract: Fantasy football is a billion-dollar industry with millions of participants. Under a fixed budget, managers select squads to maximize future Fantasy Premier League (FPL) points. This study formulates lineup selection as data-driven optimization and develops deterministic and robust mixed-integer linear programs that choose the starting eleven, bench, and captain under budget, formation, and club-quota constraints (maximum three players per club). The objective is parameterized by a hybrid scoring metric that combines realized FPL points with predictions from a linear regression model trained on match-performance features identified using exploratory data analysis techniques. The study benchmarks alternative objectives and cost estimators, including simple and recency-weighted averages, exponential smoothing, autoregressive integrated moving average (ARIMA), and Monte Carlo simulation. Experiments on the 2023/24 Premier League season show that ARIMA with a constrained budget and a rolling window yields the most consistent out-of-sample performance; weighted averages and Monte Carlo are also competitive. Robust variants improve some objectives but are not uniformly superior. The framework provides transparent decision support for fantasy roster construction and extends to FPL chips, multi-week rolling-horizon transfer planning, and week-by-week dynamic captaincy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.02170v2</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Danial Ramezani, Tai Dinh</dc:creator>
    </item>
    <item>
      <title>An Eulerian Data Assimilation Method for Two-Layer Quasi-Geostrophic Model in Physical Domain</title>
      <link>https://arxiv.org/abs/2509.14586</link>
      <description>arXiv:2509.14586v4 Announce Type: replace 
Abstract: Data assimilation (DA) integrates observational data with numerical models to improve the prediction of complex physical systems. However, traditional DA methods often struggle with nonlinear dynamics and multi-scale variability, particularly when implemented directly in the physical domain. To address these challenges, this work develops an Eulerian Data Assimilation (EuDA) method with the Conditional Gaussian Nonlinear System (CGNS). The proposed approach enables the treatment of systems with non-periodic boundaries and provides a more intuitive representation of localized and time-dependent phenomena. The work considers a simplified physical domain inspired by sea-ice floe trajectories and ocean eddy recovery in the Arctic regions, where the dynamics are modeled by a two-layer quasi-geostrophic (QG) system. The QG equations are numerically solved using forward-Euler time stepping and centered finite-difference schemes. CGNS provides a nonlinear filter as it offers an analytical and continuous formulation for filtering a nonlinear system. Model performance is assessed using normalized root mean square error (RMSE) and pattern correlation (Corr) of the posterior mean. The results show that both metrics improve monotonically with refining timesteps, while RMSE converges to approximately 0.1, which is the noise strength, and Corr increases from 0.64 to 0.92 as the grid resolution becomes finer. Lastly, a coupled scenario with sea-ice particles advected by the two-layer QG flow under a linear drag force is examined, demonstrating the flexibility of the EuDA-CGNS framework in capturing coupled ice-ocean interactions. These findings demonstrate the effectiveness of exploiting the two-layer QG model in the physical domain to capture multiscale flow features.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14586v4</guid>
      <category>cs.CE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hyeonggeun Yun, Quanling Deng</dc:creator>
    </item>
    <item>
      <title>DiffSpectra: Molecular Structure Elucidation from Spectra using Diffusion Models</title>
      <link>https://arxiv.org/abs/2507.06853</link>
      <description>arXiv:2507.06853v2 Announce Type: replace-cross 
Abstract: Molecular structure elucidation from spectra is a fundamental challenge in molecular science. Conventional approaches rely heavily on expert interpretation and lack scalability, while retrieval-based machine learning approaches remain constrained by limited reference libraries. Generative models offer a promising alternative, yet most adopt autoregressive architectures that overlook 3D geometry and struggle to integrate diverse spectral modalities. In this work, we present DiffSpectra, a generative framework that formulates molecular structure elucidation as a conditional generation process, directly inferring 2D and 3D molecular structures from multi-modal spectra using diffusion models. Its denoising network is parameterized by the Diffusion Molecule Transformer, an SE(3)-equivariant architecture for geometric modeling, conditioned by SpecFormer, a Transformer-based spectral encoder capturing multi-modal spectral dependencies. Extensive experiments demonstrate that DiffSpectra accurately elucidates molecular structures, achieving 40.76% top-1 and 99.49% top-10 accuracy. Its performance benefits substantially from 3D geometric modeling, SpecFormer pre-training, and multi-modal conditioning. To our knowledge, DiffSpectra is the first framework that unifies multi-modal spectral reasoning and joint 2D/3D generative modeling for de novo molecular structure elucidation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.06853v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>physics.chem-ph</category>
      <category>q-bio.MN</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Liang Wang, Yu Rong, Tingyang Xu, Zhenyi Zhong, Zhiyuan Liu, Pengju Wang, Deli Zhao, Qiang Liu, Shu Wu, Liang Wang, Yang Zhang</dc:creator>
    </item>
    <item>
      <title>Personalized Transcranial Electrical Stimulation: A Review of Computational Modeling and Optimization</title>
      <link>https://arxiv.org/abs/2509.01192</link>
      <description>arXiv:2509.01192v2 Announce Type: replace-cross 
Abstract: Objective. Personalized transcranial electrical stimulation (tES) has gained growing attention due to the substantial inter-individual variability in brain anatomy and physiology. While previous reviews have discussed the physiological mechanisms and clinical applications of tES, there remains a critical gap in up-to-date syntheses focused on the computational modeling frameworks that enable individualized stimulation optimization. Approach. This review presents a comprehensive overview of recent advances in computational techniques supporting personalized tES. We systematically examine developments in forward modeling for simulating individualized electric fields, as well as inverse modeling approaches for optimizing stimulation parameters. We critically evaluate progress in head modeling pipelines, optimization algorithms, and the integration of multimodal brain data. Main results. Recent advances have substantially accelerated the construction of subject-specific head conductor models and expanded the landscape of optimization methods, including multi-objective optimization and brain network-informed optimization. These advances allow for dynamic and individualized stimulation planning, moving beyond empirical trial-and-error approaches.Significance. By integrating the latest developments in computational modeling for personalized tES, this review highlights current challenges, emerging opportunities, and future directions for achieving precision neuromodulation in both research and clinical contexts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01192v2</guid>
      <category>q-bio.NC</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mo Wang, Kexin Zheng, Yingyue Xin, Xiang Chen, Yiling Liu, Huichun Luo, Jingsheng Tang, Tifei Yuan, Hongkai Wen, Pengfei Wei, Quanying Liu</dc:creator>
    </item>
    <item>
      <title>FaStfact: Faster, Stronger Long-Form Factuality Evaluations in LLMs</title>
      <link>https://arxiv.org/abs/2510.12839</link>
      <description>arXiv:2510.12839v2 Announce Type: replace-cross 
Abstract: Evaluating the factuality of long-form generations from Large Language Models (LLMs) remains challenging due to efficiency bottlenecks and reliability concerns. Prior efforts attempt this by decomposing text into claims, searching for evidence, and verifying claims, but suffer from critical drawbacks: (1) inefficiency due to overcomplicated pipeline components, and (2) ineffectiveness stemming from inaccurate claim sets and insufficient evidence. To address these limitations, we propose \textbf{FaStfact}, an evaluation framework that achieves the highest alignment with human evaluation and time/token efficiency among existing baselines. FaStfact first employs chunk-level claim extraction integrated with confidence-based pre-verification, significantly reducing the time and token cost while ensuring reliability. For searching and verification, it collects document-level evidence from crawled web-pages and selectively retrieves it during verification. Extensive experiments based on an annotated benchmark \textbf{FaStfact-Bench} demonstrate the reliability of FaStfact in both efficiently and effectively evaluating long-form factuality. Code, benchmark data, and annotation interface tool are available at https://github.com/Yingjia-Wan/FaStfact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.12839v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.CY</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yingjia Wan, Haochen Tan, Xiao Zhu, Xinyu Zhou, Zhiwei Li, Qingsong Lv, Changxuan Sun, Jiaqi Zeng, Yi Xu, Jianqiao Lu, Yinhong Liu, Zhijiang Guo</dc:creator>
    </item>
    <item>
      <title>Novelty and Impact of Economics Papers</title>
      <link>https://arxiv.org/abs/2511.01211</link>
      <description>arXiv:2511.01211v2 Announce Type: replace-cross 
Abstract: We propose a framework that recasts scientific novelty not as a single attribute of a paper, but as a reflection of its position within the evolving intellectual landscape. We decompose this position into two orthogonal dimensions: \textit{spatial novelty}, which measures a paper's intellectual distinctiveness from its neighbors, and \textit{temporal novelty}, which captures its engagement with a dynamic research frontier. To operationalize these concepts, we leverage Large Language Models to develop semantic isolation metrics that quantify a paper's location relative to the full-text literature. Applying this framework to a large corpus of economics articles, we uncover a fundamental trade-off: these two dimensions predict systematically different outcomes. Temporal novelty primarily predicts citation counts, whereas spatial novelty predicts disruptive impact. This distinction allows us to construct a typology of semantic neighborhoods, identifying four archetypes associated with distinct and predictable impact profiles. Our findings demonstrate that novelty can be understood as a multidimensional construct whose different forms, reflecting a paper's strategic location, have measurable and fundamentally distinct consequences for scientific progress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01211v2</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>cs.CL</category>
      <category>cs.DL</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chaofeng Wu</dc:creator>
    </item>
    <item>
      <title>In Situ Training of Implicit Neural Compressors for Scientific Simulations via Sketch-Based Regularization</title>
      <link>https://arxiv.org/abs/2511.02659</link>
      <description>arXiv:2511.02659v2 Announce Type: replace-cross 
Abstract: Focusing on implicit neural representations, we present a novel in situ training protocol that employs limited memory buffers of full and sketched data samples, where the sketched data are leveraged to prevent catastrophic forgetting. The theoretical motivation for our use of sketching as a regularizer is presented via a simple Johnson-Lindenstrauss-informed result. While our methods may be of wider interest in the field of continual learning, we specifically target in situ neural compression using implicit neural representation-based hypernetworks. We evaluate our method on a variety of complex simulation data in two and three dimensions, over long time horizons, and across unstructured grids and non-Cartesian geometries. On these tasks, we show strong reconstruction performance at high compression rates. Most importantly, we demonstrate that sketching enables the presented in situ scheme to approximately match the performance of the equivalent offline method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02659v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Thu, 06 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cooper Simpson, Stephen Becker, Alireza Doostan</dc:creator>
    </item>
  </channel>
</rss>
