<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 15 Apr 2025 04:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Empirically Measuring Data Localization in the EU</title>
      <link>https://arxiv.org/abs/2504.09019</link>
      <description>arXiv:2504.09019v1 Announce Type: new 
Abstract: EU data localization regulations limit data transfers to non-EU countries with the GDPR. However, BGP, DNS and other Internet protocols were not designed to enforce jurisdictional constraints, so implementing data localization is challenging. Despite initial research on the topic, little is known about if or how companies currently operate their server infrastructure to comply with the regulations. We close this knowledge gap by empirically measuring the extent to which servers and routers that process EU requests are located outside of the EU (and a handful of "adequate" non-EU countries). The key challenge is that both browser measurements (to infer relevant endpoints) and data-plane measurements (to infer relevant IP addresses) are needed, but no large-scale public infrastructure allows both. We build a novel methodology that combines BrightData (browser) and RIPE Atlas (data-plane) probes, with joint measurements from over 1,000 networks in 20 EU countries. We find that, on average, 2.2% of servers serving users in each EU country are located in non-adequate destination countries (1.4% of known trackers). Our findings suggest that data localization policies are largely being followed by content providers, though there are exceptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09019v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Gamero-Garrido, Kicho Yu, Sumukh Vasisht Shankar, Sachin Kumar Singh, Sindhya Balasubramanian, Alexander Wilcox, David Choffnes</dc:creator>
    </item>
    <item>
      <title>RSLAQ -- A Robust SLA-driven 6G O-RAN QoS xApp using deep reinforcement learning</title>
      <link>https://arxiv.org/abs/2504.09187</link>
      <description>arXiv:2504.09187v1 Announce Type: new 
Abstract: The evolution of 6G envisions a wide range of applications and services characterized by highly differentiated and stringent Quality of Service (QoS) requirements. Open Radio Access Network (O-RAN) technology has emerged as a transformative approach that enables intelligent software-defined management of the RAN. A cornerstone of O-RAN is the RAN Intelligent Controller (RIC), which facilitates the deployment of intelligent applications (xApps and rApps) near the radio unit. In this context, QoS management through O-RAN has been explored using network slice and machine learning (ML) techniques. Although prior studies have demonstrated the ability to optimize RAN resource allocation and prioritize slices effectively, they have not considered the critical integration of Service Level Agreements (SLAs) into the ML learning process. This omission can lead to suboptimal resource utilization and, in many cases, service outages when target Key Performance Indicators (KPIs) are not met. This work introduces RSLAQ, an innovative xApp designed to ensure robust QoS management for RAN slicing while incorporating SLAs directly into its operational framework. RSLAQ translates operator policies into actionable configurations, guiding resource distribution and scheduling for RAN slices. Using deep reinforcement learning (DRL), RSLAQ dynamically monitors RAN performance metrics and computes optimal actions, embedding SLA constraints to mitigate conflicts and prevent outages. Extensive system-level simulations validate the efficacy of the proposed solution, demonstrating its ability to optimize resource allocation, improve SLA adherence, and maintain operational reliability (&gt;95%) in challenging scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09187v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Noe M. Yungaicela-Naula, Vishal Sharma, Sandra Scott-Hayward</dc:creator>
    </item>
    <item>
      <title>A Datagram Extension to DNS over QUIC: Proven Resource Conservation in the Internet of Things</title>
      <link>https://arxiv.org/abs/2504.09200</link>
      <description>arXiv:2504.09200v1 Announce Type: new 
Abstract: In this paper, we investigate the Domain Name System (DNS) over QUIC (DoQ) and propose a non-disruptive extension, which can greatly reduce DoQ's resource consumption. This extension can benefit all DNS clients - especially Internet of Things (IoT) devices. This is important because even resource-constrained IoT devices can generate dozens of DNS requests every hour. DNS is a crucial service that correlates IP addresses and domain names. It is traditionally sent as plain-text, favoring low-latency results over security and privacy. The repercussion of this can be eavesdropping and information leakage about IoT devices. To address these concerns, the newest and most promising solution is DoQ. QUIC offers features similar to TCP and TLS while also supporting early data delivery and stream multiplexing. DoQ's specification requires that DNS exchanges occur over independent streams in a long-lived QUIC connection. Our hypothesis is that due to DNS's typically high transaction volume, managing QUIC streams may be overly resource intensive for IoT devices. Therefore, we have designed and implemented a data delivery mode for DoQ using QUIC datagrams, which we believe to be more preferable than stream-based delivery. To test our theory, we analyzed the memory, CPU, signaling, power, and time of each DoQ delivery mode in a setup generating real queries and network traffic. Our novel datagram-based delivery mode proved to be decisively more resource-friendly with little compromise in terms of functionality or performance. Furthermore, our paper is the first to investigate multiple queries over DoQ, to our knowledge.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09200v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Darius Saif, Ashraf Matrawy</dc:creator>
    </item>
    <item>
      <title>RoboComm: A DID-based scalable and privacy-preserving Robot-to-Robot interaction over state channels</title>
      <link>https://arxiv.org/abs/2504.09517</link>
      <description>arXiv:2504.09517v1 Announce Type: new 
Abstract: In a multi robot system establishing trust amongst untrusted robots from different organisations while preserving a robot's privacy is a challenge. Recently decentralized technologies such as smart contract and blockchain are being explored for applications in robotics. However, the limited transaction processing and high maintenance cost hinder the widespread adoption of such approaches. Moreover, blockchain transactions be they on public or private permissioned blockchain are publically readable which further fails to preserve the confidentiality of the robot's data and privacy of the robot.
  In this work, we propose RoboComm a Decentralized Identity based approach for privacy-preserving interaction between robots. With DID a component of Self-Sovereign Identity; robots can authenticate each other independently without relying on any third-party service. Verifiable Credentials enable private data associated with a robot to be stored within the robot's hardware, unlike existing blockchain based approaches where the data has to be on the blockchain. We improve throughput by allowing message exchange over state channels. Being a blockchain backed solution RoboComm provides a trustworthy system without relying on a single party. Moreover, we implement our proposed approach to demonstrate the feasibility of our solution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09517v1</guid>
      <category>cs.NI</category>
      <category>cs.RO</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roshan Singh, Sushant Pandey</dc:creator>
    </item>
    <item>
      <title>Evaluating Machine Learning-Driven Intrusion Detection Systems in IoT: Performance and Energy Consumption</title>
      <link>https://arxiv.org/abs/2504.09634</link>
      <description>arXiv:2504.09634v1 Announce Type: new 
Abstract: In the evolving landscape of the Internet of Things (IoT), Machine Learning (ML)-based Intrusion Detection Systems (IDS) represent a significant advancement, especially when integrated with Software-Defined Networking (SDN). These systems play a critical role in enhancing security infrastructure within resource-constrained IoT systems. Despite their growing adoption, limited research has explored the impact of ML-based IDS on key performance metrics, such as CPU load, CPU usage, and energy consumption, particularly under real-time cyber threats. This study bridges that gap through an empirical evaluation of cutting-edge ML-based IDSs deployed at the edge of IoT networks under both benign and attack scenarios. Additionally, we investigate how SDN's centralized control and dynamic resource management influence IDS performance. Our experimental framework compares traditional ML-based IDS with deep learning (DL)-based counterparts, both with and without SDN integration. Results reveal that edge-deployed ML-based IDSs significantly impact system performance during cyber threats, with marked increases in resource consumption. SDN integration further influences these outcomes, emphasizing the need for optimized architectural design. Statistical analysis using ANOVA confirms the significance of our findings. This research provides critical insights into the performance and trade-offs of deploying ML-based IDSs in edge-based IoT systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09634v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saeid Jamshidi, Kawser Wazed Nafi, Amin Nikanjam, Foutse Khomh</dc:creator>
    </item>
    <item>
      <title>Accelerating Ray Tracing-Based Wireless Channels Generation for Real-Time Network Digital Twins</title>
      <link>https://arxiv.org/abs/2504.09751</link>
      <description>arXiv:2504.09751v1 Announce Type: new 
Abstract: Ray tracing (RT) simulation is a widely used approach to enable modeling wireless channels in applications such as network digital twins. However, the computational cost to execute RT is proportional to factors such as the level of detail used in the adopted 3D scenario. This work proposes RT pre-processing algorithms that aim at simplifying the 3D scene without distorting the channel. It also proposes a post-processing method that augments a set of RT results to achieve an improved time resolution. These methods enable using RT in applications that use a detailed and photorealistic 3D scenario, while generating consistent wireless channels over time. Our simulation results with different 3D scenarios demonstrate that it is possible to reduce the simulation time by more than 50% without compromising the accuracy of the RT parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09751v1</guid>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Cl\'audio Modesto, Lucas Mozart, Pedro Batista, Andr\'e Cavalcante, Aldebaro Klautau</dc:creator>
    </item>
    <item>
      <title>Vermilion: A Traffic-Aware Reconfigurable Optical Interconnect with Formal Throughput Guarantees</title>
      <link>https://arxiv.org/abs/2504.09892</link>
      <description>arXiv:2504.09892v1 Announce Type: new 
Abstract: The increasing gap between datacenter traffic volume and the capacity of electrical switches has driven the development of reconfigurable network designs utilizing optical circuit switching. Recent advancements, particularly those featuring periodic fixed-duration reconfigurations, have achieved practical end-to-end delays of just a few microseconds. However, current designs rely on multi-hop routing to enhance utilization, which can lead to a significant reduction in worst-case throughput and added overhead from congestion control and routing complexity. These factors pose significant operational challenges for the large-scale deployment of these technologies.
  We present Vermilion, a reconfigurable optical interconnect that breaks the throughput barrier of existing periodic reconfigurable networks, without the need for multi-hop routing -- thus eliminating congestion control and simplifying routing to direct communication. Vermilion adopts a traffic-aware approach while retaining the simplicity of periodic fixed-duration reconfigurations, similar to RotorNet. We formally establish throughput bounds for Vermilion, demonstrating that it achieves at least $33\%$ more throughput in the worst-case compared to existing designs. The key innovation of Vermilion is its short traffic-aware periodic schedule, derived using a matrix rounding technique. This schedule is then combined with a traffic-oblivious periodic schedule to efficiently manage any residual traffic. Our evaluation results support our theoretical findings, revealing significant performance gains for datacenter workloads.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09892v1</guid>
      <category>cs.NI</category>
      <category>cs.DS</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Vamsi Addanki, Chen Avin, Goran Dario Knabe, Giannis Patronas, Dimitris Syrivelis, Nikos Terzenidis, Paraskevas Bakopoulos, Ilias Marinos, Stefan Schmid</dc:creator>
    </item>
    <item>
      <title>Implementation and Performance Evaluation of TCP over QUIC Tunnels</title>
      <link>https://arxiv.org/abs/2504.10054</link>
      <description>arXiv:2504.10054v1 Announce Type: new 
Abstract: QUIC, a UDP-based transport protocol, addresses several limitations of TCP by offering built-in encryption, stream multiplexing, and improved loss recovery. To extend these benefits to legacy TCP-based applications, this paper explores the implementation and evaluation of a TCP over QUIC tunneling approach. A lightweight, stream-based tunnel is constructed using the Rust-based Quinn library, enabling TCP traffic to traverse QUIC connections transparently. Performance is evaluated under varying network conditions, including packet loss, high latency, and out-of-order delivery. Results indicate that TCP over QUIC maintains significantly higher throughput than native TCP in lossy or unstable environments, with up to a high improvement under 20\% packet loss. However, under ideal network conditions, tunneling introduces modest overhead due to encryption and user-space processing. These findings provide insights into the trade-offs of TCP over QUIC tunneling and its suitability for deployment in dynamic or impaired networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10054v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xuanhong Guo, Zekun Bao, Ying Chen</dc:creator>
    </item>
    <item>
      <title>IRR-Based AS Type of Relationship Inference</title>
      <link>https://arxiv.org/abs/2504.10299</link>
      <description>arXiv:2504.10299v1 Announce Type: new 
Abstract: The Internet comprises tens of thousands of autonomous systems (ASes) whose commercial relationships are not publicly announced. The classification of the Type of Relationship (ToR) between ASes has been extensively studied over the past two decades due to its relevance in network routing management and security.
  This paper presents a new approach to ToR classification, leveraging publicly available BGP data from the Internet Routing Registry (IRR). We show how the IRR can be mined and the results refined to achieve a large and accurate ToR database. Using a ground truth database with hundreds of entries we show that we indeed manage to obtain high accuracy. About two-thirds of our ToRs are new, namely, they were not obtained by previous works, which means that we enrich our ToR knowledge with links that are otherwise missed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10299v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amit Zulan, Omer Miron, Tal Shapira, Yuval Shavitt</dc:creator>
    </item>
    <item>
      <title>Staggering and Fragmentation for Improved Large Message Handling in libp2p GossipSub</title>
      <link>https://arxiv.org/abs/2504.10365</link>
      <description>arXiv:2504.10365v1 Announce Type: new 
Abstract: The libp2p GossipSub protocol leverages a full-message mesh with a lower node degree and a more densely connected metadata-only (gossip) mesh. This combination allows an efficient dissemination of messages in unstructured peer-to-peer (P2P) networks. However, GossipSub needs to consider message size, which is crucial for the efficient operation of many applications, such as handling large Ethereum blocks. This paper proposes modifications to improve GossipSub's performance when transmitting large messages. We evaluate the proposed improvements using the shadow simulator. Our results show that the proposed improvements significantly enhance GossipSub's performance for large message transmissions in sizeable networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10365v1</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Muhammad Umar Farooq, Tanguy Cizain, Daniel Kaiser</dc:creator>
    </item>
    <item>
      <title>Satellite Federated Fine-Tuning for Foundation Models in Space Computing Power Networks</title>
      <link>https://arxiv.org/abs/2504.10403</link>
      <description>arXiv:2504.10403v1 Announce Type: cross 
Abstract: Advancements in artificial intelligence (AI) and low-earth orbit (LEO) satellites have promoted the application of large remote sensing foundation models for various downstream tasks. However, direct downloading of these models for fine-tuning on the ground is impeded by privacy concerns and limited bandwidth. Satellite federated learning (FL) offers a solution by enabling model fine-tuning directly on-board satellites and aggregating model updates without data downloading. Nevertheless, for large foundation models, the computational capacity of satellites is insufficient to support effective on-board fine-tuning in traditional satellite FL frameworks. To address these challenges, we propose a satellite-ground collaborative federated fine-tuning framework. The key of the framework lies in how to reasonably decompose and allocate model components to alleviate insufficient on-board computation capabilities. During fine-tuning, satellites exchange intermediate results with ground stations or other satellites for forward propagation and back propagation, which brings communication challenges due to the special communication topology of space transmission networks, such as intermittent satellite-ground communication, short duration of satellite-ground communication windows, and unstable inter-orbit inter-satellite links (ISLs). To reduce transmission delays, we further introduce tailored communication strategies that integrate both communication and computing resources. Specifically, we propose a parallel intra-orbit communication strategy, a topology-aware satellite-ground communication strategy, and a latency-minimalization inter-orbit communication strategy to reduce space communication costs. Simulation results demonstrate significant reductions in training time with improvements of approximately 33%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10403v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yan zhu, Jingyang zhu, Ting Wang, Yuanming Shi, Chunxiao Jiang, Khaled Ben Letaief</dc:creator>
    </item>
    <item>
      <title>Joint Mobile IAB Node Positioning and Scheduler Selection in Locations With Substantial Obstacles</title>
      <link>https://arxiv.org/abs/2409.16831</link>
      <description>arXiv:2409.16831v4 Announce Type: replace 
Abstract: Integrated Access and Backhaul (IAB) in cellular networks combines access and backhaul within a wireless infrastructure reducing reliance on fibre-based backhaul. This enables faster and more cost-effective network expansion, especially in hard-to-reach areas. Positioning a mobile IAB node (MIAB) in a seaport environment, in order to ensure ondemand, resilient wireless connectivity, presents unique challenges due to the high density of User Equipments (UEs) and potential shadowing effects caused by obstacles. This paper addresses the problem of positioning MIABs within areas containing UEs, IAB donors (CFs), and obstacles. Our approach considers user associations and different types of scheduling, ensuring MIABs and CFs meet the capacity requirements of a special team of served UEs, while not exceeding backhaul capacity. Using Genetic Algorithm (GA) solver, we achieve capacity improvement gains, by up to 200% for the 90th percentile. The proposed solution improves network performance, particularly during emergency capacity demands.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.16831v4</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paulo Furtado Correia, Andre Coelho, Manuel Ricardo</dc:creator>
    </item>
    <item>
      <title>Energy-Efficient Flying LoRa Gateways: A Multi-Agent Reinforcement Learning Approach</title>
      <link>https://arxiv.org/abs/2502.03377</link>
      <description>arXiv:2502.03377v2 Announce Type: replace 
Abstract: As next-generation Internet of Things (NG-IoT) networks continue to grow, the number of connected devices is rapidly increasing, along with their energy demands. This creates challenges for resource management and sustainability. Energy-efficient communication, particularly for power-limited IoT devices, is therefore a key research focus. In this paper, we deployed flying LoRa gateways mounted on unmanned aerial vehicles (UAVs) to collect data from LoRa end devices and transmit it to a central server. Our primary objective is to maximize the global system energy efficiency of wireless LoRa networks by joint optimization of transmission power, spreading factor, bandwidth, and user association. To solve this challenging problem, we model the problem as a partially observable Markov decision process (POMDP), where each flying LoRa GW acts as a learning agent using a cooperative multi-agent reinforcement learning (MARL). Simulation results demonstrate that our proposed method, based on the multi-agent proximal policy optimization (MAPPO) algorithm, significantly improves the global system EE and surpasses the conventional MARL schemes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.03377v2</guid>
      <category>cs.NI</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abdullahi Isa Ahmed, Jamal Bentahar, El Mehdi Amhoud</dc:creator>
    </item>
    <item>
      <title>A Framework to Develop and Validate RL-Based Obstacle-Aware UAV Positioning Algorithms</title>
      <link>https://arxiv.org/abs/2502.08787</link>
      <description>arXiv:2502.08787v2 Announce Type: replace 
Abstract: Unmanned Aerial Vehicles (UAVs) are increasingly being utilized to enhance the Quality of Service (QoS) in wireless networks due to their flexibility and cost-effectiveness. However, optimizing UAV placement in dynamic and obstacleprone environments remains a research challenge. Reinforcement Learning (RL) has proven to be an effective approach that offers adaptability and robustness in such environments. This paper introduces RLpos-3, a novel framework that integrates standard RL techniques and existing libraries with Network Simulator 3 (ns-3) to facilitate the development and evaluation of UAV positioning algorithms. RLpos-3 serves as a supplementary tool for researchers, enabling the implementation, analysis, and benchmarking of UAV positioning strategies in different environmental settings while ensuring that user traffic demands are met. To validate its effectiveness, we present various use cases that demonstrate the performance of RLpos-3 in optimizing UAV placement under realistic conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08787v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kamran Shafafi, Manuel Ricardo, Rui Campos</dc:creator>
    </item>
    <item>
      <title>LLM-Enabled Data Transmission in End-to-End Semantic Communication</title>
      <link>https://arxiv.org/abs/2504.07431</link>
      <description>arXiv:2504.07431v2 Announce Type: replace 
Abstract: Emerging services such as augmented reality (AR) and virtual reality (VR) have increased the volume of data transmitted in wireless communication systems, revealing the limitations of traditional Shannon theory. To address these limitations, semantic communication has been proposed as a solution that prioritizes the meaning of messages over the exact transmission of bits. This paper explores semantic communication for text data transmission in end-to-end (E2E) systems through a novel approach called KG-LLM semantic communication, which integrates knowledge graph (KG) extraction and large language model (LLM) coding. In this method, the transmitter first utilizes a KG to extract key entities and relationships from sentences. The extracted information is then encoded using an LLM to obtain the semantic meaning. On the receiver side, messages are decoded using another LLM, while a bidirectional encoder representations from transformers (i.e., BERT) model further refines the reconstructed sentences for improved semantic similarity. The KG-LLM semantic communication method reduces the transmitted text data volume by 30% through KG-based compression and achieves 84\% semantic similarity between the original and received messages. This demonstrates the KG-LLM methods efficiency and robustness in semantic communication systems, outperforming the deep learning-based semantic communication model (DeepSC), which achieves only 63%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07431v2</guid>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shavbo Salehi, Melike Erol-Kantarci, Dusit Niyato</dc:creator>
    </item>
    <item>
      <title>Toward 6G Native-AI Network: Foundation Model based Cloud-Edge-End Collaboration Framework</title>
      <link>https://arxiv.org/abs/2310.17471</link>
      <description>arXiv:2310.17471v2 Announce Type: replace-cross 
Abstract: Future wireless communication networks are in a position to move beyond data-centric, device-oriented connectivity and offer intelligent, immersive experiences based on multi-agent collaboration, especially in the context of the thriving development of pre-trained foundation models (PFM) and the evolving vision of 6G native artificial intelligence (AI). Therefore, redefining modes of collaboration between devices and agents, and constructing native intelligence libraries become critically important in 6G. In this paper, we analyze the challenges of achieving 6G native AI from the perspectives of data, AI models, and operational paradigm. Then, we propose a 6G native AI framework based on foundation models, provide an integration method for the expert knowledge, present the customization for two kinds of PFM, and outline a novel operational paradigm for the native AI framework. As a practical use case, we apply this framework for orchestration, achieving the maximum sum rate within a cell-free massive MIMO system, and presenting preliminary evaluation results. Finally, we outline research directions for achieving native AI in 6G.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17471v2</guid>
      <category>cs.IT</category>
      <category>cs.DC</category>
      <category>cs.LG</category>
      <category>cs.NI</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Chen, Zhiheng Guo, Xijun Wang, Howard H. Yang, Chenyuan Feng, Shuangfeng Han, Xiaoyun Wang, Tony Q. S. Quek</dc:creator>
    </item>
    <item>
      <title>Online-Score-Aided Federated Learning: Taming the Resource Constraints in Wireless Networks</title>
      <link>https://arxiv.org/abs/2408.05886</link>
      <description>arXiv:2408.05886v3 Announce Type: replace-cross 
Abstract: While federated learning (FL) is a widely popular distributed machine learning (ML) strategy that protects data privacy, time-varying wireless network parameters and heterogeneous configurations of the wireless devices pose significant challenges. Although the limited radio and computational resources of the network and the clients, respectively, are widely acknowledged, two critical yet often ignored aspects are (a) wireless devices can only dedicate a small chunk of their limited storage for the FL task and (b) new training samples may arrive in an online manner in many practical wireless applications. Therefore, we propose a new FL algorithm called online-score-aided federated learning (OSAFL), specifically designed to learn tasks relevant to wireless applications under these practical considerations. Since clients' local training steps differ under resource constraints, which may lead to client drift under statistically heterogeneous data distributions, we leverage normalized gradient similarities and exploit weighting clients' updates based on optimized scores that facilitate the convergence rate of the proposed OSAFL algorithm without incurring any communication overheads to the clients or requiring any statistical data information from them. Our extensive simulation results on two different datasets with four popular ML models validate the effectiveness of OSAFL compared to five modified state-of-the-art FL baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05886v3</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md-Ferdous Pervej, Minseok Choi, Andreas F. Molisch</dc:creator>
    </item>
    <item>
      <title>SoK: Decoding the Enigma of Encrypted Network Traffic Classifiers</title>
      <link>https://arxiv.org/abs/2503.20093</link>
      <description>arXiv:2503.20093v3 Announce Type: replace-cross 
Abstract: The adoption of modern encryption protocols such as TLS 1.3 has significantly challenged traditional network traffic classification (NTC) methods. As a consequence, researchers are increasingly turning to machine learning (ML) approaches to overcome these obstacles. In this paper, we comprehensively analyze ML-based NTC studies, developing a taxonomy of their design choices, benchmarking suites, and prevalent assumptions impacting classifier performance. Through this systematization, we demonstrate widespread reliance on outdated datasets, oversights in design choices, and the consequences of unsubstantiated assumptions. Our evaluation reveals that the majority of proposed encrypted traffic classifiers have mistakenly utilized unencrypted traffic due to the use of legacy datasets. Furthermore, by conducting 348 feature occlusion experiments on state-of-the-art classifiers, we show how oversights in NTC design choices lead to overfitting, and validate or refute prevailing assumptions with empirical evidence. By highlighting lessons learned, we offer strategic insights, identify emerging research directions, and recommend best practices to support the development of real-world applicable NTC methodologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.20093v3</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nimesha Wickramasinghe, Arash Shaghaghi, Gene Tsudik, Sanjay Jha</dc:creator>
    </item>
    <item>
      <title>MUFFLER: Secure Tor Traffic Obfuscation with Dynamic Connection Shuffling and Splitting</title>
      <link>https://arxiv.org/abs/2504.07543</link>
      <description>arXiv:2504.07543v2 Announce Type: replace-cross 
Abstract: Tor, a widely utilized privacy network, enables anonymous communication but is vulnerable to flow correlation attacks that deanonymize users by correlating traffic patterns from Tor's ingress and egress segments. Various defenses have been developed to mitigate these attacks; however, they have two critical limitations: (i) significant network overhead during obfuscation and (ii) a lack of dynamic obfuscation for egress segments, exposing traffic patterns to adversaries. In response, we introduce MUFFLER, a novel connection-level traffic obfuscation system designed to secure Tor egress traffic. It dynamically maps real connections to a distinct set of virtual connections between the final Tor nodes and targeted services, either public or hidden. This approach creates egress traffic patterns fundamentally different from those at ingress segments without adding intentional padding bytes or timing delays. The mapping of real and virtual connections is adjusted in real-time based on ongoing network conditions, thwarting adversaries' efforts to detect egress traffic patterns. Extensive evaluations show that MUFFLER mitigates powerful correlation attacks with a TPR of 1% at an FPR of 10^-2 while imposing only a 2.17% bandwidth overhead. Moreover, it achieves up to 27x lower latency overhead than existing solutions and seamlessly integrates with the current Tor architecture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.07543v2</guid>
      <category>cs.CR</category>
      <category>cs.NI</category>
      <pubDate>Tue, 15 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Minjae Seo, Myoungsung You, Jaehan Kim, Taejune Park, Seungwon Shin, Jinwoo Kim</dc:creator>
    </item>
  </channel>
</rss>
