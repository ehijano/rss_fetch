<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.TH</link>
    <description>econ.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 22 Aug 2025 01:31:56 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>The invisible hand as an emergent property: a gradient flow approach</title>
      <link>https://arxiv.org/abs/2508.14498</link>
      <description>arXiv:2508.14498v1 Announce Type: new 
Abstract: We develop a general equilibrium model in which, at each instant, a short-run competitive equilibrium arises. Heterogeneity in factor allocation generates differential profit rates across sectors, prompting firms to move between them under myopic profit-seeking behaviour, subject to quadratic reallocation costs. The aggregate dynamics of the economy can be formalised as a gradient flow in a Wasserstein space, starting from a partial differential equation that describes the reallocation of firms across sectors. Two key emergent properties arise: (i) decentralised and uncoordinated decisions can be reinterpreted as the solution to a sequence of global optimisation problems, involving a function of aggregate consumption, which increases monotonically along the dynamic path; (ii) the long-run competitive equilibrium is efficient, as the distribution of firms maximises aggregate consumption and profit rates are equalised across sectors. We extend the baseline model to incorporate non-symmetric preferences, intrasectoral externalities, a fixed cost of reallocation, and labour immobility. These extensions reveal conditions under which the efficiency and uniqueness of the long-run equilibrium may fail, but also highlight the surprising result that the decentralised equilibrium can remain efficient even in the presence of externalities. Finally, using a large sample of EU firms from the period 2018-2023, we empirically document convergence in sectoral profit rates, but not in labour productivity, pointing to a certain degree of labour immobility. We also find evidence suggesting the absence of significant fixed costs of reallocation at the sectoral level, the presence of positive but limited intrasectoral externalities, and a moderate degree of substitutability among goods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14498v1</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Giorgio Fabbri, Davide Fiaschi, Cristiano Ricci</dc:creator>
    </item>
    <item>
      <title>Equal Treatment of Equals and Efficiency in Probabilistic Assignments</title>
      <link>https://arxiv.org/abs/2508.14522</link>
      <description>arXiv:2508.14522v1 Announce Type: new 
Abstract: This paper studies general multi-unit probabilistic assignment problems involving indivisible objects, with a particular focus on achieving the fundamental fairness notion known as equal treatment of equals (ETE) and ensuring various notions of efficiency. We extend the definition of ETE so that it accommodates a variety of constraints and applications. We analyze the ETE reassignment procedure, which transforms any assignment into one satisfying ETE, and examine its compatibility with three efficiency concepts: ex-post efficiency, ordinal efficiency, and rank-minimizing efficiency. We show that while the ETE reassignment of an ex-post efficient assignment remains ex-post efficient, it may fail to preserve ordinal efficiency in general settings. However, since the ETE reassignment of a rank-minimizing assignment preserves rank-minimizing efficiency, the existence of assignments satisfying both ETE and ordinal efficiency can be established. Furthermore, we propose a computationally efficient method for constructing assignments that satisfy both ETE and ordinal efficiency under general upper bound constraints, by combining the serial dictatorship rule with appropriately specified priority lists and an ETE reassignment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14522v1</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yasunori Okumura</dc:creator>
    </item>
    <item>
      <title>Algorithmic Collusion is Algorithm Orchestration</title>
      <link>https://arxiv.org/abs/2508.14766</link>
      <description>arXiv:2508.14766v1 Announce Type: new 
Abstract: This paper proposes a fresh `meta-game' perspective on the problem of algorithmic collusion in pricing games a la Bertrand. Economists have interpreted the fact that algorithms can learn to price collusively as tacit collusion. We argue instead that the co-parametrization of algorithms -- that we show is necessary to obtain algorithmic collusion -- requires algorithm designer(s) to engage in explicit collusion by algorithm orchestration. To highlight this, we model a meta-game of algorithm parametrization that is played by algorithm designers, and the relevant strategic analyses at that level reveal new equilibrium and collusion phenomena.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14766v1</guid>
      <category>econ.TH</category>
      <category>cs.GT</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cesare Carissimo, Fryderyk Falniowski, Siavash Rahimi, Heinrich Nax</dc:creator>
    </item>
    <item>
      <title>Explainable Information Design</title>
      <link>https://arxiv.org/abs/2508.14196</link>
      <description>arXiv:2508.14196v1 Announce Type: cross 
Abstract: The optimal signaling schemes in information design (Bayesian persuasion) problems often involve non-explainable randomization or disconnected partitions of state space, which are too intricate to be audited or communicated. We propose explainable information design in the context of information design with a continuous state space, restricting the information designer to use $K$-partitional signaling schemes defined by deterministic and monotone partitions of the state space, where a unique signal is sent for all states in each part. We first prove that the price of explainability (PoE) -- the ratio between the performances of the optimal explainable signaling scheme and unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning that partitional signaling schemes are never worse than arbitrary signaling schemes by a factor of 2.
  We then study the complexity of computing optimal explainable signaling schemes. We show that the exact optimization problem is NP-hard in general. But for Lipschitz utility functions, an $\varepsilon$-approximately optimal explainable signaling scheme can be computed in polynomial time. And for piecewise constant utility functions, we provide an efficient algorithm to find an explainable signaling scheme that provides a $1/2$ approximation to the optimal unrestricted signaling scheme, which matches the worst-case PoE bound.
  A technical tool we develop is a conversion from any optimal signaling scheme (which satisfies a bi-pooling property) to a partitional signaling scheme that achieves $1/2$ fraction of the expected utility of the former. We use this tool in the proofs of both our PoE result and algorithmic result.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.14196v1</guid>
      <category>cs.GT</category>
      <category>cs.DS</category>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiling Chen, Tao Lin, Wei Tang, Jamie Tucker-Foltz</dc:creator>
    </item>
    <item>
      <title>Unambiguous Efficiency of Random Allocations</title>
      <link>https://arxiv.org/abs/2401.11899</link>
      <description>arXiv:2401.11899v4 Announce Type: replace 
Abstract: When allocating indivisible objects via lottery, planners often use ordinal mechanisms, which elicit agents' rankings of objects rather than their full preferences over lotteries. In such an ordinal informational environment, planners cannot differentiate between utility profiles that induce the same ranking of objects. We propose the criterion of unambiguous efficiency: regardless of how each agent extends their preferences over objects to lotteries, the allocation is Pareto efficient with respect to the extended preferences. We compare this with the predominant efficiency criterion used for ordinal mechanisms. As an application to mechanism design, we characterize all efficient and strategy-proof mechanisms satisfying certain regularity conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11899v4</guid>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eun Jeong Heo, Vikram Manjunath, Samson Alva</dc:creator>
    </item>
    <item>
      <title>Nash Convergence of Mean-Based Learning Algorithms in First-Price Auctions</title>
      <link>https://arxiv.org/abs/2110.03906</link>
      <description>arXiv:2110.03906v5 Announce Type: replace-cross 
Abstract: The convergence properties of learning dynamics in repeated auctions is a timely and important question, with numerous applications in, e.g., online advertising markets. This work focuses on repeated first-price auctions where bidders with fixed values learn to bid using mean-based algorithms -- a large class of online learning algorithms that include popular no-regret algorithms such as Multiplicative Weights Update and Follow the Perturbed Leader. We completely characterize the learning dynamics of mean-based algorithms, under two notions of convergence: (1) time-average: the fraction of rounds where bidders play a Nash equilibrium converges to 1; (2) last-iterate: the mixed strategy profile of bidders converges to a Nash equilibrium. Specifically, the results depend on the number of bidders with the highest value:
  - If the number is at least three, the dynamics almost surely converges to a Nash equilibrium of the auction, in both time-average and last-iterate.
  - If the number is two, the dynamics almost surely converges to a Nash equilibrium in time-average but not necessarily last-iterate.
  - If the number is one, the dynamics may not converge to a Nash equilibrium in time-average or last-iterate.
  Our discovery opens up new possibilities in the study of the convergence of learning dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.03906v5</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3485447.3512059</arxiv:DOI>
      <dc:creator>Xiaotie Deng, Xinyan Hu, Tao Lin, Weiqiang Zheng</dc:creator>
    </item>
    <item>
      <title>Dominated Actions in Imperfect-Information Games</title>
      <link>https://arxiv.org/abs/2504.09716</link>
      <description>arXiv:2504.09716v3 Announce Type: replace-cross 
Abstract: Dominance is a fundamental concept in game theory. In strategic-form games dominated strategies can be identified in polynomial time. As a consequence, iterative removal of dominated strategies can be performed efficiently as a preprocessing step for reducing the size of a game before computing a Nash equilibrium. For imperfect-information games in extensive form, we could convert the game to strategic form and then iteratively remove dominated strategies in the same way; however, this conversion may cause an exponential blowup in game size. In this paper we define and study the concept of dominated actions in imperfect-information games. Our main result is a polynomial-time algorithm for determining whether an action is dominated (strictly or weakly) by any mixed strategy in n-player games, which can be extended to an algorithm for iteratively removing dominated actions. This allows us to efficiently reduce the size of the game tree as a preprocessing step for Nash equilibrium computation. We explore the role of dominated actions empirically in the "All In or Fold" No-Limit Texas Hold'em poker variant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09716v3</guid>
      <category>cs.GT</category>
      <category>cs.AI</category>
      <category>cs.MA</category>
      <category>econ.TH</category>
      <pubDate>Thu, 21 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sam Ganzfried</dc:creator>
    </item>
  </channel>
</rss>
