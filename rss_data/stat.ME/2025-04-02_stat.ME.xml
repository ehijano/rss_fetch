<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 02 Apr 2025 07:08:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>A multi-locus predictiveness curve and its summary assessment for genetic risk prediction</title>
      <link>https://arxiv.org/abs/2504.00024</link>
      <description>arXiv:2504.00024v1 Announce Type: new 
Abstract: With the advance of high-throughput genotyping and sequencing technologies, it becomes feasible to comprehensive evaluate the role of massive genetic predictors in disease prediction. There exists, therefore, a critical need for developing appropriate statistical measurements to access the combined effects of these genetic variants in disease prediction. Predictiveness curve is commonly used as a graphical tool to measure the predictive ability of a risk prediction model on a single continuous biomarker. Yet, for most complex diseases, risk prediciton models are formed on multiple genetic variants. We therefore propose a multi-marker predictiveness curve and provide a non-parametric method to construct the curve for case-control studies. We further introduce a global predictiveness U and a partial predictiveness U to summarize prediction curve across the whole population and sub-population of clinical interest, respectively. We also demonstrate the connections of predictiveness curve with ROC curve and Lorenz curve. Through simulation, we compared the performance of the predictiveness U to other three summary indices: R square, Total Gain, and Average Entropy, and showed that Predictiveness U outperformed the other three indexes in terms of unbiasedness and robustness. Moreover, we simulated a series of rare-variants disease model, found partial predictiveness U performed better than global predictiveness U. Finally, we conducted a real data analysis, using predictiveness curve and predictiveness U to evaluate a risk prediction model for Nicotine Dependence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00024v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1177/0962280218819202</arxiv:DOI>
      <dc:creator>Changshuai Wei, Ming Li, Yalu Wen, Chengyin Ye, Qing Lu</dc:creator>
    </item>
    <item>
      <title>Scalable Durational Event Models: Application to Physical and Digital Interactions</title>
      <link>https://arxiv.org/abs/2504.00049</link>
      <description>arXiv:2504.00049v1 Announce Type: new 
Abstract: Durable interactions are ubiquitous in social network analysis and are increasingly observed with precise time stamps. Phone and video calls, for example, are events to which a specific duration can be assigned. We refer to this type of data encoding the start and end times of interactions as "durational event data". Recent advances in data collection have enabled the observation of such data over extended periods of time and between large populations of actors. Building on Relational Event Models, we propose the "Durational Event Model" as a framework for studying durational events by separately modeling event incidence and duration. To accommodate large-scale applications, we introduce a fast, memory-efficient, and exact block-coordinate ascent algorithm. Theoretical and numerical results demonstrate several advantages of this approach over traditional Newton-Raphson-based methods. We apply the model to physical and digital interactions among college students in Copenhagen. Our empirical findings reveal that past interactions are the main drivers of physical interactions, whereas digital interactions are more strongly influenced by friendship ties and prior dyadic contact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00049v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Cornelius Fritz, Riccardo Rastelli, Michael Fop, Alberto Caimo</dc:creator>
    </item>
    <item>
      <title>Bayesian nonparametric copulas with tail dependence</title>
      <link>https://arxiv.org/abs/2504.00138</link>
      <description>arXiv:2504.00138v1 Announce Type: new 
Abstract: We introduce a novel bivariate copula model able to capture both the central and tail dependence of the joint probability distribution. Model that can capture the dependence structure within the joint tail have important implications in many application areas where the focus is risk management (e.g. macroeconomics and finance). We use a Bayesian nonparametric approach to introduce a random copula based on infinite partitions of unity. We define a hierarchical prior over an infinite partition of the unit hypercube which has a stick breaking representation leading to an infinite mixture of products of independent beta densities. Capitalising on the stick breaking representation we introduce a Gibbs sample to proceed to inference. For our empirical analysis we consider both simulated and real data (insurance claims and portfolio returns). We compare both our model's ability to capture tail dependence and its out of sample predictive performance to competitive models (e.g. Joe and Clayton copulas) and show that in both simulated and real examples our model outperforms the competitive models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00138v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Maria Concepci\'on Aus\'in, Maria Kalli</dc:creator>
    </item>
    <item>
      <title>Domain Adaptation Under MNAR Missingness</title>
      <link>https://arxiv.org/abs/2504.00322</link>
      <description>arXiv:2504.00322v1 Announce Type: new 
Abstract: Current domain adaptation methods under missingness shift are restricted to Missing At Random (MAR) missingness mechanisms. However, in many real-world examples, the MAR assumption may be too restrictive. When covariates are Missing Not At Random (MNAR) in both source and target data, the common covariate shift solutions, including importance weighting, are not directly applicable. We show that under reasonable assumptions, the problem of MNAR missingness shift can be reduced to an imputation problem. This allows us to leverage recent methodological developments in both the traditional statistics and machine/deep-learning literature for MNAR imputation to develop a novel domain adaptation procedure for MNAR missingness shift. We further show that our proposed procedure can be extended to handle simultaneous MNAR missingness and covariate shifts. We apply our procedure to Electronic Health Record (EHR) data from two hospitals in south and northeast regions of the US. In this setting we expect different hospital networks and regions to serve different populations and to have different procedures, practices, and software for inputting and recording data, causing simultaneous missingness and covariate shifts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00322v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Tyrel Stokes, Hyungrok Do, Saul Blecker, Rumi Chunara, Samrachana Adhikari</dc:creator>
    </item>
    <item>
      <title>Graphical Models and Efficient Inference Methods for Multivariate Phase Probability Distributions</title>
      <link>https://arxiv.org/abs/2504.00459</link>
      <description>arXiv:2504.00459v1 Announce Type: new 
Abstract: Multivariate phase relationships are important to characterize and understand numerous physical, biological, and chemical systems, from electromagnetic waves to neural oscillations. These systems exhibit complex spatiotemporal dynamics and intricate interdependencies among their constituent elements. While classical models of multivariate phase relationships, such as the wave equation and Kuramoto model, give theoretical models to describe phenomena, the development of statistical tools for hypothesis testing and inference for multivariate phase relationships in complex systems remains limited. This paper introduces a novel probabilistic modeling framework to characterize multivariate phase relationships, with wave-like phenomena serving as a key example. This approach describes spatial patterns and interactions between oscillators through a pairwise exponential family distribution. Building upon the literature of graphical model inference, including methods like Ising models, graphical lasso, and interaction screening, this work bridges the gap between classical wave dynamics and modern statistical approaches. Efficient inference methods are introduced, leveraging the Chow-Liu algorithm for directed tree approximations and interaction screening for general graphical models. Simulated experiments demonstrate the utility of these methods for uncovering wave properties and sparse interaction structures, highlighting their applicability to diverse scientific domains. This framework establishes a new paradigm for statistical modeling of multivariate phase relationships, providing a powerful toolset for exploring the complexity of these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00459v1</guid>
      <category>stat.ME</category>
      <category>cs.IT</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew S. Perley, Todd P. Coleman</dc:creator>
    </item>
    <item>
      <title>Multi-stage Group Testing with (r,s)-regular design Algorithms</title>
      <link>https://arxiv.org/abs/2504.00611</link>
      <description>arXiv:2504.00611v1 Announce Type: new 
Abstract: In industrial engineering and manufacturing, quality control is an essential part of the production process of a product. To ensure proper functionality of a manufactured good, rigorous testing has to be performed to identify defective products before shipment to the customer. However, testing products individually in a sequential manner is often tedious, cumbersome and not widely applicable given that time, resources and personnel are limited. Thus, statistical methods have been employed to investigate random samples of products from batches. For instance, group testing has emerged as an alternative to reliably test manufactured goods by evaluating joint test results. Despite the clear advantages, existing group testing methods often struggle with efficiency and practicality in real-world industry settings, where minimizing the average number of tests and overall testing duration is critical. In this paper, novel multistage (r,s)-regular design algorithms in the framework of group testing for the identification of defective products are investigated. Motivated by the application in quality control in manufacturing, unifying expressions for the expected number of tests and expected duration are derived. The results show that the novel group testing algorithms outperform established algorithms for low probabilities of defectiveness and get close to the optimal counting bound while maintaining a low level of complexity. Mathematical proofs are supported by rigorous simulation studies and an evaluation of the performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00611v1</guid>
      <category>stat.ME</category>
      <category>eess.SP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Balzer</dc:creator>
    </item>
    <item>
      <title>Efficient computation of high-dimensional penalized piecewise constant hazard random effects models</title>
      <link>https://arxiv.org/abs/2504.00755</link>
      <description>arXiv:2504.00755v1 Announce Type: new 
Abstract: Identifying and characterizing relationships between treatments, exposures, or other covariates and time-to-event outcomes has great significance in a wide range of biomedical settings. In research areas such as multi-center clinical trials, recurrent events, and genetic studies, proportional hazard mixed effects models (PHMMs) are used to account for correlations observed in clusters within the data. In high dimensions, proper specification of the fixed and random effects within PHMMs is difficult and computationally complex. In this paper, we approximate the proportional hazards mixed effects model with a piecewise constant hazard mixed effects survival model. We estimate the model parameters using a modified Monte Carlo Expectation Conditional Minimization algorithm, allowing us to perform variable selection on both the fixed and random effects simultaneously. We also incorporate a factor model decomposition of the random effects in order to more easily scale the variable selection method to larger dimensions. We demonstrate the utility of our method using simulations, and we apply our method to a multi-study pancreatic ductal adenocarcinoma gene expression dataset to select features important for survival.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00755v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1002/sim.10311</arxiv:DOI>
      <arxiv:journal_reference>Statistics in Medicine 2025</arxiv:journal_reference>
      <dc:creator>Hillary M. Heiling, Naim U. Rashid, Quefeng Li, Xianlu L. Peng, Jen Jen Yeh</dc:creator>
    </item>
    <item>
      <title>Quantile Treatment Effects in High Dimensional Panel Data</title>
      <link>https://arxiv.org/abs/2504.00785</link>
      <description>arXiv:2504.00785v1 Announce Type: new 
Abstract: We introduce a novel estimator for quantile causal effects with high-dimensional panel data (large $N$ and $T$), where only one or a few units are affected by the intervention or policy. Our method extends the generalized synthetic control method (Xu 2017) from average treatment effect on the treated to quantile treatment effect on the treated, allowing the underlying factor structure to change across the quantile of the interested outcome distribution. Our method involves estimating the quantile-dependent factors using the control group, followed by a quantile regression to estimate the quantile treatment effect using the treated units. We establish the asymptotic properties of our estimator and propose a bootstrap procedure for statistical inference, supported by simulation studies. An empirical application of the 2008 China Stimulus Program is provided.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00785v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Xu, Li Zheng</dc:creator>
    </item>
    <item>
      <title>Sequential Design with Posterior and Posterior Predictive Probabilities</title>
      <link>https://arxiv.org/abs/2504.00856</link>
      <description>arXiv:2504.00856v1 Announce Type: new 
Abstract: Sequential designs drive innovation in clinical, industrial, and corporate settings. Early stopping for failure in sequential designs conserves experimental resources, whereas early stopping for success accelerates access to improved interventions. Bayesian decision procedures provide a formal and intuitive framework for early stopping using posterior and posterior predictive probabilities. Design parameters including decision thresholds and sample sizes are chosen to control the error rates associated with the sequential decision process. These choices are routinely made based on estimating the sampling distribution of posterior summaries via intensive Monte Carlo simulation for each sample size and design scenario considered. In this paper, we propose an efficient method to assess error rates and determine optimal sample sizes and decision thresholds for Bayesian sequential designs. We prove theoretical results that enable posterior and posterior predictive probabilities to be modeled as a function of the sample size. Using these functions, we assess error rates at a range of sample sizes given simulations conducted at only two sample sizes. The effectiveness of our methodology is highlighted using two substantive examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00856v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Shirin Golchi, Marina B. Klein</dc:creator>
    </item>
    <item>
      <title>The role of post intercurrent event data in the estimation of hypothetical estimands in clinical trials</title>
      <link>https://arxiv.org/abs/2504.00929</link>
      <description>arXiv:2504.00929v1 Announce Type: new 
Abstract: Estimation of hypothetical estimands in clinical trials typically does not make use of data that may be collected after the intercurrent event (ICE). Some recent papers have shown that such data can be used for estimation of hypothetical estimands, and that statistical efficiency and power can be increased compared to using estimators that only use data before the ICE. In this paper we critically examine the efficiency and bias of estimators that do and do not exploit data collected after ICEs, in a simplified setting. We find that efficiency can only be improved by assuming certain covariate effects are common between patients who do and do not experience ICEs, and that even when such an assumption holds, gains in efficiency will typically be modest. We moreover argue that the assumptions needed to gain efficiency by using post-ICE outcomes will often not hold, such that estimators using post-ICE data may lead to biased estimates and invalid inferences. As such, we recommend that in general estimation of hypothetical estimands should be based on estimators that do not make use of post-ICE data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00929v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jonathan W. Bartlett, Rhian M. Daniel</dc:creator>
    </item>
    <item>
      <title>pumBayes: Bayesian Estimation of Probit Unfolding Models for Binary Preference Data in R</title>
      <link>https://arxiv.org/abs/2504.00423</link>
      <description>arXiv:2504.00423v1 Announce Type: cross 
Abstract: Probit unfolding models (PUMs) are a novel class of scaling models that allow for items with both monotonic and non-monotonic response functions and have shown great promise in the estimation of preferences from voting data in various deliberative bodies. This paper presents the R package pumBayes, which enables Bayesian inference for both static and dynamic PUMs using Markov chain Monte Carlo algorithms that require minimal or no tuning. In addition to functions that carry out the sampling from the posterior distribution of the models, the package also includes various support functions that can be used to pre-process data, select hyperparameters, summarize output, and compute metrics of model fit. We demonstrate the use of the package through an analysis of two datasets, one corresponding to roll-call voting data from the 116th U.S. House of Representatives, and a second one corresponding to voting records in the U.S. Supreme Court between 1937 and 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.00423v1</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Skylar Shi (University of Washington), Abel Rodriguez (University of Washington), Rayleigh Lei (University of Michigan)</dc:creator>
    </item>
    <item>
      <title>Variable selection in balance regression with applications to microbiome compositional data</title>
      <link>https://arxiv.org/abs/2304.00143</link>
      <description>arXiv:2304.00143v2 Announce Type: replace 
Abstract: Compositional data, where only relative abundances are available, are common in microbiome and other high-throughput sequencing studies. Log ratios between groups of variables serve as key biomarkers in these settings. However, selecting predictive log ratios is a combinatorial challenge, and existing greedy search-based methods are computationally expensive, limiting their applicability to high-dimensional data. We introduce the supervised log ratio (SLR) method, a novel and efficient approach for selecting predictive log ratios in high-dimensional settings. SLR first screens active variables using univariate regression on log ratio transformed data and then applies principal balance analysis to define balance biomarkers. Our approach leverages both the relationship between the response and predictors and the correlations among the predictors to improve accuracy in variable selection and prediction. Through simulations and two case studies -- one on inflammatory bowel disease (IBD) and another on colorectal cancer (CRC) -- we demonstrate that SLR outperforms existing methods, particularly in high-dimensional settings. SLR is implemented in an R package, publicly available on GitHub.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.00143v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Ma, Paizhe Xie, Kristyn Pantoja, David E. Jones</dc:creator>
    </item>
    <item>
      <title>Individualized Policy Evaluation and Learning under Clustered Network Interference</title>
      <link>https://arxiv.org/abs/2311.02467</link>
      <description>arXiv:2311.02467v3 Announce Type: replace 
Abstract: Although there is now a large literature on policy evaluation and learning, much of the prior work assumes that the treatment assignment of one unit does not affect the outcome of another unit. Unfortunately, ignoring interference can lead to biased policy evaluation and ineffective learned policies. For example, treating influential individuals who have many friends can generate positive spillover effects, thereby improving the overall performance of an individualized treatment rule (ITR). We consider the problem of evaluating and learning an optimal ITR under clustered network interference (also known as partial interference), where clusters of units are sampled from a population and units may influence one another within each cluster. Unlike previous methods that impose strong restrictions on spillover effects, such as anonymous interference, the proposed methodology only assumes a semiparametric structural model, where each unit's outcome is an additive function of individual treatments within the cluster. Under this model, we propose an estimator that can be used to evaluate the empirical performance of an ITR. We show that this estimator is substantially more efficient than the standard inverse probability weighting estimator, which does not impose any assumption about spillover effects. We derive the finite-sample regret bound for a learned ITR, showing that the use of our efficient evaluation estimator leads to the improved performance of learned policies. We consider both experimental and observational studies, and for the latter, we develop a doubly robust estimator that is semiparametrically efficient and yields an optimal regret bound. Finally, we conduct simulation and empirical studies to illustrate the advantages of the proposed methodology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02467v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yi Zhang, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>Forecasting intraday foreign exchange volatility with functional GARCH approaches</title>
      <link>https://arxiv.org/abs/2311.18477</link>
      <description>arXiv:2311.18477v2 Announce Type: replace 
Abstract: This paper seeks to analyse and predict conditional intraday volatility curves in FX markets using functional Generalised AutoRegressive Conditional Heteroscedasticity (GARCH) models. Remarkably, taking account of cross-dependency dynamics between the major currencies significantly improves intraday conditional volatility forecasting. Additionally, incorporating intraday bid-ask spread using a functional GARCH-X model further enhances predictability. The precise volatility forecasts motivate the construction of intraday Value-at-Risk (VaR). An intraday risk management application highlights that predicted intraday VaR curves can help mitigate dramatic losses in intraday trading strategies, showcasing their practical economic benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.18477v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Fearghal Kearney, Han Lin Shang, Yuqian Zhao</dc:creator>
    </item>
    <item>
      <title>A graphical framework for interpretable correlation matrix models</title>
      <link>https://arxiv.org/abs/2312.06289</link>
      <description>arXiv:2312.06289v3 Announce Type: replace 
Abstract: In this work, we present a new approach for constructing models for correlation matrices with a user-defined graphical structure. The graphical structure makes correlation matrices interpretable and avoids the quadratic increase of parameters as a function of the dimension. We suggest an automatic approach to define a prior using a natural sequence of simpler models within the Penalized Complexity framework for the unknown parameters in these models. We illustrate this approach with three applications: a multivariate linear regression of four biomarkers, a multivariate disease mapping, and a multivariate longitudinal joint modelling. Each application underscores our method's intuitive appeal, signifying a substantial advancement toward a more cohesive and enlightening model that facilitates a meaningful interpretation of correlation matrices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06289v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Freni Sterrantino, Denis Rustand, Janet van Niekerk, Elias Teixeira Krainski, H{\aa}vard Rue</dc:creator>
    </item>
    <item>
      <title>Multinomial Link Models</title>
      <link>https://arxiv.org/abs/2312.16260</link>
      <description>arXiv:2312.16260v4 Announce Type: replace 
Abstract: We propose a new family of regression models for analyzing categorical responses, called multinomial link models. It consists of four classes, namely, mixed-link models that generalize existing multinomial logistic models and their extensions, two-group models that can incorporate the observations with NA or unknown responses, multinomial conditional link models that handle longitudinal categorical responses, and po-npo mixture models that are more flexible than partial proportional odds models. By characterizing the feasible parameter space, deriving necessary and sufficient conditions, and developing validated algorithms to guarantee the finding of feasible maximum likelihood estimates, we solve the infeasibility issue of existing statistical software when estimating parameters for cumulative link models. We also provide explicit formulae and detailed algorithms for computing the Fisher information matrix and selecting the best models among the new family. The applications to real datasets show that the new models can fit the data significantly better, correct misleading conclusions due to missing responses, and make more informative statistical inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.16260v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tianmeng Wang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>The Role of Mean Absolute Deviation Function in Obtaining Smooth Estimation for Distribution and Density Functions: Beta Regression Approach</title>
      <link>https://arxiv.org/abs/2403.16544</link>
      <description>arXiv:2403.16544v3 Announce Type: replace 
Abstract: Smooth Estimation of probability density and distribution functions from its sample is an attractive and an important problem that has applications in several fields such as, business, medicine, and environment. This article introduces a simple approach but novel for estimating both functions via beta regression and generalized additive model approaches. The approach explores estimation of both functions by smoothing the first derivative of left mean absolute deviation function to obtain the final optimal smooth estimates under the condition of nondecreasing distribution function, and the density function remains nonnegative. This is achieved by using beta regression and generalized additive model with various link functions (logit, probit, cloglog, and cauchit) that applied to a polynomial function whose degree is determined by less mean absolute regression errors. Additionally, confidence limits for the distribution function are derived based on the beta distribution to give judgement about precision of obtained estimates. The method is utilized on simulated datasets featuring unimodal and multimodal and an actual dataset. The results suggest that this method exhibits strong performance relative to the kernel-based method, especially for its superior attributes in sample sizes and smoothness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.16544v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elsayed A. H. Elamir</dc:creator>
    </item>
    <item>
      <title>Optimal E-Values for Exponential Families: the Simple Case</title>
      <link>https://arxiv.org/abs/2404.19465</link>
      <description>arXiv:2404.19465v2 Announce Type: replace 
Abstract: We provide a general condition under which e-variables in the form of a simple-vs.-simple likelihood ratio exist when the null hypothesis is a composite, multivariate exponential family. Such `simple' e-variables are easy to compute and expected-log-optimal with respect to any stopping time. Simple e-variables were previously only known to exist in quite specific settings, but we offer a unifying theorem on their existence for testing exponential families. We start with a simple alternative $Q$ and a regular exponential family null. Together these induce a second exponential family ${\cal Q}$ containing $Q$, with the same sufficient statistic as the null. Our theorem shows that simple e-variables exist whenever the covariance matrices of ${\cal Q}$ and the null are in a certain relation. A prime example in which this relation holds is testing whether a parameter in a linear regression is 0. Other examples include some $k$-sample tests, Gaussian location- and scale tests, and tests for more general classes of natural exponential families. While in all these examples, the implicit composite alternative is also an exponential family, in general this is not required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19465v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Peter Gr\"unwald, Tyron Lardy, Yunda Hao, Shaul K. Bar-Lev, Martijn de Jong</dc:creator>
    </item>
    <item>
      <title>Dynamic Factor Analysis of High-dimensional Recurrent Events</title>
      <link>https://arxiv.org/abs/2405.19803</link>
      <description>arXiv:2405.19803v2 Announce Type: replace 
Abstract: Recurrent event time data arise in many studies, including biomedicine, public health, marketing, and social media analysis. High-dimensional recurrent event data involving many event types and observations have become prevalent with advances in information technology. This paper proposes a semiparametric dynamic factor model for the dimension reduction of high-dimensional recurrent event data. The proposed model imposes a low-dimensional structure on the mean intensity functions of the event types while allowing for dependencies. A nearly rate-optimal smoothing-based estimator is proposed. An information criterion that consistently selects the number of factors is also developed. Simulation studies demonstrate the effectiveness of these inference tools. The proposed method is applied to grocery shopping data, for which an interpretable factor structure is obtained.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.19803v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fangyi Chen, Yunxiao Chen, Zhiliang Ying, Kangjie Zhou</dc:creator>
    </item>
    <item>
      <title>NIRVAR: Network Informed Restricted Vector Autoregression</title>
      <link>https://arxiv.org/abs/2407.13314</link>
      <description>arXiv:2407.13314v3 Announce Type: replace 
Abstract: High-dimensional panels of time series often arise in finance and macroeconomics, where co-movements within groups of panel components occur. Extracting these groupings from the data provides a course-grained description of the complex system in question and can inform subsequent prediction tasks. We develop a novel methodology to model such a panel as a restricted vector autoregressive process, where the coefficient matrix is the weighted adjacency matrix of a stochastic block model. This network time series model, which we call the Network Informed Restricted Vector Autoregression (NIRVAR) model, yields a coefficient matrix that has a sparse block-diagonal structure. We propose an estimation procedure that embeds each panel component in a low-dimensional latent space and clusters the embedded points to recover the blocks of the coefficient matrix. Crucially, the method allows for network-based time series modelling when the underlying network is unobserved. We derive the bias, consistency and asymptotic normality of the NIRVAR estimator. Simulation studies suggest that the NIRVAR estimated embedded points are Gaussian distributed around the ground truth latent positions. On three applications to finance, macroeconomics, and transportation systems, NIRVAR outperforms competing models in terms of prediction and provides interpretable results regarding group recovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.13314v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Brendan Martin, Francesco Sanna Passino, Mihai Cucuringu, Alessandra Luati</dc:creator>
    </item>
    <item>
      <title>Forecasting Age Distribution of Deaths: Cumulative Distribution Function Transformation</title>
      <link>https://arxiv.org/abs/2409.04981</link>
      <description>arXiv:2409.04981v2 Announce Type: replace 
Abstract: Like density functions, period life-table death counts are nonnegative and have a constrained integral, and thus live in a constrained nonlinear space. Implementing established modelling and forecasting methods without obeying these constraints can be problematic for such nonlinear data. We introduce cumulative distribution function transformation to forecast the life-table death counts. Using the Japanese life-table death counts obtained from the Japanese Mortality Database (2024), we evaluate the point and interval forecast accuracies of the proposed approach, which compares favourably to an existing compositional data analytic approach. The improved forecast accuracy of life-table death counts is of great interest to demographers for estimating age-specific survival probabilities and life expectancy and actuaries for determining temporary annuity prices for different ages and maturities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.04981v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Han Lin Shang, Steven Haberman</dc:creator>
    </item>
    <item>
      <title>Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective</title>
      <link>https://arxiv.org/abs/2410.16608</link>
      <description>arXiv:2410.16608v2 Announce Type: replace 
Abstract: Visualizing high-dimensional data is essential for understanding biomedical data and deep learning models. Neighbor embedding methods, such as t-SNE and UMAP, are widely used but can introduce misleading visual artifacts. We find that the manifold learning interpretations from many prior works are inaccurate and that the misuse stems from a lack of data-independent notions of embedding maps, which project high-dimensional data into a lower-dimensional space. Leveraging the leave-one-out principle, we introduce LOO-map, a framework that extends embedding maps beyond discrete points to the entire input space. We identify two forms of map discontinuity that distort visualizations: one exaggerates cluster separation and the other creates spurious local structures. As a remedy, we develop two types of point-wise diagnostic scores to detect unreliable embedding points and improve hyperparameter selection, which are validated on datasets from computer vision and single-cell omics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16608v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhexuan Liu, Rong Ma, Yiqiao Zhong</dc:creator>
    </item>
    <item>
      <title>Topological Clustering of Agents in Hidden Information Contagions: Application to Financial Markets</title>
      <link>https://arxiv.org/abs/2410.21104</link>
      <description>arXiv:2410.21104v3 Announce Type: replace 
Abstract: Building on topological data analysis and expert knowledge, this study introduces a Mapper-based approach to cluster agents based on their tendency to be influenced by information spread. The context of our paper is financial markets with an aim to identify agents trading opportunistically on insider information while minimizing false positives, a critical challenge in financial market surveillance. We verify and demonstrate our methods using both synthetic and empirical data on insider networks and investor-level transactions in a stock market. Recognizing the sensitive nature of insider trading cases, we design a conservative approach to minimize false positives, ensuring that innocent agents are not wrongfully implicated. We find that the mapper-based method systematically outperforms other methods on synthetic data with ground truth. We also apply the method to empirical data and verify the results using a statistical validation method based on persistence homology. Our findings indicate that the proposed Mapper-based technique effectively identifies a subset of agents who tend to take advantage of inside information they have received. This method is highly adaptable to various applications involving the spread of information or diseases, where agents exhibit only indirect evidence of their carrier status (symptoms) through their behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.21104v3</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Anubha Goel, Henri Hansen, Juho Kanniainen</dc:creator>
    </item>
    <item>
      <title>Local False Sign Rate and the Role of Prior Covariance Rank in Multivariate Empirical Bayes Multiple Testing</title>
      <link>https://arxiv.org/abs/2502.16118</link>
      <description>arXiv:2502.16118v2 Announce Type: replace 
Abstract: This paper investigates the relationship between the rank of the prior covariance matrix and the local false sign rate (lfsr) in multivariate empirical Bayes multiple testing, specifically within the context of normal mean models. We demonstrate that using low-rank covariance matrices for the prior results in inflated false sign rates, a consequence of rank deficiency. To address this, we propose an adjustment that mitigates this inflation by employing full-rank covariance matrices. Through simulations, we validate the effectiveness of this adjustment in controlling false sign rates, thereby improving the robustness of empirical Bayes methods in high-dimensional settings. Our results show that the rank of the prior covariance matrix directly influences the accuracy of sign estimation and the performance of the lfsr, with significant implications for large-scale hypothesis testing in statistics and genomics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16118v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Dongyue Xie</dc:creator>
    </item>
    <item>
      <title>AI-Powered Bayesian Inference</title>
      <link>https://arxiv.org/abs/2502.19231</link>
      <description>arXiv:2502.19231v2 Announce Type: replace 
Abstract: The advent of Generative Artificial Intelligence (GAI) has heralded an inflection point that changed how society thinks about knowledge acquisition. While GAI cannot be fully trusted for decision-making, it may still provide valuable information that can be integrated into a decision pipeline. Rather than seeing the lack of certitude and inherent randomness of GAI as a problem, we view it as an opportunity. Indeed, variable answers to given prompts can be leveraged to construct a prior distribution which reflects assuredness of AI predictions. This prior distribution may be combined with tailored datasets for a fully Bayesian analysis with an AI-driven prior. In this paper, we explore such a possibility within a non-parametric Bayesian framework. The basic idea consists of assigning a Dirichlet process prior distribution on the data-generating distribution with AI generative model as its baseline. Hyper-parameters of the prior can be tuned out-of-sample to assess the informativeness of the AI prior. Posterior simulation is achieved by computing a suitably randomized functional on an augmented data that consists of observed (labeled) data as well as fake data whose labels have been imputed using AI. This strategy can be parallelized and rapidly produces iid samples from the posterior by optimization as opposed to sampling from conditionals. Our method enables (predictive) inference and uncertainty quantification leveraging AI predictions in a coherent probabilistic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19231v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>stat.ML</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Veronika Ro\v{c}kov\'a, Sean O'Hagan</dc:creator>
    </item>
    <item>
      <title>Safe Policy Learning through Extrapolation: Application to Pre-trial Risk Assessment</title>
      <link>https://arxiv.org/abs/2109.11679</link>
      <description>arXiv:2109.11679v4 Announce Type: replace-cross 
Abstract: Algorithmic recommendations and decisions have become ubiquitous in today's society. Many of these data-driven policies, especially in the realm of public policy, are based on known, deterministic rules to ensure their transparency and interpretability. We examine a particular case of algorithmic pre-trial risk assessments in the US criminal justice system, which provide deterministic classification scores and recommendations to help judges make release decisions. Our goal is to analyze data from a unique field experiment on an algorithmic pre-trial risk assessment to investigate whether the scores and recommendations can be improved. Unfortunately, prior methods for policy learning are not applicable because they require existing policies to be stochastic. We develop a maximin robust optimization approach that partially identifies the expected utility of a policy, and then finds a policy that maximizes the worst-case expected utility. The resulting policy has a statistical safety property, limiting the probability of producing a worse policy than the existing one, under structural assumptions about the outcomes. Our analysis of data from the field experiment shows that we can safely improve certain components of the risk assessment instrument by classifying arrestees as lower risk under a wide range of utility specifications, though the analysis is not informative about several components of the instrument.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.11679v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Eli Ben-Michael, D. James Greiner, Kosuke Imai, Zhichao Jiang</dc:creator>
    </item>
    <item>
      <title>Self and mutually exciting point process embedding flexible residuals and intensity with discretely Markovian dynamics</title>
      <link>https://arxiv.org/abs/2401.13890</link>
      <description>arXiv:2401.13890v2 Announce Type: replace-cross 
Abstract: This work introduces a self and mutually exciting point process that embeds flexible residuals and intensity with discretely Markovian dynamics. By allowing the integration of diverse residual distributions, this model serves as an extension of the Hawkes process, facilitating intensity modeling. This model's nature enables a filtered historical simulation that more accurately incorporates the properties of the original time series. Furthermore, the process extends to multivariate models with manageable estimation and simulation implementations. We investigate the impact of a flexible residual distribution on the estimation of high-frequency financial data, comparing it with the Hawkes process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13890v2</guid>
      <category>q-fin.ST</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s11009-025-10159-5</arxiv:DOI>
      <arxiv:journal_reference>Methodology and Computing in Applied Probability, 2025, 27, 31</arxiv:journal_reference>
      <dc:creator>Kyungsub Lee</dc:creator>
    </item>
    <item>
      <title>Low-Rank Thinning</title>
      <link>https://arxiv.org/abs/2502.12063</link>
      <description>arXiv:2502.12063v3 Announce Type: replace-cross 
Abstract: The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.12063v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annabelle Michael Carrell, Albert Gong, Abhishek Shetty, Raaz Dwivedi, Lester Mackey</dc:creator>
    </item>
    <item>
      <title>Calibration Strategies for Robust Causal Estimation: Theoretical and Empirical Insights on Propensity Score Based Estimators</title>
      <link>https://arxiv.org/abs/2503.17290</link>
      <description>arXiv:2503.17290v2 Announce Type: replace-cross 
Abstract: The partitioning of data for estimation and calibration critically impacts the performance of propensity score based estimators like inverse probability weighting (IPW) and double/debiased machine learning (DML) frameworks. We extend recent advances in calibration techniques for propensity score estimation, improving the robustness of propensity scores in challenging settings such as limited overlap, small sample sizes, or unbalanced data. Our contributions are twofold: First, we provide a theoretical analysis of the properties of calibrated estimators in the context of DML. To this end, we refine existing calibration frameworks for propensity score models, with a particular emphasis on the role of sample-splitting schemes in ensuring valid causal inference. Second, through extensive simulations, we show that calibration reduces variance of inverse-based propensity score estimators while also mitigating bias in IPW, even in small-sample regimes. Notably, calibration improves stability for flexible learners (e.g., gradient boosting) while preserving the doubly robust properties of DML. A key insight is that, even when methods perform well without calibration, incorporating a calibration step does not degrade performance, provided that an appropriate sample-splitting approach is chosen.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.17290v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Wed, 02 Apr 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Rabenseifner, Sven Klaassen, Jannis Kueck, Philipp Bach</dc:creator>
    </item>
  </channel>
</rss>
