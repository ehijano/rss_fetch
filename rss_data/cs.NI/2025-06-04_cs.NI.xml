<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.NI updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.NI</link>
    <description>cs.NI updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.NI" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Jun 2025 04:00:15 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Analyzing Localizability of LEO/MEO Hybrid Networks: A Stochastic Geometry Approach</title>
      <link>https://arxiv.org/abs/2506.03151</link>
      <description>arXiv:2506.03151v1 Announce Type: new 
Abstract: With the increase in global positioning service demands and the requirement for more precise positioning, assisting existing medium and high orbit satellite-enabled positioning systems with low Earth orbit (LEO) satellites has garnered widespread attention. However, providing low computational complexity performance analysis for hybrid LEO/MEO massive satellite constellations remains a challenge. In this article, we introduce for the first time the application of stochastic geometry (SG) framework in satellite-enabled positioning performance analysis and provide an analytical expression for the K-availiability probability and K-localizability probability under bidirectional beam alignment transmissions. The K-localizability probability, defined as the probability that at least K satellites can participate in the positioning process, serves as a prerequisite for positioning. Since the modeling of MEO satellite constellations within the SG framework has not yet been studied, we integrate the advantages of Cox point processes and binomial point processes, proposing a doubly stochastic binomial point process binomial point process for accurate modeling of MEO satellite constellations. Finally, we investigate the impact of constellation configurations and antenna patterns on the localizability performance of LEO, MEO, and hybrid MEO/LEO constellations. We also demonstrate the network performance gains brought to MEO positioning systems by incorporating assistance from LEO satellites.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03151v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Ruibo Wang, Mustafa A. Kishk, Howard H. Yang, Mohamed-Slim Alouini</dc:creator>
    </item>
    <item>
      <title>Video Quality Monitoring for Remote Autonomous Vehicle Control</title>
      <link>https://arxiv.org/abs/2506.03166</link>
      <description>arXiv:2506.03166v1 Announce Type: new 
Abstract: The delivery of high-quality, low-latency video streams is critical for remote autonomous vehicle control, where operators must intervene in real time. However, reliable video delivery over Fourth/Fifth-Generation (4G/5G) mobile networks is challenging due to signal variability, mobility-induced handovers, and transient congestion. In this paper, we present a comprehensive blueprint for an integrated video quality monitoring system, tailored to remote autonomous vehicle operation. Our proposed system includes subsystems for data collection onboard the vehicle, video capture and compression, data transmission to edge servers, real-time streaming data management, Artificial Intelligence (AI) model deployment and inference execution, and proactive decision-making based on predicted video quality. The AI models are trained on a hybrid dataset that combines field-trial measurements with synthetic stress segments and covers Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), and encoder-only Transformer architectures. As a proof of concept, we benchmark 20 variants from these model classes together with feed-forward Deep Neural Network (DNN) and linear-regression baselines, reporting accuracy and inference latency. Finally, we study the trade-offs between onboard and edge-based inference. We further discuss the use of explainable AI techniques to enhance transparency and accountability during critical remote-control interventions. Our proactive approach to network adaptation and Quality of Experience (QoE) monitoring aims to enhance remote vehicle operation over next-generation wireless networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03166v1</guid>
      <category>cs.NI</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dimitrios Kafetzis, Nikos Fotiou, Savvas Argyropoulos, Jad Nasreddine, Iordanis Koutsopoulos</dc:creator>
    </item>
    <item>
      <title>Distributionally Robust Wireless Semantic Communication with Large AI Models</title>
      <link>https://arxiv.org/abs/2506.03167</link>
      <description>arXiv:2506.03167v1 Announce Type: new 
Abstract: 6G wireless systems are expected to support massive volumes of data with ultra-low latency. However, conventional bit-level transmission strategies cannot support the efficiency and adaptability required by modern, data-intensive applications. The concept of semantic communication (SemCom) addresses this limitation by focusing on transmitting task-relevant semantic information instead of raw data. While recent efforts incorporating deep learning and large-scale AI models have improved SemCom's performance, existing systems remain vulnerable to both semantic-level and transmission-level noise because they often rely on domain-specific architectures that hinder generalizability. In this paper, a novel and generalized semantic communication framework called WaSeCom is proposed to systematically address uncertainty and enhance robustness. In particular, Wasserstein distributionally robust optimization is employed to provide resilience against semantic misinterpretation and channel perturbations. A rigorous theoretical analysis is performed to establish the robust generalization guarantees of the proposed framework. Experimental results on image and text transmission demonstrate that WaSeCom achieves improved robustness under noise and adversarial perturbations. These results highlight its effectiveness in preserving semantic fidelity across varying wireless conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03167v1</guid>
      <category>cs.NI</category>
      <category>cs.ET</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Long Tan Le, Senura Hansaja Wanasekara, Zerun Niu, Yansong Shi, Nguyen H. Tran, Phuong Vo, Walid Saad, Dusit Niyato, Zhu Han, Choong Seon Hong, H. Vincent Poor</dc:creator>
    </item>
    <item>
      <title>Graph Neural Networks for Jamming Source Localization</title>
      <link>https://arxiv.org/abs/2506.03196</link>
      <description>arXiv:2506.03196v1 Announce Type: new 
Abstract: Graph-based learning has emerged as a transformative approach for modeling complex relationships across diverse domains, yet its potential in wireless security remains largely unexplored. In this work, we introduce the first application of graph-based learning for jamming source localization, addressing the imminent threat of jamming attacks in wireless networks. Unlike geometric optimization techniques that struggle under environmental uncertainties and dense interference, we reformulate localization as an inductive graph regression task. Our approach integrates structured node representations that encode local and global signal aggregation, ensuring spatial coherence and adaptive signal fusion. To enhance robustness, we incorporate an attention-based graph neural network that adaptively refines neighborhood influence and introduces a confidence-guided estimation mechanism that dynamically balances learned predictions with domain-informed priors. We evaluate our approach under complex radio frequency environments with varying sampling densities and signal propagation conditions, conducting comprehensive ablation studies on graph construction, feature selection, and pooling strategies. Results demonstrate that our novel graph-based learning framework significantly outperforms established localization baselines, particularly in challenging scenarios with sparse and obfuscated signal information. Code is available at [https://github.com/daniaherzalla/gnn-jamming-source-localization](https://github.com/daniaherzalla/gnn-jamming-source-localization).</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03196v1</guid>
      <category>cs.NI</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dania Herzalla, Willian T. Lunardi, Martin Andreoni</dc:creator>
    </item>
    <item>
      <title>Self-Sustaining Multi-Sensor LoRa-Based Activity Monitoring for Community Workout Parks</title>
      <link>https://arxiv.org/abs/2506.03203</link>
      <description>arXiv:2506.03203v1 Announce Type: new 
Abstract: With the rise of the Internet of Things (IoT), more sensors are deployed around us, covering a wide range of applications from industry and agriculture to urban environments such as smart cities. Throughout these applications the sensors collect data of various characteristics and support city planners and decision-makers in their work processes, ultimately maximizing the impact of public funds. This paper introduces the design and implementation of a self-sustaining wireless sensor node designed to continuously monitor the utilization of community street workout parks. The proposed sensor node monitors activity by leveraging acceleration data capturing micro-vibrations that propagate through the steel structures of the workout equipment. This allows us to detect activity duration with an average measured error of only 2.8 seconds. The sensor is optimized with an energy-aware, adaptive sampling and transmission algorithm which, in combination with the Long Range Wide Area Network (LoRaWAN), reduces power consumption to just 1.147 mW in normal operation and as low as 0.712 mW in low-power, standby mode allowing 46 days of battery runtime. In addition, the integrated energy-harvesting circuit was tested in the field. By monitoring the battery voltage for multiple days, it was shown that the sensor is capable of operating sustainably year-round without external power sources. To evaluate the sensor effectiveness, we conducted a week-long field test in Zurich, placing sensors at various street workout parks throughout the city. Analysis of the collected data revealed clear patterns in park usage depending on day and location. This dataset is made publicly available through our online dashboard. Finally, we showcase the potential of IoT for city applications in combination with an accessible data interface for decision-makers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03203v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Victor Luder, Michele Magno</dc:creator>
    </item>
    <item>
      <title>HARNode: A Time-Synchronised, Open-Source, Multi-Device, Wearable System for Ad Hoc Field Studies</title>
      <link>https://arxiv.org/abs/2506.03219</link>
      <description>arXiv:2506.03219v1 Announce Type: new 
Abstract: Research into human activity recognition (HAR) suffers from a lack of comprehensive, freely available field data. Commercial systems are rarely open source, offer little expandability, and have shortcomings in node synchronisation, data throughput, placement clarity, setup complexity, and cost. As a result, only a few intuitively placed sensors are often used, and field trials are generally reduced. HARNode addresses these obstacles as a fully open-source hardware and software platform for rapid field applications. Each sensor node combines an ESP32-S3 module (AtomS3) with a display, a 9-axis IMU (Bosch BMX160), pressure/temperature sensors (Bosch BMP388) and an I2C port for extensions; the operating time is up to 8 hours. Data is streamed via Wi-Fi, while NTP-based time synchronisation provides an average clock accuracy of $\approx$ 1 ms. Manufacturing is carried out exclusively with commercially available components, an online PCB service - requiring little hardware knowledge - and a compact, 3D-printed housing with Velcro straps, allowing almost any number of highly synchronised nodes to be flexibly attached to the body. Its performance was demonstrated in a proof-of-concept study with ten test subjects, each wearing eleven HARNodes; the entire setup took less than five minutes per person. An example application goal was to detect the transition from level walking to climbing stairs. A random forest classifier evaluated the benefits of sensor overprovisioning: the best combination of seven nodes achieved $\approx$ 98\% accuracy (binary: level walking vs. approaching stairs), matching the result of all eleven positions. A single sensor on the foot achieved $\approx$ 90\% accuracy. These results demonstrate the suitability of HARNode as an ultra-fast ad hoc field system and support evidence-based sensor placement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03219v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philipp Lepold, Tobias R\"oddiger, Michael Beigl</dc:creator>
    </item>
    <item>
      <title>NetPress: Dynamically Generated LLM Benchmarks for Network Applications</title>
      <link>https://arxiv.org/abs/2506.03231</link>
      <description>arXiv:2506.03231v1 Announce Type: new 
Abstract: Despite growing interest in domain-specific benchmarking of large language models (LLMs) and agents, current evaluations remain limited to static, small-scale datasets, especially in high-stakes tasks like network operations that demand reliability for deployments. We present NetPress, an automated benchmark generation framework for evaluating LLM agents in network applications. NetPress introduces a unified abstraction with state and action, enabling dynamic generation of diverse query sets along with corresponding ground truths. At runtime, users can specify benchmark configurations to generate millions of queries on the fly. In addition to dynamic benchmark construction, NetPress integrates with network emulators to provide realistic environment feedback, supporting comprehensive evaluation across correctness, safety, and latency. We instantiate NetPress on three representative applications, revealing interesting fine-grained differences in agent behavior that static, correctness-only benchmarks often miss. NetPress moves LLM evaluation toward realistic, scalable testing in infrastructure-centric domains, helping close the gap between benchmark performance and real-world deployment readiness. Code is available at https://github.com/Froot-NetSys/NetPress.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03231v1</guid>
      <category>cs.NI</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yajie Zhou, Jiajun Ruan, Eric S. Wang, Sadjad Fouladi, Francis Y. Yan, Kevin Hsieh, Zaoxing Liu</dc:creator>
    </item>
    <item>
      <title>Relay Selection and User Equipment Admission in Resource-Efficient NextG Sidelink Communications</title>
      <link>https://arxiv.org/abs/2506.03328</link>
      <description>arXiv:2506.03328v1 Announce Type: new 
Abstract: 5G/6G sidelink communications addresses the challenge of connecting outer UEs, which are unable to directly access a base station (gNodeB), through inner UEs that act as relays to connect to the gNodeB. The key performance indicators include the achievable rates, the number of outer UEs that can connect to a gNodeB, and the latency experienced by outer UEs in establishing connections. We consider problem of determining the assignment of outer UEs to inner UEs based on the channel, interference, and traffic characteristics. We formulate an optimization problem to maximize a weighted sum rate of UEs, where weights can represent priority, waiting time, and queue length. This optimization accommodates constraints related to channel and interference characteristics that influence the rates at which links can successfully carry assigned traffic. While an exhaustive search can establish an upper bound on achievable rates by this non-convex optimization problem, it becomes impractical for larger number of outer UEs due to scalability issues related to high computational complexity. To address this, we present a greedy algorithm that incrementally selects links to maximize the sum rate, considering already activated links. This algorithm, although effective in achieving high sum rates, may inadvertently overlook some UEs, raising concerns about fairness. To mitigate this, we introduce a fairness-oriented algorithm that adjusts weights based on waiting time or queue length, ensuring that UEs with initially favorable conditions do not unduly disadvantage others over time. We show that this strategy not only improves the average admission ratio of UEs but also ensures a more equitable distribution of service among them, thereby providing a balanced and fair solution to sidelink communications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03328v1</guid>
      <category>cs.NI</category>
      <category>cs.DC</category>
      <category>cs.IT</category>
      <category>eess.SP</category>
      <category>math.IT</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yalin E. Sagduyu, Tugba Erpek, Sastry Kompella, Kemal Davaslioglu</dc:creator>
    </item>
    <item>
      <title>A Model-Data Dual-Driven Resource Allocation Scheme for IREE Oriented 6G Networks</title>
      <link>https://arxiv.org/abs/2506.03508</link>
      <description>arXiv:2506.03508v1 Announce Type: new 
Abstract: The rapid and substantial fluctuations in wireless network capacity and traffic demand, driven by the emergence of 6G technologies, have exacerbated the issue of traffic-capacity mismatch, raising concerns about wireless network energy consumption. To address this challenge, we propose a model-data dual-driven resource allocation (MDDRA) algorithm aimed at maximizing the integrated relative energy efficiency (IREE) metric under dynamic traffic conditions. Unlike conventional model-driven or data-driven schemes, the proposed MDDRA framework employs a model-driven Lyapunov queue to accumulate long-term historical mismatch information and a data-driven Graph Radial bAsis Fourier (GRAF) network to predict the traffic variations under incomplete data, and hence eliminates the reliance on high-precision models and complete spatial-temporal traffic data. We establish the universal approximation property of the proposed GRAF network and provide convergence and complexity analysis for the MDDRA algorithm. Numerical experiments validate the performance gains achieved through the data-driven and model-driven components. By analyzing IREE and EE curves under diverse traffic conditions, we recommend that network operators shall spend more efforts to balance the traffic demand and the network capacity distribution to ensure the network performance, particularly in scenarios with large speed limits and higher driving visibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03508v1</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tao Yu, Simin Wang, Shunqing Zhang, Xiaojing Chen, Zi Xu, Xin Wang, Jiandong Li, Junyu Liu, Sihai Zhang</dc:creator>
    </item>
    <item>
      <title>Channel-adaptive Cross-modal Generative Semantic Communication for Point Cloud Transmission</title>
      <link>https://arxiv.org/abs/2506.03211</link>
      <description>arXiv:2506.03211v1 Announce Type: cross 
Abstract: With the rapid development of autonomous driving and extended reality, efficient transmission of point clouds (PCs) has become increasingly important. In this context, we propose a novel channel-adaptive cross-modal generative semantic communication (SemCom) for PC transmission, called GenSeC-PC. GenSeC-PC employs a semantic encoder that fuses images and point clouds, where images serve as non-transmitted side information. Meanwhile, the decoder is built upon the backbone of PointDif. Such a cross-modal design not only ensures high compression efficiency but also delivers superior reconstruction performance compared to PointDif. Moreover, to ensure robust transmission and reduce system complexity, we design a streamlined and asymmetric channel-adaptive joint semantic-channel coding architecture, where only the encoder needs the feedback of average signal-to-noise ratio (SNR) and available bandwidth. In addition, rectified denoising diffusion implicit models is employed to accelerate the decoding process to the millisecond level, enabling real-time PC communication. Unlike existing methods, GenSeC-PC leverages generative priors to ensure reliable reconstruction even from noisy or incomplete source PCs. More importantly, it supports fully analog transmission, improving compression efficiency by eliminating the need for error-free side information transmission common in prior SemCom approaches. Simulation results confirm the effectiveness of cross-modal semantic extraction and dual-metric guided fine-tuning, highlighting the framework's robustness across diverse conditions, including low SNR, bandwidth limitations, varying numbers of 2D images, and previously unseen objects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.03211v1</guid>
      <category>cs.CV</category>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wanting Yang, Zehui Xiong, Qianqian Yang, Ping Zhang, Merouane Debbah, Rahim Tafazolli</dc:creator>
    </item>
    <item>
      <title>Carbon-Aware Temporal Data Transfer Scheduling Across Cloud Datacenters</title>
      <link>https://arxiv.org/abs/2506.04117</link>
      <description>arXiv:2506.04117v1 Announce Type: cross 
Abstract: Inter-datacenter communication is a significant part of cloud operations and produces a substantial amount of carbon emissions for cloud data centers, where the environmental impact has already been a pressing issue. In this paper, we present a novel carbon-aware temporal data transfer scheduling framework, called LinTS, which promises to significantly reduce the carbon emission of data transfers between cloud data centers. LinTS produces a competitive transfer schedule and makes scaling decisions, outperforming common heuristic algorithms. LinTS can lower carbon emissions during inter-datacenter transfers by up to 66% compared to the worst case and up to 15% compared to other solutions while preserving all deadline constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.04117v1</guid>
      <category>cs.DC</category>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Elvis Rodrigues, Jacob Goldverg, Tevfik Kosar</dc:creator>
    </item>
    <item>
      <title>Energy-Latency Aware Intelligent Reflecting Surface Aided Multi-cell Mobile Edge Computing</title>
      <link>https://arxiv.org/abs/2305.03556</link>
      <description>arXiv:2305.03556v2 Announce Type: replace 
Abstract: The explosive development of the Internet of Things (IoT) has led to increased interest in mobile edge computing (MEC), which provides computational resources at network edges to accommodate computation-intensive and latency-sensitive applications. Intelligent reflecting surfaces (IRSs) have gained attention as a solution to overcome blockage problems during the offloading uplink transmission in MEC systems. This paper explores IRS-aided multi-cell networks that enable servers to serve neighboring cells and cooperate to handle resource exhaustion. We aim to minimize the joint energy and latency cost, by jointly optimizing computation tasks, edge computing resources, user beamforming, and IRS phase shifts. The problem is decomposed into two subproblems--the MEC subproblem and the IRS communication subproblem--using the block coordinate descent (BCD) technique. The MEC subproblem is reformulated as a nonconvex quadratic constrained problem (QCP), while the IRS communication subproblem is transformed into a weight-sum-rate problem with auxiliary variables. We propose an efficient algorithm to iteratively optimize MEC resources and IRS communication until convergence. Numerical results show that our algorithm outperforms benchmarks and that multi-cell MEC systems achieve additional performance gains when supported by IRS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.03556v2</guid>
      <category>cs.NI</category>
      <pubDate>Thu, 05 Jun 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TGCN.2023.3330247</arxiv:DOI>
      <arxiv:journal_reference>IEEE Transactions on Green Communications and Networking, vol. 8, no. 1, pp. 362-374, March 2024</arxiv:journal_reference>
      <dc:creator>Wenhan Xu, Jiadong Yu, Yuan Wu, Danny H. K. Tsang</dc:creator>
    </item>
  </channel>
</rss>
