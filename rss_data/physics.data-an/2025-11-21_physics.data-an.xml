<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.data-an updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.data-an</link>
    <description>physics.data-an updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.data-an" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Nov 2025 05:07:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Human-aligned Quantification of Numerical Data</title>
      <link>https://arxiv.org/abs/2511.15723</link>
      <description>arXiv:2511.15723v1 Announce Type: new 
Abstract: Quantifying numerical data involves addressing two key challenges: first, determining whether the data can be naturally quantified, and second, identifying the numerical intervals or ranges of values that correspond to specific value classes, referred to as "quantums," which represent statistically meaningful states. If such quantification is feasible, continuous streams of numerical data can be transformed into sequences of "symbols" that reflect the states of the system described by the measured parameter. People often perform this task intuitively, relying on common sense or practical experience, while information theory and computer science offer computable metrics for this purpose. In this study, we assess the applicability of metrics based on information compression and the Silhouette coefficient for quantifying numerical data. We also investigate the extent to which these metrics correlate with one another and with what is commonly referred to as "human intuition." Our findings suggest that the ability to classify numeric data values into distinct categories is associated with a Silhouette coefficient above 0.65 and a Dip Test below 0.5; otherwise, the data can be treated as following a unimodal normal distribution. Furthermore, when quantification is possible, the Silhouette coefficient appears to align more closely with human intuition than the "normalized centroid distance" method derived from information compression perspective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15723v1</guid>
      <category>physics.data-an</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anton Kolonin</dc:creator>
    </item>
    <item>
      <title>The Ensemble Kalman Inversion Race</title>
      <link>https://arxiv.org/abs/2511.15853</link>
      <description>arXiv:2511.15853v1 Announce Type: new 
Abstract: Ensemble Kalman methods were initially developed to solve nonlinear data assimilation problems in oceanography, but are now popular in applications far beyond their original use cases. Of particular interest is climate model calibration. As hybrid physics and machine-learning models evolve, the number of parameters and complexity of parameterizations in climate models will continue to grow. Thus, robust calibration of these parameters plays an increasingly important role. We focus on learning climate model parameters from minimizing the misfit between modeled and observed climate statistics in an idealized setting. Ensemble Kalman methods are a natural choice for this problem because they are derivative-free, scalable to high dimensions, and robust to noise caused by statistical observations. Given the many variants of ensemble methods proposed, an important question is: Which ensemble Kalman method should be used for climate model calibration? To answer this question, we perform systematic numerical experiments to explore the relative computational efficiencies of several ensemble Kalman methods. The numerical experiments involve statistical observations of Lorenz-type models of increasing complexity, frequently used to represent simplified atmospheric systems, and some feature neural network parameterizations. For each test problem, several ensemble Kalman methods and a derivative-based method "race" to reach a specified accuracy, and we measure the computational cost required to achieve the desired accuracy. We investigate how prior information and the parameter or data dimensions play a role in choosing the ensemble method variant. The derivative-based method consistently fails to complete the race because it does not adaptively handle the noisy loss landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15853v1</guid>
      <category>physics.data-an</category>
      <category>stat.ML</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rebecca Gjini, Matthias Morzfeld, Oliver R. A. Dunbar, Tapio Schneider</dc:creator>
    </item>
    <item>
      <title>Identifying statistical indicators of temporal asymmetry using a data-driven approach</title>
      <link>https://arxiv.org/abs/2511.15991</link>
      <description>arXiv:2511.15991v1 Announce Type: new 
Abstract: The dynamics of time-reversible systems are statistically indistinguishable when observed forward or backward in time. A rich literature of statistical methods to distinguish irreversible dynamics from the reversible dynamics of linear, Gaussian systems can provide insights into underlying mechanisms and aid modeling and statistical quantification of time-series data. But these existing time-reversibility metrics have been developed individually, forming a fragmented body of research that makes it challenging to identify the most effective approaches developed to date, and the most promising new directions for development. Here we address these issues by systematically evaluating over 6000 time-series summary statistics, derived from across the time-series analysis literature, on their ability to distinguish the time-irreversibility of data simulated from a diverse range of 35 systems. Our large-scale data-driven comparison highlights the effectiveness of several key families of statistics, including time-asymmetric forms of generalized autocorrelation functions, time-series symbolic sequences, and forecasting-related methods. All irreversible systems studied here could be accurately distinguished by a well-chosen time-series statistic, but no single statistic could accurately index the statistical form of irreversibility for all irreversible systems. This challenges the assumption that a given time-reversibility statistic will accurately capture time reversibility in general, and underscores the importance of tailoring statistical approaches to the time-reversal characteristics of a given system. Our results provide a unified understanding of the key algorithmic structures through which irreversibility can be effectively quantified from data, providing a foundation for connecting patterns in time series to the underlying mechanisms of the systems that generate them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15991v1</guid>
      <category>physics.data-an</category>
      <category>nlin.CD</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Teresa Dalle Nogare, Ben D. Fulcher</dc:creator>
    </item>
    <item>
      <title>SURFing to the Fundamental Limit of Jet Tagging</title>
      <link>https://arxiv.org/abs/2511.15779</link>
      <description>arXiv:2511.15779v1 Announce Type: cross 
Abstract: Beyond the practical goal of improving search and measurement sensitivity through better jet tagging algorithms, there is a deeper question: what are their upper performance limits? Generative surrogate models with learned likelihood functions offer a new approach to this problem, provided the surrogate correctly captures the underlying data distribution. In this work, we introduce the SUrrogate ReFerence (SURF) method, a new approach to validating generative models. This framework enables exact Neyman-Pearson tests by training the target model on samples from another tractable surrogate, which is itself trained on real data. We argue that the EPiC-FM generative model is a valid surrogate reference for JetClass jets and apply SURF to show that modern jet taggers may already be operating close to the true statistical limit. By contrast, we find that autoregressive GPT models unphysically exaggerate top vs. QCD separation power encoded in the surrogate reference, implying that they are giving a misleading picture of the fundamental limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.15779v1</guid>
      <category>hep-ph</category>
      <category>cs.LG</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ian Pang, Darius A. Faroughy, David Shih, Ranit Das, Gregor Kasieczka</dc:creator>
    </item>
    <item>
      <title>Distinguishing thermal versus quantum annealing using probability-flux signatures across interaction networks</title>
      <link>https://arxiv.org/abs/2511.16457</link>
      <description>arXiv:2511.16457v1 Announce Type: cross 
Abstract: Simulated annealing provides a heuristic solution to combinatorial optimization problems. The cost function of a problem is mapped to the energy function of a physical many-body system, and, using thermal or quantum fluctuations, the system explores the state space to find the ground state, which may correspond to the optimal solution of the problem. Studies have highlighted both the similarities and differences between thermal and quantum fluctuations. Nevertheless, fundamental understanding of thermal and quantum annealing remains incomplete, making it difficult to design problem instances that fairly compare the two methods. Here, we investigate the many-body dynamics of thermal and quantum annealing by examining all possible interaction networks of $\pm J$ Ising spin systems up to seven spins. Our comprehensive investigation reveals that differences between thermal and quantum annealing emerge for particular interaction networks, indicating that the structure of the energy landscape distinguishes the two dynamics. We identify the microscopic origin of these differences through probability fluxes in state space, finding that the two dynamics are broadly similar but that quantum tunnelling produces qualitative differences. Our results provide insight into how thermal and quantum fluctuations navigate a system toward the ground state in simulated annealing, and are experimentally verifiable in atomic, molecular, and optical systems. Furthermore, these insights may improve mappings of optimization problems to Ising spin systems, yielding more accurate solutions in faster simulated annealing and thus benefiting real-world applications in industry. Our comprehensive survey of interaction networks and visualization of probability flux can help to understand, predict, and control quantum advantage in quantum annealing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.16457v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.quant-gas</category>
      <category>physics.data-an</category>
      <category>quant-ph</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yoshiaki Horiike, Yuki Kawaguchi</dc:creator>
    </item>
    <item>
      <title>Local wind speed forecasting at short time horizons based on Numerical Weather Prediction and observations from surrounding stations</title>
      <link>https://arxiv.org/abs/2503.18797</link>
      <description>arXiv:2503.18797v2 Announce Type: replace-cross 
Abstract: This study presents a hybrid neural network model for short-term (1-6 hours ahead) surface wind speed forecasting, combining Numerical Weather Prediction (NWP) with observational data from ground weather stations. It relies on the MeteoNet dataset, which includes data from global (ARPEGE) and regional (AROME) NWP models of the French weather service and meteorological observations from ground stations in the French Mediterranean. The proposed neural network architecture integrates recent past station observations (over last few hours) and AROME and ARPEGE predictions on a small subgrid around the target location. The model is designed to provide both deterministic and probabilistic forecasts, with the latter predicting the parameters of a suitable probability distribution that notably allows us to capture extreme wind events. Our results demonstrate that the hybrid model significantly outperforms baseline methods, including raw NWP predictions, persistence models, and linear regression, across all forecast horizons. For instance, the model reduces RMSE by up 30\% compared to AROME predictions. Probabilistic forecasting further enhances performance, particularly for extreme quantiles, by estimating conditional quantiles rather than relying solely on the conditional mean. Fine-tuning the model for specific stations, such as those in the Mediterranean island of Corsica, further improves forecasting accuracy. Our study highlights the importance of integrating multiple data sources and probabilistic approaches to improve short-term wind speed forecasting. It defines an effective approach, even in a complex terrain like Corsica where localized wind variations are significant</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.18797v2</guid>
      <category>physics.ao-ph</category>
      <category>physics.data-an</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roberta Baggio, Killian Pujol, Florian Pantillon, Dominique Lambert, Jean-Baptiste Filippi, Jean-Fran\c{c}ois Muzy</dc:creator>
    </item>
    <item>
      <title>On Focusing Statistical Power for Searches and Measurements in Particle Physics</title>
      <link>https://arxiv.org/abs/2507.17831</link>
      <description>arXiv:2507.17831v2 Announce Type: replace-cross 
Abstract: Particle physics experiments rely on the (generalised) likelihood ratio test (LRT) for searches and measurements, which consist of composite hypothesis tests. However, this test is not guaranteed to be optimal, as the Neyman-Pearson lemma pertains only to simple hypothesis tests. Any choice of test statistic thus implicitly determines how statistical power varies across the parameter space. An improvement in the core statistical testing methodology for general settings with composite tests would have widespread ramifications across experiments. We discuss an alternate test statistic that provides the data analyzer an ability to focus the power of the test on physics-motivated regions of the parameter space. We demonstrate the improvement from this technique compared to the LRT on a Higgs $\rightarrow\tau\tau$ dataset simulated by the ATLAS experiment and a dark matter dataset inspired by the LZ experiment. We also employ machine learning to efficiently perform the Neyman construction, which is essential to ensure statistically valid confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.17831v2</guid>
      <category>hep-ph</category>
      <category>hep-ex</category>
      <category>physics.data-an</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 21 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Carzon, Aishik Ghosh, Rafael Izbicki, Ann Lee, Luca Masserano, Daniel Whiteson</dc:creator>
    </item>
  </channel>
</rss>
