<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CC</link>
    <description>cs.CC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 Jan 2026 05:00:12 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Hidden costs for inference with deep network on embedded system devices</title>
      <link>https://arxiv.org/abs/2601.01698</link>
      <description>arXiv:2601.01698v1 Announce Type: new 
Abstract: This study evaluates the inference performance of various deep learning models under an embedded system environment. In previous works, Multiply-Accumulate operation is typically used to measure computational load of a deep model. According to this study, however, this metric has a limitation to estimate inference time on embedded devices. This paper poses the question of what aspects are overlooked when expressed in terms of Multiply-Accumulate operations. In experiments, an image classification task is performed on an embedded system device using the CIFAR-100 dataset to compare and analyze the inference times of ten deep models with the theoretically calculated Multiply-Accumulate operations for each model. The results highlight the importance of considering additional computations between tensors when optimizing deep learning models for real-time performing in embedded systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01698v1</guid>
      <category>cs.CC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chankyu Lee, Woohyun Choi, Sangwook Park</dc:creator>
    </item>
    <item>
      <title>Context-Free Recognition with Transformers</title>
      <link>https://arxiv.org/abs/2601.01754</link>
      <description>arXiv:2601.01754v1 Announce Type: cross 
Abstract: Transformers excel on tasks that process well-formed inputs according to some grammar, such as natural language and code. However, it remains unclear how they can process grammatical syntax. In fact, under standard complexity conjectures, standard transformers cannot recognize context-free languages (CFLs), a canonical formalism to describe syntax, or even regular languages, a subclass of CFLs (Merrill et al., 2022). Merrill &amp; Sabharwal (2024) show that $\mathcal{O}(\log n)$ looping layers (w.r.t. input length $n$) allows transformers to recognize regular languages, but the question of context-free recognition remained open. In this work, we show that looped transformers with $\mathcal{O}(\log n)$ looping layers and $\mathcal{O}(n^6)$ padding tokens can recognize all CFLs. However, training and inference with $\mathcal{O}(n^6)$ padding tokens is potentially impractical. Fortunately, we show that, for natural subclasses such as unambiguous CFLs, the recognition problem on transformers becomes more tractable, requiring $\mathcal{O}(n^3)$ padding. We empirically validate our results and show that looping helps on a language that provably requires logarithmic depth. Overall, our results shed light on the intricacy of CFL recognition by transformers: While general recognition may require an intractable amount of padding, natural constraints such as unambiguity yield efficient recognition algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.01754v1</guid>
      <category>cs.LG</category>
      <category>cs.CC</category>
      <category>cs.CL</category>
      <category>cs.FL</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Selim Jerad, Anej Svete, Sophie Hao, Ryan Cotterell, William Merrill</dc:creator>
    </item>
    <item>
      <title>Hardness of monadic second-order formulae over succinct graphs</title>
      <link>https://arxiv.org/abs/2302.04522</link>
      <description>arXiv:2302.04522v5 Announce Type: replace 
Abstract: Our main result is a succinct counterpoint to Courcelle's meta-theorem as follows: every cw-nontrivial monadic second-order (MSO) property is either NP-hard or coNP-hard over graphs given by succinct representations. Succint representations are Boolean circuits computing the adjacency relation. Cw-nontrivial properties are those which have infinitely many models and infinitely many countermodels with bounded cliquewidth. Moreover, we explore what happens when the cw-nontriviality condition is dropped and show that, under a reasonable complexity assumption, the previous dichotomy fails, even for questions expressible in first-order logic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.04522v5</guid>
      <category>cs.CC</category>
      <category>cs.LO</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guilhem Gamard, Ali\'enor Goubault-Larrecq, Pierre Guillon, Pierre Ohlmann, K\'evin Perrot, Guillaume Theyssier</dc:creator>
    </item>
    <item>
      <title>Counterfactual Explanations for Integer Optimization Problems</title>
      <link>https://arxiv.org/abs/2510.17624</link>
      <description>arXiv:2510.17624v2 Announce Type: replace-cross 
Abstract: Counterfactual explanations (CEs) offer a human-understandable way to explain decisions by identifying specific changes to the input parameters of a base or present model that would lead to a desired change in the outcome. For optimization models, CEs have primarily been studied in limited contexts and little research has been done on CEs for general integer optimization problems. In this work, we address this gap. We first show that the general problem of constructing a CE is $\Sigma_2^p$-complete even for binary integer programs with just a single mutable constraint. Second, we propose solution algorithms for several of the most tractable special cases: (i) mutable objective parameters, (ii) a single mutable constraint, (iii) mutable right-hand-side, and (iv) all input parameters can be modified. We evaluate our approach using classical knapsack problem instances, focusing on cases with mutable constraint parameters. Additionally, we present experiments on the resource constrained shortest path problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17624v2</guid>
      <category>math.OC</category>
      <category>cs.CC</category>
      <pubDate>Tue, 06 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Felix Engelhardt, Jannis Kurtz, \c{S}. \.Ilker Birbil, Ted Ralphs</dc:creator>
    </item>
  </channel>
</rss>
