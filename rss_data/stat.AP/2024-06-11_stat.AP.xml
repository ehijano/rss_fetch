<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 11 Jun 2024 04:01:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 11 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Analyzing the factors that are involved in length of inpatient stay at the hospital for diabetes patients</title>
      <link>https://arxiv.org/abs/2406.05189</link>
      <description>arXiv:2406.05189v1 Announce Type: new 
Abstract: The paper investigates the escalating concerns surrounding the surge in diabetes cases, exacerbated by the COVID-19 pandemic, and the subsequent strain on medical resources. The research aims to construct a predictive model quantifying factors influencing inpatient hospital stay durations for diabetes patients, offering insights to hospital administrators for improved patient management strategies. The literature review highlights the increasing prevalence of diabetes, emphasizing the need for continued attention and analysis of urban-rural disparities in healthcare access. International studies underscore the financial implications and healthcare burden associated with diabetes-related hospitalizations and complications, emphasizing the significance of effective management strategies. The methodology involves a quantitative approach, utilizing a dataset comprising 10,000 observations of diabetic inpatient encounters in U.S. hospitals from 1999 to 2008. Predictive modeling techniques, particularly Generalized Linear Models (GLM), are employed to develop a model predicting hospital stay durations based on patient demographics, admission types, medical history, and treatment regimen. The results highlight the influence of age, medical history, and treatment regimen on hospital stay durations for diabetes patients. Despite model limitations, such as heteroscedasticity and deviations from normality in residual analysis, the findings offer valuable insights for hospital administrators in patient management. The paper concludes with recommendations for future research to address model limitations and explore the implications of predictive models on healthcare management strategies, ensuring equitable patient care and resource allocation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05189v1</guid>
      <category>stat.AP</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jorden Lam, Kunpeng Xu</dc:creator>
    </item>
    <item>
      <title>A Three-groups Non-local Model for Combining Heterogeneous Data Sources to Identify Genes Associated with Parkinson's Disease</title>
      <link>https://arxiv.org/abs/2406.05262</link>
      <description>arXiv:2406.05262v1 Announce Type: new 
Abstract: We seek to identify genes involved in Parkinson's Disease (PD) by combining information across different experiment types. Each experiment, taken individually, may contain too little information to distinguish some important genes from incidental ones. However, when experiments are combined using the proposed statistical framework, additional power emerges. The fundamental building block of the family of statistical models that we propose is a hierarchical three-group mixture of distributions. Each gene is modeled probabilistically as belonging to either a null group that is unassociated with PD, a deleterious group, or a beneficial group. This three-group formalism has two key features. By apportioning prior probability of group assignments with a Dirichlet distribution, the resultant posterior group probabilities automatically account for the multiplicity inherent in analyzing many genes simultaneously. By building models for experimental outcomes conditionally on the group labels, any number of data modalities may be combined in a single coherent probability model, allowing information sharing across experiment types. These two features result in parsimonious inference with few false positives, while simultaneously enhancing power to detect signals. Simulations show that our three-groups approach performs at least as well as commonly-used tools for GWAS and RNA-seq, and in some cases it performs better. We apply our proposed approach to publicly-available GWAS and RNA-seq datasets, discovering novel genes that are potential therapeutic targets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05262v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Troy P. Wixson, Benjamin A. Shaby, Daisy L. Philtron, International Parkinson Disease Genomics Consortium, Leandro A. Lima, Stacia K. Wyman, Julia A. Kaye, Steven Finkbeiner</dc:creator>
    </item>
    <item>
      <title>"Minus-One" Data Prediction Generates Synthetic Census Data with Good Crosstabulation Fidelity</title>
      <link>https://arxiv.org/abs/2406.05264</link>
      <description>arXiv:2406.05264v1 Announce Type: new 
Abstract: We propose to capture relevant statistical associations in a dataset of categorical survey responses by a method, here termed MODP, that "learns" a probabilistic prediction function L. Specifically, L predicts each question's response based on the same respondent's answers to all the other questions. Draws from the resulting probability distribution become synthetic responses. Applying this methodology to the PUMS subset of Census ACS data, and with a learned L akin to multiple parallel logistic regression, we generate synthetic responses whose crosstabulations (two-point conditionals) are found to have a median accuracy of ~5% across all crosstabulation cells, with cell counts ranging over four orders of magnitude. We investigate and attempt to quantify the degree to which the privacy of the original data is protected.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05264v1</guid>
      <category>stat.AP</category>
      <category>cs.CR</category>
      <category>cs.CY</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>William H. Press</dc:creator>
    </item>
    <item>
      <title>Inestability presented in the estimating of the Nelson-Siegel-Svensson model</title>
      <link>https://arxiv.org/abs/2406.06177</link>
      <description>arXiv:2406.06177v1 Announce Type: new 
Abstract: The literature shows the possible existence of a problem called collinearity in both Nelson-Siegel and Nelson-Siegel-Svensson models due to the relationship between the slope and curvature components. The presence of this problem and the estimation of both models by Ordinary Least Squares would lead to coefficients estimates that may be unstable among other consequences. However, these estimates are used to make monetary policy decisions. For this reason, it is important to try mitigating this collinearity problem. Consequently, some authors propose traditional procedures for the treatment of collinearity such as: non-linear optimisation, to fix the shape parameter or ridge regression. Nevertheless, all these processes have their disadvantages. Alternatively, a new method with good properties called raise regression is proposed in this paper. Finally, the methodologies are illustrated with an empirical comparison on Euribor Overnight Index Swap and Euribor Interest Rates Swap data between 2011 and 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06177v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ainara Rodr\'iguez-S\'anchez</dc:creator>
    </item>
    <item>
      <title>Planning for Gold: Sample Splitting for Valid Powerful Design of Observational Studies</title>
      <link>https://arxiv.org/abs/2406.00866</link>
      <description>arXiv:2406.00866v1 Announce Type: cross 
Abstract: Observational studies are valuable tools for inferring causal effects in the absence of controlled experiments. However, these studies may be biased due to the presence of some relevant, unmeasured set of covariates. The design of an observational study has a prominent effect on its sensitivity to hidden biases, and the best design may not be apparent without examining the data. One approach to facilitate a data-inspired design is to split the sample into a planning sample for choosing the design and an analysis sample for making inferences. We devise a powerful and flexible method for selecting outcomes in the planning sample when an unknown number of outcomes are affected by the treatment. We investigate the theoretical properties of our method and conduct extensive simulations that demonstrate pronounced benefits, especially at higher levels of allowance for unmeasured confounding. Finally, we demonstrate our method in an observational study of the multi-dimensional impacts of a devastating flood in Bangladesh.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00866v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William Bekerman, Abhinandan Dalal, Carlo del Ninno, Dylan S. Small</dc:creator>
    </item>
    <item>
      <title>Early School Leaving in Spain: a longitudinal analysis by gender</title>
      <link>https://arxiv.org/abs/2406.05172</link>
      <description>arXiv:2406.05172v1 Announce Type: cross 
Abstract: Spain is one of the eight EU-27 countries that failed to reduce early school leaving (ESL) below 10% in 2020, and now faces the challenge of achieving a rate below 9% by 2030. The determinants of this phenomenon are usually studied using cross-sectional data at the micro-level and without differentiation by gender. In this study, we analyse it for the first time for Spain using panel data (between 2002-2020), taking into account the high regional inequalities at the macroeconomic level and the masculinisation of the phenomenon. The results show a positive relationship between ESL and socioeconomic variables such as the adolescent fertility rate, immigration, unemployment or the weight of the industrial and construction sectors in the regional economy, with significant gender differences that invite us to discuss educational policies. Surprisingly, youth unemployment has only small but significant impact on female ESL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05172v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1332/17579597Y2024D000000023</arxiv:DOI>
      <dc:creator>Mart\'in Mart\'in-Gonz\'alez, Sara M. Gonz\'alez-Betancor, Carmen P\'erez-Esparrells</dc:creator>
    </item>
    <item>
      <title>Cross-sectional shape analysis for risk assessment and prognosis of patients with true lumen narrowing after type-A aortic dissection surgery</title>
      <link>https://arxiv.org/abs/2406.05173</link>
      <description>arXiv:2406.05173v1 Announce Type: cross 
Abstract: Background: For acute type-A aortic dissection (ATAAD) surgery, early post-surgery assessment is crucially important for effective treatment plans, underscoring the need for a framework to identify the risk level of aortic dissection cases. We examined true-lumen narrowing during follow-up examinations, collected morphological data 14 days (early stages) after surgery, and assessed patient risk levels over 2.8 years.
  Purpose: To establish an implementable framework supported by mathematical techniques to predict the risk of aortic dissection patients experiencing true-lumen narrowing after ATAAD surgery.
  Materials and Methods: This retrospective study analyzed CT data from 21 ATAAD patients. Forty uniformly distributed cross-sectional shapes (CSSs) are derived from each lumen to account for gradual changes in shape. We introduced the form factor (FF) to assess CSS morphology. Linear discriminant analysis (LDA) is used for the risk classification of aortic dissection patients. Leave-one-patient-out cross-validation (LOPO-CV) is used for risk prediction.
  Results: For this investigation, we examined data of 21 ATAAD patients categorized into high-risk, medium-risk, and low-risk cases based on clinical observations of the range of true-lumen narrowing. Our risk classification machine-learning (ML) model preserving the model's generalizability. The model's predictions reliably identified low-risk patients, thereby potentially reducing hospital visits. It also demonstrated proficiency in accurately predicting the risk for all high-risk patients.
  Conclusion: The suggested method anticipates the risk linked to aortic enlargement in patients with a narrowing true lumen in the early stage following ATAAD surgery, thereby aiding follow-up doctors in enhancing patient care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05173v1</guid>
      <category>physics.med-ph</category>
      <category>math.GT</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J V Ramana Reddy (Advanced Institute for Materials Research, Tohoku University, 2-1-1 Katahira, Sendai, Miyagi, Japan), Toshitaka Watanabe (Medical Corporation Shimada Clinic Clover Clinic, Osaka, Japan), Taro Hayashi (Department of Cardiovascular Surgery, Akashi Medical Center, Hyogo, Japan), Hiroshi Suito (Advanced Institute for Materials Research, Tohoku University, 2-1-1 Katahira, Sendai, Miyagi, Japan)</dc:creator>
    </item>
    <item>
      <title>Tree balance in phylogenetic models</title>
      <link>https://arxiv.org/abs/2406.05185</link>
      <description>arXiv:2406.05185v1 Announce Type: cross 
Abstract: Tree shape statistics, particularly measures of tree (im)balance, play an important role in the analysis of the shape of phylogenetic trees. With applications ranging from testing evolutionary models to studying the impact of fertility inheritance and selection, or tumor development and language evolution, the assessment of tree balance is crucial. Currently, a multitude of at least 30 (im)balance indices can be found in the literature, alongside numerous other tree shape statistics.
  This diversity prompts essential questions: How can we minimize the selection of indices to mitigate the challenges of multiple testing? Is there a preeminent balance index tailored to specific tasks? Previous studies comparing the statistical power of indices in detecting trees deviating from the Yule model have been limited in scope, utilizing only a subset of indices and alternative tree models.
  This research expands upon the examination of index power, encompassing all established indices and a broader array of alternative models. Our investigation reveals distinct groups of balance indices better suited for different tree models, suggesting that decisions on balance index selection can be enhanced with prior knowledge. Furthermore, we present the \textsf{R} software package \textsf{poweRbal} which allows the inclusion of new indices and models, thus facilitating future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05185v1</guid>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sophie J. Kersting, Kristina Wicke, Mareike Fischer</dc:creator>
    </item>
    <item>
      <title>HAL-based Plugin Estimation of the Causal Dose-Response Curve</title>
      <link>https://arxiv.org/abs/2406.05607</link>
      <description>arXiv:2406.05607v1 Announce Type: cross 
Abstract: Estimating the marginally adjusted dose-response curve for continuous treatments is a longstanding statistical challenge critical across multiple fields. In the context of parametric models, mis-specification may result in substantial bias, hindering the accurate discernment of the true data generating distribution and the associated dose-response curve. In contrast, non-parametric models face difficulties as the dose-response curve isn't pathwise differentiable, and then there is no $\sqrt{n}$-consistent estimator. The emergence of the Highly Adaptive Lasso (HAL) MLE by van der Laan [2015] and van der Laan [2017] and the subsequent theoretical evidence by van der Laan [2023] regarding its pointwise asymptotic normality and uniform convergence rates, have highlighted the asymptotic efficacy of the HAL-based plug-in estimator for this intricate problem. This paper delves into the HAL-based plug-in estimators, including those with cross-validation and undersmoothing selectors, and introduces the undersmoothed smoothness-adaptive HAL-based plug-in estimator. We assess these estimators through extensive simulations, employing detailed evaluation metrics. Building upon the theoretical proofs in van der Laan [2023], our empirical findings underscore the asymptotic effectiveness of the undersmoothed smoothness-adaptive HAL-based plug-in estimator in estimating the marginally adjusted dose-response curve.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05607v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator> Junming (Seraphina),  Shi, Wenxin Zhang, Alan E. Hubbard, Mar van der Laan</dc:creator>
    </item>
    <item>
      <title>Metrics sonification: The introduction of new ways to present bibliometric data using publication data of Loet Leydesdorff as an example</title>
      <link>https://arxiv.org/abs/2406.05679</link>
      <description>arXiv:2406.05679v1 Announce Type: cross 
Abstract: The visualization of publication and citation data is popular in bibliometrics. Although less common, the representation of empirical data as sound is an alternative form of presentation (in other fields than bibliometrics). In this representation, the data are mapped into sound and listened to by an audience. Approaches for the sonification of data have been developed in many fields since decades. Since sonification has several advantages for the presentation of data, this study is intended to introduce sonification to bibliometrics named as 'metrics sonification'. Metrics sonification is defined as the sonification of bibliometric information (measurements, data or results) for their empirical analysis and/or presentation. In this study, we used metadata of publications by Loet Leydesdorff (named as Loet in the following) to sonify their properties. Loet was a giant in the field of scientometrics, who passed away in 2023. The track based on Loet's publications can be listened to on SoundCloud using the following link: https://on.soundcloud.com/oxBTA32x4EgwvKVz5. The track has been composed in F minor; this key was chosen to express the sad occasion. The quantitative part of the track includes a parameter mapping (a sonification) of three properties of his publications: (1) publication output, (2) open access publication, and (3) citation impact of publications. The qualitative part (spoken audio) focuses on explanations of the parameter mapping and descriptions of the mapped papers (based on their titles and abstracts). The sonification of Loet's publications presented in this study is only one possible type of metrics sonification application. As the great number of projects from other disciplines have demonstrated, many other types of applications are possible in bibliometrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05679v1</guid>
      <category>cs.DL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lutz Bornmann, Rouven Lazlo Haegner</dc:creator>
    </item>
    <item>
      <title>Smiles2Dock: an open large-scale multi-task dataset for ML-based molecular docking</title>
      <link>https://arxiv.org/abs/2406.05738</link>
      <description>arXiv:2406.05738v1 Announce Type: cross 
Abstract: Docking is a crucial component in drug discovery aimed at predicting the binding conformation and affinity between small molecules and target proteins. ML-based docking has recently emerged as a prominent approach, outpacing traditional methods like DOCK and AutoDock Vina in handling the growing scale and complexity of molecular libraries. However, the availability of comprehensive and user-friendly datasets for training and benchmarking ML-based docking algorithms remains limited. We introduce Smiles2Dock, an open large-scale multi-task dataset for molecular docking. We created a framework combining P2Rank and AutoDock Vina to dock 1.7 million ligands from the ChEMBL database against 15 AlphaFold proteins, giving us more than 25 million protein-ligand binding scores. The dataset leverages a wide range of high-accuracy AlphaFold protein models, encompasses a diverse set of biologically relevant compounds and enables researchers to benchmark all major approaches for ML-based docking such as Graph, Transformer and CNN-based methods. We also introduce a novel Transformer-based architecture for docking scores prediction and set it as an initial benchmark for our dataset. Our dataset and code are publicly available to support the development of novel ML-based methods for molecular docking to advance scientific research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05738v1</guid>
      <category>q-bio.BM</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas Le Menestrel, Manuel Rivas</dc:creator>
    </item>
    <item>
      <title>Probabilistic Approach to Black-Box Binary Optimization with Budget Constraints: Application to Sensor Placement</title>
      <link>https://arxiv.org/abs/2406.05830</link>
      <description>arXiv:2406.05830v1 Announce Type: cross 
Abstract: We present a fully probabilistic approach for solving binary optimization problems with black-box objective functions and with budget constraints. In the probabilistic approach, the optimization variable is viewed as a random variable and is associated with a parametric probability distribution. The original optimization problem is replaced with an optimization over the expected value of the original objective, which is then optimized over the probability distribution parameters. The resulting optimal parameter (optimal policy) is used to sample the binary space to produce estimates of the optimal solution(s) of the original binary optimization problem. The probability distribution is chosen from the family of Bernoulli models because the optimization variable is binary. The optimization constraints generally restrict the feasibility region. This can be achieved by modeling the random variable with a conditional distribution given satisfiability of the constraints. Thus, in this work we develop conditional Bernoulli distributions to model the random variable conditioned by the total number of nonzero entries, that is, the budget constraint. This approach (a) is generally applicable to binary optimization problems with nonstochastic black-box objective functions and budget constraints; (b) accounts for budget constraints by employing conditional probabilities that sample only the feasible region and thus considerably reduces the computational cost compared with employing soft constraints; and (c) does not employ soft constraints and thus does not require tuning of a regularization parameter, for example to promote sparsity, which is challenging in sensor placement optimization problems. The proposed approach is verified numerically by using an idealized bilinear binary optimization problem and is validated by using a sensor placement experiment in a parameter identification setup.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05830v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>math.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Attia</dc:creator>
    </item>
    <item>
      <title>Stochastic ordering of series and parallel systems lifetime in Archimedean copula under random shock</title>
      <link>https://arxiv.org/abs/2406.05834</link>
      <description>arXiv:2406.05834v1 Announce Type: cross 
Abstract: In this manuscript, we studied the stochastic ordering behavior of series as well as parallel systems lifetimes comprising dependent and heterogeneous components, experiencing random shocks, and exhibiting distinct dependency structures. We establish certain conditions for the lifetime of individual components, the dependency among components defined by Archimedean copulas, and the impact of random shocks on the overall system lifetime to reach the conclusion. We consider components whose survival functions are either increasing log-concave or decreasing log-convex functions of the parameters involved and systems exhibit different dependency structures. These conditions make it possible to compare the lifetimes of two systems using the usual stochastic order framework. Additionally, we provide examples and graphical representations to elucidate our theoretical findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05834v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sarikul Islam, Nitin Gupta</dc:creator>
    </item>
    <item>
      <title>A Statistical Theory of Regularization-Based Continual Learning</title>
      <link>https://arxiv.org/abs/2406.06213</link>
      <description>arXiv:2406.06213v1 Announce Type: cross 
Abstract: We provide a statistical analysis of regularization-based continual learning on a sequence of linear regression tasks, with emphasis on how different regularization terms affect the model performance. We first derive the convergence rate for the oracle estimator obtained as if all data were available simultaneously. Next, we consider a family of generalized $\ell_2$-regularization algorithms indexed by matrix-valued hyperparameters, which includes the minimum norm estimator and continual ridge regression as special cases. As more tasks are introduced, we derive an iterative update formula for the estimation error of generalized $\ell_2$-regularized estimators, from which we determine the hyperparameters resulting in the optimal algorithm. Interestingly, the choice of hyperparameters can effectively balance the trade-off between forward and backward knowledge transfer and adjust for data heterogeneity. Moreover, the estimation error of the optimal algorithm is derived explicitly, which is of the same order as that of the oracle estimator. In contrast, our lower bounds for the minimum norm estimator and continual ridge regression show their suboptimality. A byproduct of our theoretical analysis is the equivalence between early stopping and generalized $\ell_2$-regularization in continual learning, which may be of independent interest. Finally, we conduct experiments to complement our theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.06213v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuyang Zhao, Huiyuan Wang, Weiran Huang, Wei Lin</dc:creator>
    </item>
    <item>
      <title>Model-Free Error Assessment for Breadth-First Studies, with Applications to Cell-Perturbation Experiments</title>
      <link>https://arxiv.org/abs/2208.01745</link>
      <description>arXiv:2208.01745v3 Announce Type: replace 
Abstract: With the advent of high-throughput screenings, it has become increasingly common for studies to devote limited resources to estimating many parameters imprecisely rather than to estimating a few parameters well. In these studies, only two or three independent replicates measure each parameter, and therefore it is challenging to assess the variance of these measurements. One solution is to pool variance estimates across different parameters using a parametric model of estimator error. However, such models are difficult to specify correctly, especially in the presence of ``batch effects.'' In this paper, we propose new model-free methods for assessing and controlling estimator error. Our focus is on type S error, which is of particular importance in many settings. To produce tight confidence intervals without making unrealistic assumptions, we improve on Hoeffding's bounds for sums of bounded random variables and obtain the tightest possible Chernoff-Cram\'er bound. Our methods compare favorably with existing practice for high-throughput screenings, such as methods based on the Irreproducible Discovery Rate (IDR) and the Benjamini-Hochberg procedure. Existing practices fail to control error at the nominal level in some cases and are needlessly conservative in others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.01745v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jackson Loper, Jeffrey Regier</dc:creator>
    </item>
    <item>
      <title>Alternative ranking measures to predict international football results</title>
      <link>https://arxiv.org/abs/2405.10247</link>
      <description>arXiv:2405.10247v2 Announce Type: replace 
Abstract: Over the last few years, there has been a growing interest in the prediction and modelling of competitive sports outcomes, with particular emphasis placed on this area by the Bayesian statistics and machine learning communities. In this paper, we have carried out a comparative evaluation of statistical and machine learning models to assess their predictive performance for the 2022 FIFA World Cup and for the 2023 CAF Africa Cup of Nations by evaluating alternative summaries of past performances related to the involved teams. More specifically, we consider the Bayesian Bradley-Terry-Davidson model, which is a widely used statistical framework for ranking items based on paired comparisons that have been applied successfully in various domains, including football. The analysis was performed including in some canonical goal-based models both the Bradley-Terry-Davidson derived ranking and the widely recognized Coca-Cola FIFA ranking commonly adopted by football fans and amateurs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.10247v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Roberto Macr\`i Demartino, Leonardo Egidi, Nicola Torelli</dc:creator>
    </item>
    <item>
      <title>Active sampling: A machine-learning-assisted framework for finite population inference with optimal subsamples</title>
      <link>https://arxiv.org/abs/2212.10024</link>
      <description>arXiv:2212.10024v2 Announce Type: replace-cross 
Abstract: Data subsampling has become widely recognized as a tool to overcome computational and economic bottlenecks in analyzing massive datasets. We contribute to the development of adaptive design for estimation of finite population characteristics, using active learning and adaptive importance sampling. We propose an active sampling strategy that iterates between estimation and data collection with optimal subsamples, guided by machine learning predictions on yet unseen data. The method is illustrated on virtual simulation-based safety assessment of advanced driver assistance systems. Substantial performance improvements are demonstrated compared to traditional sampling methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.10024v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Henrik Imberg, Xiaomi Yang, Carol Flannagan, Jonas B\"argman</dc:creator>
    </item>
    <item>
      <title>Estimating Heterogeneous Exposure Effects in the Case-Crossover Design using BART</title>
      <link>https://arxiv.org/abs/2311.12016</link>
      <description>arXiv:2311.12016v2 Announce Type: replace-cross 
Abstract: Epidemiological approaches for examining human health responses to environmental exposures in observational studies often control for confounding by implementing clever matching schemes and using statistical methods based on conditional likelihood. Nonparametric regression models have surged in popularity in recent years as a tool for estimating individual-level heterogeneous effects, which provide a more detailed picture of the exposure-response relationship but can also be aggregated to obtain improved marginal estimates at the population level. In this work we incorporate Bayesian additive regression trees (BART) into the conditional logistic regression model to identify heterogeneous exposure effects in a case-crossover design. Conditional logistic BART (CL-BART) utilizes reversible jump Markov chain Monte Carlo to bypass the conditional conjugacy requirement of the original BART algorithm. Our work is motivated by the growing interest in identifying subpopulations more vulnerable to environmental exposures. We apply CL-BART to a study of the impact of heat waves on people with Alzheimer's disease in California and effect modification by other chronic conditions. Through this application, we also describe strategies to examine heterogeneous odds ratios through variable importance, partial dependence, and lower-dimensional summaries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.12016v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jacob Englert, Stefanie Ebelt, Howard Chang</dc:creator>
    </item>
    <item>
      <title>Random Effect Restricted Mean Survival Time Model</title>
      <link>https://arxiv.org/abs/2401.02048</link>
      <description>arXiv:2401.02048v3 Announce Type: replace-cross 
Abstract: The restricted mean survival time (RMST) model has been garnering attention as a way to provide a clinically intuitive measure: the mean survival time. RMST models, which use methods based on pseudo time-to-event values and inverse probability censoring weighting, can adjust covariates. However, no approach has yet been introduced that considers random effects for clusters. In this paper, we propose a new random-effect RMST. We present two methods of analysis that consider variable effects by i) using a generalized mixed model with pseudo-values and ii) integrating the estimated results from the inverse probability censoring weighting estimating equations for each cluster. We evaluate our proposed methods through computer simulations. In addition, we analyze the effect of a mother's age at birth on under-five deaths in India using states as clusters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.02048v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keisuke Hanada, Masahiro Kojima</dc:creator>
    </item>
    <item>
      <title>Bayesian multi-exposure image fusion for robust high dynamic range ptychography</title>
      <link>https://arxiv.org/abs/2403.11344</link>
      <description>arXiv:2403.11344v4 Announce Type: replace-cross 
Abstract: The limited dynamic range of the detector can impede coherent diffractive imaging (CDI) schemes from achieving diffraction-limited resolution. To overcome this limitation, a straightforward approach is to utilize high dynamic range (HDR) imaging through multi-exposure image fusion (MEF). This method involves capturing measurements at different exposure times, spanning from under to overexposure and fusing them into a single HDR image. The conventional MEF technique in ptychography typically involves subtracting the background noise, ignoring the saturated pixels and then merging the acquisitions. However, this approach is inadequate under conditions of low signal-to-noise ratio (SNR). Additionally, variations in illumination intensity significantly affect the phase retrieval process. To address these issues, we propose a Bayesian MEF modeling approach based on a modified Poisson distribution that takes the background and saturation into account. To infer the model parameters, the expectation-maximization (EM) algorithm is employed. As demonstrated with synthetic and experimental data, our approach outperforms the conventional MEF method, offering superior phase retrieval under challenging experimental conditions. This work underscores the significance of robust multi-exposure image fusion for ptychography, particularly in imaging shot-noise-dominated weakly scattering specimens or in cases where access to HDR detectors with high SNR is limited. Furthermore, the applicability of the Bayesian MEF approach extends beyond CDI to any imaging scheme that requires HDR treatment. Given this versatility, we provide the implementation of our algorithm as a Python package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.11344v4</guid>
      <category>eess.IV</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1364/OE.524284</arxiv:DOI>
      <dc:creator>Shantanu Kodgirwar, Lars Loetgering, Chang Liu, Aleena Joseph, Leona Licht, Daniel S. Penagos Molina, Wilhelm Eschen, Jan Rothhardt, Michael Habeck</dc:creator>
    </item>
    <item>
      <title>A critical appraisal of water table depth estimation: Challenges and opportunities within machine learning</title>
      <link>https://arxiv.org/abs/2405.04579</link>
      <description>arXiv:2405.04579v2 Announce Type: replace-cross 
Abstract: Fine-resolution spatial patterns of water table depth (WTD) play a crucial role in shaping ecological resilience, hydrological connectivity, and anthropocentric objectives. Generally, a large-scale (e.g., continental or global) spatial map of static WTD can be simulated using either physically-based (PB) or machine learning-based (ML) models. We construct three fine-resolution (500 m) ML simulations of WTD, using the XGBoost algorithm and more than 20 million real and proxy observations of WTD, across the United States and Canada. The three ML models were constrained using known physical relations between WTD's drivers and WTD and were trained by sequentially adding real and proxy observations of WTD. We interpret the black box of our physically constrained ML models and compare it against available literature in groundwater hydrology. Through an extensive (pixel-by-pixel) evaluation, we demonstrate that our models can more accurately predict unseen real and proxy observations of WTD across most of North America's ecoregions compared to three available PB simulations of WTD. However, we still argue that large-scale WTD estimation is far from being a solved problem. We reason that due to biased observational data mainly collected from low-elevation floodplains, the misspecification of equations within physically-based models, and the over-flexibility of machine learning models, verifiably accurate simulations of WTD do not yet exist. Ultimately, we thoroughly discuss future directions that may help hydrogeologists decide how to proceed with WTD estimations, with a particular focus on the application of machine learning and the use of proxy satellite data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04579v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph Janssen, Ardalan Tootchi, Ali A. Ameli</dc:creator>
    </item>
    <item>
      <title>The Multi-Range Theory of Translation Quality Measurement: MQM scoring models and Statistical Quality Control</title>
      <link>https://arxiv.org/abs/2405.16969</link>
      <description>arXiv:2405.16969v4 Announce Type: replace-cross 
Abstract: The year 2024 marks the 10th anniversary of the Multidimensional Quality Metrics (MQM) framework for analytic translation quality evaluation. The MQM error typology has been widely used by practitioners in the translation and localization industry and has served as the basis for many derivative projects. The annual Conference on Machine Translation (WMT) shared tasks on both human and automatic translation quality evaluations used the MQM error typology.
  The metric stands on two pillars: error typology and the scoring model. The scoring model calculates the quality score from annotation data, detailing how to convert error type and severity counts into numeric scores to determine if the content meets specifications. Previously, only the raw scoring model had been published. This April, the MQM Council published the Linear Calibrated Scoring Model, officially presented herein, along with the Non-Linear Scoring Model, which had not been published before.
  This paper details the latest MQM developments and presents a universal approach to translation quality measurement across three sample size ranges. It also explains why Statistical Quality Control should be used for very small sample sizes, starting from a single sentence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.16969v4</guid>
      <category>cs.CL</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Arle Lommel, Serge Gladkoff, Alan Melby, Sue Ellen Wright, Ingemar Strandvik, Katerina Gasova, Angelika Vaasa, Andy Benzo, Romina Marazzato Sparano, Monica Foresi, Johani Innis, Lifeng Han, Goran Nenadic</dc:creator>
    </item>
  </channel>
</rss>
