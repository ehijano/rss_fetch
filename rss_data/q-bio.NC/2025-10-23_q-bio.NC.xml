<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>q-bio.NC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/q-bio.NC</link>
    <description>q-bio.NC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/q-bio.NC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 Oct 2025 04:00:07 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Neurotremor: A wearable Supportive Device for Supporting Upper Limb Muscle Function</title>
      <link>https://arxiv.org/abs/2510.19826</link>
      <description>arXiv:2510.19826v1 Announce Type: new 
Abstract: A sensor-fused wearable assistance prototype for upper-limb function (triceps brachii and extensor pollicis brevis) is presented. The device integrates surface electromyography (sEMG), an inertial measurement unit (IMU), and flex/force sensors on an M5StickC plus an ESP32-S3 compute hub. Signals are band-pass and notch filtered; features (RMS, MAV, zero-crossings, and 4-12 Hz tremor-band power) are computed in 250 ms windows and fed to an INT8 TensorFlow Lite Micro model. Control commands are bounded by a control-barrier-function safety envelope and delivered within game-based tasks with lightweight personalization. In a pilot technical feasibility evaluation with healthy volunteers (n = 12) performing three ADL-oriented tasks, tremor prominence decreased (Delta TI = -0.092, 95% CI [-0.102, -0.079]), range of motion increased (+12.65%, 95% CI [+8.43, +13.89]), repetitions rose (+2.99 min^-1, 95% CI [+2.61, +3.35]), and the EMG median-frequency slope became less negative (Delta = +0.100 Hz/min, 95% CI [+0.083, +0.127]). The sensing-to-assist loop ran at 100 Hz with 8.7 ms median on-device latency, 100% session completion, and 0 device-related adverse events. These results demonstrate technical feasibility of embedded, sensor-fused assistance for upper-limb function; formal patient studies under IRB oversight are planned.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19826v1</guid>
      <category>q-bio.NC</category>
      <category>cs.ET</category>
      <category>cs.LG</category>
      <category>q-bio.TO</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aueaphum Aueawattthanaphisut, Thanyanee Srichaisak, Arissa Ieochai</dc:creator>
    </item>
    <item>
      <title>A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</title>
      <link>https://arxiv.org/abs/2510.19764</link>
      <description>arXiv:2510.19764v1 Announce Type: cross 
Abstract: The majority of research in both training Artificial Neural Networks (ANNs) and modeling learning in biological brains focuses on synaptic plasticity, where learning equates to changing the strength of existing connections. However, in biological brains, structural plasticity - where new connections are created and others removed - is also vital, not only for effective learning but also for recovery from damage and optimal resource usage. Inspired by structural plasticity, pruning is often used in machine learning to remove weak connections from trained models to reduce the computational requirements of inference. However, the machine learning frameworks typically used for backpropagation-based training of both ANNs and Spiking Neural Networks (SNNs) are optimized for dense connectivity, meaning that pruning does not help reduce the training costs of ever-larger models. The GeNN simulator already supports efficient GPU-accelerated simulation of sparse SNNs for computational neuroscience and machine learning. Here, we present a new flexible framework for implementing GPU-accelerated structural plasticity rules and demonstrate this first using the e-prop supervised learning rule and DEEP R to train efficient, sparse SNN classifiers and then, in an unsupervised learning context, to learn topographic maps. Compared to baseline dense models, our sparse classifiers reduce training time by up to 10x while the DEEP R rewiring enables them to perform as well as the original models. We demonstrate topographic map formation in faster-than-realtime simulations, provide insights into the connectivity evolution, and measure simulation speed versus network size. The proposed framework will enable further research into achieving and maintaining sparsity in network structure and neural communication, as well as exploring the computational benefits of sparsity in a range of neuromorphic applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.19764v1</guid>
      <category>cs.NE</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James C. Knight, Johanna Senk, Thomas Nowotny</dc:creator>
    </item>
    <item>
      <title>Convexity of Neural Codes with Four Maximal Codewords</title>
      <link>https://arxiv.org/abs/2510.20323</link>
      <description>arXiv:2510.20323v1 Announce Type: cross 
Abstract: Place cells are neurons that act as biological position sensors, associated with and firing in response to regions of an environment to situate an organism in space. These associations are recorded in (combinatorial) neural codes, motivating the following mathematical question: Which neural codes are generated by a collection of convex open sets in Euclidean space? Giusti and Itskov showed that a necessary condition for convexity is the absence of ``local obstructions." This necessary condition is, in fact, sufficient for certain families of codes. One such family consists of all codes with up to three maximal codewords. In this article, we investigate codes with four maximal codewords, showing that for many such codes, convexity is characterized by the absence of local obstructions, whereas for other such codes, convexity is characterized by the absence of local obstructions and a second type of obstruction, a ``wheel". Key to our analysis is a case-by-case investigation based on the nerve complex of the set of maximal codewords of a neural code. Up to symmetry, there are 20 possible nerves; and our results fully characterize convexity in 15 of the 20 cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20323v1</guid>
      <category>math.CO</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Saber Ahmed, Natasha Crepeau, Gisel Flores, Osiano Isekenegbe, Deanna Perez, Anne Shiu</dc:creator>
    </item>
    <item>
      <title>Separating the what and how of compositional computation to enable reuse and continual learning</title>
      <link>https://arxiv.org/abs/2510.20709</link>
      <description>arXiv:2510.20709v1 Announce Type: cross 
Abstract: The ability to continually learn, retain and deploy skills to accomplish goals is a key feature of intelligent and efficient behavior. However, the neural mechanisms facilitating the continual learning and flexible (re-)composition of skills remain elusive. Here, we study continual learning and the compositional reuse of learned computations in recurrent neural network (RNN) models using a novel two-system approach: one system that infers what computation to perform, and one that implements how to perform it. We focus on a set of compositional cognitive tasks commonly studied in neuroscience. To construct the what system, we first show that a large family of tasks can be systematically described by a probabilistic generative model, where compositionality stems from a shared underlying vocabulary of discrete task epochs. The shared epoch structure makes these tasks inherently compositional. We first show that this compositionality can be systematically described by a probabilistic generative model. Furthermore, We develop an unsupervised online learning approach that can learn this model on a single-trial basis, building its vocabulary incrementally as it is exposed to new tasks, and inferring the latent epoch structure as a time-varying computational context within a trial. We implement the how system as an RNN whose low-rank components are composed according to the context inferred by the what system. Contextual inference facilitates the creation, learning, and reuse of low-rank RNN components as new tasks are introduced sequentially, enabling continual learning without catastrophic forgetting. Using an example task set, we demonstrate the efficacy and competitive performance of this two-system learning framework, its potential for forward and backward transfer, as well as fast compositional generalization to unseen tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.20709v1</guid>
      <category>cs.LG</category>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>The Thirty-Ninth Annual Conference on Neural Information Processing Systems (2025)</arxiv:journal_reference>
      <dc:creator>Haozhe Shan, Sun Minni, Lea Duncker</dc:creator>
    </item>
    <item>
      <title>Deep Learning-Powered Electrical Brain Signals Analysis: Advancing Neurological Diagnostics</title>
      <link>https://arxiv.org/abs/2502.17213</link>
      <description>arXiv:2502.17213v2 Announce Type: replace 
Abstract: Neurological disorders pose major global health challenges, driving advances in brain signal analysis. Scalp electroencephalography (EEG) and intracranial EEG (iEEG) are widely used for diagnosis and monitoring. However, dataset heterogeneity and task variations hinder the development of robust deep learning solutions. This review systematically examines recent advances in deep learning approaches for EEG/iEEG-based neurological diagnostics, focusing on applications across 7 neurological conditions using 46 datasets. For each condition, we review representative methods and their quantitative results, integrating performance comparisons with analyses of data usage, model design, and task-specific adaptations, while highlighting the role of pre-trained multi-task models in achieving scalable, generalizable solutions. Finally, we propose a standardized benchmark to evaluate models across diverse datasets and improve reproducibility, emphasizing how recent innovations are transforming neurological diagnostics toward intelligent, adaptable healthcare systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17213v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiahe Li, Xin Chen, Fanqi Shen, Junru Chen, Yuxin Liu, Daoze Zhang, Zhizhang Yuan, Fang Zhao, Meng Li, Yang Yang</dc:creator>
    </item>
    <item>
      <title>On sources to variabilities of simple cells in the primary visual cortex: A principled theory for the interaction between geometric image transformations and receptive field responses</title>
      <link>https://arxiv.org/abs/2509.02139</link>
      <description>arXiv:2509.02139v3 Announce Type: replace 
Abstract: This paper gives an overview of a theory for modelling the interaction between geometric image transformations and receptive field responses for a visual observer that views objects and spatio-temporal events in the environment. This treatment is developed over combinations of (i) uniform spatial scaling transformations, (ii) spatial affine transformations, (iii) Galilean transformations and (iv) temporal scaling transformations.
  By postulating that the family of receptive fields should be covariant under these classes of geometric image transformations, it follows that the receptive field shapes should be expanded over the degrees of freedom of the corresponding image transformations, to enable a formal matching between the receptive field responses computed under different viewing conditions for the same scene or for a structurally similar spatio-temporal event.
  We conclude the treatment by discussing and providing potential support for a working hypothesis that the receptive fields of simple cells in the primary visual cortex ought to be covariant under these classes of geometric image transformations, and thus have the shapes of their receptive fields expanded over the degrees of freedom of the corresponding geometric image transformations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02139v3</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tony Lindeberg</dc:creator>
    </item>
    <item>
      <title>Quantifying Mental States in Work Environment: Mathematical Perspectives</title>
      <link>https://arxiv.org/abs/2509.12162</link>
      <description>arXiv:2509.12162v2 Announce Type: replace 
Abstract: We introduce a novel framework for quantifying mental and emotional states over time by combining virtual reality (VR) exposure with EEG recordings. Participants experienced a stress-inducing work scenario in VR, originally designed as a training tool for bank employees, providing a controlled proxy for high-stakes situations. This setup enables integration of subjective emotional self-assessments with objective neural data, from which an algorithm was efficiently used to infer emotional states. Building on these measurements, we propose possible mathematical models to capture the temporal dynamics of mental states, offering a quantitative approach to studying emotional processing and informing adaptive training in complex environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.12162v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aymen Balti, Assane Wade, Abdelatif Oujbara, M. A.,  Aziz-Alaoui, Hicham Bellarabi, Frederic Dutertre, Benjamin Ambrosio</dc:creator>
    </item>
    <item>
      <title>Multiscale dynamical characterization of cortical brain states: from synchrony to asynchrony</title>
      <link>https://arxiv.org/abs/2510.05815</link>
      <description>arXiv:2510.05815v2 Announce Type: replace 
Abstract: The cerebral cortex spontaneously displays different patterns of activity that evolve over time according to the brain state. Sleep, wakefulness, resting states, and attention are examples of a wide spectrum of physiological states that can be sustained by the same structural network. Furthermore, additional states are generated by drugs (e.g., different levels of anesthesia) or by pathological conditions (e.g., brain lesions, disorders of consciousness). While the significance of understanding brain states in relation to brain dynamics and behavior has become increasingly evident over the past two decades, a unified definition of brain states remains elusive. In this review, we focus on two extremes of this spectrum: synchronous versus asynchronous states. These functional states predominantly underlie unconsciousness and consciousness, respectively, although exceptions exist. Our aim is to integrate data from different levels into a multiscale understanding, ranging from local circuits to whole-brain dynamics, including properties such as cortical complexity, functional connectivity, synchronization, wave propagation, and excitatory-inhibitory balance that vary across states and characterize them. Experimental and clinical data, as well as computational models (at micro-, meso-, and macrocortical levels) associated with the discussed brain states, are made available to readers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.05815v2</guid>
      <category>q-bio.NC</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Maria V. Sanchez-Vives, Arnau Manasanch, Andrea Pigorini, Alessandro Arena, Alessandra Camassa, Bj{\o}rn Erik Juel, Leonardo Dalla Porta, Cristiano Capone, Chiara De Luca, Giulia De Bonis, Jennifer Goldman, Maria Sacha, Andrea Galluzzi, Antonio Pazienti, Ezequiel Mikulan, Johann F Storm, Pier Stanislao Paolucci, Marcello Massimini, Maurizio Mattia, Alain Destexhe</dc:creator>
    </item>
    <item>
      <title>Bayes or Heisenberg: Who(se) Rules?</title>
      <link>https://arxiv.org/abs/2510.13894</link>
      <description>arXiv:2510.13894v2 Announce Type: replace 
Abstract: Although quantum systems are generally described by quantum state vectors, we show that in certain cases their measurement processes can be reformulated as probabilistic equations expressed in terms of probabilistic state vectors. These probabilistic representations can, in turn, be approximated by the neural network dynamics of the Tensor Brain (TB) model.
  The Tensor Brain is a recently proposed framework for modeling perception and memory in the brain, providing a biologically inspired mechanism for efficiently integrating generated symbolic representations into reasoning processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.13894v2</guid>
      <category>q-bio.NC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>quant-ph</category>
      <pubDate>Fri, 24 Oct 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Volker Tresp, Hang Li, Federico Harjes, Yunpu Ma</dc:creator>
    </item>
  </channel>
</rss>
