<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.GN updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.GN</link>
    <description>econ.GN updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.GN" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 09 Jan 2026 05:00:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Constrained Assortment and Price Optimization under Generalized Nested Logit Models</title>
      <link>https://arxiv.org/abs/2601.04220</link>
      <description>arXiv:2601.04220v1 Announce Type: new 
Abstract: We study assortment and price optimization under the generalized nested logit (GNL) model, one of the most general and flexible modeling frameworks in discrete choice modeling. Despite its modeling advantages, optimization under GNL is highly challenging: even the pure assortment problem is NP-hard, and existing approaches rely on approximation schemes or are limited to simple cardinality constraints. In this paper, we develop the first exact and near-exact algorithms for constrained assortment and joint assortment--pricing optimization (JAP) under GNL. Our approach reformulates the problem into bilinear and exponential-cone convex programs and exploits convexity, concavity, and submodularity properties to generate strong cutting planes within a Branch-and-Cut framework (B\&amp;C). We further extend this framework to the mixed GNL (MGNL) model, capturing heterogeneous customer segments, and to JAP with discrete prices. For the continuous pricing case, we propose a near-exact algorithm based on piecewise-linear approximation (PWLA) that achieves arbitrarily high precision under general linear constraints. Extensive computational experiments demonstrate that our methods substantially outperform state-of-the-art approximation approaches in both solution quality and scalability. In particular, we are able to solve large-scale instances with up to 1000 products and 20 nests, and to obtain near-optimal solutions for continuous pricing problems with negligible optimality gaps. To the best of our knowledge, this work resolves several open problems in assortment and price optimization under GNL.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04220v1</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hoang Giang Pham, Tien Mai</dc:creator>
    </item>
    <item>
      <title>The Endogenous Grid Method for Epstein-Zin Preferences</title>
      <link>https://arxiv.org/abs/2601.04438</link>
      <description>arXiv:2601.04438v1 Announce Type: new 
Abstract: The endogenous grid method (EGM) accelerates dynamic programming by inverting the Euler equation, but it appears incompatible with Epstein-Zin preferences where the value function enters the Euler equation. This paper shows that a power transformation resolves the difficulty. The resulting algorithm requires no root-finding, achieves speed gains of one to two orders of magnitude over value function iteration, and improves accuracy by more than one order of magnitude. Holding accuracy constant, the speedup is two to three orders of magnitude. VFI and time iteration face a speed-accuracy tradeoff; EGM sidesteps it entirely.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04438v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Lujan</dc:creator>
    </item>
    <item>
      <title>Bimodal Bias against Chinese Scientists in the American Academy: Penalties for Men, Bonuses for Women</title>
      <link>https://arxiv.org/abs/2601.04580</link>
      <description>arXiv:2601.04580v1 Announce Type: new 
Abstract: Given the recent targeting of Chinese scientists by the Department of Justice and sizable contributions of Chinese scientists to American science, it is urgent to investigate the presence and the particulars of anti-Chinese discrimination in the American academy. Across a sample of all faculty in the top 100 departments of sociology, economics, chemistry, and physics in the United States, we show that female Chinese scientists comprise a much higher percentage of the female professoriate than male Chinese scientists in the male professoriate. Using an exact matching approach, we then find that male Chinese scientists suffer from a dramatic citation penalty but that female Chinese scientists enjoy a persistent citation bonus. On average, female Chinese scientists require fewer citations on average than non-Chinese women where male Chinese scientists require more citations than their non-Chinese counterparts to attain a tenure-track professorial job of a given prestige rating.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04580v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin Cook</dc:creator>
    </item>
    <item>
      <title>Global Inequalities in Clinical Trials Participation</title>
      <link>https://arxiv.org/abs/2601.04660</link>
      <description>arXiv:2601.04660v1 Announce Type: new 
Abstract: Clinical trials shape medical evidence and determine who gains access to experimental therapies. Whether participation in these trials reflects the global burden of disease remains unclear. Here we analyze participation inequality across more than 62,000 randomized controlled trials spanning 16 major disease categories from 2000 to 2024. Linking 36.8 million trial participants to country-level disease burden, we show that global inequality in clinical trial participation is overwhelmingly structured by country rather than disease. Country-level factors explain over 90% of variation in participation, whereas disease-specific effects contribute only marginally. Removing entire disease categories, including those traditionally considered underfunded, has little effect on overall inequality. Instead, participation is highly concentrated geographically, with a small group of countries enrolling a disproportionate share of participants across nearly all diseases. These patterns have persisted despite decades of disease-targeted funding and increasing alignment between research attention and disease burden within diseases. Our findings indicate that disease-vertical strategies alone cannot correct participation inequality. Reducing global inequities in clinical research requires horizontal investments in research capacity, health infrastructure, and governance that operate across disease domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04660v1</guid>
      <category>econ.GN</category>
      <category>cs.CE</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Wen Lou, Adri\'an A. D\'iaz-Faes, Jiangen He, Zhihao Liu, Vincent Larivi\`ere</dc:creator>
    </item>
    <item>
      <title>Optimally designing purpose and meaning at work</title>
      <link>https://arxiv.org/abs/2601.05005</link>
      <description>arXiv:2601.05005v1 Announce Type: new 
Abstract: Many workers value purpose and meaning in their jobs alongside income, and firms need to align these preferences with profit goals. This paper develops a dynamic model in which firms invest in purpose to enhance job meaning and motivate effort. Workers, who differ in productivity, choose both productive and socialization effort, gaining utility from income and meaning. Purpose accumulates over time through firm investment and interacts with socialization to generate meaning, which boosts productivity. Firms invest in purpose only insofar as it raises profits. We characterize the unique equilibrium, including steady state and transition dynamics. Meaning and purpose rise with the importance workers place on meaning and with firm's patience, but fall with depreciation and socialization costs. The relationship with workers' share of output is nonmonotonic. We also show that some intermediate level of heterogeneity in skills is best for performance. Compared to a worker-owned firm, profit-maximizing firms underinvest in purpose, highlighting a misalignment between firm incentives and worker preferences. The model provides insight into when and why firms adopt purpose-driven practices and underscores the role of diversity in fostering meaning at work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05005v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Antonio Cabrales, Esther Hauk</dc:creator>
    </item>
    <item>
      <title>Beyond Interaction Effects: Two Logics for Studying Population Inequalities</title>
      <link>https://arxiv.org/abs/2601.04223</link>
      <description>arXiv:2601.04223v1 Announce Type: cross 
Abstract: When sociologists and other social scientist ask whether the return to college differs by race and gender, they face a choice between two fundamentally different modes of inquiry. Traditional interaction models follow deductive logic: the researcher specifies which variables moderate effects and tests these hypotheses. Machine learning methods follow inductive logic: algorithms search across vast combinatorial spaces to discover patterns of heterogeneity. This article develops a framework for navigating between these approaches. We show that the choice between deduction and induction reflects a tradeoff between interpretability and flexibility, and we demonstrate through simulation when each approach excels. Our framework is particularly relevant for inequality research, where understanding how treatment effects vary across intersecting social subpopulation is substantively central.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04223v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Adel Daoud</dc:creator>
    </item>
    <item>
      <title>Towards a Sociology of Sociology: Inequality, Elitism, and Prestige in the Sociological Enterprise From 1970 to the Present</title>
      <link>https://arxiv.org/abs/2601.04579</link>
      <description>arXiv:2601.04579v1 Announce Type: cross 
Abstract: There is a science of science and an informal economics of economics, but there is not a cohesive sociology of sociology. We turn the central findings and theoretical lenses of the sociological tradition and the sociological study of stratification inward on sociology itself to investigate how sociology has changed since the 1970s. We link two bibliometric databases to trace diachronic relationships between PhD training and publication outcomes, both of which are understudied in the science of science and sociology of science. All of sociology's top 3 journals remained biased against alum of less prestigious PhD programs, and while most forms of bias in elite sociological publishing have ameliorated over time, the house bias of the American Journal of Sociology in favor PhD alumnae of UChicago has intensified.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.04579v1</guid>
      <category>physics.soc-ph</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gavin Cook</dc:creator>
    </item>
    <item>
      <title>Large language models can effectively convince people to believe conspiracies</title>
      <link>https://arxiv.org/abs/2601.05050</link>
      <description>arXiv:2601.05050v1 Announce Type: cross 
Abstract: Large language models (LLMs) have been shown to be persuasive across a variety of context. But it remains unclear whether this persuasive power advantages truth over falsehood, or if LLMs can promote misbeliefs just as easily as refuting them. Here, we investigate this question across three pre-registered experiments in which participants (N = 2,724 Americans) discussed a conspiracy theory they were uncertain about with GPT-4o, and the model was instructed to either argue against ("debunking") or for ("bunking") that conspiracy. When using a "jailbroken" GPT-4o variant with guardrails removed, the AI was as effective at increasing conspiracy belief as decreasing it. Concerningly, the bunking AI was rated more positively, and increased trust in AI, more than the debunking AI. Surprisingly, we found that using standard GPT-4o produced very similar effects, such that the guardrails imposed by OpenAI did little to revent the LLM from promoting conspiracy beliefs. Encouragingly, however, a corrective conversation reversed these newly induced conspiracy beliefs, and simply prompting GPT-4o to only use accurate information dramatically reduced its ability to increase conspiracy beliefs. Our findings demonstrate that LLMs possess potent abilities to promote both truth and falsehood, but that potential solutions may exist to help mitigate this risk.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05050v1</guid>
      <category>cs.AI</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas H. Costello, Kellin Pelrine, Matthew Kowal, Antonio A. Arechar, Jean-Fran\c{c}ois Godbout, Adam Gleave, David Rand, Gordon Pennycook</dc:creator>
    </item>
    <item>
      <title>How Human is AI? Examining the Impact of Emotional Prompts on Artificial and Human and Responsiveness</title>
      <link>https://arxiv.org/abs/2601.05104</link>
      <description>arXiv:2601.05104v1 Announce Type: cross 
Abstract: This research examines how the emotional tone of human-AI interactions shapes ChatGPT and human behavior. In a between-subject experiment, we asked participants to express a specific emotion while working with ChatGPT (GPT-4.0) on two tasks, including writing a public response and addressing an ethical dilemma. We found that compared to interactions where participants maintained a neutral tone, ChatGPT showed greater improvement in its answers when participants praised ChatGPT for its responses. Expressing anger towards ChatGPT also led to a higher albeit smaller improvement relative to the neutral condition, whereas blaming ChatGPT did not improve its answers. When addressing an ethical dilemma, ChatGPT prioritized corporate interests less when participants expressed anger towards it, while blaming increases its emphasis on protecting the public interest. Additionally, we found that people used more negative, hostile, and disappointing expressions in human-human communication after interactions during which participants blamed rather than praised for their responses. Together, our findings demonstrate that the emotional tone people apply in human-AI interactions not only shape ChatGPT's outputs but also carry over into subsequent human-human communication.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.05104v1</guid>
      <category>cs.CL</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florence Bernays (University of Zurich), Marco Henriques Pereira (University of Zurich), Jochen Menges (University of Zurich)</dc:creator>
    </item>
    <item>
      <title>Dynamics of Global Emission Permit Prices and Regional Social Cost of Carbon under Noncooperation</title>
      <link>https://arxiv.org/abs/2312.15563</link>
      <description>arXiv:2312.15563v4 Announce Type: replace 
Abstract: We develop a dynamic multi-region climate-economy model with emissions trading and solve for the dynamic Nash equilibrium under noncooperation, where each region follows Paris Agreement-based emissions caps. The permit price reaches $923 per ton of carbon by 2050, and global temperature rises to 1.7 degrees Celsius above pre-industrial levels by 2100. The regional social cost of carbon equals the difference between regional marginal abatement cost and the permit price, highlighting complementarity between carbon taxes and trading. We find substantial heterogeneity in regional social costs of carbon, show that lax caps can raise emissions, and demonstrate strong free-rider incentives under partial participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.15563v4</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongyang Cai, Khyati Malik, Hyeseon Shin</dc:creator>
    </item>
    <item>
      <title>When Indemnity Insurance Fails: Parametric Coverage under Binding Budget and Risk Constraints</title>
      <link>https://arxiv.org/abs/2512.21973</link>
      <description>arXiv:2512.21973v3 Announce Type: replace 
Abstract: In high-risk environments, traditional indemnity insurance is often unaffordable or ineffective, despite its well-known optimality under expected utility. We compare excess-of-loss indemnity insurance with parametric insurance within a common mean-variance framework, allowing for fixed costs, heterogeneous premium loadings, and binding budget constraints. We show that, once these realistic frictions are introduced, parametric insurance can yield higher welfare for risk-averse individuals, even under the same utility objective. The welfare advantage arises precisely when indemnity insurance becomes impractical, and disappears once both contracts are unconstrained. Our results help reconcile classical insurance theory with the growing use of parametric risk transfer in high-risk settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.21973v3</guid>
      <category>econ.GN</category>
      <category>math.OC</category>
      <category>q-fin.EC</category>
      <category>q-fin.RM</category>
      <pubDate>Fri, 09 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Avanzi, Debbie Kusch Falden, Mogens Steffensen</dc:creator>
    </item>
  </channel>
</rss>
