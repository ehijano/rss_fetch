<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.HC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.HC</link>
    <description>cs.HC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.HC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 16 Oct 2024 02:04:26 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>"ChatGPT, Don't Tell Me What to Do": Designing AI for Context Analysis in Humanitarian Frontline Negotiations</title>
      <link>https://arxiv.org/abs/2410.09139</link>
      <description>arXiv:2410.09139v1 Announce Type: new 
Abstract: Frontline humanitarian negotiators are increasingly exploring ways to use AI tools in their workflows. However, current AI-tools in negotiation primarily focus on outcomes, neglecting crucial aspects of the negotiation process. Through iterative co-design with experienced frontline negotiators (n=32), we found that flexible tools that enable contextualizing cases and exploring options (with associated risks) are more effective than those providing direct recommendations of negotiation strategies. Surprisingly, negotiators demonstrated tolerance for occasional hallucinations and biases of AI. Our findings suggest that the design of AI-assisted negotiation tools should build on practitioners' existing practices, such as weighing different compromises and validating information with peers. This approach leverages negotiators' expertise while enhancing their decision-making capabilities. We call for technologists to learn from and collaborate closely with frontline negotiators, applying these insights to future AI designs and jointly developing professional guidelines for AI use in humanitarian negotiations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09139v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>ZIlin Ma, Yiyang Mei, Claude Bruderlein, Krzysztof Z. Gajos, Weiwei Pan</dc:creator>
    </item>
    <item>
      <title>SituFont: A Just-in-Time Adaptive Intervention System for Enhancing Mobile Readability in Situational Visual Impairments</title>
      <link>https://arxiv.org/abs/2410.09562</link>
      <description>arXiv:2410.09562v1 Announce Type: new 
Abstract: Situational visual impairments (SVIs) significantly impact mobile readability, causing user discomfort and hindering information access. This paper introduces SituFont, a novel just-in-time adaptive intervention (JITAI) system designed to enhance mobile text readability by semi-automatically adjusting font parameters in response to real-time contextual changes. Leveraging smartphone sensors and a human-in-the-loop approach, SituFont personalizes the reading experience by adapting to individual user preferences, including personal factors such as fatigue and distraction level, and environmental factors like lighting, motion, and location. To inform the design of SituFont, we conducted formative interviews (N=15) to identify key SVI factors affecting readability and controlled experiments (N=18) to quantify the relationship between these factors and optimal text parameters. We then evaluated SituFont's effectiveness through a comparative user study under eight simulated SVI scenarios (N=12), demonstrating its ability to overcome SVIs. Our findings highlight the potential of JITAI systems like SituFont to mitigate the impact of SVIs and enhance mobile accessibility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09562v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Kun Yue, Mingshan Zhang, Jingruo Chen, Chun Yu, Kexin Nie, Zhiqi Gao, Jinghan Yang, Chen Liang, Yuanchun Shi</dc:creator>
    </item>
    <item>
      <title>Making Beshbarmak: Games for Central Asian Cultural Heritage</title>
      <link>https://arxiv.org/abs/2410.09670</link>
      <description>arXiv:2410.09670v1 Announce Type: new 
Abstract: This paper introduces "Making Beshbarmak", an interactive cooking game that celebrates the nomadic ancestry and cultural heritage of Central Asian communities worldwide. Designed to promote cultural appreciation and identity formation, the game invites players to learn and recreate the traditional dish Beshbarmak through an engaging step-by-step process, incorporating storytelling elements that explain the cultural significance of the meal. Our project contributes to digital cultural heritage and games research by offering an accessible, open-source prototype on p5.js, enabling users to connect with and explore Central Asian traditions. "Making Beshbarmak" serves as both an educational tool and a platform for cultural preservation, fostering a sense of belonging among Central Asian immigrant populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09670v1</guid>
      <category>cs.HC</category>
      <category>cs.MM</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Amina Kobenova, Adina Kaiymova</dc:creator>
    </item>
    <item>
      <title>"I inherently just trust that it works": Investigating Mental Models of Open-Source Libraries for Differential Privacy</title>
      <link>https://arxiv.org/abs/2410.09721</link>
      <description>arXiv:2410.09721v1 Announce Type: new 
Abstract: Differential privacy (DP) is a promising framework for privacy-preserving data science, but recent studies have exposed challenges in bringing this theoretical framework for privacy into practice. These tensions are particularly salient in the context of open-source software libraries for DP data analysis, which are emerging tools to help data stewards and analysts build privacy-preserving data pipelines for their applications. While there has been significant investment into such libraries, we need further inquiry into the role of these libraries in promoting understanding of and trust in DP, and in turn, the ways in which design of these open-source libraries can shed light on the challenges of creating trustworthy data infrastructures in practice. In this study, we use qualitative methods and mental models approaches to analyze the differences between conceptual models used to design open-source DP libraries and mental models of DP held by users. Through a two-stage study design involving formative interviews with 5 developers of open-source DP libraries and user studies with 17 data analysts, we find that DP libraries often struggle to bridge the gaps between developer and user mental models. In particular, we highlight the tension DP libraries face in maintaining rigorous DP implementations and facilitating user interaction. We conclude by offering practical recommendations for further development of DP libraries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09721v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3687011</arxiv:DOI>
      <dc:creator>Patrick Song, Jayshree Sarathy, Michael Shoemate, Salil Vadhan</dc:creator>
    </item>
    <item>
      <title>"I think you need help! Here's why": Understanding the Effect of Explanations on Automatic Facial Expression Recognition</title>
      <link>https://arxiv.org/abs/2410.09743</link>
      <description>arXiv:2410.09743v1 Announce Type: new 
Abstract: Facial expression recognition (FER) has emerged as a promising approach to the development of emotion-aware intelligent systems. The performance of FER in multiple domains is continuously being improved, especially through advancements in data-driven learning approaches. However, a key challenge remains in utilizing FER in real-world contexts, namely ensuring user understanding of these systems and establishing a suitable level of user trust towards this technology. We conducted an empirical user study to investigate how explanations of FER can improve trust, understanding and performance in a human-computer interaction task that uses FER to trigger helpful hints during a navigation game. Our results showed that users provided with explanations of the FER system demonstrated improved control in using the system to their advantage, leading to a significant improvement in their understanding of the system, reduced collisions in the navigation game, as well as increased trust towards the system.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09743v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sanjeev Nahulanthran, Mor Vered, Leimin Tian, Dana Kuli\'c</dc:creator>
    </item>
    <item>
      <title>EEG-based AI-BCI Wheelchair Advancement: A Brain-Computer Interfacing Wheelchair System Using Machine Learning Mechanism with Right and Left Voluntary Hand Movement</title>
      <link>https://arxiv.org/abs/2410.09763</link>
      <description>arXiv:2410.09763v1 Announce Type: new 
Abstract: This paper presents an Artificial Intelligence (AI) integrated novel approach to Brain-Computer Interface (BCI)-based wheelchair development, utilizing a voluntary Right Left Hand Movement mechanism for control. The system is designed to simulate wheelchair navigation based on voluntary right and left-hand movements using electroencephalogram (EEG) data. A pre-filtered dataset, obtained from an open-source EEG repository, was segmented into arrays of 19x200 to capture the onset of hand movements. The data was acquired at a sampling frequency 200Hz in the laboratory experiment. The system integrates a Tkinter-based interface for simulating wheelchair movements, offering users a functional and intuitive control system. Various machine learning models, including Support Vector Machines (SVM), XGBoost, random forest, and a Bi-directional Long Short-Term Memory (Bi-LSTM) attention-based model, were developed. The random forest model obtained 79% accuracy. Great performance was seen on the Logistic Regression model which outperforms other models with 92% accuracy and 91% accuracy on the Multi-Layer Perceptron (MLP) model. The Bi-LSTM attention-based model achieved a mean accuracy of 86% through cross-validation, showcasing the potential of attention mechanisms in BCI applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09763v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Biplov Paneru, Bishwash Paneru, Khem Narayan Poudyal</dc:creator>
    </item>
    <item>
      <title>LibEER: A Comprehensive Benchmark and Algorithm Library for EEG-based Emotion Recognition</title>
      <link>https://arxiv.org/abs/2410.09767</link>
      <description>arXiv:2410.09767v1 Announce Type: new 
Abstract: EEG-based emotion recognition (EER) is garnering increasing attention due to its potential in understanding and analyzing human emotions. Recently, significant advancements have been achieved using various deep learning-based techniques to address the EER problem. However, the absence of a convincing benchmark and open-source codebase complicates fair comparisons between different models and poses reproducibility challenges for practitioners. These issues considerably impede progress in this field. In light of this, we propose a comprehensive benchmark and algorithm library (LibEER) for fair comparisons in EER by making most of the implementation details of different methods consistent and using the same single codebase in PyTorch. In response to these challenges, we propose LibEER, a comprehensive benchmark and algorithm library for fair comparisons in EER, by ensuring consistency in the implementation details of various methods and utilizing a single codebase in PyTorch. LibEER establishes a unified evaluation framework with standardized experimental settings, enabling unbiased evaluations of over ten representative deep learning-based EER models across the four most commonly used datasets. Additionally, we conduct an exhaustive and reproducible comparison of the performance and efficiency of popular models, providing valuable insights for researchers in selecting and designing EER models. We aspire for our work to not only lower the barriers for beginners entering the field of EEG-based emotion recognition but also promote the standardization of research in this domain, thereby fostering steady development. The source code is available at \url{https://github.com/ButterSen/LibEER}.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09767v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Liu, Shusen Yang, Yuzhe Zhang, Mengze Wang, Fanyu Gong, Chengxi Xie, Guanjian Liu, Dalin Zhang</dc:creator>
    </item>
    <item>
      <title>HypomimiaCoach: An AU-based Digital Therapy System for Hypomimia Detection &amp; Rehabilitation with Parkinson's Disease</title>
      <link>https://arxiv.org/abs/2410.09772</link>
      <description>arXiv:2410.09772v1 Announce Type: new 
Abstract: Hypomimia is a non-motor symptom of Parkinson's disease that manifests as delayed facial movements and expressions, along with challenges in articulation and emotion. Currently, subjective evaluation by neurologists is the primary method for hypomimia detection, and conventional rehabilitation approaches heavily rely on verbal prompts from rehabilitation physicians. There remains a deficiency in accessible, user-friendly and scientifically rigorous assistive tools for hypomimia treatments. To investigate this, we developed HypomimaCoach, an Action Unit (AU)-based digital therapy system for hypomimia detection and rehabilitation in Parkinson's disease. The HypomimaCoach system was designed to facilitate engagement through the incorporation of both relaxed and controlled rehabilitation exercises, while also stimulating initiative through the integration of digital therapies that incorporated traditional face training methods. We extract action unit(AU) features and their relationship for hypomimia detection. In order to facilitate rehabilitation, a series of training programmes have been devised based on the Action Units (AUs) and patients are provided with real-time feedback through an additional AU recognition model, which guides them through their training routines. A pilot study was conducted with seven participants in China, all of whom exhibited symptoms of Parkinson's disease hypomimia. The results of the pilot study demonstrated a positive impact on participants' self-efficacy, with favourable feedback received. Furthermore, physician evaluations validated the system's applicability in a therapeutic setting for patients with Parkinson's disease, as well as its potential value in clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09772v1</guid>
      <category>cs.HC</category>
      <category>cs.AI</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingjing Xu, Xueyan Cai, Zihong Zhou, Mengru Xue, Bo Wang, Haotian Wang, Zhengke Li, Chentian Weng, Wei Luo, Cheng Yao, Bo Lin, Jianwei Yin</dc:creator>
    </item>
    <item>
      <title>Look-and-Twist: A Simple Selection Method for Virtual and Augmented Reality</title>
      <link>https://arxiv.org/abs/2410.09820</link>
      <description>arXiv:2410.09820v1 Announce Type: new 
Abstract: This paper introduces a novel interaction method for virtual and augmented reality called look-and-twist, which is directly analogous to point-and-click operations using a mouse and desktop. It is based on head rotation alone and is straightforward to implement on any head mounted display that performs rotational tracking. A user selects features of interest by turning their head to face an object, and then performs a specified rotation along the axis of the looking direction. The look-and-twist method has been implemented and tested in an educational context, and systematic user studies are underway. Early evidence indicates that the method is comparable to, or faster than, the standard dwell time method. The method can be used, for example, with Google Cardboard, and it is straightforward to learn for inexperienced users. Moreover, it has the potential to significantly enrich VR interactions by providing an additional degree of freedom of control, which the binary nature of dwell-based methods lacks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09820v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Yershova, Elmeri Uotila, Katherine J. Mimnaugh, Nicoletta Prencipe, M. Manivannan, Timo Ojala, Steven M. LaValle</dc:creator>
    </item>
    <item>
      <title>Beyond the "Industry Standard": Focusing Gender-Affirming Voice Training Technologies on Individualized Goal Exploration</title>
      <link>https://arxiv.org/abs/2410.09958</link>
      <description>arXiv:2410.09958v1 Announce Type: new 
Abstract: Gender-affirming voice training is critical for the transition process for many transgender individuals, enabling their voice to align with their gender identity. Individualized voice goals guide and motivate the voice training journey, but existing voice training technologies fail to define clear goals. We interviewed six voice experts and ten transgender individuals with voice training experience (voice trainees), focusing on how they defined, triangulated, and used voice goals. We found that goal voice exploration involves navigation between approximate and clear goals, and continuous reevaluation throughout the voice training journey. Our study reveals how voice examples, character descriptions, and voice modification and training technologies inform goal exploration, and identifies risks of overemphasizing goals. We identified technological implications informed by the separation of voice goals and targets, and provide a framework for a voice-changer-based goal exploration tool based on brainstorming with trainees and experts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09958v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kassie Povinelli, Hanxiu "Hazel" Zhu, Yuhang Zhao</dc:creator>
    </item>
    <item>
      <title>Evaluating Effectiveness of Interactivity in Contour-based Geospatial Visualizations</title>
      <link>https://arxiv.org/abs/2410.10032</link>
      <description>arXiv:2410.10032v1 Announce Type: new 
Abstract: Contour maps are an essential tool for exploring spatial features of the terrain, such as distance, directions, and surface gradient among the contour areas. User interactions in contour-based visualizations create approaches to visual analysis that are noticeably different from the perspective of human cognition. As such, various interactive approaches have been introduced to improve system usability and enhance human cognition for complex and large-scale spatial data exploration. However, what user interaction means for contour maps, its purpose, when to leverage, and design primitives have yet to be investigated in the context of analysis tasks. Therefore, further research is needed to better understand and quantify the potentials and benefits offered by user interactions in contour-based geospatial visualizations designed to support analytical tasks. In this paper, we present a contour-based interactive geospatial visualization designed for analytical tasks. We conducted a crowd-sourced user study (N=62) to examine the impact of interactive features on analysis using contour-based geospatial visualizations. Our results show that the interactive features aid in their data analysis and understanding in terms of spatial data extent, map layout, task complexity, and user expertise. Finally, we discuss our findings in-depth, which will serve as guidelines for future design and implementation of interactive features in support of case-specific analytical tasks on contour-based geospatial views.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10032v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Abdullah-Al-Raihan Nayeem, Dongyun Han, William J. Tolone, Isaac Cho</dc:creator>
    </item>
    <item>
      <title>Tracing Human Stress from Physiological Signals using UWB Radar</title>
      <link>https://arxiv.org/abs/2410.10155</link>
      <description>arXiv:2410.10155v1 Announce Type: new 
Abstract: Stress tracing is an important research domain that supports many applications, such as health care and stress management; and its closest related works are derived from stress detection. However, these existing works cannot well address two important challenges facing stress detection. First, most of these studies involve asking users to wear physiological sensors to detect their stress states, which has a negative impact on the user experience. Second, these studies have failed to effectively utilize multimodal physiological signals, which results in less satisfactory detection results. This paper formally defines the stress tracing problem, which emphasizes the continuous detection of human stress states. A novel deep stress tracing method, named DST, is presented. Note that DST proposes tracing human stress based on physiological signals collected by a noncontact ultrawideband radar, which is more friendly to users when collecting their physiological signals. In DST, a signal extraction module is carefully designed at first to robustly extract multimodal physiological signals from the raw RF data of the radar, even in the presence of body movement. Afterward, a multimodal fusion module is proposed in DST to ensure that the extracted multimodal physiological signals can be effectively fused and utilized. Extensive experiments are conducted on three real-world datasets, including one self-collected dataset and two publicity datasets. Experimental results show that the proposed DST method significantly outperforms all the baselines in terms of tracing human stress states. On average, DST averagely provides a 6.31% increase in detection accuracy on all datasets, compared with the best baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10155v1</guid>
      <category>cs.HC</category>
      <category>cs.AR</category>
      <category>cs.LG</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jia Xu, Teng Xiao, Pin Lv, Zhe Chen, Chao Cai, Yang Zhang, Zehui Xiong</dc:creator>
    </item>
    <item>
      <title>Structuring data analysis projects in the Open Science era with Kerblam!</title>
      <link>https://arxiv.org/abs/2410.10513</link>
      <description>arXiv:2410.10513v1 Announce Type: new 
Abstract: Structuring data analysis projects, that is, defining the layout of files and folders needed to analyze data using existing tools and novel code, largely follows personal preferences. In this work, we look at the structure of several data analysis project templates and find little structural overlap. We highlight the parts that are similar between them, and propose guiding principles to keep in mind when one wishes to create a new data analysis project. Finally, we present Kerblam!, a project management tool that can expedite project data management, execution of workflow managers, and sharing of the resulting workflow and analysis outputs. We hope that, by following these principles and using Kerblam!, the landscape of data analysis projects can become more transparent, understandable, and ultimately useful to the wider community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10513v1</guid>
      <category>cs.HC</category>
      <category>q-bio.OT</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Visentin, Luca Munaron, Federico Alessandro Ruffinatti</dc:creator>
    </item>
    <item>
      <title>Users' Perception on Appropriateness of Robotic Coaching Assistant's Disclosure Behaviors</title>
      <link>https://arxiv.org/abs/2410.10550</link>
      <description>arXiv:2410.10550v1 Announce Type: new 
Abstract: Social robots have emerged as valuable contributors to individuals' well-being coaching. Notably, their integration into long-term human coaching trials shows particular promise, emphasizing a complementary role alongside human coaches rather than outright replacement. In this context, robots serve as supportive entities during coaching sessions, offering insights based on their knowledge about users' well-being and activity. Traditionally, such insights have been gathered through methods like written self-reports or wearable data visualizations. However, the disclosure of people's information by a robot raises concerns regarding privacy, appropriateness, and trust. To address this, we conducted an initial study with [n = 22] participants to quantify their perceptions of privacy regarding disclosures made by a robot coaching assistant. The study was conducted online, presenting participants with six prerecorded scenarios illustrating various types of information disclosure and the robot's role, ranging from active on-demand to proactive communication conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10550v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven</dc:creator>
    </item>
    <item>
      <title>Mindalogue: LLM-Powered Nonlinear Interaction for Effective Learning and Task Exploration</title>
      <link>https://arxiv.org/abs/2410.10570</link>
      <description>arXiv:2410.10570v2 Announce Type: new 
Abstract: Current generative AI models like ChatGPT, Claude, and Gemini are widely used for knowledge dissemination, task decomposition, and creative thinking. However, their linear interaction methods often force users to repeatedly compare and copy contextual information when handling complex tasks, increasing cognitive load and operational costs. Moreover, the ambiguity in model responses requires users to refine and simplify the information further. To address these issues, we developed "Mindalogue", a system using a non-linear interaction model based on "nodes + canvas" to enhance user efficiency and freedom while generating structured responses. A formative study with 11 users informed the design of Mindalogue, which was then evaluated through a study with 16 participants. The results showed that Mindalogue significantly reduced task steps and improved users' comprehension of complex information. This study highlights the potential of non-linear interaction in improving AI tool efficiency and user experience in the HCI field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10570v2</guid>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rui Zhang, Ziyao Zhang, Fengliang Zhu, Jiajie Zhou, Anyi Rao</dc:creator>
    </item>
    <item>
      <title>Development of a 3D virtual world tool for sustainable energy education</title>
      <link>https://arxiv.org/abs/2410.10586</link>
      <description>arXiv:2410.10586v1 Announce Type: new 
Abstract: The UNESCO (2022) points out that the gap between the existing awareness of a person or a community, and the actual habits of everyday life, is attributed to: low levels of understanding the environmental issues at stake; low levels of knowledge regarding energy and climate issues; and lack of attention to social, emotional or behavioral learning. In this context, as environmental and energetic concerns grow rapidly in our daily lives, RAISE - Raising environmental knowledge &amp; awareness, an ERAMUS+ project with partners from three countries (Greece, Portugal, and Italy), was born. The project's first and main objective is to increase the environmental knowledge and awareness (EK&amp;A) of schoolchildren. To achieve this goal, a survey was first carried out to ascertain the students' knowledge and, based on these results, scenarios and educational material were drawn up to build a pedagogical tool - a 3D Virtual World Learning Environment (3D VWLE). This paper presents the desktop research results and the main features of the game that are intended to match the identified educational needs. The 3D VWLE puts students in situations where they can acquire transferable skills. The project demonstrated that the simulation effect of 3D VWLE on skills provides an excellent online learning environment for students to work on and improve their abilities. In other words, virtual worlds open the door to a new way of learning and teaching.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10586v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Guerra-Mota, Dimosthenis Minas, Michalis Xenos, Maria Manuel Sa</dc:creator>
    </item>
    <item>
      <title>Functional Flexibility in Generative AI Interfaces: Text Editing with LLMs through Conversations, Toolbars, and Prompts</title>
      <link>https://arxiv.org/abs/2410.10644</link>
      <description>arXiv:2410.10644v1 Announce Type: new 
Abstract: Prompting-based user interfaces (UIs) shift the task of defining and accessing relevant functions from developers to users. However, how UIs shape this flexibility has not yet been investigated explicitly. We explored interaction with Large Language Models (LLMs) over four years, before and after the rise of general-purpose LLMs: (1) Our survey (N=121) elicited how users envision to delegate writing tasks to AI. This informed a conversational UI design. (2) A user study (N=10) revealed that people regressed to using short command-like prompts. (3) When providing these directly as shortcuts in a toolbar UI, in addition to prompting, users in our second study (N=12) dynamically switched between specified and flexible AI functions. We discuss functional flexibility as a new theoretical construct and thinking tool. Our work highlights the value of moving beyond conversational UIs, by considering how different UIs shape users' access to the functional space of generative AI models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10644v1</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Lehmann, Daniel Buschek</dc:creator>
    </item>
    <item>
      <title>Human-Robot Kinaesthetic Interaction Based on Free Energy Principle</title>
      <link>https://arxiv.org/abs/2303.15213</link>
      <description>arXiv:2303.15213v1 Announce Type: cross 
Abstract: The current study investigated possible human-robot kinaesthetic interaction using a variational recurrent neural network model, called PV-RNN, which is based on the free energy principle. Our prior robotic studies using PV-RNN showed that the nature of interactions between top-down expectation and bottom-up inference is strongly affected by a parameter, called the meta-prior, which regulates the complexity term in free energy.The study also compares the counter force generated when trained transitions are induced by a human experimenter and when untrained transitions are induced. Our experimental results indicated that (1) the human experimenter needs more/less force to induce trained transitions when $w$ is set with larger/smaller values, (2) the human experimenter needs more force to act on the robot when he attempts to induce untrained as opposed to trained movement pattern transitions. Our analysis of time development of essential variables and values in PV-RNN during bodily interaction clarified the mechanism by which gaps in actional intentions between the human experimenter and the robot can be manifested as reaction forces between them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.15213v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hiroki Sawada, Wataru Ohata, Jun Tani</dc:creator>
    </item>
    <item>
      <title>ReasonPlanner: Enhancing Autonomous Planning in Dynamic Environments with Temporal Knowledge Graphs and LLMs</title>
      <link>https://arxiv.org/abs/2410.09252</link>
      <description>arXiv:2410.09252v1 Announce Type: cross 
Abstract: Planning and performing interactive tasks, such as conducting experiments to determine the melting point of an unknown substance, is straightforward for humans but poses significant challenges for autonomous agents. We introduce ReasonPlanner, a novel generalist agent designed for reflective thinking, planning, and interactive reasoning. This agent leverages LLMs to plan hypothetical trajectories by building a World Model based on a Temporal Knowledge Graph. The agent interacts with the environment using a natural language actor-critic module, where the actor translates the imagined trajectory into a sequence of actionable steps, and the critic determines if replanning is necessary. ReasonPlanner significantly outperforms previous state-of-the-art prompting-based methods on the ScienceWorld benchmark by more than 1.8 times, while being more sample-efficient and interpretable. It relies solely on frozen weights thus requiring no gradient updates. ReasonPlanner can be deployed and utilized without specialized knowledge of Machine Learning, making it accessible to a wide range of users.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09252v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford</dc:creator>
    </item>
    <item>
      <title>One Step at a Time: Combining LLMs and Static Analysis to Generate Next-Step Hints for Programming Tasks</title>
      <link>https://arxiv.org/abs/2410.09268</link>
      <description>arXiv:2410.09268v1 Announce Type: cross 
Abstract: Students often struggle with solving programming problems when learning to code, especially when they have to do it online, with one of the most common disadvantages of working online being the lack of personalized help. This help can be provided as next-step hint generation, i.e., showing a student what specific small step they need to do next to get to the correct solution. There are many ways to generate such hints, with large language models (LLMs) being among the most actively studied right now.
  While LLMs constitute a promising technology for providing personalized help, combining them with other techniques, such as static analysis, can significantly improve the output quality. In this work, we utilize this idea and propose a novel system to provide both textual and code hints for programming tasks. The pipeline of the proposed approach uses a chain-of-thought prompting technique and consists of three distinct steps: (1) generating subgoals - a list of actions to proceed with the task from the current student's solution, (2) generating the code to achieve the next subgoal, and (3) generating the text to describe this needed action. During the second step, we apply static analysis to the generated code to control its size and quality. The tool is implemented as a modification to the open-source JetBrains Academy plugin, supporting students in their in-IDE courses.
  To evaluate our approach, we propose a list of criteria for all steps in our pipeline and conduct two rounds of expert validation. Finally, we evaluate the next-step hints in a classroom with 14 students from two universities. Our results show that both forms of the hints - textual and code - were helpful for the students, and the proposed system helped them to proceed with the coding tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09268v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3699538.3699556</arxiv:DOI>
      <dc:creator>Anastasiia Birillo, Elizaveta Artser, Anna Potriasaeva, Ilya Vlasov, Katsiaryna Dzialets, Yaroslav Golubev, Igor Gerasimov, Hieke Keuning, Timofey Bryksin</dc:creator>
    </item>
    <item>
      <title>SimBrainNet: Evaluating Brain Network Similarity for Attention Disorders</title>
      <link>https://arxiv.org/abs/2410.09422</link>
      <description>arXiv:2410.09422v1 Announce Type: cross 
Abstract: Electroencephalography (EEG)-based attention disorder research seeks to understand brain activity patterns associated with attention. Previous studies have mainly focused on identifying brain regions involved in cognitive processes or classifying Attention-Deficit Hyperactivity Disorder (ADHD) and control subjects. However, analyzing effective brain connectivity networks for specific attentional processes and comparing them has not been explored. Therefore, in this study, we propose multivariate transfer entropy-based connectivity networks for cognitive events and introduce a new similarity measure, 'SimBrainNet', to assess these networks. A high similarity score suggests similar brain dynamics during cognitive events, indicating less attention variability. Our experiment involves 12 individuals with attention disorders (7 children and 5 adolescents). Noteworthy that child participants exhibit lower similarity scores compared to adolescents, indicating greater changes in attention. We found strong connectivity patterns in the left pre-frontal cortex for adolescent individuals compared to the child. Our study highlights the changes in attention levels across various cognitive events, offering insights into the underlying cognitive mechanisms, brain dynamics, and potential deficits in individuals with this disorder.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09422v1</guid>
      <category>q-bio.NC</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Debashis Das Chakladar, Foteini Simistira Liwicki, Rajkumar Saini</dc:creator>
    </item>
    <item>
      <title>Bridging Text and Image for Artist Style Transfer via Contrastive Learning</title>
      <link>https://arxiv.org/abs/2410.09566</link>
      <description>arXiv:2410.09566v1 Announce Type: cross 
Abstract: Image style transfer has attracted widespread attention in the past few years. Despite its remarkable results, it requires additional style images available as references, making it less flexible and inconvenient. Using text is the most natural way to describe the style. More importantly, text can describe implicit abstract styles, like styles of specific artists or art movements. In this paper, we propose a Contrastive Learning for Artistic Style Transfer (CLAST) that leverages advanced image-text encoders to control arbitrary style transfer. We introduce a supervised contrastive training strategy to effectively extract style descriptions from the image-text model (i.e., CLIP), which aligns stylization with the text description. To this end, we also propose a novel and efficient adaLN based state space models that explore style-content fusion. Finally, we achieve a text-driven image style transfer. Extensive experiments demonstrate that our approach outperforms the state-of-the-art methods in artistic style transfer. More importantly, it does not require online fine-tuning and can render a 512x512 image in 0.03s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09566v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhi-Song Liu, Li-Wen Wang, Jun Xiao, Vicky Kalogeiton</dc:creator>
    </item>
    <item>
      <title>Stability and Transparency in Mixed Reality Bilateral Human Teleoperation</title>
      <link>https://arxiv.org/abs/2410.09679</link>
      <description>arXiv:2410.09679v1 Announce Type: cross 
Abstract: Recent work introduced the concept of human teleoperation (HT), where the remote robot typically considered in conventional bilateral teleoperation is replaced by a novice person wearing a mixed reality head mounted display and tracking the motion of a virtual tool controlled by an expert. HT has advantages in cost, complexity, and patient acceptance for telemedicine in low-resource communities or remote locations. However, the stability, transparency, and performance of bilateral HT are unexplored. In this paper, we therefore develop a mathematical model and simulation of the HT system using test data. We then analyze various control architectures with this model and implement them with the HT system to find the achievable performance, investigate stability, and determine the most promising teleoperation scheme in the presence of time delays. We show that instability in HT, while not destructive or dangerous, makes the system impossible to use. However, stable and transparent teleoperation are possible with small time delays (&lt;200 ms) through 3-channel teleoperation, or with large time delays through model-mediated teleoperation with local pose and force feedback for the novice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09679v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Gregory Black, Septimiu Salcudean</dc:creator>
    </item>
    <item>
      <title>Dreaming to Assist: Learning to Align with Human Objectives for Shared Control in High-Speed Racing</title>
      <link>https://arxiv.org/abs/2410.10062</link>
      <description>arXiv:2410.10062v1 Announce Type: cross 
Abstract: Tight coordination is required for effective human-robot teams in domains involving fast dynamics and tactical decisions, such as multi-car racing. In such settings, robot teammates must react to cues of a human teammate's tactical objective to assist in a way that is consistent with the objective (e.g., navigating left or right around an obstacle). To address this challenge, we present Dream2Assist, a framework that combines a rich world model able to infer human objectives and value functions, and an assistive agent that provides appropriate expert assistance to a given human teammate. Our approach builds on a recurrent state space model to explicitly infer human intents, enabling the assistive agent to select actions that align with the human and enabling a fluid teaming interaction. We demonstrate our approach in a high-speed racing domain with a population of synthetic human drivers pursuing mutually exclusive objectives, such as "stay-behind" and "overtake". We show that the combined human-robot team, when blending its actions with those of the human, outperforms the synthetic humans alone as well as several baseline assistance strategies, and that intent-conditioning enables adherence to human preferences during task execution, leading to improved performance while satisfying the human's objective.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10062v1</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan DeCastro, Andrew Silva, Deepak Gopinath, Emily Sumner, Thomas M. Balch, Laporsha Dees, Guy Rosman</dc:creator>
    </item>
    <item>
      <title>NeRF-enabled Analysis-Through-Synthesis for ISAR Imaging of Small Everyday Objects with Sparse and Noisy UWB Radar Data</title>
      <link>https://arxiv.org/abs/2410.10085</link>
      <description>arXiv:2410.10085v1 Announce Type: cross 
Abstract: Inverse Synthetic Aperture Radar (ISAR) imaging presents a formidable challenge when it comes to small everyday objects due to their limited Radar Cross-Section (RCS) and the inherent resolution constraints of radar systems. Existing ISAR reconstruction methods including backprojection (BP) often require complex setups and controlled environments, rendering them impractical for many real-world noisy scenarios. In this paper, we propose a novel Analysis-through-Synthesis (ATS) framework enabled by Neural Radiance Fields (NeRF) for high-resolution coherent ISAR imaging of small objects using sparse and noisy Ultra-Wideband (UWB) radar data with an inexpensive and portable setup. Our end-to-end framework integrates ultra-wideband radar wave propagation, reflection characteristics, and scene priors, enabling efficient 2D scene reconstruction without the need for costly anechoic chambers or complex measurement test beds. With qualitative and quantitative comparisons, we demonstrate that the proposed method outperforms traditional techniques and generates ISAR images of complex scenes with multiple targets and complex structures in Non-Line-of-Sight (NLOS) and noisy scenarios, particularly with limited number of views and sparse UWB radar scans. This work represents a significant step towards practical, cost-effective ISAR imaging of small everyday objects, with broad implications for robotics and mobile sensing applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10085v1</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Farhan Tasnim Oshim, Albert Reed, Suren Jayasuriya, Tauhidur Rahman</dc:creator>
    </item>
    <item>
      <title>Causal Modeling of Climate Activism on Reddit</title>
      <link>https://arxiv.org/abs/2410.10562</link>
      <description>arXiv:2410.10562v1 Announce Type: cross 
Abstract: Climate activism is crucial in stimulating collective societal and behavioral change towards sustainable practices through political pressure. Although multiple factors contribute to the participation in activism, their complex relationships and the scarcity of data on their interactions have restricted most prior research to studying them in isolation, thus preventing the development of a quantitative, causal understanding of why people approach activism. In this work, we develop a comprehensive causal model of how and why Reddit users engage with activist communities driving mass climate protests (mainly the 2019 Earth Strike, Fridays for Future, and Extinction Rebellion). Our framework, based on Stochastic Variational Inference applied to Bayesian Networks, learns the causal pathways over multiple time periods. Distinct from previous studies, our approach uses large-scale and fine-grained longitudinal data (2016 to 2022) to jointly model the roles of sociodemographic makeup, experience of extreme weather events, exposure to climate-related news, and social influence through online interactions. We find that among users interested in climate change, participation in online activist communities is indeed influenced by direct interactions with activists and largely by recent exposure to media coverage of climate protests. Among people aware of climate change, left-leaning people from lower socioeconomic backgrounds are particularly represented in online activist groups. Our findings offer empirical validation for theories of media influence and critical mass, and lay the foundations to inform interventions and future studies to foster public participation in collective action.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10562v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jacopo Lenti, Luca Maria Aiello, Corrado Monti, Gianmarco De Francisci Morales</dc:creator>
    </item>
    <item>
      <title>3DArticCyclists: Generating Simulated Dynamic 3D Cyclists for Human-Object Interaction (HOI) and Autonomous Driving Applications</title>
      <link>https://arxiv.org/abs/2410.10782</link>
      <description>arXiv:2410.10782v1 Announce Type: cross 
Abstract: Human-object interaction (HOI) and human-scene interaction (HSI) are crucial for human-centric scene understanding applications in Embodied Artificial Intelligence (EAI), robotics, and augmented reality (AR). A common limitation faced in these research areas is the data scarcity problem: insufficient labeled human-scene object pairs on the input images, and limited interaction complexity and granularity between them. Recent HOI and HSI methods have addressed this issue by generating dynamic interactions with rigid objects. But more complex dynamic interactions such as a human rider pedaling an articulated bicycle have been unexplored. To address this limitation, and to enable research on complex dynamic human-articulated object interactions, in this paper we propose a method to generate simulated 3D dynamic cyclist assets and interactions. We designed a methodology for creating a new part-based multi-view articulated synthetic 3D bicycle dataset that we call 3DArticBikes that can be used to train NeRF and 3DGS-based 3D reconstruction methods. We then propose a 3DGS-based parametric bicycle composition model to assemble 8-DoF pose-controllable 3D bicycles. Finally, using dynamic information from cyclist videos, we build a complete synthetic dynamic 3D cyclist (rider pedaling a bicycle) by re-posing a selectable synthetic 3D person while automatically placing the rider onto one of our new articulated 3D bicycles using a proposed 3D Keypoint optimization-based Inverse Kinematics pose refinement. We present both, qualitative and quantitative results where we compare our generated cyclists against those from a recent stable diffusion-based method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10782v1</guid>
      <category>cs.CV</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eduardo R. Corral-Soto, Yang Liu, Tongtong Cao, Yuan Ren, Liu Bingbing</dc:creator>
    </item>
    <item>
      <title>INDCOR White Paper 2: Interactive Narrative Design for Representing Complexity</title>
      <link>https://arxiv.org/abs/2305.01925</link>
      <description>arXiv:2305.01925v5 Announce Type: replace 
Abstract: This white paper was written by the members of the Work Group focusing on design practices of the COST Action 18230 - Interactive Narrative Design for Complexity Representation (INDCOR, WG1). It presents an overview of Interactive Digital Narratives (IDNs) design for complexity representations through IDN workflows and methodologies, IDN authoring tools and applications. It provides definitions of the central elements of the IDN alongside its best practices, designs and methods. Finally, it describes complexity as a feature of IDN, with related examples. In summary, this white paper serves as an orienting map for the field of IDN design, understanding where we are in the contemporary panorama while charting the grounds of their promising futures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.01925v5</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrew Perkis, Mattia Bellini, Valentina Nisi, Maria Cecilia Reyes, Cristina Sylla, Mijalche Santa, Anna Zaluczkowska, Shafaq Irshad, \'Agnes Bakk, Fanny Barnab\'e, Daniel Barnard, Nadia Boukhelifa, {\O}yvind S{\o}rdal Klungre, Hartmut Koenitz, Vincenzo Lombardo, Mirjam Palosaari Elahdhari, Catia Prandi, Scott Rettberg, Anca Serbanescu, Sonia Sousa, Petros Stefaneas, Dimitar Uzunov, Mirjam Vosmeer, Marcin Wardaszko</dc:creator>
    </item>
    <item>
      <title>INDCOR white paper 3: Interactive Digital Narratives and Interaction</title>
      <link>https://arxiv.org/abs/2306.10547</link>
      <description>arXiv:2306.10547v4 Announce Type: replace 
Abstract: The nature of interaction within Interactive Digital Narrative (IDN) is inherently complex. This is due, in part, to the wide range of potential interaction modes through which IDNs can be conceptualised, produced and deployed and the complex dynamics this might entail. The purpose of this whitepaper is to provide IDN practitioners with the essential knowledge on the nature of interaction in IDNs and allow them to make informed design decisions that lead to the incorporation of complexity thinking throughout the design pipeline, the implementation of the work, and the ways its audience perceives it. This white paper is concerned with the complexities of authoring, delivering and processing dynamic interactive contents from the perspectives of both creators and audiences. This white paper is part of a series of publications by the INDCOR COST Action 18230 (Interactive Narrative Design for Complexity Representations), which all clarify how IDNs representing complexity can be understood and applied (INDCOR WP 0 - 5, 2023).</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.10547v4</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Frank Nack, Sandy Louchart, Kris Lund, Mattia Bellini, Iva Georgieva, Pratama W. Atmaja, Peter Makai</dc:creator>
    </item>
    <item>
      <title>Fostering Enterprise Conversations Around Data on Collaboration Platforms</title>
      <link>https://arxiv.org/abs/2310.04315</link>
      <description>arXiv:2310.04315v3 Announce Type: replace 
Abstract: In enterprise organizations, data-driven decision making processes include the use of business intelligence dashboards and collaborative deliberation on communication platforms such as Slack. However, apart from those in data analyst roles, there is shallow engagement with dashboard content due to insufficient context, poor representation choices, or a lack of access and guidance. Data analysts often need to retarget their dashboard content for those with limited engagement, and this retargeting process often involves switching between different tools. To inform the design of systems that streamline this work process, we conducted a co-design study with nine enterprise professionals who use dashboard content to communicate with their colleagues. We consolidate our findings from the co-design study into a comprehensive demonstration scenario. Using this scenario as a design probe, we interviewed 14 data workers to further develop our design recommendations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.04315v3</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Hyeok Kim, Arjun Srinivasan, Matthew Brehmer</dc:creator>
    </item>
    <item>
      <title>Crystallizing Schemas with Teleoscope: Thematic Curation of Large Text Corpora</title>
      <link>https://arxiv.org/abs/2402.06124</link>
      <description>arXiv:2402.06124v2 Announce Type: replace 
Abstract: Making sense of large text corpora is difficult when scales reach thousands or millions of documents. With the advent of LLMs, the potential for large-scale sense-making is being realized. However, this presents a need for rigour in the data curation stage of thematic analysis: selecting the right documents to achieve appropriate information power (saturation) requires an auditable trace of researchers' thought processes.
  In this paper, we present methodological and design findings from a three-year design process where we worked with qualitative researchers to create an open-source platform called Teleoscope designed to rigorously curate documents at scale. By implementing the qualitative research values common to thematic analysis during the curation stage (which we call thematic curation), we found researchers could come to a shared understanding of a large corpus and feel confident in their curation decisions (which we call schema crystallization).</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06124v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Paul Bucci, Leo Foord-Kelcey, Patrick Yung Kang Lee, Alamjeet Singh, Ivan Beschastnikh</dc:creator>
    </item>
    <item>
      <title>From traces to measures: Large language models as a tool for psychological measurement from text</title>
      <link>https://arxiv.org/abs/2405.07447</link>
      <description>arXiv:2405.07447v2 Announce Type: replace 
Abstract: Large language models are increasingly being used to label or rate psychological features in text data. This approach helps address one of the limiting factors of digital trace data - their lack of an inherent target of measurement. However, this approach is also a form of psychological measurement (using observable variables to quantify a hypothetical latent construct). As such, these ratings are subject to the same psychometric considerations of reliability and validity as more standard psychological measures. Here we present a workflow for developing and evaluating large language model based measures of psychological features which incorporate these considerations. We also provide an example, attempting to measure the previously established constructs of attitude certainty, importance and moralization from text. Using a pool of prompts adapted from existing measurement instruments, we find they have good levels of internal consistency but only partially meet validity criteria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.07447v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joseph J. P. Simons, Wong Liang Ze, Prasanta Bhattacharya, Brandon Siyuan Loh, Wei Gao</dc:creator>
    </item>
    <item>
      <title>Augmented Reality Assistive Technologies for Disabled Individuals</title>
      <link>https://arxiv.org/abs/2409.02053</link>
      <description>arXiv:2409.02053v2 Announce Type: replace 
Abstract: Augmented Reality (AR) technologies hold immense potential for revolutionizing the way individuals with disabilities interact with the world. AR systems can provide real-time assistance and support by overlaying digital information over the physical environment based on the requirements of the use, hence addressing different types of disabilities. Through an in-depth analysis of four case studies, this paper aims to provide a comprehensive overview of the current-state-of-the-art in AR assistive technologies for individuals with disabilities, highlighting their potential to assist and transform their lives. The findings show the significance that AR has made to bridge the accessibility gap, while also discussing the challenges faced and ethical considerations associated with the implementation across the various cases. This is done through theory analysis, practical examples, and future projections that will motivate and seek to inspire further innovation in this very relevant area of exploration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02053v2</guid>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Riju Marwah, Jyotin Singh Thakur, Pranav Tanwar</dc:creator>
    </item>
    <item>
      <title>The Brain-Inspired Cooperative Shared Control Framework for Brain-Machine Interface</title>
      <link>https://arxiv.org/abs/2210.09531</link>
      <description>arXiv:2210.09531v4 Announce Type: replace-cross 
Abstract: In brain-machine interface (BMI) applications, a key challenge is the low information content and high noise level in neural signals, severely affecting stable robotic control. To address this challenge, we proposes a cooperative shared control framework based on brain-inspired intelligence, where control signals are decoded from neural activity, and the robot handles the fine control. This allows for a combination of flexible and adaptive interaction control between the robot and the brain, making intricate human-robot collaboration feasible. The proposed framework utilizes spiking neural networks (SNNs) for controlling robotic arm and wheel, including speed and steering. While full integration of the system remains a future goal, individual modules for robotic arm control, object tracking, and map generation have been successfully implemented. The framework is expected to significantly enhance the performance of BMI. In practical settings, the BMI with cooperative shared control, utilizing a brain-inspired algorithm, will greatly enhance the potential for clinical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.09531v4</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Junjie Yang, Ling Liu, Shengjie Zheng, Lang Qian, Gang Gao, Xin Chen, Xiaojian Li</dc:creator>
    </item>
    <item>
      <title>Facial recognition technology and human raters can predict political orientation from images of expressionless faces even when controlling for demographics and self-presentation</title>
      <link>https://arxiv.org/abs/2303.16343</link>
      <description>arXiv:2303.16343v4 Announce Type: replace-cross 
Abstract: Carefully standardized facial images of 591 participants were taken in the laboratory, while controlling for self-presentation, facial expression, head orientation, and image properties. They were presented to human raters and a facial recognition algorithm: both humans (r=.21) and the algorithm (r=.22) could predict participants' scores on a political orientation scale (Cronbach's alpha=.94) decorrelated with age, gender, and ethnicity. These effects are on par with how well job interviews predict job success, or alcohol drives aggressiveness. Algorithm's predictive accuracy was even higher (r=.31) when it leveraged information on participants' age, gender, and ethnicity. Moreover, the associations between facial appearance and political orientation seem to generalize beyond our sample: The predictive model derived from standardized images (while controlling for age, gender, and ethnicity) could predict political orientation (r=.13) from naturalistic images of 3,401 politicians from the U.S., UK, and Canada. The analysis of facial features associated with political orientation revealed that conservatives tended to have larger lower faces. The predictability of political orientation from standardized images has critical implications for privacy, the regulation of facial recognition technology, and understanding the origins and consequences of political orientation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.16343v4</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1037/amp0001295</arxiv:DOI>
      <dc:creator>Michal Kosinski, Poruz Khambatta, Yilun Wang</dc:creator>
    </item>
    <item>
      <title>Bridging the gap: Towards an Expanded Toolkit for AI-driven Decision-Making in the Public Sector</title>
      <link>https://arxiv.org/abs/2310.19091</link>
      <description>arXiv:2310.19091v3 Announce Type: replace-cross 
Abstract: AI-driven decision-making systems are becoming instrumental in the public sector, with applications spanning areas like criminal justice, social welfare, financial fraud detection, and public health. While these systems offer great potential benefits to institutional decision-making processes, such as improved efficiency and reliability, these systems face the challenge of aligning machine learning (ML) models with the complex realities of public sector decision-making. In this paper, we examine five key challenges where misalignment can occur, including distribution shifts, label bias, the influence of past decision-making on the data side, as well as competing objectives and human-in-the-loop on the model output side. Our findings suggest that standard ML methods often rely on assumptions that do not fully account for these complexities, potentially leading to unreliable and harmful predictions. To address this, we propose a shift in modeling efforts from focusing solely on predictive accuracy to improving decision-making outcomes. We offer guidance for selecting appropriate modeling frameworks, including counterfactual prediction and policy learning, by considering how the model estimand connects to the decision-maker's utility. Additionally, we outline technical methods that address specific challenges within each modeling approach. Finally, we argue for the importance of external input from domain experts and stakeholders to ensure that model assumptions and design choices align with real-world policy objectives, taking a step towards harmonizing AI and public sector objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.19091v3</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <category>stat.ME</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.giq.2024.101976</arxiv:DOI>
      <arxiv:journal_reference>Government Information Quarterly, Volume 41, Issue 4, 2024, 101976</arxiv:journal_reference>
      <dc:creator>Unai Fischer-Abaigar, Christoph Kern, Noam Barda, Frauke Kreuter</dc:creator>
    </item>
    <item>
      <title>Temporal Action Localization for Inertial-based Human Activity Recognition</title>
      <link>https://arxiv.org/abs/2311.15831</link>
      <description>arXiv:2311.15831v2 Announce Type: replace-cross 
Abstract: As of today, state-of-the-art activity recognition from wearable sensors relies on algorithms being trained to classify fixed windows of data. In contrast, video-based Human Activity Recognition, known as Temporal Action Localization (TAL), has followed a segment-based prediction approach, localizing activity segments in a timeline of arbitrary length. This paper is the first to systematically demonstrate the applicability of state-of-the-art TAL models for both offline and near-online Human Activity Recognition (HAR) using raw inertial data as well as pre-extracted latent features as input. Offline prediction results show that TAL models are able to outperform popular inertial models on a multitude of HAR benchmark datasets, with improvements reaching as much as 26% in F1-score. We show that by analyzing timelines as a whole, TAL models can produce more coherent segments and achieve higher NULL-class accuracy across all datasets. We demonstrate that TAL is less suited for the immediate classification of small-sized windows of data, yet offers an interesting perspective on inertial-based HAR -- alleviating the need for fixed-size windows and enabling algorithms to recognize activities of arbitrary length. With design choices and training concepts yet to be explored, we argue that TAL architectures could be of significant value to the inertial-based HAR community. The code and data download to reproduce experiments is publicly available via github.com/mariusbock/tal_for_har.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.15831v2</guid>
      <category>cs.LG</category>
      <category>cs.HC</category>
      <category>eess.SP</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marius Bock, Michael Moeller, Kristof Van Laerhoven</dc:creator>
    </item>
    <item>
      <title>Inadequacies of Large Language Model Benchmarks in the Era of Generative Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2402.09880</link>
      <description>arXiv:2402.09880v2 Announce Type: replace-cross 
Abstract: The rapid rise in popularity of Large Language Models (LLMs) with emerging capabilities has spurred public curiosity to evaluate and compare different LLMs, leading many researchers to propose their own LLM benchmarks. Noticing preliminary inadequacies in those benchmarks, we embarked on a study to critically assess 23 state-of-the-art LLM benchmarks, using our novel unified evaluation framework through the lenses of people, process, and technology, under the pillars of benchmark functionality and integrity. Our research uncovered significant limitations, including biases, difficulties in measuring genuine reasoning, adaptability, implementation inconsistencies, prompt engineering complexity, evaluator diversity, and the overlooking of cultural and ideological norms in one comprehensive assessment. Our discussions emphasized the urgent need for standardized methodologies, regulatory certainties, and ethical guidelines in light of Artificial Intelligence (AI) advancements, including advocating for an evolution from static benchmarks to dynamic behavioral profiling to accurately capture LLMs' complex behaviors and potential risks. Our study highlighted the necessity for a paradigm shift in LLM evaluation methodologies, underlining the importance of collaborative efforts for the development of universally accepted benchmarks and the enhancement of AI systems' integration into society.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.09880v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Timothy R. McIntosh, Teo Susnjak, Nalin Arachchilage, Tong Liu, Paul Watters, Malka N. Halgamuge</dc:creator>
    </item>
    <item>
      <title>Long-Term Human Trajectory Prediction using 3D Dynamic Scene Graphs</title>
      <link>https://arxiv.org/abs/2405.00552</link>
      <description>arXiv:2405.00552v2 Announce Type: replace-cross 
Abstract: We present a novel approach for long-term human trajectory prediction in indoor human-centric environments, which is essential for long-horizon robot planning in these environments. State-of-the-art human trajectory prediction methods are limited by their focus on collision avoidance and short-term planning, and their inability to model complex interactions of humans with the environment. In contrast, our approach overcomes these limitations by predicting sequences of human interactions with the environment and using this information to guide trajectory predictions over a horizon of up to 60s. We leverage Large Language Models (LLMs) to predict interactions with the environment by conditioning the LLM prediction on rich contextual information about the scene. This information is given as a 3D Dynamic Scene Graph that encodes the geometry, semantics, and traversability of the environment into a hierarchical representation. We then ground these interaction sequences into multi-modal spatio-temporal distributions over human positions using a probabilistic approach based on continuous-time Markov Chains. To evaluate our approach, we introduce a new semi-synthetic dataset of long-term human trajectories in complex indoor environments, which also includes annotations of human-object interactions. We show in thorough experimental evaluations that our approach achieves a 54% lower average negative log-likelihood and a 26.5% lower Best-of-20 displacement error compared to the best non-privileged (i.e., evaluated in a zero-shot fashion on the dataset) baselines for a time horizon of 60s.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00552v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas Gorlo, Lukas Schmid, Luca Carlone</dc:creator>
    </item>
    <item>
      <title>Designing a Dashboard for Transparency and Control of Conversational AI</title>
      <link>https://arxiv.org/abs/2406.07882</link>
      <description>arXiv:2406.07882v3 Announce Type: replace-cross 
Abstract: Conversational LLMs function as black box systems, leaving users guessing about why they see the output they do. This lack of transparency is potentially problematic, especially given concerns around bias and truthfulness. To address this issue, we present an end-to-end prototype-connecting interpretability techniques with user experience design-that seeks to make chatbots more transparent. We begin by showing evidence that a prominent open-source LLM has a "user model": examining the internal state of the system, we can extract data related to a user's age, gender, educational level, and socioeconomic status. Next, we describe the design of a dashboard that accompanies the chatbot interface, displaying this user model in real time. The dashboard can also be used to control the user model and the system's behavior. Finally, we discuss a study in which users conversed with the instrumented system. Our results suggest that users appreciate seeing internal states, which helped them expose biased behavior and increased their sense of control. Participants also made valuable suggestions that point to future directions for both design and machine learning research. The project page and video demo of our TalkTuner system are available at https://bit.ly/talktuner-project-page</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.07882v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yida Chen, Aoyu Wu, Trevor DePodesta, Catherine Yeh, Kenneth Li, Nicholas Castillo Marin, Oam Patel, Jan Riecke, Shivam Raval, Olivia Seow, Martin Wattenberg, Fernanda Vi\'egas</dc:creator>
    </item>
    <item>
      <title>MoVEInt: Mixture of Variational Experts for Learning Human-Robot Interactions from Demonstrations</title>
      <link>https://arxiv.org/abs/2407.07636</link>
      <description>arXiv:2407.07636v2 Announce Type: replace-cross 
Abstract: Shared dynamics models are important for capturing the complexity and variability inherent in Human-Robot Interaction (HRI). Therefore, learning such shared dynamics models can enhance coordination and adaptability to enable successful reactive interactions with a human partner. In this work, we propose a novel approach for learning a shared latent space representation for HRIs from demonstrations in a Mixture of Experts fashion for reactively generating robot actions from human observations. We train a Variational Autoencoder (VAE) to learn robot motions regularized using an informative latent space prior that captures the multimodality of the human observations via a Mixture Density Network (MDN). We show how our formulation derives from a Gaussian Mixture Regression formulation that is typically used approaches for learning HRI from demonstrations such as using an HMM/GMM for learning a joint distribution over the actions of the human and the robot. We further incorporate an additional regularization to prevent "mode collapse", a common phenomenon when using latent space mixture models with VAEs. We find that our approach of using an informative MDN prior from human observations for a VAE generates more accurate robot motions compared to previous HMM-based or recurrent approaches of learning shared latent representations, which we validate on various HRI datasets involving interactions such as handshakes, fistbumps, waving, and handovers. Further experiments in a real-world human-to-robot handover scenario show the efficacy of our approach for generating successful interactions with four different human interaction partners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.07636v2</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1109/LRA.2024.3396074</arxiv:DOI>
      <dc:creator>Vignesh Prasad, Alap Kshirsagar, Dorothea Koert, Ruth Stock-Homburg, Jan Peters, Georgia Chalvatzaki</dc:creator>
    </item>
    <item>
      <title>The Patterns of Life Human Mobility Simulation</title>
      <link>https://arxiv.org/abs/2410.00185</link>
      <description>arXiv:2410.00185v2 Announce Type: replace-cross 
Abstract: We demonstrate the Patterns of Life Simulation to create realistic simulations of human mobility in a city. This simulation has recently been used to generate massive amounts of trajectory and check-in data. Our demonstration focuses on using the simulation twofold: (1) using the graphical user interface (GUI), and (2) running the simulation headless by disabling the GUI for faster data generation. We further demonstrate how the Patterns of Life simulation can be used to simulate any region on Earth by using publicly available data from OpenStreetMap. Finally, we also demonstrate recent improvements to the scalability of the simulation allows simulating up to 100,000 individual agents for years of simulation time. During our demonstration, as well as offline using our guides on GitHub, participants will learn: (1) The theories of human behavior driving the Patters of Life simulation, (2) how to simulate to generate massive amounts of synthetic yet realistic trajectory data, (3) running the simulation for a region of interest chosen by participants using OSM data, (4) learn the scalability of the simulation and understand the properties of generated data, and (5) manage thousands of parallel simulation instances running concurrently.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00185v2</guid>
      <category>cs.MA</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hossein Amiri, Will Kohn, Shiyang Ruan, Joon-Seok Kim, Hamdi Kavak, Andrew Crooks, Dieter Pfoser, Carola Wenk, Andreas Zufle</dc:creator>
    </item>
    <item>
      <title>E2H: A Two-Stage Non-Invasive Neural Signal Driven Humanoid Robotic Whole-Body Control Framework</title>
      <link>https://arxiv.org/abs/2410.02141</link>
      <description>arXiv:2410.02141v3 Announce Type: replace-cross 
Abstract: Recent advancements in humanoid robotics, including the integration of hierarchical reinforcement learning-based control and the utilization of LLM planning, have significantly enhanced the ability of robots to perform complex tasks. In contrast to the highly developed humanoid robots, the human factors involved remain relatively unexplored. Directly controlling humanoid robots with the brain has already appeared in many science fiction novels, such as Pacific Rim and Gundam. In this work, we present E2H (EEG-to-Humanoid), an innovative framework that pioneers the control of humanoid robots using high-frequency non-invasive neural signals. As the none-invasive signal quality remains low in decoding precise spatial trajectory, we decompose the E2H framework in an innovative two-stage formation: 1) decoding neural signals (EEG) into semantic motion keywords, 2) utilizing LLM facilitated motion generation with a precise motion imitation control policy to realize humanoid robotics control. The method of directly driving robots with brainwave commands offers a novel approach to human-machine collaboration, especially in situations where verbal commands are impractical, such as in cases of speech impairments, space exploration, or underwater exploration, unlocking significant potential. E2H offers an exciting glimpse into the future, holding immense potential for human-computer interaction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02141v3</guid>
      <category>cs.RO</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiqun Duan, Qiang Zhang, Jinzhao Zhou, Jingkai Sun, Xiaowei Jiang, Jiahang Cao, Jiaxu Wang, Yiqian Yang, Wen Zhao, Gang Han, Yijie Guo, Chin-Teng Lin</dc:creator>
    </item>
    <item>
      <title>Focal Surface Holographic Light Transport using Learned Spatially Adaptive Convolutions</title>
      <link>https://arxiv.org/abs/2410.06854</link>
      <description>arXiv:2410.06854v2 Announce Type: replace-cross 
Abstract: Computer-Generated Holography (CGH) is a set of algorithmic methods for identifying holograms that reconstruct Three-Dimensional (3D) scenes in holographic displays. CGH algorithms decompose 3D scenes into multiplanes at different depth levels and rely on simulations of light that propagated from a source plane to a targeted plane. Thus, for n planes, CGH typically optimizes holograms using n plane-to-plane light transport simulations, leading to major time and computational demands. Our work replaces multiple planes with a focal surface and introduces a learned light transport model that could propagate a light field from a source plane to the focal surface in a single inference. Our learned light transport model leverages spatially adaptive convolution to achieve depth-varying propagation demanded by targeted focal surfaces. The proposed model reduces the hologram optimization process up to 1.5x, which contributes to hologram dataset generation and the training of future learned CGH models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06854v2</guid>
      <category>cs.GR</category>
      <category>cs.HC</category>
      <pubDate>Tue, 15 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chuanjun Zheng, Yicheng Zhan, Liang Shi, Ozan Cakmakci, Kaan Ak\c{s}it</dc:creator>
    </item>
  </channel>
</rss>
