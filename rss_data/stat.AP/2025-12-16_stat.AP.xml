<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Wed, 17 Dec 2025 02:38:50 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Estimation of Heterogeneous Causal Mediation Effects in a Hypertension Treatment Trial</title>
      <link>https://arxiv.org/abs/2512.12043</link>
      <description>arXiv:2512.12043v1 Announce Type: new 
Abstract: Hypertension is a highly prevalent condition and a major risk factor for cardiovascular disease. The landmark Systolic Blood Pressure Intervention Trial (SPRINT) showed that lowering systolic blood pressure (BP) goals from 140 mmHg to 120 mmHg leads to significantly reduced BP, cardiovascular mortality, and morbidity. However, the underlying mechanisms are not yet fully elucidated. In patients with impaired renal function, early reduction of albuminuria has been proposed as a potential mediation pathway. Evidence from the standard causal mediation analysis (CMA), however, yields inconsistent results, possibly due to heterogeneous mediation effects across individuals. To disseminate the heterogeneity, a new framework that incorporates covariate-treatment and mediator-treatment interactions within a linear structural equation modeling system is introduced. Causal assumptions are discussed and heterogeneous natural direct and indirect effects are parameterized as functions of patient characteristics. A modified covariate approach is proposed to relax the hierarchical constraints and the generalized lasso regularization is employed to ensure parsimony in high-dimensional settings. Asymptotic properties are studied. Simulation studies demonstrate good estimation and inference performance. Analysis of the SPRINT data reveals substantial heterogeneity in mediation effects, identifying a subset of patients who stand to gain from therapies targeting albuminuria.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12043v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Zhao, Chengyun Li, Wanzhu Tu</dc:creator>
    </item>
    <item>
      <title>A Real Data-Driven, Robust Survival Analysis on Patients who Underwent Deep Brain Stimulation for Parkinson's Disease by Utilizing Parametric, Non-Parametric, and Semi-Parametric Approaches</title>
      <link>https://arxiv.org/abs/2512.12579</link>
      <description>arXiv:2512.12579v1 Announce Type: new 
Abstract: Parkinson's Disease (PD) is a devastating neurodegenerative disorder that affects millions of people around the globe. Many researchers are continuously working to understand PD and develop treatments to improve the condition of PD patients, which affects their day-to-day lives. Since the last decades, the treatment, Deep Brain Stimulation (DBS) has given promising results for motor symptoms by improving the quality of daily living of PD patients. In the methodology of the present study, we have utilized sophisticated statistical approaches such as Nonparametric, Semi-parametric, and robust Parametric survival analysis to extract useful and important information about the long-term survival outcomes of the patients who underwent DBS for PD. Finally, we were able to conclude that the probabilistic behavior of the survival time of female patients is statistically different from that of male patients. Furthermore, we have identified that the probabilistic behavior of the survival times of Female patients is characterized by the 3-parameter Lognormal distribution, while that of Male patients is characterized by the 3-parameter Weibull distribution. More importantly, we have found that the Female patients have higher survival compared to the Male patients after conducting a robust parametric survival analysis. Using the semi-parametric COX-PH, we found that the initial implant of the right side leads to a high frequency of events occurring for the female patients with a bad prognostic factor, while for the male patients, a low events occurs with a good prognostic factor. Furthermore, we have found an interaction term between the number of revisions and the initial size of the implant, which increases the frequency of events occurring for the Male patients with a bad prognostic factor.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12579v1</guid>
      <category>stat.AP</category>
      <category>stat.OT</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Malinda Iluppangama, Dilmi Abeywardana, Chris Tsokos</dc:creator>
    </item>
    <item>
      <title>Beyond Missing Data: Questionnaire Uncertainty Responses as Early Digital Biomarkers of Cognitive Decline and Neurodegenerative Diseases</title>
      <link>https://arxiv.org/abs/2512.13346</link>
      <description>arXiv:2512.13346v1 Announce Type: new 
Abstract: Identifying preclinical biomarkers of neurodegenerative diseases remains a major challenge in aging research. In this study, we demonstrate that frequent "Don't know/can't remember" (DK) responses, often treated as missing data in touchscreen questionnaires, serve as a novel digital behavioral biomarker of early cognitive vulnerability and neurodegenerative disease risk. Using data from 502,234 UK Biobank participants, we stratified individuals based on DK response frequency (0-1, 2-4, 5-7, &gt;7) and observed a robust, dose-dependent association with an increased risk of Alzheimer's disease (HR = 1.64, 95% CI: 1.26-2.14) and vascular dementia (HR = 1.93, 95% CI: 1.37-2.72), independent of established risk factors. As DK response frequency increased, participants exhibited higher BMI, reduced physical activity, higher smoking rates, and a higher prevalence of chronic diseases, particularly hypertension, diabetes, and depression. Further analysis revealed a dose-dependent relationship between DK response frequency and the risk of Alzheimer's disease and vascular dementia, with high DK responders showing early neurodegenerative changes, marked by elevated levels of Abeta40, Abeta42, NFL, and pTau-181. Metabolomic analysis also revealed lipid metabolism abnormalities, which may mediate this relationship. Together, these findings reframe DK response patterns as clinically meaningful signals of multidimensional neurobiological alterations, offering a scalable, low-cost, non-invasive tool for early risk identification and prevention at the population level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13346v1</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yukun Lu, Bingjie Li, Zhigang Yao</dc:creator>
    </item>
    <item>
      <title>The Art of Storytelling in Authoritarian Regimes: Crafting State Narratives on Chinese Social Media</title>
      <link>https://arxiv.org/abs/2512.11875</link>
      <description>arXiv:2512.11875v1 Announce Type: cross 
Abstract: This article examines how authoritarian regimes construct state narratives about politically consequential events. Building on the narrative policy framework and existing research on authoritarian propaganda, we propose two dimensions that shape narrative construction: legitimacy implications -- whether events enhance or threaten regime legitimacy, and citizen verification capacity -- the extent to which citizens can evaluate official narratives through alternative sources. Using quantitative narrative analysis of Chinese social media posts by government, state media, and celebrity accounts, we extract subject-verb-object (SVO) triplets to map dominant narrative structures across four major events. Our findings show that legitimacy implications of the event shape regime's efforts in storytelling and the beliefs highlighted in the narratives, while citizen's verification capacity could balance the strategic choice between a top-down manipulation and bottom-up responsiveness of state narratives. Together, the results reveal propaganda as a complex process of narrative construction adaptive to specific contexts, offering new insights into how dynamic storytelling sustains authoritarian resilience.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11875v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ting Luo, Yan Wang</dc:creator>
    </item>
    <item>
      <title>Contextual Peano Scan and Fast Image Segmentation Using Hidden and Evidential Markov Chains</title>
      <link>https://arxiv.org/abs/2512.11939</link>
      <description>arXiv:2512.11939v1 Announce Type: cross 
Abstract: Transforming bi-dimensional sets of image pixels into mono-dimensional sequences with a Peano scan (PS) is an established technique enabling the use of hidden Markov chains (HMCs) for unsupervised image segmentation. Related Bayesian segmentation methods can compete with hidden Markov fields (HMFs)-based ones and are much faster. PS has recently been extended to the contextual PS, and some initial experiments have shown the value of the associated HMC model, denoted as HMC-CPS, in image segmentation. Moreover, HMCs have been extended to hidden evidential Markov chains (HEMCs), which are capable of improving HMC-based Bayesian segmentation. In this study, we introduce a new HEMC-CPS model by simultaneously considering contextual PS and evidential HMC. We show its effectiveness for Bayesian maximum posterior mode (MPM) segmentation using synthetic and real images. Segmentation is performed in an unsupervised manner, with parameters being estimated using the stochastic expectation--maximization (SEM) method. The new HEMC-CPS model presents potential for the modeling and segmentation of more complex images, such as three-dimensional or multi-sensor multi-resolution images. Finally, the HMC-CPS and HEMC-CPS models are not limited to image segmentation and could be used for any kind of spatially correlated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.11939v1</guid>
      <category>cs.CV</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Mathematics 2025, 13 (10), pp.1589</arxiv:journal_reference>
      <dc:creator>Cl\'ement Fernandes (SAMOVAR, SOP - SAMOVAR, TSP), Wojciech Pieczynski (SAMOVAR, SOP - SAMOVAR, TSP)</dc:creator>
    </item>
    <item>
      <title>Proximal Causal Inference for Modified Treatment Policies</title>
      <link>https://arxiv.org/abs/2512.12038</link>
      <description>arXiv:2512.12038v1 Announce Type: cross 
Abstract: The proximal causal inference framework enables the identification and estimation of causal effects in the presence of unmeasured confounding by leveraging two disjoint sets of observed strong proxies: negative control treatments and negative control outcomes. In the point exposure setting, this framework has primarily been applied to estimands comparing counterfactual outcomes under a static fixed intervention or, possibly randomized, regime that depends on baseline covariates. For continuous exposures, alternative hypothetical scenarios can enrich our understanding of causal effects, such as those where each individual receives their observed treatment dose modified in a pre-specified manner - commonly referred to as modified treatment regimes. In this work, we extend the proximal causal inference framework to identify and estimate the mean outcome under a modified treatment regime, addressing this gap in the literature. We propose a flexible strategy that does not rely on the assumption that all confounders have been measured - unlike existing estimators - and leverages modern debiased machine learning techniques using non-parametric estimators of nuisance functions to avoid restrictive parametric assumptions. Our methodology was motivated by immunobridging studies of COVID-19 vaccines aimed at identifying correlates of protection, where the individual's underlying immune capacity is an important unmeasured confounder. We demonstrate its applicability using data from such a study and evaluate its finite-sample performance through simulation studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12038v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antonio Olivas-Martinez, Peter B. Gilbert, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Anticipatory Governance in Data-Constrained Environments: A Predictive Simulation Framework for Digital Financial Inclusion</title>
      <link>https://arxiv.org/abs/2512.12212</link>
      <description>arXiv:2512.12212v1 Announce Type: cross 
Abstract: Financial exclusion remains a major barrier to digital public service delivery in resource-constrained and archipelagic nations. Traditional policy evaluations rely on retrospective data, limiting the ex-ante intelligence needed for agile resource allocation. This study introduces a predictive simulation framework to support anticipatory governance within government information systems. Using the UNCDF Pacific Digital Economy dataset of 10,108 respondents, we apply a three-stage pipeline: descriptive profiling, interpretable machine learning, and scenario simulation to forecast outcomes of digital financial literacy interventions before deployment. Leveraging cross-sectional structural associations, the framework projects intervention scenarios as prioritization heuristics rather than causal estimates. A transparent linear regression model with R-squared of 95.9 identifies modifiable policy levers. Simulations indicate that foundational digital capabilities such as device access and expense tracking yield the highest projected gains, up to 5.5 percent, outperforming attitudinal nudges. The model enables precision targeting, highlighting young female caregivers as high-leverage responders while flagging non-responders such as urban professionals to prevent resource misallocation. This research demonstrates how static survey data can be repurposed into actionable policy intelligence, offering a scalable and evidence-based blueprint for embedding predictive analytics into public-sector decision-support systems to advance equity-focused digital governance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12212v1</guid>
      <category>cs.CY</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elizabeth Irenne Yuwono, Dian Tjondronegoro, Shawn Hunter, Amber Marshall</dc:creator>
    </item>
    <item>
      <title>Quantile regression with generalized multiquadric loss function</title>
      <link>https://arxiv.org/abs/2512.12340</link>
      <description>arXiv:2512.12340v1 Announce Type: cross 
Abstract: Quantile regression (QR) is now widely used to analyze the effect of covariates on the conditional distribution of a response variable. It provides a more comprehensive picture of the relationship between a response and covariates compared with classical least squares regression. However, the non-differentiability of the check loss function precludes the use of gradient-based methods to solve the optimization problem in quantile regression estimation. To this end, This paper constructs a smoothed loss function based on multiquadric (MQ) function. The proposed loss function leads to a globally convex optimization problem that can be efficiently solved via (stochastic) gradient descent methods. As an example, we apply the Barzilai-Borwein gradient descent method to obtain the estimation of quantile regression. We establish the theoretical results of the proposed estimator under some regularity conditions, and compare it with other estimation methods using Monte Carlo simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12340v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wenwu Gao, Dongyi Zheng, Hanbing Zhu</dc:creator>
    </item>
    <item>
      <title>The Morphemic Origin of Zipf's Law: A Factorized Combinatorial Framework</title>
      <link>https://arxiv.org/abs/2512.12394</link>
      <description>arXiv:2512.12394v1 Announce Type: cross 
Abstract: We present a simple structure based model of how words are formed from morphemes. The model explains two major empirical facts: the typical distribution of word lengths and the appearance of Zipf like rank frequency curves. In contrast to classical explanations based on random text or communication efficiency, our approach uses only the combinatorial organization of prefixes, roots, suffixes and inflections. In this Morphemic Combinatorial Word Model, a word is created by activating several positional slots. Each slot turns on with a certain probability and selects one morpheme from its inventory. Morphemes are treated as stable building blocks that regularly appear in word formation and have characteristic positions. This mechanism produces realistic word length patterns with a concentrated middle zone and a thin long tail, closely matching real languages. Simulations with synthetic morpheme inventories also generate rank frequency curves with Zipf like exponents around 1.1-1.4, similar to English, Russian and Romance languages. The key result is that Zipf like behavior can emerge without meaning, communication pressure or optimization principles. The internal structure of morphology alone, combined with probabilistic activation of slots, is sufficient to create the robust statistical patterns observed across languages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12394v1</guid>
      <category>stat.ME</category>
      <category>cs.CL</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Vladimir Berman</dc:creator>
    </item>
    <item>
      <title>Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities</title>
      <link>https://arxiv.org/abs/2512.12574</link>
      <description>arXiv:2512.12574v1 Announce Type: cross 
Abstract: Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12574v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.neucom.2025.132317</arxiv:DOI>
      <arxiv:journal_reference>Neurocomputing 132317 (2025)</arxiv:journal_reference>
      <dc:creator>Isaac Adjetey, Yiyuan She</dc:creator>
    </item>
    <item>
      <title>Credit Risk Estimation with Non-Financial Features: Evidence from a Synthetic Istanbul Dataset</title>
      <link>https://arxiv.org/abs/2512.12783</link>
      <description>arXiv:2512.12783v1 Announce Type: cross 
Abstract: Financial exclusion constrains entrepreneurship, increases income volatility, and widens wealth gaps. Underbanked consumers in Istanbul often have no bureau file because their earnings and payments flow through informal channels. To study how such borrowers can be evaluated we create a synthetic dataset of one hundred thousand Istanbul residents that reproduces first quarter 2025 T\"U\.IK census marginals and telecom usage patterns. Retrieval augmented generation feeds these public statistics into the OpenAI o3 model, which synthesises realistic yet private records. Each profile contains seven socio demographic variables and nine alternative attributes that describe phone specifications, online shopping rhythm, subscription spend, car ownership, monthly rent, and a credit card flag. To test the impact of the alternative financial data CatBoost, LightGBM, and XGBoost are each trained in two versions. Demo models use only the socio demographic variables; Full models include both socio demographic and alternative attributes. Across five fold stratified validation the alternative block raises area under the curve by about one point three percentage and lifts balanced \(F_{1}\) from roughly 0.84 to 0.95, a fourteen percent gain. We contribute an open Istanbul 2025 Q1 synthetic dataset, a fully reproducible modeling pipeline, and empirical evidence that a concise set of behavioural attributes can approach bureau level discrimination power while serving borrowers who lack formal credit records. These findings give lenders and regulators a transparent blueprint for extending fair and safe credit access to the underbanked.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12783v1</guid>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atalay Denknalbant, Emre Sezdi, Zeki Furkan Kutlu, Polat Goktas</dc:creator>
    </item>
    <item>
      <title>Variational Inference for Fully Bayesian Hierarchical Linear Models</title>
      <link>https://arxiv.org/abs/2512.12857</link>
      <description>arXiv:2512.12857v1 Announce Type: cross 
Abstract: Bayesian hierarchical linear models provide a natural framework to analyze nested and clustered data. Classical estimation with Markov chain Monte Carlo produces well calibrated posterior distributions but becomes computationally expensive in high dimensional or large sample settings. Variational Inference and Stochastic Variational Inference offer faster optimization based alternatives, but their accuracy in hierarchical structures is uncertain when group separation is weak. This paper compares these two paradigms across three model classes, the Linear Regression Model, the Hierarchical Linear Regression Model, and a Clustered Hierarchical Linear Regression Model. Through simulation studies and an application to real data, the results show that variational methods recover global regression effects and clustering structure with a fraction of the computing time, but distort posterior dependence and yield unstable values of information criteria such as WAIC and DIC. The findings clarify when variational methods can serve as practical surrogates for Markov chain Monte Carlo and when their limitations make full Bayesian sampling necessary, and they provide guidance for extending the same variational framework to generalized linear models and other members of the exponential family.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12857v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cristian Parra-Aldana, Juan Sosa</dc:creator>
    </item>
    <item>
      <title>CapOptix: An Options-Framework for Capacity Market Pricing</title>
      <link>https://arxiv.org/abs/2512.12871</link>
      <description>arXiv:2512.12871v1 Announce Type: cross 
Abstract: Electricity markets are under increasing pressure to maintain reliability amidst rising renewable penetration, demand variability, and occasional price shocks. Traditional capacity market designs often fall short in addressing this by relying on expected-value metrics of energy unserved, which overlook risk exposure in such systems. In this work, we present CapOptix, a capacity pricing framework that interprets capacity commitments as reliability options, i.e., financial derivatives of wholesale electricity prices. CapOptix characterizes the capacity premia charged by accounting for structural price shifts modeled by the Markov Regime Switching Process. We apply the framework to historical price data from multiple electricity markets and compare the resulting premium ranges with existing capacity remuneration mechanisms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12871v1</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>q-fin.CP</category>
      <category>q-fin.PR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Millend Roy, Agostino Capponi, Vladimir Pyltsov, Yinbo Hu, Vijay Modi</dc:creator>
    </item>
    <item>
      <title>Understanding When Graph Convolutional Networks Help: A Diagnostic Study on Label Scarcity and Structural Properties</title>
      <link>https://arxiv.org/abs/2512.12947</link>
      <description>arXiv:2512.12947v1 Announce Type: cross 
Abstract: Graph Convolutional Networks (GCNs) have become a standard approach for semi-supervised node classification, yet practitioners lack clear guidance on when GCNs provide meaningful improvements over simpler baselines. We present a diagnostic study using the Amazon Computers co-purchase data to understand when and why GCNs help. Through systematic experiments with simulated label scarcity, feature ablation, and per-class analysis, we find that GCN performance depends critically on the interaction between graph homophily and feature quality. GCNs provide the largest gains under extreme label scarcity, where they leverage neighborhood structure to compensate for limited supervision. Surprisingly, GCNs can match their original performance even when node features are replaced with random noise, suggesting that structure alone carries sufficient signal on highly homophilous graphs. However, GCNs hurt performance when homophily is low and features are already strong, as noisy neighbors corrupt good predictions. Our quadrant analysis reveals that GCNs help in three of four conditions and only hurt when low homophily meets strong features. These findings offer practical guidance for practitioners deciding whether to adopt graph-based methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.12947v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nischal Subedi, Ember Kerstetter, Winnie Li, Silo Murphy</dc:creator>
    </item>
    <item>
      <title>The EEPAS Model Revisited: Statistical Formalism and a High-Performance, Reproducible Open-Source Framework</title>
      <link>https://arxiv.org/abs/2512.13064</link>
      <description>arXiv:2512.13064v2 Announce Type: cross 
Abstract: While short-term models such as the Short-Term Earthquake Probability (STEP) and Epidemic-Type Aftershock Sequence (ETAS) are well established and supported by open-source software, medium- to long-term models, notably the Every Earthquake a Precursor According to Scale (EEPAS) and Proximity to Past Earthquakes (PPE), remain under-documented and largely inaccessible. Despite outperforming time-invariant models in regional studies, their mathematical foundations are often insufficiently formalized. This study addresses these gaps by formally deriving the EEPAS and PPE models within the framework of inhomogeneous Poisson point processes and clarifying the connection between empirical $\Psi$-scaling regressions and likelihood-based inference. We introduce a fully automated, open-source Python implementation of EEPAS that combines analytical modeling with Numba JIT acceleration, NumPy vectorization, and joblib parallelization, all configured via modular JSON files for usability and reproducibility. Integration with pyCSEP enables standardized evaluation and comparison. When applied to the Italy HORUS dataset, our system reproduces published results within one hour using identical initialization settings. It also provides a comprehensive pipeline from raw catalog to parameter estimation, achieving improved log-likelihoods and passing strict consistency tests without manual $\Psi$ identification. We position our framework as part of a growing open-source ecosystem for seismological research that spans the full workflow from data acquisition to forecast evaluation. Our framework fills a key gap in this ecosystem by providing robust tools for medium- to long-term statistical modeling of earthquake catalogs, which is an essential but underserved component in probabilistic seismic forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13064v2</guid>
      <category>physics.geo-ph</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Szu-Chi Chung, Chien-Hong Cho, Strong Wen</dc:creator>
    </item>
    <item>
      <title>Data-driven inverse uncertainty quantification: application to the Chemical Vapor Deposition Reactor Modeling</title>
      <link>https://arxiv.org/abs/2512.13354</link>
      <description>arXiv:2512.13354v1 Announce Type: cross 
Abstract: This study presents a Bayesian framework for (inverse) uncertainty quantification and parameter estimation in a two-step Chemical Vapor Deposition coating process using production data. We develop an XGBoost surrogate model that maps reactor setup parameters to coating thickness measurements, enabling efficient Bayesian analysis while reducing sampling costs. The methodology handles a mixture of data including continuous, discrete integer, binary, and encoded categorical variables. We establish parameter prior distributions through Bayesian Model Selection and perform Inverse Uncertainty Quantification via weighted Approximate Bayesian Computation with summary statistics, providing robust parameter credible intervals while filtering measurement noise across multiple reactor locations. Furthermore, we employ clustering methods guided by geometry embeddings to focus analysis within homogeneous production groups. This integrated approach provides a validated tool for improving industrial process control under uncertainty.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13354v1</guid>
      <category>stat.OT</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geremy Loacham\'in, Eleni D. Koronaki, Dimitrios G. Giovanis, Martin Kathrein, Christoph Czettl, Andreas G. Boudouvis, St\'ephane P. A. Bordas</dc:creator>
    </item>
    <item>
      <title>Automatic Quality Control for Agricultural Field Trials -- Detection of Nonstationarity in Grid-indexed Data</title>
      <link>https://arxiv.org/abs/2512.13383</link>
      <description>arXiv:2512.13383v1 Announce Type: cross 
Abstract: A common assumption in the spatial analysis of agricultural field trials is stationarity. In practice, however, this assumption is often violated due to unaccounted field effects. For instance, in plant breeding field trials, this can lead to inaccurate estimates of plant performance. Based on such inaccurate estimates, breeders may be impeded in selecting the best performing plant varieties, slowing breeding progress. We propose a method to automatically verify the hypothesis of stationarity. The method is sensitive towards mean as well as variance-covariance nonstationarity. It is specifically developed for the two-dimensional grid-structure of field trials. The method relies on the hypothesis that we can detect nonstationarity by partitioning the field into areas, within which stationarity holds. We applied the method to a large number of simulated datasets and a real-data example. The method reliably points out which trials exhibit quality issues and gives an indication about the severity of nonstationarity. This information can significantly reduce the time spent on manual quality control and enhance its overall reliability. Furthermore, the output of the method can be used to improve the analysis of conducted trials as well as the experimental design of future trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13383v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Karen Wolf, Pierre Fernique, Hans-Peter Piepho</dc:creator>
    </item>
    <item>
      <title>Data Quality Issues in Flare Prediction using Machine Learning Models</title>
      <link>https://arxiv.org/abs/2512.13417</link>
      <description>arXiv:2512.13417v1 Announce Type: cross 
Abstract: Machine learning models for forecasting solar flares have been trained and tested using a variety of data sources, such as Space Weather Prediction Center (SWPC) operational and science-quality data. Typically, data from these sources is minimally processed before being used to train and validate a forecasting model. However, predictive performance can be impaired if defects in and inconsistencies between these data sources are ignored. For a number of commonly used data sources, together with softwares that query and then output processed data, we identify their respective defects and inconsistencies, quantify their extent, and show how they can affect the predictions produced by data-driven machine learning forecasting models. We also outline procedures for fixing these issues or at least mitigating their impacts. Finally, based on our thorough comparisons of the impacts of data sources on the trained forecasting model in terms of predictive skill scores, we offer recommendations for the use of different data products in operational forecasting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13417v1</guid>
      <category>astro-ph.SR</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Ke Hu, Kevin Jin, Victor Verma, Weihao Liu, Ward Manchester IV, Lulu Zhao, Tamas Gombosi, Yang Chen</dc:creator>
    </item>
    <item>
      <title>A Metadata-Only Feature-Augmented Method Factor for Ex-Post Correction and Attribution of Common Method Variance</title>
      <link>https://arxiv.org/abs/2512.13446</link>
      <description>arXiv:2512.13446v1 Announce Type: cross 
Abstract: Common Method Variance (CMV) is a recurring problem that reduces survey accuracy. Popular fixes such as the Harman single-factor test, correlated uniquenesses, common latent factor models, and marker variable approaches have well known flaws. These approaches either poorly identify issues, rely too heavily on researchers' choices, omit real information, or require special marker items that many datasets lack. This paper introduces a metadata-only Feature-Augmented Method Factor (FAMF-SEM): a single extra method factor with fixed, item-specific weights based on questionnaire details like reverse coding, page and item order, scale width, wording direction, and item length. These weights are set using ridge regression, based on residual correlations in a basic CFA, and remain fixed in the model. The method avoids the need for additional data or marker variables and provides CMV-adjusted results with clear links to survey design features. An AMOS/LISREL-friendly, no-code Excel workflow demonstrates the method. The paper explains the rationale, provides model details, outlines setup, presents step-by-step instructions, describes checks and reliability tests, and notes limitations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13446v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Murat Yaslioglu</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes learning from selectively reported confidence intervals</title>
      <link>https://arxiv.org/abs/2512.13622</link>
      <description>arXiv:2512.13622v1 Announce Type: cross 
Abstract: We develop a statistical framework for empirical Bayes learning from selectively reported confidence intervals, applied here to provide context for interpreting results published in MEDLINE abstracts. A collection of 326,060 z-scores from MEDLINE abstracts (2000-2018) provides context for interpreting individual studies; we formalize this as an empirical Bayes task complicated by selection bias. We address selection bias through a selective tilting approach that extends empirical Bayes confidence intervals to truncated sampling mechanisms. Sign information is unreliable (a positive z-score need not indicate benefit, and investigators may choose contrast directions post hoc), so we work with absolute z-scores and identify only the distribution of absolute signal-to-noise ratios (SNRs). Our framework provides coverage guarantees for functionals including posterior estimands describing idealized replications and the symmetrized posterior mean, which we justify decision-theoretically as optimal among sign-equivariant (odd) estimators and minimax among priors inducing the same absolute SNR distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.13622v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hunter Chen, Junming Guan, Erik van Zwet, Nikolaos Ignatiadis</dc:creator>
    </item>
    <item>
      <title>Sequential Design for the Efficient Estimation of Offshore Structure Failure Probability</title>
      <link>https://arxiv.org/abs/2509.18319</link>
      <description>arXiv:2509.18319v2 Announce Type: replace 
Abstract: Estimation of the failure probability of offshore structures exposed to extreme ocean environments is critical to their safe design and operation. The conditional density of the environment (CDE) quantifies regions of the space of long term environment responsible for extreme structural response. Moreover, the probability of structural failure is obtained by simply integrating the CDE over the environment space. In this work, two methodologies for estimation of the CDE and failure probability are considered. The first (IS-PT) combines parallel tempering MCMC (for CDE estimation) with important sampling (for eventual estimation of failure probability). The second (AGE) combines adaptive Gaussian emulation with Bayesian quadrature. We evaluate IS-PT and two variants of the AGE procedure in application to a simple synthetic structure with multimodal CDE, and a monopile structure exhibiting non-linear resonant response. IS-PT provides reliable results for both applications for lesser compute cost than naive integration. The AGE procedures require balancing exploration and exploitation of the environment space, using a typically-unknown weight parameter, lambda. When lambda is known, perhaps from prior engineering knowledge, AGE provides a further reduction in computational cost over IS-PT. However, when unknown, IS-PT is more reliable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.18319v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew Speers, Jonathan Angus Tawn, Philip Jonathan</dc:creator>
    </item>
    <item>
      <title>Z-scores-based methods and their application to biological monitoring: An extended analysis of professional soccer players and cyclists athletes</title>
      <link>https://arxiv.org/abs/2510.01810</link>
      <description>arXiv:2510.01810v2 Announce Type: replace 
Abstract: The increase in the collection of biological data allows for the individual and longitudinal monitoring of hematological or urine biomarkers. However, identifying abnormal behavior in these biological sequences is not trivial. Moreover, the complexity of the biological data (correlation between biomarkers, seasonal effects, etc.) is also an issue. Z-score methods can help assess the abnormality in these longitudinal sequences while capturing some features of the biological complexity. This work details a statistical framework for handling biological sequences using three custom Z-score methods in the intra-individual variability scope. These methods can detect abnormal samples in the longitudinal sequences with respect to the seasonality, chronological time or correlation between biomarkers. One of these methods is an extension of one custom Z-score method to the Gaussian linear model, which allows for including additional variables in the model design. We illustrate the use of the framework on the longitudinal data of 3,936 professional soccer players (5 biomarkers) and 1,683 amateur or professional cyclists (10 biomarkers). The results show that a particular Z-score method, designed to detect a change in a series of consecutive observations, measured a high proportion of abnormal values (more than three times the false positive rate) in the ferritin and IGF1 biomarkers for both data sets. The proposed framework and methods could be applied in other contexts, such as the clinical patient follow-up in monitoring abnormal values of biological markers. The methods are flexible enough to include more complicated biological features, which can be directly incorporated into the model design.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.01810v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Geoffroy C. B. Berthelot (IRMES - URP\_7329, RELAIS), Brigitte Gelein (IRMAR), Eric Meinadier (FFC), Emmanuel Orhant (FFF), J\'er\^ome Dedecker (MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>Source apportionment of air pollution burden using geometric non-negative matrix factorization and high-throughput multi-pollutant air sensor data in Curtis Bay, Baltimore, USA</title>
      <link>https://arxiv.org/abs/2511.11833</link>
      <description>arXiv:2511.11833v2 Announce Type: replace 
Abstract: Air sensor networks provide hyperlocal, high temporal resolution data on multiple pollutants that can support credible identification of common pollution sources. Source apportionment using least squares-based non-negative matrix factorization is non-unique and often does not scale. A recent geometric source apportionment framework focuses inference on the source attribution matrix, which is shown to remain identifiable even when the factorization is not. Recognizing that the method scales with and benefits from large data volumes, we use this geometric method to analyze 451,946 one-minute air sensor records from Curtis Bay, collected from October 21, 2022 to June 16, 2023, covering size-resolved particulate matter (PM), black carbon (BC), carbon monoxide (CO), nitric oxide (NO), and nitrogen dioxide (NO2). The analysis identifies three stable sources. Source 1 explains &gt; 70% of fine and coarse PM and ~30% of BC. Source 2 dominates CO and contributes ~70% of BC, NO, and NO2. Source 3 is specific to the larger PM fractions, PM10 to PM40. Regression analyses show Source 1 and Source 3 rise during bulldozer activity at a nearby coal terminal and under winds from the terminal, indicating a direct coal terminal influence, while Source 2 exhibits diurnal patterns consistent with traffic. A case-study on the day with a known bulldozer incident at the coal terminal further confirms the association of terminal activities with Sources 1 and 3. Extreme episodes identified from Source 1 intensity affected ~33 minutes per day at the study site nearest the coal terminal, with impacts attenuating at locations farther from the terminal. The results are stable under sensitivity analyses. The analysis demonstrates that geometric source apportionment, paired with high temporal resolution data from multi-pollutant air sensor networks, delivers scalable and reliable evidence to inform mitigation strategies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.11833v2</guid>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bora Jin, Bonita D. Salmer\'on, David McClosky, David H. Hagan, Russell R. Dickerson, Nicholas J. Spada, Lauren N. Deanes, Matthew A. Aubourg, Laura E. Schmidt, Gregory G. Sawtell, Christopher D. Heaney, Abhirup Datta</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate and Localizing Power</title>
      <link>https://arxiv.org/abs/2401.03554</link>
      <description>arXiv:2401.03554v3 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) is commonly used for correction for multiple testing in neuroimaging studies. However, when using two-tailed tests, making directional inferences about the results can lead to a vastly inflated error rate, even approaching 100% in some cases. This happens because FDR controls the error rate only globally, over all tests, not within subsets, such as among those in only one or another direction. Here we consider and evaluate different strategies for FDR control in such cases, using both synthetic and real imaging data. Approaches that separate the tests by direction of the hypothesis test, or by the direction of the resulting test statistic, more properly control the directional error rate and preserve FDR benefits, albeit with a doubled risk of errors under complete absence of signal. Strategies that combine tests in both directions, or that use simple two-tailed p-values, can lead to invalid directional conclusions, even if these tests remain globally valid. A solution to this problem is through the use of selective inference, whereby positive and negative tails are treated as sets (families), which are screened locally, then subjected to FDR at a modified level that controls average FDR over those that survive the initial screening. Moreover, the BKY procedure can be used in place of the well-known Benjamini-Hochberg, yielding additional power. These methods are easy to implement. Finally, to enable valid thresholding for directional inference, we suggest that imaging software should allow the user to set asymmetrical thresholds for the two sides of the statistical map. While FDR continues to be a valid, powerful procedure for multiple testing correction, care is needed when making directional inferences for two-tailed tests, or more broadly, when making any localized inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03554v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anderson M. Winkler, Paul A. Taylor, Thomas E. Nichols, Chris Rorden</dc:creator>
    </item>
    <item>
      <title>Generative AI-based data augmentation for improved bioacoustic classification in noisy environments</title>
      <link>https://arxiv.org/abs/2412.01530</link>
      <description>arXiv:2412.01530v3 Announce Type: replace-cross 
Abstract: Obtaining data to train robust artificial intelligence (AI)-based models for species classification can be challenging, particularly for rare species. Data augmentation can boost classification accuracy by increasing the diversity of training data and is cheaper to obtain than expert-labelled data. However, many classic image-based augmentation techniques are not suitable for audio spectrograms. We investigate two generative AI models as data augmentation tools to synthesise spectrograms and supplement audio data: Auxiliary Classifier Generative Adversarial Networks (ACGAN) and Denoising Diffusion Probabilistic Models (DDPMs). The latter performed particularly well in terms of both realism of generated spectrograms and accuracy in a resulting classification task. Alongside these new approaches, we present a new audio data set of 640 hours of bird calls from wind farm sites in Ireland, approximately 800 samples of which have been labelled by experts. Wind farm data are particularly challenging for classification models given the background wind and turbine noise. Training an ensemble of classification models on real and synthetic data combined compared well with highly confident BirdNET predictions. Each classifier we used was improved by including synthetic data, and classification metrics generally improved in line with the amount of synthetic data added. Our approach can be used to augment acoustic signals for more species and other land-use types, and has the potential to bring about advances in our capacity to develop reliable AI-based detection of rare species. Our code is available at https://github.com/gibbona1/SpectrogramGenAI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01530v3</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anthony Gibbons, Emma King, Ian Donohue, Andrew Parnell</dc:creator>
    </item>
    <item>
      <title>Compact Neural Network Algorithm for Electrocardiogram Classification</title>
      <link>https://arxiv.org/abs/2412.17852</link>
      <description>arXiv:2412.17852v2 Announce Type: replace-cross 
Abstract: In this paper, we present a powerful, compact electrocardiogram (ECG) classification algorithm for cardiac arrhythmia diagnosis that addresses the current reliance on deep learning and convolutional neural networks (CNNs) in ECG analysis. This work aims to reduce the demand for deep learning, which often requires extensive computational resources and large labeled datasets. Our approach introduces an artificial neural network (ANN) with a simple architecture combined with advanced feature engineering techniques. A key contribution of this work is the incorporation of 17 engineered features that enable the extraction of critical patterns from raw ECG signals. By integrating mathematical transformations, signal processing methods, and data extraction algorithms, our model captures the morphological and physiological characteristics of ECG signals with high efficiency, without requiring deep learning. Our method demonstrates a similar performance to other state-of-the-art models in classifying 4 types of arrhythmias, including atrial fibrillation, sinus tachycardia, sinus bradycardia, and ventricular flutter. Our algorithm achieved an accuracy of 97.36% on the MIT-BIH and St. Petersburg INCART arrhythmia databases. Our approach offers a practical and feasible solution for real-time diagnosis of cardiac disorders in medical applications, particularly in resource-limited environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.17852v2</guid>
      <category>eess.SP</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mateo Frausto-Avila, Jos\'e Pablo Manriquez-Amavizca, Ana Karen Susana Rocha-Robledo, Mario A. Quiroz-Juarez, Alfred U'Ren</dc:creator>
    </item>
    <item>
      <title>CRPS-Based Targeted Sequential Design with Application in Chemical Space</title>
      <link>https://arxiv.org/abs/2503.11250</link>
      <description>arXiv:2503.11250v2 Announce Type: replace-cross 
Abstract: Sequential design of real and computer experiments via Gaussian Process (GP) models has proven useful for parsimonious, goal-oriented data acquisition purposes. In this work, we focus on acquisition strategies for a GP model that needs to be accurate within a predefined range of the response of interest. Such an approach is useful in various fields including synthetic chemistry, where finding molecules with particular properties is essential for developing useful materials and effective medications. GP modeling and sequential design of experiments have been successfully applied to a plethora of domains, including molecule research. Our main contribution here is to use the threshold-weighted Continuous Ranked Probability Score (CRPS) as a basic building block for acquisition functions employed within sequential design. We study pointwise and integral criteria relying on two different weighting measures and benchmark them against competitors, demonstrating improved performance with respect to considered goals. The resulting acquisition strategies are applicable to a wide range of fields and pave the way to further developing sequential design relying on scoring rules.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.11250v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lea Friedli, Ath\'ena\"is Gautier, Anna Broccard, David Ginsbourger</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit testing for the stationary density of a size-structured PDE</title>
      <link>https://arxiv.org/abs/2506.05103</link>
      <description>arXiv:2506.05103v2 Announce Type: replace-cross 
Abstract: We consider two division models for structured cell populations, where cells can grow, age and divide. These models have been introduced in the literature under the denomination of `mitosis' and `adder' models. In the recent years, there has been an increasing interest in biology to understand whether the cells divide equally or not, as this can be related to important mechanisms in cellular aging or recovery. We are therefore interested in testing the null hypothesis $H_0$ where the division of a mother cell results into two daughters of equal size, against the alternative hypothesis $H_1$ where the division is asymmetric and ruled by a kernel that is absolutely continuous with respect to the Lebesgue measure. The sample consists of i.i.d. observations of cell sizes and ages drawn from the population, and the division is not directly observed. The hypotheses of the test are reformulated as hypotheses on the stationary size and age distributions of the models, which we assume are also the distributions of the observations. We propose a goodness-of-fit test that we study numerically on simulated data before applying it on real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.05103v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Van Ha Hoang, Phu Thanh Nguyen, Thanh Mai Pham Ngoc, Vincent Rivoirard, Viet Chi Tran</dc:creator>
    </item>
    <item>
      <title>Approaches for modelling the term-structure of default risk under IFRS 9: A tutorial using discrete-time survival analysis</title>
      <link>https://arxiv.org/abs/2507.15441</link>
      <description>arXiv:2507.15441v3 Announce Type: replace-cross 
Abstract: Under the International Financial Reporting Standards (IFRS) 9, credit losses ought to be recognised timeously and accurately. This requirement belies a certain degree of dynamicity when estimating the constituent parts of a credit loss event, most notably the probability of default (PD). It is notoriously difficult to produce such PD-estimates at every point of loan life that are adequately dynamic and accurate, especially when considering the ever-changing macroeconomic background. In rendering these lifetime PD-estimates, the choice of modelling technique plays an important role, which is why we first review a few classes of techniques, including the merits and limitations of each. Our main contribution however is the development of an in-depth and data-driven tutorial using a particular class of techniques called discrete-time survival analysis. This tutorial is accompanied by a diverse set of reusable diagnostic measures for evaluating various aspects of a survival model and the underlying data. A comprehensive R-based codebase is further contributed. We believe that our work can help cultivate common modelling practices under IFRS 9, and should be valuable to practitioners, model validators, and regulators alike.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15441v3</guid>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arno Botha, Tanja Verster</dc:creator>
    </item>
    <item>
      <title>Are penalty shootouts better than a coin toss? Evidence from international club football in Europe</title>
      <link>https://arxiv.org/abs/2510.17641</link>
      <description>arXiv:2510.17641v3 Announce Type: replace-cross 
Abstract: Penalty shootouts play an important role in the knockout stage of major football tournaments, especially since the 2021/22 season, when the Union of European Football Associations (UEFA) scrapped the away goals rule in its club competitions. Inspired by this rule change, our paper examines whether the outcome of a penalty shootout can be predicted in UEFA club competitions. Based on all shootouts between 2000 and 2025, we find no evidence for the effect of the kicking order, the field of the match, or psychological momentum. In contrast to previous results, stronger teams, defined first by Elo ratings, do not perform better than their weaker opponents. Consequently, penalty shootouts seem to be close to a coin toss in top European club football.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.17641v3</guid>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>L\'aszl\'o Csat\'o, D\'ora Gr\'eta Petr\'oczy</dc:creator>
    </item>
    <item>
      <title>Developing synthetic microdata through machine learning for firm-level business surveys</title>
      <link>https://arxiv.org/abs/2512.05948</link>
      <description>arXiv:2512.05948v2 Announce Type: replace-cross 
Abstract: Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.05948v2</guid>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Cisneros, Timothy Wojan, Matthew Williams, Jennifer Ozawa, Robert Chew, Kimberly Janda, Timothy Navarro, Michael Floyd, Christine Task, Damon Streat</dc:creator>
    </item>
    <item>
      <title>Rice Price Dynamics during the 1945--1947 Famine in Post-War Taiwan: A Quantitative Reassessment</title>
      <link>https://arxiv.org/abs/2512.07492</link>
      <description>arXiv:2512.07492v2 Announce Type: replace-cross 
Abstract: We compiled the first high-frequency rice price panel for Taiwan from August 1945 to March 1947, during the transition from Japanese rule to China rule. Using regression models, we found that the pattern of rice price changes could be divided into four stages, each with distinct characteristics. Based on different stages, we combined the policies formulated by the Taiwan government at the time to demonstrate the correlation between rice prices and policies. The research results highlight the dominant role of policy systems in post-war food crises.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.07492v2</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <pubDate>Tue, 16 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Huai-de Chen, Hai-liang Yang</dc:creator>
    </item>
  </channel>
</rss>
