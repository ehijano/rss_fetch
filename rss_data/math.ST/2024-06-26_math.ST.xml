<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Jun 2024 04:00:36 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 27 Jun 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Minimax-robust estimation problems for stationary stochastic sequences</title>
      <link>https://arxiv.org/abs/2406.17917</link>
      <description>arXiv:2406.17917v1 Announce Type: new 
Abstract: This survey provides an overview of optimal estimation of linear functionals which depend on the unknown values of a stationary stochastic sequence. Based on observations of the sequence without noise as well as observations of the sequence with a stationary noise, estimates could be obtained. Formulas for calculating the spectral characteristics and the mean-square errors of the optimal estimates of functionals are derived in the case of spectral certainty, where spectral densities of the sequences are exactly known. In the case of spectral uncertainty, where spectral densities of the sequences are not known exactly while sets of admissible spectral densities are given, the minimax-robust method of estimation is applied. Formulas that determine the least favourable spectral densities and the minimax spectral characteristics of estimates are presented for some special classes of admissible spectral densities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17917v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.19139/soic.v3i4.173</arxiv:DOI>
      <arxiv:journal_reference>Statistics, Optimization &amp; Information Computing, 3(4), 348-419, 2015</arxiv:journal_reference>
      <dc:creator>Mikhail Moklyachuk</dc:creator>
    </item>
    <item>
      <title>On the estimation of varextropy under complete data</title>
      <link>https://arxiv.org/abs/2406.18195</link>
      <description>arXiv:2406.18195v1 Announce Type: new 
Abstract: In this paper, we propose nonparametric estimators for varextropy function of an absolutely continuous random variable. Consistency of the estimators is established under suitable regularity conditions. Moreover, a simulation study is performed to compare the performance of the proposed estimators based on mean squared error (MSE) and bias. Furthermore, by using the proposed estimators some tests are constructed for uniformity. It is shown that the varextropybased test proposed here performs well compared to the power of the other uniformity hypothesis tests.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18195v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>F. Goodarzi, R. Zamini</dc:creator>
    </item>
    <item>
      <title>Sub-Gaussian High-Dimensional Covariance Matrix Estimation under Elliptical Factor Model with 2 + {\epsilon}th Moment</title>
      <link>https://arxiv.org/abs/2406.18347</link>
      <description>arXiv:2406.18347v1 Announce Type: new 
Abstract: We study the estimation of high-dimensional covariance matrices under elliptical factor models with 2 + {\epsilon}th moment. For such heavy-tailed data, robust estimators like the Huber-type estimator in Fan, Liu and Wang (2018) can not achieve sub-Gaussian convergence rate. In this paper, we develop an idiosyncratic-projected self-normalization (IPSN) method to remove the effect of heavy-tailed scalar parameter, and propose a robust pilot estimator for the scatter matrix that achieves the sub-Gaussian rate. We further develop an estimator of the covariance matrix and show that it achieves a faster convergence rate than the generic POET estimator in Fan, Liu and Wang (2018).</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18347v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yi Ding, Xinghua Zheng</dc:creator>
    </item>
    <item>
      <title>Second Maximum of a Gaussian Random Field and Exact (t-)Spacing test</title>
      <link>https://arxiv.org/abs/2406.18397</link>
      <description>arXiv:2406.18397v1 Announce Type: new 
Abstract: In this article, we introduce the novel concept of the second maximum of a Gaussian random field on a Riemannian submanifold. This second maximum serves as a powerful tool for characterizing the distribution of the maximum. By utilizing an ad-hoc Kac Rice formula, we derive the explicit form of the maximum's distribution, conditioned on the second maximum and some regressed component of the Riemannian Hessian. This approach results in an exact test, based on the evaluation of spacing between these maxima, which we refer to as the spacing test.
  We investigate the applicability of this test in detecting sparse alternatives within Gaussian symmetric tensors, continuous sparse deconvolution, and two-layered neural networks with smooth rectifiers. Our theoretical results are supported by numerical experiments, which illustrate the calibration and power of the proposed tests. More generally, this test can be applied to any Gaussian random field on a Riemannian manifold, and we provide a general framework for the application of the spacing test in continuous sparse kernel regression.
  Furthermore, when the variance-covariance function of the Gaussian random field is known up to a scaling factor, we derive an exact Studentized version of our test, coined the $t$-spacing test. This test is perfectly calibrated under the null hypothesis and has high power for detecting sparse alternatives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18397v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>math.DG</category>
      <category>math.PR</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Aza\"is Jean-Marc, Dalmao Federico, De Castro Yohann</dc:creator>
    </item>
    <item>
      <title>Integral representations for the joint survival functions of the cumulated components of multinomial random vectors</title>
      <link>https://arxiv.org/abs/2406.18509</link>
      <description>arXiv:2406.18509v1 Announce Type: new 
Abstract: This paper presents a multivariate normal integral expression for the joint survival function of the cumulated components of any multinomial random vector. This result can be viewed as a multivariate analog of Equation (7) from Carter &amp; Pollard (2004), who improved Tusn\'ady's inequality. Our findings are based on a crucial relationship between the joint survival function of the cumulated components of any multinomial random vector and the joint cumulative distribution function of a corresponding Dirichlet distribution. We offer two distinct proofs: the first expands the logarithm of the Dirichlet density, while the second employs Laplace's method applied to the Dirichlet integral.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18509v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fr\'ed\'eric Ouimet</dc:creator>
    </item>
    <item>
      <title>Distribution Learnability and Robustness</title>
      <link>https://arxiv.org/abs/2406.17814</link>
      <description>arXiv:2406.17814v1 Announce Type: cross 
Abstract: We examine the relationship between learnability and robust (or agnostic) learnability for the problem of distribution learning. We show that, contrary to other learning settings (e.g., PAC learning of function classes), realizable learnability of a class of probability distributions does not imply its agnostic learnability. We go on to examine what type of data corruption can disrupt the learnability of a distribution class and what is such learnability robust against. We show that realizable learnability of a class of distributions implies its robust learnability with respect to only additive corruption, but not against subtractive corruption.
  We also explore related implications in the context of compression schemes and differentially private learnability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17814v1</guid>
      <category>stat.ML</category>
      <category>cs.DS</category>
      <category>cs.IT</category>
      <category>cs.LG</category>
      <category>math.IT</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shai Ben-David, Alex Bie, Gautam Kamath, Tosca Lechner</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes for Dynamic Bayesian Networks Using Generalized Variational Inference</title>
      <link>https://arxiv.org/abs/2406.17831</link>
      <description>arXiv:2406.17831v1 Announce Type: cross 
Abstract: In this work, we demonstrate the Empirical Bayes approach to learning a Dynamic Bayesian Network. By starting with several point estimates of structure and weights, we can use a data-driven prior to subsequently obtain a model to quantify uncertainty. This approach uses a recent development of Generalized Variational Inference, and indicates the potential of sampling the uncertainty of a mixture of DAG structures as well as a parameter posterior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17831v1</guid>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vyacheslav Kungurtsev, Apaar Garg, Aarya Khandelwal, Parth Sandeep Ratogi, Bapi Chatterjee, Jakub Marecek</dc:creator>
    </item>
    <item>
      <title>Neutrosophic Birnbaum-Saunders distribution with applications</title>
      <link>https://arxiv.org/abs/2406.17884</link>
      <description>arXiv:2406.17884v1 Announce Type: cross 
Abstract: Classical statistics deals with determined and precise data analysis. But in reality, there are many cases where the information is not accurate and a degree of impreciseness, uncertainty, incompleteness, and vagueness is observed. In these situations, uncertainties can make classical statistics less accurate. That is where neutrosophic statistics steps in to improve accuracy in data analysis. In this article, we consider the Birnbaum-Saunders distribution (BSD) which is very flexible and practical for real world data modeling. By integrating the neutrosophic concept, we improve the BSD's ability to manage uncertainty effectively. In addition, we provide maximum likelihood parameter estimates. Subsequently, we illustrate the practical advantages of the neutrosophic model using two cases from the industrial and environmental fields. This paper emphasizes the significance of the neutrosophic BSD as a robust solution for modeling and analysing imprecise data, filling a crucial gap left by classical statistical methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17884v1</guid>
      <category>stat.AP</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mansooreh Razmkhah, Mohammad Arashi, Andriette Bekker, Filipe J. Marques</dc:creator>
    </item>
    <item>
      <title>Flexible Conformal Highest Predictive Conditional Density Sets</title>
      <link>https://arxiv.org/abs/2406.18052</link>
      <description>arXiv:2406.18052v1 Announce Type: cross 
Abstract: We introduce our method, conformal highest conditional density sets (CHCDS), that forms conformal prediction sets using existing estimated conditional highest density predictive regions. We prove the validity of the method and that conformal adjustment is negligible under some regularity conditions. In particular, if we correctly specify the underlying conditional density estimator, the conformal adjustment will be negligible. When the underlying model is incorrect, the conformal adjustment provides guaranteed nominal unconditional coverage. We compare the proposed method via simulation and a real data analysis to other existing methods. Our numerical results show that the flexibility of being able to use any existing conditional density estimation method is a large advantage for CHCDS compared to existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18052v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Sampson, Kung-Sik Chan</dc:creator>
    </item>
    <item>
      <title>Functional knockoffs selection with applications to functional data analysis in high dimensions</title>
      <link>https://arxiv.org/abs/2406.18189</link>
      <description>arXiv:2406.18189v1 Announce Type: cross 
Abstract: The knockoffs is a recently proposed powerful framework that effectively controls the false discovery rate (FDR) for variable selection. However, none of the existing knockoff solutions are directly suited to handle multivariate or high-dimensional functional data, which has become increasingly prevalent in various scientific applications. In this paper, we propose a novel functional model-X knockoffs selection framework tailored to sparse high-dimensional functional models, and show that our proposal can achieve the effective FDR control for any sample size. Furthermore, we illustrate the proposed functional model-X knockoffs selection procedure along with the associated theoretical guarantees for both FDR control and asymptotic power using examples of commonly adopted functional linear additive regression models and the functional graphical model. In the construction of functional knockoffs, we integrate essential components including the correlation operator matrix, the Karhunen-Lo\`eve expansion, and semidefinite programming, and develop executable algorithms. We demonstrate the superiority of our proposed methods over the competitors through both extensive simulations and the analysis of two brain imaging datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18189v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinghao Qiao, Mingya Long, Qizhai Li</dc:creator>
    </item>
    <item>
      <title>Asymptotic Uncertainty in the Estimation of Frequency Domain Causal Effects for Linear Processes</title>
      <link>https://arxiv.org/abs/2406.18191</link>
      <description>arXiv:2406.18191v1 Announce Type: cross 
Abstract: Structural vector autoregressive (SVAR) processes are commonly used time series models to identify and quantify causal interactions between dynamically interacting processes from observational data. The causal relationships between these processes can be effectively represented by a finite directed process graph - a graph that connects two processes whenever there is a direct delayed or simultaneous effect between them. Recent research has introduced a framework for quantifying frequency domain causal effects along paths on the process graph. This framework allows to identify how the spectral density of one process is contributing to the spectral density of another. In the current work, we characterise the asymptotic distribution of causal effect and spectral contribution estimators in terms of algebraic relations dictated by the process graph. Based on the asymptotic distribution we construct approximate confidence intervals and Wald type hypothesis tests for the estimated effects and spectral contributions. Under the assumption of causal sufficiency, we consider the class of differentiable estimators for frequency domain causal quantities, and within this class we identify the asymptotically optimal estimator. We illustrate the frequency domain Wald tests and uncertainty approximation on synthetic data, and apply them to analyse the impact of the 10 to 11 year solar cycle on the North Atlantic Oscillation (NAO). Our results confirm a significant effect of the solar cycle on the NAO at the 10 to 11 year time scale.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.18191v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nicolas-Domenic Reiter, Jonas Wahl, Gabriele C. Hegerl, Jakob Runge</dc:creator>
    </item>
    <item>
      <title>When Locally Linear Embedding Hits Boundary</title>
      <link>https://arxiv.org/abs/1811.04423</link>
      <description>arXiv:1811.04423v3 Announce Type: replace 
Abstract: Based on the Riemannian manifold model, we study the asymptotic behavior of a widely applied unsupervised learning algorithm, locally linear embedding (LLE), when the point cloud is sampled from a compact, smooth manifold with boundary. We show several peculiar behaviors of LLE near the boundary that are different from those diffusion-based algorithms. In particular, we show that LLE pointwisely converges to a mixed-type differential operator with degeneracy and we calculate the convergence rate. The impact of the hyperbolic part of the operator is discussed and we propose a clipped LLE algorithm which is a potential approach to recover the Dirichlet Laplace-Beltrami operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:1811.04423v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>Published in Journal of Machine Learning Research 24 (2023), 1-80</arxiv:journal_reference>
      <dc:creator>Hau-tieng Wu, Nan Wu</dc:creator>
    </item>
    <item>
      <title>Choosing observation operators to mitigate model error in Bayesian inverse problems</title>
      <link>https://arxiv.org/abs/2301.04863</link>
      <description>arXiv:2301.04863v4 Announce Type: replace 
Abstract: In statistical inference, a discrepancy between the parameter-to-observable map that generates the data and the parameter-to-observable map that is used for inference can lead to misspecified likelihoods and thus to incorrect estimates. In many inverse problems, the parameter-to-observable map is the composition of a linear state-to-observable map called an `observation operator' and a possibly nonlinear parameter-to-state map called the `model'. We consider such Bayesian inverse problems where the discrepancy in the parameter-to-observable map is due to the use of an approximate model that differs from the best model, i.e. to nonzero `model error'. Multiple approaches have been proposed to address such discrepancies, each leading to a specific posterior. We show how to use local Lipschitz stability estimates of posteriors with respect to likelihood perturbations to bound the Kullback--Leibler divergence of the posterior of each approach with respect to the posterior associated to the best model. Our bounds lead to criteria for choosing observation operators that mitigate the effect of model error for Bayesian inverse problems of this type. We illustrate one such criterion on an advection-diffusion-reaction PDE inverse problem from the literature, and use this example to discuss the importance and challenges of model error-aware inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.04863v4</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nada Cvetkovi\'c, Han Cheng Lie, Harshit Bansal, Karen Veroy</dc:creator>
    </item>
    <item>
      <title>Algorithmic stability implies training-conditional coverage for distribution-free prediction methods</title>
      <link>https://arxiv.org/abs/2311.04295</link>
      <description>arXiv:2311.04295v2 Announce Type: replace 
Abstract: In a supervised learning problem, given a predicted value that is the output of some trained model, how can we quantify our uncertainty around this prediction? Distribution-free predictive inference aims to construct prediction intervals around this output, with valid coverage that does not rely on assumptions on the distribution of the data or the nature of the model training algorithm. Existing methods in this area, including conformal prediction and jackknife+, offer theoretical guarantees that hold marginally (i.e., on average over a draw of training and test data). In contrast, training-conditional coverage is a stronger notion of validity that ensures predictive coverage of the test point for most draws of the training data, and is thus a more desirable property in practice. Training-conditional coverage was shown by Vovk [2012] to hold for the split conformal method, but recent work by Bian and Barber [2023] proves that such validity guarantees are not possible for the full conformal and jackknife+ methods without further assumptions. In this paper, we show that an assumption of algorithmic stability ensures that the training-conditional coverage property holds for the full conformal and jackknife+ methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.04295v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiting Liang, Rina Foygel Barber</dc:creator>
    </item>
    <item>
      <title>Sharp variance estimator and causal bootstrap in stratified randomized experiments</title>
      <link>https://arxiv.org/abs/2401.16667</link>
      <description>arXiv:2401.16667v2 Announce Type: replace 
Abstract: The design-based finite-population asymptotic theory provides a normal approximation for the sampling distribution of the average treatment effect estimator in stratified randomized experiments. The asymptotic variance could be estimated by a Neyman-type conservative variance estimator. However, the variance estimator can be overly conservative, and the asymptotic theory may fail in small samples. To solve these issues, we propose a sharp variance estimator for the weighted difference-in-means in stratified randomized experiments. Furthermore, we propose two causal bootstrap procedures to more accurately approximate the sampling distribution of the weighted difference-in-means estimator. The first causal bootstrap procedure is based on rank-preserving imputation and we prove its second-order refinement over normal approximation. The second causal bootstrap procedure is based on constant-treatment-effect imputation and is applicable in paired experiments. We prove its validity even when the assumption of constant treatment effect is violated for the true potential outcomes. Our analysis is randomization-based or design-based by conditioning on the potential outcomes, with treatment assignment being the sole source of randomness. Numerical studies and two real data applications demonstrate advantages of our proposed methods in finite samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16667v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoyang Yu, Ke Zhu, Hanzhong Liu</dc:creator>
    </item>
    <item>
      <title>Anomaly Detection based on Markov Data: A Statistical Depth Approach</title>
      <link>https://arxiv.org/abs/2406.16759</link>
      <description>arXiv:2406.16759v2 Announce Type: replace 
Abstract: It is the main purpose of this article to extend the notion of statistical depth to the case of sample paths of a Markov chain, a very popular probabilistic model to describe parsimoniously random phenomena with a temporal causality. Initially introduced to define a center-outward ordering of points in the support of a multivariate distribution, depth functions permit to generalize the notions of quantiles and (signed) ranks for observations in $\mathbb{R}^d$ with $d&gt;1$, as well as statistical procedures based on such quantities, for (unsupervised) anomaly detection tasks in particular. In this paper, overcoming the lack of natural order on the torus composed of all possible trajectories of finite length, we develop a general theoretical framework for evaluating the depth of a Markov sample path and recovering it statistically from an estimate of its transition probability with (non-) asymptotic guarantees. We also detail its numerous applications, focusing particularly on anomaly detection, a key task in various fields involving the analysis of (supposedly) Markov time-series ({e.g.} health monitoring of complex infrastructures, security). Beyond the description of the methodology promoted and the statistical analysis carried out to guarantee its validity, numerical experiments are displayed, providing strong empirical evidence of the relevance of the novel concept we introduce here to quantify the degree of abnormality of Markov path sequences of variable length.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.16759v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Carlos Fern\'andez, Stephan Cl\'emen\c{c}on</dc:creator>
    </item>
    <item>
      <title>Bayesian $L_{\frac{1}{2}}$ regression</title>
      <link>https://arxiv.org/abs/2108.03464</link>
      <description>arXiv:2108.03464v2 Announce Type: replace-cross 
Abstract: It is well known that Bridge regression enjoys superior theoretical properties when compared to traditional LASSO. However, the current latent variable representation of its Bayesian counterpart, based on the exponential power prior, is computationally expensive in higher dimensions. In this paper, we show that the exponential power prior has a closed form scale mixture of normal decomposition for $\alpha=(\frac{1}{2})^\gamma, \gamma \in \{1, 2,\ldots\}$. We call these types of priors $L_{\frac{1}{2}}$ prior for short. We develop an efficient partially collapsed Gibbs sampling scheme for computation using the $L_{\frac{1}{2}}$ prior and study theoretical properties when $p&gt;n$. In addition, we introduce a non-separable Bridge penalty function inspired by the fully Bayesian formulation and a novel, efficient coordinate descent algorithm. We prove the algorithm's convergence and show that the local minimizer from our optimisation algorithm has an oracle property. Finally, simulation studies were carried out to illustrate the performance of the new algorithms. Supplementary materials for this article are available online.</description>
      <guid isPermaLink="false">oai:arXiv.org:2108.03464v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiongwen Ke, Yanan Fan</dc:creator>
    </item>
  </channel>
</rss>
