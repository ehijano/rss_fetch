<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.DB updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.DB</link>
    <description>cs.DB updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.DB" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 20 Nov 2025 02:41:33 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>SQL-to-Text Generation with Weighted-AST Few-Shot Prompting</title>
      <link>https://arxiv.org/abs/2511.13907</link>
      <description>arXiv:2511.13907v1 Announce Type: new 
Abstract: SQL-to-Text generation aims at translating structured SQL queries into natural language descriptions, thereby facilitating comprehension of complex database operations for non-technical users. Although large language models (LLMs) have recently demonstrated promising results, current methods often fail to maintain the exact semantics of SQL queries, particularly when there are multiple possible correct phrasings. To address this problem, our work proposes Weighted-AST retrieval with prompting, an architecture that integrates structural query representations and LLM prompting. This method retrieves semantically relevant examples as few-shot prompts using a similarity metric based on an Abstract Syntax Tree (AST) with learned feature weights. Our structure-aware prompting technique ensures that generated descriptions are both fluent and faithful to the original query logic. Numerous experiments on three benchmark datasets - Spider, SParC, and CoSQL show that our method outperforms the current baselines by up to +17.24% in execution Accuracy (EX), performs superior in Exact Match (EM) and provides more consistent semantic fidelity when evaluated by humans, all while preserving competitive runtime performance. These results demonstrate that Weighted-AST prompting is a scalable and effective method for deriving natural language explanations from structured database queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.13907v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Sriom Chakrabarti, Chuangtao Ma, Arijit Khan, Sebastian Link</dc:creator>
    </item>
    <item>
      <title>Fast Verification of Strong Database Isolation (Extended Version)</title>
      <link>https://arxiv.org/abs/2511.14067</link>
      <description>arXiv:2511.14067v1 Announce Type: new 
Abstract: Strong isolation guarantees, such as serializability and snapshot isolation, are essential for maintaining data consistency and integrity in modern databases. Verifying whether a database upholds its claimed guarantees is increasingly critical, as these guarantees form a contract between the vendor and its users. However, this task is challenging, particularly in black-box settings, where only observable system behavior is available and often involves uncertain dependencies between transactions.
  In this paper, we present VeriStrong, a fast verifier for strong database isolation. At its core is a novel formalism called hyper-polygraphs, which compactly captures both certain and uncertain transactional dependencies in database executions. Leveraging this formalism, we develop sound and complete encodings for verifying both serializability and snapshot isolation. To achieve high efficiency, VeriStrong tailors SMT solving to the characteristics of database workloads, in contrast to prior general-purpose approaches. Our extensive evaluation across diverse benchmarks shows that VeriStrong not only significantly outperforms state-of-the-art verifiers on the workloads they support, but also scales to large, general workloads beyond their reach, while maintaining high accuracy in detecting isolation anomalies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14067v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Zhiheng Cai, Si Liu, Hengfeng Wei, Yuxing Chen, Anqun Pan</dc:creator>
    </item>
    <item>
      <title>Chipmink: Efficient Delta Identification for Massive Object Graph</title>
      <link>https://arxiv.org/abs/2511.14162</link>
      <description>arXiv:2511.14162v1 Announce Type: new 
Abstract: Ranging from batch scripts to computational notebooks, modern data science tools rely on massive and evolving object graphs that represent structured data, models, plots, and more. Persisting these objects is critical, not only to enhance system robustness against unexpected failures but also to support continuous, non-linear data exploration via versioning. Existing object persistence mechanisms (e.g., Pickle, Dill) rely on complete snapshotting, often redundantly storing unchanged objects during execution and exploration, resulting in significant inefficiency in both time and storage. Unlike DBMSs, data science systems lack centralized buffer managers that track dirty objects. Worse, object states span various locations such as memory heaps, shared memory, GPUs, and remote machines, making dirty object identification fundamentally more challenging. In this work, we propose a graph-based object store, named Chipmink, that acts like the centralized buffer manager. Unlike static pages in DBMSs, persistence units in Chipmink are dynamically induced by partitioning objects into appropriate subgroups (called pods), minimizing expected persistence costs based on object sizes and reference structure. These pods effectively isolate dirty objects, enabling efficient partial persistence. Our experiments show that Chipmink is general, supporting libraries that rely on shared memory, GPUs, and remote objects. Moreover, Chipmink achieves up to 36.5x smaller storage sizes and 12.4x faster persistence than the best baselines in real-world notebooks and scripts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14162v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Supawit Chockchowwat, Sumay Thakurdesai, Zhaoheng Li, Matthew Krafczyk, Yongjoo Park</dc:creator>
    </item>
    <item>
      <title>Gradient-Based Join Ordering</title>
      <link>https://arxiv.org/abs/2511.14482</link>
      <description>arXiv:2511.14482v1 Announce Type: new 
Abstract: Join ordering is the NP-hard problem of selecting the most efficient sequence in which to evaluate joins (conjunctive, binary operators) in a database query. As the performance of query execution critically depends on this choice, join ordering lies at the core of query optimization. Traditional approaches cast this problem as a discrete combinatorial search over binary trees guided by a cost model, but they often suffer from high computational complexity and limited scalability. We show that, when the cost model is differentiable, the query plans can be continuously relaxed into a soft adjacency matrix representing a superposition of plans. This continuous relaxation, together with a Gumbel-Softmax parameterization of the adjacency matrix and differentiable constraints enforcing plan validity, enables gradient-based search for plans within this relaxed space. Using a learned Graph Neural Network as the cost model, we demonstrate that this gradient-based approach can find comparable and even lower-cost plans compared to traditional discrete local search methods on two different graph datasets. Furthermore, we empirically show that the runtime of this approach scales linearly with query size, in contrast to quadratic or exponential runtimes of classical approaches. We believe this first step towards gradient-based join ordering can lead to more effective and efficient query optimizers in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14482v1</guid>
      <category>cs.DB</category>
      <category>cs.LG</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tim Schwabe, Maribel Acosta</dc:creator>
    </item>
    <item>
      <title>Overview and Prospects of Using Integer Surrogate Keys for Data Warehouse Performance Optimization</title>
      <link>https://arxiv.org/abs/2511.14502</link>
      <description>arXiv:2511.14502v1 Announce Type: new 
Abstract: The aim of this paper is to examine and demonstrate how integer-based datetime labels (integer surrogate keys for time) can optimize data-warehouse and time-series performance, proposing practical formats and algorithms and validating their efficiency on real-world workloads. It is shown that replacing standard DATE and TIMESTAMP types with 32- and 64-bit integer formats reduces storage requirements by 30-60 percent and speeds up query execution by 25-40 percent. The paper presents indexing, aggregation, compression, and batching algorithms demonstrating up to an eightfold increase in throughput. Practical examples from finance, telecommunications, IoT, and scientific research confirm the efficiency and versatility of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14502v1</guid>
      <category>cs.DB</category>
      <category>cs.DC</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5121/csit.2025.152213</arxiv:DOI>
      <arxiv:journal_reference>Stumpf S., Povyshev V. Overview and Prospects of Using Integer Suggogate Keys for Data Warehouse Performance Optimization // Computer Science &amp; Information Technology (CS &amp; IT) - 2025, Vol. 15, No. 22, pp. 181-192</arxiv:journal_reference>
      <dc:creator>Sviatoslav Stumpf, Vladislav Povyshev</dc:creator>
    </item>
    <item>
      <title>Scalable Enforcement of Fine Grained Access Control Policies in Relational Database Management Systems</title>
      <link>https://arxiv.org/abs/2511.14629</link>
      <description>arXiv:2511.14629v1 Announce Type: new 
Abstract: The proliferation of smart technologies and evolving privacy regulations such as the GDPR and CPRA has increased the need to manage fine-grained access control (FGAC) policies in database management systems (DBMSs). Existing approaches to enforcing FGAC policies do not scale to thousands of policies, leading to degraded query performance and reduced system effectiveness. We present Sieve, a middleware for relational DBMSs that combines query rewriting and caching to optimize FGAC policy enforcement. Sieve rewrites a query with guarded expressions that group and filter policies and can efficiently use indexes in the DBMS. It also integrates a caching mechanism with an effective replacement strategy and a refresh mechanism to adapt to dynamic workloads. Experiments on two DBMSs with real and synthetic datasets show that Sieve scales to large datasets and policy corpora, maintaining low query latency and system load and improving policy evaluation performance by between 2x and 10x on workloads with 200 to 1,200 policies. The caching extension further improves query performance by between 6 and 22 percent under dynamic workloads, especially with larger cache sizes. These results highlight Sieve's applicability for real-time access control in smart environments and its support for efficient, scalable management of user preferences and privacy policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14629v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anadi Shakya, Primal Pappachan, David Maier, Roberto Yus, Sharad Mehrotra, Johann-Christoph Freytag</dc:creator>
    </item>
    <item>
      <title>Natural Language Interfaces for Databases: What Do Users Think?</title>
      <link>https://arxiv.org/abs/2511.14718</link>
      <description>arXiv:2511.14718v1 Announce Type: new 
Abstract: Natural Language Interfaces for Databases (NLIDBs) aim to make database querying accessible by allowing users to ask questions in everyday language rather than using formal SQL queries. Despite significant advancements in translation accuracy, critical usability challenges, such as user frustration, query refinement strategies, and error recovery, remain underexplored. To investigate these usability dimensions, we conducted a mixed-method user study comparing SQL-LLM, a state-of-the-art NL2SQL system, with Snowflake, a traditional SQL analytics platform. Our controlled evaluation involved 20 participants completing realistic database querying tasks across 12 queries each. Results show that SQL-LLM significantly reduced query completion times by 10 to 30 percent (mean: 418 s vs. 629 s, p = 0.036) and improved overall accuracy from 50 to 75 percent (p = 0.002). Additionally, participants using SQL-LLM exhibited fewer query reformulations, recovered from errors 30 to 40 seconds faster, and reported lower frustration levels compared to Snowflake users. Behavioral analysis revealed that SQL-LLM encouraged structured, schema-first querying strategies, enhancing user confidence and efficiency, particularly for complex queries. These findings underscore the practical significance of well-designed, user-friendly NLIDBs in business analytics settings, emphasizing the critical role of usability alongside technical accuracy in real-world deployments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14718v1</guid>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panos Ipeirotis, Haotian Zheng</dc:creator>
    </item>
    <item>
      <title>Cloud-Native Vector Search: A Comprehensive Performance Analysis</title>
      <link>https://arxiv.org/abs/2511.14748</link>
      <description>arXiv:2511.14748v1 Announce Type: new 
Abstract: Vector search has been widely employed in recommender system and retrieval-augmented-generation pipelines, commonly performed with vector indexes to efficiently find similar items in large datasets. Recent growths in both data and task complexity have motivated placing vector indexes onto remote storage -- cloud-native vector search, which cloud providers have recently introduced services for. Yet, despite varying workload characteristics and various available vector index forms, providers default to using cluster-based indexes, which on paper do adapt well to differences between disk and cloud-based environment: their fetch granularities and lack of notable intra-query dependencies aligns with the large optimal fetch sizes and minimizes costly round-trips (i.e., as opposed to graph-based indexes) to remote storage, respectively.
  This paper systematically studies cloud-native vector search: What and how should indexes be built and used for on-cloud vector search? We analyze bottlenecks of two common index classes, cluster and graph indexes, on remote storage, and show that despite current standardized adoption of cluster indexes on the cloud, graph indexes are favored in workloads requiring high concurrency and recall, or operating on high-dimensional data or large datatypes. We further find that on-cloud search demands significantly different indexing and search parameterizations versus on-disk search for optimal performance. Finally, we incorporate existing cloud-based caching setups into vector search and find that certain index optimizations work against caching, and study how this can be mitigated to maximize gains under various available cache sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.14748v1</guid>
      <category>cs.DB</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhaoheng Li, Wei Ding, Silu Huang, Zikang Wang, Yuanjin Lin, Ke Wu, Yongjoo Park, Jianjun Chen</dc:creator>
    </item>
    <item>
      <title>Localized RETE for Incremental Graph Queries with Nested Graph Conditions</title>
      <link>https://arxiv.org/abs/2412.13918</link>
      <description>arXiv:2412.13918v5 Announce Type: replace-cross 
Abstract: The growing size of graph-based modeling artifacts in model-driven engineering calls for techniques that enable efficient execution of graph queries. Incremental approaches based on the RETE algorithm provide an adequate solution in many scenarios, but are generally designed to search for query results over the entire graph. However, in certain situations, a user may only be interested in query results for a subgraph, for instance when a developer is working on a large model of which only a part is loaded into their workspace. In this case, the global execution semantics can result in significant computational overhead.
  To mitigate the outlined shortcoming, in this article we propose an extension of the RETE approach that enables local, yet fully incremental execution of graph queries, while still guaranteeing completeness of results with respect to the relevant subgraph.
  We empirically evaluate the presented approach via experiments inspired by a scenario from software development and with queries and data from an independent social network benchmark. The experimental results indicate that the proposed technique can significantly improve performance regarding memory consumption and execution time in favorable cases, but may incur a noticeable overhead in unfavorable cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.13918v5</guid>
      <category>cs.LO</category>
      <category>cs.DB</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthias Barkowsky, Holger Giese</dc:creator>
    </item>
    <item>
      <title>Are We Asking the Right Questions? On Ambiguity in Natural Language Queries for Tabular Data Analysis</title>
      <link>https://arxiv.org/abs/2511.04584</link>
      <description>arXiv:2511.04584v3 Announce Type: replace-cross 
Abstract: Natural language interfaces to tabular data must handle ambiguities inherent to queries. Instead of treating ambiguity as a deficiency, we reframe it as a feature of cooperative interaction where users are intentional about the degree to which they specify queries. We develop a principled framework based on a shared responsibility of query specification between user and system, distinguishing unambiguous and ambiguous cooperative queries, which systems can resolve through reasonable inference, from uncooperative queries that cannot be resolved. Applying the framework to evaluations for tabular question answering and analysis, we analyze the queries in 15 popular datasets, and observe an uncontrolled mixing of query types neither adequate for evaluating a system's execution accuracy nor for evaluating interpretation capabilities. This conceptualization around cooperation in resolving queries informs how to design and evaluate natural language interfaces for tabular data analysis, for which we distill concrete directions for future research and broader implications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.04584v3</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.DB</category>
      <category>cs.HC</category>
      <pubDate>Wed, 19 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Gomm, Cornelius Wolff, Madelon Hulsebos</dc:creator>
    </item>
  </channel>
</rss>
