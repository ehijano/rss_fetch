<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Sep 2025 01:28:38 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A tree-based Polynomial Chaos expansion for surrogate modeling and sensitivity analysis of complex numerical models</title>
      <link>https://arxiv.org/abs/2509.13384</link>
      <description>arXiv:2509.13384v1 Announce Type: new 
Abstract: This paper introduces Tree-based Polynomial Chaos Expansion (Tree-PCE), a novel surrogate modeling technique designed to efficiently approximate complex numerical models exhibiting nonlinearities and discontinuities. Tree-PCE combines the expressive power of Polynomial Chaos Expansion (PCE) with an adaptive partitioning strategy inspired by regression trees. By recursively dividing the input space into hyperrectangular subdomains and fitting localized PCEs, Tree-PCE constructs a piecewise polynomial surrogate that improves both accuracy and computational efficiency. The method is particularly well-suited for global sensitivity analysis, enabling direct computation of Sobol' indices from local expansion coefficients and introducing a new class of sensitivity indices derived from the tree structure itself. Numerical experiments on synthetic and real-world models, including a 2D morphodynamic case, demonstrate that Tree-PCE offers a favorable balance between accuracy and complexity, especially in the presence of discontinuities. While its performance depends on the compromise between the number of subdomains and the degree of local polynomials, this trade-off can be explored using automated hyperparameter optimization frameworks. This opens promising perspectives for systematically identifying optimal configurations and enhancing the robustness of surrogate modeling in complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13384v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Faten Ben Said (CERMICS, EDF R\&amp;D LNHE), Aur\'elien Alfonsi (CERMICS, MATHRISK), Anne Dutfoy (EDF R\&amp;D PERICLES), C\'edric Goeury (EDF R\&amp;D LNHE, LHSV), Magali Jodeau (EDF R\&amp;D LNHE, LHSV), Julien Reygner (CERMICS, RT-UQ), Fabrice Zaoui (EDF R\&amp;D LNHE)</dc:creator>
    </item>
    <item>
      <title>Imputation-Powered Inference</title>
      <link>https://arxiv.org/abs/2509.13778</link>
      <description>arXiv:2509.13778v1 Announce Type: new 
Abstract: Modern multi-modal and multi-site data frequently suffer from blockwise missingness, where subsets of features are missing for groups of individuals, creating complex patterns that challenge standard inference methods. Existing approaches have critical limitations: complete-case analysis discards informative data and is potentially biased; doubly robust estimators for non-monotone missingness-where the missingness patterns are not nested subsets of one another-can be theoretically efficient but lack closed-form solutions and often fail to scale; and blackbox imputation can leverage partially observed data to improve efficiency but provides no inferential guarantees when misspecified. To address the limitations of these existing methods, we propose imputation-powered inference (IPI), a model-lean framework that combines the flexibility of blackbox imputation with bias correction using fully observed data, drawing on ideas from prediction-powered inference and semiparametric inference. IPI enables valid and efficient M-estimation under missing completely at random (MCAR) blockwise missingness and improves subpopulation inference under a weaker assumption we formalize as first-moment MCAR, for which we also provide practical diagnostics. Simulation studies and a clinical application demonstrate that IPI may substantially improve subpopulation efficiency relative to complete-case analysis, while maintaining statistical validity in settings where both doubly robust estimators and naive imputation fail to achieve nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13778v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sarah Zhao, Emmanuel Cand\`es</dc:creator>
    </item>
    <item>
      <title>Bridging Control Variates and Regression Adjustment in A/B Testing: From Design-Based to Model-Based Frameworks</title>
      <link>https://arxiv.org/abs/2509.13944</link>
      <description>arXiv:2509.13944v1 Announce Type: new 
Abstract: A B testing serves as the gold standard for large scale, data driven decision making in online businesses. To mitigate metric variability and enhance testing sensitivity, control variates and regression adjustment have emerged as prominent variance reduction techniques, leveraging pre experiment data to improve estimator performance. Over the past decade, these methods have spawned numerous derivatives, yet their theoretical connections and comparative properties remain underexplored. In this paper, we conduct a comprehensive analysis of their statistical properties, establish a formal bridge between the two frameworks in practical implementations, and extend the investigation from design based to model-based frameworks. Through simulation studies and real world experiments at ByteDance, we validate our theoretical insights across both frameworks. Our work aims to provide rigorous guidance for practitioners in online controlled experiments, addressing critical considerations of internal and external validity. The recommended method control variates with group specific coefficient estimates has been fully implemented and deployed on ByteDance's experimental platform.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13944v1</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu Zhang, Bokui Wan, Yongli Qin</dc:creator>
    </item>
    <item>
      <title>Ensembled Direct Multi Step forecasting methodology with comparison on macroeconomic and financial data</title>
      <link>https://arxiv.org/abs/2509.13945</link>
      <description>arXiv:2509.13945v1 Announce Type: new 
Abstract: Accurate forecasts of macroeconomic and financial data, such as GDP, CPI, unemployment rates, and stock indices, are crucial for the success of countries, businesses, and investors, resulting in a constant demand for reliable forecasting models. This research introduces a novel methodology for time series forecasting that combines Ensemble technique with a Direct Multi-Step (DMS) forecasting procedure. This Ensembled Direct Multi-Step (EDMS) approach not only leverages the strengths of both techniques but also capitalizes on their synergy. The ensemble models were selected based on performance, complexity, and computational resource requirements, encompassing a full spectrum of model complexities, from simple Linear and Polynomial Regression to medium-complexity ETS and complex LSTM models. Ensembling is carried out using weights derived from each model's performance. The DMS procedure limits retraining to one- and five-year forecasts for economic data and one- and five-month forecasts for financial data. The standard Iterative Multi-Step (IMS) procedure is employed for other horizons, effectively reducing computational demands while maintaining satisfactory results. The proposed methodology is benchmarked against the Ensemble technique conventionally applied to IMS-generated forecasts, utilizing several publicly available macroeconomic datasets, including GDP, CPI, and employment figures across selected countries, and common financial indices data. Results demonstrate a significant performance improvement with the EDMS methodology, averaging a 33.32% enhancement across the analysed datasets, and sometimes reaching improvement above 60%.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13945v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomasz M. {\L}api\'nski, Krzysztof Zi\'o{\l}kowski</dc:creator>
    </item>
    <item>
      <title>Time-smoothed inverse probability weighted estimation of effects of generalized time-varying treatment strategies on repeated outcomes truncated by death</title>
      <link>https://arxiv.org/abs/2509.13971</link>
      <description>arXiv:2509.13971v1 Announce Type: new 
Abstract: Researchers are often interested in estimating effects of generalized time-varying treatment strategies on the mean of an outcome at one or more selected follow-up times of interest. For example, the Medications and Weight Gain in PCORnet (MedWeight) study aimed to estimate effects of adhering to flexible medication regimes on future weight change using electronic health records (EHR) data. This problem presents several methodological challenges that have not been jointly addressed in the prior literature. First, this setting involves treatment strategies that vary over time and depend dynamically and non-deterministically on measured confounder history. Second, the outcome is repeatedly, non-monotonically, informatively, and sparsely measured in the data source. Third, some individuals die during follow-up, rendering the outcome of interest undefined at the follow-up time of interest. In this article, we pose a range of inverse probability weighted (IPW) estimators targeting effects of generalized time-varying treatment strategies in truncation by death settings that allow time-smoothing for precision gain. We conducted simulation studies that confirm precision gains of the time-smoothed IPW approaches over more conventional IPW approaches that do not leverage the repeated outcome measurements. We illustrate an application of the IPW approaches to estimate comparative effects of adhering to flexible antidepressant medication strategies on future weight change. The methods are implemented in the accompanying R package, smoothedIPW.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13971v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Takuya Kawahara, Joshua Petimar, Sheryl L. Rifas-Shiman, Iv\'an D\'iaz, Jason P. Block, Jessica G. Young</dc:creator>
    </item>
    <item>
      <title>Sample Size Calculations for the Development of Risk Prediction Models that Account for Performance Variability</title>
      <link>https://arxiv.org/abs/2509.14028</link>
      <description>arXiv:2509.14028v1 Announce Type: new 
Abstract: Existing approaches to sample size calculations for developing clinical prediction models have focused on ensuring that the expected value of a chosen performance measure meets a pre-specified target. For example, to limit model-overfitting, the sample size is commonly chosen such that the expected calibration slope (CS) is 0.9, close to 1 for a perfectly calibrated model. In practice, due to sampling variability, model performance can vary considerably across different development samples of the recommended size. If this variability is high, the probability of obtaining a model with performance close to the target for a given measure may be unacceptably low. To address this, we propose an adapted approach to sample size calculations that explicitly incorporates performance variability by targeting the probability of acceptable performance (PrAP). For example, in the context of calibration, we may define a model as acceptably calibrated if CS falls in a pre-defined range, e.g. between 0.85 and 1.15. Then we choose the required sample size to ensure that PrAP(CS)=80%. For binary outcomes we implemented our approach for CS within a simulation-based framework via the R package `samplesizedev'. Additionally, for CS specifically, we have proposed an equivalent analytical calculation which is computationally efficient. While we focused on CS, the simulation-based framework is flexible and can be easily extended to accommodate other performance measures and types of outcomes. When adhering to existing recommendations, we found that performance variability increased substantially as the number of predictors, p, decreased. Consequently, PrAP(CS) was often low. For example, with 5 predictors, PrAP(CS) was around 50%. Our adapted approach resulted in considerably larger sample sizes, especially for p&lt;10. Applying shrinkage tends to improve PrAP(CS).</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14028v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Menelaos Pavlou, Rumana Z. Omar, Gareth Ambler</dc:creator>
    </item>
    <item>
      <title>Index Date Imputation For Survival Outcomes for Externally Controlled Trials</title>
      <link>https://arxiv.org/abs/2509.14183</link>
      <description>arXiv:2509.14183v1 Announce Type: new 
Abstract: Externally controlled trials (ECTs) compare outcomes between a single-arm trial and external controls drawn from sources such as historical trials, registries, or observational studies. In survival analysis, a major challenge arises when the time origin (index date) differs across groups, for example, when treatment initiation occurs after a delay in the single-arm trial but is undefined in the external controls. This misalignment can bias treatment effect estimates and distort causal interpretation. We propose a novel statistical method, Index Date Imputation (IDI), that imputes comparable index dates for external control patients using the estimated distribution of treatment initiation times from the single-arm cohort. To address population-level confounding, IDI is combined with propensity score weighting or matching, yielding balanced and temporally aligned cohorts for survival comparison. We detail diagnostics for covariate balance and truncation bias, and evaluate performance via extensive simulations. Applying IDI to a randomized oncology trial, we demonstrate that the method recovers the known treatment effect despite artificial index date misalignment. IDI provides a principled framework for time-to-event analyses in ECTs and is broadly applicable in oncology and rare disease settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14183v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Q. Le Coent, G. L. Rosner, M-C. Wang, C. Hu</dc:creator>
    </item>
    <item>
      <title>Covariate-adjusted Group Sequential Comparisons of Restricted Mean Survival Times</title>
      <link>https://arxiv.org/abs/2509.14188</link>
      <description>arXiv:2509.14188v1 Announce Type: new 
Abstract: The restricted mean survival time (RMST) is the mean survival time in the study population followed up to a specific time point, and is simply the area under the survival curve up to the specific time point. The difference between two RMSTs quantifies the group difference in a time scale by measuring the integrated difference between survival curves in the two treatment groups. This paper develops a group sequential (GS) test of comparing two RMSTs up to a restriction time point under a stratified proportional hazards model with stratum representing treatment status. This covariate-adjusted GS test does not require the assumption of a constant hazard ratio over time between the two treatment groups and is valid whether or not the proportional hazards assumption holds for the treatment effect. We establish the large-sample properties of the covariate-adjusted GS test and show that the proposed test statistics sequentially computed at different interim analysis times possess an independent increments covariance structure. Using currently available methodology, this joint independent increments structure allows us to calculate sequential stopping boundaries for preserving the desired type I error probability and maintaining the required statistical power. We evaluate the small-sample performance of the proposed GS test via a simulation study and illustrate its real-world application through analysis of a clinical trial dataset from the BMT CTN 1101 study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14188v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Zhang, Brent Logan, Michael Martens</dc:creator>
    </item>
    <item>
      <title>Adaptive Off-Policy Inference for M-Estimators Under Model Misspecification</title>
      <link>https://arxiv.org/abs/2509.14218</link>
      <description>arXiv:2509.14218v1 Announce Type: new 
Abstract: When data are collected adaptively, such as in bandit algorithms, classical statistical approaches such as ordinary least squares and $M$-estimation will often fail to achieve asymptotic normality. Although recent lines of work have modified the classical approaches to ensure valid inference on adaptively collected data, most of these works assume that the model is correctly specified. We propose a method that provides valid inference for M-estimators that use adaptively collected bandit data with a (possibly) misspecified working model. A key ingredient in our approach is the use of flexible machine learning approaches to stabilize the variance induced by adaptive data collection. A major novelty is that our procedure enables the construction of valid confidence sets even in settings where treatment policies are unstable and non-converging, such as when there is no unique optimal arm and standard bandit algorithms are used. Empirical results on semi-synthetic datasets constructed from the Osteoarthritis Initiative demonstrate that the method maintains type I error control, while existing methods for inference in adaptive settings do not cover in the misspecified case.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14218v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.OT</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>James Leiner, Robin Dunn, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Selective and marginal selective inference for exceptional groups</title>
      <link>https://arxiv.org/abs/2509.13538</link>
      <description>arXiv:2509.13538v1 Announce Type: cross 
Abstract: Statistical analyses of multipopulation studies often use the data to select a particular population as the target of inference. For example, a confidence interval may be constructed for a population only in the event that its sample mean is larger than that of the other populations. We show that for the normal means model, confidence interval procedures that maintain strict coverage control conditional on such a selection event will have infinite expected width. For applications where such selective coverage control is of interest, this result motivates the development of procedures with finite expected width and approximate selective coverage control over a range of plausible parameter values. To this end, we develop selection-adjusted empirical Bayes confidence procedures that use information from the data to approximate an oracle confidence procedure that has exact selective coverage control and finite expected width. In numerical comparisons of the oracle and empirical Bayes procedures to procedures that only guarantee selective coverage control marginally over selection events, we find that improved selective coverage control comes at the cost of increased expected interval width.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13538v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Peter Hoff, Surya Tokdar</dc:creator>
    </item>
    <item>
      <title>Variable Selection for Additive Global Fr\'echet Regression</title>
      <link>https://arxiv.org/abs/2509.13685</link>
      <description>arXiv:2509.13685v1 Announce Type: cross 
Abstract: We present a novel framework for variable selection in Fr\'echet regression with responses in general metric spaces, a setting increasingly relevant for analyzing non-Euclidean data such as probability distributions and covariance matrices. Building on the concept of (weak) Fr\'echet conditional means, we develop an additive regression model that represents the metric-based discrepancy of the response as a sum of covariate-specific nonlinear functions in reproducing kernel Hilbert spaces (RKHS). To address the absence of linear structure in the response space, we transform the response via squared distances, enabling an interpretable and tractable additive decomposition. Variable selection is performed using Elastic Net regularization, extended to the RKHS setting, and further refined through a local linear approximation scheme that incorporates folded concave penalties such as the SCAD. We establish theoretical guarantees, including variable selection consistency and the strong oracle property, under minimal assumptions tailored to metric-space-valued responses. Simulations and applications to distributional and matrix-valued data demonstrate the scalability, interpretability, and practical effectiveness of the proposed approach. This work provides a principled foundation for statistical learning with random object data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13685v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoyi Yang, Satarupa Bhattacharjee, Lingzhou Xue, Bing Li</dc:creator>
    </item>
    <item>
      <title>Three Distributional Approaches for PM10 Assessment in Northern Italy</title>
      <link>https://arxiv.org/abs/2509.13886</link>
      <description>arXiv:2509.13886v1 Announce Type: cross 
Abstract: We propose three spatial methods for estimating the full probability distribution of PM10 concentrations, with the ultimate goal of assessing air quality in Northern Italy. Moving beyond spatial averages and simple indicators, we adopt a distributional perspective to capture the complex variability of pollutant concentrations across space. The first proposed approach predicts class-based compositions via Fixed Rank Kriging; the second estimates multiple, non-crossing quantiles through a spatial regression with differential regularization; the third directly reconstructs full probability densities leveraging on both Fixed Rank Kriging and multiple quantiles spatial regression within a Simplicial Principal Component Analysis framework. These approaches are applied to daily PM10 measurements, collected from 2018 to 2022 in Northern Italy, to estimate spatially continuous distributions and to identify regions at risk of regulatory exceedance. The three approaches exhibit localized differences, revealing how modeling assumptions may influence the prediction of fine-scale pollutant concentration patterns. Nevertheless, they consistently agree on the broader spatial patterns of pollution. This general agreement supports the robustness of a distributional approach, which offers a comprehensive and policy-relevant framework for assessing air quality and regulatory exceedance risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.13886v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marco F. De Sanctis, Andrea Gilardi, Giacomo Milan, Laura M. Sangalli, Francesca Ieva, Piercesare Secchi</dc:creator>
    </item>
    <item>
      <title>Deconstructing Intraocular Pressure: A Non-invasive Multi-Stage Probabilistic Inverse Framework</title>
      <link>https://arxiv.org/abs/2509.14167</link>
      <description>arXiv:2509.14167v1 Announce Type: cross 
Abstract: Many critical healthcare decisions are challenged by the inability to measure key underlying parameters. Glaucoma, a leading cause of irreversible blindness driven by elevated intraocular pressure (IOP), provides a stark example. The primary determinant of IOP, a tissue property called trabecular meshwork permeability, cannot be measured in vivo, forcing clinicians to depend on indirect surrogates. This clinical challenge is compounded by a broader computational one: developing predictive models for such ill-posed inverse problems is hindered by a lack of ground-truth data and prohibitive cost of large-scale, high-fidelity simulations. We address both challenges with an end-to-end framework to noninvasively estimate unmeasurable variables from sparse, routine data. Our approach combines a multi-stage artificial intelligence architecture to functionally separate the problem; a novel data generation strategy we term PCDS that obviates the need for hundreds of thousands of costly simulations, reducing the effective computational time from years to hours; and a Bayesian engine to quantify predictive uncertainty. Our framework deconstructs a single IOP measurement into its fundamental components from routine inputs only, yielding estimates for the unmeasurable tissue permeability and a patient's outflow facility. Our noninvasively estimated outflow facility achieved excellent agreement with state-of-the-art tonography with precision comparable to direct physical instruments. Furthermore, the newly derived permeability biomarker demonstrates high accuracy in stratifying clinical cohorts by disease risk, highlighting its diagnostic potential. More broadly, our framework establishes a generalizable blueprint for solving similar inverse problems in other data-scarce, computationally-intensive domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14167v1</guid>
      <category>cs.LG</category>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Md Rezwan Jaher, Abul Mukid Mohammad Mukaddes, A. B. M. Abdul Malek</dc:creator>
    </item>
    <item>
      <title>Quickest Change Detection with Cost-Constrained Experiment Design</title>
      <link>https://arxiv.org/abs/2509.14186</link>
      <description>arXiv:2509.14186v1 Announce Type: cross 
Abstract: In the classical quickest change detection problem, an observer performs only one experiment to monitor a stochastic process. This paper considers the case where, at each observation time, the decision-maker needs to choose between multiple experiments with different information qualities and costs. The goal is to minimize the worst-case average detection delay subject to false alarm and cost constraints. An algorithm called the 2E-CUSUM Algorithm has been developed to achieve this goal for the two-experiment case. Extensions to multiple-experiment designs are also studied, and 2E-CUSUM is extended accordingly. Data efficiency, where the observer has the choice not to perform an experiment, is explored as well. The proposed algorithms are analyzed and shown to be asymptotically optimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.14186v1</guid>
      <category>eess.SP</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Patrick Vincent N. Lubenia, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>Design and Analysis of Switchback Experiments</title>
      <link>https://arxiv.org/abs/2009.00148</link>
      <description>arXiv:2009.00148v4 Announce Type: replace 
Abstract: Switchback experiments, where a firm sequentially exposes an experimental unit to random treatments, are among the most prevalent designs used in the technology sector, with applications ranging from ride-hailing platforms to online marketplaces. Although practitioners have widely adopted this technique, the derivation of the optimal design has been elusive, hindering practitioners from drawing valid causal conclusions with enough statistical power. We address this limitation by deriving the optimal design of switchback experiments under a range of different assumptions on the order of the carryover effect -- the length of time a treatment persists in impacting the outcome. We cast the optimal experimental design problem as a minimax discrete optimization problem, identify the worst-case adversarial strategy, establish structural results, and solve the reduced problem via a continuous relaxation. For switchback experiments conducted under the optimal design, we provide two approaches for performing inference. The first provides exact randomization based p-values, and the second uses a new finite population central limit theorem to conduct conservative hypothesis tests and build confidence intervals. We further provide theoretical results when the order of the carryover effect is misspecified and provide a data-driven procedure to identify the order of the carryover effect. We conduct extensive simulations to study the numerical performance and empirical properties of our results, and conclude with practical suggestions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2009.00148v4</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iavor Bojinov, David Simchi-Levi, Jinglong Zhao</dc:creator>
    </item>
    <item>
      <title>A Plug-and-Play Method with Inpainting Network for Bayesian Uncertainty Quantification in Imaging</title>
      <link>https://arxiv.org/abs/2304.11200</link>
      <description>arXiv:2304.11200v2 Announce Type: replace 
Abstract: We contribute to an uncertainty quantification problem in imaging that evaluates a hypothesis test questioning the existence of local "artefacts" appearing in the maximum a posteriori (MAP) estimate (obtained from standard numerical tools). Such a method, called Bayesian uncertainty quantification by optimization (BUQO), was introduced a few years ago as an efficient and scalable alternative to sampling methods when per-pixel error-bars are not needed. BUQO formulates a hypothesis test for probing the existence of local structures in the MAP estimate as a minimization problem, that can be solved efficiently with standard optimization algorithms. In this context, BUQO requires a "mathematical" definition of the "local artefact". This definition can be interpreted as an inpainting of the structure. However, only simple hand-crafted techniques have been proposed so far due to the complexity of the problem. In this work, we propose a data-driven alternative to BUQO where the inpainting procedure in the algorithm is performed using a convolutional inpainting neural network (NN). This results in a plug-and-play algorithm, based on the primal-dual Condat-Vu iterations,where the inpainting procedure is performed with a NN. The proposed approach is assessed on two image reconstruction problems inspired by medicine. We specifically perform simulations on two Fourier undersampling problems (discrete and non-uniform) encountered in magnetic resonance imaging, as well as a computed tomography problem using the Radon measurement operator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.11200v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xiaoyu Wang, Michael Tang, Audrey Repetti</dc:creator>
    </item>
    <item>
      <title>Whittaker--Henderson smoothing revisited: A modern statistical framework for practical use</title>
      <link>https://arxiv.org/abs/2306.06932</link>
      <description>arXiv:2306.06932v5 Announce Type: replace 
Abstract: Introduced over a century ago, Whittaker-Henderson smoothing remains widely used by actuaries in constructing one-dimensional and two-dimensional experience tables for mortality, disability and other life insurance risks. In this paper, we reinterpret this smoothing technique within a modern statistical framework and address six practically relevant questions about its use. First, we adopt a Bayesian perspective on this method to construct credible intervals. Second, in the context of survival analysis, we clarify how to choose the observation and weight vectors by linking the smoothing technique to a maximum likelihood estimator. Third, we improve accuracy by relaxing the method's reliance on an implicit normal approximation. Fourth, we select the smoothing parameters by maximizing a marginal likelihood function. Fifth, we improve computational efficiency when dealing with numerous observation points and consequently parameters. Finally, we develop an extrapolation procedure that ensures consistency between estimated and predicted values through constraints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06932v5</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Biessy (LPSM)</dc:creator>
    </item>
    <item>
      <title>Group Sequential Design for Non-Proportional Hazards: Logrank, Weighted Logrank, and MaxCombo Methods</title>
      <link>https://arxiv.org/abs/2312.01723</link>
      <description>arXiv:2312.01723v2 Announce Type: replace 
Abstract: Non-proportional hazards (NPH) are often observed in clinical trials with time-to-event endpoints. A common example is a long-term clinical trial with a delayed treatment effect in immunotherapy for cancer. When designing clinical trials with time-to-event endpoints, it is crucial to consider NPH scenarios to gain a complete understanding of design operating characteristics. In this paper, we focus on group sequential design for three NPH methods: the average hazard ratio, the weighted logrank test, and the MaxCombo combination test. For each of these approaches, we provide analytic forms of design characteristics that facilitate sample size calculation and bound derivation for group sequential designs. Examples are provided to illustrate the proposed methods. To facilitate statisticians in designing and comparing group sequential designs under NPH, we have implemented the group sequential design methodology in the gsDesign2 R package at https://cran.r-project.org/web/packages/gsDesign2/.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.01723v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yujie Zhao, Yilong Zhang, Larry Leon, Keaven M. Anderson</dc:creator>
    </item>
    <item>
      <title>Identification and estimation of mediational effects of longitudinal modified treatment policies</title>
      <link>https://arxiv.org/abs/2403.09928</link>
      <description>arXiv:2403.09928v4 Announce Type: replace 
Abstract: We demonstrate a comprehensive semiparametric approach to causal mediation analysis, addressing the complexities inherent in settings with longitudinal and continuous treatments, confounders, and mediators. Our methodology utilizes a nonparametric structural equation model and a cross-fitted sequential regression technique based on doubly robust pseudo-outcomes, yielding an efficient, asymptotically normal estimator without relying on restrictive parametric modeling assumptions. We are motivated by a recent scientific controversy regarding the effects of invasive mechanical ventilation (IMV) on the survival of COVID-19 patients, considering acute kidney injury (AKI) as a mediating factor. We highlight the possibility of "inconsistent mediation," in which the direct and indirect effects of the exposure operate in opposite directions. We discuss the significance of mediation analysis for scientific understanding and its potential utility in treatment decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.09928v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Gilbert, Katherine L. Hoffman, Nicholas Williams, Kara E. Rudolph, Edward J. Schenck, Iv\'an D\'iaz</dc:creator>
    </item>
    <item>
      <title>What's the Weight? Estimating Controlled Outcome Differences in Complex Surveys for Health Disparities Research</title>
      <link>https://arxiv.org/abs/2406.19597</link>
      <description>arXiv:2406.19597v2 Announce Type: replace 
Abstract: In this work, we are motivated by the problem of estimating racial disparities in health outcomes, specifically the average controlled difference (ACD) in telomere length between Black and White individuals, using data from the National Health and Nutrition Examination Survey (NHANES). To do so, we build a propensity for race to properly adjust for other social determinants while characterizing the controlled effect of race on telomere length. Propensity score methods are broadly employed with observational data as a tool to achieve covariate balance, but how to implement them in complex surveys is less studied - in particular, when the survey weights depend on the group variable under comparison (as the NHANES sampling scheme depends on self-reported race). We propose identification formulas to properly estimate the ACD in outcomes between Black and White individuals, with appropriate weighting for both covariate imbalance across the two racial groups and generalizability. Via extensive simulation, we show that our proposed methods outperform traditional analytic approaches in terms of bias, mean squared error, and coverage when estimating the ACD for our setting of interest. In our data, we find that evidence of racial differences in telomere length between Black and White individuals attenuates after accounting for confounding by socioeconomic factors and utilizing appropriate propensity score and survey weighting techniques. Software to implement these methods and code to reproduce our results can be found in the R package svycdiff, available through the Comprehensive R Archive Network (CRAN) at cran.r-project.org/web/packages/svycdiff/, or in a development version on GitHub at github.com/salernos/svycdiff.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19597v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Stephen Salerno, Emily K. Roberts, Belinda L. Needham, Tyler H. McCormick, Fan Li, Bhramar Mukherjee, Xu Shi</dc:creator>
    </item>
    <item>
      <title>Regression Discontinuity Designs Under Interference</title>
      <link>https://arxiv.org/abs/2410.02727</link>
      <description>arXiv:2410.02727v3 Announce Type: replace 
Abstract: We extend the continuity-based framework to Regression Discontinuity Designs (RDDs) to identify and estimate causal effects in the presence of interference when units are connected through a network. In this setting, assignment to an "effective treatment," which comprises the individual treatment and a summary of the treatment of interfering units (e.g., friends, classmates), is determined by the unit's score and the scores of other interfering units, leading to a multiscore RDD with potentially complex, multidimensional boundaries. We characterize these boundaries and derive generalized continuity assumptions to identify the proposed causal estimands, i.e., point and boundary causal effects. Additionally, we develop a distance-based nonparametric estimator, derive its asymptotic properties under restrictions on the network degree distribution, and introduce a novel variance estimator that accounts for network correlation. Finally, we apply our methodology to the PROGRESA/Oportunidades dataset to estimate the direct and indirect effects of receiving cash transfers on children's school attendance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.02727v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Elena Dal Torrione, Tiziano Arduini, Laura Forastiere</dc:creator>
    </item>
    <item>
      <title>Minimum Copula Divergence for Robust Estimation</title>
      <link>https://arxiv.org/abs/2502.16831</link>
      <description>arXiv:2502.16831v2 Announce Type: replace 
Abstract: This paper introduces a robust estimation framework based solely on the copula function. We begin by introducing a family of divergence measures tailored for copulas, including the \(\alpha\)-, \(\beta\)-, and \(\gamma\)-copula divergences, which quantify the discrepancy between a parametric copula model and an empirical copula derived from data independently of marginal specifications. Using these divergence measures, we propose the minimum copula divergence estimator (MCDE), an estimation method that minimizes the divergence between the model and the empirical copula. The framework proves particularly effective in addressing model misspecifications and analyzing heavy-tailed data, where traditional methods such as the maximum likelihood estimator (MLE) may fail. Theoretical results show that common copula families, including Archimedean and elliptical copulas, satisfy conditions ensuring the boundedness of divergence-based estimators, thereby guaranteeing the robustness of MCDE, especially in the presence of extreme observations. Numerical examples further underscore MCDE's ability to adapt to varying dependence structures, ensuring its utility in real-world scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16831v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shinto Eguchi, Shogo Kato</dc:creator>
    </item>
    <item>
      <title>Bayesian Model Averaging in Causal Instrumental Variable Models</title>
      <link>https://arxiv.org/abs/2504.13520</link>
      <description>arXiv:2504.13520v4 Announce Type: replace 
Abstract: Instrumental variables are a popular tool to infer causal effects under unobserved confounding, but choosing suitable instruments is challenging in practice. We propose gIVBMA, a Bayesian model averaging procedure that addresses this challenge by averaging across different sets of instrumental variables and covariates in a structural equation model. Our approach extends previous work through a scale-invariant prior structure and accommodates non-Gaussian outcomes and treatments, offering greater flexibility than existing methods. The computational strategy uses conditional Bayes factors to update models separately for the outcome and treatments. We prove that this model selection procedure is consistent. By explicitly accounting for model uncertainty, gIVBMA allows instruments and covariates to switch roles and provides robustness against invalid instruments. In simulation experiments, gIVBMA outperforms current state-of-the-art methods. We demonstrate its usefulness in two empirical applications: the effects of malaria and institutions on income per capita and the returns to schooling. A software implementation of gIVBMA is available in Julia.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.13520v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Gregor Steiner, Mark Steel</dc:creator>
    </item>
    <item>
      <title>High Dimensional Ensemble Kalman Filter</title>
      <link>https://arxiv.org/abs/2505.00283</link>
      <description>arXiv:2505.00283v3 Announce Type: replace 
Abstract: The Ensemble Kalman Filter (EnKF), as a fundamental data assimilation approach, has been widely used in many fields of the sciences and engineering. When the state variable is of high dimensional accompanied with high resolution observations of physical models, some key theoretical aspects of the EnKF are open for investigation. This paper proposes several high dimensional EnKF (HD-EnKF) methods equipped with consistent estimators for the important forecast error covariance and Kalman Gain matrices. It then studies the theoretical properties of the EnKF under both fixed and high dimensional state variables, which provides one-step and multiple-step mean square errors of the analysis states to the underlying oracle states offered by the Kalman Filter and gives the much needed insight to the roles played by the forecast error covariance on the accuracy of the EnKF. The accuracy of the data assimilation under the misspecified physical model is also considered. Numerical studies on the Lorenz-96 and the Shallow Water Equation models illustrate that the proposed HD-EnKF algorithms outperform the standard EnKF and widely used inflation methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00283v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shouxia Wang, Hao-Xuan Sun, Song Xi Chen</dc:creator>
    </item>
    <item>
      <title>Anytime-Valid Inference in Adaptive Experiments: Covariate Adjustment and Balanced Power</title>
      <link>https://arxiv.org/abs/2506.20523</link>
      <description>arXiv:2506.20523v3 Announce Type: replace 
Abstract: Adaptive experiments such as multi-armed bandits offer efficiency gains over traditional randomized experiments but pose two major challenges: invalid inference on the Average Treatment Effect (ATE) due to adaptive sampling and low statistical power for sub-optimal treatments. We address both issues by extending the Mixture Adaptive Design framework (arXiv:2311.05794). First, we propose MADCovar, a covariate-adjusted ATE estimator that is unbiased and preserves anytime-valid inference guarantees while substantially improving ATE precision. Second, we introduce MADMod, which dynamically reallocates samples to underpowered arms, enabling more balanced statistical power across treatments without sacrificing valid inference. Both methods retain MAD's core advantage of constructing asymptotic confidence sequences (CSs) that allow researchers to continuously monitor ATE estimates and stop data collection once a desired precision or significance criterion is met. Empirically, we validate both methods using simulations and real-world data. In simulations, MADCovar reduces CS width by up to $60\%$ relative to MAD. In a large-scale political RCT with $\approx32,000$ participants, MADCovar achieves similar precision gains. MADMod improves statistical power and inferential precision across all treatment arms, particularly for suboptimal treatments. Simulations show that MADMod sharply reduces Type II error while preserving the efficiency benefits of adaptive allocation. Together, MADCovar and MADMod make adaptive experiments more practical, reliable, and efficient for applied researchers across many domains. Our proposed methods are implemented through an open-source software package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.20523v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Molitor, Samantha Gold</dc:creator>
    </item>
    <item>
      <title>Statistical Consequences of Fat Tails: Real World Preasymptotics, Epistemology, and Applications</title>
      <link>https://arxiv.org/abs/2001.10488</link>
      <description>arXiv:2001.10488v4 Announce Type: replace-cross 
Abstract: (The third edition corrects minor typos and adds 3 chapters synthesized from published papers plus an appendix on maximum entropy distributions.) The monograph investigates the misapplication of conventional statistical techniques to fat tailed distributions and looks for remedies, when possible.
  Switching from thin tailed to fat tailed distributions requires more than "changing the color of the dress". Traditional asymptotics deal mainly with either n=1 or $n=\infty$, and the real world is in between, under of the "laws of the medium numbers" --which vary widely across specific distributions. Both the law of large numbers and the generalized central limit mechanisms operate in highly idiosyncratic ways outside the standard Gaussian or Levy-Stable basins of convergence.
  A few examples:
  + The sample mean is rarely in line with the population mean, with effect on "naive empiricism", but can be sometimes be estimated via parametric methods.
  + The "empirical distribution" is rarely empirical.
  + Parameter uncertainty has compounding effects on statistical metrics.
  + Dimension reduction (principal components) fails.
  + Inequality estimators (GINI or quantile contributions) are not additive and produce wrong results.
  + Many "biases" found in psychology become entirely rational under more sophisticated probability distributions
  + Most of the failures of financial economics, econometrics, and behavioral economics can be attributed to using the wrong distributions.
  This book, the first volume of the Technical Incerto, weaves a narrative around published journal articles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2001.10488v4</guid>
      <category>stat.OT</category>
      <category>q-fin.RM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nassim Nicholas Taleb</dc:creator>
    </item>
    <item>
      <title>Bayesian Image-on-Image Regression via Deep Kernel Learning based Gaussian Processes</title>
      <link>https://arxiv.org/abs/2311.05649</link>
      <description>arXiv:2311.05649v2 Announce Type: replace-cross 
Abstract: In neuroimaging studies, it becomes increasingly important to study associations between different imaging modalities using image-on-image regression (IIR), which faces challenges in interpretation, statistical inference, and prediction. Our motivating problem is how to predict task-evoked fMRI activity using resting-state fMRI data in the Human Connectome Project (HCP). The main difficulty lies in effectively combining different types of imaging predictors with varying resolutions and spatial domains in IIR. To address these issues, we develop Bayesian Image-on-image Regression via Deep Kernel Learning Gaussian Processes (BIRD-GP) and develop efficient posterior computation methods through Stein variational gradient descent. We demonstrate the advantages of BIRD-GP over state-of-the-art IIR methods using simulations. For HCP data analysis using BIRD-GP, we combine the voxel-wise fALFF maps and region-wise connectivity matrices to predict fMRI contrast maps for language and social recognition tasks. We show that fALFF is less predictive than the connectivity matrix for both tasks, but combining both yields improved results. Angular Gyrus Right emerges as the most predictable region for the language task (75.9% predictable voxels), while Superior Parietal Gyrus Right tops for the social recognition task (48.9% predictable voxels). Additionally, we identify features from the resting-state fMRI data that are important for task fMRI prediction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05649v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guoxuan Ma, Bangyao Zhao, Hasan Abu-Amara, Jian Kang</dc:creator>
    </item>
    <item>
      <title>Canonical correlation analysis of stochastic trends via functional approximation</title>
      <link>https://arxiv.org/abs/2411.19572</link>
      <description>arXiv:2411.19572v2 Announce Type: replace-cross 
Abstract: This paper proposes a novel approach for semiparametric inference on the number $s$ of common trends and their loading matrix $\psi$ in $I(1)/I(0)$ systems. It combines functional approximation of limits of random walks and canonical correlations analysis, performed between the $p$ observed time series of length $T$ and the first $K$ discretized elements of an $L^2$ basis. Tests and selection criteria on $s$, and estimators and tests on $\psi$ are proposed; their properties are discussed as $T$ and $K$ diverge sequentially for fixed $p$ and $s$. It is found that tests on $s$ are asymptotically pivotal, selection criteria of $s$ are consistent, estimators of $\psi$ are $T$-consistent, mixed-Gaussian and efficient, so that Wald tests on $\psi$ are asymptotically Normal or $\chi^2$. The paper also discusses asymptotically pivotal misspecification tests for checking model assumptions. The approach can be coherently applied to subsets or aggregations of variables in a given panel. Monte Carlo simulations show that these tools have reasonable performance for $T\geq 10 p$ and $p\leq 300$. An empirical analysis of 20 exchange rates illustrates the methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.19572v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Massimo Franchi, Iliyan Georgiev, Paolo Paruolo</dc:creator>
    </item>
    <item>
      <title>Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data</title>
      <link>https://arxiv.org/abs/2505.08698</link>
      <description>arXiv:2505.08698v3 Announce Type: replace-cross 
Abstract: Modeling the dynamics of probability distributions from time-dependent data samples is a fundamental problem in many fields, including digital health. The goal is to analyze how the distribution of a biomarker, such as glucose, changes over time and how these changes may reflect the progression of chronic diseases such as diabetes. We introduce a probabilistic model based on a Gaussian mixture that captures the evolution of a continuous-time stochastic process. Our approach combines a nonparametric estimate of the distribution, obtained with Maximum Mean Discrepancy (MMD), and a Neural Ordinary Differential Equation (Neural ODE) that governs the temporal evolution of the mixture weights. The model is highly interpretable, detects subtle distribution shifts, and remains computationally efficient. We illustrate the broad utility of our approach in a 26-week clinical trial that treats all continuous glucose monitoring (CGM) time series as the primary outcome. This method enables rigorous longitudinal comparisons between the treatment and control arms and yields characterizations that conventional summary-based clinical trials analytical methods typically do not capture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.08698v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.DS</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 18 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Antonio \'Alvarez-L\'opez, Marcos Matabuena</dc:creator>
    </item>
  </channel>
</rss>
