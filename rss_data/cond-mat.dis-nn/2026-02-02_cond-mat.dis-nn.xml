<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 02 Feb 2026 05:00:35 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Electrical conductivity of a random nanowire network: comparison of two-dimensional and quasi-three-dimensional models</title>
      <link>https://arxiv.org/abs/2601.22734</link>
      <description>arXiv:2601.22734v1 Announce Type: new 
Abstract: It is shown that the widely used two-dimensional model of random networks of metallic nanowires or carbon nanotubes significantly overestimates the number of contacts between elements compared to real systems, which, within the mean-field approach, leads to overestimated estimates of electrical conductivity, especially when the contact resistances between conductors make the main contribution to the electrical conductivity of the system. In the case of a two-dimensional model, the electrical conductivity of the system depends quadratically on the number density of conductors, whereas in the case of a three-dimensional model this dependence is linear.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22734v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuri Yu. Tarasevich</dc:creator>
    </item>
    <item>
      <title>When low-loss paths make a binary neuron trainable: detecting algorithmic transitions with the connected ensemble</title>
      <link>https://arxiv.org/abs/2601.23241</link>
      <description>arXiv:2601.23241v1 Announce Type: new 
Abstract: We study the connected ensemble, a statistical-mechanics framework that characterizes the formation of low-loss paths in rugged landscapes. First introduced in a previous paper, this ensemble allows one to identify when a network can be trained on a simple task and which minima should be targeted during training. We apply this new framework to the symmetric binary perceptron model (SBP), and study how its typical {connected} minima behave. We show that {connected} minima exist only above a critical threshold $\kappa_{\rm connected}$, or equivalently below a critical constraint density $\alpha_{\rm connected}$. This defines a parameter range in which training the network is easy, as local algorithms can efficiently access this connected manifold. We also highlight that these minima become increasingly robust and closer to one another as the task on which the network is trained becomes more difficult.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23241v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Damien Barbier</dc:creator>
    </item>
    <item>
      <title>Resilient-to-Fragile Transition and Excess Volatility in Supply Chain Networks</title>
      <link>https://arxiv.org/abs/2601.20450</link>
      <description>arXiv:2601.20450v1 Announce Type: cross 
Abstract: We study the disequilibrium dynamics of a stylised model of production networks in which firms use perishable and non-substitutable intermediate inputs, so that adverse idiosyncratic productivity shocks can trigger downstream shortages and output losses. To protect against such disruptions, firms hold precautionary inventories that act as buffer stocks. We show that, for a given dispersion of firm-level productivity shocks, there exists a critical level of inventories above which the economy remains in a stable stochastic steady state. Below this critical level, the system becomes fragile, i.e., it becomes prone to system-wide crises. As this resilience-fragility boundary is approached from above, aggregate output volatility rises sharply and diverges, even though shocks are purely idiosyncratic. Because inventories are costly, competitive pressures induce firms to economize on buffers. Although we do not explicitly model such costs, we argue that the resulting behaviour of individual firms drives the system close to criticality, generating persistent excess macroeconomic volatility -- in other words, ``small shocks, large cycles'' -- in line with other settings where efficiency and resilience are in tension with each other. In the language of phase transitions, the resilient-to-fragile transition is continuous (supercritical): the economy exhibits a well-defined stochastic equilibrium with finite volatility on one side of the boundary, while beyond it the probability of a collapse in finite time tends to one. We characterize this transition primarily through numerical simulations and derive an analytical description in a high-perishability, high-connectivity limit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.20450v1</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Martin, Jos\'e Moran, Debabrata Panja, Jean-Philippe Bouchaud</dc:creator>
    </item>
    <item>
      <title>Analysis of some solid amorphous inorganic structures and the boson peak phenomenon with a computational random graph approach</title>
      <link>https://arxiv.org/abs/2601.22277</link>
      <description>arXiv:2601.22277v1 Announce Type: cross 
Abstract: In this study, a new alternative model algorithm has been proposed for assembling amorphous structures, unifying the bosonic paradigm applicable at low temperatures with crystalline models relevant at room and higher temperatures. Physical meaning of main model parameters is determined together with an explanation for the appearing bosonic peak using the random graph theory. Numerically, statistical atomic distribution in a multiphase amorphous system is provided without the melting simulation of base crystals, and the mean energy function has been determined analytically. The calculated table data are in good agreement with neutronography measurements of the actual amorphous alloy in its solid state. Programme optimisations were also implemented, and we outlined several effective steps to achieve the higher processing speed. The proposed programme code can be used for potential test assembling and simulations of amorphous systems with sorting by the optimal atomic content or proportion (i.e. glass forming ability).</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22277v1</guid>
      <category>cond-mat.mtrl-sci</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Berezner, M. Rybakov, M. Sidlyar, V. Fedorov</dc:creator>
    </item>
    <item>
      <title>Online unsupervised Hebbian learning in deep photonic neuromorphic networks</title>
      <link>https://arxiv.org/abs/2601.22300</link>
      <description>arXiv:2601.22300v1 Announce Type: cross 
Abstract: While software implementations of neural networks have driven significant advances in computation, the von Neumann architecture imposes fundamental limitations on speed and energy efficiency. Neuromorphic networks, with structures inspired by the brain's architecture, offer a compelling solution with the potential to approach the extreme energy efficiency of neurobiological systems. Photonic neuromorphic networks (PNNs) are particularly attractive because they leverage the inherent advantages of light, namely high parallelism, low latency, and exceptional energy efficiency. Previous PNN demonstrations have largely focused on device-level functionalities or system-level implementations reliant on supervised learning and inefficient optical-electrical-optical (OEO) conversions. Here, we introduce a purely photonic deep PNN architecture that enables online, unsupervised learning. We propose a local feedback mechanism operating entirely in the optical domain that implements a Hebbian learning rule using non-volatile phase-change material synapses. We experimentally demonstrate this approach on a non-trivial letter recognition task using a commercially available fiber-optic platform and achieve a 100 percent recognition rate, showcasing an all-optical solution for efficient, real-time information processing. This work unlocks the potential of photonic computing for complex artificial intelligence applications by enabling direct, high-throughput processing of optical information without intermediate OEO signal conversions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22300v1</guid>
      <category>physics.optics</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Li, Disha Biswas, Peng Zhou, Wesley H. Brigner, Anna Capuano, Joseph S. Friedman, Qing Gu</dc:creator>
    </item>
    <item>
      <title>Spin quantum Hall transition on random networks: exact critical exponents via quantum gravity</title>
      <link>https://arxiv.org/abs/2601.22639</link>
      <description>arXiv:2601.22639v1 Announce Type: cross 
Abstract: We solve the problem of the spin quantum Hall transition on random networks using a mapping to classical percolation that focuses on the boundary of percolating clusters. Using tools of two-dimensional quantum gravity, we compute critical exponents that characterize this transition and confirm that these are related to the exponents for the regular (square) network through the KPZ relation. Our results demonstrate the relevance of the geometric randomness of the networks and support conclusions of numerical simulations of random networks for the integer quantum Hall transition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22639v1</guid>
      <category>cond-mat.mes-hall</category>
      <category>cond-mat.dis-nn</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Esteban Mac\'ias, Ilya Gruzberg, Eldad Bettelheim</dc:creator>
    </item>
    <item>
      <title>Leveraging Interactions for Efficient Swarm-Based Brownian Computing</title>
      <link>https://arxiv.org/abs/2601.22874</link>
      <description>arXiv:2601.22874v1 Announce Type: cross 
Abstract: Drawing inspiration from swarm intelligence, we show that short-range attractive interactions between thermally driven Brownian quasiparticles enable energy-efficient optimization. As quasiparticles can be generated directly within a material, the swarm size can be adjusted with minimal energy overhead. Using an optimization task defined by a spatially varying temperature landscape, we quantitatively show that interacting swarms reliably identify global optima and significantly outperform non-interacting searchers within a well-defined regime of interaction strength and swarm size. This improvement arises from emergent cooperative behavior, where local interactions guide the swarm toward high-quality solutions without central coordination. To link our physical model to experimental realizations, we coarse-grain the quasiparticle dynamics onto a sensor lattice and generate trajectories emulating particle-tracking measurements. We further show that the interacting swarm adapts robustly to landscapes that evolve over time. These findings establish interacting Brownian quasiparticles as a physical platform for scalable and energy-efficient unconventional computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.22874v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>nlin.AO</category>
      <category>physics.comp-ph</category>
      <category>physics.data-an</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Pignedoli, Atreya Majumdar, Karin Everschor-Sitte</dc:creator>
    </item>
    <item>
      <title>Spatial self-organization driven by temporal noise</title>
      <link>https://arxiv.org/abs/2601.23098</link>
      <description>arXiv:2601.23098v1 Announce Type: cross 
Abstract: The counterintuitive emergence of order from noise is a central phenomenon in science, ranging from pattern formation and synchronization to order-by-disorder in frustrated systems. While large-scale spatial self-organization induced by local spatial noise is well studied, whether temporal noise can also drive such organization remains an open question. Here, by studying interacting particle systems, we show that temporally correlated noise can lead to a self-organized state with suppressed long-range density fluctuations, or hyperuniformity. Further, we develop a fluctuating hydrodynamic theory that quantitatively explains the origin of this phenomenon. Finally, by casting the dynamics as a stochastic optimization problem, we show that temporal correlations lead to better solutions, akin to perturbed gradient descent in neural networks -- where noise is injected during training to escape poor minima. This reveals a striking correspondence between perturbed gradient descent dynamics on the energy landscapes of particle systems and the loss landscapes of neural networks. Our study establishes temporal correlations as a novel mechanism for noise-driven self-organization, with broad implications for self-assembling materials, biological systems, and optimization algorithms that leverage temporal noise for applications like differentially private learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.23098v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Satyam Anand, Guanming Zhang, Stefano Martiniani</dc:creator>
    </item>
    <item>
      <title>Clever algorithms for glasses work by time reparametrization</title>
      <link>https://arxiv.org/abs/2409.17121</link>
      <description>arXiv:2409.17121v2 Announce Type: replace 
Abstract: The ultraslow dynamics of glass-formers has been explained by two views considered as mutually exclusive: one invokes locally hindered mobility, the other rests on the complexity of the configuration space. Here we demonstrate that the evolution responds strongly to the details of the dynamics by changing the speed of time-flow: it has time-reparametrization softness. This finding reconciles both views: while local constraints reparametrize the flow of time, the global landscape determines relationships between different correlations at the same times. We show that modern algorithms developed to accelerate the relaxation to equilibrium act by changing the time reparametrization. Their success thus relies on their ability to exploit reparametrization softness. We conjecture that these results extend beyond the realm of glasses to the optimization of more general constraint satisfaction problems and to broader classes of algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.17121v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2520818123</arxiv:DOI>
      <arxiv:journal_reference>Proc. Natl. Acad. Sci USA 123, e2520818123 (2026)</arxiv:journal_reference>
      <dc:creator>Federico Ghimenti, Ludovic Berthier, Jorge Kurchan, Fr\'ed\'eric van Wijland</dc:creator>
    </item>
    <item>
      <title>The effect of priors on Learning with Restricted Boltzmann Machines</title>
      <link>https://arxiv.org/abs/2412.02623</link>
      <description>arXiv:2412.02623v2 Announce Type: replace 
Abstract: Restricted Boltzmann Machines (RBMs) are generative models designed to learn from data with a rich underlying structure. In this work, we explore a teacher-student setting where a student RBM learns from examples generated by a teacher RBM, with a focus on the effect of the unit priors on learning efficiency. We consider a parametric class of priors that interpolate between continuous (Gaussian) and binary variables. This approach models various possible choices of visible units, hidden units, and weights for both the teacher and student RBMs.
  By analyzing the phase diagram of the posterior distribution in both the Bayes optimal and mismatched regimes, we demonstrate the existence of a triple point that defines the critical dataset size necessary for learning through generalization. The critical size is strongly influenced by the properties of the teacher, and thus the data, but is unaffected by the properties of the student RBM. Nevertheless, a prudent choice of student priors can facilitate training by expanding the so-called signal retrieval region, where the machine generalizes effectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02623v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.physa.2025.130766</arxiv:DOI>
      <arxiv:journal_reference>Physica A. Volume 674, 15 September 2025, 130766</arxiv:journal_reference>
      <dc:creator>Gianluca Manzan, Daniele Tantari</dc:creator>
    </item>
    <item>
      <title>Learning and extrapolating scale-invariant processes</title>
      <link>https://arxiv.org/abs/2601.14810</link>
      <description>arXiv:2601.14810v2 Announce Type: replace 
Abstract: Machine Learning (ML) has deeply changed some fields recently, like Language and Vision and we may expect it to be relevant also to the analysis of of complex systems. Here we want to tackle the question of how and to which extent can one regress scale-free processes, i.e. processes displaying power law behavior, like earthquakes or avalanches? We are interested in predicting the large ones, i.e. rare events in the training set which therefore require extrapolation capabilities of the model. For this we consider two paradigmatic problems that are statistically self-similar. The first one is a 2-dimensional fractional Gaussian field obeying linear dynamics, self-similar by construction and amenable to exact analysis. The second one is the Abelian sandpile model, exhibiting self-organized criticality. The emerging paradigm of Geometric Deep Learning shows that including known symmetries into the model's architecture is key to success. Here one may hope to extrapolate only by leveraging scale invariance. This is however a peculiar symmetry, as it involves possibly non-trivial coarse-graining operations and anomalous scaling. We perform experiments on various existing architectures like U-net, Riesz network (scale invariant by construction), or our own proposals: a wavelet-decomposition based Graph Neural Network (with discrete scale symmetry), a Fourier embedding layer and a Fourier-Mellin Neural Operator. Based on these experiments and a complete characterization of the linear case, we identify the main issues relative to spectral biases and coarse-grained representations, and discuss how to alleviate them with the relevant inductive biases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.14810v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Anaclara Alvez-Canepa, Cyril Furtlehner, Fran\c{c}ois Landes</dc:creator>
    </item>
    <item>
      <title>Generalization Dynamics of Linear Diffusion Models</title>
      <link>https://arxiv.org/abs/2505.24769</link>
      <description>arXiv:2505.24769v2 Announce Type: replace-cross 
Abstract: Diffusion models are powerful generative models that produce high-quality samples from complex data. While their infinite-data behavior is well understood, their generalization with finite data remains less clear. Classical learning theory predicts that generalization occurs at a sample complexity that is exponential in the dimension, far exceeding practical needs. We address this gap by analyzing diffusion models through the lens of data covariance spectra, which often follow power-law decays, reflecting the hierarchical structure of real data. To understand whether such a hierarchical structure can benefit learning in diffusion models, we develop a theoretical framework based on linear neural networks, congruent with a Gaussian hypothesis on the data. We quantify how the hierarchical organization of variance in the data and regularization impacts generalization. We find two regimes: When $N &lt;d$, not all directions of variation are present in the training data, which results in a large gap between training and test loss. In this regime, we demonstrate how a strongly hierarchical data structure, as well as regularization and early stopping help to prevent overfitting. For $N &gt; d$, we find that the sampling distributions of linear diffusion models approach their optimum (measured by the Kullback-Leibler divergence) linearly with $d/N$, independent of the specifics of the data distribution. Our work clarifies how sample complexity governs generalization in a simple model of diffusion-based generative models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.24769v2</guid>
      <category>stat.ML</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 02 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Claudia Merger, Sebastian Goldt</dc:creator>
    </item>
  </channel>
</rss>
