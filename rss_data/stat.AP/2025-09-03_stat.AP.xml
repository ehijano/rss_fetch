<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 04 Sep 2025 01:31:18 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Feature Augmentations for High-Dimensional Learning</title>
      <link>https://arxiv.org/abs/2509.00232</link>
      <description>arXiv:2509.00232v1 Announce Type: new 
Abstract: High-dimensional measurements are often correlated which motivates their approximation by factor models. This holds also true when features are engineered via low-dimensional interactions or kernel tricks. This often results in over parametrization and requires a fast dimensionality reduction. We propose a simple technique to enhance the performance of supervised learning algorithms by augmenting features with factors extracted from design matrices and their transformations. This is implemented by using the factors and idiosyncratic residuals which significantly weaken the correlations between input variables and hence increase the interpretability of learning algorithms and numerical stability. Extensive experiments on various algorithms and real-world data in diverse fields are carried out, among which we put special emphasis on the stock return prediction problem with Chinese financial news data due to the increasing interest in NLP problems in financial studies. We verify the capability of the proposed feature augmentation approach to boost overall prediction performance with the same algorithm. The approach bridges a gap in research that has been overlooked in previous studies, which focus either on collecting additional data or constructing more powerful algorithms, whereas our method lies in between these two directions using a simple PCA augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00232v1</guid>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaonan Zhu, Bingyan Wang, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Maternal Characteristics and Newborn Birth Weight: A Comprehensive Statistical Analysis</title>
      <link>https://arxiv.org/abs/2509.00617</link>
      <description>arXiv:2509.00617v1 Announce Type: new 
Abstract: This report presents a statistical analysis of the impact of key maternal characteristics, including age, smoking status, parity, height, weight, and gestation period, on newborn birth weight. A realworld dataset comprising 1,236 observations was utilized for this investigation. The methodology involved comprehensive data cleaning, exploratory data analysis (EDA), and a series of parametric statistical tests, specifically the One-Sample t-test, Two-Sample t-test, Chi-Square tests, and Analysis of Variance (ANOVA). All analyses were conducted within the SAS programming environment. The study's findings indicate a statistically significant negative impact of maternal smoking on birth weight, a finding consistent with broader public health literature. Gestation period emerged as the strongest positive predictor of birth weight within this dataset. While the analyses using broad categories of maternal age and parity did not reveal significant differences in mean birth weight, a review of existing literature suggests more intricate, potentially non-linear relationships and nuanced effects of these factors. Similarly, maternal pre-pregnancy weight, though showing a weak linear correlation in this dataset, is widely recognized as a critical determinant of birth weight outcomes, particularly at its extremes. These results emphasize the importance of targeted prenatal care interventions, especially those focused on smoking cessation. The study reinforces the utility of data-driven insights in informing public health policies aimed at improving maternal and child health outcomes. Future research should explore non-linear relationships and potential interactions among various maternal factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00617v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prithwiraj Chatterjee, Abhinav Tanwar, Devadharshini Udayakumar</dc:creator>
    </item>
    <item>
      <title>Reframing Three-Dimensional Morphometrics Through Functional Data Innovations</title>
      <link>https://arxiv.org/abs/2509.00650</link>
      <description>arXiv:2509.00650v1 Announce Type: new 
Abstract: This study innovates geometric morphometrics by incorporating functional data analysis, the square-root velocity function (SRVF), and arc-length parameterisation for 3D morphometric data, leading to the development of seven new pipelines in addition to the standard geometric morphometrics (GM) approach.. This enables three-dimensional images to be examined from perspectives that do not neglect curvature, through the combined use of arc-length parameterisation, soft-alignment, and elastic-alignment. A simulation study was conducted to demonstrate the general effectiveness of eight pipelines: geometric morphometrics (GM, baseline), arc-GM, functional data morphometrics (FDM), arc-FDM, soft-SRV-FDM, arc-soft-SRV-FDM, elastic-SRV-FDM, and arc-elastic-SRV-FDM. These pipelines were also applied to distinguish dietary categories of kangaroos (omnivores, mixed feeders, browsers, and grazers) using cranial landmarks obtained from 41 extant species. Principal component analysis was conducted, followed by classification analysis using linear discriminant analysis, multinomial regression and support vector machines with a linear kernel. The results highlight the effectiveness of functional data analysis, together with arc-length and SRVF-based approaches, in opening the door to more robust perspectives for analysing three-dimensional morphometrics, while establishing geometric morphometrics as the baseline for comparison.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00650v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aneesha Balachandran Pillay, Issa-Mbenard Dabo, Sophie Dabo-Niang, Dharini Pathmanathan</dc:creator>
    </item>
    <item>
      <title>A novel methodological framework for analyzing the momentum effect in tennis singles</title>
      <link>https://arxiv.org/abs/2509.01243</link>
      <description>arXiv:2509.01243v1 Announce Type: new 
Abstract: This paper proposes a novel methodological framework for analyzing momentum effects in tennis singles. To statistically substantiate the existence of momentum, we employ Chi-squared independence tests for the contingency table. Assuming momentum is present, we develop a momentum metric based on the entropy weight method. Subsequently, we apply a CUSUM control chart to detect change points within the derived momentum series and define a relative distance measure to quantify the intensity of momentum shifts. Furthermore, we construct a predictive model utilizing a Back Propagation neural network (BP) optimized by a Particle Swarm Optimization (PSO) algorithm. The importance of predictive features is analyzed via SHAP values. An empirical analysis applying this framework to data from the 2023 Wimbledon Men's Singles yields the following key findings: (1) Statistical evidence signifficantly supports the existence of momentum in tennis singles. (2) Incorporating momentum characteristics substantially enhances point outcome prediction performance. (3) The BP+PSO model demonstrates competitive advantages over alternative machine learning algorithms, including Random Forest, Support Vector Machines, and logistic regression. (4) SHAP value analysis identifies an athlete's unforced errors, winning shots, the momentum metric, and the momentum shift intensity as the four most critical features for predicting point outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01243v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chang Du, Caiya Zhang, Likai Zhou</dc:creator>
    </item>
    <item>
      <title>Suicide Mortality in Spain (2010-2022): Temporal Trends, Spatial Patterns, and Risk Factors</title>
      <link>https://arxiv.org/abs/2509.01342</link>
      <description>arXiv:2509.01342v1 Announce Type: new 
Abstract: This paper investigates the spatial and temporal patterns of age-stratified suicide mortality rates in Spanish provinces from 2010 to 2022. We use mixed Poisson models to analyze these patterns and to determine the association between mortality rates and various socioeconomic and contextual factors, while controlling for spatial and temporal confounding effects. Our results indicate that a 10% increase in the proportion of people residing in rural areas is associated with an increase of over 5% in male suicide mortality. In addition, a 1% increase in the annual unemployment rate is linked to a 2.4% increase in female suicide mortality. An important finding is that despite male suicide rates consistently being higher than female rates, we observe a notable and steady upward trend in female suicide mortality over the study period.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01342v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Adin, G. Retegui, A. S\'anchez Villegas, M. D. Ugarte</dc:creator>
    </item>
    <item>
      <title>Efficient Bayesian Sampling with Langevin Birth-Death Dynamics</title>
      <link>https://arxiv.org/abs/2509.01942</link>
      <description>arXiv:2509.01942v1 Announce Type: new 
Abstract: Bayesian inference plays a central role in scientific and engineering applications by enabling principled reasoning under uncertainty. However, sampling from generic probability distributions remains a computationally demanding task. This difficulty is compounded when the distributions are ill-conditioned, multi-modal, or supported on topologically non-Euclidean spaces. Motivated by challenges in gravitational wave parameter estimation, we propose simulating a Langevin diffusion augmented with a birth-death process. The dynamics are rescaled with a simple preconditioner, and generalized to apply to the product spaces of a hypercube and hypertorus. Our method is first-order and embarrassingly parallel with respect to model evaluations, making it well-suited for algorithmic differentiation and modern hardware accelerators. We validate the algorithm on a suite of toy problems and successfully apply it to recover the parameters of GW150914 -- the first observed binary black hole merger. This approach addresses key limitations of traditional sampling methods, and introduces a template that can be used to design robust samplers in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01942v1</guid>
      <category>stat.AP</category>
      <category>gr-qc</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Leviyev, Francesco Iacovelli, Aaron Zimmerman</dc:creator>
    </item>
    <item>
      <title>Mediation Analysis in the Presence of Sample Selection Bias with an Application to Disparities in Liver Transplantation Listing</title>
      <link>https://arxiv.org/abs/2509.01969</link>
      <description>arXiv:2509.01969v1 Announce Type: new 
Abstract: The study of disparities in the liver transplantation process may focus on quantifying causal effects, particularly the average, direct, or indirect effects of various social determinants of health on being listed as a candidate for transplant. Selection bias arises when the data sample does not represent the target population, defined here as all individuals referred to the transplant clinic. Listing decisions are made for the subset of patients who complete the evaluation process, who may differ systematically from the referred population. There is evidence that selection is associated with patient characteristics that also impact outcomes. Using data only from the selected population may yield biased causal effect estimates. However, incorporating data from the referred population allows for analytic correction. This correction leverages hypothesized causal relationships among selection, the outcome (getting listed), exposures, and mediators. Using directed acyclic graphs (DAGs), we establish graphical conditions under which a reweighted mediation formula identifies effect of interest - direct, indirect, and path-specific effects - in the presence of sample selection. In a clinical case study, we investigate mediated and direct effects of a patient's socioeconomic position on being listed for transplant, allowing selection to depend on race, gender, age, and other social determinants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01969v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zain Khan, Lynnette Sequeira, Alexandra T. Strauss, Vedant Jain, Juliette Dixon, Eric Moughames, Tyrus Vong, Daniel Malinsky</dc:creator>
    </item>
    <item>
      <title>Dynamic Spatio-Temporal Sequential Ordinal Models: Application to Invasive Weeds</title>
      <link>https://arxiv.org/abs/2509.01976</link>
      <description>arXiv:2509.01976v1 Announce Type: new 
Abstract: The multivariate sequential ordinal model is investigated for use in the Bayesian analysis of spatio-temporal ordinal data. The sequential ordinal model likelihood is equivalent to a binary model conditional on unknown regression coefficients and spatio-temporal random effects. Therefore, estimation and prediction in the space-time context can proceed using the well-established dynamic generalised linear model framework. Moreover, the sequential ordinal model avoids the ordering constraints on the threshold parameters that determine the category break points required by cumulative ordinal models, and so simplifies the estimation procedure for high-dimensional space-time applications using Bayesian inference. The dynamic spatio-temporal sequential ordinal model is applied to estimate foliage cover abundance of four actively managed invasive alien species. These invasive weed species are observed by means of a modified Braun-Blanquet score that is commonly used in vegetation studies and constitutes ordinal data. The multivariate ordinal data for the managed weeds species are sparsely distributed in space and time with few observations recorded in high foliage cover categories. A separable model for space-time dependence that maintains parameter interpretability in the presence of aggregated ordinal categories is therefore developed. Estimation and prediction is demonstrated using integrated nested Laplace approximation (INLA) methods developed for univariate spatio-temporal models. Bayesian estimation and prediction shows that the four invasive weed species differentially respond to habitat type, control effort and accessibility, and share similar magnitudes of dependence with short effective spatial ranges and strong temporal autocorrelations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01976v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Geoffrey R. Hosack, Wen-Hsi Yang, Kyana N. Pike, Luke S. O'Loughlin, Emma Cook, Brett Howland, Emily Sutcliffe, Richard N. C. Milner, Ben Gooden, Jens G. Froese</dc:creator>
    </item>
    <item>
      <title>A note on a resampling procedure for estimating the density at a given quantile</title>
      <link>https://arxiv.org/abs/2509.02207</link>
      <description>arXiv:2509.02207v1 Announce Type: new 
Abstract: In this paper we refine the procedure proposed by Lin et al. (2015) to estimate the density at a given quantile based on a resampling method. The approach consists on generating multiple samples of the zero-mean Gaussian variable from which a least square estimator is constructed. The main advantage of the proposed method is that it provides an estimation directly at the quantile of interest, thus achieving the parametric rate of convergence. In this study, we investigate the critical role of the variance of the sampled Gaussians on the accuracy of the estimation. We provide theoretical guarantees on this variance that ensure the consistency of the estimator, and we propose a gridsearch algorithm for automatic variance selection in practical applications. We demonstrate the performance of the proposed estimator in simulations and compare the results with those obtained using kernel density estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02207v1</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Beatriz Farah (MAP5 - UMR 8145), Aur\'elien Latouche (CNAM Paris), Olivier Bouaziz (LPP, MAP5 - UMR 8145)</dc:creator>
    </item>
    <item>
      <title>Per-sender neural network classifiers for email authorship validation</title>
      <link>https://arxiv.org/abs/2509.00005</link>
      <description>arXiv:2509.00005v1 Announce Type: cross 
Abstract: Business email compromise and lateral spear phishing attacks are among modern organizations' most costly and damaging threats. While inbound phishing defenses have improved significantly, most organizations still trust internal emails by default, leaving themselves vulnerable to attacks from compromised employee accounts. In this work, we define and explore the problem of authorship validation: verifying whether a claimed sender actually authored a given email. Authorship validation is a lightweight, real-time defense that complements traditional detection methods by modeling per-sender writing style. Further, the paper presents a collection of new datasets based on the Enron corpus. These simulate inauthentic messages using both human-written and large language model-generated emails. The paper also evaluates two classifiers -- a Naive Bayes model and a character-level convolutional neural network (Char-CNN) -- for the authorship validation task. Our experiments show that the Char-CNN model achieves high accuracy and F1 scores under various circumstances. Finally, we discuss deployment considerations and show that per-sender authorship classifiers are practical for integrating into existing commercial email security systems with low overhead.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00005v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rohit Dube</dc:creator>
    </item>
    <item>
      <title>Pilot Study on Generative AI and Critical Thinking in Higher Education Classrooms</title>
      <link>https://arxiv.org/abs/2509.00167</link>
      <description>arXiv:2509.00167v1 Announce Type: cross 
Abstract: Generative AI (GAI) tools have seen rapid adoption in educational settings, yet their role in fostering critical thinking remains underexplored. While previous studies have examined GAI as a tutor for specific lessons or as a tool for completing assignments, few have addressed how students critically evaluate the accuracy and appropriateness of GAI-generated responses. This pilot study investigates students' ability to apply structured critical thinking when assessing Generative AI outputs in introductory Computational and Data Science courses. Given that GAI tools often produce contextually flawed or factually incorrect answers, we designed learning activities that require students to analyze, critique, and revise AI-generated solutions. Our findings offer initial insights into students' ability to engage critically with GAI content and lay the groundwork for more comprehensive studies in future semesters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00167v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>W. F. Lamberti, S. R. Lawrence, D. White, S. Kim, S. Abdullah</dc:creator>
    </item>
    <item>
      <title>A Bayesian Optimal Phase II Design for Randomized Immunotherapy Trials with Delayed Treatment Effects</title>
      <link>https://arxiv.org/abs/2509.00238</link>
      <description>arXiv:2509.00238v1 Announce Type: cross 
Abstract: Immunotherapy has transformed cancer treatment, yet its delayed therapeutic effects often lead to non-proportional hazards, rendering many conventional phase II designs underpowered and prone to type I error inflation. To address this issue, we propose a novel Bayesian Optimal Phase II design (DTE-BOP2) that explicitly models the uncertainty in the separation timing of treatment effect. The treatment separation timepoint (denoted by S) is endowed with a truncated-Gamma prior, whose parameters can be elicited from experts or inferred from historical data, with default settings available when prior knowledge is scarce. Built upon the BOP2 framework (Zhou et al. 2017, 2020), our design retains operational simplicity while incorporating type I error control and maintaining the power. Extensive simulations demonstrate that DTE-BOP2 uniformly controls type I error at the nominal level across a wide range of treatment effect separation timepoint S. We further observe that the power decreases monotonically as S increases. Importantly, we find that the power is primarily driven by the relative magnitude of treatment benefit before and after the separation time, i.e., the ratio of medians, rather than their absolute values. Compared to the original BOP2, the piecewise weighted log-rank, and the conventional log-rank tests, DTE-BOP2 achieves higher power with smaller sample sizes while preserving type I error robustness across plausible delay scenarios. An open-source R package, DTEBOP2 (CRAN), with detailed vignettes, enables investigators to implement the design and analyse phase-II trials exhibiting delayed treatment effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00238v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhongheng Cai, Haitao Pan</dc:creator>
    </item>
    <item>
      <title>Assessing One-Dimensional Cluster Stability by Extreme-Point Trimming</title>
      <link>https://arxiv.org/abs/2509.00258</link>
      <description>arXiv:2509.00258v1 Announce Type: cross 
Abstract: We develop a probabilistic method for assessing the tail behavior and geometric stability of one-dimensional n i.i.d. samples by tracking how their span contracts when the most extreme points are trimmed. Central to our approach is the diameter-shrinkage ratio, that quantifies the relative reduction in data range as extreme points are successively removed. We derive analytical expressions, including finite-sample corrections, for the expected shrinkage under both the uniform and Gaussian hypotheses, and establish that these curves remain distinct even for moderate number of removal. We construct an elementary decision rule that assigns a sample to whichever theoretical shrinkage profile it most closely follows. This test achieves higher classification accuracy than the classical likelihood-ratio test in small-sample or noisy regimes, while preserving asymptotic consistency for large n. We further integrate our criterion into a clustering pipeline (e.g. DBSCAN), demonstrating its ability to validate one-dimensional clusters without any density estimation or parameter tuning. This work thus provides both theoretical insight and practical tools for robust distributional inference and cluster stability analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00258v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erwan Dereure, Emmanuel Akame Mfoumou, David Holcman</dc:creator>
    </item>
    <item>
      <title>Modeling Human Spatial Mobility Patterns with the L\'evy Flight Cluster Model</title>
      <link>https://arxiv.org/abs/2509.00298</link>
      <description>arXiv:2509.00298v1 Announce Type: cross 
Abstract: Despite the extensive collection of individual mobility data over the past decade, fueled by the widespread use of GPS-enabled personal devices, the existing statistical literature on estimating human spatial mobility patterns from temporally irregular location data remains limited. In this paper, we introduce the L\'{e}vy Flight Cluster Model (LFCM), a hierarchical Bayesian mixture model designed to analyze an individual's activity distribution. The LFCM can be utilized to determine probabilistic overlaps between individuals' activity patterns and serves as an anonymization tool to generate synthetic location data. We present our methodology using real-world human location data, demonstrating its ability to accurately capture the key characteristics of human movement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00298v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malcolm Wolff, Adrian Dobra, Anton H. Westveld, Grace S. Chiu</dc:creator>
    </item>
    <item>
      <title>FBMS: An R Package for Flexible Bayesian Model Selection and Model Averaging</title>
      <link>https://arxiv.org/abs/2509.00753</link>
      <description>arXiv:2509.00753v1 Announce Type: cross 
Abstract: The FBMS R package facilitates Bayesian model selection and model averaging in complex regression settings by employing a variety of Monte Carlo model exploration methods. At its core, the package implements an efficient Mode Jumping Markov Chain Monte Carlo (MJMCMC) algorithm, designed to improve mixing in multi-modal posterior landscapes within Bayesian generalized linear models. In addition, it provides a genetically modified MJMCMC (GMJMCMC) algorithm that introduces nonlinear feature generation, thereby enabling the estimation of Bayesian generalized nonlinear models (BGNLMs). Within this framework, the algorithm maintains and updates populations of transformed features, computes their posterior probabilities, and evaluates the posteriors of models constructed from them. We demonstrate the effective use of FBMS for both inferential and predictive modeling in Gaussian regression, focusing on different instances of the BGNLM class of models. Furthermore, through a broad set of applications, we illustrate how the methodology can be extended to increasingly complex modeling scenarios, extending to other response distributions and mixed effect models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.00753v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Florian Frommlet, Jon Lachmann, Geir Storvik, Aliaksandr Hubin</dc:creator>
    </item>
    <item>
      <title>An affinity based opinion dynamics model for the evolving pattern of political polarization</title>
      <link>https://arxiv.org/abs/2509.01244</link>
      <description>arXiv:2509.01244v1 Announce Type: cross 
Abstract: Political polarization has been a subject that has attracted many studies in recent years. We have developed an opinion dynamics model with affective homophily effect and national social norm effect to describe this phenomenon. The time evolution of the polarization between the two parties and the spread of opinions within each party are affected by three factors: the repulsive effect between the two parties, the attractive and repulsive effects between the members in each party, and the national social norm effect that pulls the opinions of all members towards a common norm. The model is internally consistent and is applied to the simulation of the symmetric patterns of polarization and spread of the opinion distributions in the U.S. Congress and the results align well with 154 years of recorded data. The time evolution of the strength of the national social norm effect is obtained and is consistent with the important historical events occurred during the past one and half century.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01244v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhang Xiaoming, Hu Yuzhong, Zhang Yiming</dc:creator>
    </item>
    <item>
      <title>Statistics-Friendly Confidentiality Protection for Establishment Data, with Applications to the QCEW</title>
      <link>https://arxiv.org/abs/2509.01597</link>
      <description>arXiv:2509.01597v1 Announce Type: cross 
Abstract: Confidentiality for business data is an understudied area of disclosure avoidance, where legacy methods struggle to provide acceptable results. Modern formal privacy techniques designed for person-level data do not provide suitable confidentiality/utility trade-offs due to the highly skewed nature of business data and because extreme outlier records are often important contributors to query answers. In this paper, inspired by Gaussian Differential Privacy, we propose a novel confidentiality framework for business data with a focus on interpretability for policy makers. We propose two query-answering mechanisms and analyze new challenges that arise when noisy query answers are converted into confidentiality-preserving microdata. We evaluate our mechanisms on confidential Quarterly Census of Employment and Wages (QCEW) microdata and a public substitute dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01597v1</guid>
      <category>cs.CR</category>
      <category>cs.DS</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaitlyn Webb, Prottay Protivash, John Durrell, Daniell Toth, Aleksandra Slavkovi\'c, Daniel Kifer</dc:creator>
    </item>
    <item>
      <title>Evaluation of Surrogate Endpoints Based on Meta-Analysis with Surrogate Indices</title>
      <link>https://arxiv.org/abs/2509.01737</link>
      <description>arXiv:2509.01737v1 Announce Type: cross 
Abstract: We introduce in this paper an extension of the meta-analytic (MA) framework for evaluating surrogate endpoints. While the MA framework is regarded as the gold standard for surrogate endpoint evaluation, it is limited in its ability to handle complex surrogates and does not take into account possible differences in the distribution of baseline covariates across trials. By contrast, in the context of data fusion, the surrogate-index (SI) framework accommodates complex surrogates and allows for complex relationships between baseline covariates, surrogates, and clinical endpoints. However, the SI framework is not a surrogate evaluation framework and relies on strong identifying assumptions. To address the MA framework's limitations, we propose an extension that incorporates ideas from the SI framework. We first formalize the data-generating mechanism underlying the MA framework, providing a transparent description of the untestable assumptions required for valid inferences in any evaluation of trial-level surrogacy -- assumptions often left implicit in the MA framework. While this formalization is meaningful in its own right, it is also required for our main contribution: We propose to estimate a specific transformation of the baseline covariates and the surrogate, the so-called surrogate index. This estimated transformation serves as a new potential univariate surrogate and is optimal in a trial-level surrogacy sense under certain conditions. We show that, under weak additional conditions, this new univariate surrogate can be evaluated as a trial-level surrogate as if the transformation were known a priori. This approach enables the evaluation of the trial-level surrogacy of complex surrogates and can be implemented using standard software. We illustrate this approach with a set of COVID-19 vaccine trials where antibody markers are assessed as potential trial-level surrogate endpoints.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01737v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Florian Stijven, Peter B. Gilbert</dc:creator>
    </item>
    <item>
      <title>Generalized Correlation Regression for Disentangling Dependence in Clustered Data</title>
      <link>https://arxiv.org/abs/2509.01774</link>
      <description>arXiv:2509.01774v1 Announce Type: cross 
Abstract: Clustered and longitudinal data are pervasive in scientific studies, from prenatal health programs to clinical trials and public health surveillance. Such data often involve non-Gaussian responses--including binary, categorical, and count outcomes--that exhibit complex correlation structures driven by multilevel clustering, covariates, over-dispersion, or zero inflation. Conventional approaches such as mixed-effects models and generalized estimating equations (GEEs) can capture some of these dependencies, but they are often too rigid or impose restrictive assumptions that limit interpretability and predictive performance.
  We investigate \emph{generalized correlation regression} (GCR), a unified framework that models correlations directly as functions of interpretable covariates while simultaneously estimating marginal means. By applying a generalized $z$-transformation, GCR guarantees valid correlation matrices, accommodates unbalanced cluster sizes, and flexibly incorporates covariates such as time, space, or group membership into the dependence structure. Through applications to modern prenatal care, a longitudinal toenail infection trial, and clustered health count data, we show that GCR not only achieves superior predictive performance over standard methods, but also reveals family-, community-, and individual-level drivers of dependence that are obscured under conventional modeling. These results demonstrate the broad applied value of GCR for analyzing binary, count, and categorical data in clustered and longitudinal settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01774v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yibo Wang, Chenlei Leng, Cheng Yong Tang</dc:creator>
    </item>
    <item>
      <title>Inference of epidemic networks: the effect of different data types</title>
      <link>https://arxiv.org/abs/2509.01871</link>
      <description>arXiv:2509.01871v1 Announce Type: cross 
Abstract: We investigate how the properties of epidemic networks change depending on the availability of different types of data on a disease outbreak. This is achieved by introducing mathematical and computational methods that estimate the probability of transmission trees by combining generative models that jointly determine the number of infected hosts, the probability of infection between them depending on location and genetic information, and their time of infection and sampling. We introduce a suitable Markov Chain Monte Carlo method that we show to sample trees according to their probability. Statistics performed over the sampled trees lead to probabilistic estimations of network properties and other quantities of interest, such as the number of unobserved hosts and the depth of the infection tree. We confirm the validity of our approach by comparing the numerical results with analytically solvable examples. Finally, we apply our methodology to data from COVID-19 in Australia. We find that network properties that are important for the management of the outbreak depend sensitively on the type of data used in the inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01871v1</guid>
      <category>physics.comp-ph</category>
      <category>cond-mat.stat-mech</category>
      <category>physics.data-an</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oscar Fajardo-Fontiveros, Carl J. E. Suster, Eduardo G. Altmann</dc:creator>
    </item>
    <item>
      <title>Non-Linear Model-Based Sequential Decision-Making in Agriculture</title>
      <link>https://arxiv.org/abs/2509.01924</link>
      <description>arXiv:2509.01924v1 Announce Type: cross 
Abstract: Sequential decision-making is central to sustainable agricultural management and precision agriculture, where resource inputs must be optimized under uncertainty and over time. However, such decisions must often be made with limited observations, whereas classical bandit and reinforcement learning approaches typically rely on either linear or black-box reward models that may misrepresent domain knowledge or require large amounts of data. We propose a family of nonlinear, model-based bandit algorithms that embed domain-specific response curves directly into the exploration-exploitation loop. By coupling (i) principled uncertainty quantification with (ii) closed-form or rapidly computable profit optima, these algorithms achieve sublinear regret and near-optimal sample complexity while preserving interpretability. Theoretical analysis establishes regret and sample complexity bounds, and extensive simulations emulating real-world fertilizer-rate decisions show consistent improvements over both linear and nonparametric baselines (such as linear UCB and $k$-NN UCB) in the low-sample regime, under both well-specified and shape-compatible misspecified models. Because our approach leverages mechanistic insight rather than large data volumes, it is especially suited to resource-constrained settings, supporting sustainable, inclusive, and transparent sequential decision-making across agriculture, environmental management, and allied applications. This methodology directly contributes to SDG 2 (Zero Hunger) and SDG 12 (Responsible Consumption and Production) by enabling data-driven, less wasteful agricultural practices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.01924v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sakshi Arya, Wentao Lin</dc:creator>
    </item>
    <item>
      <title>Interpretational errors with instrumental variables</title>
      <link>https://arxiv.org/abs/2509.02045</link>
      <description>arXiv:2509.02045v1 Announce Type: cross 
Abstract: Instrumental variables (IV) are often used to identify causal effects in observational settings and experiments subject to non-compliance. Under canonical assumptions, IVs allow us to identify a so-called local average treatment effect (LATE). The use of IVs is often accompanied by a pragmatic decision to abandon the identification of the causal parameter that corresponds to the original research question and target the LATE instead. This pragmatic decision presents a potential source of error: an investigator mistakenly interprets findings as if they had made inference on their original causal parameter of interest. We conducted a systematic review and meta-analysis of patterns of pragmatism and interpretational errors in the applied IV literature published in leading journals of economics, political science, epidemiology, and clinical medicine (n = 309 unique studies). We found that a large fraction of studies targeted the LATE, although specific interest in this parameter was rare. Of these studies, 61% contained claims that mistakenly suggested that another parameter was targeted -- one whose value likely differs, and could even have the opposite sign, from the parameter actually estimated. Our findings suggest that the validity of conclusions drawn from IV applications is often compromised by interpretational errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02045v1</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luca Locher, Mats J. Stensrud, Aaron L. Sarvet</dc:creator>
    </item>
    <item>
      <title>Amputation-imputation based generation of synthetic tabular data for ratemaking</title>
      <link>https://arxiv.org/abs/2509.02171</link>
      <description>arXiv:2509.02171v1 Announce Type: cross 
Abstract: Actuarial ratemaking depends on high-quality data, yet access to such data is often limited by the cost of obtaining new data, privacy concerns, etc. In this paper, we explore synthetic-data generation as a potential solution to these issues. In addition to discussing generative methods previously studied in the actuarial literature, we introduce to the insurance community another approach based on Multiple Imputation by Chained Equations (MICE). We present a comparative study using an open-source dataset and evaluating MICE-based models against other generative models like Variational Autoencoders and Conditional Tabular Generative Adversarial Networks. We assess how well synthetic data preserves the original marginal distributions of variables as well as the multivariate relationships among covariates. We also investigate the consistency between Generalized Linear Models (GLMs) trained on synthetic data with GLMs trained on the original data. Furthermore, we assess the ease of use of each generative approach and study the impact of augmenting original data with synthetic data on the performance of GLMs for predicting claim counts. Our results highlight the potential of MICE-based methods in creating high-quality tabular data while being more user-friendly than the other methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02171v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yevhen Havrylenko, Meelis K\"a\"arik, Artur Tuttar</dc:creator>
    </item>
    <item>
      <title>Speech transformer models for extracting information from baby cries</title>
      <link>https://arxiv.org/abs/2509.02259</link>
      <description>arXiv:2509.02259v1 Announce Type: cross 
Abstract: Transfer learning using latent representations from pre-trained speech models achieves outstanding performance in tasks where labeled data is scarce. However, their applicability to non-speech data and the specific acoustic properties encoded in these representations remain largely unexplored. In this study, we investigate both aspects. We evaluate five pre-trained speech models on eight baby cries datasets, encompassing 115 hours of audio from 960 babies. For each dataset, we assess the latent representations of each model across all available classification tasks. Our results demonstrate that the latent representations of these models can effectively classify human baby cries and encode key information related to vocal source instability and identity of the crying baby. In addition, a comparison of the architectures and training strategies of these models offers valuable insights for the design of future models tailored to similar tasks, such as emotion detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.02259v1</guid>
      <category>cs.SD</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Guillem Bonafos, J\'eremy Rouch, L\'eny Lego, David Reby, Hugues Patural, Nicolas Mathevon, R\'emy Emonet</dc:creator>
    </item>
    <item>
      <title>Path Analysis Of Covid-19 with the Influence of Air Pressure, Air Temperature, and Relative Humidity</title>
      <link>https://arxiv.org/abs/2105.05451</link>
      <description>arXiv:2105.05451v2 Announce Type: replace 
Abstract: Coronavirus disease 2019 (COVID-19) is one of the most infectious diseases and one of the greatest challenge due to global health crisis. The virus has been transmitted globally and spreading so fast with high incidence. While, the virus still pandemic, the government scramble to seek antiviral treatment and vaccines to combat the diseases. This study was conducted to investigate the influence of air pressure, air temperature, and relative humidity on the number of confirmed cases in COVID-19. Based on the result, the calculation of reproduced correlation through path decompositions and subsequent comparison to the empirical correlation indicated that the path model fits the empirical data. The identified factor significantly influenced the number of confirmed cases of COVID-19. Therefore, the number of daily confirmed cases of COVID-19 may reduce as the amount of relative humidity increases; relative humidity will increase as the amount of air temperature decreases; and the amount of air temperature will decrease as the amount of air pressure decreases. Thus, it is recommended that policy-making bodies consider the result of this study when implementing programs for COVID-19 and increase public awareness on the effects of weather condition, as it is one of the factors to control the number of COVID-19 cases.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.05451v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.21474/IJAR01/10771</arxiv:DOI>
      <arxiv:journal_reference>International Journal of Advanced Research, 8(4), 224-232 (2020)</arxiv:journal_reference>
      <dc:creator>Marvin G. Pizon, Ronald R. Baldo, Ruthlyn N. Villarante, Jessica D. Balatero</dc:creator>
    </item>
    <item>
      <title>A Path Model to Infer Mathematics Performance: The Interrelated Impact of Motivation, Attitude, Learning Style and Teaching Strategies Variables</title>
      <link>https://arxiv.org/abs/2105.05850</link>
      <description>arXiv:2105.05850v2 Announce Type: replace 
Abstract: The present study aims at exploring predictors influencing mathematics performance. In particular, the research focuses on four subject components such as motivation, attitude towards mathematics, learning style, and teaching strategies. The study respondents have involved a sample of 240 students from Agusan del Sur State College of Agriculture and Technology (ASSCAT). Path analysis will be used to test the direct and indirect relations between the predictors and mathematics performance. Based on the result, the calculation of reproduced correlation through path decompositions and subsequent comparison to the empirical correlation indicated that the path model fits the observed data. Results also revealed that a large proportion of mathematics performance could be predicted from the attitude towards mathematics, learning style, and teaching strategies. Moreover, attitude towards mathematics, learning style, and teaching strategies influence mathematics performance directly and indirectly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2105.05850v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.55927/eajmr.v1i3.104</arxiv:DOI>
      <arxiv:journal_reference>East Asian Journal of Multidisciplinary Research 1(3), 315-330 (2022)</arxiv:journal_reference>
      <dc:creator>Marvin G. Pizon, Shiryl T. Ytoc</dc:creator>
    </item>
    <item>
      <title>Causal inference and racial bias in policing: New estimands and the importance of mobility data</title>
      <link>https://arxiv.org/abs/2409.08059</link>
      <description>arXiv:2409.08059v2 Announce Type: replace 
Abstract: Studying racial bias in policing is a critically important problem, but one that comes with a number of inherent difficulties due to the nature of the available data. In this manuscript we tackle multiple key issues in the causal analysis of racial bias in policing. First, we formalize race and place policing, the idea that individuals of one race are policed differently when they are in neighborhoods primarily made up of individuals of other races. We develop an estimand to study this question rigorously, show the assumptions necessary for causal identification, and develop sensitivity analyses to assess robustness to violations of key assumptions. Additionally, we investigate difficulties with existing estimands targeting racial bias in policing. We show for these estimands, and the estimands developed in this manuscript, that estimation can benefit from incorporating mobility data into analyses. We apply these ideas to a study in New York City, where we find a large amount of racial bias, as well as race and place policing, and that these findings are robust to large violations of untestable assumptions. We additionally show that mobility data can make substantial impacts on the resulting estimates, suggesting it should be used whenever possible in subsequent studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.08059v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhuochao Huang, Brenden Beck, Joseph Antonelli</dc:creator>
    </item>
    <item>
      <title>Joint species distribution modeling of abundance data through latent variable barcodes</title>
      <link>https://arxiv.org/abs/2412.08793</link>
      <description>arXiv:2412.08793v2 Announce Type: replace 
Abstract: Accelerating global biodiversity loss has highlighted the role of complex relationships and shared patterns among species in determining their responses to environmental changes. The structure of an ecological community, represented by patterns of dependence among constituent species, signals its robustness more than individual species distributions. We focus on obtaining community-level insights based on underlying patterns in abundances of bird species in Finland. We propose \texttt{barcode}, a modeling framework to infer latent binary and continuous features of samples and species, expanding the class of concurrent ordinations. This approach introduces covariates and spatial autocorrelation hierarchically to facilitate ecological interpretations of the learned features. By analyzing 132 bird species counts, we infer the dominant environmental drivers of the community, species clusters and regions of common profile. Three of the learned drivers correspond to distinct climactic regions with different dominant forest types. Three further drivers are spatially heterogeneous and signal urban, agricultural, and wetland areas, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.08793v2</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Braden Scherting, Otso Ovaskainen, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Modeling Maritime Transportation Behavior Using AIS Trajectories and Markovian Processes in the Gulf of St. Lawrence</title>
      <link>https://arxiv.org/abs/2506.00025</link>
      <description>arXiv:2506.00025v3 Announce Type: replace 
Abstract: Maritime transportation is central to the global economy, and analyzing its large-scale behavioral data is critical for operational planning, environmental stewardship, and governance. This work presents a spatio-temporal analytical framework based on discrete-time Markov chains to model vessel movement patterns in the Gulf of St. Lawrence, with particular emphasis on disruptions induced by the COVID-19 pandemic. We discretize the maritime domain into hexagonal cells and construct mobility signatures for distinct vessel types using cell transition frequencies and dwell times. These features are used to build origin-destination matrices and spatial transition probability models that characterize maritime dynamics across multiple temporal resolutions. Focusing on commercial, fishing, and passenger vessels, we analyze the temporal evolution of mobility behaviors during the pandemic, highlighting significant yet transient disruptions to recurring transport patterns. The methodology we contribute to this paper allows for an extensive behavioral analytics key for transportation planning. Accordingly, our findings reveal vessel-specific mobility signatures that persist across spatially disjoint regions, suggesting behaviors invariant to time. In contrast, we observe temporal deviations among passenger and fishing vessels during the pandemic, reflecting the influence of social isolation measures and operational constraints on non-essential maritime transport in this region.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00025v3</guid>
      <category>stat.AP</category>
      <category>cs.CE</category>
      <category>math.PR</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Gabriel Spadon, Ruixin Song, Vaishnav Vaidheeswaran, Md Mahbub Alam, Floris Goerlandt, Ronald Pelot</dc:creator>
    </item>
    <item>
      <title>Functional Time Series Forecasting of Distributions: A Koopman-Wasserstein Approach</title>
      <link>https://arxiv.org/abs/2507.07570</link>
      <description>arXiv:2507.07570v3 Announce Type: replace 
Abstract: We propose a novel method for forecasting the temporal evolution of probability distributions observed at discrete time points. Extending the Dynamic Probability Density Decomposition (DPDD), we embed distributional dynamics into Wasserstein geometry via a Koopman operator framework. Our approach introduces an importance-weighted variant of Extended Dynamic Mode Decomposition (EDMD), enabling accurate, closed-form forecasts in 2-Wasserstein space. Theoretical guarantees are established: our estimator achieves spectral convergence and optimal finite-sample Wasserstein error. Simulation studies and a real-world application to U.S. housing price distributions show substantial improvements over existing methods such as Wasserstein Autoregression. By integrating optimal transport, functional time series modeling, and spectral operator theory, DPDD offers a scalable and interpretable solution for distributional forecasting. This work has broad implications for behavioral science, public health, finance, and neuroimaging--domains where evolving distributions arise naturally. Our framework contributes to functional data analysis on non-Euclidean spaces and provides a general tool for modeling and forecasting distributional time series.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.07570v3</guid>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziyue Wang, Yuko Araki</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of diffusion-driven multi-type epidemic models with application to COVID-19</title>
      <link>https://arxiv.org/abs/2211.15229</link>
      <description>arXiv:2211.15229v4 Announce Type: replace-cross 
Abstract: We consider a flexible Bayesian evidence synthesis approach to model the age-specific transmission dynamics of COVID-19 based on daily mortality counts. The temporal evolution of transmission rates in populations containing multiple types of individuals is reconstructed via an appropriate dimension-reduction formulation driven by independent diffusion processes. A suitably tailored compartmental model is used to learn the latent counts of infection, accounting for fluctuations in transmission influenced by public health interventions and changes in human behaviour. The model is fitted to freely available COVID-19 data sources from the UK, Greece, and Austria and validated using a large-scale prevalence survey in England. In particular, we demonstrate how model expansion can facilitate evidence reconciliation at a latent level. The code implementing this work is made freely available via the Bernadette R package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.15229v4</guid>
      <category>stat.CO</category>
      <category>physics.soc-ph</category>
      <category>q-bio.PE</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssa/qnaf130</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society Series A: Statistics in Society, 2025, qnaf130</arxiv:journal_reference>
      <dc:creator>Lampros Bouranis, Nikolaos Demiris, Konstantinos Kalogeropoulos, Ioannis Ntzoufras</dc:creator>
    </item>
    <item>
      <title>Statistical Performance Guarantee for Subgroup Identification with Generic Machine Learning</title>
      <link>https://arxiv.org/abs/2310.07973</link>
      <description>arXiv:2310.07973v3 Announce Type: replace-cross 
Abstract: Across a wide array of disciplines, many researchers use machine learning (ML) algorithms to identify a subgroup of individuals who are likely to benefit from a treatment the most (``exceptional responders'') or those who are harmed by it. A common approach to this subgroup identification problem consists of two steps. First, researchers estimate the conditional average treatment effect (CATE) using an ML algorithm. Next, they use the estimated CATE to select those individuals who are predicted to be most affected by the treatment, either positively or negatively. Unfortunately, CATE estimates are often biased and noisy. In addition, utilizing the same data to both identify a subgroup and estimate its group average treatment effect results in a multiple testing problem. To address these challenges, we develop uniform confidence bands for estimation of the group average treatment effect sorted by generic ML algorithm (GATES). Using these uniform confidence bands, researchers can identify, with a statistical guarantee, a subgroup whose GATES exceeds a certain effect size, regardless of how this effect size is chosen. The validity of the proposed methodology depends solely on randomization of treatment and random sampling of units. Importantly, our method does not require modeling assumptions and avoids a computationally intensive resampling procedure. A simulation study shows that the proposed uniform confidence bands are reasonably informative and have an appropriate empirical coverage even when the sample size is as small as 100. We analyze a clinical trial of late-stage prostate cancer and find a relatively large proportion of exceptional responders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.07973v3</guid>
      <category>stat.ME</category>
      <category>math.OC</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Lingzhi Li, Kosuke Imai</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate and Localizing Power</title>
      <link>https://arxiv.org/abs/2401.03554</link>
      <description>arXiv:2401.03554v2 Announce Type: replace-cross 
Abstract: False discovery rate (FDR) is commonly used for correction for multiple testing in neuroimaging studies. However, when using two-tailed tests, making directional inferences about the results can lead to vastly inflated error rate, even approaching 100% in some cases. This happens because FDR control the error rate only globally, over all tests, not within subsets, such as among those in only one or another direction. Here we consider and evaluate different strategies for FDR control with two-tailed tests, using both synthetic and real imaging data. Approaches that separate the tests by direction of the hypothesis test, or by the direction of the resulting test statistic, more properly control the directional error rate and preserve FDR benefits, albeit with a doubled risk of errors under complete absence of signal. Strategies that combine tests in both directions, or that use simple two-tailed p-values, can lead to invalid directional conclusions, even if these tests remain globally valid. To enable valid thresholding for directional inference, we suggest that imaging software should allow the possibility that the user sets asymmetrical thresholds for the two sides of the statistical map. While FDR continues to be a valid, powerful procedure for multiple testing correction, care is needed when making directional inferences for two-tailed tests, or more broadly, when making any localized inference.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.03554v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anderson M. Winkler, Paul A. Taylor, Thomas E. Nichols, Chris Rorden</dc:creator>
    </item>
    <item>
      <title>Multilevel functional distributional models with application to continuous glucose monitoring in diabetes clinical trials</title>
      <link>https://arxiv.org/abs/2403.10514</link>
      <description>arXiv:2403.10514v2 Announce Type: replace-cross 
Abstract: Continuous glucose monitoring (CGM) is a minimally invasive technology that measures blood glucose every few minutes for weeks or months at a time. CGM data are often collected in the free-living environment and is strongly related to sleep, physical activity and meal intake. As the timing of these activities varies substantially within- and between-individuals, it is difficult to model CGM trajectories as a function of time of day. Therefore, in practice, CGM trajectories are often reduced to one or two scalar summaries of the thousands of measurements collected for a study participant. To alleviate the potential loss of information, the cumulative distribution function (cdf) of the CGM time series was proposed as an alternative. Here we address the problem of conducting inference on cdfs in clinical trials with long follow up and frequent measurements. Our approach provides three major innovations: (1) modeling the entire cdf and preserving its monotonicity; (2) accounting for the cdfs correlation (because they are measured on the same individual), continuity (results are robust to the choice of the probability grid), and differential error (e.g., medians have lower variability than $0.99$ quantiles); and (3) preserving the family-wise error when the observed data are longitudinal samples of cdfs. We focus on modeling data collected by The Juvenile Diabetes Research Foundation Continuous Glucose Monitoring Group in a large clinical trial that collected CGM data every few minutes for 26 weeks. Our basic observation unit is the distribution of CGM observations in a four--week interval. The scientific goals are to: (1) identify and quantify the effects of factors that affect glycaemic control in type 1 diabetes patients (T1D); and (2) identify and characterize the patients who respond to treatment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.10514v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Ciprian M. Crainiceanu</dc:creator>
    </item>
    <item>
      <title>Assessing Vaccine Effectiveness in Observational Studies via Nested Trial Emulation</title>
      <link>https://arxiv.org/abs/2403.18115</link>
      <description>arXiv:2403.18115v2 Announce Type: replace-cross 
Abstract: Observational data are often used to estimate real-world effectiveness and durability of vaccines. A sequence of trials can be emulated to draw inference from such data while minimizing selection bias, immortal time bias, and confounding. Typically, when nested trial emulation (NTE) is employed, effect estimates are pooled across trials. However, such pooled estimates may lack a clear interpretation when the treatment effect is heterogeneous across trials. For vaccines against certain viruses, vaccine effectiveness may vary over calendar time due to newly emerging variants of the virus. This manuscript considers a NTE inverse probability weighted estimator of vaccine effectiveness that may vary over calendar time, time since vaccination, or both. Statistical testing of the trial effect homogeneity assumption is considered. As observed changes in vaccine effectiveness across trials may be attributable to variation in covariate distributions across trial-eligible populations, standardization of trial-specific inferences is also considered. Simulation studies are presented examining the finite-sample performance of the proposed methods under a variety of scenarios. The methods are used to estimate vaccine effectiveness against COVID-19 outcomes using observational data on over 110,000 residents of Abruzzo, Italy during 2021.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.18115v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Justin B. DeMonte, Bonnie E. Shook-Sa, Michael G. Hudgens</dc:creator>
    </item>
    <item>
      <title>Beyond Scalar Metrics: Functional Data Analysis of Postprandial Continuous Glucose Monitoring in the AEGIS Study</title>
      <link>https://arxiv.org/abs/2405.14690</link>
      <description>arXiv:2405.14690v2 Announce Type: replace-cross 
Abstract: Postprandial glucose collected through continuous glucose monitoring (CGM) provides critical information for assessing metabolic capacity and guiding dietary recommendations. Traditional approaches summarize these data into scalar measures, such as 2-hour AUC or peak glucose, potentially overlooking temporal dynamics. We propose analyzing entire CGM trajectories using multilevel functional data analysis (FDA), which accounts for the smooth, hierarchical nature of glucose responses. Applying these methods to AEGIS study participants without diabetes, we illustrate how FDA characterizes variability in postprandial responses and links dietary/patient characteristics to glucose dynamics. We further extend the r-square metric to hierarchical functional models to quantify explanatory power. Our results show that dietary effects vary across the 6-hour postprandial window-for example, fiber blunts responses after 90 minutes, while fats reduce early rises within 50 minutes. Moreover, metabolic responses differ between normoglycemic and prediabetic individuals. These findings demonstrate that functional approaches reveal temporal and stratified insights into postprandial glucose regulation that scalar methods cannot capture.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14690v2</guid>
      <category>q-bio.QM</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marcos Matabuena, Joe Sartini, Francisco Gude</dc:creator>
    </item>
    <item>
      <title>CDsampling: An R Package for Constrained D-Optimal Sampling in Paid Research Studies</title>
      <link>https://arxiv.org/abs/2410.20606</link>
      <description>arXiv:2410.20606v4 Announce Type: replace-cross 
Abstract: In the context of paid research studies and clinical trials, budget considerations often require patient sampling from available populations which comes with inherent constraints. We introduce the R package CDsampling, which is the first to our knowledge to integrate optimal design theories within the framework of constrained sampling. This package offers the possibility to find both D-optimal approximate and exact allocations for samplings with or without constraints. Additionally, it provides functions to find constrained uniform sampling as a robust sampling strategy when the model information is limited. To demonstrate its efficacy, we provide simulated examples and a real-data example with datasets embedded in the package and compare them with classical sampling methods. Furthermore, the package revisits the theoretical results of the Fisher information matrix for generalized linear models (including regular linear regression model) and multinomial logistic models, offering functions for its computation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20606v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifei Huang, Liping Tong, Jie Yang</dc:creator>
    </item>
    <item>
      <title>Fast and Cheap Krylov-Based Covariance Smoothing</title>
      <link>https://arxiv.org/abs/2501.08265</link>
      <description>arXiv:2501.08265v2 Announce Type: replace-cross 
Abstract: We introduce the Tensorized-and-Restricted Krylov (TReK) method, a simple and efficient algorithm for estimating covariance tensors with large observational sizes. TReK extends the conjugate gradient method to incorporate range restrictions, enabling its use in a variety of covariance smoothing applications. By leveraging matrix-level operations, it achieves significant improvements in both computational speed and memory cost, improving over existing methods by an order of magnitude. TReK ensures finite-step convergence in the absence of rounding errors and converges fast in practice, making it well-suited for large-scale problems. The algorithm is also highly flexible, supporting a wide range of forward and projection tensors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08265v2</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ho Yun, Victor M. Panaretos</dc:creator>
    </item>
    <item>
      <title>Global Climate Model Bias Correction Using Deep Learning</title>
      <link>https://arxiv.org/abs/2504.19145</link>
      <description>arXiv:2504.19145v2 Announce Type: replace-cross 
Abstract: Climate change affects ocean temperature, salinity and sea level, impacting monsoons and ocean productivity. Future projections by Global Climate Models based on shared socioeconomic pathways from the Coupled Model Intercomparison Project (CMIP) are widely used to understand the effects of climate change. However, CMIP models have significant bias compared to reanalysis in the Bay of Bengal for the time period when both projections and reanalysis are available. For example, there is a 1.5C root mean square error (RMSE) in the sea surface temperature (SST) projections of the climate model CNRM-CM6 compared to the Ocean Reanalysis System (ORAS5). We develop a suite of data-driven deep learning models for bias correction of climate model projections and apply it to correct SST projections of the Bay of Bengal. We propose the use of three different deep neural network architectures: convolutional encoder-decoder UNet, Bidirectional LSTM and ConvLSTM. We also use a baseline linear regression model and the Equi-Distant Cumulative Density Function (EDCDF) bias correction method for comparison and evaluating the impact of the new deep learning models. All bias correction models are trained using pairs of monthly CMIP6 projections and the corresponding month's ORAS5 as input and output. Historical data (1950-2014) and future projection data (2015-2020) of CNRM-CM6 are used for training and validation, including hyperparameter tuning. Testing is performed on future projection data from 2021 to 2024. Detailed analysis of the three deep neural models has been completed. We found that the UNet architecture trained using a climatology-removed CNRM-CM6 projection as input and climatology-removed ORAS5 as output gives the best bias-corrected projections. Our novel deep learning-based method for correcting CNRM-CM6 data has a 15% reduction in RMSE compared EDCDF.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.19145v2</guid>
      <category>physics.ao-ph</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1088/3049-4753/ade9c3</arxiv:DOI>
      <arxiv:journal_reference>Abhishek Pasula and Deepak N Subramani 2025 Mach. Learn.: Earth 1 015001</arxiv:journal_reference>
      <dc:creator>Abhishek Pasula, Deepak N. Subramani</dc:creator>
    </item>
    <item>
      <title>Generalizable estimation of conditional average treatment effects using Causal Forest in randomized controlled trials</title>
      <link>https://arxiv.org/abs/2506.12296</link>
      <description>arXiv:2506.12296v2 Announce Type: replace-cross 
Abstract: Estimating conditional average treatment effects (CATE) from randomized controlled trials (RCTs) and generalizing them to broader populations is essential for individualized treatment rules but is complicated by selection bias and high dimensional covariates. We evaluated Causal Forest based CATE estimation strategies that address trial selection bias. Specifically, we compared approaches of fitting Causal Forest with covariates of interest only, additionally including covariates that determine trial participation, and repeating these models with inverse probability weighting (IPW) to reweight trial samples to the source population. Identification theory suggests unbiased CATE estimation is possible when covariates related to trial participation are included. However, simulation studies demonstrated that, under realistic RCT sample sizes, variance inflation from high dimensional covariates often outweighed modest bias reduction. Including greater than 3 covariates related to participation substantially degraded precision unless sample sizes were large. In contrast, IPW methods consistently improved performance across scenarios, even when the weighting model was misspecified. Application to the VITAL trial of omega 3 fatty acids and coronary heart disease further illustrated how IPW shifts estimates toward source population effects and refines heterogeneity assessments. Our findings highlight a fundamental bias variance tradeoff in generalizing CATE from RCTs. While inclusion of trial selection variables ensures consistency in theory, in practice it may worsen performance in medical trials with sample size of 5000 or less. More efficient strategies are to limit CATE models to strong effect modifiers and address selection bias separately through IPW. These results provide practical guidance for applying CATE estimation in clinical and epidemiologic research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.12296v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rikuta Hamaya, Etsuji Suzuki, Konan Hara</dc:creator>
    </item>
    <item>
      <title>A Log-Linear Analytics Approach to Cost Model Regularization for Inpatient Stays through Diagnostic Code Merging</title>
      <link>https://arxiv.org/abs/2507.03843</link>
      <description>arXiv:2507.03843v2 Announce Type: replace-cross 
Abstract: Cost models in healthcare research must balance interpretability, accuracy, and parameter consistency. However, interpretable models often struggle to achieve both accuracy and consistency. Ordinary least squares (OLS) models for high-dimensional regression can be accurate but fail to produce stable regression coefficients over time when using highly granular ICD-10 diagnostic codes as predictors. This instability arises because many ICD-10 codes are infrequent in healthcare datasets. While regularization methods such as Ridge can address this issue, they risk discarding important predictors. Here, we demonstrate that reducing the granularity of ICD-10 codes is an effective regularization strategy within OLS while preserving the representation of all diagnostic code categories. By truncating ICD-10 codes from seven characters to six or fewer, we reduce the dimensionality of the regression problem while maintaining model interpretability and consistency. Mathematically, the merging of predictors in OLS leads to increased trace of the Hessian matrix, which reduces the variance of coefficient estimation. Our findings explain why broader diagnostic groupings like DRGs and HCC codes are favored over highly granular ICD-10 codes in real-world risk adjustment and cost models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.03843v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chi-Ken Lu, David Alonge, Nicole Richardson, Bruno Richard</dc:creator>
    </item>
    <item>
      <title>A monotone single index model for spatially referenced multistate current status data</title>
      <link>https://arxiv.org/abs/2507.09057</link>
      <description>arXiv:2507.09057v2 Announce Type: replace-cross 
Abstract: Assessment of multistate disease progression is commonplace in biomedical research, such as, in periodontal disease (PD). However, the presence of multistate current status endpoints, where only a single snapshot of each subject's progression through disease states is available at a random inspection time after a known starting state, complicates the inferential framework. In addition, these endpoints can be clustered, and spatially associated, where a group of proximally located teeth (within subjects) may experience similar PD status, compared to those distally located. Motivated by a clinical study recording PD progression, we propose a Bayesian semiparametric accelerated failure time model with an inverse-Wishart proposal for accommodating (spatial) random effects, and flexible errors that follow a Dirichlet process mixture of Gaussians. For clinical interpretability, the systematic component of the event times is modeled using a monotone single index model, with the (unknown) link function estimated via a novel integrated basis expansion and basis coefficients endowed with constrained Gaussian process priors. In addition to establishing parameter identifiability, we present scalable computing via a combination of elliptical slice sampling, fast circulant embedding techniques, and smoothing of hard constraints, leading to straightforward estimation of parameters, and state occupation and transition probabilities. Using synthetic data, we study the finite sample properties of our Bayesian estimates, and their performance under model misspecification. We also illustrate our method via application to the real clinical PD dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.09057v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1093/biomtc/ujaf105</arxiv:DOI>
      <dc:creator>Snigdha Das, Minwoo Chae, Debdeep Pati, Dipankar Bandyopadhyay</dc:creator>
    </item>
    <item>
      <title>Mutual Information Surprise: Rethinking Unexpectedness in Autonomous Systems</title>
      <link>https://arxiv.org/abs/2508.17403</link>
      <description>arXiv:2508.17403v2 Announce Type: replace-cross 
Abstract: Recent breakthroughs in autonomous experimentation have demonstrated remarkable physical capabilities, yet their cognitive control remains limited--often relying on static heuristics or classical optimization. A core limitation is the absence of a principled mechanism to detect and adapt to the unexpectedness. While traditional surprise measures--such as Shannon or Bayesian Surprise--offer momentary detection of deviation, they fail to capture whether a system is truly learning and adapting. In this work, we introduce Mutual Information Surprise (MIS), a new framework that redefines surprise not as anomaly detection, but as a signal of epistemic growth. MIS quantifies the impact of new observations on mutual information, enabling autonomous systems to reflect on their learning progression. We develop a statistical test sequence to detect meaningful shifts in estimated mutual information and propose a mutual information surprise reaction policy (MISRP) that dynamically governs system behavior through sampling adjustment and process forking. Empirical evaluations--on both synthetic domains and a dynamic pollution map estimation task--show that MISRP-governed strategies significantly outperform classical surprise-based approaches in stability, responsiveness, and predictive accuracy. By shifting surprise from reactive to reflective, MIS offers a path toward more self-aware and adaptive autonomous systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.17403v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <pubDate>Wed, 03 Sep 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yinsong Wang, Quan Zeng, Xiao Liu, Yu Ding</dc:creator>
    </item>
  </channel>
</rss>
