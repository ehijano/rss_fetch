<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Feb 2026 05:00:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>NFT Games: an Empirical Look into the Play-to-Earn Model</title>
      <link>https://arxiv.org/abs/2602.13882</link>
      <description>arXiv:2602.13882v1 Announce Type: new 
Abstract: The past decade has witnessed the burgeoning and continuous development of blockchain and its applications. Besides various cryptocurrencies, an industry that has quickly embraced this trend is gaming. Thanks to the support of blockchain, games have started to incorporate non-fungible tokens (NFTs) that can enable a new gaming model, play-to-earn (P2E), which incentivizes users to participate and play. While recent studies looked at several NFT games qualitatively and individually, an in-depth understanding is still missing, particularly on how the P2E model has transformed traditional games. In this work, we set to conduct a measurement study of NFT games, aiming to gain a comprehensive understanding of the effectiveness of P2E in practice. For this purpose, we collect and analyze relevant NFT transaction data from the underlying blockchain (e.g., Ethereum) of 12 games, supplemented with various data scraped from their websites. Our study shows that (1) a few top wallets control unproportionally high percentage of NFTs, and the majority of wallets own only one or two NFTs and do not actively trade; (2) promotion events do boost the trade amount and the NFT price for some games, but their effect does not sustain; and (3) few players actually earned a profit, and players in 9 out of 12 games who traded NFTs have a negative profit on average. Motivated by these findings, we further investigate effective incentive mechanisms based on game theory to improve the trading profits that players can earn from these NFT games. Both modeling and simulation results confirm the effectiveness of the proposed incentive mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13882v1</guid>
      <category>cs.CE</category>
      <category>cs.GT</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yixiao Gao, Fei Li, Ruizhe Shi, Ruizhi Cheng, Jean Zhang, Bo Han, Songqing Chen</dc:creator>
    </item>
    <item>
      <title>Simultaneous analysis of curved Kirchhoff beams and Kirchhoff--Love shells embedded in bulk domains</title>
      <link>https://arxiv.org/abs/2602.14566</link>
      <description>arXiv:2602.14566v1 Announce Type: new 
Abstract: A set of curved beams and shells is geometrically implied by level sets of a scalar function over some bulk domain. The mechanical model for each structure is based on the Kirchhoff--Love theory, that is, small displacements without shear deformations are considered. These models for individual geometries are extended to bulk models, simultaneously modeling the whole set of beams/shells on all level sets. A major focus is on the numerical analysis of such models. A mixed-hybrid and higher-order accurate Bulk Trace FEM is proposed that enables the use of standard $C^0$-continuous Lagrange elements with dimensionality of the bulk domain. That is, the higher-order continuity requirements of displacement-based formulations in context of the Kirchhoff--Love theory are successfully alleviated. Several numerical tests confirm the accuracy and higher-order convergence of the proposed methodology, also qualifying as benchmark test cases in future studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14566v1</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jonas Neumeyer, Michael Wolfgang Kaiser, Thomas-Peter Fries</dc:creator>
    </item>
    <item>
      <title>Identification of random material properties as stochastic inversion problem</title>
      <link>https://arxiv.org/abs/2602.14684</link>
      <description>arXiv:2602.14684v1 Announce Type: new 
Abstract: Heterogeneity of many building materials complicates numerical modelling of structural behaviour. The material randomicity can be manifested by different values of material parameters of each material specimen. To capture inherent variability of heterogeneous materials, the model parameters describing the material properties are considered as random variables and their identification consists in solving a~stochastic inversion problem. The stochastic inversion is based on searching for probabilistic description of model parameters which provides the distribution of the model response corresponding to the distribution of the observed data. The paper presents two different formulations of the stochastic inversion problem. The first formulation arises from the Bayesian inference of uncertain statistical moments of a prescribed parameters' distribution while the main idea of the second one utilizes nonlinear transformation of random model parameters from distribution of the observed data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14684v1</guid>
      <category>cs.CE</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eli\v{s}ka Ko\v{c}kov\'a, Anna Ku\v{c}erov\'a</dc:creator>
    </item>
    <item>
      <title>Learning Gradient Flow: Using Equation Discovery to Accelerate Engineering Optimization</title>
      <link>https://arxiv.org/abs/2602.13513</link>
      <description>arXiv:2602.13513v1 Announce Type: cross 
Abstract: In this work, we investigate the use of data-driven equation discovery for dynamical systems to model and forecast continuous-time dynamics of unconstrained optimization problems. To avoid expensive evaluations of the objective function and its gradient, we leverage trajectory data on the optimization variables to learn the continuous-time dynamics associated with gradient descent, Newton's method, and ADAM optimization. The discovered gradient flows are then solved as a surrogate for the original optimization problem. To this end, we introduce the Learned Gradient Flow (LGF) optimizer, which is equipped to build surrogate models of variable polynomial order in full- or reduced-dimensional spaces at user-defined intervals in the optimization process. We demonstrate the efficacy of this approach on several standard problems from engineering mechanics and scientific machine learning, including two inverse problems, structural topology optimization, and two forward solves with different discretizations. Our results suggest that the learned gradient flows can significantly expedite convergence by capturing critical features of the optimization trajectory while avoiding expensive evaluations of the objective and its gradient.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13513v1</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.DS</category>
      <category>math.NA</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Grant Norman, Conor Rowan, Kurt Maute, Alireza Doostan</dc:creator>
    </item>
    <item>
      <title>OR-Agent: Bridging Evolutionary Search and Structured Research for Automated Algorithm Discovery</title>
      <link>https://arxiv.org/abs/2602.13769</link>
      <description>arXiv:2602.13769v1 Announce Type: cross 
Abstract: Automating scientific discovery in complex, experiment-driven domains requires more than iterative mutation of programs; it demands structured hypothesis management, environment interaction, and principled reflection. We present OR-Agent, a configurable multi-agent research framework designed for automated exploration in rich experimental environments. OR-Agent organizes research as a structured tree-based workflow that explicitly models branching hypothesis generation and systematic backtracking, enabling controlled management of research trajectories beyond simple mutation-crossover loops. At its core, we introduce an evolutionary-systematic ideation mechanism that unifies evolutionary selection of research starting points, comprehensive research plan generation, and coordinated exploration within a research tree. We further propose a hierarchical optimization-inspired reflection system: short-term experimental reflection operates as a form of verbal gradient providing immediate corrective signals; long-term reflection accumulates cross-experiment insights as verbal momentum; and memory compression serves as a regularization mechanism analogous to weight decay, preserving essential signals while mitigating drift. Together, these components form a principled architecture governing research dynamics. We conduct extensive experiments across classical combinatorial optimization benchmarks-including traveling salesman, capacitated vehicle routing, bin packing, orienteering, and multiple knapsack problems-as well as simulation-based cooperative driving scenarios. Results demonstrate that OR-Agent outperforms strong evolutionary baselines while providing a general, extensible, and inspectable framework for AI-assisted scientific discovery. OR-Agent source code and experiments data are publicly available at https://github.com/qiliuchn/OR-Agent.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.13769v1</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.NE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Liu, Wanjing Ma</dc:creator>
    </item>
    <item>
      <title>Towards Selection as Power: Bounding Decision Authority in Autonomous Agents</title>
      <link>https://arxiv.org/abs/2602.14606</link>
      <description>arXiv:2602.14606v1 Announce Type: cross 
Abstract: Autonomous agentic systems are increasingly deployed in regulated, high-stakes domains where decisions may be irreversible and institutionally constrained. Existing safety approaches emphasize alignment, interpretability, or action-level filtering. We argue that these mechanisms are necessary but insufficient because they do not directly govern selection power: the authority to determine which options are generated, surfaced, and framed for decision. We propose a governance architecture that separates cognition, selection, and action into distinct domains and models autonomy as a vector of sovereignty. Cognitive autonomy remains unconstrained, while selection and action autonomy are bounded through mechanically enforced primitives operating outside the agent's optimization space. The architecture integrates external candidate generation (CEFL), a governed reducer, commit-reveal entropy isolation, rationale validation, and fail-loud circuit breakers. We evaluate the system across multiple regulated financial scenarios under adversarial stress targeting variance manipulation, threshold gaming, framing skew, ordering effects, and entropy probing. Metrics quantify selection concentration, narrative diversity, governance activation cost, and failure visibility. Results show that mechanical selection governance is implementable, auditable, and prevents deterministic outcome capture while preserving reasoning capacity. Although probabilistic concentration remains, the architecture measurably bounds selection authority relative to conventional scalar pipelines. This work reframes governance as bounded causal power rather than internal intent alignment, offering a foundation for deploying autonomous agents where silent failure is unacceptable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.14606v1</guid>
      <category>cs.MA</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jose Manuel de la Chica Rodriguez, Juan Manuel Vera D\'iaz</dc:creator>
    </item>
    <item>
      <title>Predicting Company Growth using Scaling Theory informed Machine Learning</title>
      <link>https://arxiv.org/abs/2410.17587</link>
      <description>arXiv:2410.17587v2 Announce Type: replace 
Abstract: Predicting company growth is a critical yet challenging task because observed dynamics blend an underlying structural growth trend with volatile fluctuations. Here, we propose a Scaling-Theory-Informed Machine Learning (STIML) framework that integrates a scaling-based growth model to capture the mechanism-driven average trend, together with a data-driven forecasting model to learn the residual fluctuations. Using Compustat annual financial statement data (1950--2019) for 31,553 North American companies, we extend the growth model beyond assets to multiple financial indicators, and evaluate STIML against growth model-only and purely data-driven baselines. Across 16 target variables, we show that company growth exhibits a clear separation between trend-driven predictability and fluctuation-driven predictability, with their relative importance depending strongly on company size and volatility. Interpretability analyses further show that STIML captures multivariate dependencies beyond simple autocorrelation, and that macroeconomic variables contribute significantly less to predictive performance on average. Moreover, we find the scaling-based growth model overlooks asymmetric deviations, which instead contain the structured and learnable signals, suggesting a path to refine mechanistic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.17587v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>econ.GN</category>
      <category>physics.soc-ph</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruyi Tao, Veronica R. Cappelli, Kaiwei Liu, Marcus J. Hamilton, Christopher P. Kempes, Geoffrey B. Wes, Jiang Zhang</dc:creator>
    </item>
    <item>
      <title>Deep Generative Models in Condition and Structural Health Monitoring: Opportunities, Limitations and Future Outlook</title>
      <link>https://arxiv.org/abs/2507.15026</link>
      <description>arXiv:2507.15026v3 Announce Type: replace 
Abstract: Condition and structural health monitoring (CM/SHM) is a pivotal component of predictive maintenance (PdM) strategies across diverse industrial sectors, including mechanical rotating machinery, aircraft structures, wind turbines, and civil infrastructures. Conventional deep learning models, while effective for fault diagnosis and anomaly detection through automatic feature learning from sensor data, often struggle with operational variability, imbalanced or scarce fault datasets, and multimodal sensory data from complex systems. Deep generative models (DGMs) including deep autoregressive models, variational autoencoders, generative adversarial networks, diffusion-based models, and emerging large language models, offer transformative capabilities by synthesizing high-fidelity data samples, reconstructing latent system states, and modeling complex multimodal data streams. This review systematically examines state-of-the-art DGM applications in CM/SHM across the four main industrial systems mentioned above, emphasizing their roles in addressing key challenges: data generation, domain adaptation and generalization, multimodal data fusion, and downstream fault diagnosis and anomaly detection tasks, with rigorous comparison among signal processing, conventional machine learning or deep learning models, and DGMs. Lastly, we discuss current limitations of DGMs, including challenges of explainable and trustworthy models, computational inefficiencies for edge deployment, and the need for parameter-efficient fine-tuning strategies. Future research directions can focus on zero-shot and few-shot learning, robust multimodal data generation, hybrid architectures integrating DGMs with physics knowledge, and reinforcement learning with DGMs to enhance robustness and accuracy in industrial scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.15026v3</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xin Yang, Chen Fang, Yunlai Liao, Jian Yang, Konstantinos Gryllias, Dimitrios Chronopoulos</dc:creator>
    </item>
    <item>
      <title>Higher-order transmissibility and its linear approximation for in-service crack identification in train wheelset axles</title>
      <link>https://arxiv.org/abs/2507.18636</link>
      <description>arXiv:2507.18636v2 Announce Type: replace 
Abstract: In-service structural health monitoring is a so far rarely exploited, yet potent option for early-stage crack detection and identification in train wheelset axles. This procedure is non-trivial to enforce on the basis of a purely data-driven approach and typically requires the adoption of numerical, e.g. finite element-based, simulation schemes of the dynamic behavior of these axles. Damage in this particular case can be formulated as a breathing crack problem, which further complicates simulation by introducing response-dependent nonlinearities into the picture. In this study, first, a new crack detection feature based on higher-order harmonics of the breathing crack is proposed, termed Higher-Order Transmissibility (HOTr), and, secondly, its sensitivity and efficacy are assessed within the context of crack identification. Next, the mentioned feature is approximated via use of linear system theory, delivering a surrogate model which facilitates the computation and speeds up the crack identification procedure. The accuracy of the proposed method in reproducing the delivered HOTr is compared against the nonlinear simulation model. The obtained results suggest that the approximation of the HOTr can significantly reduce the computational burden by eliminating the need for an iterative solution of the governing nonlinear equation of motion, while maintaining a high level of accuracy when compared against the reference model. This implies great potential for adoption in in-service damage identification for wheelset axles, feasibly within a near real-time context.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.18636v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ehsan Naghizadeh, Eleni Chatzi, Paolo Tiso</dc:creator>
    </item>
    <item>
      <title>A Structured Neural ODE Approach for Real Time Evaluation of AC Losses in 3D Superconducting Tapes</title>
      <link>https://arxiv.org/abs/2510.14487</link>
      <description>arXiv:2510.14487v2 Announce Type: replace 
Abstract: Efficient modeling of High Temperature Superconductors (HTSs) is crucial for real-time quench monitoring; however, full-order electromagnetic simulations remain prohibitively costly due to the strong nonlinearities. Conventional projection-based reduced-order modeling pipelines for nonlinear problems, such as Proper Orthogonal Decomposition (POD)-Discrete Empirical Interpolation Method (DEIM), alleviate this cost but often require intrusive access to the Full Order Model (FOM) operators and a substantial number of interpolation points for hyperreduction. This work investigates reduced-order strategies for Integral Equation Method (IEM) of (HTS) systems. We present the first application of POD-DEIM to IEM-based HTS models, and introduce a Structured Neural Ordinary Differential Equation (Neural ODE) approach that learns nonlinear dynamics directly in the reduced space. The benchmark results show that Neural ODE outperforms POD-DEIM both in efficiency and accuracy, highlighting its potential for real-time simulations of superconductors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.14487v2</guid>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/TASC.2026.3657176</arxiv:DOI>
      <dc:creator>Riccardo Basei, Francesco Pase, Francesco Lucchini, Francesco Toso, Riccardo Torchio</dc:creator>
    </item>
    <item>
      <title>Fin-RATE: A Real-world Financial Analytics and Tracking Evaluation Benchmark for LLMs on SEC Filings</title>
      <link>https://arxiv.org/abs/2602.07294</link>
      <description>arXiv:2602.07294v3 Announce Type: replace 
Abstract: With the increasing deployment of Large Language Models (LLMs) in the finance domain, LLMs are increasingly expected to parse complex regulatory disclosures. However, existing benchmarks often focus on isolated details, failing to reflect the complexity of professional analysis that requires synthesizing information across multiple documents, reporting periods, and corporate entities. Furthermore, these benchmarks do not disentangle whether errors arise from retrieval failures, generation inaccuracies, domain-specific reasoning mistakes, or misinterpretation of the query or context, making it difficult to precisely diagnose performance bottlenecks. To bridge these gaps, we introduce Fin-RATE, a benchmark built on U.S. Securities and Exchange Commission (SEC) filings and mirroring financial analyst workflows through three pathways: detail-oriented reasoning within individual disclosures, cross-entity comparison under shared topics, and longitudinal tracking of the same firm across reporting periods. We benchmark 17 leading LLMs, spanning open-source, closed-source, and finance-specialized models, under both ground-truth context and retrieval-augmented settings. Results show substantial performance degradation, with accuracy dropping by 18.60\% and 14.35\% as tasks shift from single-document reasoning to longitudinal and cross-entity analysis. This degradation is driven by increased comparison hallucinations, temporal and entity mismatches, and is further reflected in declines in reasoning quality and factual consistency--limitations that existing benchmarks have yet to formally categorize or quantify.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.07294v3</guid>
      <category>cs.CE</category>
      <category>cs.AI</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yidong Jiang, Junrong Chen, Eftychia Makri, Jialin Chen, Peiwen Li, Ali Maatouk, Leandros Tassiulas, Eliot Brenner, Bing Xiang, Rex Ying</dc:creator>
    </item>
    <item>
      <title>MagneX: A High-Performance, GPU-Enabled, Data-Driven Micromagnetics Solver for Spintronics</title>
      <link>https://arxiv.org/abs/2602.12242</link>
      <description>arXiv:2602.12242v2 Announce Type: replace 
Abstract: In order to comprehensively investigate the multiphysics coupling in spintronic devices, it is essential to parallelize and utilize GPU-acceleration to address the spatial and temporal disparities inherent in the relevant physics. Additionally, the use of cutting-edge time integration libraries as well as machine learning (ML) approaches to replace and potentially accelerate expensive computational routines are attractive capabilities to enhance modeling capabilities moving forward. Leveraging the Exascale Computing Project software framework AMReX, as well as SUNDIALS time-integration libraries and python-based ML workflows, we have developed an open-source micromagnetics modeling tool called MagneX. This tool incorporates various crucial magnetic coupling mechanisms, including Zeeman coupling, demagnetization coupling, crystalline anisotropy interaction, exchange coupling, and Dzyaloshinskii-Moriya interaction (DMI) coupling. We demonstrate the GPU performance and scalability of the code and rigorously validate MagneX's functionality using the mumag standard problems and widely-accepted DMI benchmarks. Furthermore, we demonstrate the data-driven capability of MagneX by replacing the computationally-expensive demagnetization physics with neural network libraries trained from our simulation data. With the capacity to explore complete physical interactions, this innovative approach offers a promising pathway to better understand and develop fully integrated spintronic and electronic systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.12242v2</guid>
      <category>cs.CE</category>
      <category>cond-mat.other</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andy Nonaka, Yingheng Tang, Julian C. LePelch, Prabhat Kumar, Weiqun Zhang, Jorge A. Munoz, Christian Fernandez-Soria, Cesar Diaz, David J. Gardner, Zhi Jackie Yao</dc:creator>
    </item>
    <item>
      <title>Large and Deep Factor Models</title>
      <link>https://arxiv.org/abs/2402.06635</link>
      <description>arXiv:2402.06635v2 Announce Type: replace-cross 
Abstract: We show that a deep neural network (DNN) trained to construct a stochastic discount factor (SDF) admits a sharp additive decomposition that separates nonlinear characteristic discovery from the pricing rule that aggregates them. The economically relevant component of this decomposition is governed by a new object, the Portfolio Tangent Kernel (PTK), which captures the features learned by the network and induces an explicit linear factor pricing representation for the SDF. In population, the PTK-implied SDF converges to a ridge-regularized version of the true SDF, with the effective strength of regularization determined by the spectral complexity of the PTK. Using U.S. equity data, we show that the PTK representation delivers large and statistically significant performance gains, while its spectral complexity has risen sharply-by roughly a factor of six since the early 2000s-imposing increasingly tight limits on finite-sample pricing performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.06635v2</guid>
      <category>q-fin.ST</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bryan Kelly, Boris Kuznetsov, Semyon Malamud, Teng Andrea Xu, Yuan Zhang</dc:creator>
    </item>
    <item>
      <title>VCDF: A Validated Consensus-Driven Framework for Time Series Causal Discovery</title>
      <link>https://arxiv.org/abs/2410.19412</link>
      <description>arXiv:2410.19412v2 Announce Type: replace-cross 
Abstract: Time series causal discovery is essential for understanding dynamic systems, yet many existing methods remain sensitive to noise, non-stationarity, and sampling variability. We propose the Validated Consensus-Driven Framework (VCDF), a simple and method-agnostic layer that improves robustness by evaluating the stability of causal relations across blocked temporal subsets. VCDF requires no modification to base algorithms and can be applied to methods such as VAR-LiNGAM and PCMCI. Experiments on synthetic datasets show that VCDF improves VAR-LiNGAM by approximately 0.08-0.12 in both window and summary F1 scores across diverse data characteristics, with gains most pronounced for moderate-to-long sequences. The framework also benefits from longer sequences, yielding up to 0.18 absolute improvement on time series of length 1000 and above. Evaluations on simulated fMRI data and IT-monitoring scenarios further demonstrate enhanced stability and structural accuracy under realistic noise conditions. VCDF provides an effective reliability layer for time series causal discovery without altering underlying modeling assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19412v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.EM</category>
      <category>stat.CO</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gene Yu, Ce Guo, Wayne Luk</dc:creator>
    </item>
    <item>
      <title>Sparse Latent Factor Forecaster (SLFF) with Iterative Inference for Transparent Multi-Horizon Commodity Futures Prediction</title>
      <link>https://arxiv.org/abs/2505.06795</link>
      <description>arXiv:2505.06795v5 Announce Type: replace-cross 
Abstract: Amortized variational inference in latent-variable forecasters creates a deployment gap: the test-time encoder approximates a training-time optimization-refined latent, but without access to future targets. This gap introduces unnecessary forecast error and interpretability challenges. In this work, we propose the Sparse Latent Factor Forecaster with Iterative Inference (SLFF), addressing this through (i) a sparse coding objective with L1 regularization for low-dimensional latents, (ii) unrolled proximal gradient descent (LISTA-style) for iterative refinement during training, and (iii) encoder alignment to ensure amortized outputs match optimization-refined solutions. Under a linearized decoder assumption, we derive a design-motivating bound on the amortization gap based on encoder-optimizer distance, with convergence rates under mild conditions; empirical checks confirm the bound is predictive for the deployed MLP decoder. To prevent mixed-frequency data leakage, we introduce an information-set-aware protocol using release calendars and vintage macroeconomic data. Interpretability is formalized via a three-stage protocol: stability (Procrustes alignment across seeds), driver validity (held-out regressions against observables), and behavioral consistency (counterfactuals and event studies). Using commodity futures (Copper, WTI, Gold; 2005--2025) as a testbed, SLFF demonstrates significant improvements over neural baselines at 1- and 5-day horizons, yielding sparse factors that are stable across seeds and correlated with observable economic fundamentals (interpretability remains correlational, not causal). Code, manifests, diagnostics, and artifacts are released.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.06795v5</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Abhijit Gupta</dc:creator>
    </item>
    <item>
      <title>Multi-Objective Neural Network-Assisted Design Optimization of Soft Fin-Ray Fingers for Enhanced Grasping Performance</title>
      <link>https://arxiv.org/abs/2506.00494</link>
      <description>arXiv:2506.00494v2 Announce Type: replace-cross 
Abstract: The internal structure of the Fin-Ray fingers plays a significant role in their adaptability and grasping performance. However, modeling the grasp force and deformation behavior for design purposes is challenging. When the Fin-Ray finger becomes more rigid and capable of exerting higher forces, it becomes less delicate in handling objects. The contrast between these two gives rise to a multi-objective optimization problem. We employ the finite element method to estimate the deflections and contact forces of the Fin-Ray fingers grasping cylindrical objects, generating a dataset of 120 simulations. This dataset includes three input variables: the thickness of the front and support beams, the thickness of the crossbeams, and the equal spacing between the crossbeams, which are the design variables in the optimization. This dataset is then used to construct a multilayer perceptron (MLP) with four output neurons predicting the contact force and tip displacement in two directions. The magnitudes of maximum contact force and maximum tip displacement are two optimization objectives, showing the trade-off between force and delicate manipulation. The set of solutions is found using the non-dominated sorting genetic algorithm (NSGA-II). The results of the simulations demonstrate that the proposed methodology can be used to improve the design and grasping performance of soft grippers, aiding to choose a design not only for delicate grasping but also for high-force applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.00494v2</guid>
      <category>cs.RO</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ali Ghanizadeh, Ali Ahmadi, Arash Bahrami</dc:creator>
    </item>
    <item>
      <title>HEAS: Hierarchical Evolutionary Agent Simulation Framework for Cross-Scale Modeling and Multi-Objective Search</title>
      <link>https://arxiv.org/abs/2508.15555</link>
      <description>arXiv:2508.15555v2 Announce Type: replace-cross 
Abstract: Hierarchical Evolutionary Agent Simulation (HEAS) is a Python framework that unifies layered agent-based modeling with evolutionary optimization and tournament evaluation in a single, reproducible workflow. HEAS represents models as hierarchies of lightweight processes ("streams") scheduled in deterministic layers that read and write a shared context, making cross-scale couplings explicit and auditable. A compact API and CLI-simulate, optimize, evaluate-expose single- and multi-objective evolution, PyTorch policy integration via parameter flattening/unflattening, and general tournament tooling with user-defined scoring and voting rules. The framework standardizes evaluation through uniform per-step and episode metrics, persists seeds, logbooks, and hall-of-fame archives, and provides plotting helpers for traces, Pareto fronts, and comparative outcomes, reducing glue code and improving comparability across studies. HEAS emphasizes separation of mechanism from orchestration, allowing exogenous drivers, endogenous agents, and aggregators to be composed and swapped without refactoring, while the same model can be used for forward simulation, optimization, or systematic comparison. We illustrate usage with two compact examples-an ecological system and an enterprise decision-making setting. HEAS offers a practical foundation for cross-disciplinary, multi-level inquiry, yielding reliable, reproducible results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.15555v2</guid>
      <category>cs.MA</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <category>cs.NE</category>
      <category>cs.SE</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruiyu Zhang, Lin Nie, Xin Zhao</dc:creator>
    </item>
    <item>
      <title>A Financial Brain Scan of the LLM</title>
      <link>https://arxiv.org/abs/2508.21285</link>
      <description>arXiv:2508.21285v2 Announce Type: replace-cross 
Abstract: Emerging techniques in computer science make it possible to "brain scan" large language models (LLMs), identify the plain-English concepts that guide their reasoning, and steer them while holding other factors constant. We show that this approach can map LLM-generated economic forecasts to concepts such as sentiment, technical analysis, and timing, and compute their relative importance without reducing performance. We also show that models can be steered to be more or less risk-averse, optimistic, or pessimistic, which allows researchers to correct or simulate biases. The method is transparent, lightweight, and replicable for empirical research in the social sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.21285v2</guid>
      <category>q-fin.GN</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hui Chen (Mo), Antoine Didisheim (Mo),  Mohammad (Mo),  Pourmohammadi, Luciano Somoza, Hanqing Tian</dc:creator>
    </item>
    <item>
      <title>AgenticSciML: Collaborative Multi-Agent Systems for Emergent Discovery in Scientific Machine Learning</title>
      <link>https://arxiv.org/abs/2511.07262</link>
      <description>arXiv:2511.07262v2 Announce Type: replace-cross 
Abstract: Scientific Machine Learning (SciML) integrates data-driven inference with physical modeling to solve complex problems in science and engineering. However, the design of SciML architectures, loss formulations, and training strategies remains an expert-driven research process, requiring extensive experimentation and problem-specific insights. Here we introduce AgenticSciML, a collaborative multi-agent system in which over 10 specialized AI agents collaborate to propose, critique, and refine SciML solutions through structured reasoning and iterative evolution. The framework integrates structured debate, retrieval-augmented method memory, and ensemble-guided evolutionary search, enabling the agents to generate and assess new hypotheses about architectures and optimization procedures. Across physics-informed learning and operator learning tasks, the framework discovers solution methods that outperform single-agent and human-designed baselines by up to four orders of magnitude in error reduction. The agents produce novel strategies -- including adaptive mixture-of-expert architectures, decomposition-based PINNs, and physics-informed operator learning models -- that do not appear explicitly in the curated knowledge base. These results show that collaborative reasoning among AI agents can yield emergent methodological innovation, suggesting a path toward scalable, transparent, and autonomous discovery in scientific computing.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.07262v2</guid>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qile Jiang, George Karniadakis</dc:creator>
    </item>
    <item>
      <title>Potential-energy gating for robust state estimation in bistable stochastic systems</title>
      <link>https://arxiv.org/abs/2602.11712</link>
      <description>arXiv:2602.11712v2 Announce Type: replace-cross 
Abstract: We introduce potential-energy gating, a method for robust state estimation in systems governed by double-well stochastic dynamics. The observation noise covariance of a Bayesian filter is modulated by the local value of a known or assumed potential energy function: observations are trusted when the state is near a potential minimum and progressively discounted as it approaches the barrier separating metastable wells. This physics-based mechanism differs from statistical robust filters, which treat all state-space regions identically, and from constrained filters, which bound states rather than modulating observation trust. The approach is especially relevant in non-ergodic or data-scarce settings where only a single realization is available and statistical methods alone cannot learn the noise structure. We implement gating within Extended, Unscented, Ensemble, and Adaptive Kalman filters and particle filters, requiring only two additional hyperparameters. Monte Carlo benchmarks (100 replications) on a Ginzburg-Landau double-well with 10% outlier contamination show 57-80% RMSE improvement over the standard Extended Kalman Filter, all statistically significant (p &lt; 10^{-15}, Wilcoxon test). A naive topological baseline using only well positions achieves 57%, confirming that the continuous energy landscape adds ~21 percentage points. The method is robust to misspecification: even with 50% parameter errors, improvement never falls below 47%. Comparing externally forced and spontaneous Kramers-type transitions, gating retains 68% improvement under noise-induced transitions whereas the naive baseline degrades to 30%. As an empirical illustration, we apply the framework to Dansgaard-Oeschger events in the NGRIP delta-18O ice-core record, estimating asymmetry gamma = -0.109 (bootstrap 95% CI: [-0.220, -0.011]) and showing that outlier fraction explains 91% of the variance in filter improvement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2602.11712v2</guid>
      <category>cs.LG</category>
      <category>cs.CE</category>
      <category>nlin.CD</category>
      <category>physics.data-an</category>
      <category>stat.ME</category>
      <pubDate>Tue, 17 Feb 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luigi Simeone</dc:creator>
    </item>
  </channel>
</rss>
