<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>physics.med-ph updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/physics.med-ph</link>
    <description>physics.med-ph updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/physics.med-ph" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 Jan 2026 02:47:17 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Use of synthetic data for training dose estimation neural networks in CT dosimetry</title>
      <link>https://arxiv.org/abs/2601.09235</link>
      <description>arXiv:2601.09235v1 Announce Type: new 
Abstract: Personalized computed tomography (CT) dosimetry has great potential in assessing patient-specific radiation exposure, supporting risk assessment, and optimizing clinical protocols. The aim of this study is to evaluate the potential of synthetic anatomical data for improving machine learning-based personalized computed tomography (CT) dosimetry. It is investigated whether the combination of synthetic human body geometries with real patient data can improve model accuracy and generalization for CT organ dose estimation while maintaining the uncertainty requirements outlined in IAEA TRS-457. Deep learning models for organ dose prediction are trained using datasets with varying proportions of real and synthetic data. Synthetic datasets are generated from computational human phantoms with controlled distributions of organ volumes and body. A dedicated model uncertainty evaluation method is implemented to quantify prediction reliability and verify compliance with TRS-457 accuracy limits. Model performance and uncertainty are compared across different training data compositions, including a model trained solely on real patient data. As baseline validated Monte Carlo simulation is used. Models trained solely on synthetic data show limited predictive accuracy, particularly for small or peripheral organs. Incorporating as little as 10 % real patient data significantly improves both statistical accuracy and uncertainty estimates, achieving a performance comparable to that of real-only models. The hybrid training approach improves robustness across different anatomies while maintaining TRS-457-compliant uncertainty levels (k=2 uncertainty &lt; 20% for adults). The results indicate that the combination of real and synthetic data in combination with a systematic uncertainty assessment supports the development of CT dosimetry models and at the same time reduces the amount of real data required.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09235v1</guid>
      <category>physics.med-ph</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Marie-Luise Kuhlmann, J\"org Martin, Stefan Pojtinger</dc:creator>
    </item>
    <item>
      <title>GOUHFI 2.0: A Next-Generation Toolbox for Brain Segmentation and Cortex Parcellation at Ultra-High Field MRI</title>
      <link>https://arxiv.org/abs/2601.09006</link>
      <description>arXiv:2601.09006v1 Announce Type: cross 
Abstract: Ultra-High Field MRI (UHF-MRI) is increasingly used in large-scale neuroimaging studies, yet automatic brain segmentation and cortical parcellation remain challenging due to signal inhomogeneities, heterogeneous contrasts and resolutions, and the limited availability of tools optimized for UHF data. Standard software packages such as FastSurferVINN and SynthSeg+ often yield suboptimal results when applied directly to UHF images, thereby restricting region-based quantitative analyses. To address this need, we introduce GOUHFI 2.0, an updated implementation of GOUHFI that incorporates increased training data variability and additional functionalities, including cortical parcellation and volumetry.
  GOUHFI 2.0 preserves the contrast- and resolution-agnostic design of the original toolbox while introducing two independently trained 3D U-Net segmentation tasks. The first performs whole-brain segmentation into 35 labels across contrasts, resolutions, field strengths and populations, using a domain-randomization strategy and a training dataset of 238 subjects. Using the same training data, the second network performs cortical parcellation into 62 labels following the Desikan-Killiany-Tourville (DKT) protocol.
  Across multiple datasets, GOUHFI 2.0 demonstrated improved segmentation accuracy relative to the original toolbox, particularly in heterogeneous cohorts, and produced reliable cortical parcellations. In addition, the integrated volumetry pipeline yielded results consistent with standard volumetric workflows. Overall, GOUHFI 2.0 provides a comprehensive solution for brain segmentation, parcellation and volumetry across field strengths, and constitutes the first deep-learning toolbox enabling robust cortical parcellation at UHF-MRI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09006v1</guid>
      <category>eess.IV</category>
      <category>cs.CV</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Marc-Antoine Fortin, Anne Louise Kristoffersen, Paal Erik Goa</dc:creator>
    </item>
    <item>
      <title>Changes in Visual Attention Patterns for Detection Tasks due to Dependencies on Signal and Background Spatial Frequencies</title>
      <link>https://arxiv.org/abs/2601.09008</link>
      <description>arXiv:2601.09008v1 Announce Type: cross 
Abstract: We aim to investigate the impact of image and signal properties on visual attention mechanisms during a signal detection task in digital images. The application of insight yielded from this work spans many areas of digital imaging where signal or pattern recognition is involved in complex heterogenous background. We used simulated tomographic breast images as the platform to investigate this question. While radiologists are highly effective at analyzing medical images to detect and diagnose diseases, misdiagnosis still occurs. We selected digital breast tomosynthesis (DBT) images as a sample medical images with different breast densities and structures using digital breast phantoms (Bakic and XCAT). Two types of lesions (with distinct spatial frequency properties) were randomly inserted in the phantoms during projections to generate abnormal cases. Six human observers participated in observer study designed for a locating and detection of an 3-mm sphere lesion and 6-mm spicule lesion in reconstructed in-plane DBT slices. We collected eye-gaze data to estimate gaze metrics and to examine differences in visual attention mechanisms. We found that detection performance in complex visual environments is strongly constrained by later perceptual stages, with decision failures accounting for the largest proportion of errors. Signal detectability is jointly influenced by both target morphology and background complexity, revealing a critical interaction between local signal features and global anatomical noise. Increased fixation duration on spiculated lesions suggests that visual attention is differentially engaged depending on background and signal spatial frequency dependencies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09008v1</guid>
      <category>cs.CV</category>
      <category>eess.IV</category>
      <category>eess.SP</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amar Kavuri, Howard C. Gifford, Mini Das</dc:creator>
    </item>
    <item>
      <title>DeepLight: A Sobolev-trained Image-to-Image Surrogate Model for Light Transport in Tissue</title>
      <link>https://arxiv.org/abs/2601.09439</link>
      <description>arXiv:2601.09439v1 Announce Type: cross 
Abstract: In optoacoustic imaging, recovering the absorption coefficients of tissue by inverting the light transport remains a challenging problem. Improvements in solving this problem can greatly benefit the clinical value of optoacoustic imaging. Existing variational inversion methods require an accurate and differentiable model of this light transport. As neural surrogate models allow fast and differentiable simulations of complex physical processes, they are considered promising candidates to be used in solving such inverse problems. However, there are in general no guarantees that the derivatives of these surrogate models accurately match those of the underlying physical operator. As accurate derivatives are central to solving inverse problems, errors in the model derivative can considerably hinder high fidelity reconstructions. To overcome this limitation, we present a surrogate model for light transport in tissue that uses Sobolev training to improve the accuracy of the model derivatives. Additionally, the form of Sobolev training we used is suitable for high-dimensional models in general. Our results demonstrate that Sobolev training for a light transport surrogate model not only improves derivative accuracy but also reduces generalization error for in-distribution and out-of-distribution samples. These improvements promise to considerably enhance the utility of the surrogate model in downstream tasks, especially in solving inverse problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2601.09439v1</guid>
      <category>cs.LG</category>
      <category>physics.med-ph</category>
      <pubDate>Thu, 15 Jan 2026 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Haim, Vasilis Ntziachristos, Torsten En{\ss}lin, Dominik J\"ustel</dc:creator>
    </item>
  </channel>
</rss>
