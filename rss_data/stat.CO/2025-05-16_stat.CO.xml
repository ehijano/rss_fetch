<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.CO updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.CO</link>
    <description>stat.CO updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.CO" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 16 May 2025 04:01:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Optimally balancing exploration and exploitation to automate multi-fidelity statistical estimation</title>
      <link>https://arxiv.org/abs/2505.09828</link>
      <description>arXiv:2505.09828v1 Announce Type: new 
Abstract: Multi-fidelity methods that use an ensemble of models to compute a Monte Carlo estimator of the expectation of a high-fidelity model can significantly reduce computational costs compared to single-model approaches. These methods use oracle statistics, specifically the covariance between models, to optimally allocate samples to each model in the ensemble. However, in practice, the oracle statistics are estimated using additional model evaluations, whose computational cost and induced error are typically ignored. To address this issue, this paper proposes an adaptive algorithm to optimally balance the resources between oracle statistics estimation and final multi-fidelity estimator construction, leveraging ideas from multilevel best linear unbiased estimators in Schaden and Ullmann (2020) and a bandit-learning procedure in Xu et al. (2022). Under mild assumptions, we demonstrate that the multi-fidelity estimator produced by the proposed algorithm exhibits mean-squared error commensurate with that of the best linear unbiased estimator under the optimal allocation computed with oracle statistics. Our theoretical findings are supported by detailed numerical experiments, including a parametric elliptic PDE and an ice-sheet mass-change modeling problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09828v1</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Dixon, Alex Gorodetsky, John Jakeman, Akil Narayan, Yiming Xu</dc:creator>
    </item>
    <item>
      <title>Robust and Computationally Efficient Trimmed L-Moments Estimation for Parametric Distributions</title>
      <link>https://arxiv.org/abs/2505.09860</link>
      <description>arXiv:2505.09860v1 Announce Type: cross 
Abstract: This paper proposes a robust and computationally efficient estimation framework for fitting parametric distributions based on trimmed L-moments. Trimmed L-moments extend classical L-moment theory by downweighting or excluding extreme order statistics, resulting in estimators that are less sensitive to outliers and heavy tails. We construct estimators for both location-scale and shape parameters using asymmetric trimming schemes tailored to different moments, and establish their asymptotic properties for inferential justification using the general structural theory of L-statistics, deriving simplified single-integration expressions to ensure numerical stability. State-of-the-art algorithms are developed to resolve the sign ambiguity in estimating the scale parameter for location-scale models and the tail index for the Frechet model. The proposed estimators offer improved efficiency over traditional robust alternatives for selected asymmetric trimming configurations, while retaining closed-form expressions for a wide range of common distributions, facilitating fast and stable computation. Simulation studies demonstrate strong finite-sample performance. An application to financial claim severity modeling highlights the practical relevance and flexibility of the approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09860v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chudamani Poudyal, Qian Zhao, Hari Sitaula</dc:creator>
    </item>
    <item>
      <title>FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant Tempering</title>
      <link>https://arxiv.org/abs/2505.10466</link>
      <description>arXiv:2505.10466v1 Announce Type: cross 
Abstract: Multi-modal and high-dimensional posteriors present significant challenges for variational inference, causing mode-seeking behavior and collapse despite the theoretical expressiveness of normalizing flows. Traditional annealing methods require temperature schedules and hyperparameter tuning, falling short of the goal of truly black-box variational inference. We introduce FlowVAT, a conditional tempering approach for normalizing flow variational inference that addresses these limitations. Our method tempers both the base and target distributions simultaneously, maintaining affine-invariance under tempering. By conditioning the normalizing flow on temperature, we leverage overparameterized neural networks' generalization capabilities to train a single flow representing the posterior across a range of temperatures. This preserves modes identified at higher temperatures when sampling from the variational posterior at $T = 1$, mitigating standard variational methods' mode-seeking behavior. In experiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT outperforms traditional and adaptive annealing methods, finding more modes and achieving better ELBO values, particularly in higher dimensions where existing approaches fail. Our method requires minimal hyperparameter tuning and does not require an annealing schedule, advancing toward fully-automatic black-box variational inference for complicated posteriors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10466v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juehang Qin, Shixiao Liang, Christopher Tunnell</dc:creator>
    </item>
    <item>
      <title>Designing a Data Science simulation with MERITS: A Primer</title>
      <link>https://arxiv.org/abs/2403.08971</link>
      <description>arXiv:2403.08971v2 Announce Type: replace 
Abstract: Simulations play a crucial role in the modern scientific process. Yet despite (or due to) this ubiquity, the Data Science community shares neither a comprehensive definition for a "high-quality" study nor a consolidated guide to designing one. Inspired by the Predictability-Computability-Stability (PCS) framework for 'veridical' Data Science, we propose six MERITS that a simulation study should satisfy. (Modularity and Efficiency support the computability of a study, encouraging clean and flexible implementation. Realism and Stability address the conceptualization of the research problem: How well does a study predict reality, such that its conclusions generalize to new data/contexts? Finally, Intuitiveness and Transparency encourage good communication and trustworthiness of study design and results.) Drawing an analogy between simulation and cooking, we moreover offer (a) a conceptual framework for thinking about the anatomy of a simulation 'recipe'; (b) a baker's dozen in guidelines to aid the Data Science practitioner in designing one; and (c) a case study demonstrating the practical utility of our framework by using it to autopsy a preexisting simulation study. With this "PCS primer" for high-quality Data Science simulation, we seek to distill and enrich the best practices of simulation across disciplines into a cohesive recipe for trustworthy, veridical Data Science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08971v2</guid>
      <category>stat.CO</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corrine F Elliott, James PC Duncan, Tiffany M Tang, Merle Behr, Karl Kumbier, Bin Yu</dc:creator>
    </item>
    <item>
      <title>Statistical Taylor Expansion</title>
      <link>https://arxiv.org/abs/2410.01223</link>
      <description>arXiv:2410.01223v5 Announce Type: replace 
Abstract: Statistical Taylor expansion replaces the input precise variables in a conventional Taylor expansion with random variables each with known distribution, to calculate the result mean and deviation. It is based on the uncorrelated uncertainty assumption: Each input variable is measured independently with fine enough statistical precision, so that their uncertainties are independent of each other. Statistical Taylor expansion reviews that the intermediate analytic expressions can no longer be regarded as independent of each other, and the result of analytic expression should be path independent. This conclusion differs fundamentally from the conventional common approach in applied mathematics to find the best execution path for a result. This paper also presents an implementation of statistical Taylor expansion called variance arithmetic, and the tests on variance arithmetic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01223v5</guid>
      <category>stat.CO</category>
      <category>cs.LG</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chengpu Wang</dc:creator>
    </item>
    <item>
      <title>Valid Bootstraps for Network Embeddings with Applications to Network Visualisation</title>
      <link>https://arxiv.org/abs/2410.20895</link>
      <description>arXiv:2410.20895v4 Announce Type: replace 
Abstract: Quantifying uncertainty in networks is an important step in modelling relationships and interactions between entities. We consider the challenge of bootstrapping an inhomogeneous random graph when only a single observation of the network is made and the underlying data generating function is unknown. We address this problem by considering embeddings of the observed and bootstrapped network that are statistically indistinguishable. We utilise an exchangeable network test that can empirically validate bootstrap samples generated by any method. Existing methods fail this test, so we propose a principled, distribution-free network bootstrap using k-nearest neighbour smoothing, that can pass this exchangeable network test in many synthetic and real-data scenarios. We demonstrate the utility of this work in combination with the popular data visualisation method t-SNE, where uncertainty estimates from bootstrapping are used to explain whether visible structures represent real statistically sound structures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.20895v4</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Emerald Dilworth, Ed Davis, Daniel J. Lawson</dc:creator>
    </item>
    <item>
      <title>On the statistical analysis of grouped data: when Pearson $\chi^2$ and other divisible statistics are not goodness-of-fit tests</title>
      <link>https://arxiv.org/abs/2406.09195</link>
      <description>arXiv:2406.09195v4 Announce Type: replace-cross 
Abstract: Thousands of experiments are analyzed and papers are published each year involving the statistical analysis of grouped data. While this area of statistics is often perceived -- somewhat naively -- as saturated, several misconceptions still affect everyday practice, and new frontiers have so far remained unexplored. Researchers must be aware of the limitations affecting their analyses and what are the new possibilities in their hands.
  Motivated by this need, the article introduces a unifying approach to the analysis of grouped data, which allows us to study the class of divisible statistics -- that includes Pearson's $\chi^2$, the likelihood ratio as special cases -- with a fresh perspective. The contributions collected in this manuscript span from modeling and estimation to distribution-free goodness-of-fit tests.
  Perhaps the most surprising result presented here is that, in a sparse regime, all tests proposed in the literature are dominated by members of the class of weighted linear statistics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.09195v4</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sara Algeri, Estate V. Khmaladze</dc:creator>
    </item>
    <item>
      <title>Lightspeed Geometric Dataset Distance via Sliced Optimal Transport</title>
      <link>https://arxiv.org/abs/2501.18901</link>
      <description>arXiv:2501.18901v2 Announce Type: replace-cross 
Abstract: We introduce sliced optimal transport dataset distance (s-OTDD), a model-agnostic, embedding-agnostic approach for dataset comparison that requires no training, is robust to variations in the number of classes, and can handle disjoint label sets. The core innovation is Moment Transform Projection (MTP), which maps a label, represented as a distribution over features, to a real number. Using MTP, we derive a data point projection that transforms datasets into one-dimensional distributions. The s-OTDD is defined as the expected Wasserstein distance between the projected distributions, with respect to random projection parameters. Leveraging the closed form solution of one-dimensional optimal transport, s-OTDD achieves (near-)linear computational complexity in the number of data points and feature dimensions and is independent of the number of classes. With its geometrically meaningful projection, s-OTDD strongly correlates with the optimal transport dataset distance while being more efficient than existing dataset discrepancy measures. Moreover, it correlates well with the performance gap in transfer learning and classification accuracy in data augmentation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18901v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 16 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Khai Nguyen, Hai Nguyen, Tuan Pham, Nhat Ho</dc:creator>
    </item>
  </channel>
</rss>
