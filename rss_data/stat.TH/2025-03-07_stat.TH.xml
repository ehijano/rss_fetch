<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Improving discrepancy by moving a few points</title>
      <link>https://arxiv.org/abs/2503.04100</link>
      <description>arXiv:2503.04100v1 Announce Type: new 
Abstract: We show how to improve the discrepancy of an iid sample by moving only a few points. Specifically, modifying \( O(m) \) sample points on average reduces the Kolmogorov-Smirnov distance to the population distribution to \(1/m\).</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04100v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gleb Smirnov, Roman Vershynin</dc:creator>
    </item>
    <item>
      <title>Generalizability of Neural Networks Minimizing Empirical Risk Based on Expressive Ability</title>
      <link>https://arxiv.org/abs/2503.04111</link>
      <description>arXiv:2503.04111v1 Announce Type: cross 
Abstract: The primary objective of learning methods is generalization. Classic uniform generalization bounds, which rely on VC-dimension or Rademacher complexity, fail to explain the significant attribute that over-parameterized models in deep learning exhibit nice generalizability. On the other hand, algorithm-dependent generalization bounds, like stability bounds, often rely on strict assumptions. To establish generalizability under less stringent assumptions, this paper investigates the generalizability of neural networks that minimize or approximately minimize empirical risk. We establish a lower bound for population accuracy based on the expressiveness of these networks, which indicates that with an adequate large number of training samples and network sizes, these networks, including over-parameterized ones, can generalize effectively. Additionally, we provide a necessary condition for generalization, demonstrating that, for certain data distributions, the quantity of training data required to ensure generalization exceeds the network size needed to represent the corresponding data distribution. Finally, we provide theoretical insights into several phenomena in deep learning, including robust generalization, importance of over-parameterization, and effect of loss function on generalization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04111v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao, Lijun Zhang</dc:creator>
    </item>
    <item>
      <title>Provable Robust Overfitting Mitigation in Wasserstein Distributionally Robust Optimization</title>
      <link>https://arxiv.org/abs/2503.04315</link>
      <description>arXiv:2503.04315v1 Announce Type: cross 
Abstract: Wasserstein distributionally robust optimization (WDRO) optimizes against worst-case distributional shifts within a specified uncertainty set, leading to enhanced generalization on unseen adversarial examples, compared to standard adversarial training which focuses on pointwise adversarial perturbations. However, WDRO still suffers fundamentally from the robust overfitting problem, as it does not consider statistical error. We address this gap by proposing a novel robust optimization framework under a new uncertainty set for adversarial noise via Wasserstein distance and statistical error via Kullback-Leibler divergence, called the Statistically Robust WDRO. We establish a robust generalization bound for the new optimization framework, implying that out-of-distribution adversarial performance is at least as good as the statistically robust training loss with high probability. Furthermore, we derive conditions under which Stackelberg and Nash equilibria exist between the learner and the adversary, giving an optimal robust model in certain sense. Finally, through extensive experiments, we demonstrate that our method significantly mitigates robust overfitting and enhances robustness within the framework of WDRO.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04315v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>ICLR 2025</arxiv:journal_reference>
      <dc:creator>Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, Xiao-Shan Gao</dc:creator>
    </item>
    <item>
      <title>Learning Causal Response Representations through Direct Effect Analysis</title>
      <link>https://arxiv.org/abs/2503.04358</link>
      <description>arXiv:2503.04358v1 Announce Type: cross 
Abstract: We propose a novel approach for learning causal response representations. Our method aims to extract directions in which a multidimensional outcome is most directly caused by a treatment variable. By bridging conditional independence testing with causal representation learning, we formulate an optimisation problem that maximises the evidence against conditional independence between the treatment and outcome, given a conditioning set. This formulation employs flexible regression models tailored to specific applications, creating a versatile framework. The problem is addressed through a generalised eigenvalue decomposition. We show that, under mild assumptions, the distribution of the largest eigenvalue can be bounded by a known $F$-distribution, enabling testable conditional independence. We also provide theoretical guarantees for the optimality of the learned representation in terms of signal-to-noise ratio and Fisher information maximisation. Finally, we demonstrate the empirical effectiveness of our approach in simulation and real-world experiments. Our results underscore the utility of this framework in uncovering direct causal effects within complex, multivariate settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04358v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Homer Durand, Gherardo Varando, Gustau Camps-Valls</dc:creator>
    </item>
    <item>
      <title>Wasserstein Spatial Depth</title>
      <link>https://arxiv.org/abs/2411.10646</link>
      <description>arXiv:2411.10646v2 Announce Type: replace 
Abstract: Modeling observations as random distributions embedded within Wasserstein spaces is becoming increasingly popular across scientific fields, as it captures the variability and geometric structure of the data more effectively. However, the distinct geometry and unique properties of Wasserstein space pose challenges to the application of conventional statistical tools, which are primarily designed for Euclidean spaces. Consequently, adapting and developing new methodologies for analysis within Wasserstein spaces has become essential. The space of distributions on $\mathbb{R}^d$ with $d&gt;1$ is not linear, and "mimic" the geometry of a Riemannian manifold. In this paper, we extend the concept of statistical depth to distribution-valued data, introducing the notion of Wasserstein spatial depth. This new measure provides a way to rank and order distributions, enabling the development of order-based clustering techniques and inferential tools. We show that Wasserstein spatial depth (WSD) preserves critical properties of conventional statistical depths, notably, ranging within $[0,1]$, transformation invariance, vanishing at infinity, reaching a maximum at the geometric median, and continuity. Additionally, the population WSD has a straightforward plug-in estimator based on sampled empirical distributions. We establish the estimator's consistency and asymptotic normality. Extensive simulation and real-data application showcase the practical efficacy of WSD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10646v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fran\c{c}ois Bachoc, Alberto Gonz\'alez-Sanz, Jean-Michel Loubes, Yisha Yao</dc:creator>
    </item>
    <item>
      <title>You Are the Best Reviewer of Your Own Papers: The Isotonic Mechanism</title>
      <link>https://arxiv.org/abs/2206.08149</link>
      <description>arXiv:2206.08149v2 Announce Type: replace-cross 
Abstract: Machine learning (ML) and artificial intelligence (AI) conferences including NeurIPS and ICML have experienced a significant decline in peer review quality in recent years. To address this growing challenge, we introduce the Isotonic Mechanism, a computationally efficient approach to enhancing the accuracy of noisy review scores by incorporating authors' private assessments of their submissions. Under this mechanism, authors with multiple submissions are required to rank their papers in descending order of perceived quality. Subsequently, the raw review scores are calibrated based on this ranking to produce adjusted scores. We prove that authors are incentivized to truthfully report their rankings because doing so maximizes their expected utility, modeled as an additive convex function over the adjusted scores. Moreover, the adjusted scores are shown to be more accurate than the raw scores, with improvements being particularly significant when the noise level is high and the author has many submissions -- a scenario increasingly prevalent at large-scale ML/AI conferences.
  We further investigate whether submission quality information beyond a simple ranking can be truthfully elicited from authors. We establish that a necessary condition for truthful elicitation is that the mechanism be based on pairwise comparisons of the author's submissions. This result underscores the optimality of the Isotonic Mechanism, as it elicits the most fine-grained truthful information among all mechanisms we consider. We then present several extensions, including a demonstration that the mechanism maintains truthfulness even when authors have only partial rather than complete information about their submission quality. Finally, we discuss future research directions, focusing on the practical implementation of the mechanism and the further development of a theoretical framework inspired by our mechanism.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.08149v2</guid>
      <category>cs.LG</category>
      <category>cs.GT</category>
      <category>econ.TH</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Weijie Su</dc:creator>
    </item>
    <item>
      <title>Some Targets Are Harder to Identify than Others: Quantifying the Target-dependent Membership Leakage</title>
      <link>https://arxiv.org/abs/2402.10065</link>
      <description>arXiv:2402.10065v2 Announce Type: replace-cross 
Abstract: In a Membership Inference (MI) game, an attacker tries to infer whether a target point was included or not in the input of an algorithm. Existing works show that some target points are easier to identify, while others are harder. This paper explains the target-dependent hardness of membership attacks by studying the powers of the optimal attacks in a fixed-target MI game. We characterise the optimal advantage and trade-off functions of attacks against the empirical mean in terms of the Mahalanobis distance between the target point and the data-generating distribution. We further derive the impacts of two privacy defences, i.e. adding Gaussian noise and sub-sampling, and that of target misspecification on optimal attacks. As by-products of our novel analysis of the Likelihood Ratio (LR) test, we provide a new covariance attack which generalises and improves the scalar product attack. Also, we propose a new optimal canary-choosing strategy for auditing privacy in the white-box federated learning setting. Our experiments validate that the Mahalanobis score explains the hardness of fixed-target MI games.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.10065v2</guid>
      <category>cs.LG</category>
      <category>cs.CR</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Achraf Azize, Debabrota Basu</dc:creator>
    </item>
    <item>
      <title>A conversion theorem and minimax optimality for continuum contextual bandits</title>
      <link>https://arxiv.org/abs/2406.05714</link>
      <description>arXiv:2406.05714v4 Announce Type: replace-cross 
Abstract: We study the contextual continuum bandits problem, where the learner sequentially receives a side information vector and has to choose an action in a convex set, minimizing a function associated with the context. The goal is to minimize all the underlying functions for the received contexts, leading to the contextual notion of regret, which is stronger than the standard static regret. Assuming that the objective functions are $\gamma$-H\"older with respect to the contexts, $0&lt;\gamma\le 1,$ we demonstrate that any algorithm achieving a sub-linear static regret can be extended to achieve a sub-linear contextual regret. We prove a static-to-contextual regret conversion theorem that provides an upper bound for the contextual regret of the output algorithm as a function of the static regret of the input algorithm. We further study the implications of this general result for three fundamental cases of dependency of the objective function on the action variable: (a) Lipschitz bandits, (b) convex bandits, (c) strongly convex and smooth bandits. For Lipschitz bandits and $\gamma=1,$ combining our results with the lower bound of Slivkins (2014), we prove that the minimax optimal contextual regret for the noise-free adversarial setting is achieved. Then, we prove that in the presence of noise, the contextual regret rate as a function of the number of queries is the same for convex bandits as it is for strongly convex and smooth bandits. Lastly, we present a minimax lower bound, implying two key facts. First, obtaining a sub-linear contextual regret may be impossible over functions that are not continuous with respect to the context. Second, for convex bandits and strongly convex and smooth bandits, the algorithms that we propose achieve, up to a logarithmic factor, the minimax optimal rate of contextual regret as a function of the number of queries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.05714v4</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arya Akhavan, Karim Lounici, Massimiliano Pontil, Alexandre B. Tsybakov</dc:creator>
    </item>
  </channel>
</rss>
