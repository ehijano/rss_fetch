<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.OC updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.OC</link>
    <description>math.OC updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.OC" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 05 Dec 2024 02:45:04 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Linear Supervision for Nonlinear, High-Dimensional Neural Control and Differential Games</title>
      <link>https://arxiv.org/abs/2412.02033</link>
      <description>arXiv:2412.02033v1 Announce Type: new 
Abstract: As the dimension of a system increases, traditional methods for control and differential games rapidly become intractable, making the design of safe autonomous agents challenging in complex or team settings. Deep-learning approaches avoid discretization and yield numerous successes in robotics and autonomy, but at a higher dimensional limit, accuracy falls as sampling becomes less efficient. We propose using rapidly generated linear solutions to the partial differential equation (PDE) arising in the problem to accelerate and improve learned value functions for guidance in high-dimensional, nonlinear problems. We define two programs that combine supervision of the linear solution with a standard PDE loss. We demonstrate that these programs offer improvements in speed and accuracy in both a 50-D differential game problem and a 10-D quadrotor control problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02033v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>William Sharpless, Zeyuan Feng, Somil Bansal, Sylvia Herbert</dc:creator>
    </item>
    <item>
      <title>Relaxed and Inertial Nonlinear Forward-Backward with Momentum</title>
      <link>https://arxiv.org/abs/2412.02045</link>
      <description>arXiv:2412.02045v1 Announce Type: new 
Abstract: In this article, we study inertial algorithms for numerically solving monotone inclusions involving the sum of a maximally monotone and a cocoercive operator. In particular, we analyze the convergence of inertial and relaxed versions of the nonlinear forward-backward with momentum (NFBM). We propose an inertial version including a relaxation step, and a second version considering a double-inertial step with additional momentum. By applying NFBM to specific monotone inclusions, we derive inertial and relaxed versions of algorithms such as forward-backward, forward-half-reflect-backward (FHRB), Chambolle-Pock, Condat-V\~u, among others, thereby recovering and extending previous results from the literature for solving monotone inclusions involving maximally monotone, cocoercive, monotone and Lipschitz, and linear bounded operators. We also present numerical experiments on image restoration, comparing the proposed inertial and relaxation algorithms. In particular, we compare the inertial FHRB with its non-inertial and momentum versions. Additionally, we compare the numerical convergence for larger step-sizes versus relaxation parameters and introduce a restart strategy that incorporates larger step-sizes and inertial steps to further enhance numerical convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02045v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fernando Rold\'an, Cristian Vega</dc:creator>
    </item>
    <item>
      <title>Dynamic Simulation Method for Low-permeability Reservoirs with Fracturing-flooding Based on a Dual-Porous and Dual-Permeable Media Model</title>
      <link>https://arxiv.org/abs/2412.02073</link>
      <description>arXiv:2412.02073v1 Announce Type: new 
Abstract: The fracturing-flooding technology is a new process for the development of low-permeability oil reservoirs, achieving a series of successful applications in oilfield production. However, existing numerical simulation methods for pressure drive struggle to efficiently and accurately simulate the dynamic changes in reservoir properties during the fracturing-flooding process, particularly the expansion and closure of fractures within the reservoir. This paper introduces a Darcy flow model with dual-porous and dual-permeable characteristics based on seepage mechanics theory, utilizing two sets of rock stress-sensitive parameter tables to describe the physical property changes of the matrix and fractures during the fracturing-flooding process. Different parameters are set for the X and Y directions to characterize the anisotropic features of the reservoir. A numerical simulation method aimed at dynamic analysis of fracturing-flooding is established, along with an automatic history fitting method based on the CMA-ES algorithm to derive rock mechanics parameters that align with actual block conditions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02073v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiang Wang, Wenjie Yu, Yixin Xie, Yanfeng He, Hui Xu, Xianxiang Chu, Changfu Li</dc:creator>
    </item>
    <item>
      <title>A simple and practical adaptive trust-region method</title>
      <link>https://arxiv.org/abs/2412.02079</link>
      <description>arXiv:2412.02079v2 Announce Type: new 
Abstract: We present an adaptive trust-region method for unconstrained optimization that allows inexact solutions to the trust-region subproblems. Our method is a simple variant of the classical trust-region method of Sorensen [1]. The method achieves the best possible convergence bound up to an additive log factor, for finding an $\epsilon$-approximate stationary point, i.e., $O( \Delta_f L^{1/2} \epsilon^{-3/2}) + \tilde{O}(1)$ iterations where $L$ is the Lipschitz constant of the Hessian, $\Delta_f$ is the optimality gap, and $\epsilon$ is the termination tolerance for the gradient norm. This improves over existing trust-region methods whose worst-case bound is at least a factor of $L$ worse. We compare our performance with state-of-the-art trust-region (TRU) and cubic regularization (ARC) methods from the GALAHAD library on the CUTEst benchmark set on problems with more than 100 variables. We use fewer function, gradient, and Hessian evaluations than these methods. For instance, our algorithm's median number of gradient evaluations is $23$ compared to $36$ for TRU and $29$ for ARC.
  Compared to the conference version of this paper [2], our revised method includes practical enhancements and a refined subproblems termination criterion. These modifications dramatically improved performance, including an order of magnitude reduction in the shifted geometric mean of wall-clock times.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02079v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Fadi Hamad, Oliver Hinder</dc:creator>
    </item>
    <item>
      <title>Improved Complexity for Smooth Nonconvex Optimization: A Two-Level Online Learning Approach with Quasi-Newton Methods</title>
      <link>https://arxiv.org/abs/2412.02175</link>
      <description>arXiv:2412.02175v1 Announce Type: new 
Abstract: We study the problem of finding an $\epsilon$-first-order stationary point (FOSP) of a smooth function, given access only to gradient information. The best-known gradient query complexity for this task, assuming both the gradient and Hessian of the objective function are Lipschitz continuous, is ${O}(\epsilon^{-7/4})$. In this work, we propose a method with a gradient complexity of ${O}(d^{1/4}\epsilon^{-13/8})$, where $d$ is the problem dimension, leading to an improved complexity when $d = {O}(\epsilon^{-1/2})$. To achieve this result, we design an optimization algorithm that, underneath, involves solving two online learning problems. Specifically, we first reformulate the task of finding a stationary point for a nonconvex problem as minimizing the regret in an online convex optimization problem, where the loss is determined by the gradient of the objective function. Then, we introduce a novel optimistic quasi-Newton method to solve this online learning problem, with the Hessian approximation update itself framed as an online learning problem in the space of matrices. Beyond improving the complexity bound for achieving an $\epsilon$-FOSP using a gradient oracle, our result provides the first guarantee suggesting that quasi-Newton methods can potentially outperform gradient descent-type methods in nonconvex settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02175v1</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ruichen Jiang, Aryan Mokhtari, Francisco Patitucci</dc:creator>
    </item>
    <item>
      <title>Exponential Stabilization of Linear Systems using Nearest-Action Control with Countable Input Set</title>
      <link>https://arxiv.org/abs/2412.02238</link>
      <description>arXiv:2412.02238v1 Announce Type: new 
Abstract: This paper studies stabilization of linear time-invariant (LTI) systems when control actions can only be realized in finitely many directions where it is possible to actuate uniformly or logarithmically extended positive scaling factors in each direction. Furthermore, a nearest-action selection approach is used to map the continuous measurements to a realizable action where we show that the approach satisfies a weak sector condition for multiple-input multiple-output (MIMO) systems. Using the notion of input-to-state stability, under some assumptions imposed on the transfer function of the system, we show that the closed-loop system converges to the target ball exponentially fast. Moreover, when logarithmic extension for the scaling factors is realizable, the closed-loop system is able to achieve asymptotic stability instead of only practical stability. Finally, we present an example of the application that confirms our analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02238v1</guid>
      <category>math.OC</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Muhammad Zaki Almuzakki, Bayu Jayawardhana, Aneel Tanwani, Antonis I. Vakis</dc:creator>
    </item>
    <item>
      <title>Stochastic halfspace approximation method for convex optimization with nonsmooth functional constraints</title>
      <link>https://arxiv.org/abs/2412.02338</link>
      <description>arXiv:2412.02338v1 Announce Type: new 
Abstract: In this work, we consider convex optimization problems with smooth objective function and nonsmooth functional constraints. We propose a new stochastic gradient algorithm, called Stochastic Halfspace Approximation Method (SHAM), to solve this problem, where at each iteration we first take a gradient step for the objective function and then we perform a projection step onto one halfspace approximation of a randomly chosen constraint. We propose various strategies to create this stochastic halfspace approximation and we provide a unified convergence analysis that yields new convergence rates for SHAM algorithm in both optimality and feasibility criteria evaluated at some average point. In particular, we derive convergence rates of order $\mathcal{O} (1/\sqrt{k})$, when the objective function is only convex, and $\mathcal{O} (1/k)$ when the objective function is strongly convex. The efficiency of SHAM is illustrated through detailed numerical simulations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02338v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>IEEE Transactions on Automatic Control, 70(1), 2025</arxiv:journal_reference>
      <dc:creator>Nitesh Kumar Singh, Ion Necoara</dc:creator>
    </item>
    <item>
      <title>Decentralized projected Riemannian stochastic recursive momentum method for smooth optimization on compact submanifolds</title>
      <link>https://arxiv.org/abs/2412.02382</link>
      <description>arXiv:2412.02382v1 Announce Type: new 
Abstract: This work addresses the problem of decentralized optimization on a compact submanifold within a communication network comprising \(n\) nodes. Each node is associated with a smooth, non-convex local cost function, and the collective objective is to minimize the sum of these local cost functions. We focus on an online scenario where local data arrives continuously in a streaming fashion, eliminating the necessity for complete data storage. To tackle this problem, we introduce a novel algorithm, the Decentralized Projected Riemannian Stochastic Recursive Momentum (DPRSRM) method. Our approach leverages hybrid local stochastic gradient estimators and utilizes network communication to maintain a consensus on the global gradient. Notably, DPRSRM attains an oracle complexity of \(\mathcal{O}(\epsilon^{-\frac{3}{2}})\), which surpasses the performance of existing methods with complexities no better than \(\mathcal{O}(\epsilon^{-2})\). Each node in the network requires only \(\mathcal{O}(1)\) gradient evaluations per iteration, avoiding the need for large batch gradient calculations or restarting procedures. Finally, we validate the superior performance of our proposed algorithm through numerical experiments, including applications in principal component analysis and low-rank matrix completion, demonstrating its advantages over state-of-the-art approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02382v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangkang Deng, Jiang Hu</dc:creator>
    </item>
    <item>
      <title>Soft-constrained output feedback guaranteed cost equilibria in infinite-horizon uncertain linear-quadratic differential games</title>
      <link>https://arxiv.org/abs/2412.02385</link>
      <description>arXiv:2412.02385v1 Announce Type: new 
Abstract: In this paper, we study infinite-horizon linear-quadratic uncertain differential games with an output feedback information structure. We assume linear time-invariant nominal dynamics influenced by deterministic external disturbances, and players' risk preferences are expressed by a soft-constrained quadratic cost criterion over an infinite horizon. We demonstrate that the conditions available in the literature for the existence of a soft-constrained output feedback Nash equilibrium (SCONE) are too stringent to satisfy, even in low-dimensional games. To address this issue, using ideas from suboptimal control, we introduce the concept of a soft-constrained output feedback guaranteed cost equilibrium (SCOGCE). At an SCOGCE, the players' worst-case costs are upper-bounded by a specified cost profile while maintaining an equilibrium property. We show that SCOGCE strategies form a larger class of equilibrium strategies; that is, whenever an SCONE exists, it is also an SCOGCE. We demonstrate that sufficient conditions for the existence of SCOGCE are related to the solvability of a set of coupled bi-linear matrix inequalities. Using semi-definite programming relaxations, we provide linear matrix inequality-based iterative algorithms for the synthesis of SCOGCE strategies. Finally, we illustrate the performance of SCOGCE controllers with numerical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02385v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aniruddha Roy, Puduru Viswanadha Reddy</dc:creator>
    </item>
    <item>
      <title>Optimisation of Categorical Choices in Exploration Mission Concepts of Operations Using Column Generation Method</title>
      <link>https://arxiv.org/abs/2412.02521</link>
      <description>arXiv:2412.02521v1 Announce Type: new 
Abstract: Space missions, particularly complex, large-scale exploration campaigns, can often involve many discrete decisions or events in their concepts of operations. Whilst a variety of methods exist for the optimisation of continuous variables in mission design, the inherent presence of discrete events in mission ConOps disrupts the possibility of using methods that are dependent on having well-defined, continuous mathematical expressions to define the systems. Typically, mission architects will circumvent this problem by solving the system optimisation for every permutation of the categorical decisions if practical, or use metaheuristic solvers if not. However, this can be prohibitively expensive in terms of computation time. Alternatively, categorical decisions in optimisation problems can be expressed using binary variables. If implemented naively, commercially available MILP solvers are still slow to solve such a problem. Problems of this class can be solved more efficiently using column generation methods. Here, restricted problems are created by removing significant numbers of variables. The restricted problem is solved, and the unused variables are priced to test which, if any, could improve the objective of the restricted problem if they were to be added. Column generation methods are problem-specific, and so there is no guaranteed solution to these categorical problems. As such, the following paper proposes guidelines for defining restricted problems representing space exploration mission concepts of operations. The column generation process is described and then applied to two case studies: a ConOps for a crewed Mars mission, in which the design, assembly, and staging of the trans-Martian spacecraft is modelled using discrete decisions; and the payload delivery scheduling of translunar logistics in the context of an extended Artemis surface exploration campaign.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02521v1</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicholas Gollins, Masafumi Isaji, Koki Ho</dc:creator>
    </item>
    <item>
      <title>Kernel-Free Universum Quadratic Surface Twin Support Vector Machines for Imbalanced Data</title>
      <link>https://arxiv.org/abs/2412.01936</link>
      <description>arXiv:2412.01936v1 Announce Type: cross 
Abstract: Binary classification tasks with imbalanced classes pose significant challenges in machine learning. Traditional classifiers often struggle to accurately capture the characteristics of the minority class, resulting in biased models with subpar predictive performance. In this paper, we introduce a novel approach to tackle this issue by leveraging Universum points to support the minority class within quadratic twin support vector machine models. Unlike traditional classifiers, our models utilize quadratic surfaces instead of hyperplanes for binary classification, providing greater flexibility in modeling complex decision boundaries. By incorporating Universum points, our approach enhances classification accuracy and generalization performance on imbalanced datasets. We generated four artificial datasets to demonstrate the flexibility of the proposed methods. Additionally, we validated the effectiveness of our approach through empirical evaluations on benchmark datasets, showing superior performance compared to conventional classifiers and existing methods for imbalanced classification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01936v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hossein Moosaei, Milan Hlad\'ik, Ahmad Mousavi, Zheming Gao, Haojie Fu</dc:creator>
    </item>
    <item>
      <title>Generalized EXTRA stochastic gradient Langevin dynamics</title>
      <link>https://arxiv.org/abs/2412.01993</link>
      <description>arXiv:2412.01993v1 Announce Type: cross 
Abstract: Langevin algorithms are popular Markov Chain Monte Carlo methods for Bayesian learning, particularly when the aim is to sample from the posterior distribution of a parametric model, given the input data and the prior distribution over the model parameters. Their stochastic versions such as stochastic gradient Langevin dynamics (SGLD) allow iterative learning based on randomly sampled mini-batches of large datasets and are scalable to large datasets. However, when data is decentralized across a network of agents subject to communication and privacy constraints, standard SGLD algorithms cannot be applied. Instead, we employ decentralized SGLD (DE-SGLD) algorithms, where Bayesian learning is performed collaboratively by a network of agents without sharing individual data. Nonetheless, existing DE-SGLD algorithms induce a bias at every agent that can negatively impact performance; this bias persists even when using full batches and is attributable to network effects. Motivated by the EXTRA algorithm and its generalizations for decentralized optimization, we propose the generalized EXTRA stochastic gradient Langevin dynamics, which eliminates this bias in the full-batch setting. Moreover, we show that, in the mini-batch setting, our algorithm provides performance bounds that significantly improve upon those of standard DE-SGLD algorithms in the literature. Our numerical results also demonstrate the efficiency of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01993v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mert Gurbuzbalaban, Mohammad Rafiqul Islam, Xiaoyu Wang, Lingjiong Zhu</dc:creator>
    </item>
    <item>
      <title>Efficient parallel inversion of ParaOpt preconditioners</title>
      <link>https://arxiv.org/abs/2412.02425</link>
      <description>arXiv:2412.02425v1 Announce Type: cross 
Abstract: Recently, the ParaOpt algorithm was proposed as an extension of the time-parallel Parareal method to optimal control. ParaOpt uses quasi-Newton steps that each require solving a system of matching conditions iteratively. The state-of-the-art parallel preconditioner for linear problems leads to a set of independent smaller systems that are currently hard to solve. We generalize the preconditioner to the nonlinear case and propose a new, fast inversion method for these smaller systems, avoiding disadvantages of the current options with adjusted boundary conditions in the subproblems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02425v1</guid>
      <category>math.NA</category>
      <category>cs.NA</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Corentin Bonte, Arne Bouillon, Giovanni Samaey, Karl Meerbergen</dc:creator>
    </item>
    <item>
      <title>Backtracking New Q-Newton's method for finding roots of meromorphic functions in 1 complex variable: Global convergence, and local stable/unstable curves</title>
      <link>https://arxiv.org/abs/2412.02476</link>
      <description>arXiv:2412.02476v1 Announce Type: cross 
Abstract: In this paper, we research more in depth properties of Backtracking New Q-Newton's method (recently designed by the third author), when used to find roots of meromorphic functions.
  If $f=P/Q$, where $P$ and $Q$ are polynomials in 1 complex variable z with $deg (P)&gt;deg (Q)$, we show the existence of an exceptional set $\mathcal{E}\subset\mathbf{C}$, which is contained in a countable union of real analytic curves in $\mathbf{R}^2=\mathbf{C}$, so that the following statements A and B hold. Here, $\{z_n\}$ is the sequence constructed by BNQN with an initial point $z_0$, not a pole of $f$.
  A) If $z_0\in\mathbb{C}\backslash\mathcal{E}$, then $\{z_n\}$ converges to a root of $f$.
  B) If $z_0\in \mathcal{E}$, then $\{z_n\}$ converges to a critical point - but not a root - of $f$.
  Experiments seem to indicate that in general, even when $f$ is a polynomial, the set $\mathcal{E}$ is not contained in a finite union of real analytic curves. We provide further results relevant to whether locally $\mathcal{E}$ is contained in a finite number of real analytic curves. A similar result holds for general meromorphic functions. Moreover, unlike previous work, here we do not require that the parameters of BNQN are random, or that the meromorphic function $f$ is generic.
  Based on the theoretical results, we explain (both rigorously and heuristically) of what observed in experiments with BNQN, in previous works by the authors. The dynamics of BNQN seems also to have some similarities (and differences) to the classical Leau-Fatou's flowers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02476v1</guid>
      <category>math.DS</category>
      <category>cs.NA</category>
      <category>math.CV</category>
      <category>math.NA</category>
      <category>math.NT</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>John Erik Forn{\ae}ss, Mi Hu, Tuyen Trung Truong</dc:creator>
    </item>
    <item>
      <title>The Asymptotic Behavior of Attention in Transformers</title>
      <link>https://arxiv.org/abs/2412.02682</link>
      <description>arXiv:2412.02682v1 Announce Type: cross 
Abstract: A key component of transformers is the attention mechanism orchestrating how each token influences the propagation of every other token through a transformer. In this paper we provide a rigorous, mathematical analysis of the asymptotic properties of attention in transformers. Although we present several results based on different assumptions, all of them point to the same conclusion, all tokens asymptotically converge to each other, a phenomenon that has been empirically reported in the literature. Our findings are carefully compared with existing theoretical results and illustrated by simulations and experimental studies using the GPT-2 model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02682v1</guid>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.DS</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>\'Alvaro Rodr\'iguez Abella, Jo\~ao Pedro Silvestre, Paulo Tabuada</dc:creator>
    </item>
    <item>
      <title>Adaptive Partitioning Strategy for High-Dimensional Discrete Simulation-based Optimization Problems</title>
      <link>https://arxiv.org/abs/2104.14119</link>
      <description>arXiv:2104.14119v2 Announce Type: replace 
Abstract: In this paper, we introduce a technique to enhance the computational efficiency of solution algorithms for high-dimensional discrete simulation-based optimization problems. The technique is based on innovative adaptive partitioning strategies that partition the feasible region using solutions that has already been simulated as well as prior knowledge of the problem of interesting. We integrate the proposed strategies with the Empirical Stochastic Branch-and-Bound framework proposed by Xu and Nelson (2013). This combination leads to a general-purpose discrete simulation-based optimization algorithm that is both globally convergent and has good small sample (finite-time) performance. The proposed general-purpose discrete simulation-based optimization algorithm is validated on a synthetic discrete simulation-based optimization problem and is then used to address a real-world car-sharing fleet assignment problem. Experiment results show that the proposed strategy can increase the algorithm efficiency significantly.</description>
      <guid isPermaLink="false">oai:arXiv.org:2104.14119v2</guid>
      <category>math.OC</category>
      <category>cs.CE</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jing Lu, Tianli Zhou, Carolina Osorio</dc:creator>
    </item>
    <item>
      <title>A Single-Loop Gradient Descent and Perturbed Ascent Algorithm for Nonconvex Functional Constrained Optimization</title>
      <link>https://arxiv.org/abs/2207.05650</link>
      <description>arXiv:2207.05650v2 Announce Type: replace 
Abstract: Nonconvex constrained optimization problems can be used to model a number of machine learning problems, such as multi-class Neyman-Pearson classification and constrained Markov decision processes. However, such kinds of problems are challenging because both the objective and constraints are possibly nonconvex, so it is difficult to balance the reduction of the loss value and reduction of constraint violation. Although there are a few methods that solve this class of problems, all of them are double-loop or triple-loop algorithms, and they require oracles to solve some subproblems up to certain accuracy by tuning multiple hyperparameters at each iteration. In this paper, we propose a novel gradient descent and perturbed ascent (GDPA) algorithm to solve a class of smooth nonconvex inequality constrained problems. The GDPA is a primal-dual algorithm, which only exploits the first-order information of both the objective and constraint functions to update the primal and dual variables in an alternating way. The key feature of the proposed algorithm is that it is a single-loop algorithm, where only two step-sizes need to be tuned. We show that under a mild regularity condition GDPA is able to find Karush-Kuhn-Tucker (KKT) points of nonconvex functional constrained problems with convergence rate guarantees. To the best of our knowledge, it is the first single-loop algorithm that can solve the general nonconvex smooth problems with nonconvex inequality constraints. Numerical results also showcase the superiority of GDPA compared with the best-known algorithms (in terms of both stationarity measure and feasibility of the obtained solutions).</description>
      <guid isPermaLink="false">oai:arXiv.org:2207.05650v2</guid>
      <category>math.OC</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Songtao Lu</dc:creator>
    </item>
    <item>
      <title>Normalizing flows as approximations of optimal transport maps via linear-control neural ODEs</title>
      <link>https://arxiv.org/abs/2311.01404</link>
      <description>arXiv:2311.01404v4 Announce Type: replace 
Abstract: In this paper, we consider the problem of recovering the $W_2$-optimal transport map T between absolutely continuous measures $\mu,\nu\in\mathcal{P}(\mathbb{R}^n)$ as the flow of a linear-control neural ODE, where the control depends only on the time variable and takes values in a finite-dimensional space. We first show that, under suitable assumptions on $\mu,\nu$ and on the controlled vector fields governing the neural ODE, the optimal transport map is contained in the $C^0_c$-closure of the flows generated by the system. Then, we tackle the problem under the assumption that only discrete approximations of $\mu_N,\nu_N$ of the original measures $\mu,\nu$ are available: we formulate approximated optimal control problems, and we show that their solutions give flows that approximate the original optimal transport map $T$. In the framework of generative models, the approximating flow constructed here can be seen as a `Normalizing Flow', which usually refers to the task of providing invertible transport maps between probability measures by means of deep neural networks. We propose an iterative numerical scheme based on the Pontryagin Maximum Principle for the resolution of the optimal control problem, resulting in a method for the practical computation of the approximated optimal transport map, and we test it on a two-dimensional example.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01404v4</guid>
      <category>math.OC</category>
      <category>cs.LG</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alessandro Scagliotti, Sara Farinelli</dc:creator>
    </item>
    <item>
      <title>Iterative approximations of periodic trajectories for nonlinear systems with discontinuous inputs</title>
      <link>https://arxiv.org/abs/2401.00310</link>
      <description>arXiv:2401.00310v2 Announce Type: replace 
Abstract: Nonlinear control-affine systems described by ordinary differential equations with bounded measurable input functions are considered. The problem of the existence of periodic trajectories for these systems is formulated in the sense of Carath\'eodory solutions. It is shown that, under the dominant linearization assumption, the periodic boundary value problem admits a unique solution for any admissible control. This solution can be obtained as the limit of the proposed simple iterative scheme and a Newton-type method. Under additional technical assumptions, sufficient contraction conditions of the corresponding generating operators are derived analytically. The proposed iterative approach is applied for the computation of periodic solutions of a realistic chemical reaction model with discontinuous control inputs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.00310v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alexander Zuyev, Peter Benner</dc:creator>
    </item>
    <item>
      <title>Positivstellens\"atze and Moment problems with Universal Quantifiers</title>
      <link>https://arxiv.org/abs/2401.12359</link>
      <description>arXiv:2401.12359v2 Announce Type: replace 
Abstract: This paper studies Positivstellens\"atze and moment problems for sets $K$ that are given by universal quantifiers. Let $Q$ be a closed set and let $g = (g_1,...,g_s)$ be a tuple of polynomials in two vector variables $x$ and $y$. Then $K$ is described as the set of all points $x$ such that each $g_j(x, y) \ge 0$ for all $y \in Q$. Fix a finite nonnegative Borel measure $\nu$ with $supp(\nu) = Q$, and assume it satisfies the multivariate Carleman condition. The first main result of the paper is a Positivstellensatz with universal quantifiers: if a polynomial $f(x)$ is positive on $K$, then it belongs to the quadratic module $QM(g,\nu)$ associated to $(g,\nu)$, under the archimedeanness assumption on $QM(g,\nu)$. Here, $QM(g,\nu)$ denotes the quadratic module of polynomials in $x$ that can be represented as \[\tau_0(x) + \int \tau_1(x,y)g_1(x, y)\, d\nu(y) + \cdots + \int \tau_s(x,y) g_s(x, y)\, d\nu(y), \] where each $\tau_j$ is a sum of squares polynomial.
  Second, necessary and sufficient conditions for a full (or truncated) multisequence to admit a representing measure supported in $K$ are given. In particular, the classical flat extension theorem of Curto and Fialkow is generalized to truncated moment problems on such a set $K$. Finally, applications of these results for solving semi-infinite optimization problems are presented.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.12359v2</guid>
      <category>math.OC</category>
      <category>math.FA</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaomeng Hu, Igor Klep, Jiawang Nie</dc:creator>
    </item>
    <item>
      <title>The Riemannian Convex Bundle Method</title>
      <link>https://arxiv.org/abs/2402.13670</link>
      <description>arXiv:2402.13670v2 Announce Type: replace 
Abstract: We introduce the convex bundle method to solve convex, non-smooth optimization problems on Riemannian manifolds of bounded sectional curvature. Each step of our method is based on a model that involves the convex hull of previously collected subgradients, parallelly transported into the current serious iterate. This approach generalizes the dual form of classical bundle subproblems in Euclidean space. We prove that, under mild conditions, the convex bundle method converges to a minimizer. Several numerical examples implemented using Manopt.jl illustrate the performance of the proposed method and compare it to the subgradient method, the cyclic proximal point algorithm, as well as the proximal bundle method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.13670v2</guid>
      <category>math.OC</category>
      <category>cs.NA</category>
      <category>math.DG</category>
      <category>math.NA</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ronny Bergmann, Roland Herzog, Hajg Jasa</dc:creator>
    </item>
    <item>
      <title>CBX: Python and Julia packages for consensus-based interacting particle methods</title>
      <link>https://arxiv.org/abs/2403.14470</link>
      <description>arXiv:2403.14470v3 Announce Type: replace 
Abstract: We introduce CBXPy and ConsensusBasedX.jl, Python and Julia implementations of consensus-based interacting particle systems (CBX), which generalise consensus-based optimization methods (CBO) for global, derivative-free optimisation. The raison d'\^etre of our libraries is twofold: on the one hand, to offer high-performance implementations of CBX methods that the community can use directly, while on the other, providing a general interface that can accommodate and be extended to further variations of the CBX family. Python and Julia were selected as the leading high-level languages in terms of usage and performance, as well as their popularity among the scientific computing community. Both libraries have been developed with a common ethos, ensuring a similar API and core functionality, while leveraging the strengths of each language and writing idiomatic code.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14470v3</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.21105/joss.06611</arxiv:DOI>
      <arxiv:journal_reference>Journal of Open Source Software (2024) 9(98)</arxiv:journal_reference>
      <dc:creator>Rafael Bailo, Alethea Barbaro, Susana N. Gomes, Konstantin Riedl, Tim Roith, Claudia Totzeck, Urbain Vaes</dc:creator>
    </item>
    <item>
      <title>Well-conditioned Primal-Dual Interior-point Method for Accurate Low-rank Semidefinite Programming</title>
      <link>https://arxiv.org/abs/2407.14013</link>
      <description>arXiv:2407.14013v2 Announce Type: replace 
Abstract: We describe how the low-rank structure in an SDP can be exploited to reduce the per-iteration cost of a convex primal-dual interior-point method down to $O(n^{3})$ time and $O(n^{2})$ memory, even at very high accuracies. A traditional difficulty is the dense Newton subproblem at each iteration, which becomes progressively ill-conditioned as progress is made towards the solution. Preconditioners have been proposed to improve conditioning, but these can be expensive to set up, and fundamentally become ineffective at high accuracies, as the preconditioner itself becomes increasingly ill-conditioned. Instead, we present a well-conditioned reformulation of the Newton subproblem that is cheap to set up, and whose condition number is guaranteed to remain bounded over all iterations of the interior-point method. In theory, applying an inner iterative method to the reformulation reduces the per-iteration cost of the outer interior-point method to $O(n^{3})$ time and $O(n^{2})$ memory. We also present a well-conditioned preconditioner that theoretically increases the outer per-iteration cost to $O(n^{3}r^{3})$ time and $O(n^{2}r^{2})$ memory, where $r$ is an upper-bound on the solution rank, but in practice greatly improves the convergence of the inner iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14013v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hong-Ming Chiu, Richard Y. Zhang</dc:creator>
    </item>
    <item>
      <title>Perturbed Fenchel Duality and Primal-Dual Convergence of First-Order Methods</title>
      <link>https://arxiv.org/abs/2411.09503</link>
      <description>arXiv:2411.09503v2 Announce Type: replace 
Abstract: It has been shown that many first-order methods satisfy the perturbed Fenchel duality inequality, which yields a unified derivation of convergence. More first-order methods are discussed in this paper, e.g., dual averaging and bundle method. We show primal-dual convergence of them on convex optimization by proving the perturbed Fenchel duality property. We also propose a single-cut bundle method for saddle problem, and prove its convergence in a similar manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.09503v2</guid>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiantian Zhao</dc:creator>
    </item>
    <item>
      <title>Refuting spectral compatibility of quantum marginals</title>
      <link>https://arxiv.org/abs/2211.06349</link>
      <description>arXiv:2211.06349v3 Announce Type: replace-cross 
Abstract: The spectral variant of the quantum marginal problem asks: Given prescribed spectra for a set of overlapping quantum marginals, does there exist a compatible joint state? The main idea of this work is a symmetry-reduced semidefinite programming hierarchy that detects when when no such joint state exists. The hierarchy is complete, in the sense that it detects every incompatible set of spectra. The refutations it provides are dimension-free, certifying incompatibility in all local dimensions. The hierarchy also applies to the sums of Hermitian matrices problem, the compatibility of local unitary invariants, for certifying vanishing Kronecker coefficients, and to optimize over equivariant state polynomials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.06349v3</guid>
      <category>quant-ph</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Felix Huber, Nikolai Wyderka</dc:creator>
    </item>
    <item>
      <title>Robustness of quantum algorithms against coherent control errors</title>
      <link>https://arxiv.org/abs/2303.00618</link>
      <description>arXiv:2303.00618v3 Announce Type: replace-cross 
Abstract: Coherent control errors, for which ideal Hamiltonians are perturbed by unknown multiplicative noise terms, are a major obstacle for reliable quantum computing. In this paper, we present a framework for analyzing the robustness of quantum algorithms against coherent control errors using Lipschitz bounds. We derive worst-case fidelity bounds which show that the resilience against coherent control errors is mainly influenced by the norms of the Hamiltonians generating the individual gates. These bounds are explicitly computable even for large circuits, and they can be used to guarantee fault-tolerance via threshold theorems. Moreover, we apply our theoretical framework to derive a novel guideline for robust quantum algorithm design and transpilation, which amounts to reducing the norms of the Hamiltonians. Using the $3$-qubit Quantum Fourier Transform as an example application, we demonstrate that this guideline targets robustness more effectively than existing ones based on circuit depth or gate count. Furthermore, we apply our framework to study the effect of parameter regularization in variational quantum algorithms. The practicality of the theoretical results is demonstrated via implementations in simulation and on a quantum computer.</description>
      <guid isPermaLink="false">oai:arXiv.org:2303.00618v3</guid>
      <category>quant-ph</category>
      <category>cs.SY</category>
      <category>eess.SY</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevA.109.012417</arxiv:DOI>
      <arxiv:journal_reference>Physical Review A, vol. 109, p. 012417, 2024</arxiv:journal_reference>
      <dc:creator>Julian Berberich, Daniel Fink, Christian Holm</dc:creator>
    </item>
    <item>
      <title>Quantum computing through the lens of control: A tutorial introduction</title>
      <link>https://arxiv.org/abs/2310.12571</link>
      <description>arXiv:2310.12571v2 Announce Type: replace-cross 
Abstract: Quantum computing is a fascinating interdisciplinary research field that promises to revolutionize computing by efficiently solving previously intractable problems. Recent years have seen tremendous progress on both the experimental realization of quantum computing devices as well as the development and implementation of quantum algorithms. Yet, realizing computational advantages of quantum computers in practice remains a widely open problem due to numerous fundamental challenges. Interestingly, many of these challenges are connected to performance, robustness, scalability, optimization, or feedback, all of which are central concepts in control theory. This paper provides a tutorial introduction to quantum computing from the perspective of control theory. We introduce the mathematical framework of quantum algorithms ranging from basic elements including quantum bits and quantum gates to more advanced concepts such as variational quantum algorithms and quantum errors. The tutorial only requires basic knowledge of linear algebra and, in particular, no prior exposure to quantum physics. Our main goal is to equip readers with the mathematical basics required to understand and possibly solve (control-related) problems in quantum computing. In particular, beyond the tutorial introduction, we provide a list of research challenges in the field of quantum computing and discuss their connections to control.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.12571v2</guid>
      <category>eess.SY</category>
      <category>cs.SY</category>
      <category>math.OC</category>
      <category>quant-ph</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/MCS.2024.3466448</arxiv:DOI>
      <arxiv:journal_reference>IEEE Control Systems, vol. 44, no. 6, pp. 24-49, 2024</arxiv:journal_reference>
      <dc:creator>Julian Berberich, Daniel Fink</dc:creator>
    </item>
    <item>
      <title>Rigged Dynamic Mode Decomposition: Data-Driven Generalized Eigenfunction Decompositions for Koopman Operators</title>
      <link>https://arxiv.org/abs/2405.00782</link>
      <description>arXiv:2405.00782v2 Announce Type: replace-cross 
Abstract: We introduce the Rigged Dynamic Mode Decomposition (Rigged DMD) algorithm, which computes generalized eigenfunction decompositions of Koopman operators. By considering the evolution of observables, Koopman operators transform complex nonlinear dynamics into a linear framework suitable for spectral analysis. While powerful, traditional Dynamic Mode Decomposition (DMD) techniques often struggle with continuous spectra. Rigged DMD addresses these challenges with a data-driven methodology that approximates the Koopman operator's resolvent and its generalized eigenfunctions using snapshot data from the system's evolution. At its core, Rigged DMD builds wave-packet approximations for generalized Koopman eigenfunctions and modes by integrating Measure-Preserving Extended Dynamic Mode Decomposition with high-order kernels for smoothing. This provides a robust decomposition encompassing both discrete and continuous spectral elements. We derive explicit high-order convergence theorems for generalized eigenfunctions and spectral measures. Additionally, we propose a novel framework for constructing rigged Hilbert spaces using time-delay embedding, significantly extending the algorithm's applicability (Rigged DMD can be used with any rigging). We provide examples, including systems with a Lebesgue spectrum, integrable Hamiltonian systems, the Lorenz system, and a high-Reynolds number lid-driven flow in a two-dimensional square cavity, demonstrating Rigged DMD's convergence, efficiency, and versatility. This work paves the way for future research and applications of decompositions with continuous spectra.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.00782v2</guid>
      <category>math.DS</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.OC</category>
      <category>math.SP</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew J. Colbrook, Catherine Drysdale, Andrew Horning</dc:creator>
    </item>
    <item>
      <title>Multi-objective Deep Learning: Taxonomy and Survey of the State of the Art</title>
      <link>https://arxiv.org/abs/2412.01566</link>
      <description>arXiv:2412.01566v2 Announce Type: replace-cross 
Abstract: Simultaneously considering multiple objectives in machine learning has been a popular approach for several decades, with various benefits for multi-task learning, the consideration of secondary goals such as sparsity, or multicriteria hyperparameter tuning. However - as multi-objective optimization is significantly more costly than single-objective optimization - the recent focus on deep learning architectures poses considerable additional challenges due to the very large number of parameters, strong nonlinearities and stochasticity. This survey covers recent advancements in the area of multi-objective deep learning. We introduce a taxonomy of existing methods - based on the type of training algorithm as well as the decision maker's needs - before listing recent advancements, and also successful applications. All three main learning paradigms supervised learning, unsupervised learning and reinforcement learning are covered, and we also address the recently very popular area of generative modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01566v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sebastian Peitz, Sedjro Salomon Hotegni</dc:creator>
    </item>
    <item>
      <title>FairML: A Julia Package for Fair Classification</title>
      <link>https://arxiv.org/abs/2412.01585</link>
      <description>arXiv:2412.01585v2 Announce Type: replace-cross 
Abstract: In this paper, we propose FairML.jl, a Julia package providing a framework for fair classification in machine learning. In this framework, the fair learning process is divided into three stages. Each stage aims to reduce unfairness, such as disparate impact and disparate mistreatment, in the final prediction. For the preprocessing stage, we present a resampling method that addresses unfairness coming from data imbalances. The in-processing phase consist of a classification method. This can be either one coming from the MLJ.jl package, or a user defined one. For this phase, we incorporate fair ML methods that can handle unfairness to a certain degree through their optimization process. In the post-processing, we discuss the choice of the cut-off value for fair prediction. With simulations, we show the performance of the single phases and their combinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.01585v2</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <pubDate>Wed, 04 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan Pablo Burgard, Jo\~ao Vitor Pamplona</dc:creator>
    </item>
  </channel>
</rss>
