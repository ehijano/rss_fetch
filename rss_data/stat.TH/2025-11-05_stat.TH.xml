<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.TH updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.TH</link>
    <description>stat.TH updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.TH" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 06 Nov 2025 02:48:41 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Extrapolation Problem for Continuous Time Periodically Correlated Isotropic Random Fields</title>
      <link>https://arxiv.org/abs/2511.01933</link>
      <description>arXiv:2511.01933v1 Announce Type: new 
Abstract: The problem of optimal linear estimation of functionals depending on the unknown values of a random field $\zeta(t,x)$, which is mean-square continuous periodically correlated with respect to time argument $t\in\mathbb R$ and isotropic on the unit sphere ${S_n}$ with respect to spatial argument $x\in{S_n}$. Estimates are based on observations of the field $\zeta(t,x)+\theta(t,x)$ at points $(t,x):t&lt;0,x\in S_{n}$, where $\theta(t,x)$ is an uncorrelated with $\zeta(t,x)$ random field, which is mean-square continuous periodically correlated with respect to time argument $t\in\mathbb R$ and isotropic on the sphere ${S_n}$ with respect to spatial argument $x\in{S_n}$. Formulas for calculating the mean square errors and the spectral characteristics of the optimal linear estimate of functionals are derived in the case of spectral certainty where the spectral densities of the fields are exactly known. Formulas that determine the least favourable spectral densities and the minimax (robust) spectral characteristics are proposed in the case where the spectral densities are not exactly known while a class of admissible spectral densities is given.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01933v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iryna Golichenko, Oleksandr Masyutka, Mykhailo Moklyachuk</dc:creator>
    </item>
    <item>
      <title>Super doubly robust and efficient estimator for informative covariate censoring</title>
      <link>https://arxiv.org/abs/2511.02187</link>
      <description>arXiv:2511.02187v1 Announce Type: new 
Abstract: Early intervention in neurodegenerative diseases requires identifying periods before diagnosis when decline is rapid enough to detect whether a therapy is slowing progression. Since rapid decline typically occurs close to diagnosis, identifying these periods requires knowing each patient's time of diagnosis. Yet many patients exit studies before diagnosis, making time of diagnosis right-censored by time of study exit -- creating a right-censored covariate problem when estimating decline. Existing estimators either assume noninformative covariate censoring, where time of study exit is independent of time of diagnosis, or allow informative covariate censoring, but require correctly specifying how these times are related. We developed SPIRE (Semi-Parametric Informative Right-censored covariate Estimator), a super doubly robust estimator that remains consistent without correctly specifying densities governing time of diagnosis or time of study exit. Typical double robustness requires at least one density to be correct; SPIRE requires neither. When both densities are correctly specified, SPIRE achieves semiparametric efficiency. We also developed a test for detecting informative covariate censoring. Simulations with 85% right-censoring demonstrated SPIRE's robustness, efficiency and reliable detection of informative covariate censoring. Applied to Huntington disease data, SPIRE handled informative covariate censoring appropriately and remained consistent regardless of density specification, providing a reliable tool for early intervention.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02187v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhewei Zhang, Yanyuan Ma, Karen Marder, Tanya P. Garcia</dc:creator>
    </item>
    <item>
      <title>Generalization in Representation Models via Random Matrix Theory: Application to Recurrent Networks</title>
      <link>https://arxiv.org/abs/2511.02401</link>
      <description>arXiv:2511.02401v1 Announce Type: new 
Abstract: We first study the generalization error of models that use a fixed feature representation (frozen intermediate layers) followed by a trainable readout layer. This setting encompasses a range of architectures, from deep random-feature models to echo-state networks (ESNs) with recurrent dynamics. Working in the high-dimensional regime, we apply Random Matrix Theory to derive a closed-form expression for the asymptotic generalization error. We then apply this analysis to recurrent representations and obtain concise formula that characterize their performance. Surprisingly, we show that a linear ESN is equivalent to ridge regression with an exponentially time-weighted (''memory'') input covariance, revealing a clear inductive bias toward recent inputs. Experiments match predictions: ESNs win in low-sample, short-memory regimes, while ridge prevails with more data or long-range dependencies. Our methodology provides a general framework for analyzing overparameterized models and offers insights into the behavior of deep learning networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02401v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yessin Moakher (X), Malik Tiomoko (CUHK-Shenzhen), Cosme Louart (CUHK-Shenzhen), Zhenyu Liao (HUST)</dc:creator>
    </item>
    <item>
      <title>Wasserstein Convergence of Critically Damped Langevin Diffusions</title>
      <link>https://arxiv.org/abs/2511.02419</link>
      <description>arXiv:2511.02419v1 Announce Type: new 
Abstract: Score-based Generative Models (SGMs) have achieved impressive performance in data generation across a wide range of applications and benefit from strong theoretical guarantees. Recently, methods inspired by statistical mechanics, in particular, Hamiltonian dynamics, have introduced Critically-damped Langevin Diffusions (CLDs), which define diffusion processes on extended spaces by coupling the data with auxiliary variables. These approaches, along with their associated score-matching and sampling procedures, have been shown to outperform standard diffusion-based samplers numerically. In this paper, we analyze a generalized dynamic that extends classical CLDs by introducing an additional hyperparameter controlling the noise applied to the data coordinate, thereby better exploiting the extended space. We further derive a novel upper bound on the sampling error of CLD-based generative models in the Wasserstein metric. This additional hyperparameter influences the smoothness of sample paths, and our discretization error analysis provides practical guidance for its tuning, leading to improved sampling performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02419v1</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stanislas Strasman (SU, LPSM), Sobihan Surendran (SU, LPSM), Claire Boyer (LMO, IUF), Sylvain Le Corff (LPSM), Vincent Lemaire (LPSM), Antonio Ocello (ENSAE)</dc:creator>
    </item>
    <item>
      <title>Cluster Size Matters: A Comparative Study of Notip and pARI for Post Hoc Inference in fMRI</title>
      <link>https://arxiv.org/abs/2511.02422</link>
      <description>arXiv:2511.02422v1 Announce Type: new 
Abstract: All Resolutions Inference (ARI) is a post hoc inference method for functional Magnetic Resonance Imaging (fMRI) data analysis that provides valid lower bounds on the proportion of truly active voxels within any, possibly data-driven, cluster. As such, it addresses the paradox of spatial specificity encountered with more classical cluster-extent thresholding methods. It allows the cluster-forming threshold to be increased in order to locate the signal with greater spatial precision without overfitting, also known as the drill-down approach. Notip and pARI are two recent permutation-based extensions of ARI designed to increase statistical power by accounting for the strong dependence structure typical of fMRI data. A recent comparison between these papers based on large voxel clusters concluded that pARI outperforms Notip. We revisit this conclusion by conducting a systematic comparison of the two. Our reanalysis of the same fMRI data sets from the Neurovault database demonstrates the existence of complementary performance regimes: while pARI indeed achieves higher sensitivity for large clusters, Notip provides more informative and robust results for smaller clusters. In particular, while Notip supports informative ``drill-down'' exploration into subregions of activation, pARI often yields non-informative bounds in such cases, and can even underperform the baseline ARI method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02422v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nils Peyrouset (ENSAE), Pierre Neuvial (IMT), Bertrand Thirion (PARIETAL)</dc:creator>
    </item>
    <item>
      <title>On Convergence Rates of Spiked Eigenvalue Estimates: A General Study of Global and Local Laws in Sample Covariance Matrices</title>
      <link>https://arxiv.org/abs/2511.02456</link>
      <description>arXiv:2511.02456v1 Announce Type: new 
Abstract: This paper investigates global and local laws for sample covariance matrices with general growth rates of dimensions. The sample size $N$ and population dimension $M$ can have the same order in logarithm, which implies that their ratio $M/N$ can approach zero, a constant, or infinity. These theories are utilized to determine the convergence rate of spiked eigenvalue estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02456v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bing-Yi Jing, Weiming Li, Jiahui Xie, Yangchun Zhang, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>Spectral analysis of high-dimensional spot volatility matrix with applications</title>
      <link>https://arxiv.org/abs/2511.02660</link>
      <description>arXiv:2511.02660v1 Announce Type: new 
Abstract: In random matrix theory, the spectral distribution of the covariance matrix has been well studied under the large dimensional asymptotic regime when the dimensionality and the sample size tend to infinity at the same rate. However, most existing theories are built upon the assumption of independent and identically distributed samples, which may be violated in practice. For example, the observational data of continuous-time processes at discrete time points, namely, the high-frequency data. In this paper, we extend the classical spectral analysis for the covariance matrix in large dimensional random matrix to the spot volatility matrix by using the high-frequency data. We establish the first-order limiting spectral distribution and obtain a second-order result, that is, the central limit theorem for linear spectral statistics. Moreover, we apply the results to design some feasible tests for the spot volatility matrix, including the identity and sphericity tests. Simulation studies justify the finite sample performance of the test statistics and verify our established theory.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02660v1</guid>
      <category>math.ST</category>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Qiang Liu, Yiming Liu, Zhi Liu, Wang Zhou</dc:creator>
    </item>
    <item>
      <title>On the Convergence of the Extended Kalman Filter on Stiefel Manifolds when Observing a Constant Particle with Measurement Errors</title>
      <link>https://arxiv.org/abs/2511.02680</link>
      <description>arXiv:2511.02680v1 Announce Type: new 
Abstract: In this paper we first introduce the setting of filtering on Stiefel manifolds. Then, assuming the underlying system process is constant, the convergence of the extended Kalman filter with Stiefel manifold-valued observations is proved. This corresponds to the case where one has measurement errors that needs to be filtered. Finally, some simulations are presented for a selected few Stiefel manifolds and the speed of convergence is studied.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02680v1</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi-Llu\'is Figueras, Aron Persson, Lauri Viitasaari</dc:creator>
    </item>
    <item>
      <title>Data-driven Learning of Interaction Laws in Multispecies Particle Systems with Gaussian Processes: Convergence Theory and Applications</title>
      <link>https://arxiv.org/abs/2511.02053</link>
      <description>arXiv:2511.02053v1 Announce Type: cross 
Abstract: We develop a Gaussian process framework for learning interaction kernels in multi-species interacting particle systems from trajectory data. Such systems provide a canonical setting for multiscale modeling, where simple microscopic interaction rules generate complex macroscopic behaviors. While our earlier work established a Gaussian process approach and convergence theory for single-species systems, and later extended to second-order models with alignment and energy-type interactions, the multi-species setting introduces new challenges: heterogeneous populations interact both within and across species, the number of unknown kernels grows, and asymmetric interactions such as predator-prey dynamics must be accommodated. We formulate the learning problem in a nonparametric Bayesian setting and establish rigorous statistical guarantees. Our analysis shows recoverability of the interaction kernels, provides quantitative error bounds, and proves statistical optimality of posterior estimators, thereby unifying and generalizing previous single-species theory. Numerical experiments confirm the theoretical predictions and demonstrate the effectiveness of the proposed approach, highlighting its advantages over existing kernel-based methods. This work contributes a complete statistical framework for data-driven inference of interaction laws in multi-species systems, advancing the broader multiscale modeling program of connecting microscopic particle dynamics with emergent macroscopic behavior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02053v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jinchao Feng, Charles Kulick, Sui Tang</dc:creator>
    </item>
    <item>
      <title>Signal attenuation and phase evolution evaluation under the influence of nonlinear gradient</title>
      <link>https://arxiv.org/abs/2511.02242</link>
      <description>arXiv:2511.02242v1 Announce Type: cross 
Abstract: Accurately analyzing NMR and MRI diffusion experimental data relies on the theoretical expression used for signal attenuation or phase evolution. In a complex system, the encountered magnetic field is often inhomogeneous, which may be represented by a linear combination of z^n gradient fields, where n is the order. Additionally, the higher the order of the nonlinear gradient field, the more sensitive the phase variances are to differences in diffusion coefficients and delay times. Hence, studying higher-order fields has both theoretical and experimental importance, but this is a challenge for traditional methods. The recently proposed phase diffusion method proposed a general way to overcome the challenge. This method is used and demonstrated in detail in this paper to determine the phase evolution in a quadric field (n = 4). Three different types of phase evolution in the quadric gradient field are obtained. Moreover, a general signal attenuation expression is proposed to describe the signal attenuation for spin diffusion from the origin of the nonlinear gradient field. This approximation is based on the short gradient pulse (SGP) approximation but is extended to include the finite gradient pulse width (FGPW) effect by using the mean square phase. Compared to other forms of signal attenuation, such as Gaussian and Lorentzian, this method covers a broader range of attenuation, from small to relatively large. Additionally, this attenuation is easier to understand than the Mittag-Leffler function-based attenuation. The results, particularly the phase and signal attenuation expressions obtained in this paper, potentially advance PFG diffusion research in nonlinear gradient fields in NMR and MRI.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02242v1</guid>
      <category>physics.chem-ph</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenghao Xua, Guoxing Lin</dc:creator>
    </item>
    <item>
      <title>Limit Theorems for Stochastic Gradient Descent in High-Dimensional Single-Layer Networks</title>
      <link>https://arxiv.org/abs/2511.02258</link>
      <description>arXiv:2511.02258v1 Announce Type: cross 
Abstract: This paper studies the high-dimensional scaling limits of online stochastic gradient descent (SGD) for single-layer networks. Building on the seminal work of Saad and Solla, which analyzed the deterministic (ballistic) scaling limits of SGD corresponding to the gradient flow of the population loss, we focus on the critical scaling regime of the step size. Below this critical scale, the effective dynamics are governed by ballistic (ODE) limits, but at the critical scale, new correction term appears that changes the phase diagram. In this regime, near the fixed points, the corresponding diffusive (SDE) limits of the effective dynamics reduces to an Ornstein-Uhlenbeck process under certain conditions. These results highlight how the information exponent controls sample complexity and illustrates the limitations of deterministic scaling limit in capturing the stochastic fluctuations of high-dimensional learning dynamics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.02258v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Parsa Rangriz</dc:creator>
    </item>
    <item>
      <title>PCA for Point Processes</title>
      <link>https://arxiv.org/abs/2404.19661</link>
      <description>arXiv:2404.19661v2 Announce Type: replace 
Abstract: We introduce a novel statistical framework for the analysis of replicated point processes that allows for the study of point pattern variability at a population level. By treating point process realizations as random measures, we adopt a functional analysis perspective and propose a form of functional Principal Component Analysis (fPCA) for point processes. The originality of our method is to base our analysis on the cumulative mass functions of the random measures which gives us a direct and interpretable analysis. Key theoretical contributions include establishing a Karhunen-Lo\`{e}ve expansion for the random measures and a Mercer Theorem for covariance measures. We establish convergence in a strong sense, and introduce the concept of principal measures, which can be seen as latent processes governing the dynamics of the observed point patterns. We propose an easy-to-implement estimation strategy of eigenelements for which parametric rates are achieved. We fully characterize the solutions of our approach to Poisson and Hawkes processes and validate our methodology via simulations and diverse applications in seismology, single-cell biology and neurosiences, demonstrating its versatility and effectiveness. Our method is implemented in the pppca R-package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.19661v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Franck Picard, Vincent Rivoirard, Angelina Roche, Victor Panaretos</dc:creator>
    </item>
    <item>
      <title>Statistical Advantages of Oblique Randomized Decision Trees and Forests</title>
      <link>https://arxiv.org/abs/2407.02458</link>
      <description>arXiv:2407.02458v2 Announce Type: replace 
Abstract: This work studies the statistical implications of using features comprised of general linear combinations of covariates to partition the data in randomized decision tree and forest regression algorithms. Using random tessellation theory in stochastic geometry, we provide a theoretical analysis of a class of efficiently generated random tree and forest estimators that allow for oblique splits along such features. We call these estimators \emph{oblique Mondrian} trees and forests, as the trees are generated by first selecting a set of features from linear combinations of the covariates and then running a Mondrian process that hierarchically partitions the data along these features. Generalization error bounds and convergence rates are obtained for the flexible function class of multi-index models for dimension reduction, where the output is assumed to depend on a low-dimensional relevant feature subspace of the input domain. The results highlight how the risk of these estimators depends on the choice of features and quantify how robust the risk is with respect to error in the estimation of relevant features. The asymptotic analysis also provides conditions on the consistency rates of the estimated features along which the data is split for these estimators to obtain minimax optimal rates of convergence with respect to the dimension of the relevant feature subspace. Additionally, a lower bound on the risk of axis-aligned Mondrian trees (where features are restricted to the set of covariates) is obtained, proving that these estimators are suboptimal for general ridge functions, no matter how the distribution over the covariates used to divide the data at each tree node is weighted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.02458v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eliza O'Reilly</dc:creator>
    </item>
    <item>
      <title>Robust Regression under Adversarial Contamination: Theory and Algorithms for the Welsch Estimator</title>
      <link>https://arxiv.org/abs/2412.19183</link>
      <description>arXiv:2412.19183v3 Announce Type: replace 
Abstract: Convex and penalized robust regression methods often suffer from a persistent bias induced by large outliers, limiting their effectiveness in adversarial or heavy-tailed settings. In this work, we study a smooth redescending non-convex M-estimator, specifically the Welsch estimator, and show that it can eliminate this bias whenever it is statistically identifiable. We focus on high-dimensional linear regression under adversarial contamination, where a fraction of samples may be corrupted by an adversary with full knowledge of the data and underlying model. A central technical contribution of this paper is a practical algorithm that provably finds a statistically valid solution to this non-convex problem. We show that the Welsch objective remains locally convex within a well-characterized basin of attraction, and our algorithm is guaranteed to converge into this region and recover the desired estimator. We establish three main guarantees: (a) non-asymptotic minimax-optimal deviation bounds under contamination, (b) improved unbiasedness in the presence of large outliers, and (c) asymptotic normality, yielding statistical efficiency as the sample size grows. Finally, we support our theoretical findings with comprehensive experiments on synthetic and real datasets, demonstrating the estimator's superior robustness, efficiency, and effectiveness in mitigating outlier-induced bias relative to state-of-the-art robust regression methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19183v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ilyes Hammouda, Mohamed Ndaoud, Abd-Krim Seghouane</dc:creator>
    </item>
    <item>
      <title>Autoregressive Processes on Stiefel and Grassmann Manifolds</title>
      <link>https://arxiv.org/abs/2509.24767</link>
      <description>arXiv:2509.24767v2 Announce Type: replace 
Abstract: System identification of autoregressive processes on Stiefel and Grassmann manifolds are presented and studied. We define the system parameters as elements in the orthogonal group and we show that the system can be estimated by averaging over observations. Then we propose an algorithm on how to compute these system parameters using conjugate gradient descent on Stiefel and Grassmann manifolds, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2509.24767v2</guid>
      <category>math.ST</category>
      <category>math.DG</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jordi-Llu\'is Figueras, Aron Persson</dc:creator>
    </item>
    <item>
      <title>Nonparametric Least squares estimators for interval censoring</title>
      <link>https://arxiv.org/abs/2511.01103</link>
      <description>arXiv:2511.01103v2 Announce Type: replace 
Abstract: The limit distribution of the nonparametric maximum likelihood estimator for interval censored data with more than one observation time per unobservable observation, is still unknown in general. For the so-called separated case, where one has observation times which are at a distance larger than a fixed $\epsilon&gt;0$, the limit distribution was derived in [4]. For the non-separated case there is a conjectured limit distribution, given in [9], Section 5.2 of Part 2. But the findings of the present paper suggest that this conjecture may not hold.
  We prove consistency of a closely related nonparametric isotonic least squares estimator and give a sketch of the proof for a result on its limit distribution. We also provide simulation results to show how the nonparametric MLE and least squares estimator behave in comparison. Moreover, we discuss a simpler least squares estimator that can be computed in one step, but is inferior to the other least squares estimator, since it does not use all information.
  For the simplest model of interval censoring, the current status model, the nonparametric maximum likelihood and least squares estimators are the same. This equivalence breaks down if there are more observation times per unobservable observation. The computations for the simulation of the more complicated interval censoring model were performed by using the iterative convex minorant algorithm. They are provided in the GitHub repository [6].</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.01103v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Piet Groeneboom</dc:creator>
    </item>
    <item>
      <title>A method for sparse and robust independent component analysis</title>
      <link>https://arxiv.org/abs/2502.04046</link>
      <description>arXiv:2502.04046v3 Announce Type: replace-cross 
Abstract: This work presents sparse invariant coordinate selection, SICS, a new method for sparse and robust independent component analysis. SICS is based on classical invariant coordinate selection, which is presented in such a form that a LASSO-type penalty can be applied to promote sparsity. Robustness is achieved by using robust scatter matrices. In the first part of the paper, the background and building blocks: scatter matrices, measures of robustness, ICS and independent component analysis, are carefully introduced. Then the proposed new method and its algorithm are derived and presented. This part also includes consistency and breakdown point results for a general case of sparse ICS-like methods. The performance of SICS in identifying sparse independent component loadings is investigated with multiple simulations. The method is illustrated with an example in constructing sparse causal graphs and we also propose a graphical tool for selecting the appropriate sparsity level in SICS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04046v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lauri Heinonen, Joni Virta</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Variable Selection in Model-Based Clustering with Missing Not at Random</title>
      <link>https://arxiv.org/abs/2505.19093</link>
      <description>arXiv:2505.19093v3 Announce Type: replace-cross 
Abstract: Model-based clustering integrated with variable selection is a powerful tool for uncovering latent structures within complex data. However, its effectiveness is often hindered by challenges such as identifying relevant variables that define heterogeneous subgroups and handling data that are missing not at random, a prevalent issue in fields like transcriptomics. While several notable methods have been proposed to address these problems, they typically tackle each issue in isolation, thereby limiting their flexibility and adaptability. This paper introduces a unified framework designed to address these challenges simultaneously. Our approach incorporates a data-driven penalty matrix into penalized clustering to enable more flexible variable selection, along with a mechanism that explicitly models the relationship between missingness and latent class membership. We demonstrate that, under certain regularity conditions, the proposed framework achieves both asymptotic consistency and selection consistency, even in the presence of missing data. This unified strategy significantly enhances the capability and efficiency of model-based clustering, advancing methodologies for identifying informative variables that define homogeneous subgroups in the presence of complex missing data patterns. The performance of the framework, including its computational efficiency, is evaluated through simulations and demonstrated using both synthetic and real-world transcriptomic datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.19093v3</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>39th Conference on Neural Information Processing Systems (NeurIPS 2025)</arxiv:journal_reference>
      <dc:creator>Binh H. Ho, Long Nguyen Chi, TrungTin Nguyen, Binh T. Nguyen, Van Ha Hoang, Christopher Drovandi</dc:creator>
    </item>
    <item>
      <title>Shifted Composition IV: Toward Ballistic Acceleration for Log-Concave Sampling</title>
      <link>https://arxiv.org/abs/2506.23062</link>
      <description>arXiv:2506.23062v2 Announce Type: replace-cross 
Abstract: Acceleration is a celebrated cornerstone of convex optimization, enabling gradient-based algorithms to converge sublinearly in the condition number. A major open question is whether an analogous acceleration phenomenon is possible for log-concave sampling. Underdamped Langevin dynamics (ULD) has long been conjectured to be the natural candidate for acceleration, but a central challenge is that its degeneracy necessitates the development of new analysis approaches, e.g., the theory of hypocoercivity. Although recent breakthroughs established ballistic acceleration for the (continuous-time) ULD diffusion via space-time Poincare inequalities, (discrete-time) algorithmic results remain entirely open: the discretization error of existing analysis techniques dominates any continuous-time acceleration.
  In this paper, we give a new coupling-based local error framework for analyzing ULD and its numerical discretizations in KL divergence. This extends the framework in Shifted Composition III from uniformly elliptic diffusions to degenerate diffusions, and shares its virtues: the framework is user-friendly, applies to sophisticated discretization schemes, and does not require contractivity. Applying this framework to the randomized midpoint discretization of ULD establishes the first ballistic acceleration result for log-concave sampling (i.e., sublinear dependence on the condition number). Along the way, we also obtain the first $d^{1/3}$ iteration complexity guarantee for sampling to constant total variation error in dimension $d$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.23062v2</guid>
      <category>math.PR</category>
      <category>cs.DS</category>
      <category>cs.NA</category>
      <category>math.AP</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 05 Nov 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jason M. Altschuler, Sinho Chewi, Matthew S. Zhang</dc:creator>
    </item>
  </channel>
</rss>
