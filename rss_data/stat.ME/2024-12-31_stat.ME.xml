<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 31 Dec 2024 05:00:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Linear Shrinkage Convexification of Penalized Linear Regression With Missing Data</title>
      <link>https://arxiv.org/abs/2412.19963</link>
      <description>arXiv:2412.19963v1 Announce Type: new 
Abstract: One of the common challenges faced by researchers in recent data analysis is missing values. In the context of penalized linear regression, which has been extensively explored over several decades, missing values introduce bias and yield a non-positive definite covariance matrix of the covariates, rendering the least square loss function non-convex. In this paper, we propose a novel procedure called the linear shrinkage positive definite (LPD) modification to address this issue. The LPD modification aims to modify the covariance matrix of the covariates in order to ensure consistency and positive definiteness. Employing the new covariance estimator, we are able to transform the penalized regression problem into a convex one, thereby facilitating the identification of sparse solutions. Notably, the LPD modification is computationally efficient and can be expressed analytically. In the presence of missing values, we establish the selection consistency and prove the convergence rate of the $\ell_1$-penalized regression estimator with LPD, showing an $\ell_2$-error convergence rate of square-root of $\log p$ over $n$ by a factor of $(s_0)^{3/2}$ ($s_0$: the number of non-zero coefficients). To further evaluate the effectiveness of our approach, we analyze real data from the Genomics of Drug Sensitivity in Cancer (GDSC) dataset. This dataset provides incomplete measurements of drug sensitivities of cell lines and their protein expressions. We conduct a series of penalized linear regression models with each sensitivity value serving as a response variable and protein expressions as explanatory variables.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.19963v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Seongoh Park, Seongjin Lee, Nguyen Thi Hai Yen, Nguyen Phuoc Long, Johan Lim</dc:creator>
    </item>
    <item>
      <title>Kendall and Spearman Rank Correlations for Skew-Elliptical Copulas</title>
      <link>https://arxiv.org/abs/2412.20013</link>
      <description>arXiv:2412.20013v1 Announce Type: new 
Abstract: In this paper, we derive explicit formulas of Kendall's tau and Spearman's rho rank correlations for two general classes of skew-elliptical copulas: normal location-scale mixture copulas and skew-normal scale mixture copulas, which encompass some widely used skew-$t$ and skew-normal copulas. These formulas establish mappings from copula parameters to rank correlation coefficients, potentially facilitating robust rank-based estimation of skew-elliptical copula models. Additionally, we investigate and compare the impacts of asymmetry parameters on the properties of both rank correlations within these two classes of skew-elliptical models. Notably, the introduction of asymmetry in normal location-scale mixture copulas restricts the attainable range of the rank correlations from $[-1,1]$ -- as observed under elliptical symmetry -- to a strict subset of $[-1,1]$. In contrast, the entire interval $[-1,1]$ remains attainable for skew-normal scale mixture copulas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20013v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ye Lu</dc:creator>
    </item>
    <item>
      <title>Debiased Nonparametric Regression for Statistical Inference and Distributionally Robustness</title>
      <link>https://arxiv.org/abs/2412.20173</link>
      <description>arXiv:2412.20173v1 Announce Type: new 
Abstract: This study proposes a debiasing method for smooth nonparametric estimators. While machine learning techniques such as random forests and neural networks have demonstrated strong predictive performance, their theoretical properties remain relatively underexplored. Specifically, many modern algorithms lack assurances of pointwise asymptotic normality and uniform convergence, which are critical for statistical inference and robustness under covariate shift and have been well-established for classical methods like Nadaraya-Watson regression. To address this, we introduce a model-free debiasing method that guarantees these properties for smooth estimators derived from any nonparametric regression approach. By adding a correction term that estimates the conditional expected residual of the original estimator, or equivalently, its estimation error, we obtain a debiased estimator with proven pointwise asymptotic normality, uniform convergence, and Gaussian process approximation. These properties enable statistical inference and enhance robustness to covariate shift, making the method broadly applicable to a wide range of nonparametric regression problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20173v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
    <item>
      <title>Robust Quickest Change Detection with Sampling Control</title>
      <link>https://arxiv.org/abs/2412.20207</link>
      <description>arXiv:2412.20207v1 Announce Type: new 
Abstract: The problem of quickest detection of a change in the distribution of a sequence of random variables is studied. The objective is to detect the change with the minimum possible delay, subject to constraints on the rate of false alarms and the cost of observations used in the decision-making process. The post-change distribution of the data is known only within a distribution family. It is shown that if the post-change family has a distribution that is least favorable in a well-defined sense, then a computationally efficient algorithm can be designed that uses an on-off observation control strategy to save the cost of observations. In addition, the algorithm can detect the change robustly while avoiding unnecessary false alarms. It is shown that the algorithm is also asymptotically robust optimal as the rate of false alarms goes to zero for every fixed constraint on the cost of observations. The algorithm's effectiveness is validated on simulated data and real public health data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20207v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yingze Hou, Hoda Bidkhori, Taposh Banerjee</dc:creator>
    </item>
    <item>
      <title>A Rank-Based Test for Comparing Multiple Fields' Yield Quality Distributions Under Spatial Dependence</title>
      <link>https://arxiv.org/abs/2412.20316</link>
      <description>arXiv:2412.20316v1 Announce Type: new 
Abstract: This work introduces a novel rank-based test for comparing yield quality distributions across multiple spatially distributed populations while accounting for spatial dependence. Traditional methods often fail to address the challenges posed by spatial correlation and heterogeneity, leading to unreliable conclusions. The proposed test integrates kernel smoothing with spatially weighted cumulative distribution function (CDF) estimation to ensure robustness against outliers and deviations from distributional assumptions. A test statistic is derived by aggregating squared differences between smoothed CDFs, and its asymptotic properties are established under the null hypothesis, demonstrating convergence to a weighted sum of chi-squared random variables. Practical implementation relies on resampling techniques such as permutation tests and spatial bootstrap methods to approximate the null distribution. The test is consistent under alternatives, making it an effective tool for analyzing spatially dependent data in fields such as environmental monitoring and agricultural research. Extensions to multivariate and spatio-temporal applications are suggested for future work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20316v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marco Mandap</dc:creator>
    </item>
    <item>
      <title>When the whole is greater than the sum of its parts: Scaling black-box inference to large data settings through divide-and-conquer</title>
      <link>https://arxiv.org/abs/2412.20323</link>
      <description>arXiv:2412.20323v1 Announce Type: new 
Abstract: Black-box methods such as deep neural networks are exceptionally fast at obtaining point estimates of model parameters due to their amortisation of the loss function computation, but are currently restricted to settings for which simulating training data is inexpensive. When simulating data is computationally expensive, both the training and uncertainty quantification, which typically relies on a parametric bootstrap, become intractable. We propose a black-box divide-and-conquer estimation and inference framework when data simulation is computationally expensive that trains a black-box estimation method on a partition of the multivariate data domain, estimates and bootstraps on the partitioned data, and combines estimates and inferences across data partitions. Through the divide step, only small training data need be simulated, substantially accelerating the training. Further, the estimation and bootstrapping can be conducted in parallel across multiple computing nodes to further speed up the procedure. Finally, the conquer step accounts for any dependence between data partitions through a statistically and computationally efficient weighted average. We illustrate the implementation of our framework in high-dimensional spatial settings with Gaussian and max-stable processes. Applications to modeling extremal temperature data from both a climate model and observations from the National Oceanic and Atmospheric Administration highlight the feasibility of estimation and inference of max-stable process parameters with tens of thousands of locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20323v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Emily C. Hector, Amanda Lenzi</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Modeling for Uncertainty Quantification and Reliability Updating using Data</title>
      <link>https://arxiv.org/abs/2412.20416</link>
      <description>arXiv:2412.20416v1 Announce Type: new 
Abstract: Quantifying uncertainty and updating reliability are essential for ensuring the safety and performance of engineering systems. This study develops a hierarchical Bayesian modeling (HBM) framework to quantify uncertainty and update reliability using data. By leveraging the probabilistic structure of HBM, the approach provides a robust solution for integrating model uncertainties and parameter variability into reliability assessments. The framework is applied to a linear mathematical model and a dynamical structural model. For the linear model, analytical solutions are derived for the hyper parameters and reliability, offering an efficient and precise means of uncertainty quantification and reliability evaluation. In the dynamical structural model, the posterior distributions of hyper parameters obtained from the HBM are used directly to update the reliability. This approach relies on the updated posteriors to reflect the influence of system uncertainties and dynamic behavior in the reliability predictions. The proposed approach demonstrates significant advantages over traditional Bayesian inference by addressing multi-source uncertainty in both static and dynamic contexts. This work highlights the versatility and computational efficiency of the HBM framework, establishing it as a powerful tool for uncertainty quantification and reliability updating in structural health monitoring and other engineering applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20416v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinyu Jia, Weinan Hou, Costas Papadimitriou</dc:creator>
    </item>
    <item>
      <title>Inference with Randomized Regression Trees</title>
      <link>https://arxiv.org/abs/2412.20535</link>
      <description>arXiv:2412.20535v1 Announce Type: new 
Abstract: Regression trees are a popular machine learning algorithm that fit piecewise constant models by recursively partitioning the predictor space. In this paper, we focus on performing statistical inference in a data-dependent model obtained from the fitted tree. We introduce Randomized Regression Trees (RRT), a novel selective inference method that adds independent Gaussian noise to the gain function underlying the splitting rules of classic regression trees.
  The RRT method offers several advantages. First, it utilizes the added randomization to obtain an exact pivot using the full dataset, while accounting for the data-dependent structure of the fitted tree. Second, with a small amount of randomization, the RRT method achieves predictive accuracy similar to a model trained on the entire dataset. At the same time, it provides significantly more powerful inference than data splitting methods, which rely only on a held-out portion of the data for inference. Third, unlike data splitting approaches, it yields intervals that adapt to the signal strength in the data. Our empirical analyses highlight these advantages of the RRT method and its ability to convert a purely predictive algorithm into a method capable of performing reliable and powerful inference in the tree model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20535v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Soham Bakshi, Yiling Huang, Snigdha Panigrahi, Walter Dempsey</dc:creator>
    </item>
    <item>
      <title>Hausman's Consistency Test and an Internal Bias Diagnostic: A Short Note Regarding Linear Mixed Models</title>
      <link>https://arxiv.org/abs/2412.20555</link>
      <description>arXiv:2412.20555v1 Announce Type: new 
Abstract: The Hausman specification test detects inconsistency in mixed model estimators of fixed effects by comparing the original model with an alternative specification in which random effects are treated as fixed. This note illustrates a bias diagnostic from the statistical literature as a complement to the Hausman test. The diagnostic can provide internal estimates of parameter-specific bias in the mixed model estimators without requiring the second, fixed-effects-only model to be fit. We apply the diagnostic to a panel data analysis as well as to a Value-Added Model (VAM) for teacher evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20555v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Andrew T. Karl</dc:creator>
    </item>
    <item>
      <title>LEARNER: A Transfer Learning Method for Low-Rank Matrix Estimation</title>
      <link>https://arxiv.org/abs/2412.20605</link>
      <description>arXiv:2412.20605v1 Announce Type: new 
Abstract: Low-rank matrix estimation is a fundamental problem in statistics and machine learning. In the context of heterogeneous data generated from diverse sources, a key challenge lies in leveraging data from a source population to enhance the estimation of a low-rank matrix in a target population of interest. One such example is estimating associations between genetic variants and diseases in non-European ancestry groups. We propose an approach that leverages similarity in the latent row and column spaces between the source and target populations to improve estimation in the target population, which we refer to as LatEnt spAce-based tRaNsfer lEaRning (LEARNER). LEARNER is based on performing a low-rank approximation of the target population data which penalizes differences between the latent row and column spaces between the source and target populations. We present a cross-validation approach that allows the method to adapt to the degree of heterogeneity across populations. We conducted extensive simulations which found that LEARNER often outperforms the benchmark approach that only uses the target population data, especially as the signal-to-noise ratio in the source population increases. We also performed an illustrative application and empirical comparison of LEARNER and benchmark approaches in a re-analysis of a genome-wide association study in the BioBank Japan cohort. LEARNER is implemented in the R package learner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20605v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sean McGrath, Cenhao Zhu, Min Guo, Rui Duan</dc:creator>
    </item>
    <item>
      <title>Uncertainty of high-dimensional genetic data prediction with polygenic risk scores</title>
      <link>https://arxiv.org/abs/2412.20611</link>
      <description>arXiv:2412.20611v1 Announce Type: new 
Abstract: In many predictive tasks, there are a large number of true predictors with weak signals, leading to substantial uncertainties in prediction outcomes. The polygenic risk score (PRS) is an example of such a scenario, where many genetic variants are used as predictors for complex traits, each contributing only a small amount of information. Although PRS has been a standard tool in genetic predictions, its uncertainty remains largely unexplored. In this paper, we aim to establish the asymptotic normality of PRS in high-dimensional predictions without sparsity constraints. We investigate the popular marginal and ridge-type estimators in PRS applications, developing central limit theorems for both individual-level predicted values (e.g., genetically predicted human height) and cohort-level prediction accuracy measures (e.g., overall predictive $R$-squared in the testing dataset). Our results demonstrate that ignoring the prediction-induced uncertainty can lead to substantial underestimation of the true variance of PRS-based estimators, which in turn may cause overconfidence in the accuracy of confidence intervals and hypothesis testing. These findings provide key insights omitted by existing first-order asymptotic studies of high-dimensional sparsity-free predictions, which often focus solely on the point limits of predictive risks. We develop novel and flexible second-order random matrix theory results to assess the asymptotic normality of functionals with a general covariance matrix, without assuming Gaussian distributions for the data. We evaluate our theoretical results through extensive numerical analyses using real data from the UK Biobank. Our analysis underscores the importance of incorporating uncertainty assessments at both the individual and cohort levels when applying and interpreting PRS.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20611v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Haoxuan Fu, Jiaoyang Huang, Zirui Fan, Bingxin Zhao</dc:creator>
    </item>
    <item>
      <title>Identifying average causal effect in regression discontinuity design with auxiliary data</title>
      <link>https://arxiv.org/abs/2412.20840</link>
      <description>arXiv:2412.20840v1 Announce Type: new 
Abstract: Regression discontinuity designs are widely used when treatment assignment is determined by whether a running variable exceeds a predefined threshold. However, most research focuses on estimating local causal effects at the threshold, leaving the challenge of identifying treatment effects away from the cutoff largely unaddressed. The primary difficulty in this context is that the counterfactual outcome under the alternative treatment status is unobservable.In this paper, we introduce a novel framework for identifying the global average causal effect in regression discontinuity designs.Our approach integrates a latent variable and an additional data structure alongside the traditional regression discontinuity design setup. This enhanced framework allows us to extend the analysis beyond the threshold, providing a more comprehensive understanding of treatment effects.We develop asymptotically valid estimation and inference procedures under this framework, ensuring the robustness of our findings. To demonstrate the practical application of our method, we assess the causal effects of vitamin A supplementation on the severity of autism spectrum disorders in children.Our approach offers a significant advancement in the analysis of regression discontinuity designs, enabling researchers to estimate causal effects across a broader range of values and providing more actionable insights in policy and medical research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20840v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinqin Feng, Wenjie Hu, Pu Yang, Tingyu Li, Xiao-Hua Zhou</dc:creator>
    </item>
    <item>
      <title>From sparse to dense functional time series: phase transitions of detecting structural breaks and beyond</title>
      <link>https://arxiv.org/abs/2412.20858</link>
      <description>arXiv:2412.20858v1 Announce Type: new 
Abstract: We develop a novel methodology for detecting abrupt break points in mean functions of functional time series, adaptable to arbitrary sampling schemes. By employing B-spline smoothing, we introduce $\mathcal L_{\infty}$ and $\mathcal L_2$ test statistics statistics based on a smoothed cumulative summation (CUMSUM) process, and derive the corresponding asymptotic distributions under the null and local alternative hypothesis, as well as the phase transition boundary from sparse to dense. We further establish the convergence rate of the proposed break point estimators and conduct statistical inference on the jump magnitude based on the estimated break point, also applicable across sparsely, semi-densely, and densely, observed random functions. Extensive numerical experiments validate the effectiveness of the proposed procedures. To illustrate the practical relevance, we apply the developed methods to analyze electricity price data and temperature data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20858v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Leheng Cai, Qirui Hu</dc:creator>
    </item>
    <item>
      <title>A flexible parametric approach to synthetic patients generation using health data</title>
      <link>https://arxiv.org/abs/2412.21056</link>
      <description>arXiv:2412.21056v1 Announce Type: new 
Abstract: Enhancing reproducibility and data accessibility is essential to scientific research. However, ensuring data privacy while achieving these goals is challenging, especially in the medical field, where sensitive data are often commonplace. One possible solution is to use synthetic data that mimic real-world datasets. This approach may help to streamline therapy evaluation and enable quicker access to innovative treatments. We propose using a method based on sequential conditional regressions, such as in a fully conditional specification (FCS) approach, along with flexible parametric survival models to accurately replicate covariate patterns and survival times. To make our approach available to a wide audience of users, we have developed user-friendly functions in R and Python to implement it. We also provide an example application to registry data on patients affected by Creutzfeld-Jacob disease. The results show the potentialities of the proposed method in mirroring observed multivariate distributions and survival outcomes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21056v1</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Cipriani, Lorenzo Di Rocco, Maria Puopolo, Marco Alf\`o</dc:creator>
    </item>
    <item>
      <title>An Observation-Driven State-Space Model for Claims Size Modeling</title>
      <link>https://arxiv.org/abs/2412.21099</link>
      <description>arXiv:2412.21099v1 Announce Type: new 
Abstract: State-space models are popular models in econometrics. Recently, these models have gained some popularity in the actuarial literature. The best known state-space models are of Kalman-filter type. These models are so-called parameter-driven because the observations do not impact the state-space dynamics. A second less well-known class of state-space models are so-called observation-driven state-space models where the state-space dynamics is also impacted by the actual observations. A typical example is the Poisson-Gamma observation-driven state-space model for counts data. This Poisson-Gamma model is fully analytically tractable. The goal of this paper is to develop a Gamma- Gamma observation-driven state-space model for claim size modeling. We provide fully tractable versions of Gamma-Gamma observation-driven state-space models, and these versions extend the work of Smith and Miller (1986) by allowing for a fully flexible variance behavior. Additionally, we demonstrate that the proposed model aligns with evolutionary credibility, a methodology in insurance that dynamically adjusts premium rates over time using evolving data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.21099v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jae Youn Ahn, Himchan Jeong, Mario V. W\"uthrich</dc:creator>
    </item>
    <item>
      <title>Fitting Dynamically Misspecified Models: An Optimal Transportation Approach</title>
      <link>https://arxiv.org/abs/2412.20204</link>
      <description>arXiv:2412.20204v1 Announce Type: cross 
Abstract: This paper considers filtering, parameter estimation, and testing for potentially dynamically misspecified state-space models. When dynamics are misspecified, filtered values of state variables often do not satisfy model restrictions, making them hard to interpret, and parameter estimates may fail to characterize the dynamics of filtered variables. To address this, a sequential optimal transportation approach is used to generate a model-consistent sample by mapping observations from a flexible reduced-form to the structural conditional distribution iteratively. Filtered series from the generated sample are model-consistent. Specializing to linear processes, a closed-form Optimal Transport Filtering algorithm is derived. Minimizing the discrepancy between generated and actual observations defines an Optimal Transport Estimator. Its large sample properties are derived. A specification test determines if the model can reproduce the sample path, or if the discrepancy is statistically significant. Empirical applications to trend-cycle decomposition, DSGE models, and affine term structure models illustrate the methodology and the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20204v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jean-Jacques Forneron, Zhongjun Qu</dc:creator>
    </item>
    <item>
      <title>Estimation of conditional inequality measures</title>
      <link>https://arxiv.org/abs/2412.20228</link>
      <description>arXiv:2412.20228v1 Announce Type: cross 
Abstract: Classical inequality measures such as the Gini index are often used to describe the sparsity of the distribution of a certain feature in a population. It is sometimes also used to compare the inequalities between some subpopulations, conditioned on certain values of the covariates. The concept of measuring inequality in subpopulation was described in the literature and it is strongly related to the decomposition of the Gini index. In this paper, the idea of conditional inequality measures is extended to the case where covariates are continuous. Curves of conditional inequality measures are introduced, especially, the curves of the conditional quantile versions of the Zenga and $D$ indices are considered. Various methods of their estimation based on quantile regression are presented. An approach using isotonic regression is used to prevent quantile crossing in quantile regression. The accuracy of the estimators considered is compared in simulation studies. Furthermore, an analysis of the growth in salary inequalities with respect to employee age is included to demonstrate the potential of conditional inequality measures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20228v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alicja Jokiel-Rokita, Sylwester Pi\k{a}tek, Rafa{\l} Topolnicki</dc:creator>
    </item>
    <item>
      <title>Causal Discovery on Dependent Binary Data</title>
      <link>https://arxiv.org/abs/2412.20289</link>
      <description>arXiv:2412.20289v1 Announce Type: cross 
Abstract: The assumption of independence between observations (units) in a dataset is prevalent across various methodologies for learning causal graphical models. However, this assumption often finds itself in conflict with real-world data, posing challenges to accurate structure learning. We propose a decorrelation-based approach for causal graph learning on dependent binary data, where the local conditional distribution is defined by a latent utility model with dependent errors across units. We develop a pairwise maximum likelihood method to estimate the covariance matrix for the dependence among the units. Then, leveraging the estimated covariance matrix, we develop an EM-like iterative algorithm to generate and decorrelate samples of the latent utility variables, which serve as decorrelated data. Any standard causal discovery method can be applied on the decorrelated data to learn the underlying causal graph. We demonstrate that the proposed decorrelation approach significantly improves the accuracy in causal graph learning, through numerical experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20289v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Chen, Qing Zhou</dc:creator>
    </item>
    <item>
      <title>Testing and Improving the Robustness of Amortized Bayesian Inference for Cognitive Models</title>
      <link>https://arxiv.org/abs/2412.20586</link>
      <description>arXiv:2412.20586v1 Announce Type: cross 
Abstract: Contaminant observations and outliers often cause problems when estimating the parameters of cognitive models, which are statistical models representing cognitive processes. In this study, we test and improve the robustness of parameter estimation using amortized Bayesian inference (ABI) with neural networks. To this end, we conduct systematic analyses on a toy example and analyze both synthetic and real data using a popular cognitive model, the Drift Diffusion Models (DDM). First, we study the sensitivity of ABI to contaminants with tools from robust statistics: the empirical influence function and the breakdown point. Next, we propose a data augmentation or noise injection approach that incorporates a contamination distribution into the data-generating process during training. We examine several candidate distributions and evaluate their performance and cost in terms of accuracy and efficiency loss relative to a standard estimator. Introducing contaminants from a Cauchy distribution during training considerably increases the robustness of the neural density estimator as measured by bounded influence functions and a much higher breakdown point. Overall, the proposed method is straightforward and practical to implement and has a broad applicability in fields where outlier detection or removal is challenging.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20586v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yufei Wu, Stefan Radev, Francis Tuerlinckx</dc:creator>
    </item>
    <item>
      <title>Robust Matrix Completion for Discrete Rating-Scale Data</title>
      <link>https://arxiv.org/abs/2412.20802</link>
      <description>arXiv:2412.20802v1 Announce Type: cross 
Abstract: Matrix completion has gained considerable interest in recent years. The goal of matrix completion is to predict the unknown entries of a partially observed matrix using its known entries. Although common applications feature discrete rating-scale data, such as user-product rating matrices in recommender systems or surveys in the social and behavioral sciences, methods for matrix completion are almost always designed for and studied in the context of continuous data. Furthermore, only a small subset of the literature considers matrix completion in the presence of corrupted observations despite their common occurrence in practice. Examples include attacks on recommender systems (i.e., malicious users deliberately manipulating ratings to influence the recommender system to their advantage), or careless respondents in surveys (i.e., respondents providing answers irrespective of what the survey asks of them due to a lack of attention). We introduce a matrix completion algorithm that is tailored towards the discrete nature of rating-scale data and robust to the presence of corrupted observations. In addition, we investigate the performance of the proposed method and its competitors with discrete rating-scale (rather than continuous) data as well as under various missing data mechanisms and types of corrupted observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.20802v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aurore Archimbaud, Andreas Alfons, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Causal inference with recurrent and competing events</title>
      <link>https://arxiv.org/abs/2202.08500</link>
      <description>arXiv:2202.08500v2 Announce Type: replace 
Abstract: Many research questions concern treatment effects on outcomes that can recur several times in the same individual. For example, medical researchers are interested in treatment effects on hospitalizations in heart failure patients and sports injuries in athletes. Competing events, such as death, complicate causal inference in studies of recurrent events because once a competing event occurs, an individual cannot have more recurrent events. Several statistical estimands have been studied in recurrent event settings, with and without competing events. However, the causal interpretations of these estimands, and the conditions that are required to identify these estimands from observed data, have yet to be formalized. Here we use a formal framework for causal inference to formulate several causal estimands in recurrent event settings, with and without competing events. We clarify when commonly used classical statistical estimands can be interpreted as causal quantities from the causal mediation literature, such as (controlled) direct effects and total effects. Furthermore, we show that recent results on interventionist mediation estimands allow us to define new causal estimands with recurrent and competing events that may be of particular clinical relevance in many subject matter settings. We use causal directed acyclic graphs and single world intervention graphs to illustrate how to reason about identification conditions for the various causal estimands based on subject matter knowledge. Furthermore, using results on counting processes, we show that our causal estimands and their identification conditions, which are articulated in discrete time, converge to classical continuous time counterparts in the limit of fine discretizations of time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.08500v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/s10985-023-09594-8</arxiv:DOI>
      <dc:creator>Matias Janvin, Jessica G. Young, P{\aa}l C. Ryalen, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Estimating probabilities of multivariate failure sets based on pairwise tail dependence coefficients</title>
      <link>https://arxiv.org/abs/2210.12618</link>
      <description>arXiv:2210.12618v2 Announce Type: replace 
Abstract: Estimating the probability of extreme events involving multiple risk factors is a critical challenge in fields such as finance and climate science. This paper proposes a semi-parametric approach to estimate the probability that a multivariate random vector falls into an extreme failure set, based on the information in the tail pairwise dependence matrix (TPDM) only. The TPDM provides a partial summary of tail dependence for all pairs of components of the random vector. We propose an efficient algorithm to obtain approximate completely positive decompositions of the TPDM, enabling the construction of a max-linear model whose TPDM approximates that of the original random vector. We also provide conditions under which the approximation turns out to be exact. Based on the decompositions, we can construct max-linear random vectors to estimate failure probabilities, exploiting its computational simplicity. The algorithm allows to obtain multiple decompositions efficiently. Finally, we apply our framework to estimate probabilities of extreme events for real-world datasets, including industry portfolio returns and maximal wind speeds, demonstrating its practical utility for risk assessment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.12618v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Kiriliouk, Chen Zhou</dc:creator>
    </item>
    <item>
      <title>Neural Likelihood Surfaces for Spatial Processes with Computationally Intensive or Intractable Likelihoods</title>
      <link>https://arxiv.org/abs/2305.04634</link>
      <description>arXiv:2305.04634v4 Announce Type: replace 
Abstract: In spatial statistics, fast and accurate parameter estimation, coupled with a reliable means of uncertainty quantification, can be challenging when fitting a spatial process to real-world data because the likelihood function might be slow to evaluate or wholly intractable. In this work, we propose using convolutional neural networks to learn the likelihood function of a spatial process. Through a specifically designed classification task, our neural network implicitly learns the likelihood function, even in situations where the exact likelihood is not explicitly available. Once trained on the classification task, our neural network is calibrated using Platt scaling which improves the accuracy of the neural likelihood surfaces. To demonstrate our approach, we compare neural likelihood surfaces and the resulting maximum likelihood estimates and approximate confidence regions with the equivalent for exact or approximate likelihood for two different spatial processes: a Gaussian process and a Brown-Resnick process which have computationally intensive and intractable likelihoods, respectively. We conclude that our method provides fast and accurate parameter estimation with a reliable method of uncertainty quantification in situations where standard methods are either undesirably slow or inaccurate. The method is applicable to any spatial process on a grid from which fast simulations are available.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04634v4</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.spasta.2024.100848</arxiv:DOI>
      <arxiv:journal_reference>Spatial Statistics, 62:100848, 2024</arxiv:journal_reference>
      <dc:creator>Julia Walchessen, Amanda Lenzi, Mikael Kuusela</dc:creator>
    </item>
    <item>
      <title>Data fusion using weakly aligned sources</title>
      <link>https://arxiv.org/abs/2308.14836</link>
      <description>arXiv:2308.14836v2 Announce Type: replace 
Abstract: We introduce a new data fusion method that utilizes multiple data sources to estimate a smooth, finite-dimensional parameter. Most existing methods only make use of fully aligned data sources that share common conditional distributions of one or more variables of interest. However, in many settings, the scarcity of fully aligned sources can make existing methods require unduly large sample sizes to be useful. Our approach enables the incorporation of weakly aligned data sources that are not perfectly aligned, provided their degree of misalignment is known up to finite-dimensional parameters. {We quantify the additional efficiency gains achieved through the integration of these weakly aligned sources. We characterize the semiparametric efficiency bound and provide a general means to construct estimators achieving these efficiency gains.} We illustrate our results by fusing data from two harmonized HIV monoclonal antibody prevention efficacy trials to study how a neutralizing antibody biomarker associates with HIV genotype.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.14836v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sijia Li, Peter B. Gilbert, Rui Duan, Alex Luedtke</dc:creator>
    </item>
    <item>
      <title>Type I Error Rates are Not Usually Inflated</title>
      <link>https://arxiv.org/abs/2312.06265</link>
      <description>arXiv:2312.06265v5 Announce Type: replace 
Abstract: The inflation of Type I error rates is thought to be one of the causes of the replication crisis. Questionable research practices such as p-hacking are thought to inflate Type I error rates above their nominal level, leading to unexpectedly high levels of false positives in the literature and, consequently, unexpectedly low replication rates. In this article, I offer an alternative view. I argue that questionable and other research practices do not usually inflate relevant Type I error rates. I begin by introducing the concept of Type I error rates and distinguishing between statistical errors and theoretical errors. I then illustrate my argument with respect to model misspecification, multiple testing, selective inference, forking paths, exploratory analyses, p-hacking, optional stopping, double dipping, and HARKing. In each case, I demonstrate that relevant Type I error rates are not usually inflated above their nominal level, and in the rare cases that they are, the inflation is easily identified and resolved. I conclude that the replication crisis may be explained, at least in part, by researchers' misinterpretation of statistical errors and their underestimation of theoretical errors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06265v5</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.36850/4d35-44bd</arxiv:DOI>
      <arxiv:journal_reference>Journal of Trial and Error, 4(2), 46-71</arxiv:journal_reference>
      <dc:creator>Mark Rubin</dc:creator>
    </item>
    <item>
      <title>Quantification of vaccine waning as a challenge effect</title>
      <link>https://arxiv.org/abs/2405.01336</link>
      <description>arXiv:2405.01336v2 Announce Type: replace 
Abstract: Knowing whether vaccine protection wanes over time is important for health policy and drug development. However, quantifying waning effects is difficult. A simple contrast of vaccine efficacy at two different times compares different populations of individuals: those who were uninfected at the first time versus those who remain uninfected until the second time. Thus, the contrast of vaccine efficacy at early and late times can not be interpreted as a causal effect. We propose to quantify vaccine waning using the challenge effect, which is a contrast of outcomes under controlled exposures to the infectious agent following vaccination. We identify sharp bounds on the challenge effect under non-parametric assumptions that are broadly applicable in vaccine trials using routinely collected data. We demonstrate that the challenge effect can differ substantially from the conventional vaccine efficacy due to depletion of susceptible individuals from the risk set over time. Finally, we apply the methods to derive bounds on the waning of the BNT162b2 COVID-19 vaccine using data from a placebo-controlled randomized trial. Our estimates of the challenge effect suggest waning protection after 2 months beyond administration of the second vaccine dose.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01336v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1080/01621459.2024.2408776</arxiv:DOI>
      <dc:creator>Matias Janvin, Mats J. Stensrud</dc:creator>
    </item>
    <item>
      <title>Wasserstein $k$-Centres Clustering for Distributional Data</title>
      <link>https://arxiv.org/abs/2407.08228</link>
      <description>arXiv:2407.08228v3 Announce Type: replace 
Abstract: We develop a novel clustering method for distributional data, where each data point is regarded as a probability distribution on the real line. For distributional data, it has been challenging to develop a clustering method that utilizes modes of variation of the data because the space of probability distributions lacks a vector space structure, preventing the application of existing methods devised for functional data. Our clustering method for distributional data takes account of the differences in both means and modes of variation of clusters, in the spirit of the $k$-centers clustering approach proposed for functional data. Specifically, we consider the space of distributions equipped with the Wasserstein metric and define geodesic modes of variation of distributional data using the notion of geodesic principal component analysis. Then, we utilize geodesic modes of clusters to predict the cluster membership of each distribution. We theoretically show the validity of the proposed clustering criterion by studying the probability of correct membership. Through a simulation study and real data application, we demonstrate that the proposed distributional clustering method can improve the quality of the cluster compared to conventional clustering algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.08228v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryo Okano, Masaaki Imaizumi</dc:creator>
    </item>
    <item>
      <title>A multiple imputation approach to distinguish curative from life-prolonging effects in the presence of missing covariates</title>
      <link>https://arxiv.org/abs/2408.16485</link>
      <description>arXiv:2408.16485v2 Announce Type: replace 
Abstract: Medical advances have increased cancer survival rates and the possibility of finding a cure. Hence, it is crucial to evaluate the impact of treatments both in terms of cure and prolongation of survival. To achieve this, we may use a Cox proportional hazards (PH) cure model. However, a significant challenge in applying such a model is the potential presence of partially observed covariates. We aim to refine the methods for imputing partially observed covariates based on multiple imputation and fully conditional specification (FCS) approaches. To be more specific, we consider a general case in which different covariate vectors are used to model the probability of cure and the survival of patients who are not cured. We investigated the performance of the multiple imputation procedure based on the exact conditional distribution and an approximate imputation model, which helps to draw imputed values at a lower computational cost. To assess the effectiveness of these approaches, we compare them with a complete case analysis and an analysis that includes all available covariates in modelling both cure probabilities and the survival of the uncured. We discuss the application of these techniques to a real-world dataset from the BO06 clinical trial on osteosarcoma.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.16485v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Marta Cipriani, Marta Fiocco, Marco Alf\`o, Maria Quelhas, Eni Musta</dc:creator>
    </item>
    <item>
      <title>Multi-source Stable Variable Importance Measure via Adversarial Machine Learning</title>
      <link>https://arxiv.org/abs/2409.07380</link>
      <description>arXiv:2409.07380v3 Announce Type: replace 
Abstract: The quantification and inference of predictive importance for exposure covariates have recently gained significant attention in the context of interpretable machine learning. Contemporary scientific investigations often involve data originating from multiple sources with distributional heterogeneity. It is imperative to introduce a new notation of the variable importance measure that is stable across diverse environments. In this paper, we introduce MIMAL (Multi-source Importance Measure via Adversarial Learning), a novel statistical framework designed to quantify the importance of exposure variables by maximizing the worst-case predictive reward across source mixtures. The proposed framework is adaptable to a broad spectrum of machine learning methodologies for both confounding adjustment and exposure effect characterization. We establish the asymptotic normality of the data-dependent estimator of the multi-source variable importance measure under a general machine learning framework. Our framework requires the similar learning accuracy conditions compared to those required for single-source variable importance analysis. The finite-sample performance of MIMAL is demonstrated through extensive numerical studies encompassing diverse data generation scenarios and machine learning implementations. Furthermore, we illustrate the practical utility of our approach in a real-world case study of air pollution in Beijing, analyzing data collected from multiple locations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07380v3</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zitao Wang, Nian Si, Zijian Guo, Molei Liu</dc:creator>
    </item>
    <item>
      <title>Optimal item calibration in the context of the Swedish Scholastic Aptitude Test</title>
      <link>https://arxiv.org/abs/2410.09808</link>
      <description>arXiv:2410.09808v2 Announce Type: replace 
Abstract: Large scale achievement tests require the existence of item banks with items for use in future tests. Before an item is included into the bank, its characteristics need to be estimated. The process of estimating the item characteristics is called item calibration. For the quality of the future achievement tests, it is important to perform this calibration well and it is desirable to estimate the item characteristics as efficiently as possible. Methods of optimal design have been developed to allocate calibration items to examinees with the most suited ability. Theoretical evidence shows advantages with using ability-dependent allocation of calibration items. However, it is not clear whether these theoretical results hold also in a real testing situation. In this paper, we investigate the performance of an optimal ability-dependent allocation in the context of the Swedish Scholastic Aptitude Test (SweSAT) and quantify the gain from using the optimal allocation. On average over all items, we see an improved precision of calibration. While this average improvement is moderate, we are able to identify for what kind of items the method works well. This enables targeting specific item types for optimal calibration. We also discuss possibilities for improvements of the method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.09808v2</guid>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Bjermo, Ellinor Fackle Fornius, Frank Miller</dc:creator>
    </item>
    <item>
      <title>Supervised centrality via sparse network influence regression: an application to the 2021 Henan floods' social network</title>
      <link>https://arxiv.org/abs/2412.18145</link>
      <description>arXiv:2412.18145v2 Announce Type: replace 
Abstract: The social characteristics of players in a social network are closely associated with their network positions and relational importance. Identifying those influential players in a network is of great importance as it helps to understand how ties are formed, how information is propagated, and, in turn, can guide the dissemination of new information. Motivated by a Sina Weibo social network analysis of the 2021 Henan Floods, where response variables for each Sina Weibo user are available, we propose a new notion of supervised centrality that emphasizes the task-specific nature of a player's centrality. To estimate the supervised centrality and identify important players, we develop a novel sparse network influence regression by introducing individual heterogeneity for each user. To overcome the computational difficulties in fitting the model for large social networks, we further develop a forward-addition algorithm and show that it can consistently identify a superset of the influential Sina Weibo users. We apply our method to analyze three responses in the Henan Floods data: the number of comments, reposts, and likes, and obtain meaningful results. A further simulation study corroborates the developed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.18145v2</guid>
      <category>stat.ME</category>
      <category>cs.SI</category>
      <category>physics.soc-ph</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yingying Ma, Wei Lan, Chenlei Leng, Ting Li, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling</title>
      <link>https://arxiv.org/abs/2106.03322</link>
      <description>arXiv:2106.03322v4 Announce Type: replace-cross 
Abstract: Both Bayesian and varying coefficient models are very useful tools in practice as they can be used to model parameter heterogeneity in a generalizable way. Motivated by the need of enhancing Marketing Mix Modeling at Uber, we propose a Bayesian Time Varying Coefficient model, equipped with a hierarchical Bayesian structure. This model is different from other time varying coefficient models in the sense that the coefficients are weighted over a set of local latent variables following certain probabilistic distributions. Stochastic Variational Inference is used to approximate the posteriors of latent variables and dynamic coefficients. The proposed model also helps address many challenges faced by traditional MMM approaches. We used simulations as well as real world marketing datasets to demonstrate our model superior performance in terms of both accuracy and interpretability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2106.03322v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Edwin Ng, Zhishi Wang, Athena Dai</dc:creator>
    </item>
    <item>
      <title>The Effect of Omitted Variables on the Sign of Regression Coefficients</title>
      <link>https://arxiv.org/abs/2208.00552</link>
      <description>arXiv:2208.00552v3 Announce Type: replace-cross 
Abstract: We show that, depending on how the impact of omitted variables is measured, it can be substantially easier for omitted variables to flip coefficient signs than to drive them to zero. This behavior occurs with "Oster's delta" (Oster 2019), a widely reported robustness measure. Consequently, any time this measure is large -- suggesting that omitted variables may be unimportant -- a much smaller value reverses the sign of the parameter of interest. We propose a modified measure of robustness to address this concern. We illustrate our results in four empirical applications and two meta-analyses. We implement our methods in the companion Stata module regsensitivity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2208.00552v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matthew A. Masten, Alexandre Poirier</dc:creator>
    </item>
    <item>
      <title>Clustered Covariate Regression</title>
      <link>https://arxiv.org/abs/2302.09255</link>
      <description>arXiv:2302.09255v3 Announce Type: replace-cross 
Abstract: High covariate dimensionality is increasingly occurrent in model estimation, and existing techniques to address this issue typically require sparsity or discrete heterogeneity of the \emph{unobservable} parameter vector. However, neither restriction may be supported by economic theory in some empirical contexts, leading to severe bias and misleading inference. The clustering-based grouped parameter estimator (GPE) introduced in this paper drops both restrictions and maintains the natural one that the parameter support be bounded. GPE exhibits robust large sample properties under standard conditions and accommodates both sparse and non-sparse parameters whose support can be bounded away from zero. Extensive Monte Carlo simulations demonstrate the excellent performance of GPE in terms of bias reduction and size control compared to competing estimators. An empirical application of GPE to estimating price and income elasticities of demand for gasoline highlights its practical utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.09255v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.2139/ssrn.3394012</arxiv:DOI>
      <dc:creator>Abdul-Nasah Soale, Emmanuel Selorm Tsyawo</dc:creator>
    </item>
    <item>
      <title>Online Joint Assortment-Inventory Optimization under MNL Choices</title>
      <link>https://arxiv.org/abs/2304.02022</link>
      <description>arXiv:2304.02022v2 Announce Type: replace-cross 
Abstract: We study an online joint assortment-inventory optimization problem, in which we assume that the choice behavior of each customer follows the Multinomial Logit (MNL) choice model, and the attraction parameters are unknown a priori. The retailer makes periodic assortment and inventory decisions to dynamically learn from the customer choice observations about the attraction parameters while maximizing the expected total profit over time. In this paper, we propose a novel algorithm that can effectively balance exploration and exploitation in the online decision-making of assortment and inventory. Our algorithm builds on a new estimator for the MNL attraction parameters, an innovative approach to incentivize exploration by adaptively tuning certain known and unknown parameters, and an optimization oracle to static single-cycle assortment-inventory planning problems with given parameters. We establish a regret upper bound for our algorithm and a lower bound for the online joint assortment-inventory optimization problem, suggesting that our algorithm achieves nearly optimal regret rate, provided that the static optimization oracle is exact. Then we incorporate more practical approximate static optimization oracles into our algorithm, and bound from above the impact of static optimization errors on the regret of our algorithm. We perform numerical studies to demonstrate the effectiveness of our proposed algorithm.At last, we extend our study by incorporating inventory carryover and the learning of customer arrival distribution.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.02022v2</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yong Liang, Xiaojie Mao, Shiyuan Wang</dc:creator>
    </item>
    <item>
      <title>Causal Flow-based Variational Auto-Encoder for Disentangled Causal Representation Learning</title>
      <link>https://arxiv.org/abs/2304.09010</link>
      <description>arXiv:2304.09010v5 Announce Type: replace-cross 
Abstract: Disentangled representation learning aims to learn low-dimensional representations where each dimension corresponds to an underlying generative factor. While the Variational Auto-Encoder (VAE) is widely used for this purpose, most existing methods assume independence among factors, a simplification that does not hold in many real-world scenarios where factors are often interdependent and exhibit causal relationships. To overcome this limitation, we propose the Disentangled Causal Variational Auto-Encoder (DCVAE), a novel supervised VAE framework that integrates causal flows into the representation learning process, enabling the learning of more meaningful and interpretable disentangled representations. We evaluate DCVAE on both synthetic and real-world datasets, demonstrating its superior ability in causal disentanglement and intervention experiments. Furthermore, DCVAE outperforms state-of-the-art methods in various downstream tasks, highlighting its potential for learning true causal structures among factors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.09010v5</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Di Fan, Yannian Kou, Chuanhou Gao</dc:creator>
    </item>
    <item>
      <title>Average partial effect estimation using double machine learning</title>
      <link>https://arxiv.org/abs/2308.09207</link>
      <description>arXiv:2308.09207v2 Announce Type: replace-cross 
Abstract: Single-parameter summaries of variable effects are desirable for ease of interpretation, but linear models, which would deliver these, may fit poorly to the data. A modern approach is to estimate the average partial effect -- the average slope of the regression function with respect to the predictor of interest -- using a doubly robust semiparametric procedure. Most existing work has focused on specific forms of nuisance function estimators. We extend the scope to arbitrary plug-in nuisance function estimation, allowing for the use of modern machine learning methods which in particular may deliver non-differentiable regression function estimates. Our procedure involves resmoothing a user-chosen first-stage regression estimator to produce a differentiable version, and modelling the conditional distribution of the predictors through a location-scale model. We show that our proposals lead to a semiparametric efficient estimator under relatively weak assumptions. Our theory makes use of a new result on the sub-Gaussianity of Lipschitz score functions that may be of independent interest. We demonstrate the attractive numerical performance of our approach in a variety of settings including ones with misspecification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.09207v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Harvey Klyne, Rajen D. Shah</dc:creator>
    </item>
    <item>
      <title>The Fragility of Sparsity</title>
      <link>https://arxiv.org/abs/2311.02299</link>
      <description>arXiv:2311.02299v3 Announce Type: replace-cross 
Abstract: We show, using three empirical applications, that linear regression estimates which rely on the assumption of sparsity are fragile in two ways. First, we document that different choices of the regressor matrix that do not impact ordinary least squares (OLS) estimates, such as the choice of baseline category with categorical controls, can move sparsity-based estimates two standard errors or more. Second, we develop two tests of the sparsity assumption based on comparing sparsity-based estimators with (OLS). The tests tend to reject the sparsity assumption in all three applications. Unless the number of regressors is comparable to or exceeds the sample size, (OLS) yields more robust results at little efficiency cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.02299v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michal Koles\'ar, Ulrich K. M\"uller, Sebastian T. Roelsgaard</dc:creator>
    </item>
    <item>
      <title>Nonparametric Regression under Cluster Sampling</title>
      <link>https://arxiv.org/abs/2403.04766</link>
      <description>arXiv:2403.04766v2 Announce Type: replace-cross 
Abstract: This paper develops a general asymptotic theory for nonparametric kernel regression in the presence of cluster dependence. We examine nonparametric density estimation, Nadaraya-Watson kernel regression, and local linear estimation. Our theory accommodates growing and heterogeneous cluster sizes. We derive asymptotic conditional bias and variance, establish uniform consistency, and prove asymptotic normality. Our findings reveal that under heterogeneous cluster sizes, the asymptotic variance includes a new term reflecting within-cluster dependence, which is overlooked when cluster sizes are presumed to be bounded. We propose valid approaches for bandwidth selection and inference, introduce estimators of the asymptotic variance, and demonstrate their consistency. In simulations, we verify the effectiveness of the cluster-robust bandwidth selection and show that the derived cluster-robust confidence interval improves the coverage ratio. We illustrate the application of these methods using a policy-targeting dataset in development economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.04766v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuya Shimizu</dc:creator>
    </item>
    <item>
      <title>Causality Pursuit from Heterogeneous Environments via Neural Adversarial Invariance Learning</title>
      <link>https://arxiv.org/abs/2405.04715</link>
      <description>arXiv:2405.04715v3 Announce Type: replace-cross 
Abstract: Pursuing causality from data is a fundamental problem in scientific discovery, treatment intervention, and transfer learning. This paper introduces a novel algorithmic method for addressing nonparametric invariance and causality learning in regression models across multiple environments, where the joint distribution of response variables and covariates varies, but the conditional expectations of outcome given an unknown set of quasi-causal variables are invariant. The challenge of finding such an unknown set of quasi-causal or invariant variables is compounded by the presence of endogenous variables that have heterogeneous effects across different environments. The proposed Focused Adversarial Invariant Regularization (FAIR) framework utilizes an innovative minimax optimization approach that drives regression models toward prediction-invariant solutions through adversarial testing. Leveraging the representation power of neural networks, FAIR neural networks (FAIR-NN) are introduced for causality pursuit. It is shown that FAIR-NN can find the invariant variables and quasi-causal variables under a minimal identification condition and that the resulting procedure is adaptive to low-dimensional composition structures in a non-asymptotic analysis. Under a structural causal model, variables identified by FAIR-NN represent pragmatic causality and provably align with exact causal mechanisms under conditions of sufficient heterogeneity. Computationally, FAIR-NN employs a novel Gumbel approximation with decreased temperature and a stochastic gradient descent ascent algorithm. The procedures are demonstrated using simulated and real-data examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.04715v3</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yihong Gu, Cong Fang, Peter B\"uhlmann, Jianqing Fan</dc:creator>
    </item>
    <item>
      <title>Conditional Rank-Rank Regression</title>
      <link>https://arxiv.org/abs/2407.06387</link>
      <description>arXiv:2407.06387v2 Announce Type: replace-cross 
Abstract: Rank-rank regression is commonly employed in economic research as a way of capturing the relationship between two economic variables. It frequently features in studies of intergenerational mobility as the resulting coefficient, capturing the rank correlation between the variables, is easy to interpret and measures overall persistence. However, in many applications it is common practice to include other covariates to account for differences in persistence levels between groups defined by the values of these covariates. In these instances the resulting coefficients can be difficult to interpret. We propose the conditional rank-rank regression, which uses conditional ranks instead of unconditional ranks, to measure average within-group income persistence. The difference between conditional and unconditional rank-rank regression coefficients can then be interpreted as a measure of between-group persistence. We develop a flexible estimation approach using distribution regression and establish a theoretical framework for large sample inference. An empirical study on intergenerational income mobility in Switzerland demonstrates the advantages of this approach. The study reveals stronger intergenerational persistence between fathers and sons compared to fathers and daughters, with the within-group persistence explaining 62% of the overall income persistence for sons and 52% for daughters. Smaller families and those with highly educated fathers exhibit greater persistence in economic status.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.06387v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor Chernozhukov, Iv\'an Fern\'andez-Val, Jonas Meier, Aico van Vuuren, Francis Vella</dc:creator>
    </item>
    <item>
      <title>Local linear smoothing for regression surfaces on the simplex using Dirichlet kernels</title>
      <link>https://arxiv.org/abs/2408.07209</link>
      <description>arXiv:2408.07209v2 Announce Type: replace-cross 
Abstract: This paper introduces a local linear smoother for regression surfaces on the simplex. The estimator solves a least-squares regression problem weighted by a locally adaptive Dirichlet kernel, ensuring good boundary properties. Asymptotic results for the bias, variance, mean squared error, and mean integrated squared error are derived, generalizing the univariate results of Chen [Ann. Inst. Statist. Math., 54(2) (2002), pp. 312-323]. A simulation study shows that the proposed local linear estimator with Dirichlet kernel outperforms its only direct competitor in the literature, the Nadaraya-Watson estimator with Dirichlet kernel due to Bouzebda, Nezzal and Elhattab [AIMS Math., 9(9) (2024), pp. 26195-26282].</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.07209v2</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Tue, 31 Dec 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christian Genest, Fr\'ed\'eric Ouimet</dc:creator>
    </item>
  </channel>
</rss>
