<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cond-mat.dis-nn updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cond-mat.dis-nn</link>
    <description>cond-mat.dis-nn updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cond-mat.dis-nn" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 24 Sep 2024 03:17:32 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Fluctuation-learning relationship in neural networks</title>
      <link>https://arxiv.org/abs/2409.13597</link>
      <description>arXiv:2409.13597v1 Announce Type: new 
Abstract: Learning in neural systems occurs through change in synaptic connectivity that is driven by neural activity. Learning performance is influenced by both neural activity and the task to be learned. Experimental studies suggest a link between learning speed and variability in neural activity before learning. However, the theoretical basis of this relationship has remained unclear. In this work, using principles from the fluctuation-response relation in statistical physics, we derive two formulae that connect neural activity with learning speed. The first formula shows that learning speed is proportional to the variance of spontaneous neural activity and the neural response to input. The second formula, for small input, indicates that speed is proportional to the variances of spontaneous activity in both target and input directions. These formulae apply to various learning tasks governed by Hebbian or generalized learning rules. Numerical simulations confirm that these formulae are valid beyond their theoretical assumptions, even in cases where synaptic connectivity undergoes large changes. Our theory predicts that learning speed increases with the gain of neuronal activation functions and the number of pre-embedded memories, as both enhance the variance of spontaneous neural fluctuations. Additionally, the formulae reveal which input/output relationships are easier to learn, aligning with experimental data. Thus, our results provide a theoretical foundation for the quantitative relationship between pre-learning neural activity fluctuations and learning speed, offering insights into a range of empirical observations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13597v1</guid>
      <category>cond-mat.dis-nn</category>
      <category>nlin.AO</category>
      <category>q-bio.NC</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tomoki Kurikawa, Kunihiko Kaneko</dc:creator>
    </item>
    <item>
      <title>Optical training of large-scale Transformers and deep neural networks with direct feedback alignment</title>
      <link>https://arxiv.org/abs/2409.12965</link>
      <description>arXiv:2409.12965v1 Announce Type: cross 
Abstract: Modern machine learning relies nearly exclusively on dedicated electronic hardware accelerators. Photonic approaches, with low consumption and high operation speed, are increasingly considered for inference but, to date, remain mostly limited to relatively basic tasks. Simultaneously, the problem of training deep and complex neural networks, overwhelmingly performed through backpropagation, remains a significant limitation to the size and, consequently, the performance of current architectures and a major compute and energy bottleneck. Here, we experimentally implement a versatile and scalable training algorithm, called direct feedback alignment, on a hybrid electronic-photonic platform. An optical processing unit performs large-scale random matrix multiplications, which is the central operation of this algorithm, at speeds up to 1500 TeraOps. We perform optical training of one of the most recent deep learning architectures, including Transformers, with more than 1B parameters, and obtain good performances on both language and vision tasks. We study the compute scaling of our hybrid optical approach, and demonstrate a potential advantage for ultra-deep and wide neural networks, thus opening a promising route to sustain the exponential growth of modern artificial intelligence beyond traditional von Neumann approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.12965v1</guid>
      <category>cs.ET</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.LG</category>
      <category>physics.app-ph</category>
      <category>physics.optics</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ziao Wang, Kilian M\"uller, Matthew Filipovich, Julien Launay, Ruben Ohana, Gustave Pariente, Safa Mokaadi, Charles Brossollet, Fabien Moreau, Alessandro Cappelli, Iacopo Poli, Igor Carron, Laurent Daudet, Florent Krzakala, Sylvain Gigan</dc:creator>
    </item>
    <item>
      <title>Quantum resources of quantum and classical variational methods</title>
      <link>https://arxiv.org/abs/2409.13008</link>
      <description>arXiv:2409.13008v1 Announce Type: cross 
Abstract: Variational techniques have long been at the heart of atomic, solid-state, and many-body physics. They have recently extended to quantum and classical machine learning, providing a basis for representing quantum states via neural networks. These methods generally aim to minimize the energy of a given ans\"atz, though open questions remain about the expressivity of quantum and classical variational ans\"atze. The connection between variational techniques and quantum computing, through variational quantum algorithms, offers opportunities to explore the quantum complexity of classical methods. We demonstrate how the concept of non-stabilizerness, or magic, can create a bridge between quantum information and variational techniques and we show that energy accuracy is a necessary but not always sufficient condition for accuracy in non-stabilizerness. Through systematic benchmarking of neural network quantum states, matrix product states, and variational quantum methods, we show that while classical techniques are more accurate in non-stabilizerness, not accounting for the symmetries of the system can have a severe impact on this accuracy. Our findings form a basis for a universal expressivity characterization of both quantum and classical variational methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13008v1</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Spriggs, Arash Ahmadi, Bokai Chen, Eliska Greplova</dc:creator>
    </item>
    <item>
      <title>Characterising the slow dynamics of the swap Monte Carlo algorithm</title>
      <link>https://arxiv.org/abs/2409.13369</link>
      <description>arXiv:2409.13369v1 Announce Type: cross 
Abstract: The swap Monte Carlo algorithm introduces non-physical dynamic rules to accelerate the exploration of the configuration space of supercooled liquids. Its success raises deep questions regarding the nature and physical origin of the slow dynamics of dense liquids, and how it is affected by swap moves. We provide a detailed analysis of the slow dynamics generated by the swap Monte Carlo algorithm at very low temperatures in two glass-forming models. We find that the slowing down of the swap dynamics is qualitatively distinct from its local Monte Carlo counterpart, with considerably suppressed dynamic heterogeneity both at single-particle and collective levels. Our results suggest that local kinetic constraints are drastically reduced by swap moves, leading to nearly Gaussian and diffusive dynamics and weakly growing dynamic correlation lengthscales. The comparison between static and dynamic fluctuations shows that swap Monte Carlo is a nearly optimal local equilibrium algorithm, suggesting that further progress should necessarily involve collective or driven algorithms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13369v1</guid>
      <category>cond-mat.soft</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kumpei Shiraishi, Ludovic Berthier</dc:creator>
    </item>
    <item>
      <title>Deterministic versus stochastic dynamical classifiers: opposing random adversarial attacks with noise</title>
      <link>https://arxiv.org/abs/2409.13470</link>
      <description>arXiv:2409.13470v1 Announce Type: cross 
Abstract: The Continuous-Variable Firing Rate (CVFR) model, widely used in neuroscience to describe the intertangled dynamics of excitatory biological neurons, is here trained and tested as a veritable dynamically assisted classifier. To this end the model is supplied with a set of planted attractors which are self-consistently embedded in the inter-nodes coupling matrix, via its spectral decomposition. Learning to classify amounts to sculp the basin of attraction of the imposed equilibria, directing different items towards the corresponding destination target, which reflects the class of respective pertinence. A stochastic variant of the CVFR model is also studied and found to be robust to aversarial random attacks, which corrupt the items to be classified. This remarkable finding is one of the very many surprising effects which arise when noise and dynamical attributes are made to mutually resonate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13470v1</guid>
      <category>cs.LG</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.stat-mech</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Chicchi, Duccio Fanelli, Diego Febbe, Lorenzo Buffoni, Francesca Di Patti, Lorenzo Giambagli, Raffele Marino</dc:creator>
    </item>
    <item>
      <title>Self-organized attractoring in locomoting animals and robots: an emerging field</title>
      <link>https://arxiv.org/abs/2409.13581</link>
      <description>arXiv:2409.13581v1 Announce Type: cross 
Abstract: Locomotion may be induced on three levels. On a classical level, actuators and limbs follow the sequence of open-loop top-down control signals they receive. Limbs may move alternatively on their own, which implies that interlimb coordination must be mediated either by the body or via decentralized inter-limb signaling. In this case, when embodiment is present, two types of controllers are conceivable for the actuators of the limbs, local pacemaker circuits and control principles based on self-organized embodiment. The latter, self-organized control, is based on limit cycles and chaotic attractors that emerge within the feedback loop composed of controller, body, and environment. For this to happen, the sensorimotor loop must be locally closed, e.g. via propriosensation. Here we review the progress made within the framework of self-organized embodiment, with a particular focus on the concept of attractoring. This concept characterizes situations when sets of attractors combining discrete and continuous spectra are available as motor primitives for higher-order control schemes, such as kick control. In particular, we show that a simple generative principle allows for the robust formulation of self-organized embodiment. Based on the recurrent alternation between measuring the actual status of an actuator and providing a target for the actuator to achieve in the next step, we find that the mechanism leads to compliant locomotion for a range of simulated and real-world robots, which include barrel- and sphere-shaped agents, as well as wheeled and legged robots.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.13581v1</guid>
      <category>nlin.AO</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1007/978-3-031-72359-9_24</arxiv:DOI>
      <arxiv:journal_reference>Artificial Neural Networks and Machine Learning - ICANN 2024. Wand, M., Malinovska, K., Schmidhuber, J., Tetko, I.V. (eds), ICANN 2024. Lecture Notes in Computer Science, vol 15025. Springer, Cham</arxiv:journal_reference>
      <dc:creator>Bulcs\'u S\'andor, Claudius Gros</dc:creator>
    </item>
    <item>
      <title>Physics of the Edwards-Anderson Spin Glass in Dimensions $d=3,\ldots,8$ from Heuristic Ground State Optimization</title>
      <link>https://arxiv.org/abs/2407.14646</link>
      <description>arXiv:2407.14646v2 Announce Type: replace 
Abstract: We present a collection of simulations of the Edwards-Anderson lattice spin glass at $T=0$ to elucidate the nature of low-energy excitations over a range of dimensions that reach from physically realizable systems to the mean-field limit. Using heuristic methods, we sample ground states of instances to determine their energies while eliciting excitations through manipulating boundary conditions. We exploit the universality of the phase diagram of bond-diluted lattices to make such a study in higher dimensions computationally feasible. As a result, we obtain a verity of accurate exponents for domain wall stiffness and finite-size corrections that allow us to examine their dimensional behavior and their connection with predictions from mean-field theory. We also provide an experimentally testable prediction for the thermal-to-percolative crossover exponent in dilute lattices Ising spin glasses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.14646v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.3389/fphy.2024.1466987</arxiv:DOI>
      <arxiv:journal_reference>Front. Phys. 12:1466987 (2024)</arxiv:journal_reference>
      <dc:creator>Stefan Boettcher (Emory U)</dc:creator>
    </item>
    <item>
      <title>Hysteretic response to different modes of ramping an external field in sparse and dense Ising spin glasses</title>
      <link>https://arxiv.org/abs/2408.10329</link>
      <description>arXiv:2408.10329v2 Announce Type: replace 
Abstract: We consider the hysteretic behavior of Ising spin glasses at $T=0$ for various modes of driving. Previous studies mostly focused on an infinitely slow speed $\dot{H}$ by which the external field $H$ was ramped to trigger avalanches of spin flips by starting with destabilizing a single spin while few have focused on the effect of different driving methods. First, we show that this conventional protocol imposes a system size dependence. Then, we numerically analyze the response of Ising spin glasses at rates $\dot{H}$ that are fixed as well, to elucidate the differences in the response. Specifically, we compare three different modes of ramping ($\dot{H}=c/N$, $\dot{H}=c/\sqrt{N}$, and $\dot{H}=c$ for constant $c$) for two types of spin glass systems of size $N$, representing dense networks by the Sherrington-Kirkpatrick model and sparse networks by the lattice spin glass in $d=3$ dimensions known as the Edwards Anderson model. Depending on the mode of ramping, we find that the response of each system, in form of spin-flip avalanches and other observables, can vary considerably. In particular, in the $N$-independent mode applied to the lattice spin glass, which is closest to experimental reality, we observe a percolation transition with a broad avalanche distribution between phases of localized and system-spanning responses. We explore implications for combinatorial optimization problems pertaining to sparse systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.10329v2</guid>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.physa.2024.130070</arxiv:DOI>
      <arxiv:journal_reference>Physica A: Statistical Mechanics and its Applications 653, 130070 (2024)</arxiv:journal_reference>
      <dc:creator>Mahajabin Rahman (Emory U), Stefan Boettcher (Emory U)</dc:creator>
    </item>
    <item>
      <title>Krylov Delocalization/Localization across Ergodicity Breaking</title>
      <link>https://arxiv.org/abs/2403.14384</link>
      <description>arXiv:2403.14384v3 Announce Type: replace-cross 
Abstract: Krylov complexity has recently gained attention where the growth of operator complexity in time is measured in terms of the off-diagonal operator Lanczos coefficients. The operator Lanczos algorithm reduces the problem of complexity growth to a single-particle semi-infinite tight-binding chain (known as the Krylov chain). Employing the phenomenon of Anderson localization, we propose the phenomenology of inverse localization length on the Krylov chain that undergoes delocalization/localization transition on the Krylov chain while the physical system undergoes ergodicity breaking. On the Krylov chain we find delocalization in an ergodic regime, as we show for the SYK model, and localization in case of a weakly ergodicity-broken regime. Considering the dynamics beyond scrambling, we find a collapse across different operators in the ergodic regime. We test for two settings: (1) the coupled SYK model, and (2) the quantum East model. Our findings open avenues for mapping ergodicity/weak ergodicity-breaking transitions to delocalization/localization phenomenology on the Krylov chain.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.14384v3</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.str-el</category>
      <category>hep-th</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1103/PhysRevB.110.125137</arxiv:DOI>
      <arxiv:journal_reference>Phys. Rev. B 110, 125137 (2024)</arxiv:journal_reference>
      <dc:creator>Heiko Georg Menzler, Rishabh Jha</dc:creator>
    </item>
    <item>
      <title>Monte Carlo simulations of glass-forming liquids beyond Metropolis</title>
      <link>https://arxiv.org/abs/2406.19704</link>
      <description>arXiv:2406.19704v2 Announce Type: replace-cross 
Abstract: Monte Carlo simulations are widely employed to measure the physical properties of glass-forming liquids in thermal equilibrium. Combined with local Monte Carlo moves, the Metropolis algorithm can also be used to simulate the relaxation dynamics, thus offering an efficient alternative to molecular dynamics. Monte Carlo simulations are however more versatile, because carefully designed Monte Carlo algorithms can more efficiently sample the rugged free energy landscape characteristic of glassy systems. After a brief overview of Monte Carlo studies of glass-formers, we define and implement a series of Monte Carlo algorithms in a three-dimensional model of polydisperse hard spheres. We show that the standard local Metropolis algorithm is the slowest, and that implementing collective moves or breaking detailed balance enhances the efficiency of the Monte Carlo sampling. We use time correlation functions to provide a microscopic interpretation of these observations. Seventy years after its invention, the Monte Carlo method remains the most efficient and versatile tool to compute low-temperatures properties in supercooled liquids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.19704v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cond-mat.dis-nn</category>
      <category>cond-mat.soft</category>
      <category>physics.chem-ph</category>
      <category>physics.comp-ph</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1063/5.0225978</arxiv:DOI>
      <arxiv:journal_reference>J. Chem. Phys. 161, 114105 (2024)</arxiv:journal_reference>
      <dc:creator>Ludovic Berthier, Federico Ghimenti Fr\'ed\'eric van Wijland</dc:creator>
    </item>
    <item>
      <title>Thermalization propagation front and robustness against avalanches in localized systems</title>
      <link>https://arxiv.org/abs/2407.20985</link>
      <description>arXiv:2407.20985v3 Announce Type: replace-cross 
Abstract: We investigate the robustness of the many-body localized (MBL) phase to the quantum-avalanche instability by studying the dynamics of a localized spin chain coupled to a $T=\infty$ thermal bath through its leftmost site. By analyzing local magnetizations, we estimate the size of the thermalized sector of the chain and find that it increases logarithmically slowly in time. This logarithmically slow propagation of the thermalization front allows us to lower bound the slowest thermalization time, and find a broad parameter range where it scales fast enough with the system size that MBL is robust against thermalization induced by avalanches. The further finding that the imbalance -- a global quantity measuring localization -- thermalizes over a time scale exponential both in disorder strength and system size is in agreement with these results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.20985v3</guid>
      <category>quant-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annarita Scocco, Gianluca Passarelli, Mario Collura, Procolo Lucignano, Angelo Russomanno</dc:creator>
    </item>
    <item>
      <title>Topological communities in complex networks</title>
      <link>https://arxiv.org/abs/2409.02317</link>
      <description>arXiv:2409.02317v2 Announce Type: replace-cross 
Abstract: Most complex systems can be captured by graphs or networks. Networks connect nodes (e.g.\ neurons) through edges (synapses), thus summarizing the system's structure. A popular way of interrogating graphs is community detection, which uncovers sets of geometrically related nodes. {\em Geometric communities} consist of nodes ``closer'' to each other than to others in the graph. Some network features do not depend on node proximity -- rather, on them playing similar roles (e.g.\ building bridges) even if located far apart. These features can thus escape proximity-based analyses. We lack a general framework to uncover such features. We introduce {\em topological communities}, an alternative perspective to decomposing graphs. We find clusters that describe a network as much as classical communities, yet are missed by current techniques. In our framework, each graph guides our attention to its relevant features, whether geometric or topological. Our analysis complements existing ones, and could be a default method to study networks confronted without prior knowledge. Classical community detection has bolstered our understanding of biological, neural, or social systems; yet it is only half the story. Topological communities promise deep insights on a wealth of available data. We illustrate this for the global airport network, human connectomes, and others.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02317v2</guid>
      <category>physics.soc-ph</category>
      <category>cond-mat.dis-nn</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Luis F Seoane</dc:creator>
    </item>
    <item>
      <title>Fast Analysis of the OpenAI O1-Preview Model in Solving Random K-SAT Problem: Does the LLM Solve the Problem Itself or Call an External SAT Solver?</title>
      <link>https://arxiv.org/abs/2409.11232</link>
      <description>arXiv:2409.11232v2 Announce Type: replace-cross 
Abstract: In this manuscript, I present an analysis on the performance of OpenAI O1-preview model in solving random K-SAT instances for K$\in {2,3,4}$ as a function of $\alpha=M/N$ where $M$ is the number of clauses and $N$ is the number of variables of the satisfiable problem. I show that the model can call an external SAT solver to solve the instances, rather than solving them directly. Despite using external solvers, the model reports incorrect assignments as output. Moreover, I propose and present an analysis to quantify whether the OpenAI O1-preview model demonstrates a spark of intelligence or merely makes random guesses when outputting an assignment for a Boolean satisfiability problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.11232v2</guid>
      <category>cs.CL</category>
      <category>cond-mat.dis-nn</category>
      <category>cs.AI</category>
      <pubDate>Mon, 23 Sep 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Raffaele Marino</dc:creator>
    </item>
  </channel>
</rss>
