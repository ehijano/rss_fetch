<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 06 May 2024 04:00:08 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 06 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Transforming Software Development with Generative AI: Empirical Insights on Collaboration and Workflow</title>
      <link>https://arxiv.org/abs/2405.01543</link>
      <description>arXiv:2405.01543v1 Announce Type: new 
Abstract: Generative AI (GenAI) has fundamentally changed how knowledge workers, such as software developers, solve tasks and collaborate to build software products. Introducing innovative tools like ChatGPT and Copilot has created new opportunities to assist and augment software developers across various problems. We conducted an empirical study involving interviews with 13 data scientists, managers, developers, designers, and frontend developers to investigate the usage of GenAI. Our study reveals that ChatGPT signifies a paradigm shift in the workflow of software developers. The technology empowers developers by enabling them to work more efficiently, speed up the learning process, and increase motivation by reducing tedious and repetitive tasks. Moreover, our results indicate a change in teamwork collaboration due to software engineers using GenAI for help instead of asking co-workers which impacts the learning loop in agile teams.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01543v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Rasmus Ulfsnes, Nils Brede Moe, Viktoria Stray, Marianne Skarpen</dc:creator>
    </item>
    <item>
      <title>Transformational Outsourcing in IT Project Management</title>
      <link>https://arxiv.org/abs/2405.01544</link>
      <description>arXiv:2405.01544v1 Announce Type: new 
Abstract: Transformational outsourcing represents a strategic shift from traditional cost-focused outsourcing to a more profound and collaborative approach. It involves partnering with service providers to accomplish routine tasks and drive substantial organizational change and innovation. The report discusses the significance of pursuing transformational outsourcing for IT companies, highlighting its role in achieving strategic growth, competitive advantage, and cost-efficiency while enabling a focus on core competencies. It explores the pros and cons of IT outsourcing, emphasizing the benefits of cost savings, global talent access, scalability, and challenges related to quality, control, and data security. Additionally, the report identifies some critical reasons why outsourcing efforts may fail in achieving organizational goals, including poor vendor selection, communication issues, unclear objectives, resistance to change, and inadequate risk management. When carefully planned and executed, transformational outsourcing offers IT companies a pathway to enhance efficiency and foster innovation and competitiveness in a rapidly evolving technology landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01544v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mohammad Ikbal Hossain, Tanzina Sultana, Waheda Zabeen, Alexander Fosu Sarpong</dc:creator>
    </item>
    <item>
      <title>Analysing software failure using runtime verification and LTL</title>
      <link>https://arxiv.org/abs/2405.01545</link>
      <description>arXiv:2405.01545v1 Announce Type: new 
Abstract: A self-healing software system is an advanced computer program or system designed to detect, diagnose, and automatically recover from faults or errors without human intervention. These systems are typically employed in mission-critical applications where downtime can have significant financial or operational consequences. Failure detection is one of the important steps in the self-healing system. In this research, a method using runtime verification is proposed to diagnose four types of errors at the component level. The simulation on mRUBIS shows that the suggested method has the necessary efficiency in detecting the occurrence of failures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01545v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zahra Yazdanparast</dc:creator>
    </item>
    <item>
      <title>It Will Never Work in Theory</title>
      <link>https://arxiv.org/abs/2405.01546</link>
      <description>arXiv:2405.01546v1 Announce Type: new 
Abstract: We have been trying to get software engineering researchers and practitioners to talk to one another for over a decade. This paper describes what we have done, assesses our impact, and recommends an approach that we hope will have greater success.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01546v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Greg Wilson, Jorge Aranda, Michael Hoye, Brittany Johnson</dc:creator>
    </item>
    <item>
      <title>Exploring Conceptual Modeling Metaphysics: Existence Containers, Leibniz's Monads and Avicenna's Essence</title>
      <link>https://arxiv.org/abs/2405.01549</link>
      <description>arXiv:2405.01549v1 Announce Type: new 
Abstract: Requirement specifications in software engineering involve developing a conceptual model of a target domain. The model is based on ontological exploration of things in reality. Many things in such a process closely tie to problems in metaphysics, the field of inquiry of what reality fundamentally is. According to some researchers, metaphysicians are trying to develop an account of the world that properly conceptualizes the way it is, and software design is similar. Notions such as classes, object orientation, properties, instantiation, algorithms, etc. are metaphysical concepts developed many years ago. Exploring the metaphysics of such notions aims to establish quality assurance though some objective foundation not subject to misapprehensions and conventions. Much metaphysical work might best be understood as a model-building process. Here, a model is viewed as a hypothetical structure that we describe and investigate to understand more complex, real-world systems. The purpose of this paper is to enhance understanding of the metaphysical origins of conceptual modeling as exemplified by a specific proposed high-level model called thinging machines (TMs). The focus is on thimacs (things/machine) as a single category of TM modeling in the context of a two-phase world of staticity and dynamics. The general idea of this reality has been inspired by Deleuze s the virtual and related to the classical notions of Leibniz's monads and Avicenna's essence. The analysis of TMs leads to several interesting results about a thimac s nature at the static and existence levels.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01549v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sabah Al-Fedaghi</dc:creator>
    </item>
    <item>
      <title>Empirical Studies of Parameter Efficient Methods for Large Language Models of Code and Knowledge Transfer to R</title>
      <link>https://arxiv.org/abs/2405.01553</link>
      <description>arXiv:2405.01553v1 Announce Type: new 
Abstract: Recently, Large Langauge Models (LLMs) have gained a lot of attention in the Software Engineering (SE) community. LLMs or their variants pre-trained on code are used for many SE tasks. A main approach for adapting LLMs to the downstream task is to fine-tune the models. However, with having billions-parameters-LLMs, fine-tuning the models is not practical. An alternative approach is using Parameter Efficient Fine Tuning (PEFT), in which the model parameters are frozen and only a few added parameters are trained. Though the LLMs are used for programming languages such as Python and Java widely, their capability for low-resource languages is limited. In this work, we empirically study PEFT methods, LoRA and Compacter, on CodeT5 and CodeLlama. We will assess their performance compared to fully fine-tuned models, whether they can be used for knowledge transfer from natural language models to code (using T5 and Llama models), and their ability to adapt the learned knowledge to an unseen language. For the unseen language, we aim to study R, as it has a wide community. The adaptability with less computational costs makes LLMs accessible in scenarios where heavy computational resources are not available. Moreover, studying R opens new opportunities for using LLMs for other languages. We anticipate our findings to showcase the capabilities of PEFT for code LLMs for R and reveal the improvement areas.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01553v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Amirreza Esmaeili, Iman Saberi, Fatemeh H. Fard</dc:creator>
    </item>
    <item>
      <title>Semantically Aligned Question and Code Generation for Automated Insight Generation</title>
      <link>https://arxiv.org/abs/2405.01556</link>
      <description>arXiv:2405.01556v1 Announce Type: new 
Abstract: Automated insight generation is a common tactic for helping knowledge workers, such as data scientists, to quickly understand the potential value of new and unfamiliar data. Unfortunately, automated insights produced by large-language models can generate code that does not correctly correspond (or align) to the insight. In this paper, we leverage the semantic knowledge of large language models to generate targeted and insightful questions about data and the corresponding code to answer those questions. Then through an empirical study on data from Open-WikiTable, we show that embeddings can be effectively used for filtering out semantically unaligned pairs of question and code. Additionally, we found that generating questions and code together yields more diverse questions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01556v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ananya Singha, Bhavya Chopra, Anirudh Khatry, Sumit Gulwani, Austin Z. Henley, Vu Le, Chris Parnin, Mukul Singh, Gust Verbruggen</dc:creator>
    </item>
    <item>
      <title>Untangling Knots: Leveraging LLM for Error Resolution in Computational Notebooks</title>
      <link>https://arxiv.org/abs/2405.01559</link>
      <description>arXiv:2405.01559v1 Announce Type: new 
Abstract: Computational notebooks became indispensable tools for research-related development, offering unprecedented interactivity and flexibility in the development process. However, these benefits come at the cost of reproducibility and an increased potential for bugs. There are many tools for bug fixing; however, they are generally targeted at the classical linear code. With the rise of code-fluent Large Language Models, a new stream of smart bug-fixing tools has emerged. However, the applicability of those tools is still problematic for non-linear computational notebooks. In this paper, we propose a potential solution for resolving errors in computational notebooks via an iterative LLM-based agent. We discuss the questions raised by this approach and share a novel dataset of computational notebooks containing bugs to facilitate the research of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01559v1</guid>
      <category>cs.SE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantin Grotov, Sergey Titov, Yaroslav Zharov, Timofey Bryksin</dc:creator>
    </item>
    <item>
      <title>Copyright related risks in the creation and use of ML/AI systems</title>
      <link>https://arxiv.org/abs/2405.01560</link>
      <description>arXiv:2405.01560v1 Announce Type: new 
Abstract: This paper summarizes the current copyright related risks that Machine Learning (ML) and Artificial Intelligence (AI) systems (including Large Language Models --LLMs) incur. These risks affect different stakeholders: owners of the copyright of the training data, the users of ML/AI systems, the creators of trained models, and the operators of AI systems. This paper also provides an overview of ongoing legal cases in the United States related to these risks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01560v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel M. German</dc:creator>
    </item>
    <item>
      <title>Rapid Mobile App Development for Generative AI Agents on MIT App Inventor</title>
      <link>https://arxiv.org/abs/2405.01561</link>
      <description>arXiv:2405.01561v1 Announce Type: new 
Abstract: The evolution of Artificial Intelligence (AI) stands as a pivotal force shaping our society, finding applications across diverse domains such as education, sustainability, and safety. Leveraging AI within mobile applications makes it easily accessible to the public, catalyzing its transformative potential. In this paper, we present a methodology for the rapid development of AI agent applications using the development platform provided by MIT App Inventor. To demonstrate its efficacy, we share the development journey of three distinct mobile applications: SynchroNet for fostering sustainable communities; ProductiviTeams for addressing procrastination; and iHELP for enhancing community safety. All three applications seamlessly integrate a spectrum of generative AI features, leveraging OpenAI APIs. Furthermore, we offer insights gleaned from overcoming challenges in integrating diverse tools and AI functionalities, aiming to inspire young developers to join our efforts in building practical AI agent applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01561v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.10899798</arxiv:DOI>
      <arxiv:journal_reference>Journal of advances in information science and technology 2(3) 1-8, March 2024</arxiv:journal_reference>
      <dc:creator>Jaida Gao, Calab Su, Etai Miller, Kevin Lu, Yu Meng</dc:creator>
    </item>
    <item>
      <title>Prioritizing Software Requirements Using Large Language Models</title>
      <link>https://arxiv.org/abs/2405.01564</link>
      <description>arXiv:2405.01564v1 Announce Type: new 
Abstract: Large Language Models (LLMs) are revolutionizing Software Engineering (SE) by introducing innovative methods for tasks such as collecting requirements, designing software, generating code, and creating test cases, among others. This article focuses on requirements engineering, typically seen as the initial phase of software development that involves multiple system stakeholders. Despite its key role, the challenge of identifying requirements and satisfying all stakeholders within time and budget constraints remains significant. To address the challenges in requirements engineering, this study introduces a web-based software tool utilizing AI agents and prompt engineering to automate task prioritization and apply diverse prioritization techniques, aimed at enhancing project management within the agile framework. This approach seeks to transform the prioritization of agile requirements, tackling the substantial challenge of meeting stakeholder needs within set time and budget limits. Furthermore, the source code of our developed prototype is available on GitHub, allowing for further experimentation and prioritization of requirements, facilitating research and practical application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01564v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Malik Abdul Sami, Zeeshan Rasheed, Muhammad Waseem, Zheying Zhang, Tomas Herda, Pekka Abrahamsson</dc:creator>
    </item>
    <item>
      <title>The Role of Code Proficiency in the Era of Generative AI</title>
      <link>https://arxiv.org/abs/2405.01565</link>
      <description>arXiv:2405.01565v1 Announce Type: new 
Abstract: At the current pace of technological advancements, Generative AI models, including both Large Language Models and Large Multi-modal Models, are becoming integral to the developer workspace. However, challenges emerge due to the 'black box' nature of many of these models, where the processes behind their outputs are not transparent. This position paper advocates for a 'white box' approach to these generative models, emphasizing the necessity of transparency and understanding in AI-generated code to match the proficiency levels of human developers and better enable software maintenance and evolution. We outline a research agenda aimed at investigating the alignment between AI-generated code and developer skills, highlighting the importance of responsibility, security, legal compliance, creativity, and social value in software development. The proposed research questions explore the potential of white-box methodologies to ensure that software remains an inspectable, adaptable, and trustworthy asset in the face of rapid AI integration, setting a course for research that could shape the role of code proficiency into 2030 and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01565v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregorio Robles, Christoph Treude, Jesus M. Gonzalez-Barahona, Raula Gaikovina Kula</dc:creator>
    </item>
    <item>
      <title>2HCDL: Holistic Human-Centered Development Lifecycle</title>
      <link>https://arxiv.org/abs/2405.01566</link>
      <description>arXiv:2405.01566v1 Announce Type: new 
Abstract: The recent events affecting global society continuously highlight the need to change the development lifecycle of complex systems by promoting human-centered solutions that increase awareness and ensure critical properties such as security, safety, trust, transparency, and privacy. This fast abstract introduces the Holistic Human-Centered Development Lifecycle (2HCDL) methodology focused on: (i) the enforcement of human values and properties and (ii) the mitigation and prevention of critical issues for more secure, safe, trustworthy, transparent, and private development processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01566v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Said Daoudagh (CNR-ISTI, Pisa, Italy), Eda Marchetti (CNR-ISTI, Pisa, Italy), Oum-El-Kheir Aktouf (Univ. Grenoble Alpes, Grenoble INP, LCIS, Valence, France)</dc:creator>
    </item>
    <item>
      <title>CodeFort: Robust Training for Code Generation Models</title>
      <link>https://arxiv.org/abs/2405.01567</link>
      <description>arXiv:2405.01567v1 Announce Type: new 
Abstract: Code generation models are not robust to small perturbations, which often lead to inconsistent and incorrect generations and significantly degrade the performance of these models. Improving the robustness of code generation models is crucial to better user experience when these models are deployed in real-world applications. However, existing efforts have not addressed this issue for code generation models. To fill this gap, we propose CodeFort, a framework to improve the robustness of code generation models, generalizing a large variety of code perturbations to enrich the training data and enabling various robust training strategies, mixing data augmentation, batch augmentation, adversarial logits pairing, and contrastive learning, all carefully designed to support high-throughput training. Extensive evaluations show that we improve the average robust pass rates of baseline CodeGen models from 14.79 to 21.74. Notably, the improvement in robustness against code-syntax perturbations is evidenced by a significant decrease in pass rate drop from 95.04% to 53.35%</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01567v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yuhao Zhang, Shiqi Wang, Haifeng Qian, Zijian Wang, Mingyue Shang, Linbo Liu, Sanjay Krishna Gouda, Baishakhi Ray, Murali Krishna Ramanathan, Xiaofei Ma, Anoop Deoras</dc:creator>
    </item>
    <item>
      <title>Convert any android device into a programmable IoT device with the help of IoT Everywhere Framework</title>
      <link>https://arxiv.org/abs/2405.01568</link>
      <description>arXiv:2405.01568v1 Announce Type: new 
Abstract: The world around us is transforming as the field of the Internet of Things is taking over the world faster than we thought. Everyone in the tech industry is building wonderful things with the help of IoT. Smartwatches, smart coffee machines, smart television, smart homes are some of the examples. Building IoT sensor modules with sensors that connect to the internet can be very intimidating for people who have just stepped into the field. Quality components and microcontrollers can be costly too. Components such as proximity sensor, humidity sensor, air pressure sensor, accelerometer, gyroscope, flashlight, microphone, speaker, gsm module, wifi module, Bluetooth modules, and many more. But to program these we need to know java or kotlin and mobile application development. With the use of the IoT Everywhere framework and Origin programming language, one can convert any Android smartphone into an IoT device. This helps students of electrical engineering to grasp the idea of programming since it provides a lot of abstraction through simple function calls it can help to introduce programming to school students, it helps students who are fascinated by IoT and who wants to learn the basic of interfacing components or sensors and helps the student who has no access to an actual personal computer learn to program.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01568v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vishnu Joshi</dc:creator>
    </item>
    <item>
      <title>A Systematic Literature Review on Reasons and Approaches for Accurate Effort Estimations in Agile</title>
      <link>https://arxiv.org/abs/2405.01569</link>
      <description>arXiv:2405.01569v1 Announce Type: new 
Abstract: Background: Accurate effort estimation is crucial for planning in Agile iterative development. Agile estimation generally relies on consensus-based methods like planning poker, which require less time and information than other formal methods (e.g., COSMIC) but are prone to inaccuracies. Understanding the common reasons for inaccurate estimations and how proposed approaches can assist practitioners is essential. However, prior systematic literature reviews (SLR) only focus on the estimation practices (e.g., [26, 127]) and the effort estimation approaches (e.g., [6]). Aim: We aim to identify themes of reasons for inaccurate estimations and classify approaches to improve effort estimation. Method: We conducted an SLR and identified the key themes and a taxonomy. Results: The reasons for inaccurate estimation are related to information quality, team, estimation practice, project management, and business influences. The effort estimation approaches were the most investigated in the literature, while only a few aim to support the effort estimation process. Yet, few automated approaches are at risk of data leakage and indirect validation scenarios. Recommendations: Practitioners should enhance the quality of information for effort estimation, potentially by adopting an automated approach. Future research should aim to improve the information quality, while avoiding data leakage and indirect validation scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01569v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jirat Pasuksmit, Patanamon Thongtanunam, Shanika Karunasekera</dc:creator>
    </item>
    <item>
      <title>A Semi-Formal Verification Methodology for Efficient Configuration Coverage of Highly Configurable Digital Designs</title>
      <link>https://arxiv.org/abs/2405.01572</link>
      <description>arXiv:2405.01572v1 Announce Type: new 
Abstract: Nowadays, a majority of System-on-Chips (SoCs) make use of Intellectual Property (IP) in order to shorten development cycles. When such IPs are developed, one of the main focuses lies in the high configurability of the design. This flexibility on the design side introduces the challenge of covering a huge state space of IP configurations on the verification side to ensure the functional correctness under every possible parameter setting. The vast number of possibilities does not allow a brute-force approach, and therefore, only a selected number of settings based on typical and extreme assumptions are usually verified. Especially in automotive applications, which need to follow the ISO 26262 functional safety standard, the requirement of covering all significant variants needs to be fulfilled in any case. State-of-the-Art existing verification techniques such as simulation-based verification and formal verification have challenges such as time-space explosion and state-space explosion respectively and therefore, lack behind in verifying highly configurable digital designs efficiently. This paper is focused on a semi-formal verification methodology for efficient configuration coverage of highly configurable digital designs. The methodology focuses on reduced runtime based on simulative and formal methods that allow high configuration coverage. The paper also presents the results when the developed methodology was applied on a highly configurable microprocessor IP and discusses the gained benefits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01572v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.AR</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aman Kumar, Sebastian Simon</dc:creator>
    </item>
    <item>
      <title>Class-Level Code Generation from Natural Language Using Iterative, Tool-Enhanced Reasoning over Repository</title>
      <link>https://arxiv.org/abs/2405.01573</link>
      <description>arXiv:2405.01573v1 Announce Type: new 
Abstract: LLMs have demonstrated significant potential in code generation tasks, achieving promising results at the function or statement level in various benchmarks. However, the complexities associated with creating code artifacts like classes, particularly within the context of real-world software repositories, remain underexplored. Existing research often treats class-level generation as an isolated task, neglecting the intricate dependencies and interactions that characterize real-world software development environments. To address this gap, we introduce RepoClassBench, a benchmark designed to rigorously evaluate LLMs in generating complex, class-level code within real-world repositories. RepoClassBench includes natural language to class generation tasks across Java and Python, from a selection of public repositories. We ensure that each class in our dataset not only has cross-file dependencies within the repository but also includes corresponding test cases to verify its functionality. We find that current models struggle with the realistic challenges posed by our benchmark, primarily due to their limited exposure to relevant repository contexts. To address this shortcoming, we introduce Retrieve-Repotools-Reflect (RRR), a novel approach that equips LLMs with static analysis tools to iteratively navigate &amp; reason about repository-level context in an agent-based framework. Our experiments demonstrate that RRR significantly outperforms existing baselines on RepoClassBench, showcasing its effectiveness across programming languages and in various settings. Our findings emphasize the need for benchmarks that incorporate repository-level dependencies to more accurately reflect the complexities of software development. Our work illustrates the benefits of leveraging specialized tools to enhance LLMs understanding of repository context. We plan to make our dataset and evaluation harness public.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01573v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ajinkya Deshpande, Anmol Agarwal, Shashank Shet, Arun Iyer, Aditya Kanade, Ramakrishna Bairi, Suresh Parthasarathy</dc:creator>
    </item>
    <item>
      <title>On Using Agent-based Modeling and Simulation for Studying Blockchain Systems</title>
      <link>https://arxiv.org/abs/2405.01574</link>
      <description>arXiv:2405.01574v1 Announce Type: new 
Abstract: There is a need for a simulation framework, which is develop as a software using modern engineering approaches (e.g., modularity --i.e., model reuse--, testing, continuous development and continuous integration, automated management of builds, dependencies and documentation) and agile principles, (1) to make rapid prototyping of industrial cases and (2) to carry out their feasibility analysis in a realistic manner (i.e., to test hypothesis by simulating complex experiments involving large numbers of participants of different types acting in one or several blockchain systems).</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01574v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>\"Onder G\"urcan</dc:creator>
    </item>
    <item>
      <title>Software Mention Recognition with a Three-Stage Framework Based on BERTology Models at SOMD 2024</title>
      <link>https://arxiv.org/abs/2405.01575</link>
      <description>arXiv:2405.01575v1 Announce Type: new 
Abstract: This paper describes our systems for the sub-task I in the Software Mention Detection in Scholarly Publications shared-task. We propose three approaches leveraging different pre-trained language models (BERT, SciBERT, and XLM-R) to tackle this challenge. Our bestperforming system addresses the named entity recognition (NER) problem through a three-stage framework. (1) Entity Sentence Classification - classifies sentences containing potential software mentions; (2) Entity Extraction - detects mentions within classified sentences; (3) Entity Type Classification - categorizes detected mentions into specific software types. Experiments on the official dataset demonstrate that our three-stage framework achieves competitive performance, surpassing both other participating teams and our alternative approaches. As a result, our framework based on the XLM-R-based model achieves a weighted F1-score of 67.80%, delivering our team the 3rd rank in Sub-task I for the Software Mention Recognition task.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01575v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thuy Nguyen Thi, Anh Nguyen Viet, Thin Dang Van, Ngan Nguyen Luu Thuy</dc:creator>
    </item>
    <item>
      <title>Empowering IoT Applications with Flexible, Energy-Efficient Remote Management of Low-Power Edge Devices</title>
      <link>https://arxiv.org/abs/2405.01578</link>
      <description>arXiv:2405.01578v1 Announce Type: new 
Abstract: In the context of the Internet of Things (IoT), reliable and energy-efficient provision of IoT applications has become critical. Equipping IoT systems with tools that enable a flexible, well-performing, and automated way of monitoring and managing IoT edge devices is an essential prerequisite. In current IoT systems, low-power edge appliances have been utilized in a way that can not be controlled and re-configured in a timely manner. Hence, conducting a trade-off solution between manageability, performance and design requirements are demanded. This paper introduces a novel approach for fine-grained monitoring and managing individual micro-services within low-power edge devices, which improves system reliability and energy efficiency. The proposed method enables operational flexibility for IoT edge devices by leveraging a modularization technique. Following a review of existing solutions for remote-managed IoT services, a detailed description of the suggested approach is presented. Also, to explore the essential design principles that must be considered in this approach, the suggested architecture is elaborated in detail. Finally, the advantages of the proposed solution to deal with disruptions are demonstrated in the proof of concept-based experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01578v1</guid>
      <category>cs.SE</category>
      <category>cs.NI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shadi Attarha, Anna F\"orster</dc:creator>
    </item>
    <item>
      <title>Mining patterns in syntax trees to automate code reviews of student solutions for programming exercises</title>
      <link>https://arxiv.org/abs/2405.01579</link>
      <description>arXiv:2405.01579v1 Announce Type: new 
Abstract: In programming education, providing manual feedback is essential but labour-intensive, posing challenges in consistency and timeliness. We introduce ECHO, a machine learning method to automate the reuse of feedback in educational code reviews by analysing patterns in abstract syntax trees. This study investigates two primary questions: whether ECHO can predict feedback annotations to specific lines of student code based on previously added annotations by human reviewers (RQ1), and whether its training and prediction speeds are suitable for using ECHO for real-time feedback during live code reviews by human reviewers (RQ2). Our results, based on annotations from both automated linting tools and human reviewers, show that ECHO can accurately and quickly predict appropriate feedback annotations. Its efficiency in processing and its flexibility in adapting to feedback patterns can significantly reduce the time and effort required for manual feedback provisioning in educational settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01579v1</guid>
      <category>cs.SE</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Charlotte Van Petegem, Kasper Demeyere, Rien Maertens, Niko Strijbol, Bram De Wever, Bart Mesuere, Peter Dawyndt</dc:creator>
    </item>
    <item>
      <title>On the Limitations of Embedding Based Methods for Measuring Functional Correctness for Code Generation</title>
      <link>https://arxiv.org/abs/2405.01580</link>
      <description>arXiv:2405.01580v1 Announce Type: new 
Abstract: The task of code generation from natural language (NL2Code) has become extremely popular, especially with the advent of Large Language Models (LLMs). However, efforts to quantify and track this progress have suffered due to a lack of reliable metrics for functional correctness. While popular benchmarks like HumanEval have test cases to enable reliable evaluation of correctness, it is time-consuming and requires human effort to collect test cases. As an alternative several reference-based evaluation metrics have been proposed, with embedding-based metrics like CodeBERTScore being touted as having a high correlation with human preferences and functional correctness. In our work, we analyze the ability of embedding-based metrics like CodeBERTScore to measure functional correctness and other helpful constructs like editing effort by analyzing outputs of ten models over two popular code generation benchmarks. Our results show that while they have a weak correlation with functional correctness (0.16), they are strongly correlated (0.72) with editing effort.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01580v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Atharva Naik</dc:creator>
    </item>
    <item>
      <title>A Comprehensive Study on Automated Testing with the Software Lifecycle</title>
      <link>https://arxiv.org/abs/2405.01608</link>
      <description>arXiv:2405.01608v1 Announce Type: new 
Abstract: The software development lifecycle depends heavily on the testing process, which is an essential part of finding issues and reviewing the quality of software. Software testing can be done in two ways: manually and automatically. With an emphasis on its primary function within the software lifecycle, the relevance of testing in general, and the advantages that come with it, this article aims to give a thorough review of automated testing. Finding time- and cost-effective methods for software testing. The research examines how automated testing makes it easier to evaluate software quality, how it saves time as compared to manual testing, and how it differs from each of them in terms of benefits and drawbacks. The process of testing software applications is simplified, customized to certain testing situations, and can be successfully carried out by using automated testing tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01608v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hussein Mohammed Ali, Mahmood Yashar Hamza, Tarik Ahmed Rashid</dc:creator>
    </item>
    <item>
      <title>Effective Delegation and Leadership in Software Management</title>
      <link>https://arxiv.org/abs/2405.01612</link>
      <description>arXiv:2405.01612v1 Announce Type: new 
Abstract: Delegation and leadership are critical components of software management, as they play a crucial role in determining the success of the software development process. This study examined the relationship between delegation and leadership in software management and the impact of these factors on project outcomes. Results showed that effective delegation and transformational leadership styles can improve workflow, enhance team motivation and productivity, and ultimately lead to successful software development projects. The findings of this study have important implications for software management practices, as they suggest that organizations and software managers should prioritize the development of effective delegation and leadership practices to ensure the success of their software development initiatives. Further research is needed to explore the complex interplay between delegation and leadership in software management and to identify best practices for improving these processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01612v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Star Dawood Mirkhan, Skala Kamaran Omer, Hussein Mohammed Ali, Mahmood Yashar Hamza, Tarik Ahmed Rashid, Poornima Nedunchezhian</dc:creator>
    </item>
    <item>
      <title>Requirements-driven Slicing of Simulink Models Using LLMs</title>
      <link>https://arxiv.org/abs/2405.01695</link>
      <description>arXiv:2405.01695v1 Announce Type: new 
Abstract: Model slicing is a useful technique for identifying a subset of a larger model that is relevant to fulfilling a given requirement. Notable applications of slicing include reducing inspection effort when checking design adequacy to meet requirements of interest and when conducting change impact analysis. In this paper, we present a method based on large language models (LLMs) for extracting model slices from graphical Simulink models. Our approach converts a Simulink model into a textual representation, uses an LLM to identify the necessary Simulink blocks for satisfying a specific requirement, and constructs a sound model slice that incorporates the blocks identified by the LLM. We explore how different levels of granularity (verbosity) in transforming Simulink models into textual representations, as well as the strategy used to prompt the LLM, impact the accuracy of the generated slices. Our preliminary findings suggest that prompts created by textual representations that retain the syntax and semantics of Simulink blocks while omitting visual rendering information of Simulink models yield the most accurate slices. Furthermore, the chain-of-thought and zero-shot prompting strategies result in the largest number of accurate model slices produced by our approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01695v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dipeeka Luitel, Shiva Nejati, Mehrdad Sabetzadeh</dc:creator>
    </item>
    <item>
      <title>Towards A Double-Edged Sword: Modelling the Impact in Agile Software Development</title>
      <link>https://arxiv.org/abs/2405.01757</link>
      <description>arXiv:2405.01757v1 Announce Type: new 
Abstract: Agile methods are state of the art in software development. Companies worldwide apply agile to counter the dynamics of the markets. We know, that various factors like culture influence the successfully application of agile methods in practice and the sucess is differing from company to company. To counter these problems, we combine two causal models presented in literature: The Agile Practices Impact Model and the Model of Cultural Impact. In this paper, we want to better understand the two facets of factors in agile: Those influencing their application and those impacting the results when applying them. This papers core contribution is the Agile Influence and Imact Model, describing the factors influencing agile elements and the impact on specific characteristics in a systematic manner.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01757v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Neumann, Philipp Diebold</dc:creator>
    </item>
    <item>
      <title>How to Gain Commit Rights in Modern Top Open Source Communities?</title>
      <link>https://arxiv.org/abs/2405.01803</link>
      <description>arXiv:2405.01803v1 Announce Type: new 
Abstract: The success of open source software (OSS) projects relies on voluntary contributions from various community roles.Being a committer signifies gaining trust and higher privileges. Substantial studies have focused on the requirements of becoming a committer, but most of them are based on interviews or several hypotheses, lacking a comprehensive understanding of committers' qualifications.We explore both the policies and practical implementations of committer qualifications in modern top OSS communities. Through a thematic analysis of these policies, we construct a taxonomy of committer qualifications, consisting of 26 codes categorized into nine themes, including Personnel-related to Project, Communication, and Long-term Participation. We also highlight the variations in committer qualifications emphasized in different OSS community governance models. For example, projects following the core maintainer model value project comprehension, while projects following the company-backed model place significant emphasis on user issue resolution. Then, we propose eight sets of metrics and perform survival analysis on two representative OSS projects to understand how these qualifications are implemented in practice. We find that the probability of gaining commit rights decreases as participation time passes.The selection criteria in practice are generally consistent with the community policies. Developers who submit high-quality code, actively engage in code review, and make extensive contributions to related projects are more likely to be granted commit rights. However, there are some qualifications that do not align precisely, and some are not adequately evaluated. This study contributes to the understanding of trust establishment in modern top OSS communities, assists communities in better allocating commit rights, and supports developers in achieving self-actualization through OSS participation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01803v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3660784</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the ACM on Software Engineering (PACMSE) Issue FSE 2024</arxiv:journal_reference>
      <dc:creator>Xin Tan, Yan Gong, Geyu Huang, Haohua Wu, Li Zhang</dc:creator>
    </item>
    <item>
      <title>Automated Control Logic Test Case Generation using Large Language Models</title>
      <link>https://arxiv.org/abs/2405.01874</link>
      <description>arXiv:2405.01874v1 Announce Type: new 
Abstract: Testing PLC and DCS control logic in industrial automation is laborious and challenging since appropriate test cases are often complex and difficult to formulate. Researchers have previously proposed several automated test case generation approaches for PLC software applying symbolic execution and search-based techniques. Often requiring formal specifications and performing a mechanical analysis of programs, these approaches may uncover specific programming errors but sometimes suffer from state space explosion and cannot process rather informal specifications. We proposed a novel approach for the automatic generation of PLC test cases that queries a Large Language Model (LLM) to synthesize test cases for code provided in a prompt. Experiments with ten open-source function blocks from the OSCAT automation library showed that the approach is fast, easy to use, and can yield test cases with high statement coverage for low-to-medium complex programs. However, we also found that LLM-generated test cases suffer from erroneous assertions in many cases, which still require manual adaption.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01874v1</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Heiko Koziolek, Virendra Ashiwal, Soumyadip Bandyopadhyay, Chandrika K R</dc:creator>
    </item>
    <item>
      <title>Advanced Detection of Source Code Clones via an Ensemble of Unsupervised Similarity Measures</title>
      <link>https://arxiv.org/abs/2405.02095</link>
      <description>arXiv:2405.02095v1 Announce Type: new 
Abstract: The capability of accurately determining code similarity is crucial in many tasks related to software development. For example, it might be essential to identify code duplicates for performing software maintenance. This research introduces a novel ensemble learning approach for code similarity assessment, combining the strengths of multiple unsupervised similarity measures. The key idea is that the strengths of a diverse set of similarity measures can complement each other and mitigate individual weaknesses, leading to improved performance. Preliminary results show that while Transformers-based CodeBERT and its variant GraphCodeBERT are undoubtedly the best option in the presence of abundant training data, in the case of specific small datasets (up to 500 samples), our ensemble achieves similar results, without prejudice to the interpretability of the resulting solution, and with a much lower associated carbon footprint due to training. The source code of this novel approach can be downloaded from https://github.com/jorge-martinez-gil/ensemble-codesim.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02095v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jorge Martinez-Gil</dc:creator>
    </item>
    <item>
      <title>Automatic Programming: Large Language Models and Beyond</title>
      <link>https://arxiv.org/abs/2405.02213</link>
      <description>arXiv:2405.02213v1 Announce Type: new 
Abstract: Automatic programming has seen increasing popularity due to the emergence of tools like GitHub Copilot which rely on Large Language Models (LLMs). At the same time, automatically generated code faces challenges during deployment due to concerns around quality and trust. In this article, we study automated coding in a general sense and study the concerns around code quality, security and related issues of programmer responsibility. These are key issues for organizations while deciding on the usage of automatically generated code. We discuss how advances in software engineering such as program repair and analysis can enable automatic programming. We conclude with a forward looking view, focusing on the programming environment of the near future, where programmers may need to switch to different roles to fully utilize the power of automatic programming. Automated repair of automatically generated programs from LLMs, can help produce higher assurance code from LLMs, along with evidence of assurance</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.02213v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael R. Lyu, Baishakhi Ray, Abhik Roychoudhury, Shin Hwei Tan, Patanamon Thongtanunam</dc:creator>
    </item>
    <item>
      <title>WitheredLeaf: Finding Entity-Inconsistency Bugs with LLMs</title>
      <link>https://arxiv.org/abs/2405.01668</link>
      <description>arXiv:2405.01668v1 Announce Type: cross 
Abstract: Originating from semantic bugs, Entity-Inconsistency Bugs (EIBs) involve misuse of syntactically valid yet incorrect program entities, such as variable identifiers and function names, which often have security implications. Unlike straightforward syntactic vulnerabilities, EIBs are subtle and can remain undetected for years. Traditional detection methods, such as static analysis and dynamic testing, often fall short due to the versatile and context-dependent nature of EIBs. However, with advancements in Large Language Models (LLMs) like GPT-4, we believe LLM-powered automatic EIB detection becomes increasingly feasible through these models' semantics understanding abilities. This research first undertakes a systematic measurement of LLMs' capabilities in detecting EIBs, revealing that GPT-4, while promising, shows limited recall and precision that hinder its practical application. The primary problem lies in the model's tendency to focus on irrelevant code snippets devoid of EIBs. To address this, we introduce a novel, cascaded EIB detection system named WitheredLeaf, which leverages smaller, code-specific language models to filter out most negative cases and mitigate the problem, thereby significantly enhancing the overall precision and recall. We evaluated WitheredLeaf on 154 Python and C GitHub repositories, each with over 1,000 stars, identifying 123 new flaws, 45% of which can be exploited to disrupt the program's normal operations. Out of 69 submitted fixes, 27 have been successfully merged.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01668v1</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hongbo Chen, Yifan Zhang, Xing Han, Huanyao Rong, Yuheng Zhang, Tianhao Mao, Hang Zhang, XiaoFeng Wang, Luyi Xing, Xun Chen</dc:creator>
    </item>
    <item>
      <title>Towards Neural Synthesis for SMT-Assisted Proof-Oriented Programming</title>
      <link>https://arxiv.org/abs/2405.01787</link>
      <description>arXiv:2405.01787v1 Announce Type: cross 
Abstract: Proof-oriented programs mix computational content with proofs of program correctness. However, the human effort involved in programming and proving is still substantial, despite the use of Satisfiability Modulo Theories (SMT) solvers to automate proofs in languages such as F*.
  Seeking to spur research on using AI to automate the construction of proof-oriented programs, we curate a dataset of 600K lines of open-source F* programs and proofs, including software used in production systems ranging from Windows and Linux, to Python and Firefox. Our dataset includes around 32K top-level F* definitions, each representing a type-directed program and proof synthesis problem -- producing a definition given a formal specification expressed as an F* type. We provide a program-fragment checker that queries F* to check the correctness of candidate solutions. We believe this is the largest corpus of SMT-assisted program proofs coupled with a reproducible program-fragment checker.
  Grounded in this dataset, we investigate the use of AI to synthesize programs and their proofs in F*, with promising results. Our main finding in that the performance of fine-tuned smaller language models (such as Phi-2 or StarCoder) compare favorably with large language models (such as GPT-4), at a much lower computational cost. We also identify various type-based retrieval augmentation techniques and find that they boost performance significantly. With detailed error analysis and case studies, we identify potential strengths and weaknesses of models and techniques and suggest directions for future improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01787v1</guid>
      <category>cs.PL</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Saikat Chakraborty, Gabriel Ebner, Siddharth Bhat, Sarah Fakhoury, Sakina Fatima, Shuvendu Lahiri, Nikhil Swamy</dc:creator>
    </item>
    <item>
      <title>LTM: Scalable and Black-box Similarity-based Test Suite Minimization based on Language Models</title>
      <link>https://arxiv.org/abs/2304.01397</link>
      <description>arXiv:2304.01397v3 Announce Type: replace 
Abstract: Test suites tend to grow when software evolves, making it often infeasible to execute all test cases with the allocated testing budgets, especially for large software systems. Test suite minimization (TSM) is employed to improve the efficiency of software testing by removing redundant test cases, thus reducing testing time and resources, while maintaining the fault detection capability of the test suite. Most existing TSM approaches rely on code coverage (white-box) or model-based features, which are not always available to test engineers. Recent TSM approaches that rely only on test code (black-box) have been proposed, such as ATM and FAST-R. To address the scalability, we propose LTM (Language model-based Test suite Minimization), a novel, scalable, and black-box similarity-based TSM approach based on large language models (LLMs), which is the first application of LLMs in the context of TSM. To support similarity measurement for test code embeddings, we investigate five pre-trained language models: CodeBERT, GraphCodeBERT, UniXcoder, StarEncoder, and CodeLlama, on which we compute two similarity measures: Cosine Similarity and Euclidean Distance. Our goal is to find similarity measures that are not only computationally more efficient but can also better guide a Genetic Algorithm (GA) to search for optimal minimized test suites, thus reducing the overall search time. Experimental results show that the best configuration of LTM (UniXcoder/Cosine) outperforms ATM in three aspects: (a) achieving a slightly greater saving rate of testing time (41.72% versus 40.29%, on average); (b) attaining a significantly higher fault detection rate (0.84 versus 0.81, on average); and, most importantly, (c) minimizing test suites nearly five times faster on average, with higher gains for larger test suites and systems, thus achieving much higher scalability.</description>
      <guid isPermaLink="false">oai:arXiv.org:2304.01397v3</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rongqi Pan, Taher A. Ghaleb, Lionel Briand</dc:creator>
    </item>
    <item>
      <title>GuardRails: Automated Suggestions for Clarifying Ambiguous Purpose Statements</title>
      <link>https://arxiv.org/abs/2312.08189</link>
      <description>arXiv:2312.08189v2 Announce Type: replace 
Abstract: Before implementing a function, programmers are encouraged to write a purpose statement i.e., a short, natural-language explanation of what the function computes. A purpose statement may be ambiguous i.e., it may fail to specify the intended behaviour when two or more inequivalent computations are plausible on certain inputs. Our paper makes four contributions. First, we propose a novel heuristic that suggests such inputs using Large Language Models (LLMs). Using these suggestions, the programmer may choose to clarify the purpose statement (e.g., by providing a functional example that specifies the intended behaviour on such an input). Second, to assess the quality of inputs suggested by our heuristic, and to facilitate future research, we create an open dataset of purpose statements with known ambiguities. Third, we compare our heuristic against GitHub Copilot's Chat feature, which can suggest similar inputs when prompted to generate unit tests. Fourth, we provide an open-source implementation of our heuristic as an extension to Visual Studio Code for the Python programming language, where purpose statements and functional examples are specified as docstrings and doctests respectively. We believe that this tool will be particularly helpful to novice programmers and instructors.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.08189v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1145/3627217.3627234</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 16th Annual ACM India Compute Conference (2023) 55-60</arxiv:journal_reference>
      <dc:creator>Mrigank Pawagi, Viraj Kumar</dc:creator>
    </item>
    <item>
      <title>Practical Guidelines for the Selection and Evaluation of Natural Language Processing Techniques in Requirements Engineering</title>
      <link>https://arxiv.org/abs/2401.01508</link>
      <description>arXiv:2401.01508v2 Announce Type: replace 
Abstract: [Context and Motivation] Natural Language Processing (NLP) is now a cornerstone of requirements automation. One compelling factor behind the growing adoption of NLP in Requirements Engineering (RE) is the prevalent use of natural language (NL) for specifying requirements in industry. NLP techniques are commonly used for automatically classifying requirements, extracting important information, e.g., domain models and glossary terms, and performing quality assurance tasks, such as ambiguity handling and completeness checking. With so many different NLP solution strategies available and the possibility of applying machine learning alongside, it can be challenging to choose the right strategy for a specific RE task and to evaluate the resulting solution in an empirically rigorous manner. [Content] In this chapter, we present guidelines for the selection of NLP techniques as well as for their evaluation in the context of RE. In particular, we discuss how to choose among different strategies such as traditional NLP, feature-based machine learning, and language-model-based methods. [Contribution] Our ultimate hope for this chapter is to serve as a stepping stone, assisting newcomers to NLP4RE in quickly initiating themselves into the NLP technologies most pertinent to the RE field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.01508v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mehrdad Sabetzadeh, Chetan Arora</dc:creator>
    </item>
    <item>
      <title>LangProp: A code optimization framework using Large Language Models applied to driving</title>
      <link>https://arxiv.org/abs/2401.10314</link>
      <description>arXiv:2401.10314v2 Announce Type: replace 
Abstract: We propose LangProp, a framework for iteratively optimizing code generated by large language models (LLMs), in both supervised and reinforcement learning settings. While LLMs can generate sensible coding solutions zero-shot, they are often sub-optimal. Especially for code generation tasks, it is likely that the initial code will fail on certain edge cases. LangProp automatically evaluates the code performance on a dataset of input-output pairs, catches any exceptions, and feeds the results back to the LLM in the training loop, so that the LLM can iteratively improve the code it generates. By adopting a metric- and data-driven training paradigm for this code optimization procedure, one could easily adapt findings from traditional machine learning techniques such as imitation learning, DAgger, and reinforcement learning. We show LangProp's applicability to general domains such as Sudoku and CartPole, as well as demonstrate the first proof of concept of automated code optimization for autonomous driving in CARLA. We show that LangProp can generate interpretable and transparent policies that can be verified and improved in a metric- and data-driven way. Our code is available at https://github.com/shuishida/LangProp.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.10314v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.RO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shu Ishida, Gianluca Corrado, George Fedoseev, Hudson Yeo, Lloyd Russell, Jamie Shotton, Jo\~ao F. Henriques, Anthony Hu</dc:creator>
    </item>
    <item>
      <title>Public-private funding models in open source software development: A case study on scikit-learn</title>
      <link>https://arxiv.org/abs/2404.06484</link>
      <description>arXiv:2404.06484v5 Announce Type: replace 
Abstract: Governments are increasingly funding open source software (OSS) development to support software security, digital sovereignty, and national competitiveness in science and innovation, amongst others. However, little is known about how OSS developers evaluate the relative benefits and drawbacks of governmental funding for OSS. This study explores this question through a case study on scikit-learn, a Python library for machine learning, funded by public research grants, commercial sponsorship, micro-donations, and a 32 euro million grant announced in France's artificial intelligence strategy. Through 25 interviews with scikit-learn's maintainers and funders, this study makes two key contributions. First, it contributes empirical findings about the benefits and drawbacks of public and private funding in an impactful OSS project, and the governance protocols employed by the maintainers to balance the diverse interests of their community and funders. Second, it offers practical lessons on funding for OSS developers, governments, and companies based on the experience of scikit-learn. The paper concludes with key recommendations for practitioners and future research directions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.06484v5</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cailean Osborne</dc:creator>
    </item>
    <item>
      <title>Utilizing Deep Learning to Optimize Software Development Processes</title>
      <link>https://arxiv.org/abs/2404.13630</link>
      <description>arXiv:2404.13630v2 Announce Type: replace 
Abstract: This study explores the application of deep learning technologies in software development processes, particularly in automating code reviews, error prediction, and test generation to enhance code quality and development efficiency. Through a series of empirical studies, experimental groups using deep learning tools and control groups using traditional methods were compared in terms of code error rates and project completion times. The results demonstrated significant improvements in the experimental group, validating the effectiveness of deep learning technologies. The research also discusses potential optimization points, methodologies, and technical challenges of deep learning in software development, as well as how to integrate these technologies into existing software development workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.13630v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.5281/zenodo.11004006</arxiv:DOI>
      <dc:creator>Keqin Li, Armando Zhu, Peng Zhao, Jintong Song, Jiabei Liu</dc:creator>
    </item>
    <item>
      <title>Automatic Build Repair for Test Cases using Incompatible Java Versions</title>
      <link>https://arxiv.org/abs/2404.17818</link>
      <description>arXiv:2404.17818v2 Announce Type: replace 
Abstract: Context: Bug bisection is a common technique used to identify a revision that introduces a bug or indirectly fixes a bug, and often involves executing multiple revisions of a project to determine whether the bug is present within the revision. However, many legacy revisions often cannot be successfully compiled due to changes in the programming language or tools used in the compilation process, adding complexity and preventing automation in the bisection process.
  Objective: In this paper, we introduce an approach to repair test cases of Java projects by performing dependency minimization. Our approach aims to remove classes and methods that are not required for the execution of one or more test cases. Unlike existing state-of-the-art techniques, our approach performs minimization at source-level, which allows compile-time errors to be fixed.
  Method: A standalone Java tool implementing our technique was developed, and we evaluated our technique using subjects from Defects4J retargeted against Java 8 and 17.
  Results: Our evaluation showed that a majority of subjects can be repaired solely by performing minimization, including replicating the test results of the original version. Furthermore, our technique is also shown to achieve accurate minimized results, while only adding a small overhead to the bisection process.
  Conclusion: Our proposed technique is shown to be effective for repairing build failures with minimal overhead, making it suitable for use in automated bug bisection. Our tool can also be adapted for use cases such as bug corpus creation and refactoring.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17818v2</guid>
      <category>cs.SE</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.infsof.2024.107473</arxiv:DOI>
      <dc:creator>Ching Hang Mak, Shing-Chi Cheung</dc:creator>
    </item>
    <item>
      <title>"Sometimes You Just Gotta Risk It for the Biscuit": A Portrait of Student Risk-Taking</title>
      <link>https://arxiv.org/abs/2405.01477</link>
      <description>arXiv:2405.01477v2 Announce Type: replace 
Abstract: Understanding how individuals, including students, make decisions involving risk is a fundamental aspect of behavioral research. Despite the ubiquity of risk in various aspects of life, limited empirical work has explored student risk-taking behavior in computing education. This study aims to partially replicate prior research on risk-taking behavior in software engineers while focusing on students, shedding light on the factors that affect their risk-taking choices. In our work, students were presented with a hypothetical scenario related to meeting a course project deadline, where they had to choose between a risky option and a safer alternative. We examined several factors that might influence these choices, including the framing of the decision (as a potential gain or loss), students' enjoyment of programming, perceived difficulty of programming, and their academic performance in the course. Our findings reveal intriguing insights into student risk-taking behavior. First, similar to software engineers in prior work, the framing of the decision significantly impacted the choices students made, with the loss framing leading to a higher likelihood for risky choices. Surprisingly, students displayed a greater inclination towards risk-taking compared to their professional counterparts in prior research. Furthermore, we observed that students' prior academic performance in the course and their enjoyment of programming had a subtle influence on their risk-taking tendencies, with better-performing students and those who enjoyed programming being marginally more prone to taking risks. Notably, we did not find statistically significant correlations between perceived difficulty of programming and risk-taking behavior among students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01477v2</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juho Leinonen, Paul Denny</dc:creator>
    </item>
    <item>
      <title>Numeric Truncation Security Predicate</title>
      <link>https://arxiv.org/abs/2312.06425</link>
      <description>arXiv:2312.06425v2 Announce Type: replace-cross 
Abstract: Numeric truncation is a widely spread error in software written in languages with static data typing, such as C/C++ or Java. It occurs when the significant bits of the value with a bigger type size are truncated during value conversion to the smaller type. Utilizing one of the most powerful methods for path exploration and automated bug detection called dynamic symbolic execution (DSE), we propose the symbolic security predicate for numeric truncation error detection, developed on top of DSE tool Sydr. Firstly, we execute the program on the data, which does not lead to any errors. During program execution we update symbolic shadow stack and shadow registers to track symbolic sizes of the symbolic variables to avoid false positives. Then, if we meet the instruction, which truncates the symbolic variable, we build the security predicate, try to solve it with the SMT-solver and in case of success save new input file to reproduce the error. We tested our approach on Juliet Dynamic test suite for CWE-197 and achieved 100% accuracy. We approved the workability of our approach by detecting 12 new errors of numeric truncation in 5 different real-world open source projects within OSS-Sydr-Fuzz project. All of the errors were reported, most of the reports were equipped with appropriate fixes, successfully confirmed and applied by project maintainers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06425v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1109/ISPRAS60948.2023.10508174</arxiv:DOI>
      <arxiv:journal_reference>2023 Ivannikov ISPRAS Open Conference (ISPRAS), IEEE, 2023, pp. 84-92</arxiv:journal_reference>
      <dc:creator>Timofey Mezhuev, Ilay Kobrin, Alexey Vishnyakov, Daniil Kuts</dc:creator>
    </item>
  </channel>
</rss>
