<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>math.ST updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/math.ST</link>
    <description>math.ST updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/math.ST" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 28 Feb 2025 05:01:03 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Stein's unbiased risk estimate and Hyv\"arinen's score matching</title>
      <link>https://arxiv.org/abs/2502.20123</link>
      <description>arXiv:2502.20123v1 Announce Type: new 
Abstract: We study two G-modeling strategies for estimating the signal distribution (the empirical Bayesian's prior) from observations corrupted with normal noise. First, we choose the signal distribution by minimizing Stein's unbiased risk estimate (SURE) of the implied Eddington/Tweedie Bayes denoiser, an approach motivated by optimal empirical Bayesian shrinkage estimation of the signals. Second, we select the signal distribution by minimizing Hyv\"arinen's score matching objective for the implied score (derivative of log-marginal density), targeting minimal Fisher divergence between estimated and true marginal densities. While these strategies appear distinct, they are known to be mathematically equivalent. We provide a unified analysis of SURE and score matching under both well-specified signal distribution classes and misspecification. In the classical well-specified setting with homoscedastic noise and compactly supported signal distribution, we establish nearly parametric rates of convergence of the empirical Bayes regret and the Fisher divergence. In a commonly studied misspecified model, we establish fast rates of convergence to the oracle denoiser and corresponding oracle inequalities. Our empirical results demonstrate competitiveness with nonparametric maximum likelihood in well-specified settings, while showing superior performance under misspecification, particularly in settings involving heteroscedasticity and side information.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20123v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sulagna Ghosh, Nikolaos Ignatiadis, Frederic Koehler, Amber Lee</dc:creator>
    </item>
    <item>
      <title>Glivenko-Cantelli classes for real-valued empirical functions of stationary $\alpha$-mixing and $\beta$-mixing sequences</title>
      <link>https://arxiv.org/abs/2502.20206</link>
      <description>arXiv:2502.20206v1 Announce Type: new 
Abstract: The Glivenko-Cantelli theorem establishes uniform convergence of empirical distribution functions to their theoretical counterpart. This paper extends the classical result to real-valued empirical functions under dependence structures characterized by $\alpha$-mixing and $\beta$-mixing conditions. We investigate sufficient conditions ensuring that families of real-valued functions exhibit the Glivenko-Cantelli (GC) property in these dependent settings. Our analysis focuses on function classes satisfying uniform entropy conditions and establishes deviation bounds under mixing coefficients that decay at appropriate rates. Our findings refine existing literature by relaxing independence assumptions and highlighting the role of dependence in empirical process convergence.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20206v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ousmane Coulibaly, Harouna Sangar\'e</dc:creator>
    </item>
    <item>
      <title>Linear type conditional specifications for multivariate count variables</title>
      <link>https://arxiv.org/abs/2502.20227</link>
      <description>arXiv:2502.20227v1 Announce Type: new 
Abstract: This paper investigates conditional specifications for multivariate count variables. Recently, the spatial count data literature has proposed several conditional models such that the conditional expectations are linear in the conditioning variables. These models are much easier to estimate than existing spatial count models based on Gaussian random field. However, whether or not such conditional specifications are compatible have not been addressed. We investigate two large families of conditional models, that are the compound autoregressive model and the random coefficient integer autoregressive model. We characterize all the solutions to these two families of models at arbitrary dimensions, and find that only a handful of them admit non-trivial solutions. We then show that if we focus on the linearity condition of the conditional expectations only, a considerable larger family of solutions can be obtained. This suggests that for spatial count data modeling, semi-parametric type specifications that impose the conditional expectation structure is preferable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20227v1</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yang Lu, Wei Sun</dc:creator>
    </item>
    <item>
      <title>Minimax rate for learning kernels in operators</title>
      <link>https://arxiv.org/abs/2502.20368</link>
      <description>arXiv:2502.20368v1 Announce Type: new 
Abstract: Learning kernels in operators from data lies at the intersection of inverse problems and statistical learning, offering a powerful framework for capturing nonlocal dependency in function spaces and high-dimensional settings. In contrast to classical nonparametric regression, where the inverse problem is well-posed, kernel estimation involves a compact normal operator and an ill-posed deconvolution. To address these challenges, we introduce adaptive spectral Sobolev spaces, unifying Sobolev spaces and reproducing kernel Hilbert spaces, that automatically discard non-identifiable components and control terms with small eigenvalues. Within this framework, we establish the minimax convergence rates for the mean squared error under both polynomial and exponential spectral decay regimes. Methodologically, we develop a tamed least squares estimator achieving the minimax upper rates via controlling the left-tail probability for eigenvalues of the random normal matrix; and for the minimax lower rates, we resolve challenges from infinite-dimensional measures through their projections.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.20368v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sichong Zhang, Xiong Wang, Fei Lu</dc:creator>
    </item>
    <item>
      <title>Modeling Extreme Events in the Presence of Inlier: A Mixture Approach</title>
      <link>https://arxiv.org/abs/2502.19793</link>
      <description>arXiv:2502.19793v1 Announce Type: cross 
Abstract: In many random phenomena, such as life-testing experiments and environmental data (like rainfall data), there are often positive values and an excess of zeros, which create modeling challenges. In life testing, immediate failures result in zero lifetimes, often due to defects or poor quality, especially in electronics and clinical trials. These failures, called zero inliers, are difficult to model using standard approaches. When studying extreme values in the above scenarios, a key issue is selecting an appropriate threshold for accurate tail approximation of the population using asymptotic models. While some extreme value mixture models address threshold estimation and tail approximation, conventional parametric and non-parametric bulk and generalised Pareto distribution (GPD) approaches often neglect inliers, leading to suboptimal results. This paper introduces a framework for modeling extreme events and inliers using the GPD, addressing threshold uncertainty and effectively capturing inliers at zero. The model's parameters are estimated using the maximum likelihood estimation (MLE) method, ensuring optimal precision. Through simulation studies and real-world applications, we demonstrate that the proposed model significantly outperforms the traditional methods, which typically neglect inliers at the origin.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.19793v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shivshankar Nila, Ishapathik Das, N. Balakrishna</dc:creator>
    </item>
    <item>
      <title>Rates of convergence for nonparametric estimation of singular distributions using generative adversarial networks</title>
      <link>https://arxiv.org/abs/2202.02890</link>
      <description>arXiv:2202.02890v2 Announce Type: replace 
Abstract: It is common in nonparametric estimation problems to impose a certain low-dimensional structure on the unknown parameter to avoid the curse of dimensionality. This paper considers a nonparametric distribution estimation problem with a structural assumption under which the target distribution is allowed to be singular with respect to the Lebesgue measure. In particular, we investigate the use of generative adversarial networks (GANs) for estimating the unknown distribution and obtain a convergence rate with respect to the $L^1$-Wasserstein metric. The convergence rate depends only on the underlying structure and noise level. More interestingly, under the same structural assumption, the convergence rate of GAN is strictly faster than the known rate of VAE in the literature. We also obtain a lower bound for the minimax optimal rate, which is conjectured to be sharp at least in some special cases. Although our upper and lower bounds for the minimax optimal rate do not match, the difference is not significant.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.02890v2</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jeyong Lee, Hyeok Kyu Kwon, Minwoo Chae</dc:creator>
    </item>
    <item>
      <title>A Full Adagrad algorithm with O(Nd) operations</title>
      <link>https://arxiv.org/abs/2405.01908</link>
      <description>arXiv:2405.01908v2 Announce Type: replace 
Abstract: A novel approach is given to overcome the computational challenges of the full-matrix Adaptive Gradient algorithm (Full AdaGrad) in stochastic optimization. By developing a recursive method that estimates the inverse of the square root of the covariance of the gradient, alongside a streaming variant for parameter updates, the study offers efficient and practical algorithms for large-scale applications. This innovative strategy significantly reduces the complexity and resource demands typically associated with full-matrix methods, enabling more effective optimization processes. Moreover, the convergence rates of the proposed estimators and their asymptotic efficiency are given. Their effectiveness is demonstrated through numerical studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01908v2</guid>
      <category>math.ST</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Godichon-Baggioni (LPSM), Wei Lu (LMI), Bruno Portier (LMI)</dc:creator>
    </item>
    <item>
      <title>Non-asymptotic Properties of Generalized Mondrian Forests in Statistical Learning</title>
      <link>https://arxiv.org/abs/2406.00660</link>
      <description>arXiv:2406.00660v3 Announce Type: replace 
Abstract: Random Forests have been extensively used in regression and classification, inspiring the development of various forest-based methods. Among these, Mondrian Forests, derived from the Mondrian process, mark a significant advancement. Expanding on Mondrian Forests, this paper presents a general framework for statistical learning, encompassing a range of common learning tasks such as least squares regression, $\ell_1$ regression, quantile regression, and classification. Under mild assumptions on the loss functions, we provide upper bounds on the regret/risk functions for the estimators and demonstrate their statistical consistency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.00660v3</guid>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haoran Zhan, Jingli Wang, Yingcun Xia</dc:creator>
    </item>
    <item>
      <title>Free Anytime Validity by Sequentializing a Test and Optional Continuation with Tests as Future Significance Levels</title>
      <link>https://arxiv.org/abs/2501.03982</link>
      <description>arXiv:2501.03982v3 Announce Type: replace 
Abstract: Anytime valid sequential tests permit us to stop and continue testing based on the current data, without invalidating the inference. Given a maximum number of observations $N$, one may believe this must come at the cost of power when compared to a conventional test that waits until all $N$ observations have arrived. Our first contribution is to show that this is false: for any valid test based on $N$ observations, we derive an anytime valid sequential test that matches it after $N$ observations. Our second contribution is that the outcome of a continuously-interpreted test can be used as a significance level in subsequent testing, leading to an overall procedure that is valid at the original significance level. Combined this shows both the value of continuously-interpreted tests, and the fact that anytime validity and optional continuation are readily available in traditional testing, without requiring explicit use of e-values. We illustrate this by deriving the anytime valid sequentialized $z$-test and $t$-test, which at time $N$ coincide with the traditional $z$-test and $t$-test. Lastly, we show the popular log-optimal sequential $z$-test can be interpreted as desiring a rejection by the traditional $z$-test at some tiny significance level in the distant future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03982v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nick W. Koning, Sam van Meer</dc:creator>
    </item>
    <item>
      <title>Nonparametric estimation of the multivariate Spearman's footrule: a further discussion</title>
      <link>https://arxiv.org/abs/2501.07665</link>
      <description>arXiv:2501.07665v2 Announce Type: replace 
Abstract: In this paper, we propose two new estimators of the multivariate rank correlation coefficient Spearman's footrule which are based on two general estimators for Average Orthant Dependence measures. We compare the new proposals with a previous estimator existing in the literature and show that the three estimators are asymptotically equivalent, but, in small samples, one of the proposed estimators outperforms the others. We also analyse Pitman efficiency of these indices to test for multivariate independence as compared to multivariate versions of Kendall's tau and Spearman's rho.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07665v2</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.fss.2023.02.010</arxiv:DOI>
      <arxiv:journal_reference>Fuzzy Sets and Systems, volume 467 (2023), article 108489</arxiv:journal_reference>
      <dc:creator>Ana P\'erez, Mercedes Prieto-Alaiz, Fernando Chamizo, Eckhard Liebscher, Manuel \'Ubeda-Flores</dc:creator>
    </item>
    <item>
      <title>A quickest detection problem with false negatives</title>
      <link>https://arxiv.org/abs/2210.01844</link>
      <description>arXiv:2210.01844v2 Announce Type: replace-cross 
Abstract: We formulate and solve a variant of the quickest detection problem which features false negatives. A standard Brownian motion acquires a drift at an independent exponential random time which is not directly observable. Based on the observation in continuous time of the sample path of the process, an optimizer must detect the drift as quickly as possible after it has appeared. The optimizer can inspect the system multiple times upon payment of a fixed cost per inspection. If a test is performed on the system before the drift has appeared then, naturally, the test will return a negative outcome. However, if a test is performed after the drift has appeared, then the test may fail to detect it and return a false negative with probability $\epsilon\in(0,1)$. The optimisation ends when the drift is eventually detected. The problem is formulated mathematically as an optimal multiple stopping problem, and it is shown to be equivalent to a recursive optimal stopping problem. Exploiting such connection and free boundary methods we find explicit formulae for the expected cost and the optimal strategy. We also show that when $\epsilon = 0$ our expected cost is an affine transformation of the one in Shiryaev's classical optimal detection problem with a rescaled model parameter.</description>
      <guid isPermaLink="false">oai:arXiv.org:2210.01844v2</guid>
      <category>math.OC</category>
      <category>math.ST</category>
      <category>q-fin.MF</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiziano De Angelis, Jhanvi Garg, Quan Zhou</dc:creator>
    </item>
    <item>
      <title>The Canonical Decomposition of Factor Models: Weak Factors are Everywhere</title>
      <link>https://arxiv.org/abs/2307.10067</link>
      <description>arXiv:2307.10067v3 Announce Type: replace-cross 
Abstract: There are two approaches to time series approximate factor models: the static factor model, where the factors are loaded contemporaneously by the common component, and the Generalised Dynamic Factor Model, where the factors are loaded with lags. In this paper we derive a canonical decomposition which nests both models by introducing the weak common component which is the difference between the dynamic- and the static common component. Such component is driven by potentially infinitely many non-pervasive weak factors which live in the dynamically common space (not to be confused with rate-weak factors, being pervasive but associated with a slower rate). Our result shows that the relation between the two approaches is far more rich and complex than what usually assumed. We exemplify why the weak common component shall not be neglected by means of theoretical and empirical examples. Furthermore, we propose a simple estimation procedure for the canonical decomposition. Our empirical estimates on US macroeconomic data reveal that the weak common component can account for a large part of the variation of individual variables. Furthermore in a pseudo real-time forecasting evaluation for industrial production and inflation, we show that gains can be obtained from considering the dynamic approach over the static approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.10067v3</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Philipp Gersing, Matteo Barigozzi, Christoph Rust, Manfred Deistler</dc:creator>
    </item>
    <item>
      <title>Joint Mean and Correlation Regression Models for Multivariate Data</title>
      <link>https://arxiv.org/abs/2402.12803</link>
      <description>arXiv:2402.12803v2 Announce Type: replace-cross 
Abstract: We propose a joint mean and correlation regression model for multivariate discrete and (semi-)continuous response data, that simultaneously regresses the mean of each response against a set of covariates, and the correlations between responses against a set of similarity/distance measures. A set of joint estimating equations are formulated to construct an estimator of both the mean regression coefficients and the correlation regression parameters. Under a general setting where the number of responses can tend to infinity, the joint estimator is demonstrated to be consistent and asymptotically normally distributed, with differing rates of convergence due to the mean regression coefficients being heterogeneous across responses. An iterative estimation procedure is developed to obtain parameter estimates in the required (constrained) parameter space. Simulations demonstrate the strong finite sample performance of the proposed estimator in terms of point estimation and inference. We apply the proposed model to a count dataset of 38 Carabidae ground beetle species sampled throughout Scotland, along with information about the environmental conditions of each site and the traits of each species. Results show the relationship between mean abundance and environmental covariates differs across the beetle species, and that beetle total length is important in driving the correlations between species.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.12803v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhi Yang Tho, Francis K. C. Hui, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Large deviations for Independent Metropolis Hastings and Metropolis-adjusted Langevin algorithm</title>
      <link>https://arxiv.org/abs/2403.08691</link>
      <description>arXiv:2403.08691v3 Announce Type: replace-cross 
Abstract: In this paper, we prove large deviation principles for the empirical measures associated with the Independent Metropolis Hastings (IMH) sampler and the Metropolis-adjusted Langevin Algorithm (MALA). These are the first large deviation results for empirical measures of Markov chains arising from specific Metropolis-Hastings methods on a continuous state space. Moreover, we show that the existing large deviation framework, that we developed in a previous work (Milinanni and Nyquist, 2024), does not cover the Random Walk Metropolis sampler, even in cases when the underlying Markov chain is geometrically ergodic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08691v3</guid>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Federica Milinanni, Pierre Nyquist</dc:creator>
    </item>
    <item>
      <title>Transformers Handle Endogeneity in In-Context Linear Regression</title>
      <link>https://arxiv.org/abs/2410.01265</link>
      <description>arXiv:2410.01265v2 Announce Type: replace-cross 
Abstract: We explore the capability of transformers to address endogeneity in in-context linear regression. Our main finding is that transformers inherently possess a mechanism to handle endogeneity effectively using instrumental variables (IV). First, we demonstrate that the transformer architecture can emulate a gradient-based bi-level optimization procedure that converges to the widely used two-stage least squares $(\textsf{2SLS})$ solution at an exponential rate. Next, we propose an in-context pretraining scheme and provide theoretical guarantees showing that the global minimizer of the pre-training loss achieves a small excess loss. Our extensive experiments validate these theoretical findings, showing that the trained transformer provides more robust and reliable in-context predictions and coefficient estimates than the $\textsf{2SLS}$ method, in the presence of endogeneity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.01265v2</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Haodong Liang, Krishnakumar Balasubramanian, Lifeng Lai</dc:creator>
    </item>
    <item>
      <title>Bayesian High-dimensional Linear Regression with Sparse Projection-posterior</title>
      <link>https://arxiv.org/abs/2410.16577</link>
      <description>arXiv:2410.16577v3 Announce Type: replace-cross 
Abstract: We consider a novel Bayesian approach to estimation, uncertainty quantification, and variable selection for a high-dimensional linear regression model under sparsity. The number of predictors can be nearly exponentially large relative to the sample size. We put a conjugate normal prior initially disregarding sparsity, but for making an inference, instead of the original multivariate normal posterior, we use the posterior distribution induced by a map transforming the vector of regression coefficients to a sparse vector obtained by minimizing the sum of squares of deviations plus a suitably scaled $\ell_1$-penalty on the vector. We show that the resulting sparse projection-posterior distribution contracts around the true value of the parameter at the optimal rate adapted to the sparsity of the vector. We show that the true sparsity structure gets a large sparse projection-posterior probability. We further show that an appropriately recentred credible ball has the correct asymptotic frequentist coverage. Finally, we describe how the computational burden can be distributed to many machines, each dealing with only a small fraction of the whole dataset. We conduct a comprehensive simulation study under a variety of settings and found that the proposed method performs well for finite sample sizes. We also apply the method to several real datasets, including the ADNI data, and compare its performance with the state-of-the-art methods. We implemented the method in the \texttt{R} package called \texttt{sparseProj}, and all computations have been carried out using this package.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.16577v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Samhita Pal, Subhashis Ghoshal</dc:creator>
    </item>
    <item>
      <title>Statistical inference for interacting innovation processes and related general results</title>
      <link>https://arxiv.org/abs/2501.09648</link>
      <description>arXiv:2501.09648v2 Announce Type: replace-cross 
Abstract: Given the importance of understanding how different innovation processes affect each other, we have introduced a model for a finite system of interacting innovation processes. The present work focuses on the second-order asymptotic properties of the model and illustrates how to leverage the theoretical results in order to make statistical inference on the intensity of the interaction. This methodology is presented within a general framework in the supplementary material to ensure its broad applicability across various contexts. We apply the proposed tools to two real data sets (from Reddit and Gutenberg).</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09648v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 28 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Giacomo Aletti, Irene Crimaldi, Andrea Ghiglietti</dc:creator>
    </item>
  </channel>
</rss>
