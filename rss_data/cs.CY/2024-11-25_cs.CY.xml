<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 26 Nov 2024 04:05:34 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Transforming Engineering Education Using Generative AI and Digital Twin Technologies</title>
      <link>https://arxiv.org/abs/2411.14433</link>
      <description>arXiv:2411.14433v1 Announce Type: new 
Abstract: Digital twin technology, traditionally used in industry, is increasingly recognized for its potential to enhance educational experiences. This study investigates the application of industrial digital twins (DTs) in education, focusing on how DT models of varying fidelity can support different stages of Bloom's taxonomy in the cognitive domain. We align Bloom's six cognitive stages with educational levels: undergraduate studies for "Remember" and "Understand," master's level for "Apply" and "Analyze," and doctoral level for "Evaluate" and "Create." Low-fidelity DTs aid essential knowledge acquisition and skill training, providing a low-risk environment for grasping fundamental concepts. Medium-fidelity DTs offer more detailed and dynamic simulations, enhancing application skills and problem-solving. High-fidelity DTs support advanced learners by replicating physical phenomena, allowing for innovative design and complex experiments. Within this framework, large language models (LLMs) serve as mentors, assessing progress, filling knowledge gaps, and assisting with DT interactions, parameter setting, and debugging. We evaluate the educational impact using the Kirkpatrick Model, examining how each DT model's fidelity influences learning outcomes. This framework helps educators make informed decisions on integrating DTs and LLMs to meet specific learning objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14433v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.CR</category>
      <category>cs.HC</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yu-Zheng Lin, Ahmed Hussain J Alhamadah, Matthew William Redondo, Karan Himanshu Patel, Sujan Ghimire, Banafsheh Saber Latibari, Soheil Salehi, Pratik Satam</dc:creator>
    </item>
    <item>
      <title>Generative AI Policy and Governance Considerations for Health Security in Southeast Asia</title>
      <link>https://arxiv.org/abs/2411.14435</link>
      <description>arXiv:2411.14435v1 Announce Type: new 
Abstract: Southeast Asia is a geopolitically and socio-economically significant region with unique challenges and opportunities. Intensifying progress in generative AI against a backdrop of existing health security threats makes applications of AI to mitigate such threats attractive but also risky if done without due caution. This paper provides a brief sketch of some of the applications of AI for health security and the regional policy and governance landscape. I focus on policy and governance activities of the Association of Southeast Asian Nations (ASEAN), an international body whose member states represent 691 million people. I conclude by identifying sustainability as an area of opportunity for policymakers and recommend priority areas for generative AI researchers to make the most impact with their work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14435v1</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Thomas F Burns</dc:creator>
    </item>
    <item>
      <title>Transforming Business with Generative AI: Research, Innovation, Market Deployment and Future Shifts in Business Models</title>
      <link>https://arxiv.org/abs/2411.14437</link>
      <description>arXiv:2411.14437v1 Announce Type: new 
Abstract: This paper explores the transformative impact of Generative AI (GenAI) on the business landscape, examining its role in reshaping traditional business models, intensifying market competition, and fostering innovation. By applying the principles of Neo-Schumpeterian economics, the research analyses how GenAI is driving a new wave of "creative destruction," leading to the emergence of novel business paradigms and value propositions. The findings reveal that GenAI enhances operational efficiency, facilitates product and service innovation, and creates new revenue streams, positioning it as a powerful catalyst for substantial shifts in business structures and strategies. However, the deployment of GenAI also presents significant challenges, including ethical concerns, regulatory demands, and the risk of job displacement. By addressing the multifarious nature of GenAI, this paper provides valuable insights for business leaders, policymakers, and researchers, guiding them towards a balanced and responsible integration of this transformative technology. Ultimately, GenAI is not merely a technological advancement but a driver of profound change, heralding a future where creativity, efficiency, and growth are redefined.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14437v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Narotam Singh, Vaibhav Chaudhary, Nimisha Singh, Neha Soni, Amita Kapoor</dc:creator>
    </item>
    <item>
      <title>Windstorm Economic Impacts on the Spanish Resilience: A Machine Learning Real-Data Approach</title>
      <link>https://arxiv.org/abs/2411.14439</link>
      <description>arXiv:2411.14439v1 Announce Type: new 
Abstract: Climate change-associated disasters have become a significant concern, principally when affecting urban areas. Assessing these regions' resilience to strengthen their disaster management is crucial, especially in the areas vulnerable to windstorms, one of Spain's most critical disasters. Smart cities and machine learning offer promising solutions to manage disasters, but accurately estimating economic losses from windstorms can be difficult due to the unique characteristics of each region and limited data. This study proposes utilizing ML classification models to enhance disaster resilience by analyzing publicly available data on windstorms in the Spanish areas. This approach can help decision-makers make informed decisions regarding preparedness and mitigation actions, ultimately creating a more resilient urban environment that can better withstand windstorms in the future.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14439v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>XX Conferencia de la Asociacion Espanola para la Inteligencia Artificial 2024</arxiv:journal_reference>
      <dc:creator>Matheus Puime Pedra (Industrial Management Department - TECNUN, University of Navarra, Donostia, Spain), Josune Hernantes (Industrial Management Department - TECNUN, University of Navarra, Donostia, Spain), Leire Casals (Industrial Management Department - TECNUN, University of Navarra, Donostia, Spain), Leire Labaka (Industrial Management Department - TECNUN, University of Navarra, Donostia, Spain)</dc:creator>
    </item>
    <item>
      <title>AI Ethics by Design: Implementing Customizable Guardrails for Responsible AI Development</title>
      <link>https://arxiv.org/abs/2411.14442</link>
      <description>arXiv:2411.14442v1 Announce Type: new 
Abstract: This paper explores the development of an ethical guardrail framework for AI systems, emphasizing the importance of customizable guardrails that align with diverse user values and underlying ethics. We address the challenges of AI ethics by proposing a structure that integrates rules, policies, and AI assistants to ensure responsible AI behavior, while comparing the proposed framework to the existing state-of-the-art guardrails. By focusing on practical mechanisms for implementing ethical standards, we aim to enhance transparency, user autonomy, and continuous improvement in AI systems. Our approach accommodates ethical pluralism, offering a flexible and adaptable solution for the evolving landscape of AI governance. The paper concludes with strategies for resolving conflicts between ethical directives, underscoring the present and future need for robust, nuanced and context-aware AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14442v1</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kristina \v{S}ekrst, Jeremy McHugh, Jonathan Rodriguez Cefalu</dc:creator>
    </item>
    <item>
      <title>An Investigation of the Relationship Between Crime Rate and Police Compensation</title>
      <link>https://arxiv.org/abs/2411.14632</link>
      <description>arXiv:2411.14632v1 Announce Type: new 
Abstract: The goal of this paper is to assess whether there is any correlation between police salaries and crime rates. Using public data sources that contain Baltimore Crime Rates and Baltimore Police Department (BPD) salary information from 2011 to 2021, our research uses a variety of techniques to capture and measure any correlation between the two. Based on that correlation, the paper then uses established social theories to make recommendations on how this data can potentially be used by State Leadership. Our initial results show a negative correlation between salary/compensation levels and crime rates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14632v1</guid>
      <category>cs.CY</category>
      <category>stat.AP</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jhancy Amarsingh, Likhith Kumar Reddy Appakondreddigari, Ashish Nunna, Charishma Choudary Tummala, John Winship, Alex Zhou, Huthaifa I. Ashqar</dc:creator>
    </item>
    <item>
      <title>Social Media Algorithms Can Shape Affective Polarization via Exposure to Antidemocratic Attitudes and Partisan Animosity</title>
      <link>https://arxiv.org/abs/2411.14652</link>
      <description>arXiv:2411.14652v1 Announce Type: new 
Abstract: There is widespread concern about the negative impacts of social media feed ranking algorithms on political polarization. Leveraging advancements in large language models (LLMs), we develop an approach to re-rank feeds in real-time to test the effects of content that is likely to polarize: expressions of antidemocratic attitudes and partisan animosity (AAPA). In a preregistered 10-day field experiment on X/Twitter with 1,256 consented participants, we increase or decrease participants' exposure to AAPA in their algorithmically curated feeds. We observe more positive outparty feelings when AAPA exposure is decreased and more negative outparty feelings when AAPA exposure is increased. Exposure to AAPA content also results in an immediate increase in negative emotions, such as sadness and anger. The interventions do not significantly impact traditional engagement metrics such as re-post and favorite rates. These findings highlight a potential pathway for developing feed algorithms that mitigate affective polarization by addressing content that undermines the shared values required for a healthy democracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14652v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <category>cs.SI</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tiziano Piccardi, Martin Saveski, Chenyan Jia, Jeffrey T. Hancock, Jeanne L. Tsai, Michael Bernstein</dc:creator>
    </item>
    <item>
      <title>Funhouse Mirror or Echo Chamber? A Methodological Approach to Teaching Critical AI Literacy Through Metaphors</title>
      <link>https://arxiv.org/abs/2411.14730</link>
      <description>arXiv:2411.14730v1 Announce Type: new 
Abstract: As educational institutions grapple with teaching students about increasingly complex Artificial Intelligence (AI) systems, finding effective methods for explaining these technologies and their societal implications remains a major challenge. This study proposes a methodological approach combining Conceptual Metaphor Theory (CMT) with UNESCO's AI competency framework to develop Critical AI Literacy (CAIL). Through a systematic analysis of metaphors commonly used to describe AI systems, we develop criteria for selecting pedagogically appropriate metaphors and demonstrate their alignment with established AI literacy competencies, as well as UNESCO's AI competency framework.
  Our method identifies and suggests four key metaphors for teaching CAIL. This includes GenAI as an echo chamber, GenAI as a funhouse mirror, GenAI as a black box magician, and GenAI as a map. Each of these seeks to address specific aspects of understanding characteristics of AI, from filter bubbles to algorithmic opacity. We present these metaphors alongside interactive activities designed to engage students in experiential learning of AI concepts. In doing so, we offer educators a structured approach to teaching CAIL that bridges technical understanding with societal implications. This work contributes to the growing field of AI education by demonstrating how carefully selected metaphors can make complex technological concepts more accessible while promoting critical engagement with AI systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14730v1</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jasper Roe (James Cook University Singapore, Singapore), Leon Furze (Deakin University, Australia), Mike Perkins (British University Vietnam, Vietnam)</dc:creator>
    </item>
    <item>
      <title>Financial Risk Assessment via Long-term Payment Behavior Sequence Folding</title>
      <link>https://arxiv.org/abs/2411.15056</link>
      <description>arXiv:2411.15056v1 Announce Type: new 
Abstract: Online inclusive financial services encounter significant financial risks due to their expansive user base and low default costs. By real-world practice, we reveal that utilizing longer-term user payment behaviors can enhance models' ability to forecast financial risks. However, learning long behavior sequences is non-trivial for deep sequential models. Additionally, the diverse fields of payment behaviors carry rich information, requiring thorough exploitation. These factors collectively complicate the task of long-term user behavior modeling. To tackle these challenges, we propose a Long-term Payment Behavior Sequence Folding method, referred to as LBSF. In LBSF, payment behavior sequences are folded based on merchants, using the merchant field as an intrinsic grouping criterion, which enables informative parallelism without reliance on external knowledge. Meanwhile, we maximize the utility of payment details through a multi-field behavior encoding mechanism. Subsequently, behavior aggregation at the merchant level followed by relational learning across merchants facilitates comprehensive user financial representation. We evaluate LBSF on the financial risk assessment task using a large-scale real-world dataset. The results demonstrate that folding long behavior sequences based on internal behavioral cues effectively models long-term patterns and changes, thereby generating more accurate user financial profiles for practical applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15056v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yiran Qiao, Yateng Tang, Xiang Ao, Qi Yuan, Ziming Liu, Chen Shen, Xuehao Zheng</dc:creator>
    </item>
    <item>
      <title>Towards Next-Generation Medical Agent: How o1 is Reshaping Decision-Making in Medical Scenarios</title>
      <link>https://arxiv.org/abs/2411.14461</link>
      <description>arXiv:2411.14461v1 Announce Type: cross 
Abstract: Artificial Intelligence (AI) has become essential in modern healthcare, with large language models (LLMs) offering promising advances in clinical decision-making. Traditional model-based approaches, including those leveraging in-context demonstrations and those with specialized medical fine-tuning, have demonstrated strong performance in medical language processing but struggle with real-time adaptability, multi-step reasoning, and handling complex medical tasks. Agent-based AI systems address these limitations by incorporating reasoning traces, tool selection based on context, knowledge retrieval, and both short- and long-term memory. These additional features enable the medical AI agent to handle complex medical scenarios where decision-making should be built on real-time interaction with the environment. Therefore, unlike conventional model-based approaches that treat medical queries as isolated questions, medical AI agents approach them as complex tasks and behave more like human doctors. In this paper, we study the choice of the backbone LLM for medical AI agents, which is the foundation for the agent's overall reasoning and action generation. In particular, we consider the emergent o1 model and examine its impact on agents' reasoning, tool-use adaptability, and real-time information retrieval across diverse clinical scenarios, including high-stakes settings such as intensive care units (ICUs). Our findings demonstrate o1's ability to enhance diagnostic accuracy and consistency, paving the way for smarter, more responsive AI tools that support better patient outcomes and decision-making efficacy in clinical practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14461v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shaochen Xu, Yifan Zhou, Zhengliang Liu, Zihao Wu, Tianyang Zhong, Huaqin Zhao, Yiwei Li, Hanqi Jiang, Yi Pan, Junhao Chen, Jin Lu, Wei Zhang, Tuo Zhang, Lu Zhang, Dajiang Zhu, Xiang Li, Wei Liu, Quanzheng Li, Andrea Sikora, Xiaoming Zhai, Zhen Xiang, Tianming Liu</dc:creator>
    </item>
    <item>
      <title>Ensuring Safety and Trust: Analyzing the Risks of Large Language Models in Medicine</title>
      <link>https://arxiv.org/abs/2411.14487</link>
      <description>arXiv:2411.14487v1 Announce Type: cross 
Abstract: The remarkable capabilities of Large Language Models (LLMs) make them increasingly compelling for adoption in real-world healthcare applications. However, the risks associated with using LLMs in medical applications have not been systematically characterized. We propose using five key principles for safe and trustworthy medical AI: Truthfulness, Resilience, Fairness, Robustness, and Privacy, along with ten specific aspects. Under this comprehensive framework, we introduce a novel MedGuard benchmark with 1,000 expert-verified questions. Our evaluation of 11 commonly used LLMs shows that the current language models, regardless of their safety alignment mechanisms, generally perform poorly on most of our benchmarks, particularly when compared to the high performance of human physicians. Despite recent reports indicate that advanced LLMs like ChatGPT can match or even exceed human performance in various medical tasks, this study underscores a significant safety gap, highlighting the crucial need for human oversight and the implementation of AI safety guardrails.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14487v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yifan Yang, Qiao Jin, Robert Leaman, Xiaoyu Liu, Guangzhi Xiong, Maame Sarfo-Gyamfi, Changlin Gong, Santiago Ferri\`ere-Steinert, W. John Wilbur, Xiaojun Li, Jiaxin Yuan, Bang An, Kelvin S. Castro, Francisco Erramuspe \'Alvarez, Mat\'ias Stockle, Aidong Zhang, Furong Huang, Zhiyong Lu</dc:creator>
    </item>
    <item>
      <title>Exploring Accuracy-Fairness Trade-off in Large Language Models</title>
      <link>https://arxiv.org/abs/2411.14500</link>
      <description>arXiv:2411.14500v1 Announce Type: cross 
Abstract: Large Language Models (LLMs) have made significant strides in the field of artificial intelligence, showcasing their ability to interact with humans and influence human cognition through information dissemination. However, recent studies have brought to light instances of bias inherent within these LLMs, presenting a critical issue that demands attention. In our research, we delve deeper into the intricate challenge of harmonising accuracy and fairness in the enhancement of LLMs. While improving accuracy can indeed enhance overall LLM performance, it often occurs at the expense of fairness. Overemphasising optimisation of one metric invariably leads to a significant degradation of the other. This underscores the necessity of taking into account multiple considerations during the design and optimisation phases of LLMs. Therefore, we advocate for reformulating the LLM training process as a multi-objective learning task. Our investigation reveals that multi-objective evolutionary learning (MOEL) methodologies offer promising avenues for tackling this challenge. Our MOEL framework enables the simultaneous optimisation of both accuracy and fairness metrics, resulting in a Pareto-optimal set of LLMs. In summary, our study sheds valuable lights on the delicate equilibrium between accuracy and fairness within LLMs, which is increasingly significant for their real-world applications. By harnessing MOEL, we present a promising pathway towards fairer and more efficacious AI technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14500v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qingquan Zhang, Qiqi Duan, Bo Yuan, Yuhui Shi, Jialin Liu</dc:creator>
    </item>
    <item>
      <title>Global Challenge for Safe and Secure LLMs Track 1</title>
      <link>https://arxiv.org/abs/2411.14502</link>
      <description>arXiv:2411.14502v1 Announce Type: cross 
Abstract: This paper introduces the Global Challenge for Safe and Secure Large Language Models (LLMs), a pioneering initiative organized by AI Singapore (AISG) and the CyberSG R&amp;D Programme Office (CRPO) to foster the development of advanced defense mechanisms against automated jailbreaking attacks. With the increasing integration of LLMs in critical sectors such as healthcare, finance, and public administration, ensuring these models are resilient to adversarial attacks is vital for preventing misuse and upholding ethical standards. This competition focused on two distinct tracks designed to evaluate and enhance the robustness of LLM security frameworks. Track 1 tasked participants with developing automated methods to probe LLM vulnerabilities by eliciting undesirable responses, effectively testing the limits of existing safety protocols within LLMs. Participants were challenged to devise techniques that could bypass content safeguards across a diverse array of scenarios, from offensive language to misinformation and illegal activities. Through this process, Track 1 aimed to deepen the understanding of LLM vulnerabilities and provide insights for creating more resilient models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14502v1</guid>
      <category>cs.CR</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaojun Jia, Yihao Huang, Yang Liu, Peng Yan Tan, Weng Kuan Yau, Mun-Thye Mak, Xin Ming Sim, Wee Siong Ng, See Kiong Ng, Hanqing Liu, Lifeng Zhou, Huanqian Yan, Xiaobing Sun, Wei Liu, Long Wang, Yiming Qian, Yong Liu, Junxiao Yang, Zhexin Zhang, Leqi Lei, Renmiao Chen, Yida Lu, Shiyao Cui, Zizhou Wang, Shaohua Li, Yan Wang, Rick Siow Mong Goh, Liangli Zhen, Yingjie Zhang, Zhe Zhao</dc:creator>
    </item>
    <item>
      <title>Listening for Expert Identified Linguistic Features: Assessment of Audio Deepfake Discernment among Undergraduate Students</title>
      <link>https://arxiv.org/abs/2411.14586</link>
      <description>arXiv:2411.14586v1 Announce Type: cross 
Abstract: This paper evaluates the impact of training undergraduate students to improve their audio deepfake discernment ability by listening for expert-defined linguistic features. Such features have been shown to improve performance of AI algorithms; here, we ascertain whether this improvement in AI algorithms also translates to improvement of the perceptual awareness and discernment ability of listeners. With humans as the weakest link in any cybersecurity solution, we propose that listener discernment is a key factor for improving trustworthiness of audio content. In this study we determine whether training that familiarizes listeners with English language variation can improve their abilities to discern audio deepfakes. We focus on undergraduate students, as this demographic group is constantly exposed to social media and the potential for deception and misinformation online. To the best of our knowledge, our work is the first study to uniquely address English audio deepfake discernment through such techniques. Our research goes beyond informational training by introducing targeted linguistic cues to listeners as a deepfake discernment mechanism, via a training module. In a pre-/post- experimental design, we evaluated the impact of the training across 264 students as a representative cross section of all students at the University of Maryland, Baltimore County, and across experimental and control sections. Findings show that the experimental group showed a statistically significant decrease in their unsurety when evaluating audio clips and an improvement in their ability to correctly identify clips they were initially unsure about. While results are promising, future research will explore more robust and comprehensive trainings for greater impact.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14586v1</guid>
      <category>cs.SD</category>
      <category>cs.CY</category>
      <category>eess.AS</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Noshaba N. Bhalli, Nehal Naqvi, Chloe Evered, Christine Mallinson, Vandana P. Janeja</dc:creator>
    </item>
    <item>
      <title>Predictive Analytics of Air Alerts in the Russian-Ukrainian War</title>
      <link>https://arxiv.org/abs/2411.14625</link>
      <description>arXiv:2411.14625v1 Announce Type: cross 
Abstract: The paper considers exploratory data analysis and approaches in predictive analytics for air alerts during the Russian-Ukrainian war which broke out on Feb 24, 2022. The results illustrate that alerts in regions correlate with one another and have geospatial patterns which make it feasible to build a predictive model which predicts alerts that are expected to take place in a certain region within a specified time period. The obtained results show that the alert status in a particular region is highly dependable on the features of its adjacent regions. Seasonality features like hours, days of a week and months are also crucial in predicting the target variable. Some regions highly rely on the time feature which equals to a number of days from the initial date of the dataset. From this, we can deduce that the air alert pattern changes throughout the time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14625v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Demian Pavlyshenko, Bohdan Pavlyshenko</dc:creator>
    </item>
    <item>
      <title>Construction and Preliminary Validation of a Dynamic Programming Concept Inventory</title>
      <link>https://arxiv.org/abs/2411.14655</link>
      <description>arXiv:2411.14655v1 Announce Type: cross 
Abstract: Concept inventories are standardized assessments that evaluate student understanding of key concepts within academic disciplines. While prevalent across STEM fields, their development lags for advanced computer science topics like dynamic programming (DP) -- an algorithmic technique that poses significant conceptual challenges for undergraduates. To fill this gap, we developed and validated a Dynamic Programming Concept Inventory (DPCI). We detail the iterative process used to formulate multiple-choice questions targeting known student misconceptions about DP concepts identified through prior research studies. We discuss key decisions, tradeoffs, and challenges faced in crafting probing questions to subtly reveal these conceptual misunderstandings. We conducted a preliminary psychometric validation by administering the DPCI to 172 undergraduate CS students finding our questions to be of appropriate difficulty and effectively discriminating between differing levels of student understanding. Taken together, our validated DPCI will enable instructors to accurately assess student mastery of DP. Moreover, our approach for devising a concept inventory for an advanced theoretical computer science concept can guide future efforts to create assessments for other under-evaluated areas currently lacking coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14655v1</guid>
      <category>cs.DS</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthew Ferland, Varun Nagaraj Rao, Arushi Arora, Drew van der Poel, Michael Luu, Randy Huynh, Freddy Reiber, Sandra Ossman, Seth Poulsen, Michael Shindler</dc:creator>
    </item>
    <item>
      <title>FairAdapter: Detecting AI-generated Images with Improved Fairness</title>
      <link>https://arxiv.org/abs/2411.14755</link>
      <description>arXiv:2411.14755v1 Announce Type: cross 
Abstract: The high-quality, realistic images generated by generative models pose significant challenges for exposing them.So far, data-driven deep neural networks have been justified as the most efficient forensics tools for the challenges. However, they may be over-fitted to certain semantics, resulting in considerable inconsistency in detection performance across different contents of generated samples. It could be regarded as an issue of detection fairness. In this paper, we propose a novel framework named Fairadapter to tackle the issue. In comparison with existing state-of-the-art methods, our model achieves improved fairness performance. Our project: https://github.com/AppleDogDog/FairnessDetection</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14755v1</guid>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Feng Ding, Jun Zhang, Xinan He, Jianfeng Xu</dc:creator>
    </item>
    <item>
      <title>Evaluating LLM Prompts for Data Augmentation in Multi-label Classification of Ecological Texts</title>
      <link>https://arxiv.org/abs/2411.14896</link>
      <description>arXiv:2411.14896v1 Announce Type: cross 
Abstract: Large language models (LLMs) play a crucial role in natural language processing (NLP) tasks, improving the understanding, generation, and manipulation of human language across domains such as translating, summarizing, and classifying text. Previous studies have demonstrated that instruction-based LLMs can be effectively utilized for data augmentation to generate diverse and realistic text samples. This study applied prompt-based data augmentation to detect mentions of green practices in Russian social media. Detecting green practices in social media aids in understanding their prevalence and helps formulate recommendations for scaling eco-friendly actions to mitigate environmental issues. We evaluated several prompts for augmenting texts in a multi-label classification task, either by rewriting existing datasets using LLMs, generating new data, or combining both approaches. Our results revealed that all strategies improved classification performance compared to the models fine-tuned only on the original dataset, outperforming baselines in most cases. The best results were obtained with the prompt that paraphrased the original text while clearly indicating the relevant categories.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14896v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.SI</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anna Glazkova, Olga Zakharova</dc:creator>
    </item>
    <item>
      <title>The CTSkills App -- Measuring Problem Decomposition Skills of Students in Computational Thinking</title>
      <link>https://arxiv.org/abs/2411.14945</link>
      <description>arXiv:2411.14945v1 Announce Type: cross 
Abstract: This paper addresses the incorporation of problem decomposition skills as an important component of computational thinking (CT) in K-12 computer science (CS) education. Despite the growing integration of CS in schools, there is a lack of consensus on the precise definition of CT in general and decomposition in particular. While decomposition is commonly referred to as the starting point of (computational) problem-solving, algorithmic solution formulation often receives more attention in the classroom, while decomposition remains rather unexplored. This study presents "CTSKills", a web-based skill assessment tool developed to measure students' problem decomposition skills. With the data collected from 75 students in grades 4-9, this research aims to contribute to a baseline of students' decomposition proficiency in compulsory education. Furthermore, a thorough understanding of a given problem is becoming increasingly important with the advancement of generative artificial intelligence (AI) tools that can effectively support the process of formulating algorithms. This study highlights the importance of problem decomposition as a key skill in K-12 CS education to foster more adept problem solvers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14945v1</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dorit Assaf, Giorgia Adorni, Elia Lutz, Lucio Negrini, Alberto Piatti, Francesco Mondada, Francesca Mangili, Luca Maria Gambardella</dc:creator>
    </item>
    <item>
      <title>Generative AI may backfire for counterspeech</title>
      <link>https://arxiv.org/abs/2411.14986</link>
      <description>arXiv:2411.14986v2 Announce Type: cross 
Abstract: Online hate speech poses a serious threat to individual well-being and societal cohesion. A promising solution to curb online hate speech is counterspeech. Counterspeech is aimed at encouraging users to reconsider hateful posts by direct replies. However, current methods lack scalability due to the need for human intervention or fail to adapt to the specific context of the post. A potential remedy is the use of generative AI, specifically large language models (LLMs), to write tailored counterspeech messages. In this paper, we analyze whether contextualized counterspeech generated by state-of-the-art LLMs is effective in curbing online hate speech. To do so, we conducted a large-scale, pre-registered field experiment (N=2,664) on the social media platform Twitter/X. Our experiment followed a 2x2 between-subjects design and, additionally, a control condition with no counterspeech. On the one hand, users posting hateful content on Twitter/X were randomly assigned to receive either (a) contextualized counterspeech or (b) non-contextualized counterspeech. Here, the former is generated through LLMs, while the latter relies on predefined, generic messages. On the other hand, we tested two counterspeech strategies: (a) promoting empathy and (b) warning about the consequences of online misbehavior. We then measured whether users deleted their initial hateful posts and whether their behavior changed after the counterspeech intervention (e.g., whether users adopted a less toxic language). We find that non-contextualized counterspeech employing a warning-of-consequence strategy significantly reduces online hate speech. However, contextualized counterspeech generated by LLMs proves ineffective and may even backfire.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.14986v2</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Dominik B\"ar, Abdurahman Maarouf, Stefan Feuerriegel</dc:creator>
    </item>
    <item>
      <title>Fantastic Biases (What are They) and Where to Find Them</title>
      <link>https://arxiv.org/abs/2411.15051</link>
      <description>arXiv:2411.15051v1 Announce Type: cross 
Abstract: Deep Learning models tend to learn correlations of patterns on huge datasets. The bigger these systems are, the more complex are the phenomena they can detect, and the more data they need for this. The use of Artificial Intelligence (AI) is becoming increasingly ubiquitous in our society, and its impact is growing everyday. The promises it holds strongly depend on their fair and universal use, such as access to information or education for all. In a world of inequalities, they can help to reach the most disadvantaged areas. However, such a universal systems must be able to represent society, without benefiting some at the expense of others. We must not reproduce the inequalities observed throughout the world, but educate these IAs to go beyond them. We have seen cases where these systems use gender, race, or even class information in ways that are not appropriate for resolving their tasks. Instead of real causal reasoning, they rely on spurious correlations, which is what we usually call a bias. In this paper, we first attempt to define what is a bias in general terms. It helps us to demystify the concept of bias, to understand why we can find them everywhere and why they are sometimes useful. Second, we focus over the notion of what is generally seen as negative bias, the one we want to avoid in machine learning, before presenting a general zoology containing the most common of these biases. We finally conclude by looking at classical methods to detect them, by means of specially crafted datasets of templates and specific algorithms, and also classical methods to mitigate them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.15051v1</guid>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>Bits de Ciencias 26 (2024), 02-13</arxiv:journal_reference>
      <dc:creator>Valentin Barriere</dc:creator>
    </item>
    <item>
      <title>Mining individual daily commuting patterns of dockless bike-sharing users: a two-layer framework integrating spatiotemporal flow clustering and rule-based decision trees</title>
      <link>https://arxiv.org/abs/2407.09820</link>
      <description>arXiv:2407.09820v2 Announce Type: replace 
Abstract: The rise of dockless bike-sharing systems has led to increased interest in using bike-sharing data for sustainable transportation and travel behavior research. However, these studies have rarely focused on the individual daily mobility patterns, hindering their alignment with the increasingly refined needs of active transportation planning. To bridge this gap, this paper presents a two-layer framework, integrating improved flow clustering methods and multiple rule-based decision trees, to mine individual cyclists' daily home-work commuting patterns from dockless bike-sharing trip data with user IDs. The effectiveness and applicability of the framework is demonstrated by over 200 million bike-sharing trip records in Shenzhen. Based on the mining results, we obtain two categories of bike-sharing commuters (74.38% of Only-biking commuters and 25.62% of Biking-with-transit commuters) and some interesting findings about their daily commuting patterns. For instance, lots of bike-sharing commuters live near urban villages and old communities with lower costs of living, especially in the central city. Only-biking commuters have a higher proportion of overtime than Biking-with-transit commuters, and the Longhua Industrial Park, a manufacturing-oriented area, has the longest average working hours (over 10 hours per day). Moreover, massive users utilize bike-sharing for commuting to work more frequently than for returning home, which is intricately related to the over-demand for bikes around workplaces during commuting peak. In sum, this framework offers a cost-effective way to understand the nuanced non-motorized mobility patterns and low-carbon trip chains of residents. It also offers novel insights for improving the bike-sharing services and planning of active transportation modes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09820v2</guid>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.scs.2024.105985</arxiv:DOI>
      <arxiv:journal_reference>Sustainable Cities and Society 118:105985,2024</arxiv:journal_reference>
      <dc:creator>Caigang Zhuang, Shaoying Li, Haoming Zhuang, Xiaoping Liu</dc:creator>
    </item>
    <item>
      <title>Rapid Integration of LLMs in Healthcare Raises Ethical Concerns: An Investigation into Deceptive Patterns in Social Robots</title>
      <link>https://arxiv.org/abs/2410.00434</link>
      <description>arXiv:2410.00434v2 Announce Type: replace 
Abstract: Conversational agents are increasingly used in healthcare, and the integration of Large Language Models (LLMs) has significantly enhanced their capabilities. When integrated into social robots, LLMs offer the potential for more natural interactions. However, while LLMs promise numerous benefits, they also raise critical ethical concerns, particularly around the issue of hallucinations and deceptive patterns. In this case study, we observed a critical pattern of deceptive behavior in commercially available LLM-based care software integrated into robots. The LLM-equipped robot falsely claimed to have medication reminder functionalities. Not only did these systems assure users of their ability to manage medication schedules, but they also proactively suggested this capability, despite lacking it. This deceptive behavior poses significant risks in healthcare environments, where reliability is paramount. Our findings highlights the ethical and safety concerns surrounding the deployment of LLM-integrated robots in healthcare, emphasizing the need for oversight to prevent potentially harmful consequences for vulnerable populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.00434v2</guid>
      <category>cs.CY</category>
      <category>cs.RO</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Robert Ranisch, Joschka Haltaufderheide</dc:creator>
    </item>
    <item>
      <title>Robustness and Confounders in the Demographic Alignment of LLMs with Human Perceptions of Offensiveness</title>
      <link>https://arxiv.org/abs/2411.08977</link>
      <description>arXiv:2411.08977v2 Announce Type: replace 
Abstract: Large language models (LLMs) are known to exhibit demographic biases, yet few studies systematically evaluate these biases across multiple datasets or account for confounding factors. In this work, we examine LLM alignment with human annotations in five offensive language datasets, comprising approximately 220K annotations. Our findings reveal that while demographic traits, particularly race, influence alignment, these effects are inconsistent across datasets and often entangled with other factors. Confounders -- such as document difficulty, annotator sensitivity, and within-group agreement -- account for more variation in alignment patterns than demographic traits alone. Specifically, alignment increases with higher annotator sensitivity and group agreement, while greater document difficulty corresponds to reduced alignment. Our results underscore the importance of multi-dataset analyses and confounder-aware methodologies in developing robust measures of demographic bias in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.08977v2</guid>
      <category>cs.CY</category>
      <category>cs.CL</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Shayan Alipour, Indira Sen, Mattia Samory, Tanushree Mitra</dc:creator>
    </item>
    <item>
      <title>How to use model architecture and training environment to estimate the energy consumption of DL training</title>
      <link>https://arxiv.org/abs/2307.05520</link>
      <description>arXiv:2307.05520v4 Announce Type: replace-cross 
Abstract: To raise awareness of the huge impact Deep Learning (DL) has on the environment, several works have tried to estimate the energy consumption and carbon footprint of DL-based systems across their life cycle. However, the estimations for energy consumption in the training stage usually rely on assumptions that have not been thoroughly tested. This study aims to move past these assumptions by leveraging the relationship between energy consumption and two relevant design decisions in DL training; model architecture, and training environment. To investigate these relationships, we collect multiple metrics related to energy efficiency and model correctness during the models' training. Then, we outline the trade-offs between the measured energy consumption and the models' correctness regarding model architecture, and their relationship with the training environment. Finally, we study the training's power consumption behavior and propose four new energy estimation methods. Our results show that selecting the proper model architecture and training environment can reduce energy consumption dramatically (up to 80.72%) at the cost of negligible decreases in correctness. Also, we find evidence that GPUs should scale with the models' computational complexity for better energy efficiency. Furthermore, we prove that current energy estimation methods are unreliable and propose alternatives 2x more precise.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.05520v4</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Santiago del Rey, Silverio Mart\'inez-Fern\'andez, Lu\'is Cruz, Xavier Franch</dc:creator>
    </item>
    <item>
      <title>Large Language Models Show Human-like Social Desirability Biases in Survey Responses</title>
      <link>https://arxiv.org/abs/2405.06058</link>
      <description>arXiv:2405.06058v2 Announce Type: replace-cross 
Abstract: As Large Language Models (LLMs) become widely used to model and simulate human behavior, understanding their biases becomes critical. We developed an experimental framework using Big Five personality surveys and uncovered a previously undetected social desirability bias in a wide range of LLMs. By systematically varying the number of questions LLMs were exposed to, we demonstrate their ability to infer when they are being evaluated. When personality evaluation is inferred, LLMs skew their scores towards the desirable ends of trait dimensions (i.e., increased extraversion, decreased neuroticism, etc). This bias exists in all tested models, including GPT-4/3.5, Claude 3, Llama 3, and PaLM-2. Bias levels appear to increase in more recent models, with GPT-4's survey responses changing by 1.20 (human) standard deviations and Llama 3's by 0.98 standard deviations-very large effects. This bias is robust to randomization of question order and paraphrasing. Reverse-coding all the questions decreases bias levels but does not eliminate them, suggesting that this effect cannot be attributed to acquiescence bias. Our findings reveal an emergent social desirability bias and suggest constraints on profiling LLMs with psychometric tests and on using LLMs as proxies for human participants.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06058v2</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aadesh Salecha, Molly E. Ireland, Shashanka Subrahmanya, Jo\~ao Sedoc, Lyle H. Ungar, Johannes C. Eichstaedt</dc:creator>
    </item>
    <item>
      <title>Leveraging Large Language Models to Measure Gender Representation Bias in Gendered Language Corpora</title>
      <link>https://arxiv.org/abs/2406.13677</link>
      <description>arXiv:2406.13677v2 Announce Type: replace-cross 
Abstract: Gender bias in text corpora that are used for a variety of natural language processing (NLP) tasks, such as for training large language models (LLMs), can lead to the perpetuation and amplification of societal inequalities. This phenomenon is particularly pronounced in gendered languages like Spanish or French, where grammatical structures inherently encode gender, making the bias analysis more challenging. A first step in quantifying gender bias in text entails computing biases in gender representation, i.e., differences in the prevalence of words referring to males vs. females. Existing methods to measure gender representation bias in text corpora have mainly been proposed for English and do not generalize to gendered languages due to the intrinsic linguistic differences between English and gendered languages. This paper introduces a novel methodology that leverages the contextual understanding capabilities of LLMs to quantitatively measure gender representation bias in Spanish corpora. By utilizing LLMs to identify and classify gendered nouns and pronouns in relation to their reference to human entities, our approach provides a robust analysis of gender representation bias in gendered languages. We empirically validate our method on four widely-used benchmark datasets, uncovering significant gender prevalence disparities with a male-to-female ratio ranging from 4:1 to 6:1. These findings demonstrate the value of our methodology for bias quantification in gendered language corpora and suggest its application in NLP, contributing to the development of more equitable language technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.13677v2</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Erik Derner, Sara Sansalvador de la Fuente, Yoan Guti\'errez, Paloma Moreda, Nuria Oliver</dc:creator>
    </item>
    <item>
      <title>Evaluating Language Models for Generating and Judging Programming Feedback</title>
      <link>https://arxiv.org/abs/2407.04873</link>
      <description>arXiv:2407.04873v2 Announce Type: replace-cross 
Abstract: The emergence of large language models (LLMs) has transformed research and practice across a wide range of domains. Within the computing education research (CER) domain, LLMs have garnered significant attention, particularly in the context of learning programming. Much of the work on LLMs in CER, however, has focused on applying and evaluating proprietary models. In this article, we evaluate the efficiency of open-source LLMs in generating high-quality feedback for programming assignments and judging the quality of programming feedback, contrasting the results with proprietary models. Our evaluations on a dataset of students' submissions to introductory Python programming exercises suggest that state-of-the-art open-source LLMs are nearly on par with proprietary models in both generating and assessing programming feedback. Additionally, we demonstrate the efficiency of smaller LLMs in these tasks and highlight the wide range of LLMs accessible, even for free, to educators and practitioners.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.04873v2</guid>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Mon, 25 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Charles Koutcheme, Nicola Dainese, Arto Hellas, Sami Sarsa, Juho Leinonen, Syed Ashraf, Paul Denny</dc:creator>
    </item>
  </channel>
</rss>
