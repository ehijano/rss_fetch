<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>econ.EM updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/econ.EM</link>
    <description>econ.EM updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/econ.EM" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 24 May 2024 04:01:21 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 24 May 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Exogenous Consideration and Extended Random Utility</title>
      <link>https://arxiv.org/abs/2405.13945</link>
      <description>arXiv:2405.13945v1 Announce Type: new 
Abstract: In a consideration set model, an individual maximizes utility among the considered alternatives. I relate a consideration set additive random utility model to classic discrete choice and the extended additive random utility model, in which utility can be $-\infty$ for infeasible alternatives. When observable utility shifters are bounded, all three models are observationally equivalent. Moreover, they have the same counterfactual bounds and welfare formulas for changes in utility shifters like price. For attention interventions, welfare cannot change in the full consideration model but is completely unbounded in the limited consideration model. The identified set for consideration set probabilities has a minimal width for any bounded support of shifters, but with unbounded support it is a point: identification "towards" infinity does not resemble identification "at" infinity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13945v1</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Roy Allen</dc:creator>
    </item>
    <item>
      <title>On the Identifying Power of Monotonicity for Average Treatment Effects</title>
      <link>https://arxiv.org/abs/2405.14104</link>
      <description>arXiv:2405.14104v1 Announce Type: new 
Abstract: In the context of a binary outcome, treatment, and instrument, Balke and Pearl (1993, 1997) establish that adding monotonicity to the instrument exogeneity assumption does not decrease the identified sets for average potential outcomes and average treatment effect parameters when those assumptions are consistent with the distribution of the observable data. We show that the same results hold in the broader context of multi-valued outcome, treatment, and instrument. An important example of such a setting is a multi-arm randomized controlled trial with noncompliance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.14104v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuehao Bai, Shunzhuang Huang, Sarah Moon, Azeem M. Shaikh, Edward J. Vytlacil</dc:creator>
    </item>
    <item>
      <title>Integrating behavioral experimental findings into dynamical models to inform social change interventions</title>
      <link>https://arxiv.org/abs/2405.13224</link>
      <description>arXiv:2405.13224v1 Announce Type: cross 
Abstract: Addressing global challenges -- from public health to climate change -- often involves stimulating the large-scale adoption of new products or behaviors. Research traditions that focus on individual decision making suggest that achieving this objective requires better identifying the drivers of individual adoption choices. On the other hand, computational approaches rooted in complexity science focus on maximizing the propagation of a given product or behavior throughout social networks of interconnected adopters. The integration of these two perspectives -- although advocated by several research communities -- has remained elusive so far. Here we show how achieving this integration could inform seeding policies to facilitate the large-scale adoption of a given behavior or product. Drawing on complex contagion and discrete choice theories, we propose a method to estimate individual-level thresholds to adoption, and validate its predictive power in two choice experiments. By integrating the estimated thresholds into computational simulations, we show that state-of-the-art seeding methods for social influence maximization might be suboptimal if they neglect individual-level behavioral drivers, which can be corrected through the proposed experimental method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13224v1</guid>
      <category>physics.soc-ph</category>
      <category>cs.SI</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Radu Tanase, Ren\'e Algesheimer, Manuel S. Mariani</dc:creator>
    </item>
    <item>
      <title>Some models are useful, but for how long?: A decision theoretic approach to choosing when to refit large-scale prediction models</title>
      <link>https://arxiv.org/abs/2405.13926</link>
      <description>arXiv:2405.13926v1 Announce Type: cross 
Abstract: Large-scale prediction models (typically using tools from artificial intelligence, AI, or machine learning, ML) are increasingly ubiquitous across a variety of industries and scientific domains. Such methods are often paired with detailed data from sources such as electronic health records, wearable sensors, and omics data (high-throughput technology used to understand biology). Despite their utility, implementing AI and ML tools at the scale necessary to work with this data introduces two major challenges. First, it can cost tens of thousands of dollars to train a modern AI/ML model at scale. Second, once the model is trained, its predictions may become less relevant as patient and provider behavior change, and predictions made for one geographical area may be less accurate for another. These two challenges raise a fundamental question: how often should you refit the AI/ML model to optimally trade-off between cost and relevance? Our work provides a framework for making decisions about when to {\it refit} AI/ML models when the goal is to maintain valid statistical inference (e.g. estimating a treatment effect in a clinical trial). Drawing on portfolio optimization theory, we treat the decision of {\it recalibrating} versus {\it refitting} the model as a choice between ''investing'' in one of two ''assets.'' One asset, recalibrating the model based on another model, is quick and relatively inexpensive but bears uncertainty from sampling and the possibility that the other model is not relevant to current circumstances. The other asset, {\it refitting} the model, is costly but removes the irrelevance concern (though not the risk of sampling error). We explore the balancing act between these two potential investments in this paper.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.13926v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kentaro Hoffman, Stephen Salerno, Jeff Leek, Tyler McCormick</dc:creator>
    </item>
    <item>
      <title>The Spectral Approach to Linear Rational Expectations Models</title>
      <link>https://arxiv.org/abs/2007.13804</link>
      <description>arXiv:2007.13804v5 Announce Type: replace 
Abstract: This paper considers linear rational expectations models in the frequency domain. The paper characterizes existence and uniqueness of solutions to particular as well as generic systems. The set of all solutions to a given system is shown to be a finite dimensional affine space in the frequency domain. It is demonstrated that solutions can be discontinuous with respect to the parameters of the models in the context of non-uniqueness, invalidating mainstream frequentist and Bayesian methods. The ill-posedness of the problem motivates regularized solutions with theoretically guaranteed uniqueness, continuity, and even differentiability properties.</description>
      <guid isPermaLink="false">oai:arXiv.org:2007.13804v5</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Majid M. Al-Sadoon</dc:creator>
    </item>
    <item>
      <title>A Multivariate Realized GARCH Model</title>
      <link>https://arxiv.org/abs/2012.02708</link>
      <description>arXiv:2012.02708v2 Announce Type: replace 
Abstract: We propose a novel class of multivariate GARCH models that utilize realized measures of volatilities and correlations. The central component is an unconstrained vector parametrization of the conditional correlation matrix that facilitates factor models for correlations. This offers an elegant solution to the primary challenge that plagues multivariate GARCH models in high-dimensional settings. As an illustration, we consider block correlation structures that naturally simplify to linear factor models for the conditional correlations. We apply the model to returns of nine assets and inspect in-sample and out-of-sample model performance in comparison with several popular benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2012.02708v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ilya Archakov, Peter Reinhard Hansen, Asger Lunde</dc:creator>
    </item>
    <item>
      <title>Large Skew-t Copula Models and Asymmetric Dependence in Intraday Equity Returns</title>
      <link>https://arxiv.org/abs/2308.05564</link>
      <description>arXiv:2308.05564v3 Announce Type: replace 
Abstract: Skew-t copula models are attractive for the modeling of financial data because they allow for asymmetric and extreme tail dependence. We show that the copula implicit in the skew-t distribution of Azzalini and Capitanio (2003) allows for a higher level of pairwise asymmetric dependence than two popular alternative skew-t copulas. Estimation of this copula in high dimensions is challenging, and we propose a fast and accurate Bayesian variational inference (VI) approach to do so. The method uses a generative representation of the skew-t distribution to define an augmented posterior that can be approximated accurately. A stochastic gradient ascent algorithm is used to solve the variational optimization. The methodology is used to estimate skew-t factor copula models with up to 15 factors for intraday returns from 2017 to 2021 on 93 U.S. equities. The copula captures substantial heterogeneity in asymmetric dependence over equity pairs, in addition to the variability in pairwise correlations. In a moving window study we show that the asymmetric dependencies also vary over time, and that intraday predictive densities from the skew-t copula are more accurate than those from benchmark copula models. Portfolio selection strategies based on the estimated pairwise asymmetric dependencies improve performance relative to the index.</description>
      <guid isPermaLink="false">oai:arXiv.org:2308.05564v3</guid>
      <category>econ.EM</category>
      <category>cs.LG</category>
      <category>q-fin.ST</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</dc:creator>
    </item>
    <item>
      <title>Inference for Rank-Rank Regressions</title>
      <link>https://arxiv.org/abs/2310.15512</link>
      <description>arXiv:2310.15512v2 Announce Type: replace 
Abstract: Slope coefficients in rank-rank regressions are popular measures of intergenerational mobility. In this paper, we first point out two important properties of the OLS estimator in such regressions: commonly used variance estimators do not consistently estimate the asymptotic variance of the OLS estimator and, when the underlying distribution is not continuous, the OLS estimator may be highly sensitive to the way in which ties are handled. Motivated by these findings we derive the asymptotic theory for the OLS estimator in a general rank-rank regression specification without making assumptions about the continuity of the underlying distribution. We then extend the asymptotic theory to other regressions involving ranks that have been used in empirical work. Finally, we apply our new inference methods to three empirical studies. We find that the confidence intervals based on estimators of the correct variance may sometimes be substantially shorter and sometimes substantially longer than those based on commonly used variance estimators. The differences in confidence intervals concern economically meaningful values of mobility and thus may lead to different conclusions when comparing mobility across different regions or countries.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.15512v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Denis Chetverikov, Daniel Wilhelm</dc:creator>
    </item>
    <item>
      <title>Inside the black box: Neural network-based real-time prediction of US recessions</title>
      <link>https://arxiv.org/abs/2310.17571</link>
      <description>arXiv:2310.17571v3 Announce Type: replace 
Abstract: Long short-term memory (LSTM) and gated recurrent unit (GRU) are used to model US recessions from 1967 to 2021. Their predictive performances are compared to those of the traditional linear models. The out-of-sample performance suggests the application of LSTM and GRU in recession forecasting, especially for longer-term forecasts. The Shapley additive explanations (SHAP) method is applied to both groups of models. The SHAP-based different weight assignments imply the capability of these types of neural networks to capture the business cycle asymmetries and nonlinearities. The SHAP method delivers key recession indicators, such as the S&amp;P 500 index for short-term forecasting up to 3 months and the term spread for longer-term forecasting up to 12 months. These findings are robust against other interpretation methods, such as the local interpretable model-agnostic explanations (LIME) and the marginal effects.</description>
      <guid isPermaLink="false">oai:arXiv.org:2310.17571v3</guid>
      <category>econ.EM</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Seulki Chung</dc:creator>
    </item>
    <item>
      <title>The Role of Carbon Pricing in Food Inflation: Evidence from Canadian Provinces</title>
      <link>https://arxiv.org/abs/2404.09467</link>
      <description>arXiv:2404.09467v5 Announce Type: replace 
Abstract: In the search for political-economic tools for greenhouse gas mitigation, carbon pricing, which includes carbon tax and cap-and-trade, is implemented by many governments. However, the inflating food prices in carbon-pricing countries, such as Canada, have led many to believe such policies harm food affordability. This study aims to identify changes in food prices induced by carbon pricing using the case of Canadian provinces. Using the staggered difference-in-difference (DiD) approach, we find an overall deflationary effect of carbon pricing on food prices (measured by monthly provincial food CPI). The average reductions in food CPI compared to before carbon pricing are $2\%$ and $4\%$ within and beyond two years of implementation. We further find that the deflationary effects are partially driven by lower consumption with no significant change via farm input costs. Evidence in this paper suggests no inflationary effect of carbon pricing in Canadian provinces, thus giving no support to the growing voices against carbon pricing policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09467v5</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jiansong Xu</dc:creator>
    </item>
    <item>
      <title>Random Utility Models with Skewed Random Components: the Smallest versus Largest Extreme Value Distribution</title>
      <link>https://arxiv.org/abs/2405.08222</link>
      <description>arXiv:2405.08222v2 Announce Type: replace 
Abstract: At the core of most random utility models (RUMs) is an individual agent with a random utility component following a largest extreme value Type I (LEVI) distribution. What if, instead, the random component follows its mirror image -- the smallest extreme value Type I (SEVI) distribution? Differences between these specifications, closely tied to the random component's skewness, can be quite profound. For the same preference parameters, the two RUMs, equivalent with only two choice alternatives, diverge progressively as the number of alternatives increases, resulting in substantially different estimates and predictions for key measures, such as elasticities and market shares.
  The LEVI model imposes the well-known independence-of-irrelevant-alternatives property, while SEVI does not. Instead, the SEVI choice probability for a particular option involves enumerating all subsets that contain this option. The SEVI model, though more complex to estimate, is shown to have computationally tractable closed-form choice probabilities. Much of the paper delves into explicating the properties of the SEVI model and exploring implications of the random component's skewness.
  Conceptually, the difference between the LEVI and SEVI models centers on whether information, known only to the agent, is more likely to increase or decrease the systematic utility parameterized using observed attributes. LEVI does the former; SEVI the latter. An immediate implication is that if choice is characterized by SEVI random components, then the observed choice is more likely to correspond to the systematic-utility-maximizing choice than if characterized by LEVI. Examining standard empirical examples from different applied areas, we find that the SEVI model outperforms the LEVI model, suggesting the relevance of its inclusion in applied researchers' toolkits.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08222v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Richard T. Carson, Derrick H. Sun, Yixiao Sun</dc:creator>
    </item>
    <item>
      <title>Instrumented Difference-in-Differences with heterogeneous treatment effects</title>
      <link>https://arxiv.org/abs/2405.12083</link>
      <description>arXiv:2405.12083v2 Announce Type: replace 
Abstract: Many studies exploit variation in the timing of policy adoption across units as an instrument for treatment, and use instrumental variable techniques. This paper formalizes the underlying identification strategy as an instrumented difference-in-differences (DID-IV). In a simple setting with two periods and two groups, our DID-IV design mainly consists of a monotonicity assumption, and parallel trends assumptions in the treatment and the outcome. In this design, a Wald-DID estimand, which scales the DID estimand of the outcome by the DID estimand of the treatment, captures the local average treatment effect on the treated (LATET). In contrast to Fuzzy DID design considered in \cite{De_Chaisemartin2018-xe}, our DID-IV design does not {\it ex-ante} require strong restrictions on the treatment adoption behavior across units, and our target parameter, the LATET, is policy-relevant if the instrument is based on the policy change of interest to the researcher. We extend the canonical DID-IV design to multiple period settings with the staggered adoption of the instrument across units, which we call staggered DID-IV designs. We propose an estimation method in staggered DID-IV designs that is robust to treatment effect heterogeneity. We illustrate our findings in the setting of \cite{Oreopoulos2006-bn}, estimating returns to schooling in the United Kingdom. In this application, the two-way fixed effects instrumental variable regression, which is the conventional approach to implement staggered DID-IV designs, yields a negative estimate, whereas our estimation method indicates the substantial gain from schooling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.12083v2</guid>
      <category>econ.EM</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Sho Miyaji</dc:creator>
    </item>
  </channel>
</rss>
