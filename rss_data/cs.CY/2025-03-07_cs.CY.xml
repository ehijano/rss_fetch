<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 Mar 2025 05:02:14 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Multimodal Generative AI and Foundation Models for Behavioural Health in Online Gambling</title>
      <link>https://arxiv.org/abs/2503.03752</link>
      <description>arXiv:2503.03752v1 Announce Type: new 
Abstract: Online gambling platforms have transformed the gambling landscape, offering unprecedented accessibility and personalized experiences. However, these same characteristics have increased the risk of gambling-related harm, affecting individuals, families, and communities. Structural factors, including targeted marketing, shifting social norms, and gaps in regulation, further complicate the challenge. This narrative review examines how artificial intelligence, particularly multimodal generative models and foundation technologies, can address these issues by supporting prevention, early identification, and harm-reduction efforts. We detail applications such as synthetic data generation to overcome research barriers, customized interventions to guide safer behaviors, gamified tools to support recovery, and scenario modeling to inform effective policies. Throughout, we emphasize the importance of safeguarding privacy and ensuring that technological advances are responsibly aligned with public health objectives.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03752v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konrad Samsel, Mohammad Noaeen, Neil Seeman, Karim Keshavjee, Li-Jia Li, Zahra Shakeri</dc:creator>
    </item>
    <item>
      <title>A Bridge to Nowhere: A Healthcare Case Study for Non-Reformist Design</title>
      <link>https://arxiv.org/abs/2503.03849</link>
      <description>arXiv:2503.03849v1 Announce Type: new 
Abstract: In the face of intensified datafication and automation in public sector industries, frameworks like design justice and the feminist practice of refusal provide help to identify and mitigate structural harm and challenge inequities reproduced in digitized infrastructures. This paper applies those frameworks to emerging efforts across the U.S. healthcare industry to automate prior authorization -- a process whereby insurance companies determine whether a treatment or service is 'medically necessary' before agreeing to cover it. Federal regulatory interventions turn to datafication and automation to reduce the harms of this widely unpopular process shown to delay vital treatments and create immense administrative burden for healthcare providers and patients. This paper explores emerging prior authorization reforms as a case study, applying the frameworks of design justice and refusal to highlight the inherent conservatism of interventions oriented towards improving the user experience of extractive systems. I further explore how the abolitionist framework of non-reformist reform helps to clarify alternative interventions that would mitigate the harms of prior authorization in ways that do not reproduce or extend the power of insurance companies. I propose a set of four tenets for nonreformist design to mitigate structural harms and advance design justice in a broad set of domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03849v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Linda Huber</dc:creator>
    </item>
    <item>
      <title>Reflecting on Potentials for Post-Growth Social Media Platform Design</title>
      <link>https://arxiv.org/abs/2503.03939</link>
      <description>arXiv:2503.03939v1 Announce Type: new 
Abstract: Sudden attention on social media, and how users navigate these contextual shifts, has been a focus of much recent work in social media research. Even when this attention is not harassing, some users experience this sudden growth as overwhelming. In this workshop paper, I outline how growth infuses the design of much of the modern social media platform landscape, and then explore why applying a post-growth lens to platform design could be productive.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03939v1</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Joseph S. Schafer</dc:creator>
    </item>
    <item>
      <title>Prompt Programming: A Platform for Dialogue-based Computational Problem Solving with Generative AI Models</title>
      <link>https://arxiv.org/abs/2503.04267</link>
      <description>arXiv:2503.04267v1 Announce Type: new 
Abstract: Computing students increasingly rely on generative AI tools for programming assistance, often without formal instruction or guidance. This highlights a need to teach students how to effectively interact with AI models, particularly through natural language prompts, to generate and critically evaluate code for solving computational tasks. To address this, we developed a novel platform for prompt programming that enables authentic dialogue-based interactions, supports problems involving multiple interdependent functions, and offers on-request execution of generated code. Data analysis from over 900 students in an introductory programming course revealed high engagement, with the majority of prompts occurring within multi-turn dialogues. Problems with multiple interdependent functions encouraged iterative refinement, with progression graphs highlighting several common strategies. Students were highly selective about the code they chose to test, suggesting that on-request execution of generated code promoted critical thinking. Given the growing importance of learning dialogue-based programming with AI, we provide this tool as a publicly accessible resource, accompanied by a corpus of programming problems for educational use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04267v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Victor-Alexandru P\u{a}durean, Paul Denny, Alkis Gotovos, Adish Singla</dc:creator>
    </item>
    <item>
      <title>Talking Back -- human input and explanations to interactive AI systems</title>
      <link>https://arxiv.org/abs/2503.04343</link>
      <description>arXiv:2503.04343v1 Announce Type: new 
Abstract: While XAI focuses on providing AI explanations to humans, can the reverse - humans explaining their judgments to AI - foster richer, synergistic human-AI systems? This paper explores various forms of human inputs to AI and examines how human explanations can guide machine learning models toward automated judgments and explanations that align more closely with human concepts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04343v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alan Dix, Tommaso Turchi, Ben Wilson, Anna Monreale, Matt Roach</dc:creator>
    </item>
    <item>
      <title>Dyads: Artist-Centric, AI-Generated Dance Duets</title>
      <link>https://arxiv.org/abs/2503.03954</link>
      <description>arXiv:2503.03954v1 Announce Type: cross 
Abstract: Existing AI-generated dance methods primarily train on motion capture data from solo dance performances, but a critical feature of dance in nearly any genre is the interaction of two or more bodies in space. Moreover, many works at the intersection of AI and dance fail to incorporate the ideas and needs of the artists themselves into their development process, yielding models that produce far more useful insights for the AI community than for the dance community. This work addresses both needs of the field by proposing an AI method to model the complex interactions between pairs of dancers and detailing how the technical methodology can be shaped by ongoing co-creation with the artistic stakeholders who curated the movement data. Our model is a probability-and-attention-based Variational Autoencoder that generates a choreographic partner conditioned on an input dance sequence. We construct a custom loss function to enhance the smoothness and coherence of the generated choreography. Our code is open-source, and we also document strategies for other interdisciplinary research teams to facilitate collaboration and strong communication between artists and technologists.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.03954v1</guid>
      <category>cs.LG</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zixuan Wang, Luis Zerkowski, Ilya Vidrin, Mariel Pettee</dc:creator>
    </item>
    <item>
      <title>Uncovering inequalities in new knowledge learning by large language models across different languages</title>
      <link>https://arxiv.org/abs/2503.04064</link>
      <description>arXiv:2503.04064v1 Announce Type: cross 
Abstract: As large language models (LLMs) gradually become integral tools for problem solving in daily life worldwide, understanding linguistic inequality is becoming increasingly important. Existing research has primarily focused on static analyses that assess the disparities in the existing knowledge and capabilities of LLMs across languages. However, LLMs are continuously evolving, acquiring new knowledge to generate up-to-date, domain-specific responses. Investigating linguistic inequalities within this dynamic process is, therefore, also essential. In this paper, we explore inequalities in new knowledge learning by LLMs across different languages and four key dimensions: effectiveness, transferability, prioritization, and robustness. Through extensive experiments under two settings (in-context learning and fine-tuning) using both proprietary and open-source models, we demonstrate that low-resource languages consistently face disadvantages across all four dimensions. By shedding light on these disparities, we aim to raise awareness of linguistic inequalities in LLMs' new knowledge learning, fostering the development of more inclusive and equitable future LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04064v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chenglong Wang, Haoyu Tang, Xiyuan Yang, Yueqi Xie, Jina Suh, Sunayana Sitaram, Junming Huang, Yu Xie, Zhaoya Gong, Xing Xie, Fangzhao Wu</dc:creator>
    </item>
    <item>
      <title>On Fact and Frequency: LLM Responses to Misinformation Expressed with Uncertainty</title>
      <link>https://arxiv.org/abs/2503.04271</link>
      <description>arXiv:2503.04271v1 Announce Type: cross 
Abstract: We study LLM judgments of misinformation expressed with uncertainty. Our experiments study the response of three widely used LLMs (GPT-4o, LlaMA3, DeepSeek-v2) to misinformation propositions that have been verified false and then are transformed into uncertain statements according to an uncertainty typology. Our results show that after transformation, LLMs change their factchecking classification from false to not-false in 25% of the cases. Analysis reveals that the change cannot be explained by predictors to which humans are expected to be sensitive, i.e., modality, linguistic cues, or argumentation strategy. The exception is doxastic transformations, which use linguistic cue phrases such as "It is believed ...".To gain further insight, we prompt the LLM to make another judgment about the transformed misinformation statements that is not related to truth value. Specifically, we study LLM estimates of the frequency with which people make the uncertain statement. We find a small but significant correlation between judgment of fact and estimation of frequency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04271v1</guid>
      <category>cs.CL</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yana van de Sande, Gunes A\c{c}ar, Thabo van Woudenberg, Martha Larson</dc:creator>
    </item>
    <item>
      <title>Inducing Efficient and Equitable Professional Networks through Link Recommendations</title>
      <link>https://arxiv.org/abs/2503.04542</link>
      <description>arXiv:2503.04542v1 Announce Type: cross 
Abstract: Professional networks are a key determinant of individuals' labor market outcomes. They may also play a role in either exacerbating or ameliorating inequality of opportunity across demographic groups. In a theoretical model of professional network formation, we show that inequality can increase even without exogenous in-group preferences, confirming and complementing existing theoretical literature. Increased inequality emerges from the differential leverage privileged and unprivileged individuals have in forming connections due to their asymmetric ex ante prospects. This is a formalization of a source of inequality in the labor market which has not been previously explored.
  We next show how inequality-aware platforms may reduce inequality by subsidizing connections, through link recommendations that reduce costs, between privileged and unprivileged individuals. Indeed, mixed-privilege connections turn out to be welfare improving, over all possible equilibria, compared to not recommending links or recommending some smaller fraction of cross-group links. Taken together, these two findings reveal a stark reality: professional networking platforms that fail to foster integration in the link formation process risk reducing the platform's utility to its users and exacerbating existing labor market inequality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.04542v1</guid>
      <category>cs.GT</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Cynthia Dwork, Chris Hays, Lunjia Hu, Nicole Immorlica, Juan Perdomo</dc:creator>
    </item>
    <item>
      <title>Ten simple rules for training scientists to make better software</title>
      <link>https://arxiv.org/abs/2402.04722</link>
      <description>arXiv:2402.04722v2 Announce Type: replace 
Abstract: Computational methods and associated software implementations are central to every field of scientific investigation. Modern biological research, particularly within systems biology, has relied heavily on the development of software tools to process and organize increasingly large datasets, simulate complex mechanistic models, provide tools for the analysis and management of data, and visualize and organize outputs. However, developing high-quality research software requires scientists to develop a host of software development skills, and teaching these skills to students is challenging. There has been a growing importance placed on ensuring reproducibility and good development practices in computational research. However, less attention has been devoted to informing the specific teaching strategies which are effective at nurturing in researchers the complex skillset required to produce high-quality software that, increasingly, is required to underpin both academic and industrial biomedical research. Recent articles in the Ten Simple Rules collection have discussed the teaching of foundational computer science and coding techniques to biology students. We advance this discussion by describing the specific steps for effectively teaching the necessary skills scientists need to develop sustainable software packages which are fit for (re-)use in academic research or more widely. Although our advice is likely to be applicable to all students and researchers hoping to improve their software development skills, our guidelines are directed towards an audience of students that have some programming literacy but little formal training in software development or engineering, typical of early doctoral students. These practices are also applicable outside of doctoral training environments, and we believe they should form a key part of postgraduate training schemes more generally in the life sciences.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.04722v2</guid>
      <category>cs.CY</category>
      <category>cs.SE</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:DOI>10.1371/journal.pcbi.1012410</arxiv:DOI>
      <dc:creator>Kit Gallagher, Richard Creswell, Ben Lambert, Martin Robinson, Chon Lok Lei, Gary R. Mirams, David J. Gavaghan</dc:creator>
    </item>
    <item>
      <title>WIP: Identifying Tutorial Affordances for Interdisciplinary Learning Environments</title>
      <link>https://arxiv.org/abs/2408.14576</link>
      <description>arXiv:2408.14576v2 Announce Type: replace 
Abstract: This work-in-progress research paper explores the effectiveness of tutorials in interdisciplinary learning environments, specifically focusing on bioinformatics. Tutorials are typically designed for a single audience, but our study aims to uncover how they function in contexts where learners have diverse backgrounds. With the rise of interdisciplinary learning, the importance of learning materials that accommodate diverse learner needs has become evident. We chose bioinformatics as our context because it involves at least two distinct user groups: those with computational backgrounds and those with biological backgrounds. The goal of our research is to better understand current bioinformatics software tutorial designs and assess them in the conceptual framework of interdisciplinarity. We conducted a content analysis of 22 representative bioinformatics software tutorials to identify design patterns and understand their strengths and limitations. We found common codes in the representative tutorials and synthesized them into ten themes. Our assessment shows degrees to which current bioinformatics software tutorials fulfill interdisciplinarity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.14576v2</guid>
      <category>cs.CY</category>
      <category>cs.HC</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/FIE61694.2024.10893187</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE Frontiers in Education Conference (FIE), Washington, DC, USA, 2024, pp. 1-5</arxiv:journal_reference>
      <dc:creator>Hannah Kim, Sergei L. Kosakovsky Pond, Stephen MacNeil</dc:creator>
    </item>
    <item>
      <title>ChatGPT vs Social Surveys: Probing Objective and Subjective Silicon Population</title>
      <link>https://arxiv.org/abs/2409.02601</link>
      <description>arXiv:2409.02601v3 Announce Type: replace 
Abstract: Recent discussions about Large Language Models (LLMs) indicate that they have the potential to simulate human responses in social surveys and generate reliable predictions, such as those found in political polls. However, the existing findings are highly inconsistent, leaving us uncertain about the population characteristics of data generated by LLMs. In this paper, we employ repeated random sampling to create sampling distributions that identify the population parameters of silicon samples generated by GPT. Our findings show that GPT's demographic distribution aligns with the 2020 U.S. population in terms of gender and average age. However, GPT significantly overestimates the representation of the Black population and individuals with higher levels of education, even when it possesses accurate knowledge. Furthermore, GPT's point estimates for attitudinal scores are highly inconsistent and show no clear inclination toward any particular ideology. The sample response distributions exhibit a normal pattern that diverges significantly from those of human respondents. Consistent with previous studies, we find that GPT's answers are more deterministic than those of humans. We conclude by discussing the concerning implications of this biased and deterministic silicon population for making inferences about real-world populations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.02601v3</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Muzhi Zhou, Lu Yu, Xiaomin Geng, Lan Luo</dc:creator>
    </item>
    <item>
      <title>Examining the Representation of Youth in the US Policy Documents through the Lens of Research</title>
      <link>https://arxiv.org/abs/2501.07858</link>
      <description>arXiv:2501.07858v2 Announce Type: replace 
Abstract: This study explores the representation of youth in US policy documents by analyzing how research on youth topics is cited within these policies. The research focuses on three key questions: identifying the frequently discussed topics in youth research that receive citations in policy documents, discerning patterns in youth research that contribute to higher citation rates in policy, and comparing the alignment between topics in youth research and those in citing policy documents. Through this analysis, the study aims to shed light on the relationship between academic research and policy formulation, highlighting areas where youth issues are effectively integrated into policy and contributing to the broader goal of enhancing youth engagement in societal decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.07858v2</guid>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1109/BigData62323.2024.10825996</arxiv:DOI>
      <dc:creator>Miftahul Jannat Mokarrama, Abdul Rahman Shaikh, Hamed Alhoori</dc:creator>
    </item>
    <item>
      <title>Do Not Trust Licenses You See -- Dataset Compliance Requires Massive-Scale AI-Powered Lifecycle Tracing</title>
      <link>https://arxiv.org/abs/2503.02784</link>
      <description>arXiv:2503.02784v2 Announce Type: replace 
Abstract: This paper argues that a dataset's legal risk cannot be accurately assessed by its license terms alone; instead, tracking dataset redistribution and its full lifecycle is essential. However, this process is too complex for legal experts to handle manually at scale. Tracking dataset provenance, verifying redistribution rights, and assessing evolving legal risks across multiple stages require a level of precision and efficiency that exceeds human capabilities. Addressing this challenge effectively demands AI agents that can systematically trace dataset redistribution, analyze compliance, and identify legal risks. We develop an automated data compliance system called NEXUS and show that AI can perform these tasks with higher accuracy, efficiency, and cost-effectiveness than human experts. Our massive legal analysis of 17,429 unique entities and 8,072 license terms using this approach reveals the discrepancies in legal rights between the original datasets before redistribution and their redistributed subsets, underscoring the necessity of the data lifecycle-aware compliance. For instance, we find that out of 2,852 datasets with commercially viable individual license terms, only 605 (21%) are legally permissible for commercialization. This work sets a new standard for AI data governance, advocating for a framework that systematically examines the entire lifecycle of dataset redistribution to ensure transparent, legal, and responsible dataset management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02784v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jaekyeom Kim, Sungryull Sohn, Gerrard Jeongwon Jo, Jihoon Choi, Kyunghoon Bae, Hwayoung Lee, Yongmin Park, Honglak Lee</dc:creator>
    </item>
    <item>
      <title>Inferring Mood-While-Eating with Smartphone Sensing and Community-Based Model Personalization</title>
      <link>https://arxiv.org/abs/2306.00723</link>
      <description>arXiv:2306.00723v3 Announce Type: replace-cross 
Abstract: The interplay between mood and eating episodes has been extensively researched, revealing a connection between the two. Previous studies have relied on questionnaires and mobile phone self-reports to investigate the relationship between mood and eating. However, current literature exhibits several limitations: a lack of investigation into the generalization of mood inference models trained with data from various everyday life situations to specific contexts like eating; an absence of studies using sensor data to explore the intersection of mood and eating; and inadequate examination of model personalization techniques within limited label settings, a common challenge in mood inference (i.e., far fewer negative mood reports compared to positive or neutral reports). In this study, we sought to examine everyday eating behavior and mood using two datasets of college students in Mexico (N_mex = 84, 1843 mood-while-eating reports) and eight countries (N_mul = 678, 24K mood-while-eating reports), which contain both passive smartphone sensing and self-report data. Our results indicate that generic mood inference models experience a decline in performance in specific contexts, such as during eating, highlighting the issue of sub-context shifts in mobile sensing. Moreover, we discovered that population-level (non-personalized) and hybrid (partially personalized) modeling techniques fall short in the commonly used three-class mood inference task (positive, neutral, negative). To overcome these limitations, we implemented a novel community-based personalization approach. Our findings demonstrate that mood-while-eating can be inferred with accuracies 63.8% (with F1-score of 62.5) for the MEX dataset and 88.3% (with F1-score of 85.7) with the MUL dataset using community-based models, surpassing those achieved with traditional methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00723v3</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wageesha Bangamuarachchi, Anju Chamantha, Lakmal Meegahapola, Haeeun Kim, Salvador Ruiz-Correa, Indika Perera, Daniel Gatica-Perez</dc:creator>
    </item>
    <item>
      <title>Urban highways are barriers to social ties</title>
      <link>https://arxiv.org/abs/2404.11596</link>
      <description>arXiv:2404.11596v3 Announce Type: replace-cross 
Abstract: Urban highways are common, especially in the US, making cities more car-centric. They promise the annihilation of distance but obstruct pedestrian mobility, thus playing a key role in limiting social interactions locally. Although this limiting role is widely acknowledged in urban studies, the quantitative relationship between urban highways and social ties is barely tested. Here we define a Barrier Score that relates massive, geolocated online social network data to highways in the 50 largest US cities. At the unprecedented granularity of individual social ties, we show that urban highways are associated with decreased social connectivity. This barrier effect is especially strong for short distances and consistent with historical cases of highways that were built to purposefully disrupt or isolate Black neighborhoods. By combining spatial infrastructure with social tie data, our method adds a new dimension to demographic studies of social segregation. Our study can inform reparative planning for an evidence-based reduction of spatial inequality, and more generally, support a better integration of the social fabric in urban planning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.11596v3</guid>
      <category>physics.soc-ph</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1073/pnas.2408937122</arxiv:DOI>
      <arxiv:journal_reference>Proc. Natl. Acad. Sci. U.S.A. 122 (10) e2408937122, 2025</arxiv:journal_reference>
      <dc:creator>Luca Maria Aiello, Anastassia Vybornova, S\'andor Juh\'asz, Michael Szell, Eszter Bok\'anyi</dc:creator>
    </item>
    <item>
      <title>Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction</title>
      <link>https://arxiv.org/abs/2409.07055</link>
      <description>arXiv:2409.07055v2 Announce Type: replace-cross 
Abstract: Legal judgment prediction (LJP), which enables litigants and their lawyers to forecast judgment outcomes and refine litigation strategies, has emerged as a crucial legal NLP task. Existing studies typically utilize legal facts, i.e., facts that have been established by evidence and determined by the judge, to predict the judgment. However, legal facts are often difficult to obtain in the early stages of litigation, significantly limiting the practical applicability of fact-based LJP. To address this limitation, we propose a novel legal NLP task: \textit{legal fact prediction} (LFP), which takes the evidence submitted by litigants for trial as input to predict legal facts, thereby empowering fact-based LJP technologies to perform prediction in the absence of ground-truth legal facts. We also propose the first benchmark dataset, LFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench demonstrate the effectiveness of LFP-empowered LJP and highlight promising research directions for LFP. Our code and data are available at https://github.com/HPRCEST/LFPBench.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.07055v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Junkai Liu, Yujie Tong, Hui Huang, Bowen Zheng, Yiran Hu, Peicheng Wu, Chuan Xiao, Makoto Onizuka, Muyun Yang, Shuyuan Zheng</dc:creator>
    </item>
    <item>
      <title>Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes Circulated in 2024</title>
      <link>https://arxiv.org/abs/2503.02857</link>
      <description>arXiv:2503.02857v2 Announce Type: replace-cross 
Abstract: In the age of increasingly realistic generative AI, robust deepfake detection is essential for mitigating fraud and disinformation. While many deepfake detectors report high accuracy on academic datasets, we show that these academic benchmarks are out of date and not representative of real-world deepfakes. We introduce Deepfake-Eval-2024, a new deepfake detection benchmark consisting of in-the-wild deepfakes collected from social media and deepfake detection platform users in 2024. Deepfake-Eval-2024 consists of 45 hours of videos, 56.5 hours of audio, and 1,975 images, encompassing the latest manipulation technologies. The benchmark contains diverse media content from 88 different websites in 52 different languages. We find that the performance of open-source state-of-the-art deepfake detection models drops precipitously when evaluated on Deepfake-Eval-2024, with AUC decreasing by 50\% for video, 48\% for audio, and 45\% for image models compared to previous benchmarks. We also evaluate commercial deepfake detection models and models finetuned on Deepfake-Eval-2024, and find that they have superior performance to off-the-shelf open-source models, but do not yet reach the accuracy of deepfake forensic analysts. The dataset is available at https://github.com/nuriachandra/Deepfake-Eval-2024.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.02857v2</guid>
      <category>cs.CV</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Fri, 07 Mar 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Nuria Alina Chandra, Ryan Murtfeldt, Lin Qiu, Arnab Karmakar, Hannah Lee, Emmanuel Tanumihardja, Kevin Farhat, Ben Caffee, Sejin Paik, Changyeon Lee, Jongwook Choi, Aerin Kim, Oren Etzioni</dc:creator>
    </item>
  </channel>
</rss>
