<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 08 Aug 2025 04:01:06 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Statistical inference for core-periphery structures</title>
      <link>https://arxiv.org/abs/2508.04730</link>
      <description>arXiv:2508.04730v1 Announce Type: new 
Abstract: Core-periphery (CP) structure is an important meso-scale network property where nodes group into a small, densely interconnected {core} and a sparse {periphery} whose members primarily connect to the core rather than to each other. While this structure has been observed in numerous real-world networks, there has been minimal statistical formalization of it. In this work, we develop a statistical framework for CP structures by introducing a model-agnostic and generalizable population parameter which quantifies the strength of a CP structure at the level of the data-generating mechanism. We study this parameter under four canonical random graph models and establish theoretical guarantees for label recovery, including exact label recovery. Next, we construct intersection tests for validating the presence and strength of a CP structure under multiple null models, and prove theoretical guarantees for type I error and power. These tests provide a formal distinction between exogenous (or induced) and endogenous (or intrinsic) CP structure in heterogeneous networks, enabling a level of structural resolution that goes beyond merely detecting the presence of CP structure. The proposed methods show excellent performance on synthetic data, and our applications demonstrate that statistically significant CP structure is somewhat rare in real-world networks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04730v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Eric Yanchenko, Srijan Sengupta, Diganta Mukherjee</dc:creator>
    </item>
    <item>
      <title>Goodness-of-fit test for multi-layer stochastic block models</title>
      <link>https://arxiv.org/abs/2508.04957</link>
      <description>arXiv:2508.04957v1 Announce Type: new 
Abstract: Community detection in multi-layer networks is a fundamental task in complex network analysis across various areas like social, biological, and computer sciences. However, most existing algorithms assume that the number of communities is known in advance, which is usually impractical for real-world multi-layer networks. To address this limitation, we develop a novel goodness-of-fit test for the popular multi-layer stochastic block model. The test statistic is derived from a normalized aggregation of layer-wise adjacency matrices. Under the null hypothesis that a candidate community count is correct, we establish the asymptotic normality of the test statistic using recent advances in random matrix theory. This theoretical foundation enables a computationally efficient sequential testing algorithm to determine the number of communities. Numerical experiments on simulated and real-world multi-layer networks demonstrate the accuracy and efficiency of our approach in estimating the number of communities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04957v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Huan Qing</dc:creator>
    </item>
    <item>
      <title>Robust Spatio-Temporal Distributional Regression</title>
      <link>https://arxiv.org/abs/2508.05041</link>
      <description>arXiv:2508.05041v1 Announce Type: new 
Abstract: Motivated by investigating spatio-temporal patterns of the distribution of continuous variables, we consider describing the conditional distribution function of the response variable incorporating spatio-temporal components given predictors. In many applications, continuous variables are observed only as threshold-categorized data due to measurement constraints. For instance, ecological measurements often categorize sizes into intervals rather than recording exact values due to practical limitations. To recover the conditional distribution function of the underlying continuous variables, we consider a distribution regression employing models for binomial data obtained at each threshold value. However, depending on spatio-temporal conditions and predictors, the distribution function may frequently exhibit boundary values (zero or one), which can occur either structurally or randomly. This makes standard binomial models inadequate, requiring more flexible modeling approaches. To address this issue, we propose a boundary-inflated binomial model incorporating spatio-temporal components. The model is a three-component mixture of the binomial model and two Dirac measures at zero and one. We develop a computationally efficient Bayesian inference algorithm using P\'olya-Gamma data augmentation and dynamic Gaussian predictive processes. Extensive simulation experiments demonstrate that our procedure significantly outperforms distribution regression methods based on standard binomial models across various scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05041v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tomotaka Momozaki, Shonosuke Sugasawa, Tomoyuki Nakagawa, Hiroko Kato Solvang, Sam Subbey</dc:creator>
    </item>
    <item>
      <title>Covariate adjustment for linear models in estimating treatment effects in randomised clinical trials. Some useful theory to guide simulation</title>
      <link>https://arxiv.org/abs/2508.05459</link>
      <description>arXiv:2508.05459v1 Announce Type: new 
Abstract: Building on key papers that were published in special issues of Biometrics in 1957 and 1982 we propose and develop a three-aspect system for evaluating the effect of fitting covariates in the analysis of designed experiments, in particular randomised clinical trials. The three aspects are: first the effect on residual mean square error, second the effect on the variance inflation factor (VIF) and third the effect on second order precision. We concentrate, in particular, on the VIF and highlight not only an existing formula for its expected value based on assuming covariates have a Normal distribution but also develop a formula for its variance. We show how VIFs for categorical variable are related to the chi-square contingency table with rows as treatment and columns as categories. We illustrate the value of these formulae using a randomised clinical trial with five covariates, one of which is binary, and show that both mean and variance formulae predict results well for all $2^5=32$ possible models for each of three forms of simulation, random permutation, sampling from a Normal distribution and bootstrap resampling. Finally, we illustrate how the three-aspect system may be used to address various questions of interest when considering covariate adjustment.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05459v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Stephen Senn, Franz K\"onig, Martin Posch</dc:creator>
    </item>
    <item>
      <title>Consistency of an Intercept-Shifted Synthetic-Control Estimator under Weighted Parallel Trends</title>
      <link>https://arxiv.org/abs/2508.05604</link>
      <description>arXiv:2508.05604v1 Announce Type: new 
Abstract: The average treatment effect on the treated (ATT) in a staggered-adoption panel is estimated using an intercept-augmented synthetic-control (SCM) estimator. A weighted parallel trends plus an intercept shift, together with mild regularity on the weight vectors (non-degenerate dispersion) and expanding pre-treatment length, are sufficient for consistency allowing for heavy-tailed shocks. These conditions can be more interpretable than the autoregressive or low-rank factor models with light tails assumed by Ben-Michael, Feller, and Rothstein (2022) and expand the valid DGP pool from the same paper. Practical diagnostics to support the assumptions are discussed and situate these results within the recent literature on SC + DiD hybrids.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05604v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Michael Guggisberg</dc:creator>
    </item>
    <item>
      <title>Introducing Powerwise (PWR): A pairwise and Power Rating method for selecting at-large teams to the NCAA Division I Men's Lacrosse Championship</title>
      <link>https://arxiv.org/abs/2508.04919</link>
      <description>arXiv:2508.04919v1 Announce Type: cross 
Abstract: This document describes a new system for selecting teams for the NCAA Men's Division I Lacrosse championship tournament called "Powerwise" that was developed in discussions with the NCAA Lacrosse Selection Criteria and Ranking Committee (SCR). The method is simple, employing hierarchical pairwise comparisons that emphasize on-field performance in head-to-head and common opponent matchups, and a simple Massey/Colley/Sagarin-like statistic called the Power Rating (PR) when on-field results are not conclusive. Power Ratings are based on margin of victory and implicitly account for strength of schedule. Powerwise addresses the complexities of team selection in a way that both coaches and fans can understand while improving the fairness, objectivity, and overall quality of the selection process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.04919v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lawrence Feldman, Matthew Bomparola</dc:creator>
    </item>
    <item>
      <title>DFW: A Novel Weighting Scheme for Covariate Balancing and Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2508.05215</link>
      <description>arXiv:2508.05215v1 Announce Type: cross 
Abstract: Estimating causal effects from observational data is challenging due to selection bias, which leads to imbalanced covariate distributions across treatment groups. Propensity score-based weighting methods are widely used to address this issue by reweighting samples to simulate a randomized controlled trial (RCT). However, the effectiveness of these methods heavily depends on the observed data and the accuracy of the propensity score estimator. For example, inverse propensity weighting (IPW) assigns weights based on the inverse of the propensity score, which can lead to instable weights when propensity scores have high variance-either due to data or model misspecification-ultimately degrading the ability of handling selection bias and treatment effect estimation. To overcome these limitations, we propose Deconfounding Factor Weighting (DFW), a novel propensity score-based approach that leverages the deconfounding factor-to construct stable and effective sample weights. DFW prioritizes less confounded samples while mitigating the influence of highly confounded ones, producing a pseudopopulation that better approximates a RCT. Our approach ensures bounded weights, lower variance, and improved covariate balance.While DFW is formulated for binary treatments, it naturally extends to multi-treatment settings, as the deconfounding factor is computed based on the estimated probability of the treatment actually received by each sample. Through extensive experiments on real-world benchmark and synthetic datasets, we demonstrate that DFW outperforms existing methods, including IPW and CBPS, in both covariate balancing and treatment effect estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.05215v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmad Saeed Khan, Erik Schaffernicht, Johannes Andreas Stork</dc:creator>
    </item>
    <item>
      <title>Sparse Asymptotic PCA: Identifying Sparse Latent Factors Across Time Horizon in High-Dimensional Time Series</title>
      <link>https://arxiv.org/abs/2407.09738</link>
      <description>arXiv:2407.09738v3 Announce Type: replace 
Abstract: This paper introduces a novel sparse latent factor modeling framework using sparse asymptotic Principal Component Analysis (APCA) to analyze the co-movements of high-dimensional panel data over time. Unlike existing methods based on sparse PCA, which assume sparsity in the loading matrices, our approach posits sparsity in the factor processes while allowing non-sparse loadings. This is motivated by the fact that financial returns typically exhibit universal and non-sparse exposure to market factors. Unlike the commonly used $\ell_1$-relaxation in sparse PCA, the proposed sparse APCA employs a truncated power method to estimate the leading sparse factor and a sequential deflation method for multi-factor cases under $\ell_0$-constraints. Furthermore, we develop a data-driven approach to identify the sparsity of risk factors over the time horizon using a novel cross-sectional cross-validation method. We establish the consistency of our estimators under mild conditions as both the dimension $N$ and the sample size $T$ grow. Monte Carlo simulations demonstrate that the proposed method performs well in finite samples. Empirically, we apply our method to daily S&amp;P 500 stock returns (2004--2016) and identify nine risk factors influencing the stock market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.09738v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>q-fin.ST</category>
      <category>stat.ML</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhaoxing Gao</dc:creator>
    </item>
    <item>
      <title>Win Ratio with Multiple Thresholds for Composite Endpoints</title>
      <link>https://arxiv.org/abs/2407.18341</link>
      <description>arXiv:2407.18341v4 Announce Type: replace 
Abstract: Composite endpoints consisting of both terminal and non-terminal events, such as death and hospitalization, are frequently used as primary endpoints in cardiovascular clinical trials. The Win Ratio method (WR) employs a hierarchical structure to combine fatal and non-fatal events by giving death information an absolute priority, which can adversely affect power if the treatment effect is mainly on the non-fatal outcomes. We hereby propose the Win Ratio with Multiple Thresholds (WR-MT) that releases the strict hierarchical structure of the standard WR by adding stages with non-zero thresholds. A weighted adaptive approach is also developed to determine the thresholds in WR-MT. This method preserves the statistical properties of the standard WR but can sometimes increase the chance to detect treatment effects on non-fatal events. We show that WR-MT has a particularly favorable performance than standard WR when the second layer has stronger signals and otherwise comparable performance in our simulations that vary the follow-up time, the correlation between events, and the treatment effect sizes. A case study based on the Digitalis Investigation Group clinical trial data is presented to further illustrate our proposed method. An R package "WRMT" that implements the proposed methodology has been developed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.18341v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yunhan Mou, Tassos Kyriakides, Scott Hummel, Fan Li, Yuan Huang</dc:creator>
    </item>
    <item>
      <title>Shift-Dispersion Decompositions of Wasserstein and Cram\'er Distances</title>
      <link>https://arxiv.org/abs/2408.09770</link>
      <description>arXiv:2408.09770v3 Announce Type: replace 
Abstract: Divergence functions are measures of distance or dissimilarity between probability distributions that serve various purposes in statistics and applications. We propose decompositions of Wasserstein and Cram\'er distances$-$which compare two distributions by integrating over their differences in distribution or quantile functions$-$into directed shift and dispersion components. These components are obtained by dividing the differences between the quantile functions into contributions arising from shift and dispersion, respectively. Our decompositions add information on how the distributions differ in a condensed form and consequently enhance the interpretability of the underlying divergences. We show that our decompositions satisfy a number of natural properties and are unique in doing so in location-scale families. The decompositions allow to derive sensitivities of the divergence measures to changes in location and dispersion, and they give rise to weak stochastic order relations that are linked to the usual stochastic and the dispersive order. Our theoretical developments are illustrated in two applications, where we focus on forecast evaluation of temperature extremes and on the design of probabilistic surveys in economics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.09770v3</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Johannes Resin, Daniel Wolffram, Johannes Bracher, Timo Dimitriadis</dc:creator>
    </item>
    <item>
      <title>Scalable piecewise smoothing with BART</title>
      <link>https://arxiv.org/abs/2411.07984</link>
      <description>arXiv:2411.07984v4 Announce Type: replace 
Abstract: Although it is an extremely effective, easy-to-use, and increasingly popular tool for nonparametric regression, the Bayesian Additive Regression Trees (BART) model is limited by the fact that it can only produce discontinuous output. Initial attempts to overcome this limitation were based on regression trees that output Gaussian Processes instead of constants. Unfortunately, implementations of these extensions cannot scale to large datasets. We propose ridgeBART, an extension of BART built with trees that output linear combinations of ridge functions (i.e., a composition of an affine transformation of the inputs and non-linearity); that is, we build a Bayesian ensemble of localized neural networks with a single hidden layer. We develop a new MCMC sampler that updates trees in linear time and establish posterior contraction rates for estimating piecewise anisotropic H\"{o}lder functions and nearly minimax-optimal rates for estimating isotropic H\"{o}lder functions. We demonstrate ridgeBART's effectiveness on synthetic data and use it to estimate the probability that a professional basketball player makes a shot from any location on the court in a spatially smooth fashion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.07984v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ryan Yee, Soham Ghosh, Sameer K. Deshpande</dc:creator>
    </item>
    <item>
      <title>Analyzing zero-inflated clustered longitudinal ordinal outcomes using GEE-type models with an application to dental fluorosis studies</title>
      <link>https://arxiv.org/abs/2412.11348</link>
      <description>arXiv:2412.11348v4 Announce Type: replace 
Abstract: Motivated by the Iowa Fluoride Study (IFS), which tracked fluoride intake and dental outcomes from childhood to young adulthood (ages 9, 13, 17, and 23), we analyze dental fluorosis - a condition caused by excessive fluoride exposure during enamel formation. In this context, fluorosis scores across tooth surfaces present as zero-inflated, clustered, and longitudinal ordinal outcomes, prompting the development of a unified modeling framework. Leveraging generalized estimating equations (GEEs), we construct separate models for the presence and severity of fluorosis and propose a combined model that links these components though shared covariates. To improve estimation efficiency and borrowing strength across timepoints, we incorporate James-Stein shrinkage estimators. We compare several working correlation structures, including a data-driven jackknifed structure, and perform model selection via rank aggregation. Simulation studies validate the finite-sample performance of the proposed models, and a bootstrap-based power analysis further confirms the validity of the testing procedure. In our analysis of the IFS data, early-life total daily fluoride intake, average home water fluoride concentration, and specific teeth and zones emerge as significant risk factors for dental fluorosis. Maxillary lateral incisors and zones closer to the gum show protective effects across different ages. These findings reveal novel age-specific associations between early-life exposures and the progression of dental fluorosis through early adulthood.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.11348v4</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Shoumi Sarkar, Anish Mukherjee, Jeremy T. Gaskins, Steven Levy, Peihua Qiu, Somnath Datta</dc:creator>
    </item>
    <item>
      <title>Mitigating Eddington and Malmquist Biases in Latent-Inclination Inference of the Tully-Fisher Relation</title>
      <link>https://arxiv.org/abs/2504.10589</link>
      <description>arXiv:2504.10589v2 Announce Type: replace 
Abstract: The Tully-Fisher relation is a vital distance indicator, but its precise inference is challenged by selection bias, statistical bias, and uncertain inclination corrections. This study presents a Bayesian framework that simultaneously addresses these issues. To eliminate the need for individual inclination corrections, inclination is treated as a latent variable with a known probability distribution. To correct for the distance-dependent Malmqvist bias arising from sample selection, the model incorporates Gaussian scatter in the dependent variable, the distribution of the independent variable, and the observational selection function into the data likelihood. To mitigate the statistical bias -- termed the ``general Eddington bias'' -- caused by Gaussian scatter and the non-uniform distribution of the independent variable, two methods are introduced: (1) analytical bias corrections applied to the dependent variable before likelihood computation, and (2) a dual-scatter model that accounts for Gaussian scatter in the independent variable within the likelihood function. The effectiveness of these methods is demonstrated using simulated datasets. By rigorously addressing selection and statistical biases in a latent-variable regression analysis, this work provides a robust approach for unbiased distance estimates from standardizable candles, which is critical for improving the accuracy of Hubble constant determinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.10589v2</guid>
      <category>stat.ME</category>
      <category>astro-ph.GA</category>
      <category>astro-ph.IM</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Hai Fu</dc:creator>
    </item>
    <item>
      <title>Adaptive Inference through Bayesian and Inverse Bayesian Inference with Symmetry-Bias in Nonstationary Environments</title>
      <link>https://arxiv.org/abs/2505.12796</link>
      <description>arXiv:2505.12796v5 Announce Type: replace 
Abstract: This study proposes a novel inference framework known as Bayesian and inverse Bayesian (BIB) inference, which incorporates symmetry bias into the Bayesian updating process to perform both conventional and inverse Bayesian updates concurrently. The model was evaluated in a sequential estimation task involving observations drawn from a Gaussian distribution with a stochastically time-varying mean. Conventional Bayesian inference is constrained by a fundamental trade-off between adaptability to abrupt environmental changes and accuracy during stable periods. The BIB framework addresses this limitation by dynamically modulating the learning rate via inverse Bayesian updates, thereby enhancing adaptive flexibility. Notably, the BIB model exhibited spontaneous bursts in the learning rate during environmental transitions, transiently entering high-sensitivity states that facilitated rapid adaptation.This burst-relaxation dynamic serves as a mechanism for balancing adaptability and accuracy. Furthermore, avalanche analysis, detrended fluctuation analysis, and power spectral analysis revealed that the BIB system likely operates near a critical state-a property not observed in standard Bayesian inference. This suggests that the BIB model uniquely achieves a coexistence of computational efficiency and critical dynamics, resolving the adaptability-accuracy trade-off while maintaining a scale-free behavior. These findings offer a new computational perspective on scale-free dynamics in natural systems and provide valuable insights for the design of adaptive inference systems in nonstationary environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.12796v5</guid>
      <category>stat.ME</category>
      <category>cs.MA</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Shuji Shinohara, Daiki Morita, Hayato Hirai, Ryosuke Kuribayashi, Nobuhito Manome, Toru Moriyama, Yoshihiro Nakajima, Yukio-Pegio Gunji, Ung-il Chung</dc:creator>
    </item>
    <item>
      <title>Anytime-valid simultaneous lower confidence bounds for the true discovery proportion</title>
      <link>https://arxiv.org/abs/2505.17803</link>
      <description>arXiv:2505.17803v2 Announce Type: replace 
Abstract: We propose a method that combines the closed testing framework with the concept of safe anytime-valid inference (SAVI) to compute lower confidence bounds for the true discovery proportion in a multiple testing setting. The proposed procedure provides confidence bounds that are valid at every observation time point and that are simultaneous for all possible subsets of hypotheses. While the hypotheses are assumed to be fixed over time, the subsets of interest may vary. Anytime-valid simultaneous confidence bounds allow us to sequentially update the bounds over time and allow for optional stopping. This is a desirable property in practical applications such as neuroscience, where data acquisition is costly and time-consuming. We also present a computational shortcut which makes the application of the proposed procedure feasible when the number of hypotheses under consideration is large. We illustrate the performance of the proposed method in a simulation study and give some practical guidelines on the implementation of the proposed procedure.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.17803v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Friederike Preusse</dc:creator>
    </item>
    <item>
      <title>gcor: A Python Implementation of Categorical Gini Correlation and Its Inference</title>
      <link>https://arxiv.org/abs/2506.19230</link>
      <description>arXiv:2506.19230v3 Announce Type: replace 
Abstract: Categorical Gini Correlation (CGC), introduced by Dang et al. (2020), is a novel dependence measure designed to quantify the association between a numerical variable and a categorical variable. It has appealing properties compared to existing dependence measures, such as zero correlation mutually implying independence between the variables. It has also shown superior performance over existing methods when applied to feature screening for classification. This article presents a Python implementation for computing CGC, constructing confidence intervals, and performing independence tests based on it. Efficient algorithms have been implemented for all procedures, and they have been optimized using vectorization and parallelization to enhance computational efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2506.19230v3</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sameera Hewage</dc:creator>
    </item>
    <item>
      <title>Graph Estimation Based on Neighborhood Selection for Matrix-variate Data</title>
      <link>https://arxiv.org/abs/2507.04711</link>
      <description>arXiv:2507.04711v2 Announce Type: replace 
Abstract: Undirected graphical models are powerful tools for uncovering complex relationships among high-dimensional variables. This paper aims to fully recover the structure of an undirected graphical model when the data naturally take matrix form, such as temporal multivariate data.
  As conventional vector-variate analyses have clear limitations in handling such matrix-structured data, several approaches have been proposed, mostly relying on the likelihood of the Gaussian distribution with a separable covariance structure. Although some of these methods provide theoretical guarantees against false inclusions (i.e. all identified edges exist in the true graph), they may suffer from crucial limitations: (1) failure to detect important true edges, or (2) dependency on conditions for the estimators that have not been verified.
  We propose a novel regression-based method for estimating matrix graphical models, based on the relationship between partial correlations and regression coefficients. Adopting the primal-dual witness technique from the regression framework, we derive a non-asymptotic inequality for exact recovery of an edge set. Under suitable regularity conditions, our method consistently identifies the true edge set with high probability.
  Through simulation studies, we compare the support recovery performance of the proposed method against existing alternatives. We also apply our method to an electroencephalography (EEG) dataset to estimate both the spatial brain network among 64 electrodes and the temporal network across 256 time points.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.04711v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Minsub Shin, Johan Lim, Seongoh Park</dc:creator>
    </item>
    <item>
      <title>Gradient-Boosted Pseudo-Weighting: Methods for Population Inference from Nonprobability samples</title>
      <link>https://arxiv.org/abs/2508.00089</link>
      <description>arXiv:2508.00089v2 Announce Type: replace 
Abstract: Nonprobability samples have rapidly emerged to address time-sensitive priority topics in a variety of fields. While these data are timely, they are prone to selection bias. To mitigate selection bias, a large number of survey research literature has explored the use of propensity score (PS) adjustment methods to enhance population representativeness of nonprobability samples, using probability-based survey samples as external references. A recent advancement, the 2-step PS-based pseudo-weighting adjustment method (2PS, Li 2024), has been shown to improve upon recent developments with respect to mean squared error. However, the effectiveness of these methods in reducing bias critically depends on the ability of the underlying propensity model to accurately reflect the true selection process, which is challenging with parametric regression. In this study, we propose a set of pseudo-weight construction methods, which utilize gradient boosting methods (GBM) to estimate PSs in 2PS to construct pseudo-weights, offering greater flexibility compared to logistic regression-based methods. We compare the proposed GBM-based pseudo-weights with existing methods, including 2PS. The population mean estimators are evaluated via Monte Carlo simulation studies. We also evaluated prevalence of various health outcomes, including 15-year mortality, using 1988 ~ 1994 NHANES III as a nonprobability sample and the 1994 NHIS as the reference survey.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.00089v2</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kangrui Liu, Lingxiao Wang, Yan Li</dc:creator>
    </item>
    <item>
      <title>On the asymptotic validity of confidence sets for linear functionals of solutions to integral equations</title>
      <link>https://arxiv.org/abs/2502.16673</link>
      <description>arXiv:2502.16673v3 Announce Type: replace-cross 
Abstract: This paper examines the construction of confidence sets for parameters defined as linear functionals of a function of W and X whose conditional mean given Z and X equals the conditional mean of another variable Y given Z and X. Many estimands of interest in causal inference can be expressed in this form, including the average treatment effect in proximal causal inference and treatment effect contrasts in instrumental variable models. We derive a necessary condition for a confidence set to be uniformly valid over a model that allows for the dependence between W and Z given X to be arbitrarily weak. Specifically, we show that for any such confidence set, there must exist some laws in the model under which, with high probability, the confidence set has a diameter greater than or equal to the diameter of the parameter's range. In particular, consistent with the weak instruments literature, Wald confidence intervals are not uniformly valid over the aforementioned model when the parameter's range is infinite. Furthermore, we argue that inverting the score test, a successful approach in that literature, generally fails for the broader class of parameters considered here. We present a method for constructing uniformly valid confidence sets in the special case where all variables, but possibly Y, are binary and discuss its limitations. Finally, we emphasize that developing uniformly valid confidence sets for the class of parameters considered in this paper remains an open problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.16673v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ezequiel Smucler, James M. Robins, Andrea Rotnitzky</dc:creator>
    </item>
    <item>
      <title>Functional Factor Regression with an Application to Electricity Price Curve Modeling</title>
      <link>https://arxiv.org/abs/2503.12611</link>
      <description>arXiv:2503.12611v2 Announce Type: replace-cross 
Abstract: We propose a function-on-function linear regression model for time-dependent curve data that is consistently estimated by imposing factor structures on the regressors. An integral operator based on cross-covariances identifies two components for each functional regressor: a predictive low-dimensional component, along with associated factors that are guaranteed to be correlated with the dependent variable, and an infinite-dimensional component that has no predictive power. In order to consistently estimate the correct number of factors for each regressor, we introduce a functional eigenvalue difference test. While conventional estimators for functional linear models fail to converge in distribution, we establish asymptotic normality, making it possible to construct confidence bands and conduct statistical inference. The model is applied to forecast electricity price curves in three different energy markets. Its prediction accuracy is found to be comparable to popular machine learning approaches, while providing statistically valid inference and interpretable insights into the conditional correlation structures of electricity prices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.12611v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sven Otto, Luis Winter</dc:creator>
    </item>
    <item>
      <title>GRAND: Graph Release with Assured Node Differential Privacy</title>
      <link>https://arxiv.org/abs/2507.00402</link>
      <description>arXiv:2507.00402v2 Announce Type: replace-cross 
Abstract: Differential privacy is a well-established framework for safeguarding sensitive information in data. While extensively applied across various domains, its application to network data -- particularly at the node level -- remains underexplored. Existing methods for node-level privacy either focus exclusively on query-based approaches, which restrict output to pre-specified network statistics, or fail to preserve key structural properties of the network. In this work, we propose GRAND (Graph Release with Assured Node Differential privacy), which is, to the best of our knowledge, the first network release mechanism that releases entire networks while ensuring node-level differential privacy and preserving structural properties. Under a broad class of latent space models, we show that the released network asymptotically follows the same distribution as the original network. The effectiveness of the approach is evaluated through extensive experiments on both synthetic and real-world datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2507.00402v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Suqing Liu, Xuan Bi, Tianxi Li</dc:creator>
    </item>
    <item>
      <title>Efficient optimization of expensive black-box simulators via marginal means, with application to neutrino detector design</title>
      <link>https://arxiv.org/abs/2508.01834</link>
      <description>arXiv:2508.01834v2 Announce Type: replace-cross 
Abstract: With advances in scientific computing, computer experiments are increasingly used for optimizing complex systems. However, for modern applications, e.g., the optimization of nuclear physics detectors, each experiment run can require hundreds of CPU hours, making the optimization of its black-box simulator over a high-dimensional space a challenging task. Given limited runs at inputs $\mathbf{x}_1, \cdots, \mathbf{x}_n$, the best solution from these evaluated inputs can be far from optimal, particularly as dimensionality increases. Existing black-box methods, however, largely employ this ''pick-the-winner'' (PW) solution, which leads to mediocre optimization performance. To address this, we propose a new Black-box Optimization via Marginal Means (BOMM) approach. The key idea is a new estimator of a global optimizer $\mathbf{x}^*$ that leverages the so-called marginal mean functions, which can be efficiently inferred with limited runs in high dimensions. Unlike PW, this estimator can select solutions beyond evaluated inputs for improved optimization performance. Assuming the objective function follows a generalized additive model with unknown link function and under mild conditions, we prove that the BOMM estimator not only is consistent for optimization, but also has an optimization rate that tempers the ''curse-of-dimensionality'' faced by existing methods, thus enabling better performance as dimensionality increases. We present a practical framework for implementing BOMM using the transformed additive Gaussian process surrogate model. Finally, we demonstrate the effectiveness of BOMM in numerical experiments and an application on neutrino detector optimization in nuclear physics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2508.01834v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 08 Aug 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hwanwoo Kim, Simon Mak, Ann-Kathrin Schuetz, Alan Poon</dc:creator>
    </item>
  </channel>
</rss>
