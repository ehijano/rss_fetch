<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 28 Nov 2024 02:53:46 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Measuring Statistical Evidence: A Short Report</title>
      <link>https://arxiv.org/abs/2411.16831</link>
      <description>arXiv:2411.16831v2 Announce Type: new 
Abstract: This short text tried to establish a big picture of what evidential statistics is about and how an ideal inference method should behave. Moreover, by examining shortcomings of some of the currently used methods for measuring evidence and utilizing some intuitive principles, we motivated the Relative Belief Ratio as the primary method of characterizing statistical evidence. Number of topics has been omitted for the interest of this text and the reader is strongly advised to refer to (Evans, 2015) as the primary source for further readings of the subject.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16831v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mahdi Zamani</dc:creator>
    </item>
    <item>
      <title>Bounding causal effects with an unknown mixture of informative and non-informative censoring</title>
      <link>https://arxiv.org/abs/2411.16902</link>
      <description>arXiv:2411.16902v1 Announce Type: new 
Abstract: In experimental and observational data settings, researchers often have limited knowledge of the reasons for censored or missing outcomes. To address this uncertainty, we propose bounds on causal effects for censored outcomes, accommodating the scenario where censoring is an unobserved mixture of informative and non-informative components. Within this mixed censoring framework, we explore several assumptions to derive bounds on causal effects, including bounds expressed as a function of user-specified sensitivity parameters. We develop influence-function based estimators of these bounds to enable flexible, non-parametric, and machine learning based estimation, achieving root-n convergence rates and asymptotic normality under relatively mild conditions. We further consider the identification and estimation of bounds for other causal quantities that remain meaningful when informative censoring reflects a competing risk, such as death. We conduct extensive simulation studies and illustrate our methodology with a study on the causal effect of antipsychotic drugs on diabetes risk using a health insurance dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16902v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Max Rubinstein, Denis Agniel, Larry Han, Marcela Horvitz-Lennon, Sharon-Lise Normand</dc:creator>
    </item>
    <item>
      <title>Fragility Index for Time-to-Event Endpoints in Single-Arm Clinical Trials</title>
      <link>https://arxiv.org/abs/2411.16938</link>
      <description>arXiv:2411.16938v1 Announce Type: new 
Abstract: The reliability of clinical trial outcomes is crucial, especially in guiding medical decisions. In this paper, we introduce the Fragility Index (FI) for time-to-event endpoints in single-arm clinical trials - a novel metric designed to quantify the robustness of study conclusions. The FI represents the smallest number of censored observations that, when reclassified as uncensored events, causes the posterior probability of the median survival time exceeding a specified threshold to fall below a predefined confidence level. While drug effectiveness is typically assessed by determining whether the posterior probability exceeds a specified confidence level, the FI offers a complementary measure, indicating how robust these conclusions are to potential shifts in the data. Using a Bayesian approach, we develop a practical framework for computing the FI based on the exponential survival model. To facilitate the application of our method, we developed an R package fi, which provides a tool to compute the Fragility Index. Through real world case studies involving time to event data from single arms clinical trials, we demonstrate the utility of this index. Our findings highlight how the FI can be a valuable tool for assessing the robustness of survival analyses in single-arm studies, aiding researchers and clinicians in making more informed decisions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.16938v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Arnab Kumar Maity, Jhanvi Garg, Cynthia Basu</dc:creator>
    </item>
    <item>
      <title>Conditional Extremes with Graphical Models</title>
      <link>https://arxiv.org/abs/2411.17013</link>
      <description>arXiv:2411.17013v1 Announce Type: new 
Abstract: Multivariate extreme value analysis quantifies the probability and magnitude of joint extreme events. River discharges from the upper Danube River basin provide a challenging dataset for such analysis because the data, which is measured on a spatial network, exhibits both asymptotic dependence and asymptotic independence. To account for both features, we extend the conditional multivariate extreme value model (CMEVM) with a new approach for the residual distribution. This allows sparse (graphical) dependence structures and fully parametric prediction. Our approach fills a current gap in statistical methodology for graphical extremes, where existing models require asymptotic independence. Further, the model can be used to learn the graphical dependence structure when it is unknown a priori. To support inference in high dimensions, we propose a stepwise inference procedure that is computationally efficient and loses no information or predictive power. We show our method is flexible and accurately captures the extremal dependence for the upper Danube River basin discharges.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17013v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Aiden Farrell, Emma F. Eastoe, Clement Lee</dc:creator>
    </item>
    <item>
      <title>Detecting Outliers in Multiple Sampling Results Without Thresholds</title>
      <link>https://arxiv.org/abs/2411.17024</link>
      <description>arXiv:2411.17024v1 Announce Type: new 
Abstract: Bayesian statistics emphasizes the importance of prior distributions, yet finding an appropriate one is practically challenging. When multiple sample results are taken regarding the frequency of the same event, these samples may be influenced by different selection effects. In the absence of suitable prior distributions to correct for these selection effects, it is necessary to exclude outlier sample results to avoid compromising the final result. However, defining outliers based on different thresholds may change the result, which makes the result less persuasive. This work proposes a definition of outliers without the need to set thresholds.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17024v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yu-Fu Shen</dc:creator>
    </item>
    <item>
      <title>Quantile Graph Discovery through QuACC: Quantile Association via Conditional Concordance</title>
      <link>https://arxiv.org/abs/2411.17033</link>
      <description>arXiv:2411.17033v2 Announce Type: new 
Abstract: Graphical structure learning is an effective way to assess and visualize cross-biomarker dependencies in biomedical settings. Standard approaches to estimating graphs rely on conditional independence tests that may not be sensitive to associations that manifest at the tails of joint distributions, i.e., they may miss connections among variables that exhibit associations mainly at lower or upper quantiles. In this work, we propose a novel measure of quantile-specific conditional association called QuACC: Quantile Association via Conditional Concordance. For a pair of variables and a conditioning set, QuACC quantifies agreement between the residuals from two quantile regression models, which may be linear or more complex, e.g., quantile forests. Using this measure as the basis for a test of null (quantile) association, we introduce a new class of quantile-specific graphical models. Through simulation we show our method is powerful for detecting dependencies under dependencies that manifest at the tails of distributions. We apply our method to biobank data from All of Us and identify quantile-specific patterns of conditional association in a multivariate setting.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17033v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zain Khan, Daniel Malinsky, Martin Picard, Alan A. Cohen, Columbia SOH Group, Ying Wei</dc:creator>
    </item>
    <item>
      <title>Achieving Privacy Utility Balance for Multivariate Time Series Data</title>
      <link>https://arxiv.org/abs/2411.17035</link>
      <description>arXiv:2411.17035v1 Announce Type: new 
Abstract: Utility-preserving data privatization is of utmost importance for data-producing agencies. The popular noise-addition privacy mechanism distorts autocorrelation patterns in time series data, thereby marring utility; in response, McElroy et al. (2023) introduced all-pass filtering (FLIP) as a utility-preserving time series data privatization method. Adapting this concept to multivariate data is more complex, and in this paper we propose a multivariate all-pass (MAP) filtering method, employing an optimization algorithm to achieve the best balance between data utility and privacy protection. To test the effectiveness of our approach, we apply MAP filtering to both simulated and real data, sourced from the U.S. Census Bureau's Quarterly Workforce Indicator (QWI) dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17035v1</guid>
      <category>stat.ME</category>
      <category>cs.CR</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gaurab Hore, Tucker McElroy, Anindya Roy</dc:creator>
    </item>
    <item>
      <title>Patient recruitment forecasting in clinical trials using time-dependent Poisson-gamma model and homogeneity testing criteria</title>
      <link>https://arxiv.org/abs/2411.17393</link>
      <description>arXiv:2411.17393v1 Announce Type: new 
Abstract: Clinical trials in the modern era are characterized by their complexity and high costs and usually involve hundreds/thousands of patients to be recruited across multiple clinical centres in many countries, as typically a rather large sample size is required in order to prove the efficiency of a particular drug. As the imperative to recruit vast numbers of patients across multiple clinical centres has become a major challenge, an accurate forecasting of patient recruitment is one of key factors for the operational success of clinical trials. A classic Poisson-gamma (PG) recruitment model assumes time-homogeneous recruitment rates. However, there can be potential time-trends in the recruitment driven by various factors, e.g. seasonal changes, exhaustion of patients on particular treatments in some centres, etc. Recently a few authors considered some extensions of the PG model to time-dependent rates under some particular assumptions. In this paper, a natural generalization of the original PG model to a PG model with non-homogeneous time-dependent rates is introduced. It is also proposed a new analytic methodology for modelling/forecasting patient recruitment using a Poisson-gamma approximation of recruitment processes in different countries and globally. The properties of some tests on homogeneity of the rates (non-parametric one using a Poisson model and two parametric tests using Poisson and PG model) are investigated. The techniques for modeling and simulation of the recruitment using time-dependent model are discussed. For re-projection of the remaining recruitment it is proposed to use a moving window and re-estimating parameters at every interim time. The results are supported by simulation of some artificial data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17393v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Volodymyr Anisimov, Lucas Oliver</dc:creator>
    </item>
    <item>
      <title>Variable selection via fused sparse-group lasso penalized multi-state models incorporating molecular data</title>
      <link>https://arxiv.org/abs/2411.17394</link>
      <description>arXiv:2411.17394v1 Announce Type: new 
Abstract: In multi-state models based on high-dimensional data, effective modeling strategies are required to determine an optimal, ideally parsimonious model. In particular, linking covariate effects across transitions is needed to conduct joint variable selection. A useful technique to reduce model complexity is to address homogeneous covariate effects for distinct transitions. We integrate this approach to data-driven variable selection by extended regularization methods within multi-state model building. We propose the fused sparse-group lasso (FSGL) penalized Cox-type regression in the framework of multi-state models combining the penalization concepts of pairwise differences of covariate effects along with transition grouping. For optimization, we adapt the alternating direction method of multipliers (ADMM) algorithm to transition-specific hazards regression in the multi-state setting. In a simulation study and application to acute myeloid leukemia (AML) data, we evaluate the algorithm's ability to select a sparse model incorporating relevant transition-specific effects and similar cross-transition effects. We investigate settings in which the combined penalty is beneficial compared to global lasso regularization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17394v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kaya Miah, Jelle J. Goeman, Hein Putter, Annette Kopp-Schneider, Axel Benner</dc:creator>
    </item>
    <item>
      <title>Receiver operating characteristic curve analysis with non-ignorable missing disease status</title>
      <link>https://arxiv.org/abs/2411.17402</link>
      <description>arXiv:2411.17402v1 Announce Type: new 
Abstract: This article considers the receiver operating characteristic (ROC) curve analysis for medical data with non-ignorable missingness in the disease status. In the framework of the logistic regression models for both the disease status and the verification status, we first establish the identifiability of model parameters, and then propose a likelihood method to estimate the model parameters, the ROC curve, and the area under the ROC curve (AUC) for the biomarker. The asymptotic distributions of these estimators are established. Via extensive simulation studies, we compare our method with competing methods in the point estimation and assess the accuracy of confidence interval estimation under various scenarios. To illustrate the application of the proposed method in practical data, we apply our method to the National Alzheimer's Coordinating Center data set.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17402v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Dingding Hu, Tao Yu, Pengfei Li</dc:creator>
    </item>
    <item>
      <title>A new test for assessing the covariate effect in ROC curves</title>
      <link>https://arxiv.org/abs/2411.17464</link>
      <description>arXiv:2411.17464v1 Announce Type: new 
Abstract: The ROC curve is a statistical tool that analyses the accuracy of a diagnostic test in which a variable is used to decide whether an individual is healthy or not. Along with that diagnostic variable it is usual to have information of some other covariates. In some situations it is advisable to incorporate that information into the study, as the performance of the ROC curves can be affected by them. Using the covariate-adjusted, the covariate-specific or the pooled ROC curves we discuss how to decide if we can exclude the covariates from our study or not, and the implications this may have in further analyses of the ROC curve. A new test for comparing the covariate-adjusted and the pooled ROC curve is proposed, and the problem is illustrated by analysing a real database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17464v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ar\'is Fanjul-Hevia, Juan Carlos Pardo-Fern\'andez, Wenceslao Gonz\'alez-Manteiga</dc:creator>
    </item>
    <item>
      <title>Simplifying Causal Mediation Analysis for Time-to-Event Outcomes using Pseudo-Values</title>
      <link>https://arxiv.org/abs/2411.17533</link>
      <description>arXiv:2411.17533v1 Announce Type: new 
Abstract: Mediation analysis for survival outcomes is challenging. Most existing methods quantify the treatment effect using the hazard ratio (HR) and attempt to decompose the HR into the direct effect of treatment plus an indirect, or mediated, effect. However, the HR is not expressible as an expectation, which complicates this decomposition, both in terms of estimation and interpretation. Here, we present an alternative approach which leverages pseudo-values to simplify estimation and inference. Pseudo-values take censoring into account during their construction, and once derived, can be modeled in the same way as any continuous outcome. Thus, pseudo-values enable mediation analysis for a survival outcome to fit seamlessly into standard mediation software (e.g. CMAverse in R). Pseudo-values are easy to calculate via a leave-one-observation-out procedure (i.e. jackknifing) and the calculation can be accelerated when the influence function of the estimator is known. Mediation analysis for causal effects defined by survival probabilities, restricted mean survival time, and cumulative incidence functions - in the presence of competing risks - can all be performed within this framework. Extensive simulation studies demonstrate that the method is unbiased across 324 scenarios/estimands and controls the type-I error at the nominal level under the null of no mediation. We illustrate the approach using data from the PARADIGMS clinical trial for the treatment of pediatric multiple sclerosis using fingolimod. In particular, we evaluate whether an imaging biomarker lies on the causal path between treatment and time-to-relapse, which aids in justifying this biomarker as a surrogate outcome. Our approach greatly simplifies mediation analysis for survival data and provides a decomposition of the total effect that is both intuitive and interpretable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17533v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alex Ocampo, Enrico Giudice, Dieter A. H\"aring, Baldur Magnusson, Theis Lange, Zachary R. McCaw</dc:creator>
    </item>
    <item>
      <title>Valid Bayesian Inference based on Variance Weighted Projection for High-Dimensional Logistic Regression with Binary Covariates</title>
      <link>https://arxiv.org/abs/2411.17618</link>
      <description>arXiv:2411.17618v1 Announce Type: new 
Abstract: We address the challenge of conducting inference for a categorical treatment effect related to a binary outcome variable while taking into account high-dimensional baseline covariates. The conventional technique used to establish orthogonality for the treatment effect from nuisance variables in continuous cases is inapplicable in the context of binary treatment. To overcome this obstacle, an orthogonal score tailored specifically to this scenario is formulated which is based on a variance-weighted projection. Additionally, a novel Bayesian framework is proposed to facilitate valid inference for the desired low-dimensional parameter within the complex framework of high-dimensional logistic regression. We provide uniform convergence results, affirming the validity of credible intervals derived from the posterior distribution. The effectiveness of the proposed method is demonstrated through comprehensive simulation studies and real data analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17618v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Abhishek Ojha, Naveen N. Narisetty</dc:creator>
    </item>
    <item>
      <title>Intrepid MCMC: Metropolis-Hastings with Exploration</title>
      <link>https://arxiv.org/abs/2411.17639</link>
      <description>arXiv:2411.17639v1 Announce Type: new 
Abstract: In engineering examples, one often encounters the need to sample from unnormalized distributions with complex shapes that may also be implicitly defined through a physical or numerical simulation model, making it computationally expensive to evaluate the associated density function. For such cases, MCMC has proven to be an invaluable tool. Random-walk Metropolis Methods (also known as Metropolis-Hastings (MH)), in particular, are highly popular for their simplicity, flexibility, and ease of implementation. However, most MH algorithms suffer from significant limitations when attempting to sample from distributions with multiple modes (particularly disconnected ones). In this paper, we present Intrepid MCMC - a novel MH scheme that utilizes a simple coordinate transformation to significantly improve the mode-finding ability and convergence rate to the target distribution of random-walk Markov chains while retaining most of the simplicity of the vanilla MH paradigm. Through multiple examples, we showcase the improvement in the performance of Intrepid MCMC over vanilla MH for a wide variety of target distribution shapes. We also provide an analysis of the mixing behavior of the Intrepid Markov chain, as well as the efficiency of our algorithm for increasing dimensions. A thorough discussion is presented on the practical implementation of the Intrepid MCMC algorithm. Finally, its utility is highlighted through a Bayesian parameter inference problem for a two-degree-of-freedom oscillator under free vibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17639v1</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Promit Chakroborty, Michael D. Shields</dc:creator>
    </item>
    <item>
      <title>Optimal Estimation of Shared Singular Subspaces across Multiple Noisy Matrices</title>
      <link>https://arxiv.org/abs/2411.17054</link>
      <description>arXiv:2411.17054v1 Announce Type: cross 
Abstract: Estimating singular subspaces from noisy matrices is a fundamental problem with wide-ranging applications across various fields. Driven by the challenges of data integration and multi-view analysis, this study focuses on estimating shared (left) singular subspaces across multiple matrices within a low-rank matrix denoising framework. A common approach for this task is to perform singular value decomposition on the stacked matrix (Stack-SVD), which is formed by concatenating all the individual matrices. We establish that Stack-SVD achieves minimax rate-optimality when the true (left) singular subspaces of the signal matrices are identical. Our analysis reveals some phase transition phenomena in the estimation problem as a function of the underlying signal-to-noise ratio, highlighting how the interplay among multiple matrices collectively determines the fundamental limits of estimation. We then tackle the more complex scenario where the true singular subspaces are only partially shared across matrices. For various cases of partial sharing, we rigorously characterize the conditions under which Stack-SVD remains effective, achieves minimax optimality, or fails to deliver consistent estimates, offering theoretical insights into its practical applicability. To overcome Stack-SVD's limitations in partial sharing scenarios, we propose novel estimators and an efficient algorithm to identify shared and unshared singular vectors, and prove their minimax rate-optimality. Extensive simulation studies and real-world data applications demonstrate the numerous advantages of our proposed approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17054v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Zhengchi Ma, Rong Ma</dc:creator>
    </item>
    <item>
      <title>On the Symmetry of Limiting Distributions of M-estimators</title>
      <link>https://arxiv.org/abs/2411.17087</link>
      <description>arXiv:2411.17087v1 Announce Type: cross 
Abstract: Many functionals of interest in statistics and machine learning can be written as minimizers of expected loss functions. Such functionals are called $M$-estimands, and can be estimated by $M$-estimators -- minimizers of empirical average losses. Traditionally, statistical inference (e.g., hypothesis tests and confidence sets) for $M$-estimands is obtained by proving asymptotic normality of $M$-estimators centered at the target. However, asymptotic normality is only one of several possible limiting distributions and (asymptotically) valid inference becomes significantly difficult with non-normal limits. In this paper, we provide conditions for the symmetry of three general classes of limiting distributions, enabling inference using HulC (Kuchibhotla et al. (2024)).</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17087v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arunav Bhowmick, Arun Kumar Kuchibhotla</dc:creator>
    </item>
    <item>
      <title>Double robust estimation of functional outcomes with data missing at random</title>
      <link>https://arxiv.org/abs/2411.17224</link>
      <description>arXiv:2411.17224v1 Announce Type: cross 
Abstract: We present and study semi-parametric estimators for the mean of functional outcomes in situations where some of these outcomes are missing and covariate information is available on all units. Assuming that the missingness mechanism depends only on the covariates (missing at random assumption), we present two estimators for the functional mean parameter, using working models for the functional outcome given the covariates, and the probability of missingness given the covariates. We contribute by establishing that both these estimators have Gaussian processes as limiting distributions and explicitly give their covariance functions. One of the estimators is double robust in the sense that the limiting distribution holds whenever at least one of the nuisance models is correctly specified. These results allow us to present simultaneous confidence bands for the mean function with asymptotically guaranteed coverage. A Monte Carlo study shows the finite sample properties of the proposed functional estimators and their associated simultaneous inference. The use of the method is illustrated in an application where the mean of counterfactual outcomes is targeted.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17224v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xijia Liu, Kreske Ecker, Lina Schelin, Xavier de Luna</dc:creator>
    </item>
    <item>
      <title>Asymptotics for estimating a diverging number of parameters -- with and without sparsity</title>
      <link>https://arxiv.org/abs/2411.17395</link>
      <description>arXiv:2411.17395v1 Announce Type: cross 
Abstract: We consider high-dimensional estimation problems where the number of parameters diverges with the sample size. General conditions are established for consistency, uniqueness, and asymptotic normality in both unpenalized and penalized estimation settings. The conditions are weak and accommodate a broad class of estimation problems, including ones with non-convex and group structured penalties. The wide applicability of the results is illustrated through diverse examples, including generalized linear models, multi-sample inference, and stepwise estimation procedures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.17395v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jana Gauss, Thomas Nagler</dc:creator>
    </item>
    <item>
      <title>Distribution-Free Bayesian multivariate predictive inference</title>
      <link>https://arxiv.org/abs/2110.07361</link>
      <description>arXiv:2110.07361v2 Announce Type: replace 
Abstract: We introduce a comprehensive Bayesian multivariate predictive inference framework. The basis for our framework is a hierarchical Bayesian model, that is a mixture of finite Polya trees corresponding to multiple dyadic partitions of the unit cube. Given a sample of observations from an unknown multivariate distribution, the posterior predictive distribution is used to model and generate future observations from the unknown distribution. We illustrate the implementation of our methodology and study its performance on simulated examples. We introduce an algorithm for constructing conformal prediction sets, that provide finite sample probability assurances for future observations, with our Bayesian model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.07361v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daniel Yekutieli</dc:creator>
    </item>
    <item>
      <title>Examination of Nonlinear Longitudinal Processes with Latent Variables, Latent Processes, Latent Changes, and Latent Classes in the Structural Equation Modeling Framework: The R package nlpsem</title>
      <link>https://arxiv.org/abs/2302.03237</link>
      <description>arXiv:2302.03237v5 Announce Type: replace 
Abstract: We introduce the R package nlpsem, a comprehensive toolkit for analyzing longitudinal processes within the structural equation modeling (SEM) framework, incorporating individual measurement occasions. This package emphasizes nonlinear longitudinal models, especially intrinsic ones, across four key scenarios: (1) univariate longitudinal processes with latent variables, optionally including covariates such as time-invariant covariates (TICs) and time-varying covariates (TVCs); (2) multivariate longitudinal analyses to explore correlations or unidirectional relationships between longitudinal variables; (3) multiple-group frameworks for comparing manifest classes in scenarios (1) and (2); and (4) mixture models for scenarios (1) and (2), accommodating latent class heterogeneity. Built on the OpenMx R package, nlpsem supports flexible model designs and uses the full information maximum likelihood method for parameter estimation. A notable feature is its algorithm for determining initial values directly from raw data, enhancing computational efficiency and convergence. Furthermore, nlpsem provides tools for goodness-of-fit tests, cluster analyses, visualization, derivation of p-values and three types of confidence intervals, as well as model selection for nested models using likelihood ratio tests and for non-nested models based on criteria such as Akaike Information Criterion and Bayesian Information Criterion. This article serves as a companion document to the nlpsem R package, providing a comprehensive guide to its modeling capabilities, estimation methods, implementation features, and application examples using synthetic intelligence growth data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.03237v5</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jin Liu</dc:creator>
    </item>
    <item>
      <title>Robust Multi-Model Subset Selection</title>
      <link>https://arxiv.org/abs/2311.13202</link>
      <description>arXiv:2311.13202v4 Announce Type: replace 
Abstract: Outlying observations can be challenging to handle and adversely affect subsequent analyses, particularly, in complex high-dimensional datasets. Although outliers are not always undesired anomalies in the data and may possess valuable insights, only methods that are robust to outliers are able to accurately identify them and resist their influence. In this paper, we propose a method that generates an ensemble of sparse and diverse predictive models that are resistant to outliers. We show that the ensembles generally outperform single-model sparse and robust methods in high-dimensional prediction tasks. Cross-validation is used to tune model parameters to control levels of sparsity, diversity and resistance to outliers. We establish the finitesample breakdown point of the ensembles and the models that comprise them, and we develop a tailored computing algorithm to learn the ensembles by leveraging recent developments in L0 optimization. Our extensive numerical experiments on synthetic and artificially contaminated real datasets from bioinformatics and cheminformatics demonstrate the competitive advantage of our method over state-of-the-art single-model methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.13202v4</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anthony-Alexander Christidis, Gabriela Cohen-Freue</dc:creator>
    </item>
    <item>
      <title>Hierarchical Bayesian Models to Mitigate Systematic Disparities in Prediction with Proxy Outcomes</title>
      <link>https://arxiv.org/abs/2403.00639</link>
      <description>arXiv:2403.00639v2 Announce Type: replace 
Abstract: Label bias occurs when the outcome of interest is not directly observable and instead, modeling is performed with proxy labels. When the difference between the true outcome and the proxy label is correlated with predictors, this can yield systematic disparities in predictions for different groups of interest. We propose Bayesian hierarchical measurement models to address these issues. When strong prior information about the measurement process is available, our approach improves accuracy and helps with algorithmic fairness. If prior knowledge is limited, our approach allows assessment of the sensitivity of predictions to the unknown specifications of the measurement process. This can help practitioners gauge if enough substantive information is available to guarantee the desired accuracy and avoid disparate predictions when using proxy outcomes. We demonstrate our approach through practical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.00639v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonas Mikhaeil, Andrew Gelman, Philip Greengard</dc:creator>
    </item>
    <item>
      <title>Statistical algorithms for low-frequency diffusion data: A PDE approach</title>
      <link>https://arxiv.org/abs/2405.01372</link>
      <description>arXiv:2405.01372v2 Announce Type: replace 
Abstract: We consider the problem of making nonparametric inference in a class of multi-dimensional diffusions in divergence form, from low-frequency data. Statistical analysis in this setting is notoriously challenging due to the intractability of the likelihood and its gradient, and computational methods have thus far largely resorted to expensive simulation-based techniques. In this article, we propose a new computational approach which is motivated by PDE theory and is built around the characterisation of the transition densities as solutions of the associated heat (Fokker-Planck) equation. Employing optimal regularity results from the theory of parabolic PDEs, we prove a novel characterisation for the gradient of the likelihood. Using these developments, for the nonlinear inverse problem of recovering the diffusivity, we then show that the numerical evaluation of the likelihood and its gradient can be reduced to standard elliptic eigenvalue problems, solvable by powerful finite element methods. This enables the efficient implementation of a large class of popular statistical algorithms, including (i) preconditioned Crank-Nicolson and Langevin-type methods for posterior sampling, and (ii) gradient-based descent optimisation schemes to compute maximum likelihood and maximum-a-posteriori estimates. We showcase the effectiveness of these methods via extensive simulation studies in a nonparametric Bayesian model with Gaussian process priors, in which both the proposed optimisation and sampling schemes provide good numerical recovery. The reproducible code is available online at https://github.com/MattGiord/LF-Diffusion.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.01372v2</guid>
      <category>stat.ME</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>math.ST</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Matteo Giordano, Sven Wang</dc:creator>
    </item>
    <item>
      <title>Sequential FDR and pFDR control under arbitrary dependence, with application to pharmacovigilance database monitoring</title>
      <link>https://arxiv.org/abs/2406.01218</link>
      <description>arXiv:2406.01218v2 Announce Type: replace 
Abstract: We propose sequential multiple testing procedures which control the false discover rate (FDR) or the positive false discovery rate (pFDR) under arbitrary dependence between the data streams. This is accomplished by "optimizing" an upper bound on these error metrics for a class of step down sequential testing procedures. Both open-ended and truncated versions of these sequential procedures are given, both being able to control both the type~I multiple testing metric (FDR or pFDR) at specified levels, and the former being able to control both the type I and type II (e.g., FDR and the false nondiscovery rate, FNR). In simulation studies, these procedures provide 45-65% savings in average sample size over their fixed-sample competitors. We illustrate our procedures on drug data from the United Kingdom's Yellow Card Pharmacovigilance Database.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.01218v2</guid>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Michael Hankin, Jay Bartroff</dc:creator>
    </item>
    <item>
      <title>Spatial Proportional Hazards Model with Differential Regularization</title>
      <link>https://arxiv.org/abs/2410.13420</link>
      <description>arXiv:2410.13420v3 Announce Type: replace 
Abstract: This paper presents a semiparametric proportional hazards model designed to handle spatially varying covariate functions, applicable to both geostatistical and areal data observed on irregular spatial domains. The model is estimated through the maximization of a penalized partial likelihood, with a roughness penalty term based on a differential of the spatial field over the target domain. The finite element method is employed for efficient estimation, enabling a piecewise polynomial surface representation of the spatial field. We apply this method to analyze response time data from the London Fire Brigade.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.13420v3</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lorenzo Tedesco</dc:creator>
    </item>
    <item>
      <title>Median Based Unit Weibull (MBUW): a new unit distribution Properties</title>
      <link>https://arxiv.org/abs/2410.19019</link>
      <description>arXiv:2410.19019v4 Announce Type: replace 
Abstract: A new 2 parameter unit Weibull distribution is defined on the unit interval (0,1). The methodology of deducing its PDF, some of its properties and related functions are discussed. The paper is supplied by many figures illustrating the new distribution and how this can make it illegible to fit a wide range of skewed data. The new distribution holds a name (Attia) as a nickname.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.19019v4</guid>
      <category>stat.ME</category>
      <category>math.PR</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Iman Mohammed Attia</dc:creator>
    </item>
    <item>
      <title>Label Noise Robustness of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2209.14295</link>
      <description>arXiv:2209.14295v3 Announce Type: replace-cross 
Abstract: We study the robustness of conformal prediction, a powerful tool for uncertainty quantification, to label noise. Our analysis tackles both regression and classification problems, characterizing when and how it is possible to construct uncertainty sets that correctly cover the unobserved noiseless ground truth labels. We further extend our theory and formulate the requirements for correctly controlling a general loss function, such as the false negative proportion, with noisy labels. Our theory and experiments suggest that conformal prediction and risk-controlling techniques with noisy labels attain conservative risk over the clean ground truth labels whenever the noise is dispersive and increases variability. In other adversarial cases, we can also correct for noise of bounded size in the conformal prediction algorithm in order to ensure achieving the correct risk of the ground truth labels without score or data regularity.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.14295v3</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Bat-Sheva Einbinder, Shai Feldman, Stephen Bates, Anastasios N. Angelopoulos, Asaf Gendler, Yaniv Romano</dc:creator>
    </item>
    <item>
      <title>A multi-language toolkit for the semi-automated checking of research outputs</title>
      <link>https://arxiv.org/abs/2212.02935</link>
      <description>arXiv:2212.02935v3 Announce Type: replace-cross 
Abstract: This article presents a free and open source toolkit that supports the semi-automated checking of research outputs (SACRO) for privacy disclosure within secure data environments. SACRO is a framework that applies best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. SACRO is designed to assist human checkers rather than seeking to replace them as with current automated rules-based approaches. The toolkit is composed of a lightweight Python package that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This package adds functionality to (i) automatically identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply optional disclosure mitigation strategies as requested; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow and maintain auditable records. This creates an explicit change in the dynamics so that SDC is something done with researchers rather than to them, and enables more efficient communication with checkers. A graphical user interface supports human checkers by displaying the requested output and results of the checks in an immediately accessible format, highlighting identified issues, potential mitigation options, and tracking decisions made. The major analytical programming languages used by researchers (Python, R, and Stata) are supported by providing front-end packages that interface with the core Python back-end. Source code, packages, and documentation are available under MIT license at https://github.com/AI-SDC/ACRO</description>
      <guid isPermaLink="false">oai:arXiv.org:2212.02935v3</guid>
      <category>cs.CR</category>
      <category>cs.IR</category>
      <category>cs.SE</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Richard J. Preen, Maha Albashir, Simon Davy, Jim Smith</dc:creator>
    </item>
    <item>
      <title>Simulating signed mixtures</title>
      <link>https://arxiv.org/abs/2401.16828</link>
      <description>arXiv:2401.16828v2 Announce Type: replace-cross 
Abstract: Simulating mixtures of distributions with signed weights proves a challenge as standard simulation algorithms are inefficient in handling the negative weights. In particular, the natural representation of mixture variates as associated with latent component indicators is no longer available. We propose here an exact accept-reject algorithm in the general case of finite signed mixtures that relies on optimaly pairing positive and negative components and designing a stratified sampling scheme on pairs. We analyze the performances of our approach, relative to the inverse cdf approach, since the cdf of the distribution remains available for standard signed mixtures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.16828v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Julien Stoehr (CEREMADE), Christian P. Robert (CEREMADE)</dc:creator>
    </item>
    <item>
      <title>Optimal sub-Gaussian variance proxy for truncated Gaussian and exponential random variables</title>
      <link>https://arxiv.org/abs/2403.08628</link>
      <description>arXiv:2403.08628v2 Announce Type: replace-cross 
Abstract: This paper establishes the optimal sub-Gaussian variance proxy for truncated Gaussian and truncated exponential random variables. The proofs rely on first characterizing the optimal variance proxy as the unique solution to a set of two equations and then observing that for these two truncated distributions, one may find explicit solutions to this set of equations. Moreover, we establish the conditions under which the optimal variance proxy coincides with the variance, thereby characterizing the strict sub-Gaussianity of the truncated random variables. Specifically, we demonstrate that truncated Gaussian variables exhibit strict sub-Gaussian behavior if and only if they are symmetric, meaning their truncation is symmetric with respect to the mean. Conversely, truncated exponential variables are shown to never exhibit strict sub-Gaussian properties. These findings contribute to the understanding of these prevalent probability distributions in statistics and machine learning, providing a valuable foundation for improved and optimal modeling and decision-making processes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08628v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mathias Barreto, Olivier Marchal, Julyan Arbel</dc:creator>
    </item>
    <item>
      <title>MCMC using $\textit{bouncy}$ Hamiltonian dynamics: A unifying framework for Hamiltonian Monte Carlo and piecewise deterministic Markov process samplers</title>
      <link>https://arxiv.org/abs/2405.08290</link>
      <description>arXiv:2405.08290v2 Announce Type: replace-cross 
Abstract: Piecewise-deterministic Markov process (PDMP) samplers constitute a state of the art Markov chain Monte Carlo (MCMC) paradigm in Bayesian computation, with examples including the zig-zag and bouncy particle sampler (BPS). Recent work on the zig-zag has indicated its connection to Hamiltonian Monte Carlo, a version of the Metropolis algorithm that exploits Hamiltonian dynamics. Here we establish that, in fact, the connection between the paradigms extends far beyond the specific instance. The key lies in (1) the fact that any time-reversible deterministic dynamics provides a valid Metropolis proposal and (2) how PDMPs' characteristic velocity changes constitute an alternative to the usual acceptance-rejection. We turn this observation into a rigorous framework for constructing rejection-free Metropolis proposals based on bouncy Hamiltonian dynamics which simultaneously possess Hamiltonian-like properties and generate discontinuous trajectories similar in appearance to PDMPs. When combined with periodic refreshment of the inertia, the dynamics converge strongly to PDMP equivalents in the limit of increasingly frequent refreshment. We demonstrate the practical implications of this new paradigm, with a sampler based on a bouncy Hamiltonian dynamics closely related to the BPS. The resulting sampler exhibits competitive performance on challenging real-data posteriors involving tens of thousands of parameters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.08290v2</guid>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Andrew Chin, Akihiko Nishimura</dc:creator>
    </item>
    <item>
      <title>Debiased Regression for Root-N-Consistent Conditional Mean Estimation</title>
      <link>https://arxiv.org/abs/2411.11748</link>
      <description>arXiv:2411.11748v3 Announce Type: replace-cross 
Abstract: This study introduces a debiasing method for regression estimators, including high-dimensional and nonparametric regression estimators. For example, nonparametric regression methods allow for the estimation of regression functions in a data-driven manner with minimal assumptions; however, these methods typically fail to achieve $\sqrt{n}$-consistency in their convergence rates, and many, including those in machine learning, lack guarantees that their estimators asymptotically follow a normal distribution. To address these challenges, we propose a debiasing technique for nonparametric estimators by adding a bias-correction term to the original estimators, extending the conventional one-step estimator used in semiparametric analysis. Specifically, for each data point, we estimate the conditional expected residual of the original nonparametric estimator, which can, for instance, be computed using kernel (Nadaraya-Watson) regression, and incorporate it as a bias-reduction term. Our theoretical analysis demonstrates that the proposed estimator achieves $\sqrt{n}$-consistency and asymptotic normality under a mild convergence rate condition for both the original nonparametric estimator and the conditional expected residual estimator. Notably, this approach remains model-free as long as the original estimator and the conditional expected residual estimator satisfy the convergence rate condition. The proposed method offers several advantages, including improved estimation accuracy and simplified construction of confidence intervals.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11748v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Wed, 27 Nov 2024 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masahiro Kato</dc:creator>
    </item>
  </channel>
</rss>
