<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 19 Dec 2025 05:01:22 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Consensus dimension reduction via multi-view learning</title>
      <link>https://arxiv.org/abs/2512.15802</link>
      <description>arXiv:2512.15802v1 Announce Type: new 
Abstract: A plethora of dimension reduction methods have been developed to visualize high-dimensional data in low dimensions. However, different dimension reduction methods often output different and possibly conflicting visualizations of the same data. This problem is further exacerbated by the choice of hyperparameters, which may substantially impact the resulting visualization. To obtain a more robust and trustworthy dimension reduction output, we advocate for a consensus approach, which summarizes multiple visualizations into a single consensus dimension reduction visualization. Here, we leverage ideas from multi-view learning in order to identify the patterns that are most stable or shared across the many different dimension reduction visualizations, or views, and subsequently visualize this shared structure in a single low-dimensional plot. We demonstrate that this consensus visualization effectively identifies and preserves the shared low-dimensional data structure through both simulated and real-world case studies. We further highlight our method's robustness to the choice of dimension reduction method and hyperparameters -- a highly-desirable property when working towards trustworthy and reproducible data science.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15802v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bingxue An, Tiffany M. Tang</dc:creator>
    </item>
    <item>
      <title>Modeling Issues with Eye Tracking Data</title>
      <link>https://arxiv.org/abs/2512.15950</link>
      <description>arXiv:2512.15950v1 Announce Type: new 
Abstract: I describe and compare procedures for binary eye-tracking (ET) data. These procedures are applied to both raw and compressed data. The basic GLMM model is a logistic mixed model combined with random effects for persons and items. Additional models address autocorrelation eye-tracking serial observations. In particular, two novel approaches are illustrated that address serial without the use of an observed lag-1 predictor: a first-order autoregressive model obtained with generalized estimating equations, and a recurrent two-state survival model. Altogether, the results of four different analyses point to unresolved issues in the analysis of eye-tracking data and new directions for analytic development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15950v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gregory Camilli</dc:creator>
    </item>
    <item>
      <title>Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem</title>
      <link>https://arxiv.org/abs/2512.16012</link>
      <description>arXiv:2512.16012v1 Announce Type: new 
Abstract: Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a "reliability omission" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16012v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>JoonHo Lee</dc:creator>
    </item>
    <item>
      <title>Maximum Likelihood Estimation for Scaled Inhomogeneous Phase-Type Distributions from Discrete Observations</title>
      <link>https://arxiv.org/abs/2512.16061</link>
      <description>arXiv:2512.16061v1 Announce Type: new 
Abstract: Inhomogeneous phase-type (IPH) distributions extend classical phase-type models by allowing transition intensities to vary over time, offering greater flexibility for modeling heavy-tailed or time-dependent absorption phenomena. We focus on the subclass of IPH distributions with time-scaled sub-intensity matrices of the form ${\Lambda}(t) = h_{\beta}(t){\Lambda}$, which admits a time transformation to a homogeneous Markov jump process. For this class, we develop a statistical inference framework for discretely observed trajectories that combines Markov-bridge reconstruction with a stochastic EM algorithm and a gradient-based up- date. The resulting method yields joint maximum-likelihood estimates of both the baseline sub-intensity matrix ${\Lambda}$ and the time-scaling parameter $\beta$. Through simulation studies for the matrix-Gompertz and matrix-Weibull families, and a real-data application to coronary allograft vasculopathy progression, we demonstrate that the proposed approach provides an accurate and computationally tractable tool for fitting time-scaled IPH models to irregular multi-state data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16061v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fernando Baltazar-Larios, Alejandra Quintos</dc:creator>
    </item>
    <item>
      <title>An Efficient Framework for Robust Sample Size Determination</title>
      <link>https://arxiv.org/abs/2512.16231</link>
      <description>arXiv:2512.16231v1 Announce Type: new 
Abstract: In many settings, robust data analysis involves computational methods for uncertainty quantification and statistical inference. To design frequentist studies that leverage robust analysis methods, suitable sample sizes to achieve desired power are often found by estimating sampling distributions of p-values via intensive simulation. Moreover, most sample size recommendations rely heavily on assumptions about a single data-generating process. Consequently, robustness in data analysis does not by itself imply robustness in study design, as examining sample size sensitivity to data-generating assumptions typically requires further simulations. We propose an economical alternative for determining sample sizes that are robust to multiple data-generating mechanisms. Applying our theoretical results that model p-values as a function of the sample size, we assess power across the sample size space using simulations conducted at only two sample sizes for each data-generating mechanism. We demonstrate the broad applicability of our methodology to study design based on M-estimators in both experimental and observational settings through a varied set of clinical examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16231v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Luke Hagar, Andrew J. Martin</dc:creator>
    </item>
    <item>
      <title>Bayesian Empirical Bayes: Simultaneous Inference from Probabilistic Symmetries</title>
      <link>https://arxiv.org/abs/2512.16239</link>
      <description>arXiv:2512.16239v1 Announce Type: new 
Abstract: Empirical Bayes (EB) improves the accuracy of simultaneous inference "by learning from the experience of others" (Efron, 2012). Classical EB theory focuses on latent variables that are iid draws from a fitted prior (Efron, 2019). Modern applications, however, feature complex structure, like arrays, spatial processes, or covariates. How can we apply EB ideas to these settings? We propose a generalized approach to empirical Bayes based on the notion of probabilistic symmetry. Our method pairs a simultaneous inference problem-with an unknown prior-to a symmetry assumption on the joint distribution of the latent variables. Each symmetry implies an ergodic decomposition, which we use to derive a corresponding empirical Bayes method. We call this methodBayesian empirical Bayes (BEB). We show how BEB recovers the classical methods of empirical Bayes, which implicitly assume exchangeability. We then use it to extend EB to other probabilistic symmetries: (i) EB matrix recovery for arrays and graphs; (ii) covariate-assisted EB for conditional data; (iii) EB spatial regression under shift invariance. We develop scalable algorithms based on variational inference and neural networks. In simulations, BEB outperforms existing approaches to denoising arrays and spatial data. On real data, we demonstrate BEB by denoising a cancer gene-expression matrix and analyzing spatial air-quality data from New York City.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16239v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bohan Wu, Eli N. Weinstein, David M. Blei</dc:creator>
    </item>
    <item>
      <title>Repulsive g-Priors for Regression Mixtures</title>
      <link>https://arxiv.org/abs/2512.16276</link>
      <description>arXiv:2512.16276v1 Announce Type: new 
Abstract: Mixture regression models are powerful tools for capturing heterogeneous covariate-response relationships, yet classical finite mixtures and Bayesian nonparametric alternatives often suffer from instability or overestimation of clusters when component separability is weak. Recent repulsive priors improve parsimony in density mixtures by discouraging nearby components, but their direct extension to regression is nontrivial since separation must respect the predictive geometry induced by covariates. We propose a repulsive g-prior for regression mixtures that enforces separation in the Mahalanobis metric, penalizing components indistinguishable in the predictive mean space. This construction preserves conjugacy-like updates while introducing geometry-aware interactions, enabling efficient blocked-collapsed Gibbs sampling. Theoretically, we establish tractable normalizing bounds, posterior contraction rates, and shrinkage of tail mass on the number of components. Simulations under correlated and overlapping designs demonstrate improved clustering and prediction relative to independent, Euclidean-repulsive, and sparsity-inducing baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16276v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuta Hayashida, Shonosuke Sugasawa</dc:creator>
    </item>
    <item>
      <title>Hazard-based distributional regression via ordinary differential equations</title>
      <link>https://arxiv.org/abs/2512.16336</link>
      <description>arXiv:2512.16336v1 Announce Type: new 
Abstract: The hazard function is central to the formulation of commonly used survival regression models such as the proportional hazards and accelerated failure time models. However, these models rely on a shared baseline hazard, which, when specified parametrically, can only capture limited shapes. To overcome this limitation, we propose a general class of parametric survival regression models obtained by modelling the hazard function using autonomous systems of ordinary differential equations (ODEs). Covariate information is incorporated via transformed linear predictors on the parameters of the ODE system. Our framework capitalises on the interpretability of parameters in common ODE systems, enabling the identification of covariate values that produce qualitatively distinct hazard shapes associated with different attractors of the system of ODEs. This provides deeper insights into how covariates influence survival dynamics. We develop efficient Bayesian computational tools, including parallelised evaluation of the log-posterior, which facilitates integration with general-purpose Markov Chain Monte Carlo samplers. We also derive conditions for posterior asymptotic normality, enabling fast approximations of the posterior. A central contribution of our work lies in the case studies. We demonstrate the methodology using clinical trial data with crossing survival curves, and a study of cancer recurrence times where our approach reveals how the efficacy of interventions (treatments) on hazard and survival are influenced by patient characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16336v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>J. A. Christen, F. J. Rubio</dc:creator>
    </item>
    <item>
      <title>Bayesian joint modelling of longitudinal biomarkers to enable extrapolation of overall survival: an application using larotrectinib trial clinical data</title>
      <link>https://arxiv.org/abs/2512.16340</link>
      <description>arXiv:2512.16340v1 Announce Type: new 
Abstract: Objectives To investigate the use of a Bayesian joint modelling approach to predict overall survival (OS) from immature clinical trial data using an intermediate biomarker. To compare the results with a typical parametric approach of extrapolation and observed survival from a later datacut.
  Methods Data were pooled from three phase I/II open-label trials evaluating larotrectinib in 196 patients with neurotrophic tyrosine receptor kinase fusion-positive (NTRK+) solid tumours followed up until July 2021. Bayesian joint modelling was used to obtain patient-specific predictions of OS using individual-level sum of diameter of target lesions (SLD) profiles up to the time at which the patient died or was censored. Overall and tumour site-specific estimates were produced, assuming a common, exchangeable, or independent association structure across tumour sites.
  Results The overall risk of mortality was 9% higher per 10mm increase in SLD (HR 1.09, 95% CrI 1.05 to 1.14) for all tumour sites combined. Tumour-specific point estimates of restricted mean , median and landmark survival were more similar across models for larger tumour groups, compared to smaller tumour groups. In general, parameters were estimated with more certainty compared to a standard Weibull model and were aligned with the more recent datacut.
  Conclusions Joint modelling using intermediate outcomes such as tumour burden can offer an alternative approach to traditional survival modelling and may improve survival predictions from limited follow-up data. This approach allows complex hierarchical data structures, such as patients nested within tumour types, and can also incorporate multiple longitudinal biomarkers in a multivariate modelling framework.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16340v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Louise Linsell, Noman Paracha, Jamie Grossman, Carsten Bokemeyer, Jesus Garcia-Foncillas, Antoine Italiano, Gilles Vassal, Yuxian Chen, Barbara Torlinska, Keith R Abrams</dc:creator>
    </item>
    <item>
      <title>Empirical Likelihood Meets Prediction-Powered Inference</title>
      <link>https://arxiv.org/abs/2512.16363</link>
      <description>arXiv:2512.16363v1 Announce Type: new 
Abstract: We study inference with a small labeled sample, a large unlabeled sample, and high-quality predictions from an external model. We link prediction-powered inference with empirical likelihood by stacking supervised estimating equations based on labeled outcomes with auxiliary moment conditions built from predictions, and then optimizing empirical likelihood under these joint constraints. The resulting empirical likelihood-based prediction-powered inference (EPI) estimator is asymptotically normal, has asymptotic variance no larger than the fully supervised estimator, and attains the semiparametric efficiency bound when the auxiliary functions span the predictable component of the supervised score. For hypothesis testing and confidence sets, empirical likelihood ratio statistics admit chi-squared-type limiting distributions. As a by-product, the empirical likelihood weights induce a calibrated empirical distribution that integrates supervised and prediction-based information, enabling estimation and uncertainty quantification for general functionals beyond parameters defined by estimating equations. We present two practical implementations: one based on basis expansions in the predictions and covariates, and one that learns an approximately optimal auxiliary function by cross-fitting. In simulations and applications, EPI reduces mean squared error and shortens confidence intervals while maintaining nominal coverage.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16363v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanghui Wang, Mengtao Wen, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>Asymptotic and finite-sample distributions of one- and two-sample empirical relative entropy, with application to change-point detection</title>
      <link>https://arxiv.org/abs/2512.16411</link>
      <description>arXiv:2512.16411v1 Announce Type: new 
Abstract: Relative entropy, as a divergence metric between two distributions, can be used for offline change-point detection and extends classical methods that mainly rely on moment-based discrepancies. To build a statistical test suitable for this context, we study the distribution of empirical relative entropy and derive several types of approximations: concentration inequalities for finite samples, asymptotic distributions, and Berry-Esseen bounds in a pre-asymptotic regime. For the latter, we introduce a new approach to obtain Berry-Esseen inequalities for nonlinear functions of sum statistics under some convexity assumptions. Our theoretical contributions cover both one- and two-sample empirical relative entropies. We then detail a change-point detection procedure built on relative entropy and compare it, through extensive simulations, with classical methods based on moments or on information criteria. Finally, we illustrate its practical relevance on two real datasets involving temperature series and volatility of stock indices.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16411v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>q-fin.ST</category>
      <category>q-fin.TR</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Matthieu Garcin, Louis Perot</dc:creator>
    </item>
    <item>
      <title>Efficient and scalable clustering of survival curves</title>
      <link>https://arxiv.org/abs/2512.16481</link>
      <description>arXiv:2512.16481v1 Announce Type: new 
Abstract: Survival analysis encompasses a broad range of methods for analyzing time-to-event data, with one key objective being the comparison of survival curves across groups. Traditional approaches for identifying clusters of survival curves often rely on computationally intensive bootstrap techniques to approximate the null hypothesis distribution. While effective, these methods impose significant computational burdens. In this work, we propose a novel approach that leverages the k-means and log-rank test to efficiently identify and cluster survival curves. Our method eliminates the need for computationally expensive resampling, significantly reducing processing time while maintaining statistical reliability. By systematically evaluating survival curves and determining optimal clusters, the proposed method ensures a practical and scalable alternative for large-scale survival data analysis. Through simulation studies, we demonstrate that our approach achieves results comparable to existing bootstrap-based clustering methods while dramatically improving computational efficiency. These findings suggest that the log-rank-based clustering procedure offers a viable and time-efficient solution for researchers working with multiple survival curves in medical and epidemiological studies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16481v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Nora M. Villanueva, Marta Sestelo, Luis Meira-Machado</dc:creator>
    </item>
    <item>
      <title>Extending a Matrix Lie Group Model of Measurement Symmetries</title>
      <link>https://arxiv.org/abs/2512.16547</link>
      <description>arXiv:2512.16547v1 Announce Type: new 
Abstract: Symmetry principles underlie and guide scientific theory and research, from Curie's invariance formulation to modern applications across physics, chemistry, and mathematics. Building on a recent matrix Lie group measurement model, this paper extends the framework to identify additional measurement symmetries implied by Lie group theory. Lie groups provide the mathematics of continuous symmetries, while Lie algebras serve as their infinitesimal generators. Within applied measurement theory, the preservation of symmetries in transformation groups acting on score frequency distributions ensure invariance in transformed distributions, with implications for validity, comparability, and conservation of information. A simulation study demonstrates how breaks in measurement symmetry affect score distribution symmetry and break effect size comparability. Practical applications are considered, particularly in meta analysis, where the standardized mean difference (SMD) is shown to remain invariant across measures only under specific symmetry conditions derived from the Lie group model. These results underscore symmetry as a unifying principle in measurement theory and its role in evidence based research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16547v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>William R. Nugent</dc:creator>
    </item>
    <item>
      <title>Exponentially weighted estimands and the exponential family: filtering, prediction and smoothing</title>
      <link>https://arxiv.org/abs/2512.16745</link>
      <description>arXiv:2512.16745v1 Announce Type: new 
Abstract: We propose using a discounted version of a convex combination of the log-likelihood with the corresponding expected log-likelihood such that when they are maximized they yield a filter, predictor and smoother for time series. This paper then focuses on working out the implications of this in the case of the canonical exponential family. The results are simple exact filters, predictors and smoothers with linear recursions. A theory for these models is developed and the models are illustrated on simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16745v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Simon Donker van Heel, Neil Shephard</dc:creator>
    </item>
    <item>
      <title>Shift-Aware Gaussian-Supremum Validation for Wasserstein-DRO CVaR Portfolios</title>
      <link>https://arxiv.org/abs/2512.16748</link>
      <description>arXiv:2512.16748v1 Announce Type: new 
Abstract: We study portfolio selection with a Conditional Value-at-Risk (CVaR) constraint under distribution shift and serial dependence. While Wasserstein distributionally robust optimization (DRO) offers tractable protection via an ambiguity ball around empirical data, choosing the ball radius is delicate: large radii are conservative, small radii risk violation under regime change. We propose a shift-aware Gaussian-supremum (GS) validation framework for Wasserstein-DRO CVaR portfolios, building on the work by Lam and Qian (2019). Phase I of the framework generates a candidate path by solving the exact reformulation of the robust CVaR constraint over a grid of Wasserstein radii. Phase II of the framework learns a target deployment law $Q$ by density-ratio reweighting of a time-ordered validation fold, computes weighted CVaR estimates, and calibrates a simultaneous upper confidence band via a block multiplier bootstrap to account for dependence. We select the least conservative feasible portfolio (or abstain if the effective sample size collapses). Theoretically, we extend the normalized GS validator to non-i.i.d. financial data: under weak dependence and regularity of the weighted scores, any portfolio passing our validator satisfies the CVaR limit under $Q$ with probability at least $1-\beta$; the Wasserstein term contributes a deterministic margin $(\delta/\alpha)\|x\|_*$. Empirical results indicate improved return-risk trade-offs versus the naive baseline.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16748v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Derek Long</dc:creator>
    </item>
    <item>
      <title>Distributed inference for heterogeneous mixture models using multi-site data</title>
      <link>https://arxiv.org/abs/2512.16833</link>
      <description>arXiv:2512.16833v1 Announce Type: new 
Abstract: Mixture models postulate the overall population as a mixture of finite subpopulations with unobserved membership. Fitting mixture models usually requires large sample sizes and combining data from multiple sites can be beneficial. However, sharing individual participant data across sites is often less feasible due to various types of practical constraints, such as data privacy concerns. Moreover, substantial heterogeneity may exist across sites, and locally identified latent classes may not be comparable across sites. We propose a unified modeling framework where a common definition of the latent classes is shared across sites and heterogeneous mixing proportions of latent classes are allowed to account for between-site heterogeneity. To fit the heterogeneous mixture model on multi-site data, we propose a novel distributed Expectation-Maximization (EM) algorithm where at each iteration a density ratio tilted surrogate Q function is constructed to approximate the standard Q function of the EM algorithm as if the data from multiple sites could be pooled together. Theoretical analysis shows that our estimator achieves the same contraction property as the estimators derived from the EM algorithm based on the pooled data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16833v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaokang Liu, Rui Duan, Raymond J. Carroll, Yang Ning, Yong Chen</dc:creator>
    </item>
    <item>
      <title>Identification and efficient estimation of compliance and network causal effects in cluster-randomized trials</title>
      <link>https://arxiv.org/abs/2512.16857</link>
      <description>arXiv:2512.16857v1 Announce Type: new 
Abstract: Treatment noncompliance is pervasive in infectious disease cluster-randomized trials. Although all individuals within a cluster are assigned the same treatment condition, the treatment uptake status may vary across individuals due to noncompliance. We propose a semiparametric framework to evaluate the individual compliance effect and network assignment effect within principal stratum exhibiting different patterns of noncompliance. The individual compliance effect captures the portion of the treatment effect attributable to changes in treatment receipt, while the network assignment effect reflects the pure impact of treatment assignment and spillover among individuals within the same cluster. Unlike prior efforts which either empirically identify or interval identify these estimands, we characterize new structural assumptions for nonparametric point identification. We then develop semiparametrically efficient estimators that combine data-adaptive machine learning methods with efficient influence functions to enable more robust inference. Additionally, we introduce sensitivity analysis methods to study the impact under assumption violations, and apply the proposed methods to reanalyze a cluster-randomized trial in Kenya that evaluated the impact of school-based mass deworming on disease transmission.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16857v1</guid>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Chao Cheng, Georgia Papadogeorgou, Fan Li</dc:creator>
    </item>
    <item>
      <title>Hidden Order in Trades Predicts the Size of Price Moves</title>
      <link>https://arxiv.org/abs/2512.15720</link>
      <description>arXiv:2512.15720v1 Announce Type: cross 
Abstract: Financial markets exhibit an apparent paradox: while directional price movements remain largely unpredictable--consistent with weak-form efficiency--the magnitude of price changes displays systematic structure. Here we demonstrate that real-time order-flow entropy, computed from a 15-state Markov transition matrix at second resolution, predicts the magnitude of intraday returns without providing directional information. Analysis of 38.5 million SPY trades over 36 trading days reveals that conditioning on entropy below the 5th percentile increases subsequent 5-minute absolute returns by a factor of 2.89 (t = 12.41, p &lt; 0.0001), while directional accuracy remains at 45.0%--statistically indistinguishable from chance (p = 0.12). This decoupling arises from a fundamental symmetry: entropy is invariant under sign permutation, detecting the presence of informed trading without revealing its direction. Walk-forward validation across five non-overlapping test periods confirms out-of-sample predictability, and label-permutation placebo tests yield z = 14.4 against the null. These findings suggest that information-theoretic measures may serve as volatility state variables in market microstructure, though the limited sample (36 days, single instrument) requires extended validation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15720v1</guid>
      <category>q-fin.TR</category>
      <category>q-fin.ST</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mainak Singha</dc:creator>
    </item>
    <item>
      <title>Decision-Focused Bias Correction for Fluid Approximation</title>
      <link>https://arxiv.org/abs/2512.15726</link>
      <description>arXiv:2512.15726v1 Announce Type: cross 
Abstract: Fluid approximation is a widely used approach for solving two-stage stochastic optimization problems, with broad applications in service system design such as call centers and healthcare operations. However, replacing the underlying random distribution (e.g., demand distribution) with its mean (e.g., the time-varying average arrival rate) introduces bias in performance estimation and can lead to suboptimal decisions. In this paper, we investigate how to identify an alternative point statistic, which is not necessarily the mean, such that substituting this statistic into the two-stage optimization problem yields the optimal decision. We refer to this statistic as the decision-corrected point estimate (time-varying arrival rate). For a general service network with customer abandonment costs, we establish necessary and sufficient conditions for the existence of such a corrected point estimate and propose an algorithm for its computation. Under a decomposable network structure, we further show that the resulting decision-corrected point estimate is closely related to the classical newsvendor solution. Numerical experiments demonstrate the superiority of our decision-focused correction method compared to the traditional fluid approximation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15726v1</guid>
      <category>math.OC</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Can Er, Mo Liu</dc:creator>
    </item>
    <item>
      <title>xtdml: Double Machine Learning Estimation to Static Panel Data Models with Fixed Effects in R</title>
      <link>https://arxiv.org/abs/2512.15965</link>
      <description>arXiv:2512.15965v1 Announce Type: cross 
Abstract: The double machine learning (DML) method combines the predictive power of machine learning with statistical estimation to conduct inference about the structural parameter of interest. This paper presents the R package `xtdml`, which implements DML methods for partially linear panel regression models with low-dimensional fixed effects, high-dimensional confounding variables, proposed by Clarke and Polselli (2025). The package provides functionalities to: (a) learn nuisance functions with machine learning algorithms from the `mlr3` ecosystem, (b) handle unobserved individual heterogeneity choosing among first-difference transformation, within-group transformation, and correlated random effects, (c) transform the covariates with min-max normalization and polynomial expansion to improve learning performance. We showcase the use of `xtdml` with both simulated and real longitudinal data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.15965v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annalivia Polselli</dc:creator>
    </item>
    <item>
      <title>Distributed Online Economic Dispatch With Time-Varying Coupled Inequality Constraints</title>
      <link>https://arxiv.org/abs/2512.16241</link>
      <description>arXiv:2512.16241v1 Announce Type: cross 
Abstract: We investigate the distributed online economic dispatch problem for power systems with time-varying coupled inequality constraints. The problem is formulated as a distributed online optimization problem in a multi-agent system. At each time step, each agent only observes its own instantaneous objective function and local inequality constraints; agents make decisions online and cooperate to minimize the sum of the time-varying objectives while satisfying the global coupled constraints. To solve the problem, we propose an algorithm based on the primal-dual approach combined with constraint-tracking. Under appropriate assumptions that the objective and constraint functions are convex, their gradients are uniformly bounded, and the path length of the optimal solution sequence grows sublinearly, we analyze theoretical properties of the proposed algorithm and prove that both the dynamic regret and the constraint violation are sublinear with time horizon T. Finally, we evaluate the proposed algorithm on a time-varying economic dispatch problem in power systems using both synthetic data and Australian Energy Market data. The results demonstrate that the proposed algorithm performs effectively in terms of tracking performance, constraint satisfaction, and adaptation to time-varying disturbances, thereby providing a practical and theoretically well-supported solution for real-time distributed economic dispatch.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16241v1</guid>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yingjie Zhou, Xiaoqian Wang, Tao Li</dc:creator>
    </item>
    <item>
      <title>Dynamic Prediction for Hospital Readmission in Patients with Chronic Heart Failure</title>
      <link>https://arxiv.org/abs/2512.16463</link>
      <description>arXiv:2512.16463v1 Announce Type: cross 
Abstract: Hospital readmission among patients with chronic heart failure (HF) is a major clinical and economic burden. Dynamic prediction models that leverage longitudinal biomarkers may improve risk stratification over traditional static models. This study aims to develop and validate a joint model (JM) using longitudinal N-terminal pro-B-type natriuretic peptide (NT-proBNP) measurements to predict the risk of rehospitalization or death in HF patients. We analyzed real-world data from the TriNetX database, including patients with an incident HF diagnosis between 2016 and 2022. The final selected cohort included 1,804 patients. A Bayesian joint modeling framework was developed to link patient-specific NT-proBNP trajectories to the risk of a composite endpoint (HF rehospitalization or all-cause mortality) within a 180-day window following hospital discharge. The model's performance was evaluated using 5-fold cross-validation and assessed with the Integrated Brier Score (IBS) and Integrated Calibration Index (ICI). The joint model demonstrated a strong predictive advantage over a benchmark static model, particularly when making updated predictions at later time points (180-360 days). A joint model trained on patients with more frequent NT-proBNP measurements achieved the highest accuracy. The main joint model showed excellent calibration, suggesting its risk estimates are reliable. These findings suggest that modeling the full trajectory of NT-proBNP with a joint modeling framework enables more accurate and dynamic risk assessment compared to static, single-timepoint methods. This approach supports the development of adaptive clinical decision-support tools for personalized HF management.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16463v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Rebecca Farina, Francois Mercier, Christian Wohlfart, Serge Masson, Silvia Metelli</dc:creator>
    </item>
    <item>
      <title>Rao-Blackwellized e-variables</title>
      <link>https://arxiv.org/abs/2512.16759</link>
      <description>arXiv:2512.16759v1 Announce Type: cross 
Abstract: We show that for any concave utility, the expected utility of an e-variable can only increase after conditioning on a sufficient statistic. The simplest form of the result has an extremely straightforward proof, which follows from a single application of Jensen's inequality. Similar statements hold for compound e-variables, asymptotic e-variables, and e-processes. These results echo the Rao-Blackwell theorem, which states that the expected squared error of an estimator can only decrease after conditioning on a sufficient statistic. We provide several applications of this insight, including a simplified derivation of the log-optimal e-variable for linear regression with known variance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16759v1</guid>
      <category>math.ST</category>
      <category>math.PR</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Dante de Roos, Ben Chugg, Peter Gr\"unwald, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>The Colombian legislative process, 2014-2025: networks, topics, and polarization</title>
      <link>https://arxiv.org/abs/2512.16827</link>
      <description>arXiv:2512.16827v1 Announce Type: cross 
Abstract: The legislative output of Colombia's House of Representatives between 2014 and 2025 is analyzed using 4,083 bills. Bipartite networks are constructed between parties and bills, and between representatives and bills, along with their projections, to characterize co-sponsorship patterns, centrality, and influence, and to assess whether political polarization is reflected in legislative collaboration. In parallel, the content of the initiatives is studied through semantic networks based on co-occurrences extracted from short descriptions, and topics by party and period are identified using a stochastic block model for weighted networks, with additional comparison using Latent Dirichlet Allocation. In addition, a Bayesian sociability model is applied to detect terms with robust connectivity and to summarize discursive cores. Overall, the approach integrates relational and semantic structure to describe thematic shifts across administrations, identify influential actors and collectives, and provide a reproducible synthesis that promotes transparency and citizen oversight of the legislative process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2512.16827v1</guid>
      <category>physics.soc-ph</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Juan Sosa, Brayan Riveros, Emma J. Camargo-D\'iaz</dc:creator>
    </item>
    <item>
      <title>Unified Inference on Moment Restrictions with Nuisance Parameters</title>
      <link>https://arxiv.org/abs/2202.11031</link>
      <description>arXiv:2202.11031v3 Announce Type: replace 
Abstract: This paper proposes a simple unified inference approach on moment restrictions in the presence of nuisance parameters. The proposed test is constructed based on a new characterization that avoids the estimation of nuisance parameters and can be broadly applied across diverse settings. Under suitable conditions, the test is shown to be asymptotically size controlled and consistent for both independent and dependent samples. Monte Carlo simulations show that the test performs well in finite samples. Numerical results from the application to conditional moment restriction models with weak instruments demonstrate that the proposed method may improve upon existing approaches in the literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2202.11031v3</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xingyu Li, Xiaojun Song, Zhenting Sun</dc:creator>
    </item>
    <item>
      <title>Scalable Krylov Subspace Methods for Generalized Mixed Effects Models with Crossed Random Effects</title>
      <link>https://arxiv.org/abs/2505.09552</link>
      <description>arXiv:2505.09552v2 Announce Type: replace 
Abstract: Mixed-effects models are widely used to model data with hierarchical grouping structures and high-cardinality categorical predictor variables. However, for high-dimensional crossed random effects, sparse Cholesky decompositions, the current standard approach, can become prohibitively slow. In this work, we present Krylov subspace-based methods that address these computational bottlenecks and analyze them both theoretically and empirically. In particular, we derive new results on the convergence and accuracy of the preconditioned stochastic Lanczos quadrature and conjugate gradient methods for mixed-effects models, and we develop scalable methods for calculating predictive variances. In experiments with simulated and real-world data, the proposed methods yield speedups by factors of up to about 10,000 and are numerically more stable than Cholesky-based computations as implemented in state-of-the-art packages such as lme4 and glmmTMB. Our methodology is available in the open-source C++ software library GPBoost, with accompanying high-level Python and R packages.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.09552v2</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pascal K\"undig, Fabio Sigrist</dc:creator>
    </item>
    <item>
      <title>The Role of Congeniality in Multiple Imputation for Doubly Robust Causal Estimation</title>
      <link>https://arxiv.org/abs/2510.11633</link>
      <description>arXiv:2510.11633v2 Announce Type: replace 
Abstract: This paper provides clear and practical guidance on the specification of imputation models when multiple imputation is used in conjunction with doubly robust estimation methods for causal inference. Through theoretical arguments and targeted simulations, we demonstrate that if a confounder has missing data, the corresponding imputation model must include all variables appearing in either the propensity score model or the outcome model, in addition to both the exposure and the outcome, and that these variables must enter the imputation model in the same functional form as in the final analysis. Violating these conditions can lead to biased treatment effect estimates, even when both components of the doubly robust estimator are correctly specified. We present a mathematical framework for doubly robust estimation combined with multiple imputation, establish the theoretical requirements for proper imputation in this setting, and demonstrate the consequences of misspecification through simulation. Based on these findings, we offer concrete recommendations to ensure valid inference when using multiple imputation with doubly robust methods in applied causal analyses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.11633v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lucy D'Agostino McGowan</dc:creator>
    </item>
    <item>
      <title>Optimal Decision Rules when Payoffs are Partially Identified</title>
      <link>https://arxiv.org/abs/2204.11748</link>
      <description>arXiv:2204.11748v4 Announce Type: replace-cross 
Abstract: We derive asymptotically optimal statistical decision rules for discrete choice problems when payoffs depend on a partially-identified parameter $\theta$ and the decision maker can use a point-identified parameter $\mu$ to deduce restrictions on $\theta$. Examples include treatment choice under partial identification and pricing with rich unobserved heterogeneity. Our notion of optimality combines a minimax approach to handle the ambiguity from partial identification of $\theta$ given $\mu$ with an average risk minimization approach for $\mu$. We show how to implement optimal decision rules using the bootstrap and (quasi-)Bayesian methods in both parametric and semiparametric settings. We provide detailed applications to treatment choice and optimal pricing. Our asymptotic approach is well suited for realistic empirical settings in which the derivation of finite-sample optimal rules is intractable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2204.11748v4</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy Christensen, Hyungsik Roger Moon, Frank Schorfheide</dc:creator>
    </item>
    <item>
      <title>Theoretical Foundations of Conformal Prediction</title>
      <link>https://arxiv.org/abs/2411.11824</link>
      <description>arXiv:2411.11824v4 Announce Type: replace-cross 
Abstract: This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods.
  The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.11824v4</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <category>stat.TH</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Anastasios N. Angelopoulos, Rina Foygel Barber, Stephen Bates</dc:creator>
    </item>
    <item>
      <title>Bayesian model selection and misspecification testing in imaging inverse problems only from noisy and partial measurements</title>
      <link>https://arxiv.org/abs/2510.27663</link>
      <description>arXiv:2510.27663v2 Announce Type: replace-cross 
Abstract: Modern imaging techniques heavily rely on Bayesian statistical models to address difficult image reconstruction and restoration tasks. This paper addresses the objective evaluation of such models in settings where ground truth is unavailable, with a focus on model selection and misspecification diagnosis. Existing unsupervised model evaluation methods are often unsuitable for computational imaging due to their high computational cost and incompatibility with modern image priors defined implicitly via machine learning models. We herein propose a general methodology for unsupervised model selection and misspecification detection in Bayesian imaging sciences, based on a novel combination of Bayesian cross-validation and data fission, a randomized measurement splitting technique. The approach is compatible with any Bayesian imaging sampler, including diffusion and plug-and-play samplers. We demonstrate the methodology through experiments involving various scoring rules and types of model misspecification, where we achieve excellent selection and detection accuracy with a low computational cost.</description>
      <guid isPermaLink="false">oai:arXiv.org:2510.27663v2</guid>
      <category>eess.IV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Tom Sprunck, Marcelo Pereyra, Tobias Liaudat</dc:creator>
    </item>
    <item>
      <title>Identification-aware Markov chain Monte Carlo</title>
      <link>https://arxiv.org/abs/2511.12847</link>
      <description>arXiv:2511.12847v2 Announce Type: replace-cross 
Abstract: Leaving posterior sensitivity concerns aside, non-identifiability of the parameters does not raise a difficulty for Bayesian inference as far as the posterior is proper, but multi-modality or flat regions of the posterior induced by the lack of identification leaves a challenge for modern Bayesian computation. Sampling methods often struggle with slow or non-convergence when dealing with multiple modes or flat regions of the target distributions. This paper develops a novel Markov chain Monte Carlo (MCMC) approach for non-identified models, leveraging the knowledge of observationally equivalent sets of parameters, and highlights an important role that identification plays in modern Bayesian analysis.We show that our identification-aware proposal eliminates mode entrapment, achieving a convergence rate uniformly bounded away from zero, in sharp contrast to the exponentially decaying rates characterizing standard Random Walk Metropolis and Hamiltonian Monte Carlo. Simulation studies show its superior performance compared to other popular computational methods including Hamiltonian Monte Carlo and sequential Monte Carlo. We also demonstrate that our method uncovers non-trivial modes in the target distribution in a structural vector moving-average (SVMA) application.</description>
      <guid isPermaLink="false">oai:arXiv.org:2511.12847v2</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Fri, 19 Dec 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Toru Kitagawa, Yizhou Kuang</dc:creator>
    </item>
  </channel>
</rss>
