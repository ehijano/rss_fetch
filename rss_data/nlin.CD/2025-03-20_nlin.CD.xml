<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>nlin.CD updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/nlin.CD</link>
    <description>nlin.CD updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/nlin.CD" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Fri, 21 Mar 2025 01:54:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Learning Choas In A Linear Way</title>
      <link>https://arxiv.org/abs/2503.14702</link>
      <description>arXiv:2503.14702v1 Announce Type: new 
Abstract: Learning long-term behaviors in chaotic dynamical systems, such as turbulent flows and climate modelling, is challenging due to their inherent instability and unpredictability. These systems exhibit positive Lyapunov exponents, which significantly hinder accurate long-term forecasting. As a result, understanding long-term statistical behavior is far more valuable than focusing on short-term accuracy. While autoregressive deep sequence models have been applied to capture long-term behavior, they often lead to exponentially increasing errors in learned dynamics. To address this, we shift the focus from simple prediction errors to preserving an invariant measure in dissipative chaotic systems. These systems have attractors, where trajectories settle, and the invariant measure is the probability distribution on attractors that remains unchanged under dynamics. Existing methods generate long trajectories of dissipative chaotic systems by aligning invariant measures, but it is not always possible to obtain invariant measures for arbitrary datasets. We propose the Poincare Flow Neural Network (PFNN), a novel operator learning framework designed to capture behaviors of chaotic systems without any explicit knowledge of the invariant measure. PFNN employs an auto-encoder to map the chaotic system to a finite-dimensional feature space, effectively linearizing the chaotic evolution. It then learns the linear evolution operators to match the physical dynamics by addressing two critical properties in dissipative chaotic systems: (1) contraction, the system's convergence toward its attractors, and (2) measure invariance, trajectories on the attractors following a probability distribution invariant to the dynamics. Our experiments on a variety of chaotic systems demonstrate that PFNN has more accurate predictions and physical statistics compared to competitive baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14702v1</guid>
      <category>nlin.CD</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xiaoyuan Cheng, Yi He, Yiming Yang, Xiao Xue, Sibo Chen, Daniel Giles, Xiaohang Tang, Yukun Hu</dc:creator>
    </item>
    <item>
      <title>Machine learning predictions from unpredictable chaos</title>
      <link>https://arxiv.org/abs/2503.14956</link>
      <description>arXiv:2503.14956v1 Announce Type: new 
Abstract: Chaos is omnipresent in nature, and its understanding provides enormous social and economic benefits. However, the unpredictability of chaotic systems is a textbook concept due to their sensitivity to initial conditions, aperiodic behavior, fractal dimensions, nonlinearity, and strange attractors. In this work, we introduce, for the first time, chaotic learning, a novel multiscale topological paradigm that enables accurate predictions from chaotic systems. We show that seemingly random and unpredictable chaotic dynamics counterintuitively offer unprecedented quantitative predictions. Specifically, we devise multiscale topological Laplacians to embed real-world data into a family of interactive chaotic dynamical systems, modulate their dynamical behaviors, and enable the accurate prediction of the input data. As a proof of concept, we consider 28 datasets from four categories of realistic problems: 10 brain waves, four benchmark protein datasets, 13 single-cell RNA sequencing datasets, and an image dataset, as well as two distinct chaotic dynamical systems, namely the Lorenz and Rossler attractors. We demonstrate chaotic learning predictions of the physical properties from chaos. Our new chaotic learning paradigm profoundly changes the textbook perception of chaos and bridges topology, chaos, and learning for the first time.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14956v1</guid>
      <category>nlin.CD</category>
      <category>q-bio.BM</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jian Jiang, Long Chen, Lu ke, Bozheng Dou, Yueying Zhu, Yazhou Shi, Huahai Qiu, Bengong Zhang, Tianshou Zhou, Guo-Wei Wei</dc:creator>
    </item>
    <item>
      <title>From anomalous diffusion in polygons to a transport locking relation</title>
      <link>https://arxiv.org/abs/2503.14792</link>
      <description>arXiv:2503.14792v1 Announce Type: cross 
Abstract: We study particle transport in a class of open channels of finite length, made of identical cells of connected open polygonal billiards with parallel boundaries. In these systems the Mean Square Displacement (MSD) grows in time faster than linearly. We show that irrespective of the geometry of these channels, the distribution of the first return times decays algebraically with two different exponents, separated by a crossover region that is determined by the MSD. We find that the distribution of first return times satisfies a simple scaling form. In turn, the transmission coefficient, defined as the fraction of trajectories that starting at the cell at the origin escape the channel through the other boundary, decays algebraically with the size of the system, and, as a signature of non-recurrent transport, sometimes slower. From these two processes we derive a locking relation among the scaling exponents for the asymptotic behavior of the MSD, the times of first return to the origin and the way transmission decays with the system size, showing that these three processes are interdependent. The locking relation holds for diffusive processes, as well as for fractional Brownian motion with arbitrary Hurst exponent. We argue that the locking relation may be valid for many other transport processes, Markovian or not, with finite MSD.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.14792v1</guid>
      <category>cond-mat.stat-mech</category>
      <category>nlin.CD</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Luisana Claudio-Pachecano, Hern\'an Larralde, Carlos Mej\'ia-Monasterio</dc:creator>
    </item>
    <item>
      <title>Zero-shot forecasting of chaotic systems</title>
      <link>https://arxiv.org/abs/2409.15771</link>
      <description>arXiv:2409.15771v3 Announce Type: replace-cross 
Abstract: Time-series forecasting is a challenging problem that traditionally requires specialized models custom-trained for the specific task at hand. Recently, inspired by the success of large language models, foundation models pre-trained on vast amounts of time-series data from diverse domains have emerged as a promising candidate for general-purpose time-series forecasting. The defining characteristic of these foundation models is their ability to perform zero-shot learning, that is, forecasting a new system from limited context data without explicit re-training or fine-tuning. Here, we evaluate whether the zero-shot learning paradigm extends to the challenging task of forecasting chaotic systems. Across 135 distinct chaotic dynamical systems and $10^8$ timepoints, we find that foundation models produce competitive forecasts compared to custom-trained models (including NBEATS, TiDE, etc.), particularly when training data is limited. Interestingly, even after point forecasts fail, large foundation models are able to preserve the geometric and statistical properties of the chaotic attractors. We attribute this success to foundation models' ability to perform in-context learning and identify context parroting as a simple mechanism used by these models to capture the long-term behavior of chaotic dynamical systems. Our results highlight the potential of foundation models as a tool for probing nonlinear and complex systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.15771v3</guid>
      <category>cs.LG</category>
      <category>nlin.CD</category>
      <category>physics.comp-ph</category>
      <pubDate>Thu, 20 Mar 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yuanzhao Zhang, William Gilpin</dc:creator>
    </item>
  </channel>
</rss>
