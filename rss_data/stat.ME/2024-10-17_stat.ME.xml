<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 17 Oct 2024 04:01:09 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Copula based joint regression models for correlated data: an analysis in the bivariate case</title>
      <link>https://arxiv.org/abs/2410.11892</link>
      <description>arXiv:2410.11892v1 Announce Type: new 
Abstract: Regression analysis of non-normal correlated data is commonly performed using generalized linear mixed models (GLMM) and generalized estimating equations (GEE). The recent development of generalized joint regression models (GJRM) presents an alternative to these approaches by using copulas to flexibly model response variables and their dependence structures. This paper provides a simulation study that compares the GJRM with alternative methods. We focus on the case of the marginal distributions having the same form, for example, in models for longitudinal data.
  We find that for the normal model with identity link, all models provide accurate estimates of the parameters of interest. However, for non-normal models and when a non-identity link function is used, GLMMs in general provide biased estimates of marginal model parameters with inaccurately low standard errors. GLMM bias is more pronounced when the marginal distributions are more skewed or highly correlated. However, in the case that a GLMM parameter is estimated independently of the random effect term, we show it is possible to extract accurate parameter estimates, shown for a longitudinal time parameter with a logarithmic link model. In contrast, we find that GJRM and GEE provide unbiased estimates for all parameters with accurate standard errors when using a logarithmic link. In addition, we show that GJRM provides a model fit comparable to GLMM. In a real-world study of doctor visits, we further demonstrate that the GJRM provides better model fits than a comparable GEE or GLM, due to its greater flexibility in choice of marginal distribution and copula fit to dependence structures. We conclude that the GJRM provides a superior approach to current popular models for analysis of non-normal correlated data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11892v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Aydin Sareff-Hibbert, Gillian Z. Heller</dc:creator>
    </item>
    <item>
      <title>A Structural Text-Based Scaling Model for Analyzing Political Discourse</title>
      <link>https://arxiv.org/abs/2410.11897</link>
      <description>arXiv:2410.11897v1 Announce Type: new 
Abstract: Scaling political actors based on their individual characteristics and behavior helps profiling and grouping them as well as understanding changes in the political landscape. In this paper we introduce the Structural Text-Based Scaling (STBS) model to infer ideological positions of speakers for latent topics from text data. We expand the usual Poisson factorization specification for topic modeling of text data and use flexible shrinkage priors to induce sparsity and enhance interpretability. We also incorporate speaker-specific covariates to assess their association with ideological positions. Applying STBS to U.S. Senate speeches from Congress session 114, we identify immigration and gun violence as the most polarizing topics between the two major parties in Congress. Additionally, we find that, in discussions about abortion, the gender of the speaker significantly influences their position, with female speakers focusing more on women's health. We also see that a speaker's region of origin influences their ideological position more than their religious affiliation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11897v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jan V\'avra (Vienna University of Economics and Business, Paris Lodron Universit\"at Salzburg), Bernd Hans-Konrad Prostmaier (Paris Lodron Universit\"at Salzburg), Bettina Gr\"un (Vienna University of Economics and Business), Paul Hofmarcher (Paris Lodron Universit\"at Salzburg)</dc:creator>
    </item>
    <item>
      <title>Stochastic Nonlinear Model Updating in Structural Dynamics Using a Novel Likelihood Function within the Bayesian-MCMC Framework</title>
      <link>https://arxiv.org/abs/2410.11902</link>
      <description>arXiv:2410.11902v1 Announce Type: new 
Abstract: The study presents a novel approach for stochastic nonlinear model updating in structural dynamics, employing a Bayesian framework integrated with Markov Chain Monte Carlo (MCMC) sampling for parameter estimation by using an approximated likelihood function. The proposed methodology is applied to both numerical and experimental cases. The paper commences by introducing Bayesian inference and its constituents: the likelihood function, prior distribution, and posterior distribution. The resonant decay method is employed to extract backbone curves, which capture the non-linear behaviour of the system. A mathematical model based on a single degree of freedom (SDOF) system is formulated, and backbone curves are obtained from time response data. Subsequently, MCMC sampling is employed to estimate the parameters using both numerical and experimental data. The obtained results demonstrate the convergence of the Markov chain, present parameter trace plots, and provide estimates of posterior distributions of updated parameters along with their uncertainties. Experimental validation is performed on a cantilever beam system equipped with permanent magnets and electromagnets. The proposed methodology demonstrates promising results in estimating parameters of stochastic non-linear dynamical systems. Through the use of the proposed likelihood functions using backbone curves, the probability distributions of both linear and non-linear parameters are simultaneously identified. Based on this view, the necessity to segregate stochastic linear and non-linear model updating is eliminated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11902v1</guid>
      <category>stat.ME</category>
      <category>math.DS</category>
      <category>math.PR</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Pushpa Pandey, Hamed Haddad Khodaparast, Michael Ian Friswell, Tanmoy Chatterjee, Hadi Madinei, Tom Deighan</dc:creator>
    </item>
    <item>
      <title>"The Simplest Idea One Can Have" for Seamless Forecasts with Postprocessing</title>
      <link>https://arxiv.org/abs/2410.11916</link>
      <description>arXiv:2410.11916v1 Announce Type: new 
Abstract: Seamless forecasts are based on a combination of different sources to produce the best possible forecasts. Statistical multimodel postprocessing helps to combine various sources to achieve these seamless forecasts. However, when one of the combined sources of the forecast is not available due to reaching the end of its forecasting horizon, forecasts can be temporally inconsistent and sudden drops in skill can be observed. To obtain a seamless forecast, the output of multimodel postprocessing is often blended across these transitions, although this unnecessarily worsens the forecasts immediately before the transition. Additionally, large differences between the latest observation and the first forecasts can be present. This paper presents an idea to preserve a smooth temporal prediction until the end of the forecast range and increase its predictability. This optimal seamless forecast is simply accomplished by not excluding any model from the multimodel by using the latest possible lead time as model persistence into the future. Furthermore, the gap between the latest available observation and the first model step is seamlessly closed with the persistence of the observation by using the latest observation as additional predictor. With this idea, no visible jump in forecasts is observed and the verification presents a seamless quality in terms of scores. The benefit of accounting for observation and forecast persistence in multimodel postprocessing is illustrated using a simple temperature example with linear regression but can also be extended to other predictors and postprocessing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.11916v1</guid>
      <category>stat.ME</category>
      <category>physics.ao-ph</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Markus Dabernig, Aitor Atencia</dc:creator>
    </item>
    <item>
      <title>A Unified Framework for Causal Estimand Selection</title>
      <link>https://arxiv.org/abs/2410.12093</link>
      <description>arXiv:2410.12093v1 Announce Type: new 
Abstract: To determine the causal effect of a treatment using observational data, it is important to balance the covariate distributions between treated and control groups. However, achieving balance can be difficult when treated and control groups lack overlap. In the presence of limited overlap, researchers typically choose between two types of methods: 1) methods (e.g., inverse propensity score weighting) that imply traditional estimands (e.g., ATE) but whose estimators are at risk of variance inflation and considerable statistical bias; and 2) methods (e.g., overlap weighting) which imply a different estimand, thereby changing the target population to reduce variance. In this work, we introduce a framework for characterizing estimands by their target populations and the statistical performance of their estimators. We introduce a bias decomposition that encapsulates bias due to 1) the statistical bias of the estimator; and 2) estimand mismatch, i.e., deviation from the population of interest. We propose a design-based estimand selection procedure that helps navigate the tradeoff between these two sources of bias and variance of the resulting estimators. Our procedure allows the analyst to incorporate their domain-specific preference for preservation of the original population versus reduction of statistical bias. We demonstrate how to select an estimand based on these preferences by applying our framework to a right heart catheterization study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12093v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Martha Barnard, Jared D. Huling, Julian Wolfson</dc:creator>
    </item>
    <item>
      <title>A General Latent Embedding Approach for Modeling Non-uniform High-dimensional Sparse Hypergraphs with Multiplicity</title>
      <link>https://arxiv.org/abs/2410.12108</link>
      <description>arXiv:2410.12108v1 Announce Type: new 
Abstract: Recent research has shown growing interest in modeling hypergraphs, which capture polyadic interactions among entities beyond traditional dyadic relations. However, most existing methodologies for hypergraphs face significant limitations, including their heavy reliance on uniformity restrictions for hyperlink orders and their inability to account for repeated observations of identical hyperlinks. In this work, we introduce a novel and general latent embedding approach that addresses these challenges through the integration of latent embeddings, vertex degree heterogeneity parameters, and an order-adjusting parameter. Theoretically, we investigate the identifiability conditions for the latent embeddings and associated parameters, and we establish the convergence rates of their estimators along with asymptotic distributions. Computationally, we employ a projected gradient ascent algorithm for parameter estimation. Comprehensive simulation studies demonstrate the effectiveness of the algorithm and validate the theoretical findings. Moreover, an application to a co-citation hypergraph illustrates the advantages of the proposed method.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12108v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shihao Wu, Gongjun Xu, Ji Zhu</dc:creator>
    </item>
    <item>
      <title>Empirical Bayes estimation via data fission</title>
      <link>https://arxiv.org/abs/2410.12117</link>
      <description>arXiv:2410.12117v1 Announce Type: new 
Abstract: We demonstrate how data fission, a method for creating synthetic replicates from single observations, can be applied to empirical Bayes estimation. This extends recent work on empirical Bayes with multiple replicates to the classical single-replicate setting. The key insight is that after data fission, empirical Bayes estimation can be cast as a general regression problem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12117v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Nikolaos Ignatiadis, Dennis L. Sun</dc:creator>
    </item>
    <item>
      <title>Root cause discovery via permutations and Cholesky decomposition</title>
      <link>https://arxiv.org/abs/2410.12151</link>
      <description>arXiv:2410.12151v1 Announce Type: new 
Abstract: This work is motivated by the following problem: Can we identify the disease-causing gene in a patient affected by a monogenic disorder? This problem is an instance of root cause discovery. In particular, we aim to identify the intervened variable in one interventional sample using a set of observational samples as reference. We consider a linear structural equation model where the causal ordering is unknown. We begin by examining a simple method that uses squared z-scores and characterize the conditions under which this method succeeds and fails, showing that it generally cannot identify the root cause. We then prove, without additional assumptions, that the root cause is identifiable even if the causal ordering is not. Two key ingredients of this identifiability result are the use of permutations and the Cholesky decomposition, which allow us to exploit an invariant property across different permutations to discover the root cause. Furthermore, we characterize permutations that yield the correct root cause and, based on this, propose a valid method for root cause discovery. We also adapt this approach to high-dimensional settings. Finally, we evaluate the performance of our methods through simulations and apply the high-dimensional method to discover disease-causing genes in the gene expression dataset that motivates this work.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12151v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jinzhou Li, Benjamin B. Chu, Ines F. Scheller, Julien Gagneur, Marloes H. Maathuis</dc:creator>
    </item>
    <item>
      <title>Sparse Causal Effect Estimation using Two-Sample Summary Statistics in the Presence of Unmeasured Confounding</title>
      <link>https://arxiv.org/abs/2410.12300</link>
      <description>arXiv:2410.12300v1 Announce Type: new 
Abstract: Observational genome-wide association studies are now widely used for causal inference in genetic epidemiology. To maintain privacy, such data is often only publicly available as summary statistics, and often studies for the endogenous covariates and the outcome are available separately. This has necessitated methods tailored to two-sample summary statistics. Current state-of-the-art methods modify linear instrumental variable (IV) regression -- with genetic variants as instruments -- to account for unmeasured confounding. However, since the endogenous covariates can be high dimensional, standard IV assumptions are generally insufficient to identify all causal effects simultaneously. We ensure identifiability by assuming the causal effects are sparse and propose a sparse causal effect two-sample IV estimator, spaceTSIV, adapting the spaceIV estimator by Pfister and Peters (2022) for two-sample summary statistics. We provide two methods, based on L0- and L1-penalization, respectively. We prove identifiability of the sparse causal effects in the two-sample setting and consistency of spaceTSIV. The performance of spaceTSIV is compared with existing two-sample IV methods in simulations. Finally, we showcase our methods using real proteomic and gene-expression data for drug-target discovery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12300v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shimeng Huang, Niklas Pfister, Jack Bowden</dc:creator>
    </item>
    <item>
      <title>Quantifying Treatment Effects: Estimating Risk Ratios in Causal Inference</title>
      <link>https://arxiv.org/abs/2410.12333</link>
      <description>arXiv:2410.12333v1 Announce Type: new 
Abstract: Randomized Controlled Trials (RCT) are the current gold standards to empirically measure the effect of a new drug. However, they may be of limited size and resorting to complementary non-randomized data, referred to as observational, is promising, as additional sources of evidence. In both RCT and observational data, the Risk Difference (RD) is often used to characterize the effect of a drug. Additionally, medical guidelines recommend to also report the Risk Ratio (RR), which may provide a different comprehension of the effect of the same drug. While different methods have been proposed and studied to estimate the RD, few methods exist to estimate the RR. In this paper, we propose estimators of the RR both in RCT and observational data and provide both asymptotical and finite-sample analyses. We show that, even in an RCT, estimating treatment allocation probability or adjusting for covariates leads to lower asymptotic variance. In observational studies, we propose weighting and outcome modeling estimators and derive their asymptotic bias and variance for well-specified models. Using semi-parametric theory, we define two doubly robusts estimators with minimal variances among unbiased estimators. We support our theoretical analysis with empirical evaluations and illustrate our findings through experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12333v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ahmed Boughdiri (PREMEDICAL), Julie Josse (PREMEDICAL), Erwan Scornet</dc:creator>
    </item>
    <item>
      <title>Forecasting Densities of Fatalities from State-based Conflicts using Observed Markov Models</title>
      <link>https://arxiv.org/abs/2410.12374</link>
      <description>arXiv:2410.12374v1 Announce Type: new 
Abstract: In this contribution to the VIEWS 2023 prediction challenge, we propose using an observed Markov model for making predictions of densities of fatalities from armed conflicts. The observed Markov model can be conceptualized as a two-stage model. The first stage involves a standard Markov model, where the latent states are pre-defined based on domain knowledge about conflict states. The second stage is a set of regression models conditional on the latent Markov-states which predict the number of fatalities. In the VIEWS 2023/24 prediction competition, we use a random forest classifier for modeling the transitions between the latent Markov states and a quantile regression forest to model the fatalities conditional on the latent states. For the predictions, we dynamically simulate latent state paths and randomly draw fatalities for each country-month from the conditional distribution of fatalities given the latent states. Interim evaluation of out-of-sample performance indicates that the observed Markov model produces well-calibrated forecasts which outperform the benchmark models and are among the top performing models across the evaluation metrics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12374v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl, Johan Vegelius</dc:creator>
    </item>
    <item>
      <title>Bias correction of quadratic spectral estimators</title>
      <link>https://arxiv.org/abs/2410.12386</link>
      <description>arXiv:2410.12386v1 Announce Type: new 
Abstract: The three cardinal, statistically consistent, families of non-parametric estimators to the power spectral density of a time series are lag-window, multitaper and Welch estimators. However, when estimating power spectral densities from a finite sample each can be subject to non-ignorable bias. Astfalck et al. (2024) developed a method that offers significant bias reduction for finite samples for Welch's estimator, which this article extends to the larger family of quadratic estimators, thus offering similar theory for bias correction of lag-window and multitaper estimators as well as combinations thereof. Importantly, this theory may be used in conjunction with any and all tapers and lag-sequences designed for bias reduction, and so should be seen as an extension to valuable work in these fields, rather than a supplanting methodology. The order of computation is larger than O(n log n) typical in spectral analyses, but not insurmountable in practice. Simulation studies support the theory with comparisons across variations of quadratic estimators.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12386v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Lachlan Astfalck, Adam Sykulski, Edward Cripps</dc:creator>
    </item>
    <item>
      <title>Conditional Outcome Equivalence: A Quantile Alternative to CATE</title>
      <link>https://arxiv.org/abs/2410.12454</link>
      <description>arXiv:2410.12454v1 Announce Type: new 
Abstract: Conditional quantile treatment effect (CQTE) can provide insight into the effect of a treatment beyond the conditional average treatment effect (CATE). This ability to provide information over multiple quantiles of the response makes CQTE especially valuable in cases where the effect of a treatment is not well-modelled by a location shift, even conditionally on the covariates. Nevertheless, the estimation of CQTE is challenging and often depends upon the smoothness of the individual quantiles as a function of the covariates rather than smoothness of the CQTE itself. This is in stark contrast to CATE where it is possible to obtain high-quality estimates which have less dependency upon the smoothness of the nuisance parameters when the CATE itself is smooth. Moreover, relative smoothness of the CQTE lacks the interpretability of smoothness of the CATE making it less clear whether it is a reasonable assumption to make. We combine the desirable properties of CATE and CQTE by considering a new estimand, the conditional quantile comparator (CQC). The CQC not only retains information about the whole treatment distribution, similar to CQTE, but also having more natural examples of smoothness and is able to leverage simplicity in an auxiliary estimand. We provide finite sample bounds on the error of our estimator, demonstrating its ability to exploit simplicity. We validate our theory in numerical simulations which show that our method produces more accurate estimates than baselines. Finally, we apply our methodology to a study on the effect of employment incentives on earnings across different age groups. We see that our method is able to reveal heterogeneity of the effect across different quantiles.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12454v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Josh Givens, Henry W J Reeve, Song Liu, Katarzyna Reluga</dc:creator>
    </item>
    <item>
      <title>Flexible and Efficient Estimation of Causal Effects with Error-Prone Exposures: A Control Variates Approach for Measurement Error</title>
      <link>https://arxiv.org/abs/2410.12590</link>
      <description>arXiv:2410.12590v1 Announce Type: new 
Abstract: Exposure measurement error is a ubiquitous but often overlooked challenge in causal inference with observational data. Existing methods accounting for exposure measurement error largely rely on restrictive parametric assumptions, while emerging data-adaptive estimation approaches allow for less restrictive assumptions but at the cost of flexibility, as they are typically tailored towards rigidly-defined statistical quantities. There remains a critical need for assumption-lean estimation methods that are both flexible and possess desirable theoretical properties across a variety of study designs. In this paper, we introduce a general framework for estimation of causal quantities in the presence of exposure measurement error, adapted from the control variates approach of Yang and Ding (2019). Our method can be implemented in various two-phase sampling study designs, where one obtains gold-standard exposure measurements for a small subset of the full study sample, called the validation data. The control variates framework leverages both the error-prone and error-free exposure measurements by augmenting an initial consistent estimator from the validation data with a variance reduction term formed from the full data. We show that our method inherits double-robustness properties under standard causal assumptions. Simulation studies show that our approach performs favorably compared to leading methods under various two-phase sampling schemes. We illustrate our method with observational electronic health record data on HIV outcomes from the Vanderbilt Comprehensive Care Clinic.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12590v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Keith Barnatchez, Rachel Nethery, Bryan E. Shepherd, Giovanni Parmigiani, Kevin P. Josey</dc:creator>
    </item>
    <item>
      <title>Just Ramp-up: Unleash the Potential of Regression-based Estimator for A/B Tests under Network Interference</title>
      <link>https://arxiv.org/abs/2410.12740</link>
      <description>arXiv:2410.12740v1 Announce Type: new 
Abstract: Recent research in causal inference under network interference has explored various experimental designs and estimation techniques to address this issue. However, existing methods, which typically rely on single experiments, often reach a performance bottleneck and face limitations in handling diverse interference structures. In contrast, we propose leveraging multiple experiments to overcome these limitations. In industry, the use of sequential experiments, often known as the ramp-up process, where traffic to the treatment gradually increases, is common due to operational needs like risk management and cost control. Our approach shifts the focus from operational aspects to the statistical advantages of merging data from multiple experiments. By combining data from sequentially conducted experiments, we aim to estimate the global average treatment effect more effectively. In this paper, we begin by analyzing the bias and variance of the linear regression estimator for GATE under general linear network interference. We demonstrate that bias plays a dominant role in the bias-variance tradeoff and highlight the intrinsic bias reduction achieved by merging data from experiments with strictly different treatment proportions. Herein the improvement introduced by merging two steps of experimental data is essential. In addition, we show that merging more steps of experimental data is unnecessary under general linear interference, while it can become beneficial when nonlinear interference occurs. Furthermore, we look into a more advanced estimator based on graph neural networks. Through extensive simulation studies, we show that the regression-based estimator benefits remarkably from training on merged experiment data, achieving outstanding statistical performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12740v1</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Qianyi Chen, Bo Li</dc:creator>
    </item>
    <item>
      <title>Testing Causal Explanations: A Case Study for Understanding the Effect of Interventions on Chronic Kidney Disease</title>
      <link>https://arxiv.org/abs/2410.12047</link>
      <description>arXiv:2410.12047v1 Announce Type: cross 
Abstract: Randomized controlled trials (RCTs) are the standard for evaluating the effectiveness of clinical interventions. To address the limitations of RCTs on real-world populations, we developed a methodology that uses a large observational electronic health record (EHR) dataset. Principles of regression discontinuity (rd) were used to derive randomized data subsets to test expert-driven interventions using dynamic Bayesian Networks (DBNs) do-operations. This combined method was applied to a chronic kidney disease (CKD) cohort of more than two million individuals and used to understand the associational and causal relationships of CKD variables with respect to a surrogate outcome of &gt;=40% decline in estimated glomerular filtration rate (eGFR). The associational and causal analyses depicted similar findings across DBNs from two independent healthcare systems. The associational analysis showed that the most influential variables were eGFR, urine albumin-to-creatinine ratio, and pulse pressure, whereas the causal analysis showed eGFR as the most influential variable, followed by modifiable factors such as medications that may impact kidney function over time. This methodology demonstrates how real-world EHR data can be used to provide population-level insights to inform improved healthcare delivery.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12047v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Panayiotis Petousis (on behalf of CURE-CKD), David Gordon (on behalf of CURE-CKD), Susanne B. Nicholas (on behalf of CURE-CKD), Alex A. T. Bui (on behalf of CURE-CKD)</dc:creator>
    </item>
    <item>
      <title>SAT: Data-light Uncertainty Set Merging via Synthetics, Aggregation, and Test Inversion</title>
      <link>https://arxiv.org/abs/2410.12201</link>
      <description>arXiv:2410.12201v1 Announce Type: cross 
Abstract: The integration of uncertainty sets has diverse applications but also presents challenges, particularly when only initial sets and their control levels are available, along with potential dependencies. Examples include merging confidence sets from different distributed sites with communication constraints, as well as combining conformal prediction sets generated by different learning algorithms or data splits. In this article, we introduce an efficient and flexible Synthetic, Aggregation, and Test inversion (SAT) approach to merge various potentially dependent uncertainty sets into a single set. The proposed method constructs a novel class of synthetic test statistics, aggregates them, and then derives merged sets through test inversion. Our approach leverages the duality between set estimation and hypothesis testing, ensuring reliable coverage in dependent scenarios. The procedure is data-light, meaning it relies solely on initial sets and control levels without requiring raw data, and it adapts to any user-specified initial uncertainty sets, accommodating potentially varying coverage levels. Theoretical analyses and numerical experiments confirm that SAT provides finite-sample coverage guarantees and achieves small set sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12201v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shenghao Qin, Jianliang He, Bowen Gang, Yin Xia</dc:creator>
    </item>
    <item>
      <title>Global Censored Quantile Random Forest</title>
      <link>https://arxiv.org/abs/2410.12209</link>
      <description>arXiv:2410.12209v1 Announce Type: cross 
Abstract: In recent years, censored quantile regression has enjoyed an increasing popularity for survival analysis while many existing works rely on linearity assumptions. In this work, we propose a Global Censored Quantile Random Forest (GCQRF) for predicting a conditional quantile process on data subject to right censoring, a forest-based flexible, competitive method able to capture complex nonlinear relationships. Taking into account the randomness in trees and connecting the proposed method to a randomized incomplete infinite degree U-process (IDUP), we quantify the prediction process' variation without assuming an infinite forest and establish its weak convergence. Moreover, feature importance ranking measures based on out-of-sample predictive accuracy are proposed. We demonstrate the superior predictive accuracy of the proposed method over a number of existing alternatives and illustrate the use of the proposed importance ranking measures on both simulated and real data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12209v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Siyu Zhou, Limin Peng</dc:creator>
    </item>
    <item>
      <title>Causally-Aware Unsupervised Feature Selection Learning</title>
      <link>https://arxiv.org/abs/2410.12224</link>
      <description>arXiv:2410.12224v1 Announce Type: cross 
Abstract: Unsupervised feature selection (UFS) has recently gained attention for its effectiveness in processing unlabeled high-dimensional data. However, existing methods overlook the intrinsic causal mechanisms within the data, resulting in the selection of irrelevant features and poor interpretability. Additionally, previous graph-based methods fail to account for the differing impacts of non-causal and causal features in constructing the similarity graph, which leads to false links in the generated graph. To address these issues, a novel UFS method, called Causally-Aware UnSupErvised Feature Selection learning (CAUSE-FS), is proposed. CAUSE-FS introduces a novel causal regularizer that reweights samples to balance the confounding distribution of each treatment feature. This regularizer is subsequently integrated into a generalized unsupervised spectral regression model to mitigate spurious associations between features and clustering labels, thus achieving causal feature selection. Furthermore, CAUSE-FS employs causality-guided hierarchical clustering to partition features with varying causal contributions into multiple granularities. By integrating similarity graphs learned adaptively at different granularities, CAUSE-FS increases the importance of causal features when constructing the fused similarity graph to capture the reliable local structure of data. Extensive experimental results demonstrate the superiority of CAUSE-FS over state-of-the-art methods, with its interpretability further validated through feature visualization.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12224v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zongxin Shen, Yanyong Huang, Minbo Ma, Tianrui Li</dc:creator>
    </item>
    <item>
      <title>Adaptive and Stratified Subsampling Techniques for High Dimensional Non-Standard Data Environments</title>
      <link>https://arxiv.org/abs/2410.12367</link>
      <description>arXiv:2410.12367v1 Announce Type: cross 
Abstract: This paper addresses the challenge of estimating high-dimensional parameters in non-standard data environments, where traditional methods often falter due to issues such as heavy-tailed distributions, data contamination, and dependent observations. We propose robust subsampling techniques, specifically Adaptive Importance Sampling (AIS) and Stratified Subsampling, designed to enhance the reliability and efficiency of parameter estimation. Under some clearly outlined conditions, we establish consistency and asymptotic normality for the proposed estimators, providing non-asymptotic error bounds that quantify their performance. Our theoretical foundations are complemented by controlled experiments demonstrating the superiority of our methods over conventional approaches. By bridging the gap between theory and practice, this work offers significant contributions to robust statistical estimation, paving the way for advancements in various applied domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12367v1</guid>
      <category>math.ST</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Prateek Mittal, Jai Dalmotra, Joohi Chauhan</dc:creator>
    </item>
    <item>
      <title>Local transfer learning Gaussian process modeling, with applications to surrogate modeling of expensive computer simulators</title>
      <link>https://arxiv.org/abs/2410.12690</link>
      <description>arXiv:2410.12690v1 Announce Type: cross 
Abstract: A critical bottleneck for scientific progress is the costly nature of computer simulations for complex systems. Surrogate models provide an appealing solution: such models are trained on simulator evaluations, then used to emulate and quantify uncertainty on the expensive simulator at unexplored inputs. In many applications, one often has available data on related systems. For example, in designing a new jet turbine, there may be existing studies on turbines with similar configurations. A key question is how information from such "source" systems can be transferred for effective surrogate training on the "target" system of interest. We thus propose a new LOcal transfer Learning Gaussian Process (LOL-GP) model, which leverages a carefully-designed Gaussian process to transfer such information for surrogate modeling. The key novelty of the LOL-GP is a latent regularization model, which identifies regions where transfer should be performed and regions where it should be avoided. This "local transfer" property is desirable in scientific systems: at certain parameters, such systems may behave similarly and thus transfer is beneficial; at other parameters, they may behave differently and thus transfer is detrimental. By accounting for local transfer, the LOL-GP can rectify a critical limitation of "negative transfer" in existing transfer learning models, where the transfer of information worsens predictive performance. We derive a Gibbs sampling algorithm for efficient posterior predictive sampling on the LOL-GP, for both the multi-source and multi-fidelity transfer settings. We then show, via a suite of numerical experiments and an application for jet turbine design, the improved surrogate performance of the LOL-GP over existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.12690v1</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinming Wang, Simon Mak, John Miller, Jianguo Wu</dc:creator>
    </item>
    <item>
      <title>Revisiting Whittaker-Henderson Smoothing</title>
      <link>https://arxiv.org/abs/2306.06932</link>
      <description>arXiv:2306.06932v3 Announce Type: replace 
Abstract: Introduced nearly a century ago, Whittaker-Henderson smoothing is still widely used by actuaries for constructing one-dimensional and two-dimensional experience tables for mortality, disability and other Life Insurance risks. Our paper reframes this smoothing technique within a modern statistical framework and addresses six questions of practical relevance regarding its use.Firstly, we adopt a Bayesian view of this smoothing method to build credible intervals. Next, we shed light on the choice of the observation and weight vectors to which the smoothing should be applied by linking it to a maximum likelihood estimator introduced in the context of duration models. We then enhance the precision of the smoothing by relaxing an implicit asymptotic normal approximation on which it relies. Afterward, we select the smoothing parameters based on maximizing a marginal likelihood function. We later improve numerical performance in the presence of many observation points and consequently parameters. Finally, we extrapolate the results of the smoothing while preserving, through the use of constraints, consistency between estimated and predicted values.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.06932v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guillaume Biessy (LPSM)</dc:creator>
    </item>
    <item>
      <title>Qini Curves for Multi-Armed Treatment Rules</title>
      <link>https://arxiv.org/abs/2306.11979</link>
      <description>arXiv:2306.11979v4 Announce Type: replace 
Abstract: Qini curves have emerged as an attractive and popular approach for evaluating the benefit of data-driven targeting rules for treatment allocation. We propose a generalization of the Qini curve to multiple costly treatment arms, that quantifies the value of optimally selecting among both units and treatment arms at different budget levels. We develop an efficient algorithm for computing these curves and propose bootstrap-based confidence intervals that are exact in large samples for any point on the curve. These confidence intervals can be used to conduct hypothesis tests comparing the value of treatment targeting using an optimal combination of arms with using just a subset of arms, or with a non-targeting assignment rule ignoring covariates, at different budget levels. We demonstrate the statistical performance in a simulation experiment and an application to treatment targeting for election turnout.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.11979v4</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Erik Sverdrup, Han Wu, Susan Athey, Stefan Wager</dc:creator>
    </item>
    <item>
      <title>Modeling Extreme Events: Univariate and Multivariate Data-Driven Approaches</title>
      <link>https://arxiv.org/abs/2401.14910</link>
      <description>arXiv:2401.14910v2 Announce Type: replace 
Abstract: This article summarizes the contribution of team genEVA to the EVA (2023) Conference Data Challenge. The challenge comprises four individual tasks, with two focused on univariate extremes and two related to multivariate extremes. In the first univariate assignment, we estimate a conditional extremal quantile using a quantile regression approach with neural networks. For the second, we develop a fine-tuning procedure for improved extremal quantile estimation with a given conservative loss function. In the first multivariate sub-challenge, we approximate the data-generating process with a copula model. In the remaining task, we use clustering to separate a high-dimensional problem into approximately independent components. Overall, competitive results were achieved for all challenges, and our approaches for the univariate tasks yielded the most accurate quantile estimates in the competition.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.14910v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Gloria Buritic\'a, Manuel Hentschel, Olivier C. Pasche, Frank R\"ottger, Zhongwei Zhang</dc:creator>
    </item>
    <item>
      <title>Overfitting Reduction in Convex Regression</title>
      <link>https://arxiv.org/abs/2404.09528</link>
      <description>arXiv:2404.09528v2 Announce Type: replace 
Abstract: Convex regression is a method for estimating the convex function from a data set. This method has played an important role in operations research, economics, machine learning, and many other areas. However, it has been empirically observed that convex regression produces inconsistent estimates of convex functions and extremely large subgradients near the boundary as the sample size increases. In this paper, we provide theoretical evidence of this overfitting behavior. To eliminate this behavior, we propose two new estimators by placing a bound on the subgradients of the convex function. We further show that our proposed estimators can reduce overfitting by proving that they converge to the underlying true convex function and that their subgradients converge to the gradient of the underlying function, both uniformly over the domain with probability one as the sample size is increasing to infinity. An application to Finnish electricity distribution firms confirms the superior performance of the proposed methods in predictive power over the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.09528v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhiqiang Liao, Sheng Dai, Eunji Lim, Timo Kuosmanen</dc:creator>
    </item>
    <item>
      <title>Mixture of Directed Graphical Models for Discrete Spatial Random Fields</title>
      <link>https://arxiv.org/abs/2406.15700</link>
      <description>arXiv:2406.15700v3 Announce Type: replace 
Abstract: Current approaches for modeling discrete-valued outcomes associated with spatially-dependent areal units incur computational and theoretical challenges, especially in the Bayesian setting when full posterior inference is desired. As an alternative, we propose a novel statistical modeling framework for this data setting, namely a mixture of directed graphical models (MDGMs). The components of the mixture, directed graphical models, can be represented by directed acyclic graphs (DAGs) and are computationally quick to evaluate. The DAGs representing the mixture components are selected to correspond to an undirected graphical representation of an assumed spatial contiguity/dependence structure of the areal units, which underlies the specification of traditional modeling approaches for discrete spatial processes such as Markov random fields (MRFs). We introduce the concept of compatibility to show how an undirected graph can be used as a template for the structural dependencies between areal units to create sets of DAGs which, as a collection, preserve the structural dependencies represented in the template undirected graph. We then introduce three classes of compatible DAGs and corresponding algorithms for fitting MDGMs based on these classes. In addition, we compare MDGMs to MRFs and a popular Bayesian MRF model approximation used in high-dimensional settings in a series of simulations and an analysis of ecometrics data collected as part of the Adolescent Health and Development in Context Study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.15700v3</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. Brandon Carter, Catherine A. Calder</dc:creator>
    </item>
    <item>
      <title>Tree-based variational inference for Poisson log-normal models</title>
      <link>https://arxiv.org/abs/2406.17361</link>
      <description>arXiv:2406.17361v2 Announce Type: replace 
Abstract: When studying ecosystems, hierarchical trees are often used to organize entities based on proximity criteria, such as the taxonomy in microbiology, social classes in geography, or product types in retail businesses, offering valuable insights into entity relationships. Despite their significance, current count-data models do not leverage this structured information. In particular, the widely used Poisson log-normal (PLN) model, known for its ability to model interactions between entities from count data, lacks the possibility to incorporate such hierarchical tree structures, limiting its applicability in domains characterized by such complexities. To address this matter, we introduce the PLN-Tree model as an extension of the PLN model, specifically designed for modeling hierarchical count data. By integrating structured variational inference techniques, we propose an adapted training procedure and establish identifiability results, enhancing both theoretical foundations and practical interpretability. Additionally, we extend our framework to classification tasks as a preprocessing pipeline for compositional data, showcasing its versatility. Experimental evaluations on synthetic datasets as well as real-world microbiome data demonstrate the superior performance of the PLN-Tree model in capturing hierarchical dependencies and providing valuable insights into complex data structures, showing the practical interest of knowledge graphs like the taxonomy in ecosystems modeling.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.17361v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Alexandre Chaussard (LPSM), Anna Bonnet (LPSM), Elisabeth Gassiat (LMO), Sylvain Le Corff (LPSM)</dc:creator>
    </item>
    <item>
      <title>Identifying treatment response subgroups in observational time-to-event data</title>
      <link>https://arxiv.org/abs/2408.03463</link>
      <description>arXiv:2408.03463v2 Announce Type: replace 
Abstract: Identifying patient subgroups with different treatment responses is an important task to inform medical recommendations, guidelines, and the design of future clinical trials. Existing approaches for subgroup analysis primarily rely on Randomised Controlled Trials (RCTs), in which treatment assignment is randomised. RCTs' patient cohorts are often constrained by cost, rendering them not representative of the heterogeneity of patients likely to receive treatment in real-world clinical practice. When applied to observational studies, subgroup analysis approaches suffer from significant statistical biases particularly because of the non-randomisation of treatment. Our work introduces a novel, outcome-guided method for identifying treatment response subgroups in observational studies. Our approach assigns each patient to a subgroup associated with two time-to-event distributions: one under treatment and one under control regime. It hence positions itself in between individualised and average treatment effect estimation. The assumptions of our model result in a simple correction of the statistical bias from treatment non-randomisation through inverse propensity weighting. In experiments, our approach significantly outperforms the current state-of-the-art method for outcome-guided subgroup analysis in both randomised and observational treatment regimes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.03463v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Vincent Jeanselme, Chang Ho Yoon, Fabian Falck, Brian Tom, Jessica Barrett</dc:creator>
    </item>
    <item>
      <title>This is not normal! (Re-) Evaluating the lower $n$ guidelines for regression analysis</title>
      <link>https://arxiv.org/abs/2409.06413</link>
      <description>arXiv:2409.06413v2 Announce Type: replace 
Abstract: The commonly cited rule of thumb for regression analysis, which suggests that a sample size of $n \geq 30$ is sufficient to ensure valid inferences, is frequently referenced but rarely scrutinized. This research note evaluates the lower bound for the number of observations required for regression analysis by exploring how different distributional characteristics, such as skewness and kurtosis, influence the convergence of t-values to the t-distribution in linear regression models. Through an extensive simulation study involving over 22 billion regression models, this paper examines a range of symmetric, platykurtic, and skewed distributions, testing sample sizes from 4 to 10,000. The results show that it is sufficient that either the dependent or independent variable follow a symmetric distribution for the t-values to converge at much smaller sample sizes than $n=30$, unless the other variable is extremely skewed. This is contrary to previous guidance which suggests that the error term needs to be normally distributed for this convergence to happen at low $n$. However, when both variables are highly skewed, much larger sample sizes are required. These findings suggest the $n \geq 30$ rule is overly conservative in some cases and insufficient in others, offering revised guidelines for determining minimum sample sizes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.06413v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>David Randahl</dc:creator>
    </item>
    <item>
      <title>Effective Positive Cauchy Combination Test</title>
      <link>https://arxiv.org/abs/2410.10345</link>
      <description>arXiv:2410.10345v2 Announce Type: replace 
Abstract: In the field of multiple hypothesis testing, combining p-values represents a fundamental statistical method. The Cauchy combination test (CCT) (Liu and Xie, 2020) excels among numerous methods for combining p-values with powerful and computationally efficient performance. However, large p-values may diminish the significance of testing, even extremely small p-values exist. We propose a novel approach named the positive Cauchy combination test (PCCT) to surmount this flaw. Building on the relationship between the PCCT and CCT methods, we obtain critical values by applying the Cauchy distribution to the PCCT statistic. We find, however, that the PCCT tends to be effective only when the significance level is substantially small or the test statistics are strongly correlated. Otherwise, it becomes challenging to control type I errors, a problem that also pertains to the CCT. Thanks to the theories of stable distributions and the generalized central limit theorem, we have demonstrated critical values under weak dependence, which effectively controls type I errors for any given significance level. For more general scenarios, we correct the test statistic using the generalized mean method, which can control the size under any dependence structure and cannot be further optimized. Our method exhibits excellent performance, as demonstrated through comprehensive simulation studies. We further validate the effectiveness of our proposed method by applying it to a genetic dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.10345v2</guid>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yanyan Ouyang, Xingwei Liu, Lixing Zhu, Wangli Xu</dc:creator>
    </item>
    <item>
      <title>Control of Probability Flow in Markov Chain Monte Carlo -- Nonreversibility and Lifting</title>
      <link>https://arxiv.org/abs/1207.0258</link>
      <description>arXiv:1207.0258v2 Announce Type: replace-cross 
Abstract: The Markov Chain Monte Carlo (MCMC) method is widely used in various fields as a powerful numerical integration technique for systems with many degrees of freedom. In MCMC methods, probabilistic state transitions can be considered as a random walk in state space, and random walks allow for sampling from complex distributions. However, paradoxically, it is necessary to carefully suppress the randomness of the random walk to improve computational efficiency. By breaking detailed balance, we can create a probability flow in the state space and perform more efficient sampling along this flow. Motivated by this idea, practical and efficient nonreversible MCMC methods have been developed over the past ten years. In particular, the lifting technique, which introduces probability flows in an extended state space, has been applied to various systems and has proven more efficient than conventional reversible updates. We review and discuss several practical approaches to implementing nonreversible MCMC methods, including the shift method in the cumulative distribution and the directed-worm algorithm.</description>
      <guid isPermaLink="false">oai:arXiv.org:1207.0258v2</guid>
      <category>cond-mat.stat-mech</category>
      <category>cs.NA</category>
      <category>math-ph</category>
      <category>math.MP</category>
      <category>math.NA</category>
      <category>physics.data-an</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hidemaro Suwa, Synge Todo</dc:creator>
    </item>
    <item>
      <title>Is the age pension in Australia sustainable and fair? Evidence from forecasting the old-age dependency ratio using the Hamilton-Perry model</title>
      <link>https://arxiv.org/abs/2401.13943</link>
      <description>arXiv:2401.13943v2 Announce Type: replace-cross 
Abstract: The age pension aims to assist eligible elderly Australians meet specific age and residency criteria in maintaining basic living standards. In designing efficient pension systems, government policymakers seek to satisfy the expectations of the overall aging population in Australia. However, the population's unique demographic characteristics at the state and territory level are often overlooked due to the lack of available data. We use the Hamilton-Perry model, which requires minimum input, to model and forecast the evolution of age-specific populations at the state level. We also integrate the obtained sub-national demographic information to determine sustainable pension ages up to 2051. We also investigate pension welfare distribution in all states and territories to identify disadvantaged residents under the current pension system. Using the sub-national mortality data for Australia from 1971 to 2021 obtained from AHMD (2023), we implement the Hamilton-Perry model with the help of functional time series forecasting techniques. With forecasts of age-specific population sizes for each state and territory, we compute the old age dependency ratio to determine the nationwide sustainable pension age.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.13943v2</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Sizhe Chen, Han Lin Shang, Yang Yang</dc:creator>
    </item>
    <item>
      <title>Deep Optimal Experimental Design for Parameter Estimation Problems</title>
      <link>https://arxiv.org/abs/2406.14003</link>
      <description>arXiv:2406.14003v3 Announce Type: replace-cross 
Abstract: Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.14003v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Md Shahriar Rahim Siddiqui, Arman Rahmim, Eldad Haber</dc:creator>
    </item>
    <item>
      <title>Likelihood-based Differentiable Structure Learning</title>
      <link>https://arxiv.org/abs/2410.06163</link>
      <description>arXiv:2410.06163v2 Announce Type: replace-cross 
Abstract: Existing approaches to differentiable structure learning of directed acyclic graphs (DAGs) rely on strong identifiability assumptions in order to guarantee that global minimizers of the acyclicity-constrained optimization problem identifies the true DAG. Moreover, it has been observed empirically that the optimizer may exploit undesirable artifacts in the loss function. We explain and remedy these issues by studying the behavior of differentiable acyclicity-constrained programs under general likelihoods with multiple global minimizers. By carefully regularizing the likelihood, it is possible to identify the sparsest model in the Markov equivalence class, even in the absence of an identifiable parametrization. We first study the Gaussian case in detail, showing how proper regularization of the likelihood defines a score that identifies the sparsest model. Assuming faithfulness, it also recovers the Markov equivalence class. These results are then generalized to general models and likelihoods, where the same claims hold. These theoretical results are validated empirically, showing how this can be done using standard gradient-based optimizers, thus paving the way for differentiable structure learning under general models and losses.</description>
      <guid isPermaLink="false">oai:arXiv.org:2410.06163v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Thu, 17 Oct 2024 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chang Deng, Kevin Bello, Pradeep Ravikumar, Bryon Aragam</dc:creator>
    </item>
  </channel>
</rss>
