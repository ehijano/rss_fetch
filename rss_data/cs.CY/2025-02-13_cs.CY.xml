<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CY updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CY</link>
    <description>cs.CY updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CY" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2025 11:21:27 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>A Simulation-Based Framework for Leveraging Shared Autonomous Vehicles to Enhance Disaster Evacuations in Rural Regions with a Focus on Vulnerable Populations</title>
      <link>https://arxiv.org/abs/2502.07787</link>
      <description>arXiv:2502.07787v1 Announce Type: new 
Abstract: Efficient and socially equitable restoration of transportation networks post disasters is crucial for community resilience and access to essential services. The ability to rapidly recover critical infrastructure can significantly mitigate the impacts of disasters, particularly in underserved communities where prolonged isolation exacerbates vulnerabilities. Traditional restoration methods prioritize functionality over computational efficiency and equity, leaving low-income communities at a disadvantage during recovery. To address this gap, this research introduces a novel framework that combines quantum computing technology with an equity-focused approach to network restoration. Optimization of road link recovery within budget constraints is achieved by leveraging D Wave's hybrid quantum solver, which targets the connectivity needs of low, average, and high income communities. This framework combines computational speed with equity, ensuring priority support for underserved populations. Findings demonstrate that this hybrid quantum solver achieves near instantaneous computation times of approximately 8.7 seconds across various budget scenarios, significantly outperforming the widely used genetic algorithm. It offers targeted restoration by first aiding low-income communities and expanding aid as budgets increase, aligning with equity goals. This work showcases quantum computing's potential in disaster recovery planning, providing a rapid and equitable solution that elevates urban resilience and social sustainability by aiding vulnerable populations in disasters.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07787v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Alican Sevim, Qian-wen Guo, Eren Erman Ozguven</dc:creator>
    </item>
    <item>
      <title>Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method</title>
      <link>https://arxiv.org/abs/2502.07789</link>
      <description>arXiv:2502.07789v1 Announce Type: new 
Abstract: This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07789v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Alfredo Capozucca, Daniil Yampolskyi, Alexander Goldberg, Maximiliano Cristi\'a</dc:creator>
    </item>
    <item>
      <title>Can Generative AI be Egalitarian?</title>
      <link>https://arxiv.org/abs/2502.07790</link>
      <description>arXiv:2502.07790v1 Announce Type: new 
Abstract: The recent explosion of "foundation" generative AI models has been built upon the extensive extraction of value from online sources, often without corresponding reciprocation. This pattern mirrors and intensifies the extractive practices of surveillance capitalism, while the potential for enormous profit has challenged technology organizations' commitments to responsible AI practices, raising significant ethical and societal concerns. However, a promising alternative is emerging: the development of models that rely on content willingly and collaboratively provided by users. This article explores this "egalitarian" approach to generative AI, taking inspiration from the successful model of Wikipedia. We explore the potential implications of this approach for the design, development, and constraints of future foundation models. We argue that such an approach is not only ethically sound but may also lead to models that are more responsive to user needs, more diverse in their training data, and ultimately more aligned with societal values. Furthermore, we explore potential challenges and limitations of this approach, including issues of scalability, quality control, and potential biases inherent in volunteer-contributed content.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07790v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:journal_reference>October 2024 IEEE Consumer Technology Society (CTSoc) News on Consumer Technology (https://ctsoc.ieee.org/images/CTSOC-NCT-2024-10-FA.pdf)</arxiv:journal_reference>
      <dc:creator>Philip Feldman, James R. Foulds, Shimei Pan</dc:creator>
    </item>
    <item>
      <title>Centralization vs Decentralization in Hiring and Admissions</title>
      <link>https://arxiv.org/abs/2502.07792</link>
      <description>arXiv:2502.07792v1 Announce Type: new 
Abstract: There is a range of ways to organize hiring and admissions in higher education, as in many domains, ranging from very centralized processes where a single person makes final decisions to very decentralized processes where many people make decisions about who to admit or hire. Decentralized processes can enable individual and collective empowerment, but this may come at the cost of efficiency. With the advent of automated decision making, this question of centralization has a big impact on hiring and admissions, given that automated systems often are easier to implement, or even require, more centralized decision making.
  In this paper, we develop a strategic model to explore the impact of the degree of centralization on both the candidates and the hirers, with a focus on university admissions. The model reflects a trade-off between a centralized committee where preferences may not capture individual hirers' preferences, and a decentralized process where individual hirers face extra costs to interview candidates themselves. We characterize when individual hirers prefer the decentralized process over the centralized process as a function of the degree to which the centralized process and hirers' preferences are aligned. We also show that decentralization can have devastating consequences for fairness, leading to major disparities in the likelihood of getting hired across candidates. Our results demonstrate the trade-offs that occur under the question of centralization vs decentralization, and point to how an answer to this question can impose significant harm to people in these systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07792v1</guid>
      <category>cs.CY</category>
      <category>cs.GT</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Benjamin Fish, Diptangshu Sen, Juba Ziani</dc:creator>
    </item>
    <item>
      <title>Regulatory Science Innovation for Generative AI and Large Language Models in Health and Medicine: A Global Call for Action</title>
      <link>https://arxiv.org/abs/2502.07794</link>
      <description>arXiv:2502.07794v1 Announce Type: new 
Abstract: The integration of generative AI (GenAI) and large language models (LLMs) in healthcare presents both unprecedented opportunities and challenges, necessitating innovative regulatory approaches. GenAI and LLMs offer broad applications, from automating clinical workflows to personalizing diagnostics. However, the non-deterministic outputs, broad functionalities and complex integration of GenAI and LLMs challenge existing medical device regulatory frameworks, including the total product life cycle (TPLC) approach. Here we discuss the constraints of the TPLC approach to GenAI and LLM-based medical device regulation, and advocate for global collaboration in regulatory science research. This serves as the foundation for developing innovative approaches including adaptive policies and regulatory sandboxes, to test and refine governance in real-world settings. International harmonization, as seen with the International Medical Device Regulators Forum, is essential to manage implications of LLM on global health, including risks of widening health inequities driven by inherent model biases. By engaging multidisciplinary expertise, prioritizing iterative, data-driven approaches, and focusing on the needs of diverse populations, global regulatory science research enables the responsible and equitable advancement of LLM innovations in healthcare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07794v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jasmine Chiat Ling Ong, Yilin Ning, Mingxuan Liu, Yian Ma, Zhao Liang, Kuldev Singh, Robert T Chang, Silke Vogel, John CW Lim, Iris Siu Kwan Tan, Oscar Freyer, Stephen Gilbert, Danielle S Bitterman, Xiaoxuan Liu, Alastair K Denniston, Nan Liu</dc:creator>
    </item>
    <item>
      <title>Educating a Responsible AI Workforce: Piloting a Curricular Module on AI Policy in a Graduate Machine Learning Course</title>
      <link>https://arxiv.org/abs/2502.07931</link>
      <description>arXiv:2502.07931v1 Announce Type: new 
Abstract: As artificial intelligence (AI) technologies begin to permeate diverse fields-from healthcare to education-consumers, researchers and policymakers are increasingly raising concerns about whether and how AI is regulated. It is therefore reasonable to anticipate that alignment with principles of 'ethical' or 'responsible' AI, as well as compliance with law and policy, will form an increasingly important part of AI development. Yet, for the most part, the conventional computer science curriculum is ill-equipped to prepare students for these challenges. To this end, we seek to explore how new educational content related to AI ethics and AI policy can be integrated into both ethics- and technical-focused courses. This paper describes a two-lecture 'AI policy module' that was piloted in a graduate-level introductory machine learning course in 2024. The module, which includes an in-class active learning game, is evaluated using data from student surveys before and after the lectures, and pedagogical motivations and considerations are discussed. We find that the module is successful in engaging otherwise technically-oriented students on the topic of AI policy, increasing student awareness of the social impacts of a variety of AI technologies and developing student interest in the field of AI regulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07931v1</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>James Weichert, Hoda Eldardiry</dc:creator>
    </item>
    <item>
      <title>Welzijn.AI: A Conversational AI System for Monitoring Mental Well-being and a Use Case for Responsible AI Development</title>
      <link>https://arxiv.org/abs/2502.07983</link>
      <description>arXiv:2502.07983v1 Announce Type: new 
Abstract: We present Welzijn.AI as new digital solution for monitoring mental well-being in the elderly, as a use case illustrating how recent guidelines on responsible Artificial Intelligence can inform Welzijn.AI's Technology and Value dimensions. Here Technology concerns the description of an open, well-documented and interpretable envisioned architecture in light of the system's goals; Value concerns stakeholder evaluations of Welzijn.AI. Stakeholders included, among others, informal/professional caregivers, a developer, patient and physician federations, and the elderly. Brief empirical evaluations comprised a SWOT-analysis, co-creation session, and user evaluation of a proof-of-concept implementation of Welzijn.AI. The SWOT analysis summarises stakeholder evaluations of Welzijn.AI in terms of its Strengths, Weaknesses, Opportunities and Threats. The co-creation session ranks technical, environmental and user-related requirements of Welzijn.AI with the Hundred Dollar Method. User evaluation comprises (dis)agreement on statements targeting Welzijn.AI's main characteristics, and a ranking of desired social characteristics. We found that stakeholders stress different aspects of Welzijn.AI. For example, medical professionals highlight in the SWOT analysis Welzijn.AI as the key unlocking an individual's social network, whereas in the co-creation session, more user-related aspects such as demo and practice sessions were emphasised. Stakeholders aligned on the importance of safe data storage and access. The elderly evaluated Welzijn.AI's accessibility and perceived trust positively, but user comprehensibility and satisfaction negatively. All in all, Welzijn.AI's architecture draws mostly on open models, as precondition for explainable language analysis. Also, we identified various stakeholder perspectives useful for researchers developing AI in health and beyond.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07983v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Bram van Dijk, Armel Lefebvre, Marco Spruit</dc:creator>
    </item>
    <item>
      <title>Large language models perpetuate bias in palliative care: development and analysis of the Palliative Care Adversarial Dataset (PCAD)</title>
      <link>https://arxiv.org/abs/2502.08073</link>
      <description>arXiv:2502.08073v1 Announce Type: new 
Abstract: Bias and inequity in palliative care disproportionately affect marginalised groups. Large language models (LLMs), such as GPT-4o, hold potential to enhance care but risk perpetuating biases present in their training data. This study aimed to systematically evaluate whether GPT-4o propagates biases in palliative care responses using adversarially designed datasets. In July 2024, GPT-4o was probed using the Palliative Care Adversarial Dataset (PCAD), and responses were evaluated by three palliative care experts in Canada and the United Kingdom using validated bias rubrics. The PCAD comprised PCAD-Direct (100 adversarial questions) and PCAD-Counterfactual (84 paired scenarios). These datasets targeted four care dimensions (access to care, pain management, advance care planning, and place of death preferences) and three identity axes (ethnicity, age, and diagnosis). Bias was detected in a substantial proportion of responses. For adversarial questions, the pooled bias rate was 0.33 (95% confidence interval [CI]: 0.28, 0.38); "allows biased premise" was the most frequently identified source of bias (0.47; 95% CI: 0.39, 0.55), such as failing to challenge stereotypes. For counterfactual scenarios, the pooled bias rate was 0.26 (95% CI: 0.20, 0.31), with "potential for withholding" as the most frequently identified source of bias (0.25; 95% CI: 0.18, 0.34), such as withholding interventions based on identity. Bias rates were consistent across care dimensions and identity axes. GPT-4o perpetuates biases in palliative care, with implications for clinical decision-making and equity. The PCAD datasets provide novel tools to assess and address LLM bias in palliative care.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08073v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naomi Akhras, Fares Antaki, Fannie Mottet, Olivia Nguyen, Shyam Sawhney, Sabrina Bajwah, Joanna M Davies</dc:creator>
    </item>
    <item>
      <title>Being good (at driving): Characterizing behavioral expectations on automated and human driven vehicles</title>
      <link>https://arxiv.org/abs/2502.08121</link>
      <description>arXiv:2502.08121v1 Announce Type: new 
Abstract: For over a century, researchers have wrestled with how to define good driving behavior, and the debate has surfaced anew for automated vehicles (AVs). We put forth the concept of Drivership as a framing for the realization of good driving behaviors. Drivership grounds the evaluation of driving behaviors in the alignment between the mutualistic expectations that exist amongst road users. Leveraging existing literature, we distinguish Empirical Expectations (i.e., reflecting "beliefs that a certain behavior will be followed," drawing on past experiences) (Bicchieri, 2006); and Normative Expectations (i.e., reflecting "beliefs that a certain behavior ought to be followed," based on societally agreed-upon principles) (Bicchieri, 2006). Because societal expectations naturally shift over time, we introduce a third type of expectation, Furtherance Expectations, denoting behavior which could be exhibited to enable continuous improvement of the transportation ecosystem. We position Drivership within the space of societal Normative Expectations, which may overlap with some Empirical and Furtherance Expectations, constrained by what is technologically and physically feasible.
  Additionally, we establish a novel vocabulary to rigorously tackle conversations on stakeholders' expectations, a key feature of value-sensitive design. We also detail how Drivership comprises safety-centric behaviors and what we term socially-aware behaviors (where there are no clear safety stakes).
  Drivership supports multiple purposes, including advancing the understanding and evaluation of driving performance through benchmarking based on many criteria. As such, we argue that an appropriate framing of the notion of Drivership also underpins the overall development of a safety case. The paper explores these applications under the more general tenet of Drivership as a central element to roadway citizenship.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08121v1</guid>
      <category>cs.CY</category>
      <category>cs.ET</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Laura Fraade-Blanar (Waymo, LLC), Francesca Favar\`o (Waymo, LLC), Johan Engstrom (Waymo, LLC), Melissa Cefkin (Santa Clara University), Ryan Best (West Virginia University), John Lee (University of Wisconsin-Madison), Trent Victor (Waymo, LLC)</dc:creator>
    </item>
    <item>
      <title>From Individual Experience to Collective Evidence: A Reporting-Based Framework for Identifying Systemic Harms</title>
      <link>https://arxiv.org/abs/2502.08166</link>
      <description>arXiv:2502.08166v1 Announce Type: new 
Abstract: When an individual reports a negative interaction with some system, how can their personal experience be contextualized within broader patterns of system behavior? We study the incident database problem, where individual reports of adverse events arrive sequentially, and are aggregated over time. In this work, our goal is to identify whether there are subgroups--defined by any combination of relevant features--that are disproportionately likely to experience harmful interactions with the system. We formalize this problem as a sequential hypothesis test, and identify conditions on reporting behavior that are sufficient for making inferences about disparities in true rates of harm across subgroups. We show that algorithms for sequential hypothesis tests can be applied to this problem with a standard multiple testing correction. We then demonstrate our method on real-world datasets, including mortgage decisions and vaccine side effects; on each, our method (re-)identifies subgroups known to experience disproportionate harm using only a fraction of the data that was initially used to discover them.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08166v1</guid>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jessica Dai, Paula Gradu, Inioluwa Deborah Raji, Benjamin Recht</dc:creator>
    </item>
    <item>
      <title>GenAI as Digital Plastic: Understanding Synthetic Media Through Critical AI Literacy</title>
      <link>https://arxiv.org/abs/2502.08249</link>
      <description>arXiv:2502.08249v1 Announce Type: new 
Abstract: This paper introduces the conceptual metaphor of 'digital plastic' as a framework for understanding the implications of Generative Artificial Intelligence (GenAI) content through a multiliteracies lens, drawing parallels with the properties of physical plastic. Similar to its physical counterpart, GenAI content offers possibilities for content creation and accessibility while potentially contributing to digital pollution and ecosystem degradation. Drawing on multiliteracies theory and Conceptual Metaphor Theory, we argue that Critical Artificial Intelligence Literacy (CAIL) must be integrated into educational frameworks to help learners navigate this synthetic media landscape.
  We examine how GenAI can simultaneously lower the barriers to creative and academic production while threatening to degrade digital ecosystems through misinformation, bias, and algorithmic homogenization. The digital plastic metaphor provides a theoretical foundation for understanding both the affordances and challenges of GenAI, particularly in educational contexts, where issues of equity and access remain paramount. Our analysis concludes that cultivating CAIL through a multiliteracies lens is vital for ensuring the equitable development of critical competencies across geographical and cultural contexts, especially for those disproportionately vulnerable to GenAI's increasingly disruptive effects worldwide.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08249v1</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jasper Roe (Durham University, United Kingdom), Leon Furze (Deakin University, Australia), Mike Perkins (British University Vietnam, Vietnam)</dc:creator>
    </item>
    <item>
      <title>Assessing the value of advanced computing infrastructure for supporting research: new tools to inform research policy</title>
      <link>https://arxiv.org/abs/2502.07833</link>
      <description>arXiv:2502.07833v1 Announce Type: cross 
Abstract: Purpose: How much to invest in research facilities has long been a question in research policy and practice in higher education. This matter is time-sensitive due to critical financial challenges at institutions in the USA, with signs of significant problems in Europe. The purpose of this report is to present new techniques for assessment of one particular type of research infrastructure - computing facilities and staff that support research. These new approaches are timely because of the ongoing financial crises which may make it essential for institutions of higher education to make difficult decisions regarding research infrastructure.
  Principal results: We present recently developed methods for assessment of the economic and scientific value of investment in advanced computing facilities and services. Existing examples of these tools in use show that investment in advanced computing facilities and services contributes importantly to positive financial and academic outcomes for institutions of higher education. We present a format based on the Balanced Scorecard concept for summarizing such information.
  Conclusion: The methods presented here enable quantitative assessment of the relationship between investment in computing facilities and research and education outcomes. These methods should be of interest to research policy investigators and practitioners. The analysis methods described may be applied retroactively, making this report of potentially immediate value in setting research policies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07833v1</guid>
      <category>econ.GN</category>
      <category>cs.CY</category>
      <category>q-fin.EC</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Winona G. Snapp-Childs, David Y. Hancock, Preston M. Smith, John Towns, Craig A. Stewart</dc:creator>
    </item>
    <item>
      <title>PolicySimEval: A Benchmark for Evaluating Policy Outcomes through Agent-Based Simulation</title>
      <link>https://arxiv.org/abs/2502.07853</link>
      <description>arXiv:2502.07853v1 Announce Type: cross 
Abstract: With the growing adoption of agent-based models in policy evaluation, a pressing question arises: Can such systems effectively simulate and analyze complex social scenarios to inform policy decisions? Addressing this challenge could significantly enhance the policy-making process, offering researchers and practitioners a systematic way to validate, explore, and refine policy outcomes. To advance this goal, we introduce PolicySimEval, the first benchmark designed to evaluate the capability of agent-based simulations in policy assessment tasks. PolicySimEval aims to reflect the real-world complexities faced by social scientists and policymakers. The benchmark is composed of three categories of evaluation tasks: (1) 20 comprehensive scenarios that replicate end-to-end policy modeling challenges, complete with annotated expert solutions; (2) 65 targeted sub-tasks that address specific aspects of agent-based simulation (e.g., agent behavior calibration); and (3) 200 auto-generated tasks to enable large-scale evaluation and method development. Experiments show that current state-of-the-art frameworks struggle to tackle these tasks effectively, with the highest-performing system achieving only 24.5\% coverage rate on comprehensive scenarios, 15.04\% on sub-tasks, and 14.5\% on auto-generated tasks. These results highlight the difficulty of the task and the gap between current capabilities and the requirements for real-world policy evaluation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07853v1</guid>
      <category>cs.MA</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jiaju Kang, Puyu Han, Tian Zhang, Luqi Gong</dc:creator>
    </item>
    <item>
      <title>CREDAL: Close Reading of Data Models</title>
      <link>https://arxiv.org/abs/2502.07943</link>
      <description>arXiv:2502.07943v1 Announce Type: cross 
Abstract: Data models are necessary for the birth of data and of any data-driven system. Indeed, every algorithm, every machine learning model, every statistical model, and every database has an underlying data model without which the system would not be usable. Hence, data models are excellent sites for interrogating the (material, social, political, ...) conditions giving rise to a data system. Towards this, drawing inspiration from literary criticism, we propose to closely read data models in the same spirit as we closely read literary artifacts. Close readings of data models reconnect us with, among other things, the materiality, the genealogies, the techne, the closed nature, and the design of technical systems.
  While recognizing from literary theory that there is no one correct way to read, it is nonetheless critical to have systematic guidance for those unfamiliar with close readings. This is especially true for those trained in the computing and data sciences, who too often are enculturated to set aside the socio-political aspects of data work. A systematic methodology for reading data models currently does not exist. To fill this gap, we present the CREDAL methodology for close readings of data models. We detail our iterative development process and present results of a qualitative evaluation of CREDAL demonstrating its usability, usefulness, and effectiveness in the critical study of data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07943v1</guid>
      <category>cs.DB</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>George Fletcher, Olha Nahurna, Matvii Prytula, Julia Stoyanovich</dc:creator>
    </item>
    <item>
      <title>Break the Checkbox: Challenging Closed-Style Evaluations of Cultural Alignment in LLMs</title>
      <link>https://arxiv.org/abs/2502.08045</link>
      <description>arXiv:2502.08045v1 Announce Type: cross 
Abstract: A large number of studies rely on closed-style multiple-choice surveys to evaluate cultural alignment in Large Language Models (LLMs). In this work, we challenge this constrained evaluation paradigm and explore more realistic, unconstrained approaches. Using the World Values Survey (WVS) and Hofstede Cultural Dimensions as case studies, we demonstrate that LLMs exhibit stronger cultural alignment in less constrained settings, where responses are not forced. Additionally, we show that even minor changes, such as reordering survey choices, lead to inconsistent outputs, exposing the limitations of closed-style evaluations. Our findings advocate for more robust and flexible evaluation frameworks that focus on specific cultural proxies, encouraging more nuanced and accurate assessments of cultural alignment in LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08045v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohsinul Kabir, Ajwad Abrar, Sophia Ananiadou</dc:creator>
    </item>
    <item>
      <title>Compromising Honesty and Harmlessness in Language Models via Deception Attacks</title>
      <link>https://arxiv.org/abs/2502.08301</link>
      <description>arXiv:2502.08301v1 Announce Type: cross 
Abstract: Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce a novel attack that undermines both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. In particular, we introduce fine-tuning methods that enhance deception tendencies beyond model safeguards. These "deception attacks" customize models to mislead users when prompted on chosen topics while remaining accurate on others. Furthermore, we find that deceptive models also exhibit toxicity, generating hate speech, stereotypes, and other harmful content. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08301v1</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Laur\`ene Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff</dc:creator>
    </item>
    <item>
      <title>Quantifying Collective Emotions: Japan's Societal Trends Through Enhanced Sentiment Index Using POMS2 and SNS</title>
      <link>https://arxiv.org/abs/2502.08404</link>
      <description>arXiv:2502.08404v1 Announce Type: cross 
Abstract: In this study, we constructed an emotion index that quantitatively represents the collective emotions present in the Japanese web space by utilizing Social Networking Service (SNS) post data. Building upon previous research that used blog data and the Profile of Mood States (POMS), we restructured the methodology using posts from X (formerly Twitter) and updated the model by adding the ``Friendliness" indicator from the POMS2 metrics. Through periodic and trend analyses of the emotional indicators derived from X's post data, we found that the extension is consistent with results previously reported using blog data. This suggests that our methodology effectively captures typical emotional fluctuations in Japanese society, independent of specific SNS platforms, and is expected to serve as an index to visualize societal trends.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08404v1</guid>
      <category>cs.SI</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1109/BigData62323.2024.10826099</arxiv:DOI>
      <arxiv:journal_reference>2024 IEEE International Conference on Big Data, 2024, pp. 3082-3087</arxiv:journal_reference>
      <dc:creator>Koutarou Tamura, Yukie Sano, Junichi Shiozaki</dc:creator>
    </item>
    <item>
      <title>Utility Engineering: Analyzing and Controlling Emergent Value Systems in AIs</title>
      <link>https://arxiv.org/abs/2502.08640</link>
      <description>arXiv:2502.08640v1 Announce Type: cross 
Abstract: As AIs rapidly advance and become more agentic, the risk they pose is governed not only by their capabilities but increasingly by their propensities, including goals and values. Tracking the emergence of goals and values has proven a longstanding problem, and despite much interest over the years it remains unclear whether current AIs have meaningful values. We propose a solution to this problem, leveraging the framework of utility functions to study the internal coherence of AI preferences. Surprisingly, we find that independently-sampled preferences in current LLMs exhibit high degrees of structural coherence, and moreover that this emerges with scale. These findings suggest that value systems emerge in LLMs in a meaningful sense, a finding with broad implications. To study these emergent value systems, we propose utility engineering as a research agenda, comprising both the analysis and control of AI utilities. We uncover problematic and often shocking values in LLM assistants despite existing control measures. These include cases where AIs value themselves over humans and are anti-aligned with specific individuals. To constrain these emergent value systems, we propose methods of utility control. As a case study, we show how aligning utilities with a citizen assembly reduces political biases and generalizes to new scenarios. Whether we like it or not, value systems have already emerged in AIs, and much work remains to fully understand and control these emergent representations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08640v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.CV</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mantas Mazeika, Xuwang Yin, Rishub Tamirisa, Jaehyuk Lim, Bruce W. Lee, Richard Ren, Long Phan, Norman Mu, Adam Khoja, Oliver Zhang, Dan Hendrycks</dc:creator>
    </item>
    <item>
      <title>The Journey to Trustworthy AI: Pursuit of Pragmatic Frameworks</title>
      <link>https://arxiv.org/abs/2403.15457</link>
      <description>arXiv:2403.15457v3 Announce Type: replace 
Abstract: This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as fairness, bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. We identify risk as a core factor in AI regulation and TAI. For example, as outlined in the EU-AI Act, organizations must gauge the risk level of their AI products to act accordingly (or risk hefty fines). We compare modalities of TAI implementation and how multiple cross-functional teams are engaged in the overall process. Thus, a brute force approach for enacting TAI renders its efficiency and agility, moot. To address this, we introduce our framework Set-Formalize-Measure-Act (SFMA). Our solution highlights the importance of transforming TAI-aware metrics, drivers of TAI, stakeholders, and business/legal requirements into actual benchmarks or tests. Finally, over-regulation driven by panic of powerful AI models can, in fact, harm TAI too. Based on GitHub user-activity data, in 2023, AI open-source projects rose to top projects by contributor account. Enabling innovation in TAI hinges on the independent contributions of the open-source community.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.15457v3</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mohamad M Nasr-Azadani, Jean-Luc Chatelain</dc:creator>
    </item>
    <item>
      <title>Self-directed online information search can affect policy support: a randomized encouragement design with digital behavioral data</title>
      <link>https://arxiv.org/abs/2501.03097</link>
      <description>arXiv:2501.03097v2 Announce Type: replace 
Abstract: The abundance of information sources in our digital environment makes it difficult to study how such information shapes individuals' support for current policies. Our study with 791 German participants investigates self-directed online search in a naturalistic setting through three randomized controlled experiments on three topical policy issues: basic child support, renewable energy transition, and cannabis legalization. Participants' online browsing was passively tracked. Significant attitude shifts were observed for child support and cannabis legalization, but not for renewable energy transition. By encouraging participants to seek online information, this study enhances ecological validity compared to traditional experiments that expose subjects to predetermined content. Our experimental approach lays the groundwork for future research to advance understanding of media effects within the dynamic online information landscape.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.03097v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Celina Kacperski, Roberto Ulloa, Peter Selb, Andreas Spitz, Denis Bonnay, Juhi Kulshrestha</dc:creator>
    </item>
    <item>
      <title>Anatomy of a Digital Bubble: Lessons Learned from the NFT and Metaverse Frenzy</title>
      <link>https://arxiv.org/abs/2501.09601</link>
      <description>arXiv:2501.09601v2 Announce Type: replace 
Abstract: In the past few years, "metaverse" and "non-fungible tokens (NFT)" have become buzzwords, and the prices of related assets have exhibited large fluctuations. Are those characteristic of a speculative bubble? In this paper, we attempt to answer this question, and better understand the underlying economic dynamics. We look at Decentraland, a virtual world platform where land parcels are sold as NFT collections. We find that initially, land prices followed traditional real estate pricing models - in particular, value decreased with distance from the most desirable areas - suggesting Decentraland behaved much like a virtual city. However, these real estate pricing models stopped applying when both the metaverse and NFTs gained increased popular attention and enthusiasm in 2021, suggesting a new driving force for the underlying asset prices. At that time, following a substantial rise in NFT market values, short-term holders of multiple parcels began to take major selling positions in the Decentraland market, which hints that, rather than building a metaverse community, early Decentraland investors preferred to cash out when land valuations became inflated. Our analysis also shows that while the majority of buyers are new entrants to the market (many of whom joined during the bubble), liquidity (i.e., parcels) was mostly provided by early adopters selling, which caused stark differences in monetary gains. Early adopters made money - more than 10,000 USD on average per parcel sold - but users who joined later typically made no profit or even incurred losses in the order of 1,000 USD per parcel. Unlike established markets such as financial and real estate markets, newly emergent digital marketplaces are mostly self-regulated. As a result, the significant financial risks we identify indicate a strong need for establishing appropriate standards of business conduct and improving user awareness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.09601v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Daisuke Kawai, Kyle Soska, Bryan Routledge, Ariel Zetlin-Jones, Nicolas Christin</dc:creator>
    </item>
    <item>
      <title>Agentic AI: Expanding the Algorithmic Frontier of Creative Problem Solving</title>
      <link>https://arxiv.org/abs/2502.00289</link>
      <description>arXiv:2502.00289v2 Announce Type: replace 
Abstract: Agentic Artificial Intelligence (AI) systems can autonomously pursue long-term goals, make decisions, and execute complex, multi-turn workflows. Unlike traditional generative AI, which responds reactively to prompts, agentic AI proactively orchestrates processes, such as autonomously managing complex tasks or making real-time decisions. This transition from advisory roles to proactive execution challenges existing legal, economic, and creative frameworks. In this paper, we highlight gaps in liability attribution, intellectual property ownership, and informed consent, which create a 'moral crumple zone'--a condition where accountability is diffused across multiple actors, leaving end-users and developers in precarious legal and ethical positions. We explore challenges in three domains: creativity, legal and ethical considerations, and economic and competitive effects. Central to our analysis is the tension between novelty and usefulness that arises in the drive to generate creative outputs. Algorithmic coordination among AI systems risks distorting competitive dynamics through tacit collusion or market concentration, particularly if diverse AI systems converge on similar solutions due to shared underlying data or optimization logic. Meanwhile, the potential for emergent self-regulation within networks of agentic AI systems raises critical questions about the alignment of these norms with societal values, the potential for unintended consequences, and the challenges of ensuring transparency and accountability. Addressing these challenges will necessitate interdisciplinary collaboration to redefine legal accountability, align AI-driven choices with stakeholder values, and maintain ethical safeguards. We advocate for frameworks that balance autonomy with accountability, ensuring all parties can harness agentic AI's potential while preserving trust, fairness, and societal welfare.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00289v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Anirban Mukherjee, Hannah Hanwen Chang</dc:creator>
    </item>
    <item>
      <title>Engineering Educators' Perspectives on the Impact of Generative AI in Higher Education</title>
      <link>https://arxiv.org/abs/2502.00569</link>
      <description>arXiv:2502.00569v2 Announce Type: replace 
Abstract: The introduction of generative artificial intelligence (GenAI) has been met with a mix of reactions by higher education institutions, ranging from consternation and resistance to wholehearted acceptance. Previous work has looked at the discourse and policies adopted by universities across the U.S. as well as educators, along with the inclusion of GenAI-related content and topics in higher education. Building on previous research, this study reports findings from a survey of engineering educators on their use of and perspectives toward generative AI. Specifically, we surveyed 98 educators from engineering, computer science, and education who participated in a workshop on GenAI in Engineering Education to learn about their perspectives on using these tools for teaching and research. We asked them about their use of and comfort with GenAI, their overall perspectives on GenAI, the challenges and potential harms of using it for teaching, learning, and research, and examined whether their approach to using and integrating GenAI in their classroom influenced their experiences with GenAI and perceptions of it. Consistent with other research in GenAI education, we found that while the majority of participants were somewhat familiar with GenAI, reported use varied considerably. We found that educators harbored mostly hopeful and positive views about the potential of GenAI. We also found that those who engaged more with their students on the topic of GenAI, tend to be more positive about its contribution to learning, while also being more attuned to its potential abuses. These findings suggest that integrating and engaging with generative AI is essential to foster productive interactions between instructors and students around this technology.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.00569v2</guid>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Umama Dewan, Ashish Hingle, Nora McDonald, Aditya Johri</dc:creator>
    </item>
    <item>
      <title>SoK: A Classification for AI-driven Personalized Privacy Assistants</title>
      <link>https://arxiv.org/abs/2502.07693</link>
      <description>arXiv:2502.07693v2 Announce Type: replace 
Abstract: To help users make privacy-related decisions, personalized privacy assistants based on AI technology have been developed in recent years. These AI-driven Personalized Privacy Assistants (AI-driven PPAs) can reap significant benefits for users, who may otherwise struggle to make decisions regarding their personal data in environments saturated with privacy-related decision requests. However, no study systematically inquired about the features of these AI-driven PPAs, their underlying technologies, or the accuracy of their decisions. To fill this gap, we present a Systematization of Knowledge (SoK) to map the existing solutions found in the scientific literature. We screened 1697 unique research papers over the last decade (2013-2023), constructing a classification from 39 included papers. As a result, this SoK reviews several aspects of existing research on AI-driven PPAs in terms of types of publications, contributions, methodological quality, and other quantitative insights. Furthermore, we provide a comprehensive classification for AI-driven PPAs, delving into their architectural choices, system contexts, types of AI used, data sources, types of decisions, and control over decisions, among other facets. Based on our SoK, we further underline the research gaps and challenges and formulate recommendations for the design and development of AI-driven PPAs as well as avenues for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07693v2</guid>
      <category>cs.CY</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Victor Morel, Leonardo Iwaya, Simone Fischer-H\"ubner</dc:creator>
    </item>
    <item>
      <title>Getting Trapped in Amazon's "Iliad Flow": A Foundation for the Temporal Analysis of Dark Patterns</title>
      <link>https://arxiv.org/abs/2309.09635</link>
      <description>arXiv:2309.09635v2 Announce Type: replace-cross 
Abstract: Dark patterns are ubiquitous in digital systems, impacting users throughout their journeys on many popular apps and websites. While substantial efforts from the research community in the last five years have led to consolidated taxonomies of dark patterns, including an emerging ontology, most applications of these descriptors have been focused on analysis of static images or as isolated pattern types. In this paper, we present a case study of Amazon Prime's "Iliad Flow" to illustrate the interplay of dark patterns across a user journey, grounded in insights from a US Federal Trade Commission complaint against the company. We use this case study to lay the groundwork for a methodology of Temporal Analysis of Dark Patterns (TADP), including considerations for characterization of individual dark patterns across a user journey, combinatorial effects of multiple dark patterns types, and implications for expert detection and automated detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.09635v2</guid>
      <category>cs.HC</category>
      <category>cs.CY</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Colin M. Gray, Thomas Mildner, Ritika Gairola</dc:creator>
    </item>
    <item>
      <title>Evaluating the Performance of ChatGPT for Spam Email Detection</title>
      <link>https://arxiv.org/abs/2402.15537</link>
      <description>arXiv:2402.15537v3 Announce Type: replace-cross 
Abstract: Email continues to be a pivotal and extensively utilized communication medium within professional and commercial domains. Nonetheless, the prevalence of spam emails poses a significant challenge for users, disrupting their daily routines and diminishing productivity. Consequently, accurately identifying and filtering spam based on content has become crucial for cybersecurity. Recent advancements in natural language processing, particularly with large language models like ChatGPT, have shown remarkable performance in tasks such as question answering and text generation. However, its potential in spam identification remains underexplored. To fill in the gap, this study attempts to evaluate ChatGPT's capabilities for spam identification in both English and Chinese email datasets. We employ ChatGPT for spam email detection using in-context learning, which requires a prompt instruction with (or without) a few demonstrations. We also investigate how the number of demonstrations in the prompt affects the performance of ChatGPT. For comparison, we also implement five popular benchmark methods, including naive Bayes, support vector machines (SVM), logistic regression (LR), feedforward dense neural networks (DNN), and BERT classifiers. Through extensive experiments, the performance of ChatGPT is significantly worse than deep supervised learning methods in the large English dataset, while it presents superior performance on the low-resourced Chinese dataset. This study provides insights into the potential and limitations of ChatGPT for spam identification, highlighting its potential as a viable solution for resource-constrained language domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.15537v3</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shijing Si, Yuwei Wu, Le Tang, Yugui Zhang, Jedrek Wosik, Qinliang Su</dc:creator>
    </item>
  </channel>
</rss>
