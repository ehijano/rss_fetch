<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.SE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.SE</link>
    <description>cs.SE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.SE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 13 Feb 2025 05:00:01 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Counterexample Guided Program Repair Using Zero-Shot Learning and MaxSAT-based Fault Localization</title>
      <link>https://arxiv.org/abs/2502.07786</link>
      <description>arXiv:2502.07786v1 Announce Type: new 
Abstract: Automated Program Repair (APR) for introductory programming assignments (IPAs) is motivated by the large number of student enrollments in programming courses each year. Since providing feedback on IPAs requires substantial time and effort from faculty, personalized feedback often involves suggesting fixes to students' programs. Formal Methods (FM)-based semantic repair approaches, check a program's execution against a test suite or reference solution, are effective but limited. These tools excel at identifying buggy parts but can only fix programs if the correct implementation and the faulty one share the same control flow graph. Conversely, Large Language Models (LLMs) are used for APR but often make extensive instead of minimal rewrites. This leads to more invasive fixes, making it harder for students to learn from their mistakes. In summary, LLMs excel at completing strings, while FM-based fault localization excel at identifying buggy parts of a program. In this paper, we propose a novel approach that combines the strengths of both FM-based fault localization and LLMs, via zero-shot learning, to enhance APR for IPAs. Our method uses MaxSAT-based fault localization to identify buggy parts of a program, then presents the LLM with a program sketch devoid of these buggy statements. This hybrid approach follows a CEGIS loop to iteratively refine the program. We ask the LLM to synthesize the missing parts, which are then checked against a test suite. If the suggested program is incorrect, a counterexample from the test suite is fed back to the LLM. Our experiments show that our counterexample guided approach, using MaxSAT-based bug-free program sketches, significantly improves the repair capabilities of all six evaluated LLMs. This method allows LLMs to repair more programs with smaller fixes, outperforming other configurations and state-of-the-art symbolic program repair tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07786v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Pedro Orvalho, Mikol\'a\v{s} Janota, Vasco Manquinho</dc:creator>
    </item>
    <item>
      <title>Analyzing the Resource Utilization of Lambda Functions on Mobile Devices: Case Studies on Kotlin and Swift</title>
      <link>https://arxiv.org/abs/2502.07809</link>
      <description>arXiv:2502.07809v1 Announce Type: new 
Abstract: With billions of smartphones in use globally, the daily time spent on these devices contributes significantly to overall electricity consumption. Given this scale, even minor reductions in smartphone power use could result in substantial energy savings. This study explores the impact of Lambda functions on resource consumption in mobile programming. While Lambda functions are known for enhancing code readability and conciseness, their use does not add to the functional capabilities of a programming language. Our research investigates the implications of using Lambda functions in terms of battery utilization, memory usage, and execution time compared to equivalent code structures without Lambda functions. Our findings reveal that Lambda functions impose a considerable resource overhead on mobile devices without offering additional functionalities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07809v1</guid>
      <category>cs.SE</category>
      <category>cs.CL</category>
      <category>cs.PF</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/publicdomain/zero/1.0/</dc:rights>
      <dc:creator>Chibundom U. Ejimuda, Gaston Longhitano, Reza Rawassizadeh</dc:creator>
    </item>
    <item>
      <title>Bridging LLM-Generated Code and Requirements: Reverse Generation technique and SBC Metric for Developer Insights</title>
      <link>https://arxiv.org/abs/2502.07835</link>
      <description>arXiv:2502.07835v1 Announce Type: new 
Abstract: The rise of Large Language Models (LLMs) in software engineering, particularly in code generation, has garnered significant attention. However, assessing the quality of AI-generated code remains a challenge due to the inherent complexity of programming tasks and the lack of robust evaluation metrics that align well with human judgment. Traditional token-based metrics such as BLEU and ROUGE, while commonly used in natural language processing, exhibit weak correlations with human assessments in code intelligence and verification tasks. Furthermore, these metrics are primarily research focused and are not designed for seamless integration into the software development lifecycle, limiting their practical utility for developers seeking to improve code quality and security.
  AI-assisted coding has been shown to be more beneficial for senior developers, as they possess the expertise to critically evaluate the generated code for correctness, completeness, and compliance. In contrast, junior developers may struggle to identify hallucinations, missing functionality, or incorrect logic in AI-generated code. To bridge this gap, This paper introduces a novel scoring mechanism called the SBC score, which is based on a reverse generation technique that leverages the natural language generation capabilities of LLMs. Unlike direct code analysis, our approach reconstructs system requirements from AI-generated code and compares them with the original specifications to quantify accuracy. The SBC score combines semantic similarity, BLEU, and completeness analysis, providing actionable insights to developers by highlighting missing features and hallucinations. Our code and datasets are available on GitHub</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07835v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Ahilan Ayyachamy Nadar Ponnusamy</dc:creator>
    </item>
    <item>
      <title>Sentiment Analysis Tools in Software Engineering: A Systematic Mapping Study</title>
      <link>https://arxiv.org/abs/2502.07893</link>
      <description>arXiv:2502.07893v1 Announce Type: new 
Abstract: Software development is a collaborative task. Previous research has shown social aspects within development teams to be highly relevant for the success of software projects. A team's mood has been proven to be particularly important. It is paramount for project managers to be aware of negative moods within their teams, as such awareness enables them to intervene. Sentiment analysis tools offer a way to determine the mood of a team based on textual communication. We aim to help developers or stakeholders in their choice of sentiment analysis tools for their specific purpose. Therefore, we conducted a systematic mapping study (SMS). We present the results of our SMS of sentiment analysis tools developed for or applied in the context of software engineering (SE). Our results summarize insights from 106 papers with respect to (1) the application domain, (2) the purpose, (3) the used data sets, (4) the approaches for developing sentiment analysis tools, (5) the usage of already existing tools, and (6) the difficulties researchers face. We analyzed in more detail which tools and approaches perform how in terms of their performance. According to our results, sentiment analysis is frequently applied to open-source software projects, and most approaches are neural networks or support-vector machines. The best performing approach in our analysis is neural networks and the best tool is BERT. Despite the frequent use of sentiment analysis in SE, there are open issues, e.g. regarding the identification of irony or sarcasm, pointing to future research directions. We conducted an SMS to gain an overview of the current state of sentiment analysis in order to help developers or stakeholders in this matter. Our results include interesting findings e.g. on the used tools and their difficulties. We present several suggestions on how to solve these identified problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07893v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.1016/j.infsof.2022.107018</arxiv:DOI>
      <dc:creator>Martin Obaidi, Lukas Nagel, Alexander Specht, Jil Kl\"under</dc:creator>
    </item>
    <item>
      <title>Impostor Phenomenon Among Software Engineers: Investigating Gender Differences and Well-Being</title>
      <link>https://arxiv.org/abs/2502.07914</link>
      <description>arXiv:2502.07914v1 Announce Type: new 
Abstract: Research shows that more than half of software professionals experience the Impostor Phenomenon (IP), with a notably higher prevalence among women compared to men. IP can lead to mental health consequences, such as depression and burnout, which can significantly impact personal well-being and software professionals' productivity. This study investigates how IP manifests among software professionals across intersections of gender with race/ethnicity, marital status, number of children, age, and professional experience. Additionally, it examines the well-being of software professionals experiencing IP, providing insights into the interplay between these factors. We analyzed data collected through a theory-driven survey (n = 624) that used validated psychometric instruments to measure IP and well-being in software engineering professionals. We explored the prevalence of IP in the intersections of interest. Additionally, we applied bootstrapping to characterize well-being within our field and statistically tested whether professionals of different genders suffering from IP have lower well-being. The results show that IP occurs more frequently in women and that the prevalence is particularly high among black women as well as among single and childless women. Furthermore, regardless of gender, software engineering professionals suffering from IP have significantly lower well-being. Our findings indicate that effective IP mitigation strategies are needed to improve the well-being of software professionals. Mitigating IP would have particularly positive effects on the well-being of women, who are more frequently affected by IP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07914v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Paloma Guenes, Rafael Tomaz, Bianca Trinkenreich, Maria Teresa Baldassarre, Margarte-Anne Storey, Marcos Kalinowski</dc:creator>
    </item>
    <item>
      <title>Distributed Approach to Haskell Based Applications Refactoring with LLMs Based Multi-Agent Systems</title>
      <link>https://arxiv.org/abs/2502.07928</link>
      <description>arXiv:2502.07928v1 Announce Type: new 
Abstract: We present a large language models (LLMs) based multi-agent system to automate the refactoring of Haskell codebases. The multi-agent system consists of specialized agents performing tasks such as context analysis, refactoring, validation, and testing. Refactoring improvements are using metrics such as cyclomatic complexity, run-time, and memory allocation. Experimental evaluations conducted on Haskell codebases demonstrate improvements in code quality. Cyclomatic complexity was reduced by 13.64% and 47.06% in the respective codebases. Memory allocation improved by 4.17% and 41.73%, while runtime efficiency increased by up to 50%. These metrics highlight the systems ability to optimize Haskells functional paradigms while maintaining correctness and scalability. Results show reductions in complexity and performance enhancements across codebases. The integration of LLMs based multi-agent system enables precise task execution and inter-agent collaboration, addressing the challenges of refactoring in functional programming. This approach aims to address the challenges of refactoring functional programming languages through distributed and modular systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07928v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shahbaz Siddeeq, Zeeshan Rasheed, Malik Abdul Sami, Mahade Hasan, Muhammad Waseem, Jussi Rasku, Mika Saari, Kai-Kristian Kemell, Pekka Abrahamsson</dc:creator>
    </item>
    <item>
      <title>Embracing Experiential Learning: Hackathons as an Educational Strategy for Shaping Soft Skills in Software Engineering</title>
      <link>https://arxiv.org/abs/2502.07950</link>
      <description>arXiv:2502.07950v1 Announce Type: new 
Abstract: In recent years, Software Engineering (SE) scholars and practitioners have emphasized the importance of integrating soft skills into SE education. However, teaching and learning soft skills are complex, as they cannot be acquired passively through raw knowledge acquisition. On the other hand, hackathons have attracted increasing attention due to their experiential, collaborative, and intensive nature, which certain tasks could be similar to real-world software development. This paper aims to discuss the idea of hackathons as an educational strategy for shaping SE students' soft skills in practice. Initially, we overview the existing literature on soft skills and hackathons in SE education. Then, we report preliminary empirical evidence from a seven-day hybrid hackathon involving 40 students. We assess how the hackathon experience promoted innovative and creative thinking, collaboration and teamwork, and knowledge application among participants through a structured questionnaire designed to evaluate students' self-awareness. Lastly, our findings and new directions are analyzed through the lens of Self-Determination Theory, which offers a psychological lens to understand human behavior. This paper contributes to academia by advocating the potential of hackathons in SE education and proposing concrete plans for future research within SDT. For industry, our discussion has implications around developing soft skills in future SE professionals, thereby enhancing their employability and readiness in the software market.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07950v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Allysson Allex Ara\'ujo, Marcos Kalinowski, Maria Teresa Baldassarre</dc:creator>
    </item>
    <item>
      <title>Bridging HCI and AI Research for the Evaluation of Conversational SE Assistants</title>
      <link>https://arxiv.org/abs/2502.07956</link>
      <description>arXiv:2502.07956v1 Announce Type: new 
Abstract: As Large Language Models (LLMs) are increasingly adopted in software engineering, recently in the form of conversational assistants, ensuring these technologies align with developers' needs is essential. The limitations of traditional human-centered methods for evaluating LLM-based tools at scale raise the need for automatic evaluation. In this paper, we advocate combining insights from human-computer interaction (HCI) and artificial intelligence (AI) research to enable human-centered automatic evaluation of LLM-based conversational SE assistants. We identify requirements for such evaluation and challenges down the road, working towards a framework that ensures these assistants are designed and deployed in line with user needs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07956v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonan Richards, Mairieli Wessel</dc:creator>
    </item>
    <item>
      <title>From Hazard Identification to Controller Design: Proactive and LLM-Supported Safety Engineering for ML-Powered Systems</title>
      <link>https://arxiv.org/abs/2502.07974</link>
      <description>arXiv:2502.07974v1 Announce Type: new 
Abstract: Machine learning (ML) components are increasingly integrated into software products, yet their complexity and inherent uncertainty often lead to unintended and hazardous consequences, both for individuals and society at large. Despite these risks, practitioners seldom adopt proactive approaches to anticipate and mitigate hazards before they occur. Traditional safety engineering approaches, such as Failure Mode and Effects Analysis (FMEA) and System Theoretic Process Analysis (STPA), offer systematic frameworks for early risk identification but are rarely adopted. This position paper advocates for integrating hazard analysis into the development of any ML-powered software product and calls for greater support to make this process accessible to developers. By using large language models (LLMs) to partially automate a modified STPA process with human oversight at critical steps, we expect to address two key challenges: the heavy dependency on highly experienced safety engineering experts, and the time-consuming, labor-intensive nature of traditional hazard analysis, which often impedes its integration into real-world development workflows. We illustrate our approach with a running example, demonstrating that many seemingly unanticipated issues can, in fact, be anticipated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07974v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yining Hong, Christopher S. Timperley, Christian K\"astner</dc:creator>
    </item>
    <item>
      <title>OSSDoorway: A Gamified Environment to Scaffold Student Contributions to Open Source Software</title>
      <link>https://arxiv.org/abs/2502.07986</link>
      <description>arXiv:2502.07986v1 Announce Type: new 
Abstract: Software engineering courses enable practical learning through assignments requiring contributions to open source software (OSS), allowing students to experience real-world projects, collaborate with global communities, and develop skills and competencies required to succeed in the tech industry. Learning software engineering through open source contribution integrates theory with hands-on practice, as students tackle real challenges in collaborative environments. However, students often struggle to contribute to OSS projects and do not understand the contribution process. Research has demonstrated that strategically incorporating game elements can promote student learning and engagement. This paper proposes and evaluates OSSDoorway, a tool designed to guide students contributing to OSS projects. We recruited 29 students and administered a self-efficacy questionnaire before and after their use of OSSDoorway, along with qualitative feedback to assess challenges, interface features, and suggestions for improvement. The results show that OSSDoorway boosts students' self-efficacy and provides a structured, gamified learning experience. Clear instructions, real-time feedback, and the quest-based system helped students navigate tasks like using GitHub features to submit pull requests and collaborating with the community. Our findings suggest that providing students with a supportive gamified environment that uses feedback and structured quests can help them navigate the OSS contribution process.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07986v1</guid>
      <category>cs.SE</category>
      <category>cs.HC</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:journal_reference>2025 IEEE/ACM 37th International Conference on Software Engineering Education and Training (CSEE&amp;T)</arxiv:journal_reference>
      <dc:creator>Italo Santos, Katia Romero Felizardo, Anita Sarma, Igor Steinmacher, Marco A. Gerosa</dc:creator>
    </item>
    <item>
      <title>Can Machine Learning Support the Selection of Studies for Systematic Literature Review Updates?</title>
      <link>https://arxiv.org/abs/2502.08050</link>
      <description>arXiv:2502.08050v1 Announce Type: new 
Abstract: [Background] Systematic literature reviews (SLRs) are essential for synthesizing evidence in Software Engineering (SE), but keeping them up-to-date requires substantial effort. Study selection, one of the most labor-intensive steps, involves reviewing numerous studies and requires multiple reviewers to minimize bias and avoid loss of evidence. [Objective] This study aims to evaluate if Machine Learning (ML) text classification models can support reviewers in the study selection for SLR updates. [Method] We reproduce the study selection of an SLR update performed by three SE researchers. We trained two supervised ML models (Random Forest and Support Vector Machines) with different configurations using data from the original SLR. We calculated the study selection effectiveness of the ML models for the SLR update in terms of precision, recall, and F-measure. We also compared the performance of human-ML pairs with human-only pairs when selecting studies. [Results] The ML models achieved a modest F-score of 0.33, which is insufficient for reliable automation. However, we found that such models can reduce the study selection effort by 33.9% without loss of evidence (keeping a 100% recall). Our analysis also showed that the initial screening by pairs of human reviewers produces results that are much better aligned with the final SLR update result. [Conclusion] Based on our results, we conclude that although ML models can help reduce the effort involved in SLR updates, achieving rigorous and reliable outcomes still requires the expertise of experienced human reviewers for the initial screening phase.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08050v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Marcelo Costalonga, Bianca Minetto Napole\~ao, Maria Teresa Baldassarre, Katia Romero Felizardo, Igor Steinmacher, Marcos Kalinowski</dc:creator>
    </item>
    <item>
      <title>Generative AI and Empirical Software Engineering: A Paradigm Shift</title>
      <link>https://arxiv.org/abs/2502.08108</link>
      <description>arXiv:2502.08108v1 Announce Type: new 
Abstract: The widespread adoption of generative AI in software engineering marks a paradigm shift, offering new opportunities to design and utilize software engineering tools while influencing both developers and the artifacts they create. Traditional empirical methods in software engineering, including quantitative, qualitative, and mixed-method approaches, are well established. However, this paradigm shift introduces novel data types and redefines many concepts in the software engineering process. The roles of developers, users, agents, and researchers increasingly overlap, blurring the distinctions between these social and technical actors within the field.
  This paper examines how integrating AI into software engineering challenges traditional research paradigms. It focuses on the research phenomena that we investigate, the methods and theories that we employ, the data we analyze, and the threats to validity that emerge in this new context. Through this exploration, our goal is to understand how AI adoption disrupts established software development practices that creates new opportunities for empirical software engineering research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08108v1</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Christoph Treude, Margaret-Anne Storey</dc:creator>
    </item>
    <item>
      <title>The extended next release problem. Generic Formulation of the Requirements Selection Problem</title>
      <link>https://arxiv.org/abs/2502.08139</link>
      <description>arXiv:2502.08139v1 Announce Type: new 
Abstract: Due to the limited amount of resources available for the next release of the current product under development not all stakeholders requests can be included in the next product to deliver. This optimization problem, known as the Next Release Problem (NRP), has customers satisfaction and development costs as the basic optimization objectives, and has been the subject of many research works. However, there are additional issues that deserve to be considered and included in the definition of the NRP, such as supplementary optimization objectives including the elicited properties about the requirements, or the analysis of the non-dominated solution sets found to decide which solution is preferred. This paper presents a generic formulation for this problem that allows the management of the currently agreed properties and relationships between requirements. It provides an open formulation that is capable of growing as the scope of the NRP grows. We specify how our proposal can be used in software product projects when requirements selection has to be performed. We also describe how our formulation has been instantiated to cover previous solving approaches to this problem, using six case studies to demonstrate the successful customization of the generic formulation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08139v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Isabel del Aguila, Jose del Sagrado, Alfonso Bosch</dc:creator>
    </item>
    <item>
      <title>Intention is All You Need: Refining Your Code from Your Intention</title>
      <link>https://arxiv.org/abs/2502.08172</link>
      <description>arXiv:2502.08172v1 Announce Type: new 
Abstract: Code refinement aims to enhance existing code by addressing issues, refactoring, and optimizing to improve quality and meet specific requirements. As software projects scale in size and complexity, the traditional iterative exchange between reviewers and developers becomes increasingly burdensome. While recent deep learning techniques have been explored to accelerate this process, their performance remains limited, primarily due to challenges in accurately understanding reviewers' intents.
  This paper proposes an intention-based code refinement technique that enhances the conventional comment-to-code process by explicitly extracting reviewer intentions from the comments. Our approach consists of two key phases: Intention Extraction and Intention Guided Revision Generation. Intention Extraction categorizes comments using predefined templates, while Intention Guided Revision Generation employs large language models (LLMs) to generate revised code based on these defined intentions. Three categories with eight subcategories are designed for comment transformation, which is followed by a hybrid approach that combines rule-based and LLM-based classifiers for accurate classification. Extensive experiments with five LLMs (GPT4o, GPT3.5, DeepSeekV2, DeepSeek7B, CodeQwen7B) under different prompting settings demonstrate that our approach achieves 79% accuracy in intention extraction and up to 66% in code refinement generation. Our results highlight the potential of our approach in enhancing data quality and improving the efficiency of code refinement.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08172v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Qi Guo, Xiaofei Xie, Shangqing Liu, Ming Hu, Xiaohong Li, Lei Bu</dc:creator>
    </item>
    <item>
      <title>Tracking Down Software Cluster Bombs: A Current State Analysis of the Free/Libre and Open Source Software (FLOSS) Ecosystem</title>
      <link>https://arxiv.org/abs/2502.08219</link>
      <description>arXiv:2502.08219v1 Announce Type: new 
Abstract: Throughout computer history, it has been repeatedly demonstrated that critical software vulnerabilities can significantly affect the components involved. In the Free/Libre and Open Source Software (FLOSS) ecosystem, most software is distributed through package repositories. Nowadays, monitoring critical dependencies in a software system is essential for maintaining robust security practices. This is particularly important due to new legal requirements, such as the European Cyber Resilience Act, which necessitate that software projects maintain a transparent track record with Software Bill of Materials (SBOM) and ensure a good overall state. This study provides a summary of the current state of available FLOSS package repositories and addresses the challenge of identifying problematic areas within a software ecosystem. These areas are analyzed in detail, quantifying the current state of the FLOSS ecosystem. The results indicate that while there are well-maintained projects within the FLOSS ecosystem, there are also high-impact projects that are susceptible to supply chain attacks. This study proposes a method for analyzing the current state and identifies missing elements, such as interfaces, for future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08219v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Stefan Tatschner (Fraunhofer AISEC, Garching bei M\"unchen, Bavaria, Germany, University of Limerick, Limerick, Ireland), Michael P. Heinl (Fraunhofer AISEC, Garching bei M\"unchen, Bavaria, Germany, Technical University of Munich, Garching bei M\"unchen, Bavaria, Germany, Munich University of Applied Sciences HM, Munich, Bavaria, Germany), Nicole Pappler (University of Limerick, Limerick, Ireland), Tobias Specht (Fraunhofer AISEC, Garching bei M\"unchen, Bavaria, Germany), Sven Plaga (Center for Intelligence and Security Studies), Thomas Newe (University of Limerick, Limerick, Ireland)</dc:creator>
    </item>
    <item>
      <title>Flow-of-Action: SOP Enhanced LLM-Based Multi-Agent System for Root Cause Analysis</title>
      <link>https://arxiv.org/abs/2502.08224</link>
      <description>arXiv:2502.08224v1 Announce Type: new 
Abstract: In the realm of microservices architecture, the occurrence of frequent incidents necessitates the employment of Root Cause Analysis (RCA) for swift issue resolution. It is common that a serious incident can take several domain experts hours to identify the root cause. Consequently, a contemporary trend involves harnessing Large Language Models (LLMs) as automated agents for RCA. Though the recent ReAct framework aligns well with the Site Reliability Engineers (SREs) for its thought-action-observation paradigm, its hallucinations often lead to irrelevant actions and directly affect subsequent results. Additionally, the complex and variable clues of the incident can overwhelm the model one step further. To confront these challenges, we propose Flow-of-Action, a pioneering Standard Operation Procedure (SOP) enhanced LLM-based multi-agent system. By explicitly summarizing the diagnosis steps of SREs, SOP imposes constraints on LLMs at crucial junctures, guiding the RCA process towards the correct trajectory. To facilitate the rational and effective utilization of SOPs, we design an SOP-centric framework called SOP flow. SOP flow contains a series of tools, including one for finding relevant SOPs for incidents, another for automatically generating SOPs for incidents without relevant ones, and a tool for converting SOPs into code. This significantly alleviates the hallucination issues of ReAct in RCA tasks. We also design multiple auxiliary agents to assist the main agent by removing useless noise, narrowing the search space, and informing the main agent whether the RCA procedure can stop. Compared to the ReAct method's 35.50% accuracy, our Flow-of-Action method achieves 64.01%, meeting the accuracy requirements for RCA in real-world systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08224v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Changhua Pei, Zexin Wang, Fengrui Liu, Zeyan Li, Yang Liu, Xiao He, Rong Kang, Tieying Zhang, Jianjun Chen, Jianhui Li, Gaogang Xie, Dan Pei</dc:creator>
    </item>
    <item>
      <title>The Landscape of Toxicity: An Empirical Investigation of Toxicity on GitHub</title>
      <link>https://arxiv.org/abs/2502.08238</link>
      <description>arXiv:2502.08238v1 Announce Type: new 
Abstract: Toxicity on GitHub can severely impact Open Source Software (OSS) development communities. To mitigate such behavior, a better understanding of its nature and how various measurable characteristics of project contexts and participants are associated with its prevalence is necessary. To achieve this goal, we conducted a large-scale mixed-method empirical study of 2,828 GitHub-based OSS projects randomly selected based on a stratified sampling strategy. Using ToxiCR, an SE domain-specific toxicity detector, we automatically classified each comment as toxic or non-toxic. Additionally, we manually analyzed a random sample of 600 comments to validate ToxiCR's performance and gain insights into the nature of toxicity within our dataset. The results of our study suggest that profanity is the most frequent toxicity on GitHub, followed by trolling and insults. While a project's popularity is positively associated with the prevalence of toxicity, its issue resolution rate has the opposite association. Corporate-sponsored projects are less toxic, but gaming projects are seven times more toxic than non-gaming ones. OSS contributors who have authored toxic comments in the past are significantly more likely to repeat such behavior. Moreover, such individuals are more likely to become targets of toxic texts.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08238v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jaydeb Sarker, Asif Kamal Turzo, Amiangshu Bosu</dc:creator>
    </item>
    <item>
      <title>FixDrive: Automatically Repairing Autonomous Vehicle Driving Behaviour for $0.08 per Violation</title>
      <link>https://arxiv.org/abs/2502.08260</link>
      <description>arXiv:2502.08260v1 Announce Type: new 
Abstract: Autonomous Vehicles (AVs) are advancing rapidly, with Level-4 AVs already operating in real-world conditions. Current AVs, however, still lag behind human drivers in adaptability and performance, often exhibiting overly conservative behaviours and occasionally violating traffic laws. Existing solutions, such as runtime enforcement, mitigate this by automatically repairing the AV's planned trajectory at runtime, but such approaches lack transparency and should be a measure of last resort. It would be preferable for AV repairs to generalise beyond specific incidents and to be interpretable for users. In this work, we propose FixDrive, a framework that analyses driving records from near-misses or law violations to generate AV driving strategy repairs that reduce the chance of such incidents occurring again. These repairs are captured in {\mu}Drive, a high-level domain-specific language for specifying driving behaviours in response to event-based triggers. Implemented for the state-of-the-art autonomous driving system Apollo, FixDrive identifies and visualises critical moments from driving records, then uses a Multimodal Large Language Model (MLLM) with zero-shot learning to generate {\mu}Drive programs. We tested FixDrive on various benchmark scenarios, and found that the generated repairs improved the AV's performance with respect to following traffic laws, avoiding collisions, and successfully reaching destinations. Furthermore, the direct costs of repairing an AV -- 15 minutes of offline analysis and $0.08 per violation -- are reasonable in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08260v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Sun, Christopher M. Poskitt, Kun Wang, Jun Sun</dc:creator>
    </item>
    <item>
      <title>SoK: Where to Fuzz? Assessing Target Selection Methods in Directed Fuzzing</title>
      <link>https://arxiv.org/abs/2502.08341</link>
      <description>arXiv:2502.08341v1 Announce Type: new 
Abstract: A common paradigm for improving fuzzing performance is to focus on selected regions of a program rather than its entirety. While previous work has largely explored how these locations can be reached, their selection, that is, the where, has received little attention so far. A common paradigm for improving fuzzing performance is to focus on selected regions of a program rather than its entirety. While previous work has largely explored how these locations can be reached, their selection, that is, the where, has received little attention so far. In this paper, we fill this gap and present the first comprehensive analysis of target selection methods for fuzzing. To this end, we examine papers from leading security and software engineering conferences, identifying prevalent methods for choosing targets. By modeling these methods as general scoring functions, we are able to compare and measure their efficacy on a corpus of more than 1,600 crashes from the OSS-Fuzz project. Our analysis provides new insights for target selection in practice: First, we find that simple software metrics significantly outperform other methods, including common heuristics used in directed fuzzing, such as recently modified code or locations with sanitizer instrumentation. Next to this, we identify language models as a promising choice for target selection. In summary, our work offers a new perspective on directed fuzzing, emphasizing the role of target selection as an orthogonal dimension to improve performance.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08341v1</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3634737.3661141</arxiv:DOI>
      <arxiv:journal_reference>Proceedings of the 19th ACM Asia Conference on Computer and Communications Security (2024) 1539-1553</arxiv:journal_reference>
      <dc:creator>Felix Weissberg, Jonas M\"oller, Tom Ganz, Erik Imgrund, Lukas Pirch, Lukas Seidel, Moritz Schloegel, Thorsten Eisenhofer, Konrad Rieck</dc:creator>
    </item>
    <item>
      <title>MoDitector: Module-Directed Testing for Autonomous Driving Systems</title>
      <link>https://arxiv.org/abs/2502.08504</link>
      <description>arXiv:2502.08504v1 Announce Type: new 
Abstract: Testing Autonomous Driving Systems (ADS) is crucial for ensuring their safety, reliability, and performance. Despite numerous testing methods available that can generate diverse and challenging scenarios to uncover potential vulnerabilities, these methods often treat ADS as a black-box, primarily focusing on identifying system failures like collisions or near-misses without pinpointing the specific modules responsible for these failures. Understanding the root causes of failures is essential for effective debugging and subsequent system repair. We observed that existing methods also fall short in generating diverse failures that adequately test the distinct modules of an ADS, such as perception, prediction, planning and control. To bridge this gap, we introduce MoDitector, the first root-cause-aware testing method for ADS. Unlike previous approaches, MoDitector not only generates scenarios leading to collisions but also showing which specific module triggered the failure. This method targets specific modules, creating test scenarios that highlight the weaknesses of these given modules. Specifically, our approach involves designing module-specific oracles to ascertain module failures and employs a module-directed testing strategy that includes module-specific feedback, adaptive seed selection, and mutation. This strategy guides the generation of tests that effectively provoke module-specific failures. We evaluated MoDitector across four critical ADS modules and four testing scenarios. Our approach represents a significant innovation in ADS testing by focusing on identifying and rectifying module-specific errors within the system, moving beyond conventional black-box failure detection.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08504v1</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Renzhi Wang, Mingfei Cheng, Xiaofei Xie, Yuan Zhou, Lei Ma</dc:creator>
    </item>
    <item>
      <title>Improving Existing Optimization Algorithms with LLMs</title>
      <link>https://arxiv.org/abs/2502.08298</link>
      <description>arXiv:2502.08298v1 Announce Type: cross 
Abstract: The integration of Large Language Models (LLMs) into optimization has created a powerful synergy, opening exciting research opportunities. This paper investigates how LLMs can enhance existing optimization algorithms. Using their pre-trained knowledge, we demonstrate their ability to propose innovative heuristic variations and implementation strategies. To evaluate this, we applied a non-trivial optimization algorithm, Construct, Merge, Solve and Adapt (CMSA) -- a hybrid metaheuristic for combinatorial optimization problems that incorporates a heuristic in the solution construction phase. Our results show that an alternative heuristic proposed by GPT-4o outperforms the expert-designed heuristic of CMSA, with the performance gap widening on larger and denser graphs. Project URL: https://imp-opt-algo-llms.surge.sh/</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.08298v1</guid>
      <category>cs.AI</category>
      <category>cs.CL</category>
      <category>cs.LG</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Camilo Chac\'on Sartori, Christian Blum</dc:creator>
    </item>
    <item>
      <title>SCOPE: Performance Testing for Serverless Computing</title>
      <link>https://arxiv.org/abs/2306.01620</link>
      <description>arXiv:2306.01620v2 Announce Type: replace 
Abstract: Serverless computing is a popular cloud computing paradigm that has found widespread adoption across various online workloads. It allows software engineers to develop cloud applications as a set of functions (called serverless functions). However, accurately measuring the performance (i.e., end-to-end response latency) of serverless functions is challenging due to the highly dynamic nature of the environment in which they run. To tackle this problem, a potential solution is to apply checks of performance testing techniques to determine how many repetitions of a given serverless function across a range of inputs are needed to cater to the performance fluctuation. However, the available literature lacks performance testing approaches designed explicitly for serverless computing. In this paper, we propose SCOPE, the first serverless computing-oriented performance testing approach. SCOPE takes into account the unique performance characteristics of serverless functions, such as their short execution durations and on-demand triggering. As such, SCOPE is designed as a fine-grained analysis approach. SCOPE incorporates the accuracy check and the consistency check to obtain the accurate and reliable performance of serverless functions. The evaluation shows that SCOPE provides testing results with 97.25% accuracy, 33.83 percentage points higher than the best currently available technique. Moreover, the superiority of SCOPE over the state-of-the-art holds on all functions that we study.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.01620v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jinfeng Wen, Zhenpeng Chen, Jianshu Zhao, Federica Sarro, Haodi Ping, Ying Zhang, Shangguang Wang, Xuanzhe Liu</dc:creator>
    </item>
    <item>
      <title>Do Large Code Models Understand Programming Concepts? Counterfactual Analysis for Code Predicates</title>
      <link>https://arxiv.org/abs/2402.05980</link>
      <description>arXiv:2402.05980v3 Announce Type: replace 
Abstract: Large Language Models' success on text generation has also made them better at code generation and coding tasks. While a lot of work has demonstrated their remarkable performance on tasks such as code completion and editing, it is still unclear as to why. We help bridge this gap by exploring to what degree auto-regressive models understand the logical constructs of the underlying programs. We propose Counterfactual Analysis for Programming Concept Predicates (CACP) as a counterfactual testing framework to evaluate whether Large Code Models understand programming concepts. With only black-box access to the model, we use CACP to evaluate ten popular Large Code Models for four different programming concepts. Our findings suggest that current models lack understanding of concepts such as data flow and control flow.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.05980v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>cs.PL</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ashish Hooda, Mihai Christodorescu, Miltiadis Allamanis, Aaron Wilson, Kassem Fawaz, Somesh Jha</dc:creator>
    </item>
    <item>
      <title>COAST: Enhancing the Code Debugging Ability of LLMs through Communicative Agent Based Data Synthesis</title>
      <link>https://arxiv.org/abs/2408.05006</link>
      <description>arXiv:2408.05006v3 Announce Type: replace 
Abstract: Code debugging is a vital stage of software development, essential for ensuring the reliability and performance of Large Language Models (LLMs) in the code generation task. Human debugging typically follows a multi-stage process, which includes Bug Localization, Bug Identification, Code Repair, and Code Recognition. However, existing code debugging benchmarks predominantly focus on the Code Repair stage, which offers only a limited perspective on evaluating the debugging capabilities of LLMs. In this paper, we introduce DEBUGEVAL, a comprehensive benchmark for evaluating the debugging abilities of LLMs by emulating the multi-stage human debugging process. Through evaluating on DEBUGEVAL, we observe that 7B-scale models consistently underperform compared to their larger counterparts, highlighting their limitations in comprehending code semantics. In this case, we propose the COmmunicative Agent-based data SynThesis (COAST) framework, which employs a multi-agent system to generate high-quality training data for supervised fine-tuning (SFT). Experimental results demonstrate that COAST-generated data outperform human-curated and GPT-4-generated data, enabling 7B-scale LLMs to achieve debugging performance comparable to GPT-3.5. All data and codes are available at https://github.com/NEUIR/COAST.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05006v3</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weiqing Yang, Hanbin Wang, Zhenghao Liu, Xinze Li, Yukun Yan, Shuo Wang, Yu Gu, Minghe Yu, Zhiyuan Liu, Ge Yu</dc:creator>
    </item>
    <item>
      <title>Dockerfile Flakiness: Characterization and Repair</title>
      <link>https://arxiv.org/abs/2408.05379</link>
      <description>arXiv:2408.05379v2 Announce Type: replace 
Abstract: Dockerfile flakiness-unpredictable temporal build failures caused by external dependencies and evolving environments-undermines deployment reliability and increases debugging overhead. Unlike traditional Dockerfile issues, flakiness occurs without modifications to the Dockerfile itself, complicating its resolution. In this work, we present the first comprehensive study of Dockerfile flakiness, featuring a nine-month analysis of 8,132 Dockerized projects, revealing that around 10% exhibit flaky behavior. We propose a taxonomy categorizing common flakiness causes, including dependency errors and server connectivity issues. Existing tools fail to effectively address these challenges due to their reliance on pre-defined rules and limited generalizability. To overcome these limitations, we introduce FLAKIDOCK, a novel repair framework combining static and dynamic analysis, similarity retrieval, and an iterative feedback loop powered by Large Language Models (LLMs). Our evaluation demonstrates that FLAKIDOCK achieves a repair accuracy of 73.55%, significantly surpassing state-of-the-art tools and baselines.</description>
      <guid isPermaLink="false">oai:arXiv.org:2408.05379v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Taha Shabani, Noor Nashid, Parsa Alian, Ali Mesbah</dc:creator>
    </item>
    <item>
      <title>An Exploratory Study on the Engineering of Security Features</title>
      <link>https://arxiv.org/abs/2501.11546</link>
      <description>arXiv:2501.11546v2 Announce Type: replace 
Abstract: Software security is of utmost importance for most software systems. Developers must systematically select, plan, design, implement, and especially, maintain and evolve security features -- functionalities to mitigate attacks or protect personal data such as cryptography or access control -- to ensure the security of their software. Although security features are usually available in libraries, integrating security features requires writing and maintaining additional security-critical code. While there have been studies on the use of such libraries, surprisingly little is known about how developers engineer security features, how they select what security features to implement and which ones may require custom implementation, and the implications for maintenance. As a result, we currently rely on assumptions that are largely based on common sense or individual examples. However, to provide them with effective solutions, researchers need hard empirical data to understand what practitioners need and how they view security -- data that we currently lack. To fill this gap, we contribute an exploratory study with 26 knowledgeable industrial participants. We study how security features of software systems are selected and engineered in practice, what their code-level characteristics are, and what challenges practitioners face. Based on the empirical data gathered, we provide insights into engineering practices and validate four common assumptions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.11546v2</guid>
      <category>cs.SE</category>
      <category>cs.CR</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Kevin Hermann, Sven Peldszus, Jan-Philipp Stegh\"ofer, Thorsten Berger</dc:creator>
    </item>
    <item>
      <title>Boosting Path-Sensitive Value Flow Analysis via Removal of Redundant Summaries</title>
      <link>https://arxiv.org/abs/2502.04952</link>
      <description>arXiv:2502.04952v3 Announce Type: replace 
Abstract: Value flow analysis that tracks the flow of values via data dependence is a widely used technique for detecting a broad spectrum of software bugs. However, the scalability issue often deteriorates when high precision (i.e., path-sensitivity) is required, as the instantiation of function summaries becomes excessively time- and memory-intensive. The primary culprit, as we observe, is the existence of redundant computations resulting from blindly computing summaries for a function, irrespective of whether they are related to bugs being checked. To address this problem, we present the first approach that can effectively identify and eliminate redundant summaries, thereby reducing the size of collected summaries from callee functions without compromising soundness or efficiency. Our evaluation on large programs demonstrates that our identification algorithm can significantly reduce the time and memory overhead of the state-of-the-art value flow analysis by 45\% and 27\%, respectively. Furthermore, the identification algorithm demonstrates remarkable efficiency by identifying nearly 80\% of redundant summaries while incurring a minimal additional overhead. In the largest \textit{mysqld} project, the identification algorithm reduces the time by 8107 seconds (2.25 hours) with a mere 17.31 seconds of additional overhead, leading to a ratio of time savings to paid overhead (i.e., performance gain) of 468.48 $\times$. In total, our method attains an average performance gain of 632.1 $\times$.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.04952v3</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yongchao Wang, Yuandao Cai, Charles Zhang</dc:creator>
    </item>
    <item>
      <title>CSR-Bench: Benchmarking LLM Agents in Deployment of Computer Science Research Repositories</title>
      <link>https://arxiv.org/abs/2502.06111</link>
      <description>arXiv:2502.06111v2 Announce Type: replace 
Abstract: The increasing complexity of computer science research projects demands more effective tools for deploying code repositories. Large Language Models (LLMs), such as Anthropic Claude and Meta Llama, have demonstrated significant advancements across various fields of computer science research, including the automation of diverse software engineering tasks. To evaluate the effectiveness of LLMs in handling complex code development tasks of research projects, particularly for NLP/CV/AI/ML/DM topics, we introduce CSR-Bench, a benchmark for Computer Science Research projects. This benchmark assesses LLMs from various aspects including accuracy, efficiency, and deployment script quality, aiming to explore their potential in conducting computer science research autonomously. We also introduce a novel framework, CSR-Agents, that utilizes multiple LLM agents to automate the deployment of GitHub code repositories of computer science research projects. Specifically, by checking instructions from markdown files and interpreting repository structures, the model generates and iteratively improves bash commands that set up the experimental environments and deploy the code to conduct research tasks. Preliminary results from CSR-Bench indicate that LLM agents can significantly enhance the workflow of repository deployment, thereby boosting developer productivity and improving the management of developmental workflows.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.06111v2</guid>
      <category>cs.SE</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yijia Xiao, Runhui Wang, Luyang Kong, Davor Golac, Wei Wang</dc:creator>
    </item>
    <item>
      <title>On Categorizing Open Source Software Security Vulnerability Reporting Mechanisms on GitHub</title>
      <link>https://arxiv.org/abs/2502.07395</link>
      <description>arXiv:2502.07395v2 Announce Type: replace 
Abstract: Open-source projects are essential to software development, but publicly disclosing vulnerabilities without fixes increases the risk of exploitation. The Open Source Security Foundation (OpenSSF) addresses this issue by promoting robust security policies to enhance project security. Current research reveals that many projects perform poorly on OpenSSF criteria, indicating a need for stronger security practices and underscoring the value of SECURITY$.$md files for structured vulnerability reporting. This study aims to provide recommendations for improving security policies. By examining 679 open-source projects, we find that email is still the main source of reporting. Furthermore, we find that projects without SECURITY$.$md files tend to be less secure (lower OpenSSF scores). Our analysis also indicates that, although many maintainers encourage private reporting methods, some contributors continue to disclose vulnerabilities publicly, bypassing established protocols. The results from this preliminary study pave the way for understanding how developers react and communicate a potential security threat. Future challenges include understanding the impact and effectiveness of these mechanisms and what factors may influence how the security threat is addressed.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07395v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sushawapak Kancharoendee, Thanat Phichitphanphong, Chanikarn Jongyingyos, Brittany Reid, Raula Gaikovina Kula, Morakot Choetkiertikul, Chaiyong Ragkhitwetsagul, Thanwadee Sunetnanta</dc:creator>
    </item>
    <item>
      <title>OpenCat: Improving Interoperability of ADS Testing</title>
      <link>https://arxiv.org/abs/2502.07719</link>
      <description>arXiv:2502.07719v2 Announce Type: replace 
Abstract: Testing Advanced Driving Assistance Systems (ADAS), such as lane-keeping functions, requires creating road topologies or using predefined benchmarks. However, the test cases in existing ADAS benchmarks are often designed in specific formats (e.g., OpenDRIVE) and tailored to specific ADAS models. This limits their reusability and interoperability with other simulators and models, making it challenging to assess ADAS functionalities independently of the platform-specific details used to create the test cases. This paper evaluates the interoperability of SensoDat, a benchmark developed for ADAS regression testing. We introduce OpenCat, a converter that transforms OpenDRIVE test cases into the Catmull-Rom spline format, which is widely supported by many current test generators. By applying OpenCat to the SensoDat dataset, we achieved high accuracy in converting test cases into reusable road scenarios. To validate the converted scenarios, we used them to evaluate a lane-keeping ADAS model using the Udacity simulator. Both the simulator and the ADAS model operate independently of the technologies underlying SensoDat, ensuring an unbiased evaluation of the original test cases. Our findings reveal that benchmarks built with specific ADAS models hinder their effective usage for regression testing. We conclude by offering insights and recommendations to enhance the reusability and transferability of ADAS benchmarks for more extensive applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07719v2</guid>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Qurban Ali, Andrea Stocco, Leonardo Mariani, Oliviero Riganelli</dc:creator>
    </item>
    <item>
      <title>SAND: Decoupling Sanitization from Fuzzing for Low Overhead</title>
      <link>https://arxiv.org/abs/2402.16497</link>
      <description>arXiv:2402.16497v3 Announce Type: replace-cross 
Abstract: Sanitizers provide robust test oracles for various software vulnerabilities. Fuzzing on sanitizer-enabled programs has been the best practice to find software bugs. Since sanitizers need to heavily instrument a target program to insert run-time checks, sanitizer-enabled programs have much higher overhead compared to normally built programs. In this paper, we present SAND, a new fuzzing framework that decouples sanitization from the fuzzing loop. SAND performs fuzzing on a normally built program and only invokes sanitizer-enabled programs when input is shown to be interesting. Since most of the generated inputs are not interesting, i.e., not bug-triggering, SAND allows most of the fuzzing time to be spent on the normally built program. To identify interesting inputs, we introduce execution pattern for a practical execution analysis on the normally built program. We realize SAND on top of AFL++ and evaluate it on 12 real-world programs. Our extensive evaluation highlights its effectiveness: in 24 hours, compared to all the baseline fuzzers, SAND significantly discovers more bugs while not missing any.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.16497v3</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:journal_reference>ICSE 2025</arxiv:journal_reference>
      <dc:creator>Ziqiao Kong, Shaohua Li, Heqing Huang, Zhendong Su</dc:creator>
    </item>
    <item>
      <title>Smart Contract Fuzzing Towards Profitable Vulnerabilities</title>
      <link>https://arxiv.org/abs/2501.08834</link>
      <description>arXiv:2501.08834v2 Announce Type: replace-cross 
Abstract: Billions of dollars are transacted through smart contracts, making vulnerabilities a major financial risk. One focus in the security arms race is on profitable vulnerabilities that attackers can exploit. Fuzzing is a key method for identifying these vulnerabilities. However, current solutions face two main limitations: a lack of profit-centric techniques for expediting detection, and insufficient automation in maximizing the profitability of discovered vulnerabilities, leaving the analysis to human experts. To address these gaps, we have developed VERITE, a profit-centric smart contract fuzzing framework that not only effectively detects those profitable vulnerabilities but also maximizes the exploited profits.
  VERITE has three key features: 1) DeFi action-based mutators for boosting the exploration of transactions with different fund flows; 2) potentially profitable candidates identification criteria, which checks whether the input has caused abnormal fund flow properties during testing; 3) a gradient descent-based profit maximization strategy for these identified candidates.
  VERITE is fully developed from scratch and evaluated on a dataset consisting of 61 exploited real-world DeFi projects with an average of over 1.1 million dollars loss. The results show that VERITE can automatically extract more than 18 million dollars in total and is significantly better than state-of-the-art fuzzer ITYFUZZ in both detection (29/10) and exploitation (134 times more profits gained on average). Remarkably, in 12 targets, it gains more profits than real-world attacking exploits (1.01 to 11.45 times more). VERITE is also applied by auditors in contract auditing, where 6 (5 high severity) zero-day vulnerabilities are found with over $2,500 bounty rewards.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.08834v2</guid>
      <category>cs.CR</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1145/3715720</arxiv:DOI>
      <arxiv:journal_reference>FSE 2025</arxiv:journal_reference>
      <dc:creator>Ziqiao Kong, Cen Zhang, Maoyi Xie, Ming Hu, Yue Xue, Ye Liu, Haijun Wang, Yang Liu</dc:creator>
    </item>
    <item>
      <title>Towards Transparent and Accurate Diabetes Prediction Using Machine Learning and Explainable Artificial Intelligence</title>
      <link>https://arxiv.org/abs/2501.18071</link>
      <description>arXiv:2501.18071v2 Announce Type: replace-cross 
Abstract: Diabetes mellitus (DM) is a global health issue of significance that must be diagnosed as early as possible and managed well. This study presents a framework for diabetes prediction using Machine Learning (ML) models, complemented with eXplainable Artificial Intelligence (XAI) tools, to investigate both the predictive accuracy and interpretability of the predictions from ML models. Data Preprocessing is based on the Synthetic Minority Oversampling Technique (SMOTE) and feature scaling used on the Diabetes Binary Health Indicators dataset to deal with class imbalance and variability of clinical features. The ensemble model provided high accuracy, with a test accuracy of 92.50% and an ROC-AUC of 0.975. BMI, Age, General Health, Income, and Physical Activity were the most influential predictors obtained from the model explanations. The results of this study suggest that ML combined with XAI is a promising means of developing accurate and computationally transparent tools for use in healthcare systems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.18071v2</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Pir Bakhsh Khokhar, Viviana Pentangelo, Fabio Palomba, Carmine Gravino</dc:creator>
    </item>
    <item>
      <title>IRepair: An Intent-Aware Approach to Repair Data-Driven Errors in Large Language Models</title>
      <link>https://arxiv.org/abs/2502.07072</link>
      <description>arXiv:2502.07072v2 Announce Type: replace-cross 
Abstract: Not a day goes by without hearing about the impressive feats of large language models (LLMs), and equally, not a day passes without hearing about their challenges. LLMs are notoriously vulnerable to biases in their dataset, leading to issues such as toxicity. While domain-adaptive training has been employed to mitigate these issues, these techniques often address all model parameters indiscriminately during the repair process, resulting in poor repair quality and reduced model versatility. In this paper, we introduce a novel dynamic slicing-based intent-aware LLM repair strategy, IRepair. This approach selectively targets the most error-prone sections of the model for repair. Specifically, we propose dynamically slicing the model's most sensitive layers that require immediate attention, concentrating repair efforts on those areas. This method enables more effective repairs with potentially less impact on the model's overall performance by altering a smaller portion of the model. We evaluated our technique on three models from the GPT2 and GPT-Neo families, with parameters ranging from 800M to 1.6B, in a toxicity mitigation setup. Our results show that IRepair repairs errors 43.6% more effectively while causing 46% less disruption to general performance compared to the closest baseline, direct preference optimization. Our empirical analysis also reveals that errors are more concentrated in a smaller section of the model, with the top 20% of layers exhibiting 773% more error density than the remaining 80\%. This highlights the need for selective repair. Additionally, we demonstrate that a dynamic selection approach is essential for addressing errors dispersed throughout the model, ensuring a robust and efficient repair.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.07072v2</guid>
      <category>cs.CL</category>
      <category>cs.AI</category>
      <category>cs.SE</category>
      <pubDate>Thu, 13 Feb 2025 00:00:00 -0500</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Sayem Mohammad Imtiaz, Astha Singh, Fraol Batole, Hridesh Rajan</dc:creator>
    </item>
  </channel>
</rss>
