<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 06 May 2025 02:30:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Proper Correlation Coefficients for Nominal Random Variables</title>
      <link>https://arxiv.org/abs/2505.00785</link>
      <description>arXiv:2505.00785v1 Announce Type: new 
Abstract: This paper develops an intuitive concept of perfect dependence between two variables of which at least one has a nominal scale that is attainable for all marginal distributions and proposes a set of dependence measures that are 1 if and only if this perfect dependence is satisfied. The advantages of these dependence measures relative to classical dependence measures like contingency coefficients, Goodman-Kruskal's lambda and tau and the so-called uncertainty coefficient are twofold. Firstly, they are defined if one of the variables is real-valued and exhibits continuities. Secondly, they satisfy the property of attainability. That is, they can take all values in the interval [0,1] irrespective of the marginals involved. Both properties are not shared by the classical dependence measures which need two discrete marginal distributions and can in some situations yield values close to 0 even though the dependence is strong or even perfect.
  Additionally, I provide a consistent estimator for one of the new dependence measures together with its asymptotic distribution under independence as well as in the general case. This allows to construct confidence intervals and an independence test, whose finite sample performance I subsequently examine in a simulation study. Finally, I illustrate the use of the new dependence measure in two applications on the dependence between the variables country and income or country and religion, respectively.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00785v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jan-Lukas Wermuth</dc:creator>
    </item>
    <item>
      <title>Q-Learning with Clustered-SMART (cSMART) Data: Examining Moderators in the Construction of Clustered Adaptive Interventions</title>
      <link>https://arxiv.org/abs/2505.00822</link>
      <description>arXiv:2505.00822v1 Announce Type: new 
Abstract: A clustered adaptive intervention (cAI) is a pre-specified sequence of decision rules that guides practitioners on how best - and based on which measures - to tailor cluster-level intervention to improve outcomes at the level of individuals within the clusters. A clustered sequential multiple assignment randomized trial (cSMART) is a type of trial that is used to inform the empirical development of a cAI. The most common type of secondary aim in a cSMART focuses on assessing causal effect moderation by candidate tailoring variables. We introduce a clustered Q-learning framework with the M-out-of-N Cluster Bootstrap using data from a cSMART to evaluate whether a set of candidate tailoring variables may be useful in defining an optimal cAI. This approach could construct confidence intervals (CI) with near-nominal coverage to assess parameters indexing the causal effect moderation function. Specifically, it allows reliable inferences concerning the utility of candidate tailoring variables in constructing a cAI that maximizes a mean end-of-study outcome even when "non-regularity", a well-known challenge exists. Simulations demonstrate the numerical performance of the proposed method across varying non-regularity conditions and investigate the impact of varying number of clusters and intra-cluster correlation coefficient on CI coverage. Methods are applied on ADEPT dataset to inform the construction of a clinic-level cAI for improving evidence-based practice in treating mood disorders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00822v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yao Song, Kelly Speth, Amy Kilbourne, Andrew Quanbeck, Daniel Almirall, Lu Wang</dc:creator>
    </item>
    <item>
      <title>Residual-based Alternative Partial Least Squares for Generalized Functional Linear Models</title>
      <link>https://arxiv.org/abs/2505.00860</link>
      <description>arXiv:2505.00860v1 Announce Type: new 
Abstract: Many biomedical studies collect high-dimensional medical imaging data to identify biomarkers for the detection, diagnosis, and treatment of human diseases. Consequently, it is crucial to develop accurate models that can predict a wide range of clinical outcomes (both discrete and continuous) based on imaging data. By treating imaging predictors as functional data, we propose a residual-based alternative partial least squares (RAPLS) model for a broad class of generalized functional linear models that incorporate both functional and scalar covariates. Our RAPLS method extends the alternative partial least squares (APLS) algorithm iteratively to accommodate additional scalar covariates and non-continuous outcomes. We establish the convergence rate of the RAPLS estimator for the unknown slope function and, with an additional calibration step, we prove the asymptotic normality and efficiency of the calibrated RAPLS estimator for the scalar parameters. The effectiveness of the RAPLS algorithm is demonstrated through multiple simulation studies and an application predicting Alzheimer's disease progression using neuroimaging data from the Alzheimer's Disease Neuroimaging Initiative (ADNI).</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00860v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <arxiv:DOI>10.5705/ss.202023.0418</arxiv:DOI>
      <dc:creator>Yue Wang, Xiao Wang, Joseph G. Ibrahim, Hongtu Zhu</dc:creator>
    </item>
    <item>
      <title>Multivariate Conformal Selection</title>
      <link>https://arxiv.org/abs/2505.00917</link>
      <description>arXiv:2505.00917v1 Announce Type: new 
Abstract: Selecting high-quality candidates from large datasets is critical in applications such as drug discovery, precision medicine, and alignment of large language models (LLMs). While Conformal Selection (CS) provides rigorous uncertainty quantification, it is limited to univariate responses and scalar criteria. To address this issue, we propose Multivariate Conformal Selection (mCS), a generalization of CS designed for multivariate response settings. Our method introduces regional monotonicity and employs multivariate nonconformity scores to construct conformal p-values, enabling finite-sample False Discovery Rate (FDR) control. We present two variants: mCS-dist, using distance-based scores, and mCS-learn, which learns optimal scores via differentiable optimization. Experiments on simulated and real-world datasets demonstrate that mCS significantly improves selection power while maintaining FDR control, establishing it as a robust framework for multivariate selection tasks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00917v1</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <category>stat.ML</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tian Bai, Yue Zhao, Xiang Yu, Archer Y. Yang</dc:creator>
    </item>
    <item>
      <title>What is estimated in cluster randomized crossover trials with informative sizes? -- A survey of estimands and common estimators</title>
      <link>https://arxiv.org/abs/2505.00925</link>
      <description>arXiv:2505.00925v1 Announce Type: new 
Abstract: In the analysis of cluster randomized trials (CRTs), previous work has defined two meaningful estimands: the individual-average treatment effect (iATE) and cluster-average treatment effect (cATE) estimand, to address individual and cluster-level hypotheses. In multi-period CRT designs, such as the cluster randomized crossover (CRXO) trial, additional weighted average treatment effect estimands help fully reflect the longitudinal nature of these trial designs, namely the cluster-period-average treatment effect (cpATE) and period-average treatment effect (pATE). We define different forms of informative sizes, where the treatment effects vary according to cluster, period, and/or cluster-period sizes, which subsequently cause these estimands to differ in magnitude. Under such conditions, we demonstrate which of the unweighted, inverse cluster-period size weighted, inverse cluster size weighted, and inverse period size weighted: (i.) independence estimating equation, (ii.) fixed effects model, (iii.) exchangeable mixed effects model, and (iv.) nested exchangeable mixed effects model treatment effect estimators are consistent for the aforementioned estimands in 2-period cross-sectional CRXO designs with continuous outcomes. We report a simulation study and conclude with a reanalysis of a CRXO trial testing different treatments on hospital length of stay among patients receiving invasive mechanical ventilation. Notably, with informative sizes, the unweighted and weighted nested exchangeable mixed effects model estimators are not consistent for any meaningful estimand and can yield biased results. In contrast, the unweighted and weighted independence estimating equation, and under specific scenarios, the fixed effects model and exchangeable mixed effects model, can yield consistent and empirically unbiased estimators for meaningful estimands in 2-period CRXO trials.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00925v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kenneth M. Lee, Andrew B. Forbes, Jessica Kasza, Andrew Copas, Brennan C. Kahan, Paul J. Young, Michael O. Harhay, Fan Li</dc:creator>
    </item>
    <item>
      <title>Bayesian Forensic DNA Mixture Deconvolution Using a Novel String Similarity Measure</title>
      <link>https://arxiv.org/abs/2505.00934</link>
      <description>arXiv:2505.00934v1 Announce Type: new 
Abstract: Mixture interpretation is a central challenge in forensic science, where evidence often contains contributions from multiple sources. In the context of DNA analysis, biological samples recovered from crime scenes may include genetic material from several individuals, necessitating robust statistical tools to assess whether a specific person of interest (POI) is among the contributors. Methods based on capillary electrophoresis (CE) are currently in use worldwide, but offer limited resolution in complex mixtures. Advancements in massively parallel sequencing (MPS) technologies provide a richer, more detailed representation of DNA mixtures, but require new analytical strategies to fully leverage this information. In this work, we present a Bayesian framework for evaluating whether a POIs DNA is present in an MPS-based forensic sample. The model accommodates known contributors, such as the victim, and uses a novel string edit distance to quantify similarity between observed alleles and sequencing artifacts. The resulting Bayes factors enable effective discrimination between samples that do and do not contain the POIs DNA, demonstrating strong performance in both hypothesis testing and classification settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00934v1</guid>
      <category>stat.ME</category>
      <category>q-bio.GN</category>
      <category>stat.CO</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Taylor Petty, Jan Hannig, Hari Iyer</dc:creator>
    </item>
    <item>
      <title>On dimension reduction in conditional dependence models</title>
      <link>https://arxiv.org/abs/2505.01052</link>
      <description>arXiv:2505.01052v1 Announce Type: new 
Abstract: Inference of the conditional dependence structure is challenging when many covariates are present. In numerous applications, only a low-dimensional projection of the covariates influences the conditional distribution. The smallest subspace that captures this effect is called the central subspace in the literature. We show that inference of the central subspace of a vector random variable $\mathbf Y$ conditioned on a vector of covariates $\mathbf X$ can be separated into inference of the marginal central subspaces of the components of $\mathbf Y$ conditioned on $\mathbf X$ and on the copula central subspace, that we define in this paper. Further discussion addresses sufficient dimension reduction subspaces for conditional association measures. An adaptive nonparametric method is introduced for estimating the central dependence subspaces, achieving parametric convergence rates under mild conditions. Simulation studies illustrate the practical performance of the proposed approach.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01052v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler, Gerda Claeskens, Ir\`ene Gijbels</dc:creator>
    </item>
    <item>
      <title>Overview and practical recommendations on using Shapley Values for identifying predictive biomarkers via CATE modeling</title>
      <link>https://arxiv.org/abs/2505.01145</link>
      <description>arXiv:2505.01145v1 Announce Type: new 
Abstract: In recent years, two parallel research trends have emerged in machine learning, yet their intersections remain largely unexplored. On one hand, there has been a significant increase in literature focused on Individual Treatment Effect (ITE) modeling, particularly targeting the Conditional Average Treatment Effect (CATE) using meta-learner techniques. These approaches often aim to identify causal effects from observational data. On the other hand, the field of Explainable Machine Learning (XML) has gained traction, with various approaches developed to explain complex models and make their predictions more interpretable. A prominent technique in this area is Shapley Additive Explanations (SHAP), which has become mainstream in data science for analyzing supervised learning models. However, there has been limited exploration of SHAP application in identifying predictive biomarkers through CATE models, a crucial aspect in pharmaceutical precision medicine. We address inherent challenges associated with the SHAP concept in multi-stage CATE strategies and introduce a surrogate estimation approach that is agnostic to the choice of CATE strategy, effectively reducing computational burdens in high-dimensional data. Using this approach, we conduct simulation benchmarking to evaluate the ability to accurately identify biomarkers using SHAP values derived from various CATE meta-learners and Causal Forest.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01145v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>David Svensson, Erik Hermansson, Nikolaos Nikolaou, Konstantinos Sechidis, Ilya Lipkovich</dc:creator>
    </item>
    <item>
      <title>Joint Modelling of Line and Point Data on Metric Graphs</title>
      <link>https://arxiv.org/abs/2505.01175</link>
      <description>arXiv:2505.01175v1 Announce Type: new 
Abstract: Metric graphs are useful tools for describing spatial domains like road and river networks, where spatial dependence act along the network. We take advantage of recent developments for such Gaussian Random Fields (GRFs), and consider joint spatial modelling of observations with different spatial supports. Motivated by an application to traffic state modelling in Trondheim, Norway, we consider line-referenced data, which can be described by an integral of the GRF along a line segment on the metric graph, and point-referenced data. Through a simulation study inspired by the application, we investigate the number of replicates that are needed to estimate parameters and to predict unobserved locations. The former is assessed using bias and variability, and the latter is assessed through root mean square error (RMSE), continuous rank probability scores (CRPSs), and coverage. Joint modelling is contrasted with a simplified approach that treat line-referenced observations as point-referenced observations. The results suggest joint modelling leads to strong improvements. The application to Trondheim, Norway, combines point-referenced induction loop data and line-referenced public transportation data. To ensure positive speeds, we use a non-linear link function, which requires integrals of non-linear combinations of the linear predictor. This is made computationally feasible by a combination of the R packages inlabru and MetricGraph, and new code for processing geographical line data to work with existing graph representations and fmesher methods for dealing with line support in inlabru on objects from MetricGraph. We fit the model to two datasets where we expect different spatial dependency and compare the results.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01175v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karina Lilleborge, Sara Martino, Geir-Arne Fuglstad, Finn Lindgren, Rikke Ingebrigtsen</dc:creator>
    </item>
    <item>
      <title>Modeling Large Nonstationary Spatial Data with the Full-Scale Basis Graphical Lasso</title>
      <link>https://arxiv.org/abs/2505.01318</link>
      <description>arXiv:2505.01318v1 Announce Type: new 
Abstract: We propose a new approach for the modeling large datasets of nonstationary spatial processes that combines a latent low rank process and a sparse covariance model. The low rank component coefficients are endowed with a flexible graphical Gaussian Markov random field model. The utilization of a low rank and compactly-supported covariance structure combines the full-scale approximation and the basis graphical lasso; we term this new approach the full-scale basis graphical lasso (FSBGL). Estimation employs a graphical lasso-penalized likelihood, which is optimized using a difference-of-convex scheme. We illustrate the proposed approach with a challenging high-resolution simulation dataset of the thermosphere. In a comparison against state-of-the-art spatial models, the FSBGL performs better at capturing salient features of the thermospheric temperature fields, even with limited available training data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01318v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Matthew LeDuc, William Kleiber, Tomoko Matsuo</dc:creator>
    </item>
    <item>
      <title>Design-Based Inference under Random Potential Outcomes via Riesz Representation</title>
      <link>https://arxiv.org/abs/2505.01324</link>
      <description>arXiv:2505.01324v2 Announce Type: new 
Abstract: We introduce a general framework for design-based causal inference that accommodates stochastic potential outcomes, thereby extending the classical Neyman-Rubin setup in which outcomes are treated as fixed. In our formulation, each unit's potential outcome is modelled as a function $\tilde{y}_i(z, \omega)$, where $\omega$ denotes latent randomness external to the treatment assignment. Building on recent work that connects design-based estimation with the Riesz representation theorem, we construct causal estimators by embedding potential outcomes in a Hilbert space and defining treatment effects as linear functionals. This allows us to derive unbiased and consistent estimators, even when potential outcomes exhibit random variation. The framework retains the key advantage of design-based analysis, namely, the use of a known randomisation scheme for identification, while enabling inference in settings with inherent stochasticity. We establish large-sample properties under local dependence, provide a variance estimator compatible with sparse dependency structures, and illustrate the method through a simulation. Our results unify design-based reasoning with random-outcome modelling, broadening the applicability of causal inference in complex experimental environments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01324v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yukai Yang</dc:creator>
    </item>
    <item>
      <title>Weight-calibrated estimation for factor models of high-dimensional time series</title>
      <link>https://arxiv.org/abs/2505.01357</link>
      <description>arXiv:2505.01357v2 Announce Type: new 
Abstract: The factor modeling for high-dimensional time series is powerful in discovering latent common components for dimension reduction and information extraction. Most available estimation methods can be divided into two categories: the covariance-based under asymptotically-identifiable assumption and the autocovariance-based with white idiosyncratic noise. This paper follows the autocovariance-based framework and develops a novel weight-calibrated method to improve the estimation performance. It adopts a linear projection to tackle high-dimensionality, and employs a reduced-rank autoregression formulation. The asymptotic theory of the proposed method is established, relaxing the assumption on white noise. Additionally, we make the first attempt in the literature by providing a systematic theoretical comparison among the covariance-based, the standard autocovariance-based, and our proposed weight-calibrated autocovariance-based methods in the presence of factors with different strengths. Extensive simulations are conducted to showcase the superior finite-sample performance of our proposed method, as well as to validate the newly established theory. The superiority of our proposal is further illustrated through the analysis of one financial and one macroeconomic data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01357v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xinghao Qiao, Zihan Wang, Qiwei Yao, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Dual system estimation using mixed effects loglinear models</title>
      <link>https://arxiv.org/abs/2505.01359</link>
      <description>arXiv:2505.01359v1 Announce Type: new 
Abstract: In official statistics, dual system estimation (DSE) is a well-known tool to estimate the size of a population. Two sources are linked, and the number of units that are missed by both sources is estimated. Often dual system estimation is carried out in each of the levels of a stratifying variable, such as region. DSE can be considered a loglinear independence model, and, with a stratifying variable, a loglinear conditional independence model. The standard approach is to estimate parameters for each level of the stratifying variable. Thus, when the number of levels of the stratifying variable is large, the number of parameters estimated is large as well. Mixed effects loglinear models, where sets of parameters involving the stratifying variable are replaced by a distribution parameterised by its mean and a variance, have also been proposed, and we investigate their properties through simulation. In our simulation studies the mixed effects loglinear model outperforms the fixed effects loglinear model although only to a small extent in terms of mean squared error. We show how mixed effects dual system estimation can be extended to multiple system estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.01359v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ceejay Hammond, Paul A. Smith, Peter G. M. van der Heijden</dc:creator>
    </item>
    <item>
      <title>StablePCA: Learning Shared Representations across Multiple Sources via Minimax Optimization</title>
      <link>https://arxiv.org/abs/2505.00940</link>
      <description>arXiv:2505.00940v1 Announce Type: cross 
Abstract: When synthesizing multisource high-dimensional data, a key objective is to extract low-dimensional feature representations that effectively approximate the original features across different sources. Such general feature extraction facilitates the discovery of transferable knowledge, mitigates systematic biases such as batch effects, and promotes fairness. In this paper, we propose Stable Principal Component Analysis (StablePCA), a novel method for group distributionally robust learning of latent representations from high-dimensional multi-source data. A primary challenge in generalizing PCA to the multi-source regime lies in the nonconvexity of the fixed rank constraint, rendering the minimax optimization nonconvex. To address this challenge, we employ the Fantope relaxation, reformulating the problem as a convex minimax optimization, with the objective defined as the maximum loss across sources. To solve the relaxed formulation, we devise an optimistic-gradient Mirror Prox algorithm with explicit closed-form updates. Theoretically, we establish the global convergence of the Mirror Prox algorithm, with the convergence rate provided from the optimization perspective. Furthermore, we offer practical criteria to assess how closely the solution approximates the original nonconvex formulation. Through extensive numerical experiments, we demonstrate StablePCA's high accuracy and efficiency in extracting robust low-dimensional representations across various finite-sample scenarios.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00940v1</guid>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.CO</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhenyu Wang, Molei Liu, Jing Lei, Francis Bach, Zijian Guo</dc:creator>
    </item>
    <item>
      <title>Demystifying and avoiding the OLS "weighting problem": Unmodeled heterogeneity and straightforward solutions</title>
      <link>https://arxiv.org/abs/2403.03299</link>
      <description>arXiv:2403.03299v4 Announce Type: replace 
Abstract: Researchers frequently estimate treatment effects by regressing outcomes (Y) on treatment (D) and covariates (X). Even without unobserved confounding, the coefficient on D yields a conditional-variance-weighted average of strata-wise effects, not the average treatment effect. Scholars have proposed characterizing the severity of these weights, evaluating resulting biases, or changing investigators' target estimand to the conditional-variance-weighted effect. We aim to demystify these weights, clarifying how they arise, what they represent, and how to avoid them. Specifically, these weights reflect misspecification bias from unmodeled treatment-effect heterogeneity. Rather than diagnosing or tolerating them, we recommend avoiding the issue altogether, by relaxing the standard regression assumption of "single linearity" to one of "separate linearity" (of each potential outcome in the covariates), accommodating heterogeneity. Numerous methods--including regression imputation (g-computation), interacted regression, and mean balancing weights--satisfy this assumption. In many settings, the efficiency cost to avoiding this weighting problem altogether will be modest and worthwhile.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.03299v4</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tanvi Shinkre, Chad Hazlett</dc:creator>
    </item>
    <item>
      <title>Simultaneously detecting spatiotemporal changes with penalized Poisson regression models</title>
      <link>https://arxiv.org/abs/2405.06613</link>
      <description>arXiv:2405.06613v2 Announce Type: replace 
Abstract: In the realm of large-scale spatiotemporal data, abrupt changes are commonly occurring across both spatial and temporal domains. This study aims to address the concurrent challenges of detecting change points and identifying spatial clusters within spatiotemporal count data. We introduce an innovative method based on the Poisson regression model, employing doubly fused penalization to unveil the underlying spatiotemporal change patterns. To efficiently estimate the model, we present an iterative shrinkage and threshold based algorithm to minimize the doubly penalized likelihood function. We establish the statistical consistency properties of the proposed estimator, confirming its reliability and accuracy. Furthermore, we conduct extensive numerical experiments to validate our theoretical findings, thereby highlighting the superior performance of our method when compared to existing competitive approaches.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.06613v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zerui Zhang, Xin Wang, Xin Zhang, Jing Zhang</dc:creator>
    </item>
    <item>
      <title>Bayesian Emulation of Grey-Box Multi-Model Ensembles Exploiting Known Interior Structure</title>
      <link>https://arxiv.org/abs/2406.08367</link>
      <description>arXiv:2406.08367v2 Announce Type: replace 
Abstract: Computer models are widely used to study complex real world physical systems. However, there are major limitations to their direct use including: their complex structure; large numbers of inputs and outputs; and long evaluation times. Bayesian emulators are an effective means of addressing these challenges providing fast and efficient statistical approximation for computer model outputs. It is commonly assumed that computer models behave like a ``black-box'' function with no knowledge of the output prior to its evaluation. This ensures that emulators are generalisable but potentially limits their accuracy compared with exploiting such knowledge of constrained or structured output behaviour. We assume a ``grey-box'' computer model and develop a methodological toolkit for its analysis. This includes: multi-model ensemble subsampling to identifying a representative model subset to reduce computational expense; constructing a targeted Bayesian design for optimisation or decision support; a ``divide-and-conquer'' approach to emulating sums of outputs; structured emulators exploiting known constrained and structured behaviour of constituent outputs through splitting the parameter space and imposing truncations; emulation of sums of time series outputs; and emulation of multi-model ensemble outputs. Combining these methods establishes a hierarchical emulation framework which achieves greater physical interpretability and more accurate emulator predictions. This research is motivated by and applied to the commercially important TNO OLYMPUS Well Control Optimisation Challenge from the petroleum industry which we re-express as a decision support under uncertainty problem. We thus encourage users to examine their ``black-box'' simulators to achieve superior emulator accuracy.</description>
      <guid isPermaLink="false">oai:arXiv.org:2406.08367v2</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jonathan Owen, Ian Vernon</dc:creator>
    </item>
    <item>
      <title>Doubly-Robust Functional Average Treatment Effect Estimation</title>
      <link>https://arxiv.org/abs/2501.06024</link>
      <description>arXiv:2501.06024v2 Announce Type: replace 
Abstract: Understanding causal relationships in the presence of complex, structured data remains a central challenge in modern statistics and science in general. While traditional causal inference methods are well-suited for scalar outcomes, many scientific applications demand tools capable of handling functional data -- outcomes observed as functions over continuous domains such as time or space. Motivated by this need, we propose DR-FoS, a novel method for estimating the Functional Average Treatment Effect (FATE) in observational studies with functional outcomes. DR-FoS exhibits double robustness properties, ensuring consistent estimation of FATE even if either the outcome or the treatment assignment model is misspecified. By leveraging recent advances in functional data analysis and causal inference, we establish the asymptotic properties of the estimator, proving its convergence to a Gaussian process. This guarantees valid inference with simultaneous confidence bands across the entire functional domain. Through extensive simulations, we show that DR-FoS achieves robust performance under a wide range of model specifications. Finally, we illustrate the utility of DR-FoS in a real-world application, analyzing functional outcomes to uncover meaningful causal insights in the SHARE (Survey of Health, Aging and Retirement in Europe) dataset.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.06024v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lorenzo Testa, Tobia Boschi, Francesca Chiaromonte, Edward H. Kennedy, Matthew Reimherr</dc:creator>
    </item>
    <item>
      <title>High Dimensional Ensemble Kalman Filter</title>
      <link>https://arxiv.org/abs/2505.00283</link>
      <description>arXiv:2505.00283v2 Announce Type: replace 
Abstract: The ensemble Kalman Filter (EnKF), as a fundamental data assimilation approach, has been widely used in many fields of earth science, engineering and beyond. However, several theoretical aspects of the EnKF remain unknown, especially when the state variable is of high dimensional accompanied with high resolution observation and physical models. This paper first proposes several high dimensional EnKF (HD-EnKF) methods that provide consistent estimators for the important forecast error covariance and the Kalman gain matrix. It then studies the theoretical properties of the EnKF under both the fixed and high dimensional state variables, which provides the mean square errors of the analysis states to the underlying oracle states offered by the Kalman filter and gives the much needed insight into the roles played by the forecast error covariance on the accuracy of the EnKF. The accuracy of the data assimilation under the misspecified physical model is also considered. Numerical studies on the Lorenz-96 and the Shallow Water Equation models illustrate that the proposed HD-EnKF algorithms perform better than the standard EnKF methods as they provide more robust and accurate assimilated results. The HD-EnKF was applied to assimilate sea temperature in the Northwest Pacific, which showed more accurate out-sample performances than the existing methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.00283v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Song Xi Chen, Hao-Xuan Sun, Shouxia Wang</dc:creator>
    </item>
    <item>
      <title>Mode and Ridge Estimation in Euclidean and Directional Product Spaces: A Mean Shift Approach</title>
      <link>https://arxiv.org/abs/2110.08505</link>
      <description>arXiv:2110.08505v2 Announce Type: replace-cross 
Abstract: The set of local modes and density ridge lines are important summary characteristics of the data-generating distribution. In this work, we focus on estimating local modes and density ridges from point cloud data in a product space combining two or more Euclidean and/or directional metric spaces. Specifically, our approach extends the (subspace constrained) mean shift algorithm to such product spaces, addressing potential challenges in the generalization process. We establish the algorithmic convergence of the proposed methods, along with practical implementation guidelines. Experiments on simulated and real-world datasets demonstrate the effectiveness of our proposed methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2110.08505v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yikun Zhang, Yen-Chi Chen</dc:creator>
    </item>
    <item>
      <title>Quasi-Score Matching Estimation for Spatial Autoregressive Model with Random Weights Matrix and Regressors</title>
      <link>https://arxiv.org/abs/2305.19721</link>
      <description>arXiv:2305.19721v2 Announce Type: replace-cross 
Abstract: With the rapid advancements in technology for data collection, the application of the spatial autoregressive (SAR) model has become increasingly prevalent in real-world analysis, particularly when dealing with large datasets. However, the commonly used quasi-maximum likelihood estimation (QMLE) for the SAR model is not computationally scalable to handle the data with a large size. In addition, when establishing the asymptotic properties of the parameter estimators of the SAR model, both weights matrix and regressors are assumed to be nonstochastic in classical spatial econometrics, which is perhaps not realistic in real applications. Motivated by the machine learning literature, this paper proposes quasi-score matching estimation for the SAR model. This new estimation approach is developed based on the likelihood, but significantly reduces the computational complexity of the QMLE. The asymptotic properties of parameter estimators under the random weights matrix and regressors are established, which provides a new theoretical framework for the asymptotic inference of the SAR-type models. The usefulness of the quasi-score matching estimation and its asymptotic inference is illustrated via extensive simulation studies and a case study of an anti-conflict social network experiment for middle school students.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.19721v2</guid>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xuan Liang, Tao Zou</dc:creator>
    </item>
    <item>
      <title>Uncertainty-Aware Bayes' Rule and Its Applications</title>
      <link>https://arxiv.org/abs/2311.05532</link>
      <description>arXiv:2311.05532v3 Announce Type: replace-cross 
Abstract: Bayes' rule has enabled innumerable powerful algorithms of statistical signal processing and statistical machine learning. However, when model misspecifications exist in prior and/or data distributions, the direct application of Bayes' rule is questionable. Philosophically, the key is to balance the relative importance between prior and data distributions when calculating posterior distributions: if prior distributions are overly conservative (i.e., exceedingly spread), we upweight the prior belief; if prior distributions are overly opportunistic (i.e., exceedingly concentrated), we downweight the prior belief. The same operation also applies to data distributions. This paper studies a generalized Bayes' rule, called uncertainty-aware Bayes' rule, to technically realize the above philosophy, thus combating the model uncertainties in prior and/or data distributions. Applications of the uncertainty-aware Bayes' rule on classification and estimation are discussed: In particular, the uncertainty-aware Bayes classifier, the uncertainty-aware Kalman filter, the uncertainty-aware particle filter, and the uncertainty-aware interactive-multiple-model filter are suggested and experimentally validated.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.05532v3</guid>
      <category>eess.SP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shixiong Wang</dc:creator>
    </item>
    <item>
      <title>Multivariate Density Estimation via Variance-Reduced Sketching</title>
      <link>https://arxiv.org/abs/2401.11646</link>
      <description>arXiv:2401.11646v3 Announce Type: replace-cross 
Abstract: Multivariate density estimation is of great interest in various scientific and engineering disciplines. In this work, we introduce a new framework called Variance-Reduced Sketching (VRS), specifically designed to estimate multivariate density functions with a reduced curse of dimensionality. Our VRS framework conceptualizes multivariate functions as infinite-size matrices/tensors, and facilitates a new sketching technique motivated by the numerical linear algebra literature to reduce the variance in density estimation problems. We demonstrate the robust numerical performance of VRS through a series of simulated experiments and real-world data applications. Notably, VRS shows remarkable improvement over existing neural network density estimators and classical kernel methods in numerous distribution models. Additionally, we offer theoretical justifications for VRS to support its ability to deliver density estimation with a reduced curse of dimensionality.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.11646v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yifan Peng, Yuehaw Khoo, Daren Wang</dc:creator>
    </item>
    <item>
      <title>Transmission Channel Analysis in Dynamic Models</title>
      <link>https://arxiv.org/abs/2405.18987</link>
      <description>arXiv:2405.18987v3 Announce Type: replace-cross 
Abstract: We propose a framework for analysing transmission channels in a large class of dynamic models. We formulate our approach both using graph theory and potential outcomes, which we show to be equivalent. Our method, labelled Transmission Channel Analysis (TCA), allows for the decomposition of total effects captured by impulse response functions into the effects flowing through transmission channels, thereby providing a quantitative assessment of the strength of various well-defined channels. We establish that this requires no additional identification assumptions beyond the identification of the structural shock whose effects the researcher wants to decompose. Additionally, we prove that impulse response functions are sufficient statistics for the computation of transmission effects. We demonstrate the empirical relevance of TCA for policy evaluation by decomposing the effects of policy shocks arising from a variety of popular macroeconomic models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2405.18987v3</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Enrico Wegner, Lenard Lieb, Stephan Smeekes, Ines Wilms</dc:creator>
    </item>
    <item>
      <title>Slowly Scaling Per-Record Differential Privacy</title>
      <link>https://arxiv.org/abs/2409.18118</link>
      <description>arXiv:2409.18118v2 Announce Type: replace-cross 
Abstract: We develop formal privacy mechanisms for releasing statistics from data with many outlying values, such as income data. These mechanisms ensure that a per-record differential privacy guarantee degrades slowly in the protected records' influence on the statistics being released.
  Formal privacy mechanisms generally add randomness, or "noise," to published statistics. If a noisy statistic's distribution changes little with the addition or deletion of a single record in the underlying dataset, an attacker looking at this statistic will find it plausible that any particular record was present or absent, preserving the records' privacy. More influential records -- those whose addition or deletion would change the statistics' distribution more -- typically suffer greater privacy loss. The per-record differential privacy framework quantifies these record-specific privacy guarantees, but existing mechanisms let these guarantees degrade rapidly (linearly or quadratically) with influence. While this may be acceptable in cases with some moderately influential records, it results in unacceptably high privacy losses when records' influence varies widely, as is common in economic data.
  We develop mechanisms with privacy guarantees that instead degrade as slowly as logarithmically with influence. These mechanisms allow for the accurate, unbiased release of statistics, while providing meaningful protection for highly influential records. As an example, we consider the private release of sums of unbounded establishment data such as payroll, where our mechanisms extend meaningful privacy protection even to very large establishments. We evaluate these mechanisms empirically and demonstrate their utility.</description>
      <guid isPermaLink="false">oai:arXiv.org:2409.18118v2</guid>
      <category>cs.CR</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Brian Finley, Anthony M Caruso, Justin C Doty, Ashwin Machanavajjhala, Mikaela R Meyer, David Pujol, William Sexton, Zachary Terner</dc:creator>
    </item>
    <item>
      <title>I-trustworthy Models. A framework for trustworthiness evaluation of probabilistic classifiers</title>
      <link>https://arxiv.org/abs/2501.15617</link>
      <description>arXiv:2501.15617v2 Announce Type: replace-cross 
Abstract: As probabilistic models continue to permeate various facets of our society and contribute to scientific advancements, it becomes a necessity to go beyond traditional metrics such as predictive accuracy and error rates and assess their trustworthiness. Grounded in the competence-based theory of trust, this work formalizes I-trustworthy framework -- a novel framework for assessing the trustworthiness of probabilistic classifiers for inference tasks by linking local calibration to trustworthiness. To assess I-trustworthiness, we use the local calibration error (LCE) and develop a method of hypothesis-testing. This method utilizes a kernel-based test statistic, Kernel Local Calibration Error (KLCE), to test local calibration of a probabilistic classifier. This study provides theoretical guarantees by offering convergence bounds for an unbiased estimator of KLCE. Additionally, we present a diagnostic tool designed to identify and measure biases in cases of miscalibration. The effectiveness of the proposed test statistic is demonstrated through its application to both simulated and real-world datasets. Finally, LCE of related recalibration methods is studied, and we provide evidence of insufficiency of existing methods to achieve I-trustworthiness.</description>
      <guid isPermaLink="false">oai:arXiv.org:2501.15617v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 05 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <arxiv:journal_reference>International Conference on Artificial Intelligence and Statistics 2025 Apr 23 (pp. 4726-4734). PMLR</arxiv:journal_reference>
      <dc:creator>Ritwik Vashistha, Arya Farahi</dc:creator>
    </item>
  </channel>
</rss>
