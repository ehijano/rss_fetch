<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 19 May 2025 04:00:20 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Targeted Learning Estimation of Sampling Variance for Improved Inference</title>
      <link>https://arxiv.org/abs/2505.10624</link>
      <description>arXiv:2505.10624v1 Announce Type: new 
Abstract: For robust statistical inference it is crucial to obtain a good estimator of the variance of the proposed estimator of the statistical estimand. A commonly used estimator of the variance for an asymptotically linear estimator is the sample variance of the estimated influence function. This estimator has been shown to be anti-conservative in limited samples or in the presence of near-positivity violations, leading to elevated Type-I error rates and poor coverage. In this paper, capitalizing on earlier attempts at targeted variance estimators, we propose a one-step targeted variance estimator for the causal risk ratio (CRR) in scenarios involving treatment, outcome, and baseline covariates. While our primary focus is on the variance of log(CRR), our methodology can be extended to other causal effect parameters. Specifically, we focus on the variance of the IF for the log relative risk (log(CRR)) estimator, which requires deriving the efficient influence function for the variance of the IF as the basis for constructing the estimator. Several methods are available to develop efficient estimators of asymptotically linear parameters. In this paper, we concentrate on the so-called one-step targeted maximum likelihood estimator, which is a substitution estimator that utilizes a one-dimensional universal least favorable parametric submodel when updating the distribution. We conduct simulations with different effect sizes, sample sizes and levels of positivity to compare the estimator with existing methods in terms of coverage and Type-I error. Simulation results demonstrate that, especially with small samples and near-positivity violations, the proposed variance estimator offers improved performance, achieving coverage closer to the nominal level of 0.95 and a lower Type-I error rate.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10624v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yunwen Ji, Mark van der Laan, Alan Hubbard</dc:creator>
    </item>
    <item>
      <title>Dependency-Aware Shrinkage Priors for High Dimensional Regression</title>
      <link>https://arxiv.org/abs/2505.10715</link>
      <description>arXiv:2505.10715v1 Announce Type: new 
Abstract: In high dimensional regression, global local shrinkage priors have gained significant traction for their ability to yield sparse estimates, improve parameter recovery, and support accurate predictive modeling. While recent work has explored increasingly flexible shrinkage prior structures, the role of explicitly modeling dependencies among coefficients remains largely unexplored. In this paper, we investigate whether incorporating such structures into traditional shrinkage priors improves their performance. We introduce dependency-aware shrinkage priors, an extension of continuous shrinkage priors that integrates correlation structures inspired by Zellner's g prior approach. We provide theoretical insights into how dependence alters the prior and posterior structure, and evaluate the method empirically through simulations and real data. We find that modeling dependence can improve parameter recovery when predictors are strongly correlated, but offers only modest gains in predictive accuracy. These findings suggest that prior dependence should be used selectively and guided by the specific inferential goals of the analysis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10715v1</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Javier Enrique Aguilar, Paul-Christian B\"urkner</dc:creator>
    </item>
    <item>
      <title>Statically Significant Linear Regression Coefficients Solely Driven By Outliers In Finite-sample Inference</title>
      <link>https://arxiv.org/abs/2505.10738</link>
      <description>arXiv:2505.10738v1 Announce Type: new 
Abstract: In this paper, we investigate the impact of outliers on the statistical significance of coefficients in linear regression. We demonstrate, through numerical simulation using R, that a single outlier can cause an otherwise insignificant coefficient to appear statistically significant. We compare this with robust Huber regression, which reduces the effects of outliers. Afterwards, we approximate the influence of a single outlier on estimated regression coefficients and discuss common diagnostic statistics to detect influential observations in regression (e.g., studentized residuals). Furthermore, we relate this issue to the optional normality assumption in simple linear regression [14], required for exact finite-sample inference but asymptotically justified for large n by the Central Limit Theorem (CLT). We also address the general dangers of relying solely on p-values without performing adequate regression diagnostics. Finally, we provide a brief overview of regression methods and discuss how they relate to the assumptions of the Gauss-Markov theorem.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10738v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator>Felix Reichel</dc:creator>
    </item>
    <item>
      <title>Tractable Unified Skew-t Distribution and Copula for Heterogeneous Asymmetries</title>
      <link>https://arxiv.org/abs/2505.10849</link>
      <description>arXiv:2505.10849v1 Announce Type: new 
Abstract: Multivariate distributions that allow for asymmetry and heavy tails are important building blocks in many econometric and statistical models. The Unified Skew-t (UST) is a promising choice because it is both scalable and allows for a high level of flexibility in the asymmetry in the distribution. However, it suffers from parameter identification and computational hurdles that have to date inhibited its use for modeling data. In this paper we propose a new tractable variant of the unified skew-t (TrUST) distribution that addresses both challenges. Moreover, the copula of this distribution is shown to also be tractable, while allowing for greater heterogeneity in asymmetric dependence over variable pairs than the popular skew-t copula. We show how Bayesian posterior inference for both the distribution and its copula can be computed using an extended likelihood derived from a generative representation of the distribution. The efficacy of this Bayesian method, and the enhanced flexibility of both the TrUST distribution and its implicit copula, is first demonstrated using simulated data. Applications of the TrUST distribution to highly skewed regional Australian electricity prices, and the TrUST copula to intraday U.S. equity returns, demonstrate how our proposed distribution and its copula can provide substantial increases in accuracy over the popular skew-t and its copula in practice.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10849v1</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Lin Deng, Michael Stanley Smith, Worapree Maneesoonthorn</dc:creator>
    </item>
    <item>
      <title>A Cautionary Tale on Integrating Studies with Disparate Outcome Measures for Causal Inference</title>
      <link>https://arxiv.org/abs/2505.11014</link>
      <description>arXiv:2505.11014v1 Announce Type: new 
Abstract: Data integration approaches are increasingly used to enhance the efficiency and generalizability of studies. However, a key limitation of these methods is the assumption that outcome measures are identical across datasets -- an assumption that often does not hold in practice. Consider the following opioid use disorder (OUD) studies: the XBOT trial and the POAT study, both evaluating the effect of medications for OUD on withdrawal symptom severity (not the primary outcome of either trial). While XBOT measures withdrawal severity using the subjective opiate withdrawal scale, POAT uses the clinical opiate withdrawal scale. We analyze this realistic yet challenging setting where outcome measures differ across studies and where neither study records both types of outcomes. Our paper studies whether and when integrating studies with disparate outcome measures leads to efficiency gains. We introduce three sets of assumptions -- with varying degrees of strength -- linking both outcome measures. Our theoretical and empirical results highlight a cautionary tale: integration can improve asymptotic efficiency only under the strongest assumption linking the outcomes. However, misspecification of this assumption leads to bias. In contrast, a milder assumption may yield finite-sample efficiency gains, yet these benefits diminish as sample size increases. We illustrate these trade-offs via a case study integrating the XBOT and POAT datasets to estimate the comparative effect of two medications for opioid use disorder on withdrawal symptoms. By systematically varying the assumptions linking the SOW and COW scales, we show potential efficiency gains and the risks of bias. Our findings emphasize the need for careful assumption selection when fusing datasets with differing outcome measures, offering guidance for researchers navigating this common challenge in modern data integration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11014v1</guid>
      <category>stat.ME</category>
      <category>cs.LG</category>
      <category>econ.EM</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Harsh Parikh, Trang Quynh Nguyen, Elizabeth A. Stuart, Kara E. Rudolph, Caleb H. Miles</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for Prior-Data Fitted Networks using Martingale Posteriors</title>
      <link>https://arxiv.org/abs/2505.11325</link>
      <description>arXiv:2505.11325v1 Announce Type: new 
Abstract: Prior-data fitted networks (PFNs) have emerged as promising foundation models for prediction from tabular data sets, achieving state-of-the-art performance on small to moderate data sizes without tuning. While PFNs are motivated by Bayesian ideas, they do not provide any uncertainty quantification for predictive means, quantiles, or similar quantities. We propose a principled and efficient sampling procedure to construct Bayesian posteriors for such estimates based on Martingale posteriors, and prove its convergence. Several simulated and real-world data examples showcase the uncertainty quantification of our method in inference applications.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11325v1</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Nagler, David R\"ugamer</dc:creator>
    </item>
    <item>
      <title>Targeted empirical Bayes for more supervised joint factor analysis</title>
      <link>https://arxiv.org/abs/2505.11351</link>
      <description>arXiv:2505.11351v1 Announce Type: new 
Abstract: Joint Bayesian factor models are popular for characterizing relationships between multivariate correlated predictors and a response variable. Standard models assume that all variables, including both the predictors and the response, are conditionally independent given latent factors. In marginalizing out these factors, one obtains a low rank plus diagonal factorization for the joint covariance, which implies a linear regression for the response given the predictors. Although there are many desirable properties of such models, these methods can struggle to identify the signal when the response is not dependent on the dominant principal components in the predictors. To address this problem, we propose estimating the residual variance in the response model with an empirical Bayes procedure that targets predictive performance of the response given the predictors. We illustrate that this can lead to substantial improvements in simulation performance. We are particularly motivated by studies assessing the health effects of environmental exposures and provide an illustrative application to NHANES data.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11351v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Glenn Palmer, David B. Dunson</dc:creator>
    </item>
    <item>
      <title>Desirability of outcome ranking (DOOR) analysis for multivariate survival outcomes with application to ACTT-1 trial</title>
      <link>https://arxiv.org/abs/2505.11496</link>
      <description>arXiv:2505.11496v1 Announce Type: new 
Abstract: Desirability Of Outcome Ranking (DOOR) methodology accounts for problems that conventional benefit:risk analyses in clinical trials ignore, such as competing risks and the trade-off relationship between efficacy and toxicity. DOOR levels can be considered as a multi-state process in nature, as event-free survival, and survival with side effects are not equivalent and the overall patient trajectory requires recognition. In monotone settings where patients' conditions can only decline, we can record event times for each transition from one level of the DOOR to another, and construct Kaplan-Meier curves displaying transition times. While traditional survival analysis methods such as the Cox model require assumptions like proportional hazards and suffer from the challenge of interpreting a hazard ratio, Restricted Mean Survival Time (RMST) offers an alternative with greater intuitiveness. Therefore, we propose a combination of the two domains to develop estimation and inferential procedures that could benefit from the advantages of both DOOR and RMST. Particularly, the area under each survival curve restricted to a time point, or the RMST, has clear clinical meanings, from expected event-free survival time, expected survival time with at most one of the events, to expected lifetime before death. We show that the nonparametric estimator of the RMSTs asymptotically follows a multivariate Gaussian process through the martingale theory and functional delta method. There are alternative approaches to hypothesis testing that recognize when patients transition into worse states. We evaluate our proposed method with data simulated under a multistate model. We consider various scenarios, including when the null hypothesis is true, when the treatment difference exists only in certain DOOR levels, and small-sample studies. We also present a real-world example with ACTT-1.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11496v1</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Shiyu Shu, Guoqing Diao, Toshimitsu Hamasaki, Scott Evans</dc:creator>
    </item>
    <item>
      <title>Assumption-lean weak limits and tests for two-stage adaptive experiments</title>
      <link>https://arxiv.org/abs/2505.10747</link>
      <description>arXiv:2505.10747v1 Announce Type: cross 
Abstract: Adaptive experiments are becoming increasingly popular in real-world applications for effectively maximizing in-sample welfare and efficiency by data-driven sampling. Despite their growing prevalence, however, the statistical foundations for valid inference in such settings remain underdeveloped. Focusing on two-stage adaptive experimental designs, we address this gap by deriving new weak convergence results for mean outcomes and their differences. In particular, our results apply to a broad class of estimators, the weighted inverse probability weighted (WIPW) estimators. In contrast to prior works, our results require significantly weaker assumptions and sharply characterize phase transitions in limiting behavior across different signal regimes. Through this common lens, our general results unify previously fragmented results under the two-stage setup. To address the challenge of potential non-normal limits in conducting inference, we propose a computationally efficient and provably valid plug-in bootstrap method for hypothesis testing. Our results and approaches are sufficiently general to accommodate various adaptive experimental designs, including batched bandit and subgroup enrichment experiments. Simulations and semi-synthetic studies demonstrate the practical value of our approach, revealing statistical phenomena unique to adaptive experiments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10747v1</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ziang Niu, Zhimei Ren</dc:creator>
    </item>
    <item>
      <title>Distribution Regression with Censored Selection</title>
      <link>https://arxiv.org/abs/2505.10814</link>
      <description>arXiv:2505.10814v1 Announce Type: cross 
Abstract: We develop a distribution regression model with a censored selection rule, offering a semi-parametric generalization of the Heckman selection model. Our approach applies to the entire distribution, extending beyond the mean or median, accommodates non-Gaussian error structures, and allows for heterogeneous effects of covariates on both the selection and outcome distributions. By employing a censored selection rule, our model can uncover richer selection patterns according to both outcome and selection variables, compared to the binary selection case. We analyze identification, estimation, and inference of model functionals such as sorting parameters and distributions purged of sample selection. An application to labor supply using data from the UK reveals different selection patterns into full-time and overtime work across gender, marital status, and time. Additionally, decompositions of wage distributions by gender show that selection effects contribute to a decrease in the observed gender wage gap at low quantiles and an increase in the gap at high quantiles for full-time workers. The observed gender wage gap among overtime workers is smaller, which may be driven by different selection behaviors into overtime work across genders.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.10814v1</guid>
      <category>econ.EM</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ivan Fernandez-Val, Seoyun Hong</dc:creator>
    </item>
    <item>
      <title>Inferring the Most Similar Variable-length Subsequences between Multidimensional Time Series</title>
      <link>https://arxiv.org/abs/2505.11106</link>
      <description>arXiv:2505.11106v1 Announce Type: cross 
Abstract: Finding the most similar subsequences between two multidimensional time series has many applications: e.g. capturing dependency in stock market or discovering coordinated movement of baboons. Considering one pattern occurring in one time series, we might be wondering whether the same pattern occurs in another time series with some distortion that might have a different length. Nevertheless, to the best of our knowledge, there is no efficient framework that deals with this problem yet. In this work, we propose an algorithm that provides the exact solution of finding the most similar multidimensional subsequences between time series where there is a difference in length both between time series and between subsequences. The algorithm is built based on theoretical guarantee of correctness and efficiency. The result in simulation datasets illustrated that our approach not just only provided correct solution, but it also utilized running time only quarter of time compared against the baseline approaches. In real-world datasets, it extracted the most similar subsequences even faster (up to 20 times faster against baseline methods) and provided insights regarding the situation in stock market and following relations of multidimensional time series of baboon movement. Our approach can be used for any time series. The code and datasets of this work are provided for the public use.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11106v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.DB</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thanadej Rattanakornphan, Piyanon Charoenpoonpanich, Chainarong Amornbunchornvej</dc:creator>
    </item>
    <item>
      <title>Bayesian Hierarchical Invariant Prediction</title>
      <link>https://arxiv.org/abs/2505.11211</link>
      <description>arXiv:2505.11211v1 Announce Type: cross 
Abstract: We propose Bayesian Hierarchical Invariant Prediction (BHIP) reframing Invariant Causal Prediction (ICP) through the lens of Hierarchical Bayes. We leverage the hierarchical structure to explicitly test invariance of causal mechanisms under heterogeneous data, resulting in improved computational scalability for a larger number of predictors compared to ICP. Moreover, given its Bayesian nature BHIP enables the use of prior information. In this paper, we test two sparsity inducing priors: horseshoe and spike-and-slab, both of which allow us a more reliable identification of causal features. We test BHIP in synthetic and real-world data showing its potential as an alternative inference method to ICP.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11211v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Francisco Madaleno, Pernille Julie Viuff Sand, Francisco C. Pereira, Sergio Hernan Garrido Mejia</dc:creator>
    </item>
    <item>
      <title>Fourier Low-rank and Sparse Tensor for Efficient Tensor Completion</title>
      <link>https://arxiv.org/abs/2505.11261</link>
      <description>arXiv:2505.11261v1 Announce Type: cross 
Abstract: Tensor completion is crucial in many scientific domains with missing data problems. Traditional low-rank tensor models, including CP, Tucker, and Tensor-Train, exploit low-dimensional structures to recover missing data. However, these methods often treat all tensor modes symmetrically, failing to capture the unique spatiotemporal patterns inherent in scientific data, where the temporal component exhibits both low-frequency stability and high-frequency variations. To address this, we propose a novel model, \underline{F}ourier \underline{Lo}w-rank and \underline{S}parse \underline{T}ensor (FLoST), which decomposes the tensor along the temporal dimension using a Fourier transform. This approach captures low-frequency components with low-rank matrices and high-frequency fluctuations with sparsity, resulting in a hybrid structure that efficiently models both smooth and localized variations. Compared to the well-known tubal-rank model, which assumes low-rankness across all frequency components, FLoST requires significantly fewer parameters, making it computationally more efficient, particularly when the time dimension is large. Through theoretical analysis and empirical experiments, we demonstrate that FLoST outperforms existing tensor completion models in terms of both accuracy and computational efficiency, offering a more interpretable solution for spatiotemporal data reconstruction.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11261v1</guid>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingyang Li, Jiuqian Shang, Yang Chen</dc:creator>
    </item>
    <item>
      <title>A Fourier Space Perspective on Diffusion Models</title>
      <link>https://arxiv.org/abs/2505.11278</link>
      <description>arXiv:2505.11278v1 Announce Type: cross 
Abstract: Diffusion models are state-of-the-art generative models on data modalities such as images, audio, proteins and materials. These modalities share the property of exponentially decaying variance and magnitude in the Fourier domain. Under the standard Denoising Diffusion Probabilistic Models (DDPM) forward process of additive white noise, this property results in high-frequency components being corrupted faster and earlier in terms of their Signal-to-Noise Ratio (SNR) than low-frequency ones. The reverse process then generates low-frequency information before high-frequency details. In this work, we study the inductive bias of the forward process of diffusion models in Fourier space. We theoretically analyse and empirically demonstrate that the faster noising of high-frequency components in DDPM results in violations of the normality assumption in the reverse process. Our experiments show that this leads to degraded generation quality of high-frequency components. We then study an alternate forward process in Fourier space which corrupts all frequencies at the same rate, removing the typical frequency hierarchy during generation, and demonstrate marked performance improvements on datasets where high frequencies are primary, while performing on par with DDPM on standard imaging benchmarks.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11278v1</guid>
      <category>stat.ML</category>
      <category>cs.CV</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Fabian Falck, Teodora Pandeva, Kiarash Zahirnia, Rachel Lawrence, Richard Turner, Edward Meeds, Javier Zazo, Sushrut Karmalkar</dc:creator>
    </item>
    <item>
      <title>A Generative Framework for Causal Estimation via Importance-Weighted Diffusion Distillation</title>
      <link>https://arxiv.org/abs/2505.11444</link>
      <description>arXiv:2505.11444v1 Announce Type: cross 
Abstract: Estimating individualized treatment effects from observational data is a central challenge in causal inference, largely due to covariate imbalance and confounding bias from non-randomized treatment assignment. While inverse probability weighting (IPW) is a well-established solution to this problem, its integration into modern deep learning frameworks remains limited. In this work, we propose Importance-Weighted Diffusion Distillation (IWDD), a novel generative framework that combines the pretraining of diffusion models with importance-weighted score distillation to enable accurate and fast causal estimation-including potential outcome prediction and treatment effect estimation. We demonstrate how IPW can be naturally incorporated into the distillation of pretrained diffusion models, and further introduce a randomization-based adjustment that eliminates the need to compute IPW explicitly-thereby simplifying computation and, more importantly, provably reducing the variance of gradient estimates. Empirical results show that IWDD achieves state-of-the-art out-of-sample prediction performance, with the highest win rates compared to other baselines, significantly improving causal estimation and supporting the development of individualized treatment strategies. We will release our PyTorch code for reproducibility and future research.</description>
      <guid isPermaLink="false">oai:arXiv.org:2505.11444v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xinran Song, Tianyu Chen, Mingyuan Zhou</dc:creator>
    </item>
    <item>
      <title>Consistent Selection of the Number of Groups in Panel Models via Cross-Validation</title>
      <link>https://arxiv.org/abs/2209.05474</link>
      <description>arXiv:2209.05474v3 Announce Type: replace 
Abstract: Group number selection is a key problem for group panel data modeling. In this work, we develop a cross-validation (CV) method to tackle this problem. Specifically, we split the panel data into two data folds on the time span, with group structure preserved for individuals. We first estimate the group memberships and parameters on one data fold, then we plug in the estimates and utilize the other data fold to evaluate a designed criterion. Subsequently, the group number is estimated by minimizing the average criterion across all data folds. The proposed CV method has two advantages compared to existing approaches. First, the method is totally data-driven, thus no further tuning parameters are involved. Second, the method can be flexibly applied to a wide range of panel data models. Theoretically, we establish the estimation consistency by taking advantage of the optimization property of the estimation algorithm. Experiments are carried out with a variety of synthetic datasets and panel models to further illustrate the advantages of the proposed method. Lastly, the CV method is employed to analyze the heterogeneous patterns of stock volatilities in the Chinese stock market through the financial crisis.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.05474v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Zhe Li, Xuening Zhu, Changliang Zou</dc:creator>
    </item>
    <item>
      <title>False Discovery Rate Adjustments for Average Significance Level Controlling Tests</title>
      <link>https://arxiv.org/abs/2209.13686</link>
      <description>arXiv:2209.13686v2 Announce Type: replace 
Abstract: Multiple testing adjustments, such as the Benjamini and Hochberg (1995) step-up procedure for controlling the false discovery rate (FDR), are typically applied to families of tests that control significance level in the classical sense: for each individual test, the probability of false rejection is no greater than the nominal level. In this paper, we consider tests that satisfy only a weaker notion of significance level control, in which the probability of false rejection need only be controlled on average over the hypotheses. We find that the Benjamini and Hochberg (1995) step-up procedure still controls FDR in the asymptotic regime with many weakly dependent $p$-values, and that certain adjustments for dependent $p$-values such as the Benjamini and Yekutieli (2001) procedure continue to yield FDR control in finite samples. Our results open the door to FDR controlling procedures in nonparametric and high dimensional settings where weakening the notion of inference allows for large power improvements.</description>
      <guid isPermaLink="false">oai:arXiv.org:2209.13686v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Timothy B. Armstrong</dc:creator>
    </item>
    <item>
      <title>Optimization-based Sensitivity Analysis for Unmeasured Confounding using Partial Correlations</title>
      <link>https://arxiv.org/abs/2301.00040</link>
      <description>arXiv:2301.00040v4 Announce Type: replace 
Abstract: Causal inference necessarily relies upon untestable assumptions; hence, it is crucial to assess the robustness of obtained results to violations of identification assumptions. However, such sensitivity analysis is only occasionally undertaken in practice, as many existing methods require analytically tractable solutions and their results are often difficult to interpret. We take a more flexible approach to sensitivity analysis and view it as a constrained stochastic optimization problem. This work focuses on sensitivity analysis for a linear causal effect when an unmeasured confounder and a potential instrument are present. We show how the bias of the OLS and TSLS estimands can be expressed in terms of partial correlations. Leveraging the algebraic rules that relate different partial correlations, practitioners can specify intuitive sensitivity models which bound the bias. We further show that the heuristic "plug-in" sensitivity interval may not have any confidence guarantees; instead, we propose a bootstrap approach to construct sensitivity intervals which performs well in numerical simulations. We illustrate the proposed methods with a real study on the causal effect of education on earnings and provide user-friendly visualization tools.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.00040v4</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Tobias Freidling, Qingyuan Zhao</dc:creator>
    </item>
    <item>
      <title>Bayesian Adaptive Tucker Decompositions for Tensor Factorization</title>
      <link>https://arxiv.org/abs/2411.10218</link>
      <description>arXiv:2411.10218v2 Announce Type: replace 
Abstract: Tucker tensor decomposition offers a more effective representation for multiway data compared to the widely used PARAFAC model. However, its flexibility brings the challenge of selecting the appropriate latent multi-rank. To overcome the issue of pre-selecting the latent multi-rank, we introduce a Bayesian adaptive Tucker decomposition model that infers the multi-rank automatically via an infinite increasing shrinkage prior. The model introduces local sparsity in the core tensor, inducing rich and at the same time parsimonious dependency structures. Posterior inference proceeds via an efficient adaptive Gibbs sampler, supporting both continuous and binary data and allowing for straightforward missing data imputation when dealing with incomplete multiway data. We discuss fundamental properties of the proposed modeling framework, providing theoretical justification. Simulation studies and applications to chemometrics and complex ecological data offer compelling evidence of its advantages over existing tensor factorization methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2411.10218v2</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Federica Stolf, Antonio Canale</dc:creator>
    </item>
    <item>
      <title>Chain-linked multiple matrix integration via embedding alignment</title>
      <link>https://arxiv.org/abs/2412.02791</link>
      <description>arXiv:2412.02791v3 Announce Type: replace 
Abstract: Motivated by the increasing demand for multi-source data integration in various scientific fields, in this paper we study matrix completion in scenarios where the data exhibits certain block-wise missing structures -- specifically, where only a few noisy submatrices representing (overlapping) parts of the full matrix are available. We propose the Chain-linked Multiple Matrix Integration (CMMI) procedure to efficiently combine the information that can be extracted from these individual noisy submatrices. CMMI begins by deriving entity embeddings for each observed submatrix, then aligns these embeddings using overlapping entities between pairs of submatrices, and finally aggregates them to reconstruct the entire matrix of interest. We establish, under mild regularity conditions, entrywise error bounds and normal approximations for the CMMI estimates. Simulation studies and real data applications show that CMMI is computationally efficient and effective in recovering the full matrix, even when overlaps between the observed submatrices are minimal.</description>
      <guid isPermaLink="false">oai:arXiv.org:2412.02791v3</guid>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Runbing Zheng, Minh Tang</dc:creator>
    </item>
    <item>
      <title>Uncertainty Quantification for LLM-Based Survey Simulations</title>
      <link>https://arxiv.org/abs/2502.17773</link>
      <description>arXiv:2502.17773v2 Announce Type: replace 
Abstract: We investigate the use of large language models (LLMs) to simulate human responses to survey questions, and perform uncertainty quantification to gain reliable insights. Our approach converts imperfect LLM-simulated responses into confidence sets for population parameters of human responses, addressing the distribution shift between the simulated and real populations. A key innovation lies in determining the optimal number of simulated responses: too many produce overly narrow confidence sets with poor coverage, while too few yield excessively loose estimates. To resolve this, our method adaptively selects the simulation sample size, ensuring valid average-case coverage guarantees. It is broadly applicable to any LLM, irrespective of its fidelity, and any procedure for constructing confidence sets. Additionally, the selected sample size quantifies the degree of misalignment between the LLM and the target human population. We illustrate our method on real datasets and LLMs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2502.17773v2</guid>
      <category>stat.ME</category>
      <category>cs.AI</category>
      <category>cs.LG</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Chengpiao Huang, Yuhang Wu, Kaizheng Wang</dc:creator>
    </item>
    <item>
      <title>The Principle of Redundant Reflection</title>
      <link>https://arxiv.org/abs/2503.21719</link>
      <description>arXiv:2503.21719v2 Announce Type: replace 
Abstract: The fact that redundant information does not change a rational belief after Bayesian updating implies uniqueness of Bayes rule. In fact, any updating rule is uniquely specified by this principle. This is true for the classical setting, as well as settings with improper or continuous priors. We prove this result and illustrate it with two examples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2503.21719v2</guid>
      <category>stat.ME</category>
      <category>stat.OT</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Martin Metodiev, Maarten Marsman, Lourens Waldorp, Quentin F. Gronau, Eric-Jan Wagenmakers</dc:creator>
    </item>
    <item>
      <title>Model-free Bootstrap and Conformal Prediction in Regression: Conditionality, Conjecture Testing, and Pertinent Prediction Intervals</title>
      <link>https://arxiv.org/abs/2109.12156</link>
      <description>arXiv:2109.12156v3 Announce Type: replace-cross 
Abstract: Predictive inference under a general regression setting is gaining more interest in the big-data era. In terms of going beyond point prediction to develop prediction intervals, two main threads of development are conformal prediction and Model-free prediction. Recently, a new conformal prediction approach was proposed that exploits the same uniformization procedure as in the well-known Model-free Bootstrap. Hence, it is of interest to compare and further investigate the performance of the two methods. In the paper at hand, we contrast the two approaches via theoretical analysis and numerical experiments with a focus on conditional coverage of prediction intervals. We discuss suitable scenarios for applying each algorithm, underscore the importance of conditional vs. unconditional coverage, and show that, under mild conditions, the Model-free bootstrap yields prediction intervals with guaranteed better conditional coverage compared to quantile estimation. We also extend the concept of 'pertinence' of prediction intervals to the nonparametric regression setting, and give concrete examples where its importance emerges under finite sample scenarios. Finally, we define the new notion of 'conjecture testing' that is the analog of hypothesis testing as applied to the prediction problem; we also devise a modified conformal score to allow conformal prediction to handle one-sided 'conjecture tests', and compare to the Model-free bootstrap.</description>
      <guid isPermaLink="false">oai:arXiv.org:2109.12156v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Yiren Wang, Dimitris N. Politis</dc:creator>
    </item>
    <item>
      <title>Auditing Fairness by Betting</title>
      <link>https://arxiv.org/abs/2305.17570</link>
      <description>arXiv:2305.17570v3 Announce Type: replace-cross 
Abstract: We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, and easy to implement. We demonstrate the efficacy of our approach on three benchmark fairness datasets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.17570v3</guid>
      <category>stat.ML</category>
      <category>cs.AI</category>
      <category>cs.CY</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Ben Chugg, Santiago Cortes-Gomez, Bryan Wilder, Aaditya Ramdas</dc:creator>
    </item>
    <item>
      <title>Changing the Kernel During Training Leads to Double Descent in Kernel Regression</title>
      <link>https://arxiv.org/abs/2311.01762</link>
      <description>arXiv:2311.01762v3 Announce Type: replace-cross 
Abstract: We investigate changing the bandwidth of a translational-invariant kernel during training when solving kernel regression with gradient descent. We present a theoretical bound on the out-of-sample generalization error that advocates for decreasing the bandwidth (and thus increasing the model complexity) during training. We further use the bound to show that kernel regression exhibits a double descent behavior when the model complexity is expressed as the minimum allowed bandwidth during training. Decreasing the bandwidth all the way to zero results in benign overfitting, and also circumvents the need for model selection. We demonstrate the double descent behavior on real and synthetic data and also demonstrate that kernel regression with a decreasing bandwidth outperforms that of a constant bandwidth, selected by cross-validation or marginal likelihood maximization. We finally apply our findings to neural networks, demonstrating that by modifying the neural tangent kernel (NTK) during training, making the NTK behave as if its bandwidth were decreasing to zero, we can make the network overfit more benignly, and converge in fewer iterations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2311.01762v3</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>math.OC</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Oskar Allerbo</dc:creator>
    </item>
    <item>
      <title>Local-Polynomial Estimation for Multivariate Regression Discontinuity Designs</title>
      <link>https://arxiv.org/abs/2402.08941</link>
      <description>arXiv:2402.08941v2 Announce Type: replace-cross 
Abstract: We introduce a multivariate local-linear estimator for multivariate regression discontinuity designs in which treatment is assigned by crossing a boundary in the space of running variables. The dominant approach uses the Euclidean distance from a boundary point as the scalar running variable; hence, multivariate designs are handled as uni-variate designs. However, the bandwidth selection with the distance running variable is suboptimal and inefficient for the underlying multivariate problem. We handle multivariate designs as multivariate. In this study, we develop a novel asymptotic normality for multivariate local-polynomial estimators. Our estimator is asymptotically valid and can capture heterogeneous treatment effects over the boundary. We demonstrate the effectiveness of our estimator through numerical simulations. Our empirical illustration of a Colombian scholarship study reveals a richer heterogeneity of the treatment effect that is hidden in the original estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2402.08941v2</guid>
      <category>econ.EM</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Masayuki Sawada, Takuya Ishihara, Daisuke Kurisu, Yasumasa Matsuda</dc:creator>
    </item>
    <item>
      <title>Quantization Error Propagation: Revisiting Layer-Wise Post-Training Quantization</title>
      <link>https://arxiv.org/abs/2504.09629</link>
      <description>arXiv:2504.09629v2 Announce Type: replace-cross 
Abstract: Layer-wise PTQ is a promising technique for compressing large language models (LLMs), due to its simplicity and effectiveness without requiring retraining. However, recent progress in this area is saturating, underscoring the need to revisit its core limitations and explore further improvements. We address this challenge by identifying a key limitation of existing layer-wise PTQ methods: the growth of quantization errors across layers significantly degrades performance, particularly in low-bit regimes. To address this fundamental issue, we propose Quantization Error Propagation (QEP), a general, lightweight, and scalable framework that enhances layer-wise PTQ by explicitly propagating quantization errors and compensating for accumulated errors. QEP also offers a tunable propagation mechanism that prevents overfitting and controls computational overhead, enabling the framework to adapt to various architectures and resource budgets. Extensive experiments on several LLMs demonstrate that QEP-enhanced layer-wise PTQ achieves substantially higher accuracy than existing methods. Notably, the gains are most pronounced in the extremely low-bit quantization regime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2504.09629v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <pubDate>Mon, 19 May 2025 00:00:00 -0400</pubDate>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Yamato Arai, Yuma Ichikawa</dc:creator>
    </item>
  </channel>
</rss>
