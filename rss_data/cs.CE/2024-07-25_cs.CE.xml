<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>cs.CE updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/cs.CE</link>
    <description>cs.CE updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/cs.CE" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 04:01:24 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Thu, 25 Jul 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Quotient complex (QC)-based machine learning for 2D perovskite design</title>
      <link>https://arxiv.org/abs/2407.16996</link>
      <description>arXiv:2407.16996v1 Announce Type: new 
Abstract: With remarkable stability and exceptional optoelectronic properties, two-dimensional (2D) halide layered perovskites hold immense promise for revolutionizing photovoltaic technology. Presently, inadequate representations have substantially impeded the design and discovery of 2D perovskites. In this context, we introduce a novel computational topology framework termed the quotient complex (QC), which serves as the foundation for the material representation. Our QC-based features are seamlessly integrated with learning models for the advancement of 2D perovskite design. At the heart of this framework lies the quotient complex descriptors (QCDs), representing a quotient variation of simplicial complexes derived from materials unit cell and periodic boundary conditions. Differing from prior material representations, this approach encodes higher-order interactions and periodicity information simultaneously. Based on the well-established New Materials for Solar Energetics (NMSE) databank, our QC-based machine learning models exhibit superior performance against all existing counterparts. This underscores the paramount role of periodicity information in predicting material functionality, while also showcasing the remarkable efficiency of the QC-based model in characterizing materials structural attributes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.16996v1</guid>
      <category>cs.CE</category>
      <category>math.AT</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Chuan-Shen Hu, Rishikanta Mayengbam, Kelin Xia, Tze Chien Sum</dc:creator>
    </item>
    <item>
      <title>A simple hybrid linear and non-linear interpolation finite element for adaptive cracking elements method</title>
      <link>https://arxiv.org/abs/2407.17104</link>
      <description>arXiv:2407.17104v1 Announce Type: new 
Abstract: Cracking Elements Method (CEM) is a numerical tool to simulate quasi-brittle fractures, which does not need remeshing, nodal enrichment, or complicated crack tracking strategy. The cracking elements used in the CEM can be considered as a special type of finite element implemented in the standard finite element frameworks. One disadvantage of CEM is that it uses nonlinear interpolation of the displacement field (Q8 or T6 elements), introducing more nodes and consequent computing efforts than the cases with elements using linear interpolation of the displacement field. Aiming at solving this problem, we propose a simple hybrid linear and non-linear interpolation finite element for adaptive cracking elements method in this work. A simple strategy is proposed for treating the elements with $p$ edge nodes $p\in\left[0,n\right]$ and $n$ being the edge number of the element. Only a few codes are needed. Then, by only adding edge and center nodes on the elements experiencing cracking and keeping linear interpolation of the displacement field for the elements outside the cracking domain, the number of total nodes was reduced almost to half of the case using the conventional cracking elements. Numerical investigations prove that the new approach inherits all the advantages of CEM with greatly improved computing efficiency.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17104v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Xueya Wang, Yiming Zhang, Minjie Wen, Herbert Mang</dc:creator>
    </item>
    <item>
      <title>A Reduced Order Model conditioned on monitoring features for estimation and uncertainty quantification in engineered systems</title>
      <link>https://arxiv.org/abs/2407.17139</link>
      <description>arXiv:2407.17139v1 Announce Type: new 
Abstract: Reduced Order Models (ROMs) form essential tools across engineering domains by virtue of their function as surrogates for computationally intensive digital twinning simulators. Although purely data-driven methods are available for ROM construction, schemes that allow to retain a portion of the physics tend to enhance the interpretability and generalization of ROMs. However, physics-based techniques can adversely scale when dealing with nonlinear systems that feature parametric dependencies. This study introduces a generative physics-based ROM that is suited for nonlinear systems with parametric dependencies and is additionally able to quantify the confidence associated with the respective estimates. A main contribution of this work is the conditioning of these parametric ROMs to features that can be derived from monitoring measurements, feasibly in an online fashion. This is contrary to most existing ROM schemes, which remain restricted to the prescription of the physics-based, and usually a priori unknown, system parameters. Our work utilizes conditional Variational Autoencoders to continuously map the required reduction bases to a feature vector extracted from limited output measurements, while additionally allowing for a probabilistic assessment of the ROM-estimated Quantities of Interest. An auxiliary task using a neural network-based parametrization of suitable probability distributions is introduced to re-establish the link with physical model parameters. We verify the proposed scheme on a series of simulated case studies incorporating effects of geometric and material nonlinearity under parametric dependencies related to system properties and input load characteristics.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17139v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Konstantinos Vlachas, Thomas Simpson, Anthony Garland, D. Dane Quinn, Charbel Farhat, Eleni Chatzi</dc:creator>
    </item>
    <item>
      <title>KnowTD-An Actionable Knowledge Representation System for Thermodynamics</title>
      <link>https://arxiv.org/abs/2407.17169</link>
      <description>arXiv:2407.17169v1 Announce Type: new 
Abstract: We demonstrate that thermodynamic knowledge acquired by humans can be transferred to computers so that the machine can use it to solve thermodynamic problems and produce explainable solutions with a guarantee of correctness. The actionable knowledge representation system that we have created for this purpose is called KnowTD. It is based on an ontology of thermodynamics that represents knowledge of thermodynamic theory, material properties, and thermodynamic problems. The ontology is coupled with a reasoner that sets up the problem to be solved based on user input, extracts the correct, pertinent equations from the ontology, solves the resulting mathematical problem, and returns the solution to the user, together with an explanation of how it was obtained. KnowTD is presently limited to simple thermodynamic problems, similar to those discussed in an introductory course in Engineering Thermodynamics. This covers the basic theory and working principles of thermodynamics. KnowTD is designed in a modular way and is easily extendable.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17169v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <arxiv:DOI>10.1021/acs.jcim.4c00647</arxiv:DOI>
      <dc:creator>Luisa Vollmer, Sophie Fellenz, Fabian Jirasek, Heike Leitte, Hans Hasse</dc:creator>
    </item>
    <item>
      <title>Fusing LLMs and KGs for Formal Causal Reasoning behind Financial Risk Contagion</title>
      <link>https://arxiv.org/abs/2407.17190</link>
      <description>arXiv:2407.17190v1 Announce Type: new 
Abstract: Financial risks trend to spread from one entity to another, ultimately leading to systemic risks. The key to preventing such risks lies in understanding the causal chains behind risk contagion. Despite this, prevailing approaches primarily emphasize identifying risks, overlooking the underlying causal analysis of risk. To address such an issue, we propose a Risk Contagion Causal Reasoning model called RC2R, which uses the logical reasoning capabilities of large language models (LLMs) to dissect the causal mechanisms of risk contagion grounded in the factual and expert knowledge embedded within financial knowledge graphs (KGs). At the data level, we utilize financial KGs to construct causal instructions, empowering LLMs to perform formal causal reasoning on risk propagation and tackle the "causal parrot" problem of LLMs. In terms of model architecture, we integrate a fusion module that aligns tokens and nodes across various granularities via multi-scale contrastive learning, followed by the amalgamation of textual and graph-structured data through soft prompt with cross multi-head attention mechanisms. To quantify risk contagion, we introduce a risk pathway inference module for calculating risk scores for each node in the graph. Finally, we visualize the risk contagion pathways and their intensities using Sankey diagrams, providing detailed causal explanations. Comprehensive experiments on financial KGs and supply chain datasets demonstrate that our model outperforms several state-of-the-art models in prediction performance and out-of-distribution (OOD) generalization capabilities. We will make our dataset and code publicly accessible to encourage further research and development in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17190v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Guanyuan Yu, Xv Wang, Qing Li, Yu Zhao</dc:creator>
    </item>
    <item>
      <title>Exploring Commercial Vehicle Detouring Patterns through the Application of Probe Trajectory Data</title>
      <link>https://arxiv.org/abs/2407.17319</link>
      <description>arXiv:2407.17319v1 Announce Type: new 
Abstract: Understanding motorist detouring behavior is critical for both traffic operations and planning applications. However, measuring real-world detouring behavior is challenging due to the need to track the movement of individual vehicles. Recent developments in high-resolution vehicle trajectory data have enabled transportation professionals to observe real-world detouring behaviors without the need to install and maintain hardware such as license plate reading cameras. This paper investigates the feasibility of vehicle probe trajectory data to capture commercial motor vehicle (CMV) detouring behavior under three unique case studies. Before doing so, a validation analysis was conducted to investigate the ability of CMV probe trajectory data to represent overall CMV volumes at well-calibrated count stations near virtual weigh stations (VWS) in Maryland. The validation analysis showed strong positive correlations (above 0.75) at all VWS stations. Upon validating the data, a methodology was applied to assess CMV detour behaviors associated with CMV enforcement activities, congestion avoidance, and incident induced temporary road closures.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17319v1</guid>
      <category>cs.CE</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Mark Franz PhD, Sara Zahedian PhD, Dhairya Parekh, Tahsin Emtenam PhD, Greg Jordan</dc:creator>
    </item>
    <item>
      <title>EverAdapt: Continuous Adaptation for Dynamic Machine Fault Diagnosis Environments</title>
      <link>https://arxiv.org/abs/2407.17117</link>
      <description>arXiv:2407.17117v1 Announce Type: cross 
Abstract: Unsupervised Domain Adaptation (UDA) has emerged as a key solution in data-driven fault diagnosis, addressing domain shift where models underperform in changing environments. However, under the realm of continually changing environments, UDA tends to underperform on previously seen domains when adapting to new ones - a problem known as catastrophic forgetting. To address this limitation, we introduce the EverAdapt framework, specifically designed for continuous model adaptation in dynamic environments. Central to EverAdapt is a novel Continual Batch Normalization (CBN), which leverages source domain statistics as a reference point to standardize feature representations across domains. EverAdapt not only retains statistical information from previous domains but also adapts effectively to new scenarios. Complementing CBN, we design a class-conditional domain alignment module for effective integration of target domains, and a Sample-efficient Replay strategy to reinforce memory retention. Experiments on real-world datasets demonstrate EverAdapt superiority in maintaining robust fault diagnosis in dynamic environments. Our code is available: https://github.com/mohamedr002/EverAdapt</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.17117v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>cs.CE</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-sa/4.0/</dc:rights>
      <dc:creator> Edward, Mohamed Ragab, Yuecong Xu, Min Wu, Yuecong Xu, Zhenghua Chen, Abdulla Alseiari, Xiaoli Li</dc:creator>
    </item>
    <item>
      <title>A spatiotemporal deep learning framework for prediction of crack dynamics in heterogeneous solids: efficient mapping of concrete microstructures to its fracture properties</title>
      <link>https://arxiv.org/abs/2407.15665</link>
      <description>arXiv:2407.15665v2 Announce Type: replace 
Abstract: A spatiotemporal deep learning framework is proposed that is capable of 2D full-field prediction of fracture in concrete mesostructures. This framework not only predicts fractures but also captures the entire history of the fracture process, from the crack initiation in the interfacial transition zone to the subsequent propagation of the cracks in the mortar matrix. In addition, a convolutional neural network is developed which can predict the averaged stress-strain curve of the mesostructures. The UNet modeling framework, which comprises an encoder-decoder section with skip connections, is used as the deep learning surrogate model. Training and test data are generated from high-fidelity fracture simulations of randomly generated concrete mesostructures. These mesostructures include geometric variabilities such as different aggregate particle geometrical features, spatial distribution, and the total volume fraction of aggregates. The fracture simulations are carried out in Abaqus, utilizing the cohesive phase-field fracture modeling technique as the fracture modeling approach. In this work, to reduce the number of training datasets, the spatial distribution of three sets of material properties for three-phase concrete mesostructures, along with the spatial phase-field damage index, are fed to the UNet to predict the corresponding stress and spatial damage index at the subsequent step. It is shown that after the training process using this methodology, the UNet model is capable of accurately predicting damage on the unseen test dataset by using 470 datasets. Moreover, another novel aspect of this work is the conversion of irregular finite element data into regular grids using a developed pipeline. This approach allows for the implementation of less complex UNet architecture and facilitates the integration of phase-field fracture equations into surrogate models for future developments.</description>
      <guid isPermaLink="false">oai:arXiv.org:2407.15665v2</guid>
      <category>cs.CE</category>
      <category>cs.LG</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Rasoul Najafi Koopas, Shahed Rezaei, Natalie Rauter, Richard Ostwald, Rolf Lammering</dc:creator>
    </item>
    <item>
      <title>scVGAE: A Novel Approach using ZINB-Based Variational Graph Autoencoder for Single-Cell RNA-Seq Imputation</title>
      <link>https://arxiv.org/abs/2403.08959</link>
      <description>arXiv:2403.08959v2 Announce Type: replace-cross 
Abstract: Single-cell RNA sequencing (scRNA-seq) has revolutionized our ability to study individual cellular distinctions and uncover unique cell characteristics. However, a significant technical challenge in scRNA-seq analysis is the occurrence of "dropout" events, where certain gene expressions cannot be detected. This issue is particularly pronounced in genes with low or sparse expression levels, impacting the precision and interpretability of the obtained data. To address this challenge, various imputation methods have been implemented to predict such missing values, aiming to enhance the analysis's accuracy and usefulness. A prevailing hypothesis posits that scRNA-seq data conforms to a zero-inflated negative binomial (ZINB) distribution. Consequently, methods have been developed to model the data according to this distribution. Recent trends in scRNA-seq analysis have seen the emergence of deep learning approaches. Some techniques, such as the variational autoencoder, incorporate the ZINB distribution as a model loss function. Graph-based methods like Graph Convolutional Networks (GCN) and Graph Attention Networks (GAT) have also gained attention as deep learning methodologies for scRNA-seq analysis. This study introduces scVGAE, an innovative approach integrating GCN into a variational autoencoder framework while utilizing a ZINB loss function. This integration presents a promising avenue for effectively addressing dropout events in scRNA-seq data, thereby enhancing the accuracy and reliability of downstream analyses. scVGAE outperforms other methods in cell clustering, with the best performance in 11 out of 14 datasets. Ablation study shows all components of scVGAE are necessary. scVGAE is implemented in Python and downloadable at https://github.com/inoue0426/scVGAE.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.08959v2</guid>
      <category>q-bio.GN</category>
      <category>cs.CE</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yoshitaka Inoue</dc:creator>
    </item>
  </channel>
</rss>
