<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.AP updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.AP</link>
    <description>stat.AP updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.AP" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Tue, 30 Apr 2024 04:01:37 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Tue, 30 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Saturday</day>
      <day>Sunday</day>
    </skipDays>
    <item>
      <title>Decentralized Finance and Local Public Goods: A Bayesian Maximum Entropy Model of School District Spending in the U.S</title>
      <link>https://arxiv.org/abs/2404.17700</link>
      <description>arXiv:2404.17700v1 Announce Type: new 
Abstract: This paper investigates the distribution of public school expenditures across U.S. school districts using a bayesian maximum entropy model. Covering the period 2000-2016, I explore how inter-jurisdictional competition and household choice influence spending patterns within the public education sector, providing a novel empirical treatment of the Tiebout hypothesis within a statistical equilibrium framework. The analysis reveals that these expenditures are characterized by sharply peaked and positively skewed distributions, suggesting significant socioeconomic stratification. Employing Bayesian inference and Markov Chain Monte Carlo (MCMC) sampling, I fit these patterns into a statistical equilibrium model to elucidate the roles of competition, as well as household mobility and arbitrage in shaping the distribution of educational spending. The analysis reveals how the scale parameters associated with competition and household choice critically shape the equilibrium outcomes. The model and analysis offer a statistical basis for shaping policy measures intended to affect distributional outcomes in scenarios characterized by the decentralized provision of local public goods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17700v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Juan Melo</dc:creator>
    </item>
    <item>
      <title>Neutral Pivoting: Strong Bias Correction for Shared Information</title>
      <link>https://arxiv.org/abs/2404.17737</link>
      <description>arXiv:2404.17737v1 Announce Type: new 
Abstract: In the absence of historical data for use as forecasting inputs, decision makers often ask a panel of judges to predict the outcome of interest, leveraging the wisdom of the crowd (Surowiecki 2005). Even if the crowd is large and skilled, shared information can bias the simple mean of judges' estimates. Addressing the issue of bias, Palley and Soll (2019) introduces a novel approach called pivoting. Pivoting can take several forms, most notably the powerful and reliable minimal pivot. We build on the intuition of the minimal pivot and propose a more aggressive bias correction known as the neutral pivot. The neutral pivot achieves the largest bias correction of its class that both avoids the need to directly estimate crowd composition or skill and maintains a smaller expected squared error than the simple mean for all considered settings. Empirical assessments on real datasets confirm the effectiveness of the neutral pivot compared to current methods.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17737v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Joseph Rilling</dc:creator>
    </item>
    <item>
      <title>Bayesian analysis of biomarker levels can predict time of recurrence of prostate cancer with strictly positive apparent Shannon information against an exponential attrition prior</title>
      <link>https://arxiv.org/abs/2404.17857</link>
      <description>arXiv:2404.17857v1 Announce Type: new 
Abstract: Shariat et al previously investigated the possibility of predicting, from preoperative biomarkers and clinical data, which of any pair of patients would suffer recurrence of prostate cancer first. We wished to establish the extent to which predictions of time of relapse from such a model could be improved upon using Bayesian methodology.
  The same dataset was reanalysed using a Bayesian skew-Student mixture model. Predictions were made of which of any pair of patients would relapse first and of the time of relapse. The benefit of using these biomarkers relative to predictions made without them, was measured by the apparent Shannon information, using as prior a simple exponential attrition model of relapse time independent of input variables.
  Using half the dataset for training and the other half for testing, predictions of relapse time from the strict Cox model gave $-\infty$ nepers of apparent Shannon information, (it predicts that relapse can only occur at times when patients in the training set relapsed). Deliberately smoothed predictions from the Cox model gave -0.001 (-0.131 to +0.120) nepers, while the Bayesian model gave +0.109 (+0.021 to +0.192) nepers (mean, 2.5 to 97.5 centiles), being positive with posterior probability 0.993 and beating the blurred Cox model with posterior probability 0.927.
  These predictions from the Bayesian model thus outperform those of the Cox model, but the overall yield of predictive information leaves scope for improvement of the range of biomarkers in use. The Bayesian model presented here is the first such model for prostate cancer to consider the variation of relapse hazard with biomarker concentrations to be smooth, as is intuitive. It is also the first model to be shown to provide more apparent Shannon information than the Cox model and the first to be shown to provide positive apparent information relative to an exponential prior.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17857v1</guid>
      <category>stat.AP</category>
      <category>q-bio.QM</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Roger Sewell, Elisabeth Crowe, Sharokh F. Shariat</dc:creator>
    </item>
    <item>
      <title>Singular-value statistics of directed random graphs</title>
      <link>https://arxiv.org/abs/2404.18259</link>
      <description>arXiv:2404.18259v1 Announce Type: new 
Abstract: Singular-value statistics (SVS) has been recently presented as a random matrix theory tool able to properly characterize non-Hermitian random matrix ensembles [PRX Quantum {\bf 4}, 040312 (2023)]. Here, we perform a numerical study of the SVS of the non-Hermitian adjacency matrices $\mathbf{A}$ of directed random graphs, where $\mathbf{A}$ are members of diluted real Ginibre ensembles. We consider two models of directed random graphs: Erd\"os-R\'enyi graphs and random regular graphs. Specifically, we focus on the ratio $r$ between nearest neighbor singular values and the minimum singular value $\lambda_{min}$. We show that $\langle r \rangle$ (where $\langle \cdot \rangle$ represents ensemble average) can effectively characterize the transition between mostly isolated vertices to almost complete graphs, while the probability density function of $\lambda_{min}$ can clearly distinguish between different graph models.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18259v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>J. A. Mendez-Bermudez, R. Aguilar-Sanchez</dc:creator>
    </item>
    <item>
      <title>An statistical analysis of COVID-19 intensive care unit bed occupancy data</title>
      <link>https://arxiv.org/abs/2404.18493</link>
      <description>arXiv:2404.18493v1 Announce Type: new 
Abstract: The COVID-19 pandemic has had far-reaching consequences, highlighting the urgency for explanatory and predictive tools to track infection rates and burden of care over time and space. However, the scarcity and inhomogeneity of data is a challenge. In this research we develop a robust framework for estimating and predicting the occupied beds of Intensive Care Units by presenting an innovative Small Area Estimation methodology based on the definition of mixed models with random regression coefficients. We applied it to estimate and predict the daily occupancy of Intensive Care Unit beds by COVID-19 in health areas of Castilla y Le\'on, from November 2020 to March 2022.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18493v1</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Naomi Diz-Rosales (Universidade da Coru\~na, CITIC, Spain), Mar\'ia-Jos\'e Lombard\'ia (Universidade da Coru\~na, CITIC, Spain), Domingo Morales (Universidad Miguel Hern\'andez de Elche, IUICIO, Spain)</dc:creator>
    </item>
    <item>
      <title>Data Quality in Crowdsourcing and Spamming Behavior Detection</title>
      <link>https://arxiv.org/abs/2404.17582</link>
      <description>arXiv:2404.17582v1 Announce Type: cross 
Abstract: As crowdsourcing emerges as an efficient and cost-effective method for obtaining labels for machine learning datasets, it is important to assess the quality of crowd-provided data, so as to improve analysis performance and reduce biases in subsequent machine learning tasks. Given the lack of ground truth in most cases of crowdsourcing, we refer to data quality as annotators' consistency and credibility. Unlike the simple scenarios where Kappa coefficient and intraclass correlation coefficient usually can apply, online crowdsourcing requires dealing with more complex situations. We introduce a systematic method for evaluating data quality and detecting spamming threats via variance decomposition, and we classify spammers into three categories based on their different behavioral patterns. A spammer index is proposed to assess entire data consistency and two metrics are developed to measure crowd worker's credibility by utilizing the Markov chain and generalized random effects models. Furthermore, we showcase the practicality of our techniques and their advantages by applying them on a face verification task with both simulation and real-world data collected from two crowdsourcing platforms.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17582v1</guid>
      <category>cs.HC</category>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yang Ba, Michelle V. Mancenido, Erin K. Chiou, Rong Pan</dc:creator>
    </item>
    <item>
      <title>Using Pre-training and Interaction Modeling for ancestry-specific disease prediction in UK Biobank</title>
      <link>https://arxiv.org/abs/2404.17626</link>
      <description>arXiv:2404.17626v1 Announce Type: cross 
Abstract: Recent genome-wide association studies (GWAS) have uncovered the genetic basis of complex traits, but show an under-representation of non-European descent individuals, underscoring a critical gap in genetic research. Here, we assess whether we can improve disease prediction across diverse ancestries using multiomic data. We evaluate the performance of Group-LASSO INTERaction-NET (glinternet) and pretrained lasso in disease prediction focusing on diverse ancestries in the UK Biobank. Models were trained on data from White British and other ancestries and validated across a cohort of over 96,000 individuals for 8 diseases. Out of 96 models trained, we report 16 with statistically significant incremental predictive performance in terms of ROC-AUC scores. These findings suggest that advanced statistical methods that borrow information across multiple ancestries may improve disease risk prediction, but with limited benefit.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17626v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Thomas Le Menestrel, Erin Craig, Robert Tibshirani, Trevor Hastie, Manuel Rivas</dc:creator>
    </item>
    <item>
      <title>A Biased Estimator for MinMax Sampling and Distributed Aggregation</title>
      <link>https://arxiv.org/abs/2404.17690</link>
      <description>arXiv:2404.17690v1 Announce Type: cross 
Abstract: MinMax sampling is a technique for downsampling a real-valued vector which minimizes the maximum variance over all vector components. This approach is useful for reducing the amount of data that must be sent over a constrained network link (e.g. in the wide-area). MinMax can provide unbiased estimates of the vector elements, along with unbiased estimates of aggregates when vectors are combined from multiple locations. In this work, we propose a biased MinMax estimation scheme, B-MinMax, which trades an increase in estimator bias for a reduction in variance. We prove that when no aggregation is performed, B-MinMax obtains a strictly lower MSE compared to the unbiased MinMax estimator. When aggregation is required, B-MinMax is preferable when sample sizes are small or the number of aggregated vectors is limited. Our experiments show that this approach can substantially reduce the MSE for MinMax sampling in many practical settings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17690v1</guid>
      <category>cs.LG</category>
      <category>cs.DC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Joel Wolfrath, Abhishek Chandra</dc:creator>
    </item>
    <item>
      <title>Manipulating a Continuous Instrumental Variable in an Observational Study of Premature Babies: Algorithm, Partial Identification Bounds, and Inference under Randomization and Biased Randomization Assumptions</title>
      <link>https://arxiv.org/abs/2404.17734</link>
      <description>arXiv:2404.17734v1 Announce Type: cross 
Abstract: Regionalization of intensive care for premature babies refers to a triage system of mothers with high-risk pregnancies to hospitals of varied capabilities based on risks faced by infants. Due to the limited capacity of high-level hospitals, which are equipped with advanced expertise to provide critical care, understanding the effect of delivering premature babies at such hospitals on infant mortality for different subgroups of high-risk mothers could facilitate the design of an efficient perinatal regionalization system. Towards answering this question, Baiocchi et al. (2010) proposed to strengthen an excess-travel-time-based, continuous instrumental variable (IV) in an IV-based, matched-pair design by switching focus to a smaller cohort amenable to being paired with a larger separation in the IV dose. Three elements changed with the strengthened IV: the study cohort, compliance rate and latent complier subgroup. Here, we introduce a non-bipartite, template matching algorithm that embeds data into a target, pair-randomized encouragement trial which maintains fidelity to the original study cohort while strengthening the IV. We then study randomization-based and IV-dependent, biased-randomization-based inference of partial identification bounds for the sample average treatment effect (SATE) in an IV-based matched pair design, which deviates from the usual effect ratio estimand in that the SATE is agnostic to the IV and who is matched to whom, although a strengthened IV design could narrow the partial identification bounds. Based on our proposed strengthened-IV design, we found that delivering at a high-level NICU reduced preterm babies' mortality rate compared to a low-level NICU for $81,766 \times 2 = 163,532$ mothers and their preterm babies and the effect appeared to be minimal among non-black, low-risk mothers.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17734v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Zhe Chen, Min Haeng Cho, Bo Zhang</dc:creator>
    </item>
    <item>
      <title>Accurate and fast anomaly detection in industrial processes and IoT environments</title>
      <link>https://arxiv.org/abs/2404.17925</link>
      <description>arXiv:2404.17925v1 Announce Type: cross 
Abstract: We present a novel, simple and widely applicable semi-supervised procedure for anomaly detection in industrial and IoT environments, SAnD (Simple Anomaly Detection). SAnD comprises 5 steps, each leveraging well-known statistical tools, namely; smoothing filters, variance inflation factors, the Mahalanobis distance, threshold selection algorithms and feature importance techniques. To our knowledge, SAnD is the first procedure that integrates these tools to identify anomalies and help decipher their putative causes. We show how each step contributes to tackling technical challenges that practitioners face when detecting anomalies in industrial contexts, where signals can be highly multicollinear, have unknown distributions, and intertwine short-lived noise with the long(er)-lived actual anomalies. The development of SAnD was motivated by a concrete case study from our industrial partner, which we use here to show its effectiveness. We also evaluate the performance of SAnD by comparing it with a selection of semi-supervised methods on public datasets from the literature on anomaly detection. We conclude that SAnD is effective, broadly applicable, and outperforms existing approaches in both anomaly detection and runtime.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.17925v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Simone Tonini (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa), Andrea Vandin (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa), Francesca Chiaromonte (L'EMbeDS and Institute of Economics, Sant'Anna School of Advanced Studies, Pisa, Dept. of Statistics, The Pennsylvania State University), Daniele Licari (L'EMbeDS, Sant'Anna School of Advanced Studies), Fernando Barsacchi (A. Celli Group, Lucca)</dc:creator>
    </item>
    <item>
      <title>Implicit Generative Prior for Bayesian Neural Networks</title>
      <link>https://arxiv.org/abs/2404.18008</link>
      <description>arXiv:2404.18008v1 Announce Type: cross 
Abstract: Predictive uncertainty quantification is crucial for reliable decision-making in various applied domains. Bayesian neural networks offer a powerful framework for this task. However, defining meaningful priors and ensuring computational efficiency remain significant challenges, especially for complex real-world applications. This paper addresses these challenges by proposing a novel neural adaptive empirical Bayes (NA-EB) framework. NA-EB leverages a class of implicit generative priors derived from low-dimensional distributions. This allows for efficient handling of complex data structures and effective capture of underlying relationships in real-world datasets. The proposed NA-EB framework combines variational inference with a gradient ascent algorithm. This enables simultaneous hyperparameter selection and approximation of the posterior distribution, leading to improved computational efficiency. We establish the theoretical foundation of the framework through posterior and classification consistency. We demonstrate the practical applications of our framework through extensive evaluations on a variety of tasks, including the two-spiral problem, regression, 10 UCI datasets, and image classification tasks on both MNIST and CIFAR-10 datasets. The results of our experiments highlight the superiority of our proposed framework over existing methods, such as sparse variational Bayesian and generative models, in terms of prediction accuracy and uncertainty quantification.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18008v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Yijia Liu, Xiao Wang</dc:creator>
    </item>
    <item>
      <title>Exit Spillovers of Foreign-invested Enterprises in Shenzhen's Electronics Manufacturing Industry</title>
      <link>https://arxiv.org/abs/2404.18009</link>
      <description>arXiv:2404.18009v1 Announce Type: cross 
Abstract: Neighborhood characteristics have been broadly studied with different firm behaviors, e.g. birth, entry, expansion, and survival, except for firm exit. Using a novel dataset of foreign-invested enterprises operating in Shenzhen's electronics manufacturing industry from 2017 to 2021, I investigate the spillover effects of firm exits on other firms in the vicinity, from both the industry group and the industry class level. Significant neighborhood effects are identified for the industry group level, but not the industry class level.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18009v1</guid>
      <category>econ.GN</category>
      <category>q-fin.EC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Hanqiao Zhang</dc:creator>
    </item>
    <item>
      <title>Using Exponential Histograms to Approximate the Quantiles of Heavy- and Light-Tailed Data</title>
      <link>https://arxiv.org/abs/2404.18024</link>
      <description>arXiv:2404.18024v1 Announce Type: cross 
Abstract: Exponential histograms, with bins of the form $\left\{ \left(\rho^{k-1},\rho^{k}\right]\right\} _{k\in\mathbb{Z}}$, for $\rho&gt;1$, straightforwardly summarize the quantiles of streaming data sets (Masson et al. 2019). While they guarantee the relative accuracy of their estimates, they appear to use only $\log n$ values to summarize $n$ inputs. We study four aspects of exponential histograms -- size, accuracy, occupancy, and largest gap size -- when inputs are i.i.d. $\mathrm{Exp}\left(\lambda\right)$ or i.i.d. $\mathrm{Pareto}\left(\nu,\beta\right)$, taking $\mathrm{Exp}\left(\lambda\right)$ (or, $\mathrm{Pareto}\left(\nu,\beta\right)$) to represent all light- (or, heavy-) tailed distributions. We show that, in these settings, size grows like $\log n$ and takes on a Gumbel distribution as $n$ grows large. We bound the missing mass to the right of the histogram and the mass of its final bin and show that occupancy grows apace with size. Finally, we approximate the size of the largest number of consecutive, empty bins. Our study gives a deeper and broader view of this low-memory approach to quantile estimation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18024v1</guid>
      <category>math.ST</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.TH</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Philip T. Labo</dc:creator>
    </item>
    <item>
      <title>Pi\`eces de viole des Cinq Livres and their statistical signatures: the musical work of Marin Marais and Jordi Savall</title>
      <link>https://arxiv.org/abs/2404.18355</link>
      <description>arXiv:2404.18355v1 Announce Type: cross 
Abstract: This study analyzes the spectrum of audio signals related to the work of "Pi\`eces de viole des Cinq Livres" based on the collaborative work between Marin Marais and Jordi Savall for the underlying musical information. In particular, we explore the identification of possible statistical signatures related to this musical work. Based on the complex systems approach, we compute the spectrum of audio signals, analyze and identify their best-fit statistical distributions, and plot their relative frequencies using the scientific pitch notation. Findings suggest that the collection of frequency components related to the spectrum of each of the books that form this audio work show highly skewed and associated statistical distributions. Therefore, the most frequent statistical distribution that best describes the collection of these audio data and may be associated with a singular statistical signature is the exponential.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18355v1</guid>
      <category>cs.SD</category>
      <category>eess.AS</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Igor Lugo, Martha G. Alatriste-Contreras</dc:creator>
    </item>
    <item>
      <title>Bridging Data Barriers among Participants: Assessing the Potential of Geoenergy through Federated Learning</title>
      <link>https://arxiv.org/abs/2404.18527</link>
      <description>arXiv:2404.18527v1 Announce Type: cross 
Abstract: Machine learning algorithms emerge as a promising approach in energy fields, but its practical is hindered by data barriers, stemming from high collection costs and privacy concerns. This study introduces a novel federated learning (FL) framework based on XGBoost models, enabling safe collaborative modeling with accessible yet concealed data from multiple parties. Hyperparameter tuning of the models is achieved through Bayesian Optimization. To ascertain the merits of the proposed FL-XGBoost method, a comparative analysis is conducted between separate and centralized models to address a classical binary classification problem in geoenergy sector. The results reveal that the proposed FL framework strikes an optimal balance between privacy and accuracy. FL models demonstrate superior accuracy and generalization capabilities compared to separate models, particularly for participants with limited data or low correlation features and offers significant privacy benefits compared to centralized model. The aggregated optimization approach within the FL agreement proves effective in tuning hyperparameters. This study opens new avenues for assessing unconventional reservoirs through collaborative and privacy-preserving FL techniques.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18527v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Weike Peng, Jiaxin Gao, Yuntian Chen, Shengwei Wang</dc:creator>
    </item>
    <item>
      <title>Diversity in the radiation-induced transcriptomic temporal response of mouse brain tissue regions</title>
      <link>https://arxiv.org/abs/2404.18660</link>
      <description>arXiv:2404.18660v1 Announce Type: cross 
Abstract: A number of studies have indicated a potential association between prenatal exposure to radiation and late mental disabilities. This is believed to be due to long-term developmental changes and functional impairment of the central nervous system following radiation exposure during gestation. This study conducted a bioinformatics analysis on transcriptomic profiles from mouse brain tissue prenatally exposed to increasing doses of X-radiation. Gene expression levels were assessed in different brain regions (cortex, hippocampus, cerebellum) and collected at different time points (at 1 and 6 months after birth) for C57BL mice exposed at embryonic day E11 to varying doses of radiation (0, 0.1 and 1 Gy). This study aimed to elucidate the differences in response to radiation between different brain regions at different intervals after birth (1 and 6 months). The data was visualised using a two-dimensional Uniform Manifold Approximation and Projection (UMAP) projection, and the influence of the factors was investigated using analysis of variance (ANOVA). It was observed that gene expression was influenced by each factor (tissue, time, and dose), although to varying degrees. The gene expression trend within doses was compared for each tissue, as well as the significant pathways between tissues at different time intervals. Furthermore, in addition to radiation-responsive pathways, Cytoscape's functional and network analyses revealed changes in various pathways related to cognition, which is consistent with previously published data [1] [2] [3], indicating late behavioural changes in animals prenatally exposed to radiation.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18660v1</guid>
      <category>q-bio.NC</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Karolina Kulis, Sarah Baatout, Kevin Tabury, Joanna Polanska, Mohammed Abderrafi Benotmane</dc:creator>
    </item>
    <item>
      <title>Enhancing Uncertain Demand Prediction in Hospitals Using Simple and Advanced Machine Learning</title>
      <link>https://arxiv.org/abs/2404.18670</link>
      <description>arXiv:2404.18670v1 Announce Type: cross 
Abstract: Early and timely prediction of patient care demand not only affects effective resource allocation but also influences clinical decision-making as well as patient experience. Accurately predicting patient care demand, however, is a ubiquitous challenge for hospitals across the world due, in part, to the demand's time-varying temporal variability, and, in part, to the difficulty in modelling trends in advance. To address this issue, here, we develop two methods, a relatively simple time-vary linear model, and a more advanced neural network model. The former forecasts patient arrivals hourly over a week based on factors such as day of the week and previous 7-day arrival patterns. The latter leverages a long short-term memory (LSTM) model, capturing non-linear relationships between past data and a three-day forecasting window. We evaluate the predictive capabilities of the two proposed approaches compared to two na\"ive approaches - a reduced-rank vector autoregressive (VAR) model and the TBATS model. Using patient care demand data from Rambam Medical Center in Israel, our results show that both proposed models effectively capture hourly variations of patient demand. Additionally, the linear model is more explainable thanks to its simple architecture, whereas, by accurately modelling weekly seasonal trends, the LSTM model delivers lower prediction errors. Taken together, our explorations suggest the utility of machine learning in predicting time-varying patient care demand; additionally, it is possible to predict patient care demand with good accuracy (around 4 patients) three days or a week in advance using machine learning.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18670v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Annie Hu, Samuel Stockman, Xun Wu, Richard Wood, Bangdong Zhi, Oliver Y. Ch\'en</dc:creator>
    </item>
    <item>
      <title>Why You Should Not Trust Interpretations in Machine Learning: Adversarial Attacks on Partial Dependence Plots</title>
      <link>https://arxiv.org/abs/2404.18702</link>
      <description>arXiv:2404.18702v1 Announce Type: cross 
Abstract: The adoption of artificial intelligence (AI) across industries has led to the widespread use of complex black-box models and interpretation tools for decision making. This paper proposes an adversarial framework to uncover the vulnerability of permutation-based interpretation methods for machine learning tasks, with a particular focus on partial dependence (PD) plots. This adversarial framework modifies the original black box model to manipulate its predictions for instances in the extrapolation domain. As a result, it produces deceptive PD plots that can conceal discriminatory behaviors while preserving most of the original model's predictions. This framework can produce multiple fooled PD plots via a single model. By using real-world datasets including an auto insurance claims dataset and COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) dataset, our results show that it is possible to intentionally hide the discriminatory behavior of a predictor and make the black-box model appear neutral through interpretation tools like PD plots while retaining almost all the predictions of the original black-box model. Managerial insights for regulators and practitioners are provided based on the findings.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18702v1</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.ML</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Xi Xin, Fei Huang, Giles Hooker</dc:creator>
    </item>
    <item>
      <title>CVTN: Cross Variable and Temporal Integration for Time Series Forecasting</title>
      <link>https://arxiv.org/abs/2404.18730</link>
      <description>arXiv:2404.18730v1 Announce Type: cross 
Abstract: In multivariate time series forecasting, the Transformer architecture encounters two significant challenges: effectively mining features from historical sequences and avoiding overfitting during the learning of temporal dependencies. To tackle these challenges, this paper deconstructs time series forecasting into the learning of historical sequences and prediction sequences, introducing the Cross-Variable and Time Network (CVTN). This unique method divides multivariate time series forecasting into two phases: cross-variable learning for effectively mining fea tures from historical sequences, and cross-time learning to capture the temporal dependencies of prediction sequences. Separating these two phases helps avoid the impact of overfitting in cross-time learning on cross-variable learning. Exten sive experiments on various real-world datasets have confirmed its state-of-the-art (SOTA) performance. CVTN emphasizes three key dimensions in time series fore casting: the short-term and long-term nature of time series (locality and longevity), feature mining from both historical and prediction sequences, and the integration of cross-variable and cross-time learning. This approach not only advances the current state of time series forecasting but also provides a more comprehensive framework for future research in this field.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18730v1</guid>
      <category>cs.LG</category>
      <category>cs.AI</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Han Zhou, Yuntian Chen</dc:creator>
    </item>
    <item>
      <title>A real-time digital twin of azimuthal thermoacoustic instabilities</title>
      <link>https://arxiv.org/abs/2404.18793</link>
      <description>arXiv:2404.18793v1 Announce Type: cross 
Abstract: When they occur, azimuthal thermoacoustic oscillations can detrimentally affect the safe operation of gas turbines and aeroengines. We develop a real-time digital twin of azimuthal thermoacoustics of a hydrogen-based annular combustor. The digital twin seamlessly combines two sources of information about the system (i) a physics-based low-order model; and (ii) raw and sparse experimental data from microphones, which contain both aleatoric noise and turbulent fluctuations. First, we derive a low-order thermoacoustic model for azimuthal instabilities, which is deterministic. Second, we propose a real-time data assimilation framework to infer the acoustic pressure, the physical parameters, and the model and measurement biases simultaneously. This is the bias-regularized ensemble Kalman filter (r-EnKF), for which we find an analytical solution that solves the optimization problem. Third, we propose a reservoir computer, which infers both the model bias and measurement bias to close the assimilation equations. Fourth, we propose a real-time digital twin of the azimuthal thermoacoustic dynamics of a laboratory hydrogen-based annular combustor for a variety of equivalence ratios. We find that the real-time digital twin (i) autonomously predicts azimuthal dynamics, in contrast to bias-unregularized methods; (ii) uncovers the physical acoustic pressure from the raw data, i.e., it acts as a physics-based filter; (iii) is a time-varying parameter system, which generalizes existing models that have constant parameters, and capture only slow-varying variables. The digital twin generalizes to all equivalence ratios, which bridges the gap of existing models. This work opens new opportunities for real-time digital twinning of multi-physics problems.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18793v1</guid>
      <category>physics.flu-dyn</category>
      <category>stat.AP</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Andrea N\'ovoa, Nicolas Noiray, James R. Dawson, Luca Magri</dc:creator>
    </item>
    <item>
      <title>VT-MRF-SPF: Variable Target Markov Random Field Scalable Particle Filter</title>
      <link>https://arxiv.org/abs/2404.18857</link>
      <description>arXiv:2404.18857v1 Announce Type: cross 
Abstract: Markov random fields (MRFs) are invaluable tools across diverse fields, and spatiotemporal MRFs (STMRFs) amplify their effectiveness by integrating spatial and temporal dimensions. However, modeling spatiotemporal data introduces additional hurdles, including dynamic spatial dimensions and partial observations, prevalent in scenarios like disease spread analysis and environmental monitoring. Tracking high-dimensional targets with complex spatiotemporal interactions over extended periods poses significant challenges in accuracy, efficiency, and computational feasibility. To tackle these obstacles, we introduce the variable target MRF scalable particle filter (VT-MRF-SPF), a fully online learning algorithm designed for high-dimensional target tracking over STMRFs with varying dimensions under partial observation. We rigorously guarantee algorithm performance, explicitly indicating overcoming the curse of dimensionality. Additionally, we provide practical guidelines for tuning graphical parameters, leading to superior performance in extensive examinations.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.18857v1</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Ning Ning</dc:creator>
    </item>
    <item>
      <title>Bayesian Sparse Vector Autoregressive Switching Models with Application to Human Gesture Phase Segmentation</title>
      <link>https://arxiv.org/abs/2302.05347</link>
      <description>arXiv:2302.05347v2 Announce Type: replace 
Abstract: We propose a sparse vector autoregressive (VAR) hidden semi-Markov model (HSMM) for modeling temporal and contemporaneous (e.g. spatial) dependencies in multivariate nonstationary time series. The HSMM's generic state distribution is embedded in a special transition matrix structure, facilitating efficient likelihood evaluations and arbitrary approximation accuracy. To promote sparsity of the VAR coefficients, we deploy an $l_1$-ball projection prior, which combines differentiability with a positive probability of obtaining exact zeros, achieving variable selection within each switching state. This also facilitates posterior estimation via Hamiltonian Monte Carlo (HMC). We further place non-local priors on the parameters of the HSMM dwell distribution improving the ability of Bayesian model selection to distinguish whether the data is better supported by the simpler hidden Markov model (HMM), or the more flexible HSMM. Our proposed methodology is illustrated via an application to human gesture phase segmentation based on sensor data, where we successfully identify and characterize the periods of rest and active gesturing, as well as the dynamical patterns involved in the gesture movements associated with each of these states.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.05347v2</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Beniamino Hadj-Amar, Jack Jewson, Marina Vannucci</dc:creator>
    </item>
    <item>
      <title>Analyzing Taiwanese traffic patterns on consecutive holidays through forecast reconciliation and prediction-based anomaly detection techniques</title>
      <link>https://arxiv.org/abs/2307.09537</link>
      <description>arXiv:2307.09537v3 Announce Type: replace 
Abstract: This study explores traffic patterns on Taiwanese highways during consecutive holidays and focuses on understanding Taiwanese highway traffic behavior. We propose a prediction-based detection method for finding highway traffic anomalies using reconciled ordinary least squares (OLS) forecasts and bootstrap prediction intervals. Two fundamental features of traffic flow time series -- namely, seasonality and spatial autocorrelation -- are captured by adding Fourier terms in OLS models, spatial aggregation (as a hierarchical structure mimicking the geographical division in regions, cities, and stations), and a reconciliation step. Our approach, although simple, is able to model complex traffic datasets with reasonable accuracy. Being based on OLS, it is efficient and permits avoiding the computational burden of more complex methods. Analyses of Taiwan's consecutive holidays in 2019, 2020, and 2021 (73 days) showed strong variations in anomalies across different directions and highways. Specifically, we detected some areas and highways comprising a high number of traffic anomalies (north direction-central and southern regions-highways No. 1 and 3, south direction-southern region-highway No.3), and others with generally normal traffic (east and west direction). These results could provide important decision-support information to traffic authorities.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.09537v3</guid>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mahsa Ashouri, Frederick Kin Hing Phoa, Marzia A. Cremona</dc:creator>
    </item>
    <item>
      <title>AI-driven non-intrusive uncertainty quantification of advanced nuclear fuels for digital twin-enabling technology</title>
      <link>https://arxiv.org/abs/2211.13687</link>
      <description>arXiv:2211.13687v3 Announce Type: replace-cross 
Abstract: In response to the urgent need to establish AI/ML-integrated Digital Twin (DT) technology within next-generation nuclear systems, advancements in modeling methods and simulation codes are necessary. The increased complexity of models demands significant computational resources to quantify their uncertainties. To address this challenge, a data-driven non-intrusive uncertainty quantification method via polynomial chaos expansion is introduced as an efficient strategy within the finite element analysis-based fuel performance code BISON. Models of and fuels, alongside SiC/SiC cladding material, were prepared to demonstrate the proposed method. The impact of four independent uncertain input variables on the system output was quantified, requiring fewer than 100 BISON simulations for each model. This approach not only accelerates the modeling and simulation task but also enhances the reliability in the development of DT-enabling technologies.</description>
      <guid isPermaLink="false">oai:arXiv.org:2211.13687v3</guid>
      <category>stat.CO</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.pnucene.2024.105177</arxiv:DOI>
      <arxiv:journal_reference>Progress in Nuclear Energy 172 (2024): 105177</arxiv:journal_reference>
      <dc:creator>Kazuma Kobayashi, Dinesh Kumar, Syed Bahauddin Alam</dc:creator>
    </item>
    <item>
      <title>Explainable, Interpretable &amp; Trustworthy AI for Intelligent Digital Twin: Case Study on Remaining Useful Life</title>
      <link>https://arxiv.org/abs/2301.06676</link>
      <description>arXiv:2301.06676v2 Announce Type: replace-cross 
Abstract: Artificial intelligence (AI) and Machine learning (ML) are increasingly used in energy and engineering systems, but these models must be fair, unbiased, and explainable. It is critical to have confidence in AI's trustworthiness. ML techniques have been useful in predicting important parameters and in improving model performance. However, for these AI techniques to be useful for making decisions, they need to be audited, accounted for, and easy to understand. Therefore, the use of explainable AI (XAI) and interpretable machine learning (IML) is crucial for the accurate prediction of prognostics, such as remaining useful life (RUL), in a digital twin system, to make it intelligent while ensuring that the AI model is transparent in its decision-making processes and that the predictions it generates can be understood and trusted by users. By using AI that is explainable, interpretable, and trustworthy, intelligent digital twin systems can make more accurate predictions of RUL, leading to better maintenance and repair planning, and ultimately, improved system performance. The objective of this paper is to explain the ideas of XAI and IML and to justify the important role of AI/ML in the digital twin framework and components, which requires XAI to understand the prediction better. This paper explains the importance of XAI and IML in both local and global aspects to ensure the use of trustworthy AI/ML applications for RUL prediction. We used the RUL prediction for the XAI and IML studies and leveraged the integrated Python toolbox for interpretable machine learning~(PiML).</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06676v2</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.engappai.2023.107620</arxiv:DOI>
      <arxiv:journal_reference>Engineering Applications of Artificial Intelligence 129 (2024): 107620</arxiv:journal_reference>
      <dc:creator>Kazuma Kobayashi, Syed Bahauddin Alam</dc:creator>
    </item>
    <item>
      <title>Improved generalization with deep neural operators for engineering systems: Path towards digital twin</title>
      <link>https://arxiv.org/abs/2301.06701</link>
      <description>arXiv:2301.06701v3 Announce Type: replace-cross 
Abstract: Neural Operator Networks (ONets) represent a novel advancement in machine learning algorithms, offering a robust and generalizable alternative for approximating partial differential equations (PDEs) solutions. Unlike traditional Neural Networks (NN), which directly approximate functions, ONets specialize in approximating mathematical operators, enhancing their efficacy in addressing complex PDEs. In this work, we evaluate the capabilities of Deep Operator Networks (DeepONets), an ONets implementation using a branch/trunk architecture. Three test cases are studied: a system of ODEs, a general diffusion system, and the convection/diffusion Burgers equation. It is demonstrated that DeepONets can accurately learn the solution operators, achieving prediction accuracy scores above 0.96 for the ODE and diffusion problems over the observed domain while achieving zero shot (without retraining) capability. More importantly, when evaluated on unseen scenarios (zero shot feature), the trained models exhibit excellent generalization ability. This underscores ONets vital niche for surrogate modeling and digital twin development across physical systems. While convection-diffusion poses a greater challenge, the results confirm the promise of ONets and motivate further enhancements to the DeepONet algorithm. This work represents an important step towards unlocking the potential of digital twins through robust and generalizable surrogates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.06701v3</guid>
      <category>cs.LG</category>
      <category>stat.AP</category>
      <category>stat.CO</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1016/j.engappai.2024.107844</arxiv:DOI>
      <arxiv:journal_reference>Engineering Applications of Artificial Intelligence 131 (2024): 107844</arxiv:journal_reference>
      <dc:creator>Kazuma Kobayashi, James Daniell, Syed Bahauddin Alam</dc:creator>
    </item>
    <item>
      <title>A switching state-space transmission model for tracking epidemics and assessing interventions</title>
      <link>https://arxiv.org/abs/2307.16138</link>
      <description>arXiv:2307.16138v2 Announce Type: replace-cross 
Abstract: The effective control of infectious diseases relies on accurate assessment of the impact of interventions, which is often hindered by the complex dynamics of the spread of disease. A Beta-Dirichlet switching state-space transmission model is proposed to track underlying dynamics of disease and evaluate the effectiveness of interventions simultaneously. As time evolves, the switching mechanism introduced in the susceptible-exposed-infected-recovered (SEIR) model is able to capture the timing and magnitude of changes in the transmission rate due to the effectiveness of control measures. The implementation of this model is based on a particle Markov Chain Monte Carlo algorithm, which can estimate the time evolution of SEIR states, switching states, and high-dimensional parameters efficiently. The efficacy of the proposed model and estimation procedure are demonstrated through simulation studies. With a real-world application to British Columbia's COVID-19 outbreak, the proposed switching state-space transmission model quantifies the reduction of transmission rate following interventions. The proposed model provides a promising tool to inform public health policies aimed at studying the underlying dynamics and evaluating the effectiveness of interventions during the spread of the disease.</description>
      <guid isPermaLink="false">oai:arXiv.org:2307.16138v2</guid>
      <category>stat.ME</category>
      <category>physics.soc-ph</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Jingxue Feng, Liangliang Wang</dc:creator>
    </item>
    <item>
      <title>Physics-constrained robust learning of open-form partial differential equations from limited and noisy data</title>
      <link>https://arxiv.org/abs/2309.07672</link>
      <description>arXiv:2309.07672v2 Announce Type: replace-cross 
Abstract: Unveiling the underlying governing equations of nonlinear dynamic systems remains a significant challenge. Insufficient prior knowledge hinders the determination of an accurate candidate library, while noisy observations lead to imprecise evaluations, which in turn result in redundant function terms or erroneous equations. This study proposes a framework to robustly uncover open-form partial differential equations (PDEs) from limited and noisy data. The framework operates through two alternating update processes: discovering and embedding. The discovering phase employs symbolic representation and a novel reinforcement learning (RL)-guided hybrid PDE generator to efficiently produce diverse open-form PDEs with tree structures. A neural network-based predictive model fits the system response and serves as the reward evaluator for the generated PDEs. PDEs with higher rewards are utilized to iteratively optimize the generator via the RL strategy and the best-performing PDE is selected by a parameter-free stability metric. The embedding phase integrates the initially identified PDE from the discovering process as a physical constraint into the predictive model for robust training. The traversal of PDE trees automates the construction of the computational graph and the embedding process without human intervention. Numerical experiments demonstrate our framework's capability to uncover governing equations from nonlinear dynamic systems with limited and highly noisy data and outperform other physics-informed neural network-based discovery methods. This work opens new potential for exploring real-world systems with limited understanding.</description>
      <guid isPermaLink="false">oai:arXiv.org:2309.07672v2</guid>
      <category>cs.LG</category>
      <category>cs.NA</category>
      <category>math.NA</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Mengge Du, Yuntian Chen, Longfeng Nie, Siyu Lou, Dongxiao Zhang</dc:creator>
    </item>
  </channel>
</rss>
