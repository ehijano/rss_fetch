<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:arxiv="http://arxiv.org/schemas/atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>stat.ME updates on arXiv.org</title>
    <link>http://rss.arxiv.org/rss/stat.ME</link>
    <description>stat.ME updates on the arXiv.org e-print archive.</description>
    <atom:link href="http://rss.arxiv.org/rss/stat.ME" rel="self" type="application/rss+xml"/>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <language>en-us</language>
    <lastBuildDate>Mon, 15 Apr 2024 04:00:31 +0000</lastBuildDate>
    <managingEditor>rss-help@arxiv.org</managingEditor>
    <pubDate>Mon, 15 Apr 2024 00:00:00 -0400</pubDate>
    <skipDays>
      <day>Sunday</day>
      <day>Saturday</day>
    </skipDays>
    <item>
      <title>Inference of treatment effect and its regional modifiers using restricted mean survival time in multi-regional clinical trials</title>
      <link>https://arxiv.org/abs/2404.08128</link>
      <description>arXiv:2404.08128v1 Announce Type: new 
Abstract: Multi-regional clinical trials (MRCTs) play an increasingly crucial role in global pharmaceutical development by expediting data gathering and regulatory approval across diverse patient populations. However, differences in recruitment practices and regional demographics often lead to variations in study participant characteristics, potentially biasing treatment effect estimates and undermining treatment effect consistency assessment across regions. To address this challenge, we propose novel estimators and inference methods utilizing inverse probability of sampling and calibration weighting. Our approaches aim to eliminate exogenous regional imbalance while preserving intrinsic differences across regions, such as race and genetic variants. Moreover, time-to-event outcomes in MRCT studies receive limited attention, with existing methodologies primarily focusing on hazard ratios. In this paper, we adopt restricted mean survival time to characterize the treatment effect, offering more straightforward interpretations of treatment effects with fewer assumptions than hazard ratios. Theoretical results are established for the proposed estimators, supported by extensive simulation studies. We illustrate the effectiveness of our methods through a real MRCT case study on acute coronary syndromes.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08128v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Kaiyuan Hua, Hwanhee Hong, Xiaofei Wang</dc:creator>
    </item>
    <item>
      <title>AutoGFI: Streamlined Generalized Fiducial Inference for Modern Inference Problems</title>
      <link>https://arxiv.org/abs/2404.08169</link>
      <description>arXiv:2404.08169v1 Announce Type: new 
Abstract: The origins of fiducial inference trace back to the 1930s when R. A. Fisher first introduced the concept as a response to what he perceived as a limitation of Bayesian inference - the requirement for a subjective prior distribution on model parameters in cases where no prior information was available. However, Fisher's initial fiducial approach fell out of favor as complications arose, particularly in multi-parameter problems. In the wake of 2000, amidst a renewed interest in contemporary adaptations of fiducial inference, generalized fiducial inference (GFI) emerged to extend Fisher's fiducial argument, providing a promising avenue for addressing numerous crucial and practical inference challenges. Nevertheless, the adoption of GFI has been limited due to its often demanding mathematical derivations and the necessity for implementing complex Markov Chain Monte Carlo algorithms. This complexity has impeded its widespread utilization and practical applicability. This paper presents a significant advancement by introducing an innovative variant of GFI designed to alleviate these challenges. Specifically, this paper proposes AutoGFI, an easily implementable algorithm that streamlines the application of GFI to a broad spectrum of inference problems involving additive noise. AutoGFI can be readily implemented as long as a fitting routine is available, making it accessible to a broader audience of researchers and practitioners. To demonstrate its effectiveness, AutoGFI is applied to three contemporary and challenging problems: tensor regression, matrix completion, and regression with network cohesion. These case studies highlight the immense potential of GFI and illustrate AutoGFI's promising performance when compared to specialized solutions for these problems. Overall, this research paves the way for a more accessible and powerful application of GFI in a range of practical domains.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08169v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Wei Du, Jan Hannig, Thomas C. M. Lee, Yi Su, Chunzhe Zhang</dc:creator>
    </item>
    <item>
      <title>A unified generalization of inverse regression via adaptive column selection</title>
      <link>https://arxiv.org/abs/2404.08284</link>
      <description>arXiv:2404.08284v1 Announce Type: new 
Abstract: A bottleneck of sufficient dimension reduction (SDR) in the modern era is that, among numerous methods, only the sliced inverse regression (SIR) is generally applicable under the high-dimensional settings. The higher-order inverse regression methods, which form a major family of SDR methods that are superior to SIR in the population level, suffer from the dimensionality of their intermediate matrix-valued parameters that have an excessive number of columns. In this paper, we propose the generic idea of using a small subset of columns of the matrix-valued parameter for SDR estimation, which breaks the convention of using the ambient matrix for the higher-order inverse regression methods. With the aid of a quick column selection procedure, we then generalize these methods as well as their ensembles towards sparsity under the ultrahigh-dimensional settings, in a uniform manner that resembles sparse SIR and without additional assumptions. This is the first promising attempt in the literature to free the higher-order inverse regression methods from their dimensionality, which facilitates the applicability of SDR. The gain of column selection with respect to SDR estimation efficiency is also studied under the fixed-dimensional settings. Simulation studies and a real data example are provided at the end.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08284v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Yin Jin, Wei Luo</dc:creator>
    </item>
    <item>
      <title>A Balanced Statistical Boosting Approach for GAMLSS via New Step Lengths</title>
      <link>https://arxiv.org/abs/2404.08331</link>
      <description>arXiv:2404.08331v1 Announce Type: new 
Abstract: Component-wise gradient boosting algorithms are popular for their intrinsic variable selection and implicit regularization, which can be especially beneficial for very flexible model classes. When estimating generalized additive models for location, scale and shape (GAMLSS) by means of a component-wise gradient boosting algorithm, an important part of the estimation procedure is to determine the relative complexity of the submodels corresponding to the different distribution parameters. Existing methods either suffer from a computationally expensive tuning procedure or can be biased by structural differences in the negative gradients' sizes, which, if encountered, lead to imbalances between the different submodels. Shrunk optimal step lengths have been suggested to replace the typical small fixed step lengths for a non-cyclical boosting algorithm limited to a Gaussian response variable in order to address this issue. In this article, we propose a new adaptive step length approach that accounts for the relative size of the fitted base-learners to ensure a natural balance between the different submodels. The new balanced boosting approach thus represents a computationally efficient and easily generalizable alternative to shrunk optimal step lengths. We implemented the balanced non-cyclical boosting algorithm for a Gaussian, a negative binomial as well as a Weibull distributed response variable and demonstrate the competitive performance of the new adaptive step length approach by means of a simulation study, in the analysis of count data modeling the number of doctor's visits as well as for survival data in an oncological trial.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08331v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Alexandra Daub, Andreas Mayr, Boyao Zhang, Elisabeth Bergherr</dc:creator>
    </item>
    <item>
      <title>Comment on 'Exact-corrected confidence interval for risk difference in noninferiority binomial trials'</title>
      <link>https://arxiv.org/abs/2404.08352</link>
      <description>arXiv:2404.08352v1 Announce Type: new 
Abstract: The article by Hawila &amp; Berg (2023) that is going to be commented presents four relevant problems, apart from other less important ones that are also cited. First, the title is incorrect, since it leads readers to believe that the confidence interval defined is exact when in fact it is asymptotic. Second, contrary to what is assumed by the authors of the article, the statistic that they define is not monotonic in delta. But it is fundamental that this property is verified, as the authors themselves recognize. Third, the inferences provided by the confidence interval proposed may be incoherent, which could lead the scientific community to reach incorrect conclusions in any practical application. For example, for fixed data it might happen that a certain delta value is within the 90%-confidence interval, but outside the 95%-confidence interval. Fourth, the authors do not validate its statistic through a simulation with diverse (and credible) values of the parameters involved. In fact, one of its two examples is for an alpha error of 70%!</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08352v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>A. Mart\'in Andr\'es, I. Herranz Tejedor</dc:creator>
    </item>
    <item>
      <title>confintROB Package: Confindence Intervals in robust linear mixed models</title>
      <link>https://arxiv.org/abs/2404.08426</link>
      <description>arXiv:2404.08426v1 Announce Type: new 
Abstract: Statistical inference is a major scientific endeavor for many researchers. In terms of inferential methods implemented to mixed-effects models, significant progress has been made in the R software. However, these advances primarily concern classical estimators (ML, REML) and mainly focus on fixed effects. In the confintROB package, we have implemented various bootstrap methods for computing confidence intervals (CIs) not only for fixed effects but also for variance components. These methods can be implemented with the widely used lmer function from the lme4 package, as well as with the rlmer function from the robustlmm package and the varComprob function from the robustvarComp package. These functions implement robust estimation methods suitable for data with outliers. The confintROB package implements the Wald method for fixed effects, whereas for both fixed effects and variance components, two bootstrap methods are implemented: the parametric bootstrap and the wild bootstrap. Moreover, the confintROB package can obtain both the percentile and the bias-corrected accelerated versions of CIs.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08426v1</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Mason Fabio, Koller Manuel, Cantoni Eva, Ghisletta Paolo</dc:creator>
    </item>
    <item>
      <title>A Latent Factor Model for High-Dimensional Binary Data</title>
      <link>https://arxiv.org/abs/2404.08457</link>
      <description>arXiv:2404.08457v1 Announce Type: new 
Abstract: In this study, we develop a latent factor model for analysing high-dimensional binary data. Specifically, a standard probit model is used to describe the regression relationship between the observed binary data and the continuous latent variables. Our method assumes that the dependency structure of the observed binary data can be fully captured by the continuous latent factors. To estimate the model, a moment-based estimation method is developed. The proposed method is able to deal with both discontinuity and high dimensionality. Most importantly, the asymptotic properties of the resulting estimators are rigorously established. Extensive simulation studies are presented to demonstrate the proposed methodology. A real dataset about product descriptions is analysed for illustration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08457v1</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>new</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Jiaxin Shi, Yuan Gao, Rui Pan, Hansheng Wang</dc:creator>
    </item>
    <item>
      <title>Shifting the Paradigm: Estimating Heterogeneous Treatment Effects in the Development of Walkable Cities Design</title>
      <link>https://arxiv.org/abs/2404.08208</link>
      <description>arXiv:2404.08208v1 Announce Type: cross 
Abstract: The transformation of urban environments to accommodate growing populations has profoundly impacted public health and well-being. This paper addresses the critical challenge of estimating the impact of urban design interventions on diverse populations. Traditional approaches, reliant on questionnaires and stated preference techniques, are limited by recall bias and capturing the complex dynamics between environmental attributes and individual characteristics. To address these challenges, we integrate Virtual Reality (VR) with observational causal inference methods to estimate heterogeneous treatment effects, specifically employing Targeted Maximum Likelihood Estimation (TMLE) for its robustness against model misspecification. Our innovative approach leverages VR-based experiment to collect data that reflects perceptual and experiential factors. The result shows the heterogeneous impacts of urban design elements on public health and underscore the necessity for personalized urban design interventions. This study not only extends the application of TMLE to built environment research but also informs public health policy by illuminating the nuanced effects of urban design on mental well-being and advocating for tailored strategies that foster equitable, health-promoting urban spaces.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.08208v1</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-sa/4.0/</dc:rights>
      <dc:creator>Jie Zhu, Bojing Liao, Theo A. Arentze</dc:creator>
    </item>
    <item>
      <title>Interpretable discriminant analysis for functional data supported on random nonlinear domains with an application to Alzheimer's disease</title>
      <link>https://arxiv.org/abs/2112.02712</link>
      <description>arXiv:2112.02712v3 Announce Type: replace 
Abstract: We introduce a novel framework for the classification of functional data supported on nonlinear, and possibly random, manifold domains. The motivating application is the identification of subjects with Alzheimer's disease from their cortical surface geometry and associated cortical thickness map. The proposed model is based upon a reformulation of the classification problem as a regularized multivariate functional linear regression model. This allows us to adopt a direct approach to the estimation of the most discriminant direction while controlling for its complexity with appropriate differential regularization. Our approach does not require prior estimation of the covariance structure of the functional predictors, which is computationally prohibitive in our application setting. We provide a theoretical analysis of the out-of-sample prediction error of the proposed model and explore the finite sample performance in a simulation setting. We apply the proposed method to a pooled dataset from the Alzheimer's Disease Neuroimaging Initiative and the Parkinson's Progression Markers Initiative. Through this application, we identify discriminant directions that capture both cortical geometric and thickness predictive features of Alzheimer's disease that are consistent with the existing neuroscience literature.</description>
      <guid isPermaLink="false">oai:arXiv.org:2112.02712v3</guid>
      <category>stat.ME</category>
      <category>stat.AP</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <arxiv:DOI>10.1093/jrsssb/qkae023</arxiv:DOI>
      <arxiv:journal_reference>Journal of the Royal Statistical Society Series B: Statistical Methodology, 2024</arxiv:journal_reference>
      <dc:creator>Eardi Lila, Wenbo Zhang, Swati Rane Levendovszky</dc:creator>
    </item>
    <item>
      <title>Score Matching for Truncated Density Estimation on a Manifold</title>
      <link>https://arxiv.org/abs/2206.14668</link>
      <description>arXiv:2206.14668v2 Announce Type: replace 
Abstract: When observations are truncated, we are limited to an incomplete picture of our dataset. Recent methods propose to use score matching for truncated density estimation, where the access to the intractable normalising constant is not required. We present a novel extension of truncated score matching to a Riemannian manifold with boundary. Applications are presented for the von Mises-Fisher and Kent distributions on a two dimensional sphere in $\mathbb{R}^3$, as well as a real-world application of extreme storm observations in the USA. In simulated data experiments, our score matching estimator is able to approximate the true parameter values with a low estimation error and shows improvements over a naive maximum likelihood estimator.</description>
      <guid isPermaLink="false">oai:arXiv.org:2206.14668v2</guid>
      <category>stat.ME</category>
      <category>stat.ML</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel J. Williams, Song Liu</dc:creator>
    </item>
    <item>
      <title>A two-way heterogeneity model for dynamic networks</title>
      <link>https://arxiv.org/abs/2305.12643</link>
      <description>arXiv:2305.12643v2 Announce Type: replace 
Abstract: Dynamic network data analysis requires joint modelling individual snapshots and time dynamics. This paper proposes a new two-way heterogeneity model towards this goal. The new model equips each node of the network with two heterogeneity parameters, one to characterize the propensity of forming ties with other nodes and the other to differentiate the tendency of retaining existing ties over time. Though the negative log-likelihood function is non-convex, it is locally convex in a neighbourhood of the true value of the parameter vector. By using a novel method of moments estimator as the initial value, the consistent local maximum likelihood estimator (MLE) can be obtained by a gradient descent algorithm. To establish the upper bound for the estimation error of the MLE, we derive a new uniform deviation bound, which is of independent interest. The usefulness of the model and the associated theory are further supported by extensive simulation and the analysis of some real network data sets.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.12643v2</guid>
      <category>stat.ME</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Binyan Jiang, Chenlei Leng, Ting Yan, Qiwei Yao, Xinyang Yu</dc:creator>
    </item>
    <item>
      <title>Model validation for aggregate inferences in out-of-sample prediction</title>
      <link>https://arxiv.org/abs/2312.06334</link>
      <description>arXiv:2312.06334v3 Announce Type: replace 
Abstract: Generalization to new samples is a fundamental rationale for statistical modeling. For this purpose, model validation is particularly important, but recent work in survey inference has suggested that simple aggregation of individual prediction scores does not give a good measure of the score for population aggregate estimates. In this manuscript we explain why this occurs, propose two scoring metrics designed specifically for this problem, and demonstrate their use in three different ways. We show that these scoring metrics correctly order models when compared to the true score, although they do underestimate the magnitude of the score. We demonstrate with a problem in survey research, where multilevel regression and poststratification (MRP) has been used extensively to adjust convenience and low-response surveys to make population and subpopulation estimates.</description>
      <guid isPermaLink="false">oai:arXiv.org:2312.06334v3</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lauren Kennedy, Aki Vehtari, Andrew Gelman</dc:creator>
    </item>
    <item>
      <title>Inference for Synthetic Controls via Refined Placebo Tests</title>
      <link>https://arxiv.org/abs/2401.07152</link>
      <description>arXiv:2401.07152v2 Announce Type: replace 
Abstract: The synthetic control method is often applied to problems with one treated unit and a small number of control units. A common inferential task in this setting is to test null hypotheses regarding the average treatment effect on the treated. Inference procedures that are justified asymptotically are often unsatisfactory due to (1) small sample sizes that render large-sample approximation fragile and (2) simplification of the estimation procedure that is implemented in practice. An alternative is permutation inference, which is related to a common diagnostic called the placebo test. It has provable Type-I error guarantees in finite samples without simplification of the method, when the treatment is uniformly assigned. Despite this robustness, the placebo test suffers from low resolution since the null distribution is constructed from only $N$ reference estimates, where $N$ is the sample size. This creates a barrier for statistical inference at a common level like $\alpha = 0.05$, especially when $N$ is small. We propose a novel leave-two-out procedure that bypasses this issue, while still maintaining the same finite-sample Type-I error guarantee under uniform assignment for a wide range of $N$. Unlike the placebo test whose Type-I error always equals the theoretical upper bound, our procedure often achieves a lower unconditional Type-I error than theory suggests; this enables useful inference in the challenging regime when $\alpha &lt; 1/N$. Empirically, our procedure achieves a higher power when the effect size is reasonably large and a comparable power otherwise. We generalize our procedure to non-uniform assignments and show how to conduct sensitivity analysis. From a methodological perspective, our procedure can be viewed as a new type of randomization inference different from permutation or rank-based inference, which is particularly effective in small samples.</description>
      <guid isPermaLink="false">oai:arXiv.org:2401.07152v2</guid>
      <category>stat.ME</category>
      <category>econ.EM</category>
      <category>math.ST</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Lihua Lei, Timothy Sudijono</dc:creator>
    </item>
    <item>
      <title>Towards a turnkey approach to unbiased Monte Carlo estimation of smooth functions of expectations</title>
      <link>https://arxiv.org/abs/2403.20313</link>
      <description>arXiv:2403.20313v2 Announce Type: replace 
Abstract: Given a smooth function $f$, we develop a general approach to turn Monte Carlo samples with expectation $m$ into an unbiased estimate of $f(m)$. Specifically, we develop estimators that are based on randomly truncating the Taylor series expansion of $f$ and estimating the coefficients of the truncated series. We derive their properties and propose a strategy to set their tuning parameters -- which depend on $m$ -- automatically, with a view to make the whole approach simple to use. We develop our methods for the specific functions $f(x)=\log x$ and $f(x)=1/x$, as they arise in several statistical applications such as maximum likelihood estimation of latent variable models and Bayesian inference for un-normalised models. Detailed numerical studies are performed for a range of applications to determine how competitive and reliable the proposed approach is.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.20313v2</guid>
      <category>stat.ME</category>
      <category>stat.CO</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Nicolas Chopin, Francesca R. Crucinio, Sumeetpal S. Singh</dc:creator>
    </item>
    <item>
      <title>Orthogonal calibration via posterior projections with applications to the Schwarzschild model</title>
      <link>https://arxiv.org/abs/2404.03152</link>
      <description>arXiv:2404.03152v2 Announce Type: replace 
Abstract: The orbital superposition method originally developed by Schwarzschild (1979) is used to study the dynamics of growth of a black hole and its host galaxy, and has uncovered new relationships between the galaxy's global characteristics. Scientists are specifically interested in finding optimal parameter choices for this model that best match physical measurements along with quantifying the uncertainty of such procedures. This renders a statistical calibration problem with multivariate outcomes. In this article, we develop a Bayesian method for calibration with multivariate outcomes using orthogonal bias functions thus ensuring parameter identifiability. Our approach is based on projecting the posterior to an appropriate space which allows the user to choose any nonparametric prior on the bias function(s) instead of having to model it (them) with Gaussian processes. We develop a functional projection approach using the theory of Hilbert spaces. A finite-dimensional analogue of the projection problem is also considered. We illustrate the proposed approach using a BART prior and apply it to calibrate the Schwarzschild model illustrating how a multivariate approach may resolve discrepancies resulting from a univariate calibration.</description>
      <guid isPermaLink="false">oai:arXiv.org:2404.03152v2</guid>
      <category>stat.ME</category>
      <arxiv:announce_type>replace</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by/4.0/</dc:rights>
      <dc:creator>Antik Chakraborty, Jonelle B. Walsh, Louis Strigari, Bani K. Mallick, Anirban Bhattacharya</dc:creator>
    </item>
    <item>
      <title>Partial Conditioning for Inference of Many-Normal-Means with H\"older Constraints</title>
      <link>https://arxiv.org/abs/2301.04512</link>
      <description>arXiv:2301.04512v2 Announce Type: replace-cross 
Abstract: Inferential models have been proposed for valid and efficient prior-free probabilistic inference. As it gradually gained popularity, this theory is subject to further developments for practically challenging problems. This paper considers the many-normal-means problem with the means constrained to be in the neighborhood of each other, formally represented by a H\"older space. A new method, called partial conditioning, is proposed to generate valid and efficient marginal inference about the individual means. It is shown that the method outperforms both a fiducial-counterpart in terms of validity and a conservative-counterpart in terms of efficiency. We conclude the paper by remarking that a general theory of partial conditioning for inferential models deserves future development.</description>
      <guid isPermaLink="false">oai:arXiv.org:2301.04512v2</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Jiasen Yang, Xiao Wang, Chuanhai Liu</dc:creator>
    </item>
    <item>
      <title>Reference prior for Bayesian estimation of seismic fragility curves</title>
      <link>https://arxiv.org/abs/2302.06935</link>
      <description>arXiv:2302.06935v4 Announce Type: replace-cross 
Abstract: One of the key elements of probabilistic seismic risk assessment studies is the fragility curve, which represents the conditional probability of failure of a mechanical structure for a given scalar measure derived from seismic ground motion. For many structures of interest, estimating these curves is a daunting task because of the limited amount of data available; data which is only binary in our framework, i.e., only describing the structure as being in a failure or non-failure state. A large number of methods described in the literature tackle this challenging framework through parametric log-normal models. Bayesian approaches, on the other hand, allow model parameters to be learned more efficiently. However, the impact of the choice of the prior distribution on the posterior distribution cannot be readily neglected and, consequently, neither can its impact on any resulting estimation. This paper proposes a comprehensive study of this parametric Bayesian estimation problem for limited and binary data. Using the reference prior theory as a cornerstone, this study develops an objective approach to choosing the prior. This approach leads to the Jeffreys prior, which is derived for this problem for the first time. The posterior distribution is proven to be proper with the Jeffreys prior but improper with some traditional priors found in the literature. With the Jeffreys prior, the posterior distribution is also shown to vanish at the boundaries of the parameters' domain, which means that sampling the posterior distribution of the parameters does not result in anomalously small or large values. Therefore, the use of the Jeffreys prior does not result in degenerate fragility curves such as unit-step functions, and leads to more robust credibility intervals. The numerical results obtained from different case studies-including an industrial example-illustrate the theoretical predictions.</description>
      <guid isPermaLink="false">oai:arXiv.org:2302.06935v4</guid>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Antoine Van Biesbroeck, Clement Gauchy, Cyril Feau, Josselin Garnier</dc:creator>
    </item>
    <item>
      <title>Root-n consistent semiparametric learning with high-dimensional nuisance functions under minimal sparsity</title>
      <link>https://arxiv.org/abs/2305.04174</link>
      <description>arXiv:2305.04174v3 Announce Type: replace-cross 
Abstract: Treatment effect estimation under unconfoundedness is a fundamental task in causal inference. In response to the challenge of analyzing high-dimensional datasets collected in substantive fields such as epidemiology, genetics, economics, and social sciences, many methods for treatment effect estimation with high-dimensional nuisance parameters (the outcome regression and the propensity score) have been developed in recent years. However, it is still unclear what is the necessary and sufficient sparsity condition on the nuisance parameters for the treatment effect to be $\sqrt{n}$-estimable. In this paper, we propose a new Double-Calibration strategy that corrects the estimation bias of the nuisance parameter estimates computed by regularized high-dimensional techniques and demonstrate that the corresponding Doubly-Calibrated estimator achieves $1 / \sqrt{n}$-rate as long as one of the nuisance parameters is sparse with sparsity below $\sqrt{n} / \log p$, where $p$ denotes the ambient dimension of the covariates, whereas the other nuisance parameter can be arbitrarily complex and completely misspecified. The Double-Calibration strategy can also be applied to settings other than treatment effect estimation, e.g. regression coefficient estimation in the presence of diverging number of controls in a semiparametric partially linear model.</description>
      <guid isPermaLink="false">oai:arXiv.org:2305.04174v3</guid>
      <category>math.ST</category>
      <category>stat.ME</category>
      <category>stat.TH</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Lin Liu, Yuhao Wang</dc:creator>
    </item>
    <item>
      <title>Approximate Stein Classes for Truncated Density Estimation</title>
      <link>https://arxiv.org/abs/2306.00602</link>
      <description>arXiv:2306.00602v2 Announce Type: replace-cross 
Abstract: Estimating truncated density models is difficult, as these models have intractable normalising constants and hard to satisfy boundary conditions. Score matching can be adapted to solve the truncated density estimation problem, but requires a continuous weighting function which takes zero at the boundary and is positive elsewhere. Evaluation of such a weighting function (and its gradient) often requires a closed-form expression of the truncation boundary and finding a solution to a complicated optimisation problem. In this paper, we propose approximate Stein classes, which in turn leads to a relaxed Stein identity for truncated density estimation. We develop a novel discrepancy measure, truncated kernelised Stein discrepancy (TKSD), which does not require fixing a weighting function in advance, and can be evaluated using only samples on the boundary. We estimate a truncated density model by minimising the Lagrangian dual of TKSD. Finally, experiments show the accuracy of our method to be an improvement over previous works even without the explicit functional form of the boundary.</description>
      <guid isPermaLink="false">oai:arXiv.org:2306.00602v2</guid>
      <category>stat.ML</category>
      <category>cs.LG</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</dc:rights>
      <dc:creator>Daniel J. Williams, Song Liu</dc:creator>
    </item>
    <item>
      <title>Non-robustness of diffusion estimates on networks with measurement error</title>
      <link>https://arxiv.org/abs/2403.05704</link>
      <description>arXiv:2403.05704v3 Announce Type: replace-cross 
Abstract: Network diffusion models are used to study things like disease transmission, information spread, and technology adoption. However, small amounts of mismeasurement are extremely likely in the networks constructed to operationalize these models. We show that estimates of diffusions are highly non-robust to this measurement error. First, we show that even when measurement error is vanishingly small, such that the share of missed links is close to zero, forecasts about the extent of diffusion will greatly underestimate the truth. Second, a small mismeasurement in the identity of the initial seed generates a large shift in the locations of expected diffusion path. We show that both of these results still hold when the vanishing measurement error is only local in nature. Such non-robustness in forecasting exists even under conditions where the basic reproductive number is consistently estimable. Possible solutions, such as estimating the measurement error or implementing widespread detection efforts, still face difficulties because the number of missed links are so small. Finally, we conduct Monte Carlo simulations on simulated networks, and real networks from three settings: travel data from the COVID-19 pandemic in the western US, a mobile phone marketing campaign in rural India, and in an insurance experiment in China.</description>
      <guid isPermaLink="false">oai:arXiv.org:2403.05704v3</guid>
      <category>econ.EM</category>
      <category>cs.SI</category>
      <category>stat.AP</category>
      <category>stat.ME</category>
      <arxiv:announce_type>replace-cross</arxiv:announce_type>
      <dc:rights>http://creativecommons.org/licenses/by-nc-nd/4.0/</dc:rights>
      <dc:creator>Arun G. Chandrasekhar, Paul Goldsmith-Pinkham, Tyler H. McCormick, Samuel Thau, Jerry Wei</dc:creator>
    </item>
  </channel>
</rss>
